<!doctype html>
<html lang="zh-cn" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.101.0" />
<link rel="canonical" type="text/html" href="https://blog.huweihuang.com/linux-notes/llm/">
<link rel="alternate" type="application/rss&#43;xml" href="https://blog.huweihuang.com/linux-notes/llm/index.xml">
<meta name="robots" content="noindex, nofollow">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>大模型 | 胡伟煌</title>
<meta name="description" content="">
<meta property="og:title" content="大模型" />
<meta property="og:description" content="Kubernetes学习笔记" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://blog.huweihuang.com/linux-notes/llm/" /><meta property="og:site_name" content="胡伟煌" />

<meta itemprop="name" content="大模型">
<meta itemprop="description" content="Kubernetes学习笔记"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="大模型"/>
<meta name="twitter:description" content="Kubernetes学习笔记"/>




<link rel="preload" href="/scss/main.min.79e3930541ca05b5395c14b2a313b798fad1dc69f9a14aefa57b62eaa9f65f14.css" as="style">
<link href="/scss/main.min.79e3930541ca05b5395c14b2a313b798fad1dc69f9a14aefa57b62eaa9f65f14.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.6.0.min.js"
  integrity="sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK"
  crossorigin="anonymous"></script>
<script
  src="https://unpkg.com/lunr@2.3.9/lunr.min.js"
  integrity="sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli"
  crossorigin="anonymous"></script>
<link rel="stylesheet" href="/css/prism.css"/>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-114718458-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" width="64" height="64"><path d="M15.9.476a2.14 2.14.0 00-.823.218L3.932 6.01c-.582.277-1.005.804-1.15 1.432L.054 19.373c-.13.56-.025 1.147.3 1.627q.057.087.12.168l7.7 9.574c.407.5 1.018.787 1.662.784h12.35c.646.001 1.258-.3 1.664-.793l7.696-9.576c.404-.5.555-1.16.4-1.786L29.2 7.43c-.145-.628-.57-1.155-1.15-1.432L16.923.695A2.14 2.14.0 0015.89.476z" fill="#326ce5"/><path d="M16.002 4.542c-.384.027-.675.356-.655.74v.188c.018.213.05.424.092.633a6.22 6.22.0 01.066 1.21c-.038.133-.114.253-.218.345l-.015.282c-.405.034-.807.096-1.203.186-1.666.376-3.183 1.24-4.354 2.485l-.24-.17c-.132.04-.274.025-.395-.04a6.22 6.22.0 01-.897-.81 5.55 5.55.0 00-.437-.465l-.148-.118c-.132-.106-.294-.167-.463-.175a.64.64.0 00-.531.236c-.226.317-.152.756.164.983l.138.11a5.55 5.55.0 00.552.323c.354.197.688.428.998.7a.74.74.0 01.133.384l.218.2c-1.177 1.766-1.66 3.905-1.358 6.006l-.28.08c-.073.116-.17.215-.286.288a6.22 6.22.0 01-1.194.197 5.57 5.57.0 00-.64.05l-.177.04h-.02a.67.67.0 00-.387 1.132.67.67.0 00.684.165h.013l.18-.02c.203-.06.403-.134.598-.218.375-.15.764-.265 1.162-.34.138.008.27.055.382.135l.3-.05c.65 2.017 2.016 3.726 3.84 4.803l-.122.255c.056.117.077.247.06.376-.165.382-.367.748-.603 1.092a5.58 5.58.0 00-.358.533l-.085.18a.67.67.0 00.65 1.001.67.67.0 00.553-.432l.083-.17c.076-.2.14-.404.192-.61.177-.437.273-.906.515-1.196a.54.54.0 01.286-.14l.15-.273a8.62 8.62.0 006.146.015l.133.255c.136.02.258.095.34.205.188.358.34.733.456 1.12a5.57 5.57.0 00.194.611l.083.17a.67.67.0 001.187.131.67.67.0 00.016-.701l-.087-.18a5.55 5.55.0 00-.358-.531c-.23-.332-.428-.686-.6-1.057a.52.52.0 01.068-.4 2.29 2.29.0 01-.111-.269c1.82-1.085 3.18-2.8 3.823-4.82l.284.05c.102-.093.236-.142.373-.138.397.076.786.2 1.162.34.195.09.395.166.598.23.048.013.118.024.172.037h.013a.67.67.0 00.841-.851.67.67.0 00-.544-.446l-.194-.046a5.57 5.57.0 00-.64-.05c-.404-.026-.804-.092-1.194-.197-.12-.067-.22-.167-.288-.288l-.27-.08a8.65 8.65.0 00-1.386-5.993l.236-.218c-.01-.137.035-.273.124-.378.307-.264.64-.497.99-.696a5.57 5.57.0 00.552-.323l.146-.118a.67.67.0 00-.133-1.202.67.67.0 00-.696.161l-.148.118a5.57 5.57.0 00-.437.465c-.264.302-.556.577-.873.823a.74.74.0 01-.404.044l-.253.18c-1.46-1.53-3.427-2.48-5.535-2.67.0-.1-.013-.25-.015-.297-.113-.078-.192-.197-.218-.332a6.23 6.23.0 01.076-1.207c.043-.21.073-.42.092-.633v-.2c.02-.384-.27-.713-.655-.74zm-.834 5.166-.2 3.493h-.015c-.01.216-.137.4-.332.504s-.426.073-.6-.054l-2.865-2.03a6.86 6.86.0 013.303-1.799c.234-.05.47-.088.707-.114zm1.668.0c1.505.187 2.906.863 3.99 1.924l-2.838 2.017c-.175.14-.415.168-.618.072s-.333-.3-.336-.524zm-6.72 3.227 2.62 2.338v.015c.163.142.234.363.186.574s-.21.378-.417.435v.01l-3.362.967a6.86 6.86.0 01.974-4.34zm11.753.0c.796 1.295 1.148 2.814 1.002 4.327l-3.367-.97v-.013c-.21-.057-.37-.224-.417-.435s.023-.43.186-.574l2.6-2.327zm-6.404 2.52h1.072l.655.832-.238 1.04-.963.463-.965-.463-.227-1.04zm3.434 2.838c.045-.005.1-.005.135.0l3.467.585c-.5 1.44-1.487 2.67-2.775 3.493l-1.34-3.244a.59.59.0 01.509-.819zm-5.823.015c.196.003.377.104.484.268s.124.37.047.55v.013l-1.332 3.218C11 21.54 10.032 20.325 9.517 18.9l3.437-.583c.038-.004.077-.004.116.0zm2.904 1.4a.59.59.0 01.537.308h.013l1.694 3.057-.677.2c-1.246.285-2.547.218-3.758-.194l1.7-3.057c.103-.18.293-.29.5-.295z" fill="#fff" stroke="#fff" stroke-width=".055"/></svg></span><span class="font-weight-bold">胡伟煌</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/kubernetes-notes/" ><span>Kubernetes学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/k8s-source-code-analysis/" ><span>Kubernetes源码分析</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/golang-notes/" ><span>Golang学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link active" href="/linux-notes/" ><span class="active">Linux学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/about/" ><span>About</span></a>
			</li>
			
			
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block"><input
  type="search"
  class="form-control td-search-input"
  placeholder="&#xf002; 站内搜索…"
  aria-label="站内搜索…"
  autocomplete="off"
  
  data-offline-search-index-json-src="/offline-search-index.728161da729d505f546fca7612fd931d.json"
  data-offline-search-base-href="/"
  data-offline-search-max-results="10"
>
</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/linux-notes/llm/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">大模型</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-093f126ec9e750c4adca83e14d272d51">基于Ollama构建本地大模型</a></li>


    
  
    
    
	
<li>2: <a href="#pg-e44128ff477d39d7fb8644a3a4a27ea6">大模型相关概念</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-093f126ec9e750c4adca83e14d272d51">1 - 基于Ollama构建本地大模型</h1>
    
	<p>本文主要介绍如何通过<code>Ollama</code>和<code>OpenWebUI</code>来搭建一个本地私有化运行的大模型工具。私有化大模型的构建主要用于解决<code>数据的安全性问题</code>，对于大部分私有数据不适合通过外部的大模型网站来上传和分析。</p>
<h1 id="1-ollama-与-openwebui-介绍">1. Ollama 与 OpenWebUI 介绍</h1>
<h2 id="1-1-ollama简介">1.1. Ollama简介</h2>
<p>Ollama 是一个 <strong>本地运行的 AI 大模型管理工具</strong>，可以让你在本地 <strong>快速拉取、管理和运行</strong> 各种开源大语言模型（如 LLaMA、Mistral、deepseek 等），而无需依赖云端 API。它的主要特点包括：</p>
<ul>
<li><strong>简易安装</strong>：支持 macOS、Linux 和 Windows（WSL）。</li>
<li><strong>本地推理</strong>：在本地设备上直接运行 LLM，保护数据隐私。</li>
<li><strong>模型管理</strong>：可以像使用 Docker 一样 <code>ollama run llama2</code> 轻松拉取和运行模型。</li>
<li><strong>自定义模型</strong>：支持通过 <code>Modelfile</code> 进行微调和定制。</li>
<li><strong>支持 API</strong>：可以通过 Python、Node.js 等语言调用 Ollama 提供的本地 REST API。</li>
</ul>
<p>Ollama 适用于本地 AI 代理、嵌入式 AI 应用、隐私保护的智能助手等场景。你可以用它来运行大语言模型，而无需自己搭建复杂的推理环境。</p>
<h2 id="1-2-openwebui简介">1.2. OpenWebUI简介</h2>
<p><strong>Open-WebUI</strong> 是一个 <strong>开源的 Web 用户界面</strong>，用于管理和使用本地或远程的大语言模型（LLM），比如 Ollama、OpenAI、Gemini 等。它的主要特点包括：</p>
<ul>
<li><strong>友好的 Web 界面</strong>：提供 ChatGPT 类似的对话 UI，方便交互。</li>
<li><strong>支持多种后端</strong>：可以连接 <strong>Ollama、OpenAI API、本地 LLM</strong> 等。</li>
<li><strong>多用户支持</strong>：适用于团队协作。</li>
<li><strong>对话历史管理</strong>：可保存和管理聊天记录。</li>
<li><strong>插件和自定义功能</strong>：支持扩展，适用于不同应用场景。</li>
</ul>
<p>它可以让本地 LLM 变得更加易用，适合个人、企业部署本地 AI 助手。</p>
<h1 id="2-部署ollama">2. 部署ollama</h1>
<h2 id="2-1-脚本安装-ollama">2.1. 脚本安装<code>ollama</code></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh <span style="color:#000;font-weight:bold">|</span> sh
</span></span></code></pre></div><p>输出</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>&gt;&gt;&gt; Installing ollama to /usr/local
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Downloading Linux amd64 bundle
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic">######################################################################## 100.0%</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama user...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to render group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to video group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding current user to ollama group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama systemd service...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Enabling and starting ollama service...
</span></span><span style="display:flex;"><span>Created symlink /etc/systemd/system/default.target.wants/ollama.service -&gt; /etc/systemd/system/ollama.service.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; The Ollama API is now available at 127.0.0.1:11434.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Install complete. Run <span style="color:#4e9a06">&#34;ollama&#34;</span> from the <span style="color:#204a87">command</span> line.
</span></span><span style="display:flex;"><span>WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.
</span></span></code></pre></div><p>默认服务监听的地址为：<code>127.0.0.1:11434</code></p>
<h2 id="2-2-查看-ollama-服务状态">2.2. 查看<code>ollama</code>服务状态</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>systemctl status ollama
</span></span><span style="display:flex;"><span>* ollama.service - Ollama Service
</span></span><span style="display:flex;"><span>     Loaded: loaded <span style="color:#ce5c00;font-weight:bold">(</span>/etc/systemd/system/ollama.service<span style="color:#000;font-weight:bold">;</span> enabled<span style="color:#000;font-weight:bold">;</span> vendor preset: enabled<span style="color:#ce5c00;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>     Active: active <span style="color:#ce5c00;font-weight:bold">(</span>running<span style="color:#ce5c00;font-weight:bold">)</span> since Fri 2025-02-07 17:21:55 +08<span style="color:#000;font-weight:bold">;</span> 23s ago
</span></span><span style="display:flex;"><span>   Main PID: <span style="color:#0000cf;font-weight:bold">53472</span> <span style="color:#ce5c00;font-weight:bold">(</span>ollama<span style="color:#ce5c00;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>      Tasks: <span style="color:#0000cf;font-weight:bold">10</span>
</span></span><span style="display:flex;"><span>     Memory: 10.3M
</span></span><span style="display:flex;"><span>     CGroup: /system.slice/ollama.service
</span></span><span style="display:flex;"><span>             <span style="color:#4e9a06">`</span>-53472 /usr/local/bin/ollama serve
</span></span></code></pre></div><p>查看<code>ollama</code>命令</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama --help</span>
</span></span><span style="display:flex;"><span>Large language model runner
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Usage:
</span></span><span style="display:flex;"><span>  ollama <span style="color:#ce5c00;font-weight:bold">[</span>flags<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>  ollama <span style="color:#ce5c00;font-weight:bold">[</span>command<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Available Commands:
</span></span><span style="display:flex;"><span>  serve       Start ollama
</span></span><span style="display:flex;"><span>  create      Create a model from a Modelfile
</span></span><span style="display:flex;"><span>  show        Show information <span style="color:#204a87;font-weight:bold">for</span> a model
</span></span><span style="display:flex;"><span>  run         Run a model
</span></span><span style="display:flex;"><span>  stop        Stop a running model
</span></span><span style="display:flex;"><span>  pull        Pull a model from a registry
</span></span><span style="display:flex;"><span>  push        Push a model to a registry
</span></span><span style="display:flex;"><span>  list        List models
</span></span><span style="display:flex;"><span>  ps          List running models
</span></span><span style="display:flex;"><span>  cp          Copy a model
</span></span><span style="display:flex;"><span>  rm          Remove a model
</span></span><span style="display:flex;"><span>  <span style="color:#204a87">help</span>        Help about any <span style="color:#204a87">command</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Flags:
</span></span><span style="display:flex;"><span>  -h, --help      <span style="color:#204a87">help</span> <span style="color:#204a87;font-weight:bold">for</span> ollama
</span></span><span style="display:flex;"><span>  -v, --version   Show version information
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Use <span style="color:#4e9a06">&#34;ollama [command] --help&#34;</span> <span style="color:#204a87;font-weight:bold">for</span> more information about a command.
</span></span></code></pre></div><h2 id="2-3-拉取一个大模型">2.3. 拉取一个大模型</h2>
<p>可以在 <a href="https://ollama.com/search">https://ollama.com/search</a> 网站上，选择一个所需要的大模型，例如<code>deepseek-r1:7b</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载指定的大模型，例如deepseek</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
</span></span><span style="display:flex;"><span>pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">387</span> B
</span></span><span style="display:flex;"><span>pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
</span></span><span style="display:flex;"><span>pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">148</span> B
</span></span><span style="display:flex;"><span>pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">487</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama list</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      MODIFIED
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    4.7 GB    <span style="color:#0000cf;font-weight:bold">12</span> minutes ago
</span></span></code></pre></div><h2 id="2-4-运行大模型">2.4. 运行大模型</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama run deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; 你是谁
</span></span><span style="display:flex;"><span>&lt;think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&lt;/think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; /bye
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 查看正在运行的模型</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama ps</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      PROCESSOR    UNTIL
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    5.5 GB    100% CPU     <span style="color:#0000cf;font-weight:bold">3</span> minutes from now
</span></span></code></pre></div><h2 id="2-5-修改ollama服务地址和目录">2.5. 修改ollama服务地址和目录</h2>
<h3 id="2-5-1-修改ollama服务地址">2.5.1. 修改ollama服务地址</h3>
<p>ollama服务默认监听127.0.0.1, 如果要修改监听地址，则可以添加<code>Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vi /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Unit<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Description</span><span style="color:#ce5c00;font-weight:bold">=</span>Ollama Service
</span></span><span style="display:flex;"><span><span style="color:#000">After</span><span style="color:#ce5c00;font-weight:bold">=</span>network-online.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Service<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;OLLAMA_HOST=0.0.0.0:11434&#34;</span>   <span style="color:#8f5902;font-style:italic"># 增加环境变量</span>
</span></span><span style="display:flex;"><span><span style="color:#000">ExecStart</span><span style="color:#ce5c00;font-weight:bold">=</span>/usr/local/bin/ollama serve
</span></span><span style="display:flex;"><span><span style="color:#000">User</span><span style="color:#ce5c00;font-weight:bold">=</span>ollama
</span></span><span style="display:flex;"><span><span style="color:#000">Group</span><span style="color:#ce5c00;font-weight:bold">=</span>ollama
</span></span><span style="display:flex;"><span><span style="color:#000">Restart</span><span style="color:#ce5c00;font-weight:bold">=</span>always
</span></span><span style="display:flex;"><span><span style="color:#000">RestartSec</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#0000cf;font-weight:bold">3</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Install<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">WantedBy</span><span style="color:#ce5c00;font-weight:bold">=</span>default.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 重启服务</span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama
</span></span><span style="display:flex;"><span>systemctl status ollama
</span></span></code></pre></div><h3 id="2-5-2-修改ollama数据目录">2.5.2. 修改ollama数据目录</h3>
<p>参考：<a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored">ollama/docs/faq.md</a></p>
<p>默认存储目录</p>
<ul>
<li>macOS: <code>~/.ollama/models</code></li>
<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>
<li>Windows: <code>C:\Users\%username%\.ollama\models</code></li>
</ul>
<p>以linux系统为例，修改默认的存储目录：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#000">dir</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;/data/ollama/models&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 创建目录并分配权限</span>
</span></span><span style="display:flex;"><span>mkdir -p /data/ollama/models
</span></span><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 添加环境变量OLLAMA_MODELS</span>
</span></span><span style="display:flex;"><span>vi /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Service<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;OLLAMA_MODELS=/data/ollama/models&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 重启服务</span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama
</span></span><span style="display:flex;"><span>systemctl status ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 迁移数据</span>
</span></span><span style="display:flex;"><span>cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models
</span></span><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span></code></pre></div><h1 id="3-部署open-webui">3. 部署open-webui</h1>
<h2 id="3-1-单独部署open-webui">3.1. 单独部署open-webui</h2>
<p>如果已经部署了ollama服务，可以通过以下命令单独部署open-webui，修改<code>OLLAMA_BASE_URL</code>为ollama的服务地址。如果使用host-network，默认服务监听端口为<code>8080</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d --network<span style="color:#ce5c00;font-weight:bold">=</span>host -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p>环境变量</p>
<ul>
<li><code>OLLAMA_BASE_URL:http://OLLAMA_HOST:11434</code> : 设置ollama服务的地址</li>
<li><code>HF_HUB_OFFLINE: &quot;1&quot;</code>：设置模型为离线的环境</li>
<li><code>ENABLE_OPENAI_API: &quot;false&quot;</code>：设置关闭openai的接口</li>
</ul>
<p><strong>访问open-webui服务：</strong></p>
<p>访问<code>http://服务器IP:8080</code>，注册用户名密码然后登录。就可以使用本地的大模型服务。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739864579/article/linux/llm/open-webui.png" alt=""></p>
<h2 id="3-2-部署open-webui和ollama服务">3.2. 部署open-webui和ollama服务</h2>
<p>如果不想单独部署ollama，可以通过open-webui:ollama镜像，同时部署open-webui和ollama，两个服务集成在同一个镜像中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载镜像</span>
</span></span><span style="display:flex;"><span>docker pull ghcr.io/open-webui/open-webui:ollama
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 运行open-webui:ollama</span>
</span></span><span style="display:flex;"><span>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v ollama-open-webui:/app/backend/data --name ollama-open-webui --restart always ghcr.io/open-webui/open-webui:ollama
</span></span></code></pre></div><p>查看服务</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker images</span>
</span></span><span style="display:flex;"><span>REPOSITORY                      TAG       IMAGE ID       CREATED        SIZE
</span></span><span style="display:flex;"><span>ghcr.io/open-webui/open-webui   ollama    29d60b4958c8   <span style="color:#0000cf;font-weight:bold">4</span> days ago     8.02GB
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker ps</span>
</span></span><span style="display:flex;"><span>CONTAINER ID   IMAGE                                  COMMAND           CREATED          STATUS                    PORTS                              NAMES
</span></span><span style="display:flex;"><span>3175fc20c608   ghcr.io/open-webui/open-webui:ollama   <span style="color:#4e9a06">&#34;bash start.sh&#34;</span>   <span style="color:#0000cf;font-weight:bold">16</span> minutes ago   Up <span style="color:#0000cf;font-weight:bold">16</span> minutes <span style="color:#ce5c00;font-weight:bold">(</span>healthy<span style="color:#ce5c00;font-weight:bold">)</span>   0.0.0.0:3000-&gt;8080/tcp             ollama-open-webui
</span></span></code></pre></div><p>登录容器下载大模型文件</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 登录容器</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker exec -it 3175fc20c608 bash</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载指定的大模型</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
</span></span><span style="display:flex;"><span>pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">387</span> B
</span></span><span style="display:flex;"><span>pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
</span></span><span style="display:flex;"><span>pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">148</span> B
</span></span><span style="display:flex;"><span>pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">487</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama list</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      MODIFIED
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    4.7 GB    <span style="color:#0000cf;font-weight:bold">12</span> minutes ago
</span></span></code></pre></div><p>则可以访问所部属服务器的地址和端口来访问open-webui的服务。</p>
<h2 id="3-3-构建本地知识库">3.3. 构建本地知识库</h2>
<h3 id="3-3-1-自定义文件分析">3.3.1. 自定义文件分析</h3>
<p>可以通过页面上传本地的知识库文件，让AI回答关于自定义文件中的内容。</p>
<p>例如：我通过文件自定义了内容，提问张飞的电话号码，则可以通过文章中的内容来回答。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739871922/article/linux/llm/phone-chat.png" alt=""></p>
<p>其中自定义文档的内容如下：</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739869289/article/linux/llm/ollama-docs.png" alt=""></p>
<p>同样可以上传其他文件来构建一个本地大模型知识库。然后借助大模型来查询和分析数据内容。</p>
<h3 id="3-3-2-本地化数据存储">3.3.2. 本地化数据存储</h3>
<p>其中open-webui的本地化数据存储在容器内的<code>/app/backend/data/</code>目录下。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>/app/backend/data# ls -l
</span></span><span style="display:flex;"><span>total <span style="color:#0000cf;font-weight:bold">236</span>
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">7</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">11</span> 10:48 cache
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">2</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:11 uploads
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">3</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:11 vector_db
</span></span><span style="display:flex;"><span>-rw-r--r-- <span style="color:#0000cf;font-weight:bold">1</span> root root <span style="color:#0000cf;font-weight:bold">229376</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:38 webui.db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 可以从uploads目录看到上传的本地文件</span>
</span></span><span style="display:flex;"><span>/app/backend/data/uploads# cat 117e6f99-0657-40d1-ab6f-1bea81e78053_ollama-docs.md
</span></span><span style="display:flex;"><span>张飞的电话号码是u987438274
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>曹操的电话号码是123456
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>关羽的电话号码是5352345
</span></span></code></pre></div><h2 id="3-4-faq">3.4. FAQ</h2>
<h3 id="1-open-webui页面无法选择模型">1）open-webui页面无法选择模型</h3>
<p><strong>问题：</strong></p>
<p>当单独部署open-webui，可能会遇到open-webui页面无法选择模型具体的现象如下：</p>
<img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739864579/article/linux/llm/open-webui-error.png" title="" alt="" width="709">
<p>open-webui日志报错：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>INFO  <span style="color:#ce5c00;font-weight:bold">[</span>open_webui.routers.ollama<span style="color:#ce5c00;font-weight:bold">]</span> get_all_models<span style="color:#ce5c00;font-weight:bold">()</span>
</span></span><span style="display:flex;"><span>ERROR <span style="color:#ce5c00;font-weight:bold">[</span>open_webui.routers.ollama<span style="color:#ce5c00;font-weight:bold">]</span> Connection error: Cannot connect to host 1.1.1.1:11434 ssl:default <span style="color:#ce5c00;font-weight:bold">[</span>Connect call failed <span style="color:#ce5c00;font-weight:bold">(</span><span style="color:#4e9a06">&#39;1.1.1.1&#39;</span>, 11434<span style="color:#ce5c00;font-weight:bold">)]</span>
</span></span></code></pre></div><p><strong>原因：</strong></p>
<p>按官网命令使用<code>端口映射</code>的网络模式，如果OLLAMA_BASE_URL配置为127.0.0.1则访问不到单独部署的ollama服务，如果改用具体的ollama的IP也可能存在访问失败的问题。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d -p 3000:8080 -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p><strong>解决方案：</strong></p>
<p>docker网络模式改为<code>host-network</code>的网络模式</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d --network<span style="color:#ce5c00;font-weight:bold">=</span>host -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><h3 id="2-数据目录没权限permission-denied">2）数据目录没权限permission denied</h3>
<p>如果用户修改了ollama的models的存储目录，出现ollama服务重启失败，或者pull model数据报错</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 修改ollama的model目录后ollama服务重启报错</span>
</span></span><span style="display:flex;"><span>Error: mkdir /data/ollama: permission denied
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 迁移model数据后出现没权限，因为使用了root命令执行</span>
</span></span><span style="display:flex;"><span>cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:70b</span>
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>Error: open /data/ollama/models/manifests/registry.ollama.ai/library/deepseek-r1/70b: permission denied
</span></span></code></pre></div><p><strong>原因：</strong></p>
<p>ollama默认使用的用户名是 ollama，因此需要给目录添加用户的权限，例如：目录创建和model文件迁移是通过root或其他用户执行的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span></code></pre></div><h1 id="4-总结">4. 总结</h1>
<p>本文主要介绍了ollama和open-webui的部署，从而搭建一个<code>本地化私有的大模型工具</code>，<code>所有的数据都存储在本地</code>。可以通过上传文件来分析本地的数据，类似构建<code>本地大模型知识库</code>。</p>
<p>不过本地大模型的响应速度依赖于大模型本身和本地的资源，包括cpu和gpu，没有gpu资源也可以运行。在资源较小的情况下，大模型回答问题的速度比较慢。如果完全需要离线的大模型分析数据，在资源受限的情况下需要再进一步做优化才能得到比较好的体验。</p>
<p>参考：</p>
<ul>
<li><a href="https://ollama.com/download/linux">https://ollama.com/download/linux</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/faq.md">https://github.com/ollama/ollama/blob/main/docs/faq.md</a></li>
<li><a href="https://docs.openwebui.com/getting-started/quick-start">https://docs.openwebui.com/getting-started/quick-start</a></li>
<li><a href="https://github.com/open-webui/open-webui#troubleshooting">https://github.com/open-webui/open-webui#troubleshooting</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e44128ff477d39d7fb8644a3a4a27ea6">2 - 大模型相关概念</h1>
    
	<blockquote>
<p>本文主要介绍大模型领域常用的名词概念等。</p>
</blockquote>
<h1 id="mcp">MCP</h1>
<h2 id="mcp的概念">MCP的概念</h2>
<p>MCP的全称是<code>Model Context Protocol</code>，即模型上下文协议。根据官网的解释，MCP 是一个开放协议，它规范了应用程序向 LLM 提供<code>上下文</code>的方式。MCP 就像 AI 应用程序的 <code>USB-C </code>端口一样。正如 USB-C 提供了一种标准化的方式将您的设备连接到各种外围设备和配件一样，MCP 也提供了一种标准化的方式将 AI 模型连接到不同的数据源和工具。</p>
<p>简单理解<code>模型上下文协议</code>，可以把它拆成三个部分：</p>
<ul>
<li>
<p><code>模型</code>：协议的使用方即大模型工具。</p>
</li>
<li>
<p><code>上下文</code>：提供存储上下文的功能，类似于存储大模型历史交互的记忆。</p>
</li>
<li>
<p><code>协议</code>：本质是一种标准，类比与TCP协议，http协议，而MCP可以类比于http之上的业务层协议。</p>
</li>
</ul>
<p>它的目的是 <strong>定义模型上下文的结构、状态传递、缓存方式、状态更新机制等内容</strong>，以支持复杂推理任务中的“状态保持”和“多轮交互”。</p>
<h2 id="为什么需要mcp">为什么需要MCP</h2>
<p>在大模型推理中，尤其是以下场景：</p>
<ul>
<li>
<p>多轮对话（如 ChatGPT）</p>
</li>
<li>
<p>Streaming 推理</p>
</li>
<li>
<p>长上下文处理（如 RAG、LLM agent）</p>
</li>
<li>
<p>分布式推理流水线（分stage执行）</p>
</li>
</ul>
<p>我们需要在模型之间、服务组件之间传递“上下文状态”（如 past key value、token buffer、history、session id）。这个时候，就需要一套 <strong>标准的协议</strong> 来描述和控制这些状态 —— MCP 就是为此而生。</p>
<p>可以将 MCP 看作在 HTTP 或 gRPC 上层的“<strong>业务语义协议</strong>”：</p>
<pre tabindex="0"><code>┌──────────────────────────────────────────────┐
│                Application (LLM API)         │
│     ┌─────────────┐                          │
│     │   MCP 协议   │  &lt;-- 管理模型上下文状态   │
│     └─────────────┘                          │
│         ↑ 使用 protobuf / JSON               │
│     ┌─────────────┐                          │
│     │   HTTP/gRPC  │  &lt;-- 实际传输协议        │
│     └─────────────┘                          │
│         ↑ 使用 TCP/IP                        │
└──────────────────────────────────────────────┘
</code></pre><h2 id="类比">类比</h2>
<p><strong>MCP 是“大脑助手的记忆本”</strong></p>
<p>想象你在和一个 AI 助手（比如 ChatGPT）对话，它像一个演员在扮演各种角色。</p>
<p><strong>如果没有 MCP：</strong></p>
<p>助手每次都“失忆”，你要一遍遍重复之前的内容，它无法记得你是谁、想干嘛、聊过啥。</p>
<p><strong>如果有 MCP：</strong></p>
<p>就像它有了一本“<strong>记忆本</strong>”：</p>
<ul>
<li>
<p>每次你对它说话，它都记下“对话历史”、“你正在聊的主题”、“你的偏好”</p>
</li>
<li>
<p>它能从记忆本中取出之前你说的话，继续展开对话</p>
</li>
<li>
<p>这个“记忆本”就由 <strong>MCP 协议定义结构、内容和传递方式</strong></p>
</li>
</ul>
<p>🧠 <strong>形象理解</strong>：</p>
<blockquote>
<p>MCP 就像是 AI 模型的大脑助手，负责“记住你们聊过什么”，以及“接下来该怎么回复你”。</p>
</blockquote>
<h1 id="mcp-server">MCP server</h1>
<p>MCP server是模型上下文协议（Model Context Protocol）中的一个组件，它扮演着AI模型与外部数据、工具和功能之间连接的桥梁。它就像一个工具箱，为AI模型提供各种功能，比如访问文件、调用API、执行计算等，可以实现更复杂的任务。﻿</p>
<p>更详细地说，MCP server是一种轻量级服务程序，负责提供数据、工具和提示，帮助AI模型理解和执行任务。它通过标准化的接口和协议，将外部能力暴露给AI模型，让AI模型能够访问各种外部资源，例如数据库、API、文件等。</p>
<p>mcp server 链接：</p>
<ul>
<li><a href="https://mcp.ad/">https://mcp.ad/</a></li>
</ul>
<h1 id="prompt">Prompt</h1>
<blockquote>
<p>Prompt 就是你给大模型的“指令 + 提示 +上下文”，用来引导它输出你想要的结果。</p>
</blockquote>
<p><strong>Prompt 是 MCP 协议中的核心内容之一</strong>，而 <strong>MCP 是管理 Prompt 的“协议和系统”</strong>，用来组织、封装、传输、复用 prompt（以及上下文）以便模型推理使用。</p>
<p><strong>Prompt 是 MCP 协议中的核心内容之一</strong>，而 <strong>MCP 是管理 Prompt 的“协议和系统”</strong>，用来组织、封装、传输、复用 prompt（以及上下文）以便模型推理使用。</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Prompt</th>
<th>MCP（Model Context Protocol）</th>
</tr>
</thead>
<tbody>
<tr>
<td>本质</td>
<td>一段输入文本（或 token 序列）</td>
<td>一种上下文协议，组织模型输入/状态/上下文的格式与流程</td>
</tr>
<tr>
<td>作用</td>
<td>引导模型生成内容</td>
<td>管理 Prompt 和上下文的结构、状态、生命周期</td>
</tr>
<tr>
<td>是否直接传给模型</td>
<td>✅ 是模型输入的一部分</td>
<td>✅ 会生成 prompt，并交给模型</td>
</tr>
<tr>
<td>是谁的子集？</td>
<td>Prompt 是 MCP 的<strong>一部分字段</strong></td>
<td>MCP 是对 Prompt 及其上下文的<strong>结构化封装</strong></td>
</tr>
<tr>
<td>类比关系</td>
<td>你要说的一句话</td>
<td>你整场对话的“聊天记录 + 状态 + 上下文管理系统”</td>
</tr>
</tbody>
</table>
<p>示例：</p>
<p>Prompt：</p>
<blockquote>
<p>“你是一个翻译专家，请将下面这段话翻译为英文：‘我爱编程’”</p>
</blockquote>
<p>最终形成一个这样的 MCP payload：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">{</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">&#34;session_id&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;abc123&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;stage&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;prompt&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;inputs&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;prompt&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;你是一个翻译专家，请将下面这段话翻译为英文：‘我爱编程’&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;history&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">[</span><span style="color:#a40000">...</span><span style="color:#000;font-weight:bold">],</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;role&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;user&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">},</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;generation_config&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;temperature&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#0000cf;font-weight:bold">0.8</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;top_p&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#0000cf;font-weight:bold">0.95</span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">},</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;kv_cache_refs&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">[],</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;position&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#0000cf;font-weight:bold">128</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">}</span>
</span></span></code></pre></div><h1 id="token">Token</h1>
<blockquote>
<p><strong>Token 是大模型处理语言时的最小单位</strong>，相当于模型的“语言颗粒度”。它可能是一个词、一个子词、一个字符，甚至是一部分单词。</p>
</blockquote>
<p>人说话是用句子、单词，大模型不能直接理解这些自然语言，它必须先<strong>把文本切割成小块（token）</strong>，再转成数字（embedding）才能理解。</p>
<p>比如下面这句话：<code>你好，世界！</code></p>
<p>对人来说是 几 个字，但对模型来说，它可能被切分成 <strong>2~4 个 token</strong>，取决于使用的 tokenizer（分词器）。</p>
<p><strong>为什么 token 很重要？</strong></p>
<ul>
<li>
<p><strong>计费单位</strong>（OpenAI/Claude/其他 API）是按 token 收费的</p>
</li>
<li>
<p><strong>上下文长度限制</strong> 是按 token 算的，不是按“字”算的</p>
</li>
<li>
<p><strong>模型性能 &amp; 精度</strong> 和 token 分布、长度密切相关</p>
</li>
<li>
<p><strong>Prompt 工程</strong> 就是用尽可能少的 token 实现更强的引导效果</p>
</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://modelcontextprotocol.io/introduction">Introduction - Model Context Protocol</a></li>
</ul>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/huweihuang/blog.huweihuang.com" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2025 www.huweihuang.com 保留所有权利</small>
        <small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">隐私政策</a></small>
	
		<p class="mt-2"><a href="/about/">About</a></p>
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"
    integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN"
    crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js"
    integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA=="
    crossorigin="anonymous"></script>





<script src='/js/tabpane-persist.js'></script>




















<script src="/js/main.min.91798a335c881f1b6b805085ba4aa22d1dbd2b0b18d105d05189fa104ddae350.js" integrity="sha256-kXmKM1yIHxtrgFCFukqiLR29KwsY0QXQUYn6EE3a41A=" crossorigin="anonymous"></script>



<script src='/js/prism.js'></script>



  </body>
</html>
