<!doctype html>
<html lang="zh-cn" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.101.0" />
<link rel="canonical" type="text/html" href="https://blog.huweihuang.com/linux-notes/llm/">
<link rel="alternate" type="application/rss&#43;xml" href="https://blog.huweihuang.com/linux-notes/llm/index.xml">
<meta name="robots" content="noindex, nofollow">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>大模型 | 胡伟煌</title>
<meta name="description" content="">
<meta property="og:title" content="大模型" />
<meta property="og:description" content="Kubernetes学习笔记" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://blog.huweihuang.com/linux-notes/llm/" /><meta property="og:site_name" content="胡伟煌" />

<meta itemprop="name" content="大模型">
<meta itemprop="description" content="Kubernetes学习笔记"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="大模型"/>
<meta name="twitter:description" content="Kubernetes学习笔记"/>




<link rel="preload" href="/scss/main.min.79e3930541ca05b5395c14b2a313b798fad1dc69f9a14aefa57b62eaa9f65f14.css" as="style">
<link href="/scss/main.min.79e3930541ca05b5395c14b2a313b798fad1dc69f9a14aefa57b62eaa9f65f14.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.6.0.min.js"
  integrity="sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK"
  crossorigin="anonymous"></script>
<script
  src="https://unpkg.com/lunr@2.3.9/lunr.min.js"
  integrity="sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli"
  crossorigin="anonymous"></script>
<link rel="stylesheet" href="/css/prism.css"/>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-114718458-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" width="64" height="64"><path d="M15.9.476a2.14 2.14.0 00-.823.218L3.932 6.01c-.582.277-1.005.804-1.15 1.432L.054 19.373c-.13.56-.025 1.147.3 1.627q.057.087.12.168l7.7 9.574c.407.5 1.018.787 1.662.784h12.35c.646.001 1.258-.3 1.664-.793l7.696-9.576c.404-.5.555-1.16.4-1.786L29.2 7.43c-.145-.628-.57-1.155-1.15-1.432L16.923.695A2.14 2.14.0 0015.89.476z" fill="#326ce5"/><path d="M16.002 4.542c-.384.027-.675.356-.655.74v.188c.018.213.05.424.092.633a6.22 6.22.0 01.066 1.21c-.038.133-.114.253-.218.345l-.015.282c-.405.034-.807.096-1.203.186-1.666.376-3.183 1.24-4.354 2.485l-.24-.17c-.132.04-.274.025-.395-.04a6.22 6.22.0 01-.897-.81 5.55 5.55.0 00-.437-.465l-.148-.118c-.132-.106-.294-.167-.463-.175a.64.64.0 00-.531.236c-.226.317-.152.756.164.983l.138.11a5.55 5.55.0 00.552.323c.354.197.688.428.998.7a.74.74.0 01.133.384l.218.2c-1.177 1.766-1.66 3.905-1.358 6.006l-.28.08c-.073.116-.17.215-.286.288a6.22 6.22.0 01-1.194.197 5.57 5.57.0 00-.64.05l-.177.04h-.02a.67.67.0 00-.387 1.132.67.67.0 00.684.165h.013l.18-.02c.203-.06.403-.134.598-.218.375-.15.764-.265 1.162-.34.138.008.27.055.382.135l.3-.05c.65 2.017 2.016 3.726 3.84 4.803l-.122.255c.056.117.077.247.06.376-.165.382-.367.748-.603 1.092a5.58 5.58.0 00-.358.533l-.085.18a.67.67.0 00.65 1.001.67.67.0 00.553-.432l.083-.17c.076-.2.14-.404.192-.61.177-.437.273-.906.515-1.196a.54.54.0 01.286-.14l.15-.273a8.62 8.62.0 006.146.015l.133.255c.136.02.258.095.34.205.188.358.34.733.456 1.12a5.57 5.57.0 00.194.611l.083.17a.67.67.0 001.187.131.67.67.0 00.016-.701l-.087-.18a5.55 5.55.0 00-.358-.531c-.23-.332-.428-.686-.6-1.057a.52.52.0 01.068-.4 2.29 2.29.0 01-.111-.269c1.82-1.085 3.18-2.8 3.823-4.82l.284.05c.102-.093.236-.142.373-.138.397.076.786.2 1.162.34.195.09.395.166.598.23.048.013.118.024.172.037h.013a.67.67.0 00.841-.851.67.67.0 00-.544-.446l-.194-.046a5.57 5.57.0 00-.64-.05c-.404-.026-.804-.092-1.194-.197-.12-.067-.22-.167-.288-.288l-.27-.08a8.65 8.65.0 00-1.386-5.993l.236-.218c-.01-.137.035-.273.124-.378.307-.264.64-.497.99-.696a5.57 5.57.0 00.552-.323l.146-.118a.67.67.0 00-.133-1.202.67.67.0 00-.696.161l-.148.118a5.57 5.57.0 00-.437.465c-.264.302-.556.577-.873.823a.74.74.0 01-.404.044l-.253.18c-1.46-1.53-3.427-2.48-5.535-2.67.0-.1-.013-.25-.015-.297-.113-.078-.192-.197-.218-.332a6.23 6.23.0 01.076-1.207c.043-.21.073-.42.092-.633v-.2c.02-.384-.27-.713-.655-.74zm-.834 5.166-.2 3.493h-.015c-.01.216-.137.4-.332.504s-.426.073-.6-.054l-2.865-2.03a6.86 6.86.0 013.303-1.799c.234-.05.47-.088.707-.114zm1.668.0c1.505.187 2.906.863 3.99 1.924l-2.838 2.017c-.175.14-.415.168-.618.072s-.333-.3-.336-.524zm-6.72 3.227 2.62 2.338v.015c.163.142.234.363.186.574s-.21.378-.417.435v.01l-3.362.967a6.86 6.86.0 01.974-4.34zm11.753.0c.796 1.295 1.148 2.814 1.002 4.327l-3.367-.97v-.013c-.21-.057-.37-.224-.417-.435s.023-.43.186-.574l2.6-2.327zm-6.404 2.52h1.072l.655.832-.238 1.04-.963.463-.965-.463-.227-1.04zm3.434 2.838c.045-.005.1-.005.135.0l3.467.585c-.5 1.44-1.487 2.67-2.775 3.493l-1.34-3.244a.59.59.0 01.509-.819zm-5.823.015c.196.003.377.104.484.268s.124.37.047.55v.013l-1.332 3.218C11 21.54 10.032 20.325 9.517 18.9l3.437-.583c.038-.004.077-.004.116.0zm2.904 1.4a.59.59.0 01.537.308h.013l1.694 3.057-.677.2c-1.246.285-2.547.218-3.758-.194l1.7-3.057c.103-.18.293-.29.5-.295z" fill="#fff" stroke="#fff" stroke-width=".055"/></svg></span><span class="font-weight-bold">胡伟煌</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/kubernetes-notes/" ><span>Kubernetes学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/k8s-source-code-analysis/" ><span>Kubernetes源码分析</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/golang-notes/" ><span>Golang学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link active" href="/linux-notes/" ><span class="active">Linux学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/about/" ><span>About</span></a>
			</li>
			
			
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block"><input
  type="search"
  class="form-control td-search-input"
  placeholder="&#xf002; 站内搜索…"
  aria-label="站内搜索…"
  autocomplete="off"
  
  data-offline-search-index-json-src="/offline-search-index.2f3d54c0f8eab0ab567f21fb0df68403.json"
  data-offline-search-base-href="/"
  data-offline-search-max-results="10"
>
</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/linux-notes/llm/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">大模型</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-093f126ec9e750c4adca83e14d272d51">基于Ollama构建本地大模型</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-093f126ec9e750c4adca83e14d272d51">1 - 基于Ollama构建本地大模型</h1>
    
	<p>本文主要介绍如何通过<code>Ollama</code>和<code>OpenWebUI</code>来搭建一个本地私有化运行的大模型工具。私有化大模型的构建主要用于解决<code>数据的安全性问题</code>，对于大部分私有数据不适合通过外部的大模型网站来上传和分析。</p>
<h1 id="1-ollama-与-openwebui-介绍">1. Ollama 与 OpenWebUI 介绍</h1>
<h2 id="1-1-ollama简介">1.1. Ollama简介</h2>
<p>Ollama 是一个 <strong>本地运行的 AI 大模型管理工具</strong>，可以让你在本地 <strong>快速拉取、管理和运行</strong> 各种开源大语言模型（如 LLaMA、Mistral、deepseek 等），而无需依赖云端 API。它的主要特点包括：</p>
<ul>
<li><strong>简易安装</strong>：支持 macOS、Linux 和 Windows（WSL）。</li>
<li><strong>本地推理</strong>：在本地设备上直接运行 LLM，保护数据隐私。</li>
<li><strong>模型管理</strong>：可以像使用 Docker 一样 <code>ollama run llama2</code> 轻松拉取和运行模型。</li>
<li><strong>自定义模型</strong>：支持通过 <code>Modelfile</code> 进行微调和定制。</li>
<li><strong>支持 API</strong>：可以通过 Python、Node.js 等语言调用 Ollama 提供的本地 REST API。</li>
</ul>
<p>Ollama 适用于本地 AI 代理、嵌入式 AI 应用、隐私保护的智能助手等场景。你可以用它来运行大语言模型，而无需自己搭建复杂的推理环境。</p>
<h2 id="1-2-openwebui简介">1.2. OpenWebUI简介</h2>
<p><strong>Open-WebUI</strong> 是一个 <strong>开源的 Web 用户界面</strong>，用于管理和使用本地或远程的大语言模型（LLM），比如 Ollama、OpenAI、Gemini 等。它的主要特点包括：</p>
<ul>
<li><strong>友好的 Web 界面</strong>：提供 ChatGPT 类似的对话 UI，方便交互。</li>
<li><strong>支持多种后端</strong>：可以连接 <strong>Ollama、OpenAI API、本地 LLM</strong> 等。</li>
<li><strong>多用户支持</strong>：适用于团队协作。</li>
<li><strong>对话历史管理</strong>：可保存和管理聊天记录。</li>
<li><strong>插件和自定义功能</strong>：支持扩展，适用于不同应用场景。</li>
</ul>
<p>它可以让本地 LLM 变得更加易用，适合个人、企业部署本地 AI 助手。</p>
<h1 id="2-部署ollama">2. 部署ollama</h1>
<h2 id="2-1-脚本安装-ollama">2.1. 脚本安装<code>ollama</code></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh <span style="color:#000;font-weight:bold">|</span> sh
</span></span></code></pre></div><p>输出</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>&gt;&gt;&gt; Installing ollama to /usr/local
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Downloading Linux amd64 bundle
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic">######################################################################## 100.0%</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama user...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to render group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to video group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding current user to ollama group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama systemd service...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Enabling and starting ollama service...
</span></span><span style="display:flex;"><span>Created symlink /etc/systemd/system/default.target.wants/ollama.service -&gt; /etc/systemd/system/ollama.service.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; The Ollama API is now available at 127.0.0.1:11434.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Install complete. Run <span style="color:#4e9a06">&#34;ollama&#34;</span> from the <span style="color:#204a87">command</span> line.
</span></span><span style="display:flex;"><span>WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.
</span></span></code></pre></div><p>默认服务监听的地址为：<code>127.0.0.1:11434</code></p>
<h2 id="2-2-查看-ollama-服务状态">2.2. 查看<code>ollama</code>服务状态</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>systemctl status ollama
</span></span><span style="display:flex;"><span>* ollama.service - Ollama Service
</span></span><span style="display:flex;"><span>     Loaded: loaded <span style="color:#ce5c00;font-weight:bold">(</span>/etc/systemd/system/ollama.service<span style="color:#000;font-weight:bold">;</span> enabled<span style="color:#000;font-weight:bold">;</span> vendor preset: enabled<span style="color:#ce5c00;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>     Active: active <span style="color:#ce5c00;font-weight:bold">(</span>running<span style="color:#ce5c00;font-weight:bold">)</span> since Fri 2025-02-07 17:21:55 +08<span style="color:#000;font-weight:bold">;</span> 23s ago
</span></span><span style="display:flex;"><span>   Main PID: <span style="color:#0000cf;font-weight:bold">53472</span> <span style="color:#ce5c00;font-weight:bold">(</span>ollama<span style="color:#ce5c00;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>      Tasks: <span style="color:#0000cf;font-weight:bold">10</span>
</span></span><span style="display:flex;"><span>     Memory: 10.3M
</span></span><span style="display:flex;"><span>     CGroup: /system.slice/ollama.service
</span></span><span style="display:flex;"><span>             <span style="color:#4e9a06">`</span>-53472 /usr/local/bin/ollama serve
</span></span></code></pre></div><p>查看<code>ollama</code>命令</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama --help</span>
</span></span><span style="display:flex;"><span>Large language model runner
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Usage:
</span></span><span style="display:flex;"><span>  ollama <span style="color:#ce5c00;font-weight:bold">[</span>flags<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>  ollama <span style="color:#ce5c00;font-weight:bold">[</span>command<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Available Commands:
</span></span><span style="display:flex;"><span>  serve       Start ollama
</span></span><span style="display:flex;"><span>  create      Create a model from a Modelfile
</span></span><span style="display:flex;"><span>  show        Show information <span style="color:#204a87;font-weight:bold">for</span> a model
</span></span><span style="display:flex;"><span>  run         Run a model
</span></span><span style="display:flex;"><span>  stop        Stop a running model
</span></span><span style="display:flex;"><span>  pull        Pull a model from a registry
</span></span><span style="display:flex;"><span>  push        Push a model to a registry
</span></span><span style="display:flex;"><span>  list        List models
</span></span><span style="display:flex;"><span>  ps          List running models
</span></span><span style="display:flex;"><span>  cp          Copy a model
</span></span><span style="display:flex;"><span>  rm          Remove a model
</span></span><span style="display:flex;"><span>  <span style="color:#204a87">help</span>        Help about any <span style="color:#204a87">command</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Flags:
</span></span><span style="display:flex;"><span>  -h, --help      <span style="color:#204a87">help</span> <span style="color:#204a87;font-weight:bold">for</span> ollama
</span></span><span style="display:flex;"><span>  -v, --version   Show version information
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Use <span style="color:#4e9a06">&#34;ollama [command] --help&#34;</span> <span style="color:#204a87;font-weight:bold">for</span> more information about a command.
</span></span></code></pre></div><h2 id="2-3-拉取一个大模型">2.3. 拉取一个大模型</h2>
<p>可以在 <a href="https://ollama.com/search">https://ollama.com/search</a> 网站上，选择一个所需要的大模型，例如<code>deepseek-r1:7b</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载指定的大模型，例如deepseek</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
</span></span><span style="display:flex;"><span>pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">387</span> B
</span></span><span style="display:flex;"><span>pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
</span></span><span style="display:flex;"><span>pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">148</span> B
</span></span><span style="display:flex;"><span>pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">487</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama list</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      MODIFIED
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    4.7 GB    <span style="color:#0000cf;font-weight:bold">12</span> minutes ago
</span></span></code></pre></div><h2 id="2-4-运行大模型">2.4. 运行大模型</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama run deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; 你是谁
</span></span><span style="display:flex;"><span>&lt;think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&lt;/think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; /bye
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 查看正在运行的模型</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama ps</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      PROCESSOR    UNTIL
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    5.5 GB    100% CPU     <span style="color:#0000cf;font-weight:bold">3</span> minutes from now
</span></span></code></pre></div><h2 id="2-5-修改ollama服务地址和目录">2.5. 修改ollama服务地址和目录</h2>
<h3 id="2-5-1-修改ollama服务地址">2.5.1. 修改ollama服务地址</h3>
<p>ollama服务默认监听127.0.0.1, 如果要修改监听地址，则可以添加<code>Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vi /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Unit<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Description</span><span style="color:#ce5c00;font-weight:bold">=</span>Ollama Service
</span></span><span style="display:flex;"><span><span style="color:#000">After</span><span style="color:#ce5c00;font-weight:bold">=</span>network-online.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Service<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;OLLAMA_HOST=0.0.0.0:11434&#34;</span>   <span style="color:#8f5902;font-style:italic"># 增加环境变量</span>
</span></span><span style="display:flex;"><span><span style="color:#000">ExecStart</span><span style="color:#ce5c00;font-weight:bold">=</span>/usr/local/bin/ollama serve
</span></span><span style="display:flex;"><span><span style="color:#000">User</span><span style="color:#ce5c00;font-weight:bold">=</span>ollama
</span></span><span style="display:flex;"><span><span style="color:#000">Group</span><span style="color:#ce5c00;font-weight:bold">=</span>ollama
</span></span><span style="display:flex;"><span><span style="color:#000">Restart</span><span style="color:#ce5c00;font-weight:bold">=</span>always
</span></span><span style="display:flex;"><span><span style="color:#000">RestartSec</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#0000cf;font-weight:bold">3</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Install<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">WantedBy</span><span style="color:#ce5c00;font-weight:bold">=</span>default.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 重启服务</span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama
</span></span><span style="display:flex;"><span>systemctl status ollama
</span></span></code></pre></div><h3 id="2-5-2-修改ollama数据目录">2.5.2. 修改ollama数据目录</h3>
<p>参考：<a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored">ollama/docs/faq.md</a></p>
<p>默认存储目录</p>
<ul>
<li>macOS: <code>~/.ollama/models</code></li>
<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>
<li>Windows: <code>C:\Users\%username%\.ollama\models</code></li>
</ul>
<p>以linux系统为例，修改默认的存储目录：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#000">dir</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;/data/ollama/models&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 创建目录并分配权限</span>
</span></span><span style="display:flex;"><span>mkdir -p /data/ollama/models
</span></span><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 添加环境变量OLLAMA_MODELS</span>
</span></span><span style="display:flex;"><span>vi /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Service<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;OLLAMA_MODELS=/data/ollama/models&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 重启服务</span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama
</span></span><span style="display:flex;"><span>systemctl status ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 迁移数据</span>
</span></span><span style="display:flex;"><span>cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models
</span></span></code></pre></div><h1 id="3-部署open-webui">3. 部署open-webui</h1>
<h2 id="3-1-单独部署open-webui">3.1. 单独部署open-webui</h2>
<p>如果已经部署了ollama服务，可以通过以下命令单独部署open-webui，修改<code>OLLAMA_BASE_URL</code>为ollama的服务地址。如果使用host-network，默认服务监听端口为<code>8080</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d --network<span style="color:#ce5c00;font-weight:bold">=</span>host -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p>环境变量</p>
<ul>
<li><code>OLLAMA_BASE_URL:http://OLLAMA_HOST:11434</code> : 设置ollama服务的地址</li>
<li><code>HF_HUB_OFFLINE: &quot;1&quot;</code>：设置模型为离线的环境</li>
<li><code>ENABLE_OPENAI_API: &quot;false&quot;</code>：设置关闭openai的接口</li>
</ul>
<p><strong>访问open-webui服务：</strong></p>
<p>访问<code>http://服务器IP:8080</code>，注册用户名密码然后登录。就可以使用本地的大模型服务。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739864579/article/linux/llm/open-webui.png" alt=""></p>
<h2 id="3-2-部署open-webui和ollama服务">3.2. 部署open-webui和ollama服务</h2>
<p>如果不想单独部署ollama，可以通过open-webui:ollama镜像，同时部署open-webui和ollama，两个服务集成在同一个镜像中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载镜像</span>
</span></span><span style="display:flex;"><span>docker pull ghcr.io/open-webui/open-webui:ollama
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 运行open-webui:ollama</span>
</span></span><span style="display:flex;"><span>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v ollama-open-webui:/app/backend/data --name ollama-open-webui --restart always ghcr.io/open-webui/open-webui:ollama
</span></span></code></pre></div><p>查看服务</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker images</span>
</span></span><span style="display:flex;"><span>REPOSITORY                      TAG       IMAGE ID       CREATED        SIZE
</span></span><span style="display:flex;"><span>ghcr.io/open-webui/open-webui   ollama    29d60b4958c8   <span style="color:#0000cf;font-weight:bold">4</span> days ago     8.02GB
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker ps</span>
</span></span><span style="display:flex;"><span>CONTAINER ID   IMAGE                                  COMMAND           CREATED          STATUS                    PORTS                              NAMES
</span></span><span style="display:flex;"><span>3175fc20c608   ghcr.io/open-webui/open-webui:ollama   <span style="color:#4e9a06">&#34;bash start.sh&#34;</span>   <span style="color:#0000cf;font-weight:bold">16</span> minutes ago   Up <span style="color:#0000cf;font-weight:bold">16</span> minutes <span style="color:#ce5c00;font-weight:bold">(</span>healthy<span style="color:#ce5c00;font-weight:bold">)</span>   0.0.0.0:3000-&gt;8080/tcp             ollama-open-webui
</span></span></code></pre></div><p>登录容器下载大模型文件</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 登录容器</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker exec -it 3175fc20c608 bash</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载指定的大模型</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
</span></span><span style="display:flex;"><span>pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">387</span> B
</span></span><span style="display:flex;"><span>pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
</span></span><span style="display:flex;"><span>pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">148</span> B
</span></span><span style="display:flex;"><span>pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">487</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama list</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      MODIFIED
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    4.7 GB    <span style="color:#0000cf;font-weight:bold">12</span> minutes ago
</span></span></code></pre></div><p>则可以访问所部属服务器的地址和端口来访问open-webui的服务。</p>
<h2 id="3-3-构建本地知识库">3.3. 构建本地知识库</h2>
<h3 id="3-3-1-自定义文件分析">3.3.1. 自定义文件分析</h3>
<p>可以通过页面上传本地的知识库文件，让AI回答关于自定义文件中的内容。</p>
<p>例如：我通过文件自定义了内容，提问张飞的电话号码，则可以通过文章中的内容来回答。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739871922/article/linux/llm/phone-chat.png" alt=""></p>
<p>其中自定义文档的内容如下：</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739869289/article/linux/llm/ollama-docs.png" alt=""></p>
<p>同样可以上传其他文件来构建一个本地大模型知识库。然后借助大模型来查询和分析数据内容。</p>
<h3 id="3-3-2-本地化数据存储">3.3.2. 本地化数据存储</h3>
<p>其中open-webui的本地化数据存储在容器内的<code>/app/backend/data/</code>目录下。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>/app/backend/data# ls -l
</span></span><span style="display:flex;"><span>total <span style="color:#0000cf;font-weight:bold">236</span>
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">7</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">11</span> 10:48 cache
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">2</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:11 uploads
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">3</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:11 vector_db
</span></span><span style="display:flex;"><span>-rw-r--r-- <span style="color:#0000cf;font-weight:bold">1</span> root root <span style="color:#0000cf;font-weight:bold">229376</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:38 webui.db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 可以从uploads目录看到上传的本地文件</span>
</span></span><span style="display:flex;"><span>/app/backend/data/uploads# cat 117e6f99-0657-40d1-ab6f-1bea81e78053_ollama-docs.md
</span></span><span style="display:flex;"><span>张飞的电话号码是u987438274
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>曹操的电话号码是123456
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>关羽的电话号码是5352345
</span></span></code></pre></div><h2 id="3-4-faq">3.4. FAQ</h2>
<h3 id="1-open-webui页面无法选择模型">1）open-webui页面无法选择模型</h3>
<p><strong>问题：</strong></p>
<p>当单独部署open-webui，可能会遇到open-webui页面无法选择模型具体的现象如下：</p>
<img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739864579/article/linux/llm/open-webui-error.png" title="" alt="" width="709">
<p>open-webui日志报错：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>INFO  <span style="color:#ce5c00;font-weight:bold">[</span>open_webui.routers.ollama<span style="color:#ce5c00;font-weight:bold">]</span> get_all_models<span style="color:#ce5c00;font-weight:bold">()</span>
</span></span><span style="display:flex;"><span>ERROR <span style="color:#ce5c00;font-weight:bold">[</span>open_webui.routers.ollama<span style="color:#ce5c00;font-weight:bold">]</span> Connection error: Cannot connect to host 1.1.1.1:11434 ssl:default <span style="color:#ce5c00;font-weight:bold">[</span>Connect call failed <span style="color:#ce5c00;font-weight:bold">(</span><span style="color:#4e9a06">&#39;1.1.1.1&#39;</span>, 11434<span style="color:#ce5c00;font-weight:bold">)]</span>
</span></span></code></pre></div><p><strong>原因：</strong></p>
<p>按官网命令使用<code>端口映射</code>的网络模式，如果OLLAMA_BASE_URL配置为127.0.0.1则访问不到单独部署的ollama服务，如果改用具体的ollama的IP也可能存在访问失败的问题。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d -p 3000:8080 -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p><strong>解决方案：</strong></p>
<p>docker网络模式改为<code>host-network</code>的网络模式</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d --network<span style="color:#ce5c00;font-weight:bold">=</span>host -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><h1 id="4-总结">4. 总结</h1>
<p>本文主要介绍了ollama和open-webui的部署，从而搭建一个<code>本地化私有的大模型工具</code>，<code>所有的数据都存储在本地</code>。可以通过上传文件来分析本地的数据，类似构建<code>本地大模型知识库</code>。</p>
<p>不过本地大模型的响应速度依赖于大模型本身和本地的资源，包括cpu和gpu，没有gpu资源也可以运行。在资源较小的情况下，大模型回答问题的速度比较慢。如果完全需要离线的大模型分析数据，在资源受限的情况下需要再进一步做优化才能得到比较好的体验。</p>
<p>参考：</p>
<ul>
<li><a href="https://ollama.com/download/linux">https://ollama.com/download/linux</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/faq.md">https://github.com/ollama/ollama/blob/main/docs/faq.md</a></li>
<li><a href="https://docs.openwebui.com/getting-started/quick-start">https://docs.openwebui.com/getting-started/quick-start</a></li>
<li><a href="https://github.com/open-webui/open-webui#troubleshooting">https://github.com/open-webui/open-webui#troubleshooting</a></li>
</ul>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/huweihuang/blog.huweihuang.com" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2025 www.huweihuang.com 保留所有权利</small>
        <small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">隐私政策</a></small>
	
		<p class="mt-2"><a href="/about/">About</a></p>
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"
    integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN"
    crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js"
    integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA=="
    crossorigin="anonymous"></script>





<script src='/js/tabpane-persist.js'></script>




















<script src="/js/main.min.91798a335c881f1b6b805085ba4aa22d1dbd2b0b18d105d05189fa104ddae350.js" integrity="sha256-kXmKM1yIHxtrgFCFukqiLR29KwsY0QXQUYn6EE3a41A=" crossorigin="anonymous"></script>



<script src='/js/prism.js'></script>



  </body>
</html>
