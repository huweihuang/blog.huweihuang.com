<!doctype html>
<html lang="zh-cn" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.101.0" />
<link rel="canonical" type="text/html" href="https://blog.huweihuang.com/linux-notes/llm/">
<link rel="alternate" type="application/rss&#43;xml" href="https://blog.huweihuang.com/linux-notes/llm/index.xml">
<meta name="robots" content="noindex, nofollow">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/favicons/android-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/favicons/android-192x192.png" sizes="192x192">

<title>大模型 | 胡伟煌</title>
<meta name="description" content="">
<meta property="og:title" content="大模型" />
<meta property="og:description" content="Kubernetes学习笔记" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://blog.huweihuang.com/linux-notes/llm/" /><meta property="og:site_name" content="胡伟煌" />

<meta itemprop="name" content="大模型">
<meta itemprop="description" content="Kubernetes学习笔记"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="大模型"/>
<meta name="twitter:description" content="Kubernetes学习笔记"/>




<link rel="preload" href="/scss/main.min.79e3930541ca05b5395c14b2a313b798fad1dc69f9a14aefa57b62eaa9f65f14.css" as="style">
<link href="/scss/main.min.79e3930541ca05b5395c14b2a313b798fad1dc69f9a14aefa57b62eaa9f65f14.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.6.0.min.js"
  integrity="sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK"
  crossorigin="anonymous"></script>
<script
  src="https://unpkg.com/lunr@2.3.9/lunr.min.js"
  integrity="sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli"
  crossorigin="anonymous"></script>
<link rel="stylesheet" href="/css/prism.css"/>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-114718458-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" width="64" height="64"><path d="M15.9.476a2.14 2.14.0 00-.823.218L3.932 6.01c-.582.277-1.005.804-1.15 1.432L.054 19.373c-.13.56-.025 1.147.3 1.627q.057.087.12.168l7.7 9.574c.407.5 1.018.787 1.662.784h12.35c.646.001 1.258-.3 1.664-.793l7.696-9.576c.404-.5.555-1.16.4-1.786L29.2 7.43c-.145-.628-.57-1.155-1.15-1.432L16.923.695A2.14 2.14.0 0015.89.476z" fill="#326ce5"/><path d="M16.002 4.542c-.384.027-.675.356-.655.74v.188c.018.213.05.424.092.633a6.22 6.22.0 01.066 1.21c-.038.133-.114.253-.218.345l-.015.282c-.405.034-.807.096-1.203.186-1.666.376-3.183 1.24-4.354 2.485l-.24-.17c-.132.04-.274.025-.395-.04a6.22 6.22.0 01-.897-.81 5.55 5.55.0 00-.437-.465l-.148-.118c-.132-.106-.294-.167-.463-.175a.64.64.0 00-.531.236c-.226.317-.152.756.164.983l.138.11a5.55 5.55.0 00.552.323c.354.197.688.428.998.7a.74.74.0 01.133.384l.218.2c-1.177 1.766-1.66 3.905-1.358 6.006l-.28.08c-.073.116-.17.215-.286.288a6.22 6.22.0 01-1.194.197 5.57 5.57.0 00-.64.05l-.177.04h-.02a.67.67.0 00-.387 1.132.67.67.0 00.684.165h.013l.18-.02c.203-.06.403-.134.598-.218.375-.15.764-.265 1.162-.34.138.008.27.055.382.135l.3-.05c.65 2.017 2.016 3.726 3.84 4.803l-.122.255c.056.117.077.247.06.376-.165.382-.367.748-.603 1.092a5.58 5.58.0 00-.358.533l-.085.18a.67.67.0 00.65 1.001.67.67.0 00.553-.432l.083-.17c.076-.2.14-.404.192-.61.177-.437.273-.906.515-1.196a.54.54.0 01.286-.14l.15-.273a8.62 8.62.0 006.146.015l.133.255c.136.02.258.095.34.205.188.358.34.733.456 1.12a5.57 5.57.0 00.194.611l.083.17a.67.67.0 001.187.131.67.67.0 00.016-.701l-.087-.18a5.55 5.55.0 00-.358-.531c-.23-.332-.428-.686-.6-1.057a.52.52.0 01.068-.4 2.29 2.29.0 01-.111-.269c1.82-1.085 3.18-2.8 3.823-4.82l.284.05c.102-.093.236-.142.373-.138.397.076.786.2 1.162.34.195.09.395.166.598.23.048.013.118.024.172.037h.013a.67.67.0 00.841-.851.67.67.0 00-.544-.446l-.194-.046a5.57 5.57.0 00-.64-.05c-.404-.026-.804-.092-1.194-.197-.12-.067-.22-.167-.288-.288l-.27-.08a8.65 8.65.0 00-1.386-5.993l.236-.218c-.01-.137.035-.273.124-.378.307-.264.64-.497.99-.696a5.57 5.57.0 00.552-.323l.146-.118a.67.67.0 00-.133-1.202.67.67.0 00-.696.161l-.148.118a5.57 5.57.0 00-.437.465c-.264.302-.556.577-.873.823a.74.74.0 01-.404.044l-.253.18c-1.46-1.53-3.427-2.48-5.535-2.67.0-.1-.013-.25-.015-.297-.113-.078-.192-.197-.218-.332a6.23 6.23.0 01.076-1.207c.043-.21.073-.42.092-.633v-.2c.02-.384-.27-.713-.655-.74zm-.834 5.166-.2 3.493h-.015c-.01.216-.137.4-.332.504s-.426.073-.6-.054l-2.865-2.03a6.86 6.86.0 013.303-1.799c.234-.05.47-.088.707-.114zm1.668.0c1.505.187 2.906.863 3.99 1.924l-2.838 2.017c-.175.14-.415.168-.618.072s-.333-.3-.336-.524zm-6.72 3.227 2.62 2.338v.015c.163.142.234.363.186.574s-.21.378-.417.435v.01l-3.362.967a6.86 6.86.0 01.974-4.34zm11.753.0c.796 1.295 1.148 2.814 1.002 4.327l-3.367-.97v-.013c-.21-.057-.37-.224-.417-.435s.023-.43.186-.574l2.6-2.327zm-6.404 2.52h1.072l.655.832-.238 1.04-.963.463-.965-.463-.227-1.04zm3.434 2.838c.045-.005.1-.005.135.0l3.467.585c-.5 1.44-1.487 2.67-2.775 3.493l-1.34-3.244a.59.59.0 01.509-.819zm-5.823.015c.196.003.377.104.484.268s.124.37.047.55v.013l-1.332 3.218C11 21.54 10.032 20.325 9.517 18.9l3.437-.583c.038-.004.077-.004.116.0zm2.904 1.4a.59.59.0 01.537.308h.013l1.694 3.057-.677.2c-1.246.285-2.547.218-3.758-.194l1.7-3.057c.103-.18.293-.29.5-.295z" fill="#fff" stroke="#fff" stroke-width=".055"/></svg></span><span class="font-weight-bold">胡伟煌</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/kubernetes-notes/" ><span>Kubernetes学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/k8s-source-code-analysis/" ><span>Kubernetes源码分析</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link active" href="/linux-notes/" ><span class="active">Linux学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/golang-notes/" ><span>Golang学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/python-notes/" ><span>Python学习笔记</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				
				
				
				
				<a class="nav-link" href="/about/" ><span>About</span></a>
			</li>
			
			
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block"><input
  type="search"
  class="form-control td-search-input"
  placeholder="&#xf002; 站内搜索…"
  aria-label="站内搜索…"
  autocomplete="off"
  
  data-offline-search-index-json-src="/offline-search-index.5ffac8a474f1a0b6cd3b2070a250b3cd.json"
  data-offline-search-base-href="/"
  data-offline-search-max-results="10"
>
</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/linux-notes/llm/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">大模型</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-093f126ec9e750c4adca83e14d272d51">基于Ollama构建本地大模型</a></li>


    
  
    
    
	
<li>2: <a href="#pg-e44128ff477d39d7fb8644a3a4a27ea6">大模型相关概念</a></li>


    
  
    
    
	
<li>3: <a href="#pg-4beca5f671634e8081d9d49b28182303">大模型原理</a></li>


    
  
    
    
	
<li>4: <a href="#pg-878ad3564d3c6c133c245adba55de604">RAG工程</a></li>


    
  
    
    
	
<li>5: <a href="#pg-7b65909556914234bb5eaf029e28201c">大模型微调</a></li>


    
  
    
    
	
<li>6: <a href="#pg-4fdff5e5273cc1b3e9dfe04c52604d40">大模型agent概述</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-093f126ec9e750c4adca83e14d272d51">1 - 基于Ollama构建本地大模型</h1>
    
	<p>本文主要介绍如何通过<code>Ollama</code>和<code>OpenWebUI</code>来搭建一个本地私有化运行的大模型工具。私有化大模型的构建主要用于解决<code>数据的安全性问题</code>，对于大部分私有数据不适合通过外部的大模型网站来上传和分析。</p>
<h1 id="1-ollama-与-openwebui-介绍">1. Ollama 与 OpenWebUI 介绍</h1>
<h2 id="1-1-ollama简介">1.1. Ollama简介</h2>
<p>Ollama 是一个 <strong>本地运行的 AI 大模型管理工具</strong>，可以让你在本地 <strong>快速拉取、管理和运行</strong> 各种开源大语言模型（如 LLaMA、Mistral、deepseek 等），而无需依赖云端 API。它的主要特点包括：</p>
<ul>
<li><strong>简易安装</strong>：支持 macOS、Linux 和 Windows（WSL）。</li>
<li><strong>本地推理</strong>：在本地设备上直接运行 LLM，保护数据隐私。</li>
<li><strong>模型管理</strong>：可以像使用 Docker 一样 <code>ollama run llama2</code> 轻松拉取和运行模型。</li>
<li><strong>自定义模型</strong>：支持通过 <code>Modelfile</code> 进行微调和定制。</li>
<li><strong>支持 API</strong>：可以通过 Python、Node.js 等语言调用 Ollama 提供的本地 REST API。</li>
</ul>
<p>Ollama 适用于本地 AI 代理、嵌入式 AI 应用、隐私保护的智能助手等场景。你可以用它来运行大语言模型，而无需自己搭建复杂的推理环境。</p>
<h2 id="1-2-openwebui简介">1.2. OpenWebUI简介</h2>
<p><strong>Open-WebUI</strong> 是一个 <strong>开源的 Web 用户界面</strong>，用于管理和使用本地或远程的大语言模型（LLM），比如 Ollama、OpenAI、Gemini 等。它的主要特点包括：</p>
<ul>
<li><strong>友好的 Web 界面</strong>：提供 ChatGPT 类似的对话 UI，方便交互。</li>
<li><strong>支持多种后端</strong>：可以连接 <strong>Ollama、OpenAI API、本地 LLM</strong> 等。</li>
<li><strong>多用户支持</strong>：适用于团队协作。</li>
<li><strong>对话历史管理</strong>：可保存和管理聊天记录。</li>
<li><strong>插件和自定义功能</strong>：支持扩展，适用于不同应用场景。</li>
</ul>
<p>它可以让本地 LLM 变得更加易用，适合个人、企业部署本地 AI 助手。</p>
<h1 id="2-部署ollama">2. 部署ollama</h1>
<h2 id="2-1-脚本安装-ollama">2.1. 脚本安装<code>ollama</code></h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh <span style="color:#000;font-weight:bold">|</span> sh
</span></span></code></pre></div><p>输出</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>&gt;&gt;&gt; Installing ollama to /usr/local
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Downloading Linux amd64 bundle
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic">######################################################################## 100.0%</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama user...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to render group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to video group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding current user to ollama group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama systemd service...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Enabling and starting ollama service...
</span></span><span style="display:flex;"><span>Created symlink /etc/systemd/system/default.target.wants/ollama.service -&gt; /etc/systemd/system/ollama.service.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; The Ollama API is now available at 127.0.0.1:11434.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Install complete. Run <span style="color:#4e9a06">&#34;ollama&#34;</span> from the <span style="color:#204a87">command</span> line.
</span></span><span style="display:flex;"><span>WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.
</span></span></code></pre></div><p>默认服务监听的地址为：<code>127.0.0.1:11434</code></p>
<h2 id="2-2-查看-ollama-服务状态">2.2. 查看<code>ollama</code>服务状态</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>systemctl status ollama
</span></span><span style="display:flex;"><span>* ollama.service - Ollama Service
</span></span><span style="display:flex;"><span>     Loaded: loaded <span style="color:#ce5c00;font-weight:bold">(</span>/etc/systemd/system/ollama.service<span style="color:#000;font-weight:bold">;</span> enabled<span style="color:#000;font-weight:bold">;</span> vendor preset: enabled<span style="color:#ce5c00;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>     Active: active <span style="color:#ce5c00;font-weight:bold">(</span>running<span style="color:#ce5c00;font-weight:bold">)</span> since Fri 2025-02-07 17:21:55 +08<span style="color:#000;font-weight:bold">;</span> 23s ago
</span></span><span style="display:flex;"><span>   Main PID: <span style="color:#0000cf;font-weight:bold">53472</span> <span style="color:#ce5c00;font-weight:bold">(</span>ollama<span style="color:#ce5c00;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>      Tasks: <span style="color:#0000cf;font-weight:bold">10</span>
</span></span><span style="display:flex;"><span>     Memory: 10.3M
</span></span><span style="display:flex;"><span>     CGroup: /system.slice/ollama.service
</span></span><span style="display:flex;"><span>             <span style="color:#4e9a06">`</span>-53472 /usr/local/bin/ollama serve
</span></span></code></pre></div><p>查看<code>ollama</code>命令</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama --help</span>
</span></span><span style="display:flex;"><span>Large language model runner
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Usage:
</span></span><span style="display:flex;"><span>  ollama <span style="color:#ce5c00;font-weight:bold">[</span>flags<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>  ollama <span style="color:#ce5c00;font-weight:bold">[</span>command<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Available Commands:
</span></span><span style="display:flex;"><span>  serve       Start ollama
</span></span><span style="display:flex;"><span>  create      Create a model from a Modelfile
</span></span><span style="display:flex;"><span>  show        Show information <span style="color:#204a87;font-weight:bold">for</span> a model
</span></span><span style="display:flex;"><span>  run         Run a model
</span></span><span style="display:flex;"><span>  stop        Stop a running model
</span></span><span style="display:flex;"><span>  pull        Pull a model from a registry
</span></span><span style="display:flex;"><span>  push        Push a model to a registry
</span></span><span style="display:flex;"><span>  list        List models
</span></span><span style="display:flex;"><span>  ps          List running models
</span></span><span style="display:flex;"><span>  cp          Copy a model
</span></span><span style="display:flex;"><span>  rm          Remove a model
</span></span><span style="display:flex;"><span>  <span style="color:#204a87">help</span>        Help about any <span style="color:#204a87">command</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Flags:
</span></span><span style="display:flex;"><span>  -h, --help      <span style="color:#204a87">help</span> <span style="color:#204a87;font-weight:bold">for</span> ollama
</span></span><span style="display:flex;"><span>  -v, --version   Show version information
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Use <span style="color:#4e9a06">&#34;ollama [command] --help&#34;</span> <span style="color:#204a87;font-weight:bold">for</span> more information about a command.
</span></span></code></pre></div><h2 id="2-3-拉取一个大模型">2.3. 拉取一个大模型</h2>
<p>可以在 <a href="https://ollama.com/search">https://ollama.com/search</a> 网站上，选择一个所需要的大模型，例如<code>deepseek-r1:7b</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载指定的大模型，例如deepseek</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
</span></span><span style="display:flex;"><span>pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">387</span> B
</span></span><span style="display:flex;"><span>pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
</span></span><span style="display:flex;"><span>pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">148</span> B
</span></span><span style="display:flex;"><span>pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">487</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama list</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      MODIFIED
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    4.7 GB    <span style="color:#0000cf;font-weight:bold">12</span> minutes ago
</span></span></code></pre></div><h2 id="2-4-运行大模型">2.4. 运行大模型</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama run deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; 你是谁
</span></span><span style="display:flex;"><span>&lt;think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&lt;/think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; /bye
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 查看正在运行的模型</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama ps</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      PROCESSOR    UNTIL
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    5.5 GB    100% CPU     <span style="color:#0000cf;font-weight:bold">3</span> minutes from now
</span></span></code></pre></div><h2 id="2-5-修改ollama服务地址和目录">2.5. 修改ollama服务地址和目录</h2>
<h3 id="2-5-1-修改ollama服务地址">2.5.1. 修改ollama服务地址</h3>
<p>ollama服务默认监听127.0.0.1, 如果要修改监听地址，则可以添加<code>Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vi /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Unit<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Description</span><span style="color:#ce5c00;font-weight:bold">=</span>Ollama Service
</span></span><span style="display:flex;"><span><span style="color:#000">After</span><span style="color:#ce5c00;font-weight:bold">=</span>network-online.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Service<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;OLLAMA_HOST=0.0.0.0:11434&#34;</span>   <span style="color:#8f5902;font-style:italic"># 增加环境变量</span>
</span></span><span style="display:flex;"><span><span style="color:#000">ExecStart</span><span style="color:#ce5c00;font-weight:bold">=</span>/usr/local/bin/ollama serve
</span></span><span style="display:flex;"><span><span style="color:#000">User</span><span style="color:#ce5c00;font-weight:bold">=</span>ollama
</span></span><span style="display:flex;"><span><span style="color:#000">Group</span><span style="color:#ce5c00;font-weight:bold">=</span>ollama
</span></span><span style="display:flex;"><span><span style="color:#000">Restart</span><span style="color:#ce5c00;font-weight:bold">=</span>always
</span></span><span style="display:flex;"><span><span style="color:#000">RestartSec</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#0000cf;font-weight:bold">3</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Install<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">WantedBy</span><span style="color:#ce5c00;font-weight:bold">=</span>default.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 重启服务</span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama
</span></span><span style="display:flex;"><span>systemctl status ollama
</span></span></code></pre></div><h3 id="2-5-2-修改ollama数据目录">2.5.2. 修改ollama数据目录</h3>
<p>参考：<a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored">ollama/docs/faq.md</a></p>
<p>默认存储目录</p>
<ul>
<li>macOS: <code>~/.ollama/models</code></li>
<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>
<li>Windows: <code>C:\Users\%username%\.ollama\models</code></li>
</ul>
<p>以linux系统为例，修改默认的存储目录：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#000">dir</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;/data/ollama/models&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 创建目录并分配权限</span>
</span></span><span style="display:flex;"><span>mkdir -p /data/ollama/models
</span></span><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 添加环境变量OLLAMA_MODELS</span>
</span></span><span style="display:flex;"><span>vi /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ce5c00;font-weight:bold">[</span>Service<span style="color:#ce5c00;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#000">Environment</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;OLLAMA_MODELS=/data/ollama/models&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 重启服务</span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama
</span></span><span style="display:flex;"><span>systemctl status ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 迁移数据</span>
</span></span><span style="display:flex;"><span>cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models
</span></span><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span></code></pre></div><h1 id="3-部署open-webui">3. 部署open-webui</h1>
<h2 id="3-1-单独部署open-webui">3.1. 单独部署open-webui</h2>
<p>如果已经部署了ollama服务，可以通过以下命令单独部署open-webui，修改<code>OLLAMA_BASE_URL</code>为ollama的服务地址。如果使用host-network，默认服务监听端口为<code>8080</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d --network<span style="color:#ce5c00;font-weight:bold">=</span>host -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p>环境变量</p>
<ul>
<li><code>OLLAMA_BASE_URL:http://OLLAMA_HOST:11434</code> : 设置ollama服务的地址</li>
<li><code>HF_HUB_OFFLINE: &quot;1&quot;</code>：设置模型为离线的环境</li>
<li><code>ENABLE_OPENAI_API: &quot;false&quot;</code>：设置关闭openai的接口</li>
</ul>
<p><strong>访问open-webui服务：</strong></p>
<p>访问<code>http://服务器IP:8080</code>，注册用户名密码然后登录。就可以使用本地的大模型服务。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739864579/article/linux/llm/open-webui.png" alt=""></p>
<h2 id="3-2-部署open-webui和ollama服务">3.2. 部署open-webui和ollama服务</h2>
<p>如果不想单独部署ollama，可以通过open-webui:ollama镜像，同时部署open-webui和ollama，两个服务集成在同一个镜像中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载镜像</span>
</span></span><span style="display:flex;"><span>docker pull ghcr.io/open-webui/open-webui:ollama
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 运行open-webui:ollama</span>
</span></span><span style="display:flex;"><span>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v ollama-open-webui:/app/backend/data --name ollama-open-webui --restart always ghcr.io/open-webui/open-webui:ollama
</span></span></code></pre></div><p>查看服务</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker images</span>
</span></span><span style="display:flex;"><span>REPOSITORY                      TAG       IMAGE ID       CREATED        SIZE
</span></span><span style="display:flex;"><span>ghcr.io/open-webui/open-webui   ollama    29d60b4958c8   <span style="color:#0000cf;font-weight:bold">4</span> days ago     8.02GB
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker ps</span>
</span></span><span style="display:flex;"><span>CONTAINER ID   IMAGE                                  COMMAND           CREATED          STATUS                    PORTS                              NAMES
</span></span><span style="display:flex;"><span>3175fc20c608   ghcr.io/open-webui/open-webui:ollama   <span style="color:#4e9a06">&#34;bash start.sh&#34;</span>   <span style="color:#0000cf;font-weight:bold">16</span> minutes ago   Up <span style="color:#0000cf;font-weight:bold">16</span> minutes <span style="color:#ce5c00;font-weight:bold">(</span>healthy<span style="color:#ce5c00;font-weight:bold">)</span>   0.0.0.0:3000-&gt;8080/tcp             ollama-open-webui
</span></span></code></pre></div><p>登录容器下载大模型文件</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 登录容器</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># docker exec -it 3175fc20c608 bash</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 下载指定的大模型</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:7b</span>
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
</span></span><span style="display:flex;"><span>pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">387</span> B
</span></span><span style="display:flex;"><span>pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
</span></span><span style="display:flex;"><span>pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">148</span> B
</span></span><span style="display:flex;"><span>pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏  <span style="color:#0000cf;font-weight:bold">487</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama list</span>
</span></span><span style="display:flex;"><span>NAME              ID              SIZE      MODIFIED
</span></span><span style="display:flex;"><span>deepseek-r1:7b    0a8c26691023    4.7 GB    <span style="color:#0000cf;font-weight:bold">12</span> minutes ago
</span></span></code></pre></div><p>则可以访问所部属服务器的地址和端口来访问open-webui的服务。</p>
<h2 id="3-3-构建本地知识库">3.3. 构建本地知识库</h2>
<h3 id="3-3-1-自定义文件分析">3.3.1. 自定义文件分析</h3>
<p>可以通过页面上传本地的知识库文件，让AI回答关于自定义文件中的内容。</p>
<p>例如：我通过文件自定义了内容，提问张飞的电话号码，则可以通过文章中的内容来回答。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739871922/article/linux/llm/phone-chat.png" alt=""></p>
<p>其中自定义文档的内容如下：</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739869289/article/linux/llm/ollama-docs.png" alt=""></p>
<p>同样可以上传其他文件来构建一个本地大模型知识库。然后借助大模型来查询和分析数据内容。</p>
<h3 id="3-3-2-本地化数据存储">3.3.2. 本地化数据存储</h3>
<p>其中open-webui的本地化数据存储在容器内的<code>/app/backend/data/</code>目录下。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>/app/backend/data# ls -l
</span></span><span style="display:flex;"><span>total <span style="color:#0000cf;font-weight:bold">236</span>
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">7</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">11</span> 10:48 cache
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">2</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:11 uploads
</span></span><span style="display:flex;"><span>drwxr-xr-x <span style="color:#0000cf;font-weight:bold">3</span> root root   <span style="color:#0000cf;font-weight:bold">4096</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:11 vector_db
</span></span><span style="display:flex;"><span>-rw-r--r-- <span style="color:#0000cf;font-weight:bold">1</span> root root <span style="color:#0000cf;font-weight:bold">229376</span> Feb <span style="color:#0000cf;font-weight:bold">18</span> 06:38 webui.db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 可以从uploads目录看到上传的本地文件</span>
</span></span><span style="display:flex;"><span>/app/backend/data/uploads# cat 117e6f99-0657-40d1-ab6f-1bea81e78053_ollama-docs.md
</span></span><span style="display:flex;"><span>张飞的电话号码是u987438274
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>曹操的电话号码是123456
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>关羽的电话号码是5352345
</span></span></code></pre></div><h2 id="3-4-faq">3.4. FAQ</h2>
<h3 id="1-open-webui页面无法选择模型">1）open-webui页面无法选择模型</h3>
<p><strong>问题：</strong></p>
<p>当单独部署open-webui，可能会遇到open-webui页面无法选择模型具体的现象如下：</p>
<img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1739864579/article/linux/llm/open-webui-error.png" title="" alt="" width="709">
<p>open-webui日志报错：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>INFO  <span style="color:#ce5c00;font-weight:bold">[</span>open_webui.routers.ollama<span style="color:#ce5c00;font-weight:bold">]</span> get_all_models<span style="color:#ce5c00;font-weight:bold">()</span>
</span></span><span style="display:flex;"><span>ERROR <span style="color:#ce5c00;font-weight:bold">[</span>open_webui.routers.ollama<span style="color:#ce5c00;font-weight:bold">]</span> Connection error: Cannot connect to host 1.1.1.1:11434 ssl:default <span style="color:#ce5c00;font-weight:bold">[</span>Connect call failed <span style="color:#ce5c00;font-weight:bold">(</span><span style="color:#4e9a06">&#39;1.1.1.1&#39;</span>, 11434<span style="color:#ce5c00;font-weight:bold">)]</span>
</span></span></code></pre></div><p><strong>原因：</strong></p>
<p>按官网命令使用<code>端口映射</code>的网络模式，如果OLLAMA_BASE_URL配置为127.0.0.1则访问不到单独部署的ollama服务，如果改用具体的ollama的IP也可能存在访问失败的问题。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d -p 3000:8080 -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p><strong>解决方案：</strong></p>
<p>docker网络模式改为<code>host-network</code>的网络模式</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d --network<span style="color:#ce5c00;font-weight:bold">=</span>host -e <span style="color:#000">OLLAMA_BASE_URL</span><span style="color:#ce5c00;font-weight:bold">=</span>http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><h3 id="2-数据目录没权限permission-denied">2）数据目录没权限permission denied</h3>
<p>如果用户修改了ollama的models的存储目录，出现ollama服务重启失败，或者pull model数据报错</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 修改ollama的model目录后ollama服务重启报错</span>
</span></span><span style="display:flex;"><span>Error: mkdir /data/ollama: permission denied
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 迁移model数据后出现没权限，因为使用了root命令执行</span>
</span></span><span style="display:flex;"><span>cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># ollama pull deepseek-r1:70b</span>
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>Error: open /data/ollama/models/manifests/registry.ollama.ai/library/deepseek-r1/70b: permission denied
</span></span></code></pre></div><p><strong>原因：</strong></p>
<p>ollama默认使用的用户名是 ollama，因此需要给目录添加用户的权限，例如：目录创建和model文件迁移是通过root或其他用户执行的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo chown -R ollama:ollama /data/ollama/models
</span></span></code></pre></div><h1 id="4-总结">4. 总结</h1>
<p>本文主要介绍了ollama和open-webui的部署，从而搭建一个<code>本地化私有的大模型工具</code>，<code>所有的数据都存储在本地</code>。可以通过上传文件来分析本地的数据，类似构建<code>本地大模型知识库</code>。</p>
<p>不过本地大模型的响应速度依赖于大模型本身和本地的资源，包括cpu和gpu，没有gpu资源也可以运行。在资源较小的情况下，大模型回答问题的速度比较慢。如果完全需要离线的大模型分析数据，在资源受限的情况下需要再进一步做优化才能得到比较好的体验。</p>
<p>参考：</p>
<ul>
<li><a href="https://ollama.com/download/linux">https://ollama.com/download/linux</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/faq.md">https://github.com/ollama/ollama/blob/main/docs/faq.md</a></li>
<li><a href="https://docs.openwebui.com/getting-started/quick-start">https://docs.openwebui.com/getting-started/quick-start</a></li>
<li><a href="https://github.com/open-webui/open-webui#troubleshooting">https://github.com/open-webui/open-webui#troubleshooting</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e44128ff477d39d7fb8644a3a4a27ea6">2 - 大模型相关概念</h1>
    
	<blockquote>
<p>本文主要介绍大模型领域常用的名词概念等。</p>
</blockquote>
<h1 id="mcp">MCP</h1>
<h2 id="mcp的概念">MCP的概念</h2>
<p>MCP的全称是<code>Model Context Protocol</code>，即模型上下文协议。根据官网的解释，MCP 是一个开放协议，它规范了应用程序向 LLM 提供<code>上下文</code>的方式。MCP 就像 AI 应用程序的 <code>USB-C </code>端口一样。正如 USB-C 提供了一种标准化的方式将您的设备连接到各种外围设备和配件一样，MCP 也提供了一种标准化的方式将 AI 模型连接到不同的数据源和工具。</p>
<p>简单理解<code>模型上下文协议</code>，可以把它拆成三个部分：</p>
<ul>
<li>
<p><code>模型</code>：协议的使用方即大模型工具。</p>
</li>
<li>
<p><code>上下文</code>：提供存储上下文的功能，类似于存储大模型历史交互的记忆。</p>
</li>
<li>
<p><code>协议</code>：本质是一种标准，类比与TCP协议，http协议，而MCP可以类比于http之上的业务层协议。</p>
</li>
</ul>
<p>它的目的是 <strong>定义模型上下文的结构、状态传递、缓存方式、状态更新机制等内容</strong>，以支持复杂推理任务中的“状态保持”和“多轮交互”。</p>
<h2 id="为什么需要mcp">为什么需要MCP</h2>
<p>在大模型推理中，尤其是以下场景：</p>
<ul>
<li>
<p>多轮对话（如 ChatGPT）</p>
</li>
<li>
<p>Streaming 推理</p>
</li>
<li>
<p>长上下文处理（如 RAG、LLM agent）</p>
</li>
<li>
<p>分布式推理流水线（分stage执行）</p>
</li>
</ul>
<p>我们需要在模型之间、服务组件之间传递“上下文状态”（如 past key value、token buffer、history、session id）。这个时候，就需要一套 <strong>标准的协议</strong> 来描述和控制这些状态 —— MCP 就是为此而生。</p>
<p>可以将 MCP 看作在 HTTP 或 gRPC 上层的“<strong>业务语义协议</strong>”：</p>
<pre tabindex="0"><code>┌──────────────────────────────────────────────┐
│                Application (LLM API)         │
│     ┌─────────────┐                          │
│     │   MCP 协议   │  &lt;-- 管理模型上下文状态   │
│     └─────────────┘                          │
│         ↑ 使用 protobuf / JSON               │
│     ┌─────────────┐                          │
│     │   HTTP/gRPC  │  &lt;-- 实际传输协议        │
│     └─────────────┘                          │
│         ↑ 使用 TCP/IP                        │
└──────────────────────────────────────────────┘
</code></pre><h2 id="类比">类比</h2>
<p><strong>MCP 是“大脑助手的记忆本”</strong></p>
<p>想象你在和一个 AI 助手（比如 ChatGPT）对话，它像一个演员在扮演各种角色。</p>
<p><strong>如果没有 MCP：</strong></p>
<p>助手每次都“失忆”，你要一遍遍重复之前的内容，它无法记得你是谁、想干嘛、聊过啥。</p>
<p><strong>如果有 MCP：</strong></p>
<p>就像它有了一本“<strong>记忆本</strong>”：</p>
<ul>
<li>
<p>每次你对它说话，它都记下“对话历史”、“你正在聊的主题”、“你的偏好”</p>
</li>
<li>
<p>它能从记忆本中取出之前你说的话，继续展开对话</p>
</li>
<li>
<p>这个“记忆本”就由 <strong>MCP 协议定义结构、内容和传递方式</strong></p>
</li>
</ul>
<p>🧠 <strong>形象理解</strong>：</p>
<blockquote>
<p>MCP 就像是 AI 模型的大脑助手，负责“记住你们聊过什么”，以及“接下来该怎么回复你”。</p>
</blockquote>
<h1 id="mcp-server">MCP server</h1>
<p>MCP server是模型上下文协议（Model Context Protocol）中的一个组件，它扮演着AI模型与外部数据、工具和功能之间连接的桥梁。它就像一个工具箱，为AI模型提供各种功能，比如访问文件、调用API、执行计算等，可以实现更复杂的任务。﻿</p>
<p>更详细地说，MCP server是一种轻量级服务程序，负责提供数据、工具和提示，帮助AI模型理解和执行任务。它通过标准化的接口和协议，将外部能力暴露给AI模型，让AI模型能够访问各种外部资源，例如数据库、API、文件等。</p>
<p>mcp server 链接：</p>
<ul>
<li><a href="https://mcp.ad/">https://mcp.ad/</a></li>
</ul>
<h1 id="prompt">Prompt</h1>
<blockquote>
<p>Prompt 就是你给大模型的“指令 + 提示 +上下文”，用来引导它输出你想要的结果。</p>
</blockquote>
<p><strong>Prompt 是 MCP 协议中的核心内容之一</strong>，而 <strong>MCP 是管理 Prompt 的“协议和系统”</strong>，用来组织、封装、传输、复用 prompt（以及上下文）以便模型推理使用。</p>
<p><strong>Prompt 是 MCP 协议中的核心内容之一</strong>，而 <strong>MCP 是管理 Prompt 的“协议和系统”</strong>，用来组织、封装、传输、复用 prompt（以及上下文）以便模型推理使用。</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Prompt</th>
<th>MCP（Model Context Protocol）</th>
</tr>
</thead>
<tbody>
<tr>
<td>本质</td>
<td>一段输入文本（或 token 序列）</td>
<td>一种上下文协议，组织模型输入/状态/上下文的格式与流程</td>
</tr>
<tr>
<td>作用</td>
<td>引导模型生成内容</td>
<td>管理 Prompt 和上下文的结构、状态、生命周期</td>
</tr>
<tr>
<td>是否直接传给模型</td>
<td>✅ 是模型输入的一部分</td>
<td>✅ 会生成 prompt，并交给模型</td>
</tr>
<tr>
<td>是谁的子集？</td>
<td>Prompt 是 MCP 的<strong>一部分字段</strong></td>
<td>MCP 是对 Prompt 及其上下文的<strong>结构化封装</strong></td>
</tr>
<tr>
<td>类比关系</td>
<td>你要说的一句话</td>
<td>你整场对话的“聊天记录 + 状态 + 上下文管理系统”</td>
</tr>
</tbody>
</table>
<p>示例：</p>
<p>Prompt：</p>
<blockquote>
<p>“你是一个翻译专家，请将下面这段话翻译为英文：‘我爱编程’”</p>
</blockquote>
<p>最终形成一个这样的 MCP payload：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">{</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">&#34;session_id&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;abc123&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;stage&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;prompt&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;inputs&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;prompt&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;你是一个翻译专家，请将下面这段话翻译为英文：‘我爱编程’&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;history&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">[</span><span style="color:#a40000">...</span><span style="color:#000;font-weight:bold">],</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;role&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#4e9a06">&#34;user&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">},</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;generation_config&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;temperature&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#0000cf;font-weight:bold">0.8</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87;font-weight:bold">&#34;top_p&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#0000cf;font-weight:bold">0.95</span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">},</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;kv_cache_refs&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#000;font-weight:bold">[],</span>
</span></span><span style="display:flex;"><span>  <span style="color:#204a87;font-weight:bold">&#34;position&#34;</span><span style="color:#000;font-weight:bold">:</span> <span style="color:#0000cf;font-weight:bold">128</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">}</span>
</span></span></code></pre></div><h1 id="token">Token</h1>
<blockquote>
<p><strong>Token 是大模型处理语言时的最小单位</strong>，相当于模型的“语言颗粒度”。它可能是一个词、一个子词、一个字符，甚至是一部分单词。</p>
</blockquote>
<p>人说话是用句子、单词，大模型不能直接理解这些自然语言，它必须先<strong>把文本切割成小块（token）</strong>，再转成数字（embedding）才能理解。</p>
<p>比如下面这句话：<code>你好，世界！</code></p>
<p>对人来说是 几 个字，但对模型来说，它可能被切分成 <strong>2~4 个 token</strong>，取决于使用的 tokenizer（分词器）。</p>
<p><strong>为什么 token 很重要？</strong></p>
<ul>
<li>
<p><strong>计费单位</strong>（OpenAI/Claude/其他 API）是按 token 收费的</p>
</li>
<li>
<p><strong>上下文长度限制</strong> 是按 token 算的，不是按“字”算的</p>
</li>
<li>
<p><strong>模型性能 &amp; 精度</strong> 和 token 分布、长度密切相关</p>
</li>
<li>
<p><strong>Prompt 工程</strong> 就是用尽可能少的 token 实现更强的引导效果</p>
</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://modelcontextprotocol.io/introduction">Introduction - Model Context Protocol</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4beca5f671634e8081d9d49b28182303">3 - 大模型原理</h1>
    
	<blockquote>
<p>本文基于<a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/tree/main">GitHub - AlibabaCloudDocs/aliyun_acp_learning</a>内容整理。</p>
</blockquote>
<h1 id="1-大模型是如何工作的">1. 大模型是如何工作的</h1>
<h2 id="1-1-大模型的问答工作流程">1.1. 大模型的问答工作流程</h2>
<p>大模型问答工作流程主要有以下五个阶段</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338382/article/linux/llm/llm_flow.png" alt=""></p>
<h3 id="1-1-1-输入文本分词化">1.1.1. 输入文本分词化</h3>
<p>分词（Token）是大模型处理文本的基本单元，通常是词语、词组或者符号。我们需要将“ACP is a very”这个句子分割成更小且具有独立语义的词语（Token），并且为每个Token分配一个ID。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338375/article/linux/llm/token.jpg" alt=""></p>
<h3 id="1-1-2-token向量化">1.1.2. Token向量化</h3>
<p>将分词（Token）转换为数字使得其被计算机理解，即将每个Token转换为固定维度的向量。</p>
<h3 id="1-1-3-大模型推理">1.1.3. 大模型推理</h3>
<p>大模型通过已有数据的训练，它会计算所有可能的token的概率，并选出下一个输出的token。当大模型回答私域知识，即不涉及训练的内容的时候，则无法回答出问题。</p>
<h3 id="1-1-4-输出token">1.1.4. 输出Token</h3>
<p>大模型会根据token的概率来随机进行挑选，即问题完全相同，每次的回答都可能略有不同。为了控制问题的随机性，可以通过temperature和top_p来调整。</p>
<h3 id="1-1-5-输出文本">1.1.5. 输出文本</h3>
<p>循环第三和第四的步骤，直到输出特殊的Token（如EOS，end of sentence）或输出的长度达到阈值，从而结束回答，并输出所有的内容。也可以使用流式输出，即预测下一个token后立即返回给用户。</p>
<h2 id="1-2-影响大模型内存生成的随机性参数">1.2. 影响大模型内存生成的随机性参数</h2>
<p>大模型随机性和多样性的2个重要的参数是<code>temperature</code>和<code>top_p</code>。</p>
<h3 id="1-2-1-temperature-调整候选token集合的概率分布">1.2.1. temperature：调整候选Token集合的概率分布</h3>
<p>temperature是一个调节器，它通过改变候选Token（next-token）的概率分布，影响大模型的内容生成。</p>
<p>针对不同使用场景，可参考以下建议设置 temperature 参数：</p>
<ul>
<li>
<p><strong>明确答案（如生成代码）：调低温度。</strong></p>
</li>
<li>
<p><strong>创意多样（如广告文案）：调高温度。</strong></p>
</li>
<li>
<p>无特殊需求：使用默认温度（通常为中温度范围）。</p>
</li>
</ul>
<p>需要注意的是，当 temperature=0 时，虽然会最大限度降低随机性，但无法保证每次输出完全一致。温度值越高，模型生成的内容更具变化和多样性。可参考<a href="https://github.com/huggingface/transformers/blob/v4.49.0/src/transformers/generation/logits_process.py#L226">temperature的底层算法实现</a>。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338375/article/linux/llm/temperature.jpg" alt=""></p>
<h3 id="1-2-2-top-p-控制候选token集合的采样范围">1.2.2. top_p：控制候选Token集合的采样范围</h3>
<p>top_p 是一种筛选机制，用于从候选 Token 集合中选出符合特定条件的“小集合”。具体方法是：按概率从高到低排序，选取累计概率达到设定阈值的 Token 组成新的候选集合，从而缩小选择范围。</p>
<p>top_p值对大模型生成内容的影响可总结为：</p>
<ul>
<li>
<p><strong>值越大 ：候选范围越广，内容更多样化，适合创意写作、诗歌生成等场景。</strong></p>
</li>
<li>
<p><strong>值越小 ：候选范围越窄，输出更稳定，适合新闻初稿、代码生成等需要明确答案的场景。</strong></p>
</li>
<li>
<p>极小值（如 0.0001）：理论上模型只选择概率最高的 Token，输出非常稳定。但实际上，由于分布式系统、模型输出的额外调整等因素可能引入的微小随机性，仍无法保证每次输出完全一致。</p>
</li>
</ul>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338378/article/linux/llm/top_p.jpg" alt=""></p>
<h3 id="1-2-3-如何使用temperature和top-p">1.2.3. 如何使用temperature和top_p</h3>
<p>为了确保生成内容的可控性，建议不要同时调整temperature和top_p，可以通过控制变量法，逐步调整其中一个参数来实现微调。在其他的大模型也会增加其他参数来控制内容的多样性，例如通义千问模型的参数<code>seed</code>。具体可以参考对应的API说明。</p>
<h1 id="2-让大模型回答私域知识">2. 让大模型回答私域知识</h1>
<p>由于大模型是根据已知的知识训练成的，因此他无法回答未训练的知识，例如私域知识。好比人无法回答他所不知道的事情。如何让大模型回答私域的知识，就是把私域的知识喂给它。</p>
<h2 id="2-1-初步方案-在提示词中喂知识">2.1. 初步方案：在提示词中喂知识</h2>
<p>例如：</p>
<p>在已知的文档中找到跟问题相关的信息，然后放入prompt中一起请求大模型接口。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#000">user_question</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#4e9a06">&#34;我是软件一组的，请问项目管理应该用什么工具&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#000">knowledge</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#4e9a06">&#34;&#34;&#34;公司项目管理工具有两种选择：
</span></span></span><span style="display:flex;"><span><span style="color:#4e9a06">  1. **Jira**：对于软件开发团队来说，Jira 是一个非常强大的工具，支持敏捷开发方法，如Scrum和Kanban。它提供了丰富的功能，包括问题跟踪、时间跟踪等。
</span></span></span><span style="display:flex;"><span><span style="color:#4e9a06">
</span></span></span><span style="display:flex;"><span><span style="color:#4e9a06">  2. **Microsoft Project**：对于大型企业或复杂项目，Microsoft Project 提供了详细的计划制定、资源分配和成本控制等功能。它更适合那些需要严格控制项目时间和成本的场景。
</span></span></span><span style="display:flex;"><span><span style="color:#4e9a06">
</span></span></span><span style="display:flex;"><span><span style="color:#4e9a06">  在一般情况下请使用Microsoft Project，公司购买了完整的许可证。软件研发一组、三组和四组正在使用Jira，计划于2026年之前逐步切换至Microsoft Project。
</span></span></span><span style="display:flex;"><span><span style="color:#4e9a06">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#000">response</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">get_qwen_stream_response</span><span style="color:#000;font-weight:bold">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">user_prompt</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">user_question</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8f5902;font-style:italic"># 将公司项目管理工具相关的知识作为背景信息传入系统提示词</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">system_prompt</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;你负责教育内容开发公司的答疑，你的名字叫公司小蜜，你要回答学员的问题。&#34;</span><span style="color:#ce5c00;font-weight:bold">+</span> <span style="color:#000">knowledge</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">temperature</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#0000cf;font-weight:bold">0.7</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">top_p</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#0000cf;font-weight:bold">0.8</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">for</span> <span style="color:#000">chunk</span> <span style="color:#204a87;font-weight:bold">in</span> <span style="color:#000">response</span><span style="color:#000;font-weight:bold">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#000">chunk</span><span style="color:#000;font-weight:bold">,</span> <span style="color:#000">end</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span></code></pre></div><p>上述的方式会引入新的问题，比如你无法把所有的数据都喂给大模型，会带来一些问题：</p>
<ol>
<li>
<p><strong>效率低</strong>：上下文越长，大模型处理所需的时间就越长，导致用户等待时间增加。</p>
</li>
<li>
<p><strong>成本高</strong>：大部分模型是按输入和输出的文本量计费的，冗长的上下文意味着更高的成本。</p>
</li>
<li>
<p><strong>信息干扰</strong>：如果上下文中包含了大量与当前问题无关的信息，就像在开卷考试时给了考生一本错误科目的教科书，反而会干扰模型的判断，导致回答质量下降。</p>
</li>
</ol>
<p>因此如何将更精准的信息喂给大模型，就是上下文工程（Context Engineering）。</p>
<h2 id="2-2-上下文工程">2.2. 上下文工程</h2>
<p>上下文工程的核心技术包括：</p>
<ul>
<li>
<p>RAG（检索增强生成）：从外部知识库检索信息，为模型提供精准的回答依据。</p>
</li>
<li>
<p>Prompt（提示词工程）：精心设计指令，精确引导模型的思考方式和输出格式。</p>
</li>
<li>
<p>Tool（工具使用）：赋予模型外部工具，例如实时搜索获取在线信息。</p>
</li>
<li>
<p>Memory（记忆机制）：为模型建立长短期记忆，使其可以在连续的对话中理解历史上下文。</p>
</li>
</ul>
<h1 id="3-推理大模型">3. 推理大模型</h1>
<p>推理模型相较于通用大模型多出了“<code>思考过程</code>”，就像解数学题时有人会先在草稿纸上一步步推导，而不是直接报答案，减少模型出现“拍脑袋”的错误。</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>推理模型</th>
<th>通用模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>设计目标</td>
<td>专注于<strong>逻辑推理、多步问题求解、数学计算</strong>等需要深度分析的任务</td>
<td>面向<strong>通用对话、知识问答、文本生成</strong>等广泛场景</td>
</tr>
<tr>
<td>训练数据侧重</td>
<td>大量<strong>数学题解、代码逻辑、科学推理</strong>数据集增强推理能力</td>
<td>覆盖<strong>百科、文学、对话</strong>等多领域海量数据</td>
</tr>
<tr>
<td>典型输出特征</td>
<td>输出包含<strong>完整推导</strong>步骤，注重逻辑链条的完整性</td>
<td>输出<strong>简洁直接</strong>，侧重结果的自然语言表达</td>
</tr>
<tr>
<td>响应速度</td>
<td>复杂推理任务<strong>响应较慢</strong>（需多步计算）</td>
<td>常规任务<strong>响应更快</strong>（单步生成为主）</td>
</tr>
</tbody>
</table>
<p>如何选择：</p>
<ul>
<li><strong>明确的通用任务</strong>：对于明确定义的问题，<strong>通用模型</strong>一般能够很好地处理。</li>
<li><strong>复杂任务</strong>：对于非常复杂的任务，且需要给出相对<strong>更精确和可靠</strong>的答案，推荐使用<strong>推理模型</strong>。这些任务可能有：
<ul>
<li>模糊的任务：任务相关信息很少，你无法提供模型相对明确的指引。</li>
<li>大海捞针：传递大量非结构化数据，提取最相关的信息或寻找关联/差别。</li>
<li>调试和改进代码：需要审查并进一步调试、改进大量代码。</li>
</ul>
</li>
<li><strong>速度和成本</strong>：一般来说推理模型的推理时间较长，如果你对于时间和成本敏感，且任务复杂度不高，<strong>通用模型</strong>可能是更好的选择。</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_1_%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E6%96%B0%E4%BA%BA%E7%AD%94%E7%96%91%E6%9C%BA%E5%99%A8%E4%BA%BA.ipynb">用大模型构建新人答疑机器人</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-878ad3564d3c6c133c245adba55de604">4 - RAG工程</h1>
    
	<blockquote>
<p>本文基于<a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/tree/main">GitHub - AlibabaCloudDocs/aliyun_acp_learning</a>内容整理。</p>
</blockquote>
<h1 id="1-rag-检索增强生成">1. RAG（检索增强生成）</h1>
<p><strong>RAG（Retrieval-Augmented Generation，检索增强生成）</strong> 就是实现上下文工程的强大技术方案。</p>
<p>核心思想：</p>
<ul>
<li>
<p>检索：检索与私域知识相关的知识片段。</p>
</li>
<li>
<p>增强：与用户的问题合并喂给大模型。</p>
</li>
<li>
<p>生成：生成大模型返回的回答。</p>
</li>
</ul>
<p>构建RAG主要为2个阶段：</p>
<h2 id="1-1-建立索引">1.1. 建立索引</h2>
<p>建立索引是为了将私有知识文档或片段转换为可以高效检索的形式。通过将文件内容分割并转化为多维向量（使用专用 Embedding 模型），并结合向量存储保留文本的语义信息，方便进行相似度计算。向量化使得模型能够高效检索和匹配相关内容，特别是在处理大规模知识库时，显著提高了查询的准确性和响应速度。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338374/article/linux/llm/rag_1.png" alt=""></p>
<p>建议索引主要包含以下步骤：</p>
<ol>
<li>
<p><strong>文档解析</strong>：需要将各种格式的文档（pdf, word）解析成大模型可理解的格式。</p>
</li>
<li>
<p><strong>文本分段</strong>：对解析后的文档进行分类分段，以便可以快速找到。</p>
</li>
<li>
<p><strong>文本向量化</strong>：对文本进行数字化，以便进行相似度比较和寻址。</p>
</li>
<li>
<p><strong>存储索引</strong>：将向量化后的数据存储到向量数据库，增加后续的查找速度。</p>
</li>
</ol>
<h2 id="1-2-检索与生成">1.2. 检索与生成</h2>
<p>检索生成是根据用户的提问，从索引中检索相关的文档片段，这些片段会与提问一起输入到大模型生成最终的回答。这样大模型就能够回答私有知识问题了。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338376/article/linux/llm/rag_2.png" alt=""></p>
<ol>
<li>
<p>检索：检索会把问题同向量数据库的进行相似度比较，并找出最相关的段落。</p>
</li>
<li>
<p>生成：将问题和检索的内容生成提示词喂给大模型，利用大模型的总结能力返回答案。</p>
</li>
</ol>
<h2 id="1-3-实践rag应用">1.3. 实践RAG应用</h2>
<p><strong>1、基于公司的制度文件创建RAG应用</strong>，步骤如下：</p>
<ol>
<li>
<p><strong>解析文本文件</strong></p>
</li>
<li>
<p><strong>创建索引</strong></p>
</li>
<li>
<p><strong>创建提问引擎（设置流式输出）</strong></p>
</li>
<li>
<p><strong>输入问题并输出答案</strong></p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 导入依赖</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">from</span> <span style="color:#000">llama_index.embeddings.dashscope</span> <span style="color:#204a87;font-weight:bold">import</span> <span style="color:#000">DashScopeEmbedding</span><span style="color:#000;font-weight:bold">,</span><span style="color:#000">DashScopeTextEmbeddingModels</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">from</span> <span style="color:#000">llama_index.core</span> <span style="color:#204a87;font-weight:bold">import</span> <span style="color:#000">SimpleDirectoryReader</span><span style="color:#000;font-weight:bold">,</span><span style="color:#000">VectorStoreIndex</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">from</span> <span style="color:#000">llama_index.llms.openai_like</span> <span style="color:#204a87;font-weight:bold">import</span> <span style="color:#000">OpenAILike</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 这两行代码是用于消除 WARNING 警告信息，避免干扰阅读学习，生产环境中建议根据需要来设置日志级别</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">import</span> <span style="color:#000">logging</span>
</span></span><span style="display:flex;"><span><span style="color:#000">logging</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">basicConfig</span><span style="color:#000;font-weight:bold">(</span><span style="color:#000">level</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">logging</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">ERROR</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;正在解析文件...&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># LlamaIndex提供了SimpleDirectoryReader方法，可以直接将指定文件夹中的文件加载为document对象，对应着解析过程</span>
</span></span><span style="display:flex;"><span><span style="color:#000">documents</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">SimpleDirectoryReader</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#39;./docs&#39;</span><span style="color:#000;font-weight:bold">)</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">load_data</span><span style="color:#000;font-weight:bold">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;正在创建索引...&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># from_documents方法包含切片与建立索引步骤</span>
</span></span><span style="display:flex;"><span><span style="color:#000">index</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">VectorStoreIndex</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">from_documents</span><span style="color:#000;font-weight:bold">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">documents</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8f5902;font-style:italic"># 指定embedding 模型</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">embed_model</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">DashScopeEmbedding</span><span style="color:#000;font-weight:bold">(</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8f5902;font-style:italic"># 你也可以使用阿里云提供的其它embedding模型：https://help.aliyun.com/zh/model-studio/getting-started/models#3383780daf8hw</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000">model_name</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">DashScopeTextEmbeddingModels</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">TEXT_EMBEDDING_V2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">))</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;正在创建提问引擎...&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#000">query_engine</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">index</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">as_query_engine</span><span style="color:#000;font-weight:bold">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8f5902;font-style:italic"># 设置为流式输出</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">streaming</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#204a87;font-weight:bold">True</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8f5902;font-style:italic"># 此处使用qwen-plus模型，你也可以使用阿里云提供的其它qwen的文本生成模型：https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">llm</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">OpenAILike</span><span style="color:#000;font-weight:bold">(</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000">model</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;qwen-plus&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000">api_base</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span style="color:#000;font-weight:bold">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000">api_key</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">os</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">getenv</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;DASHSCOPE_API_KEY&#34;</span><span style="color:#000;font-weight:bold">),</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000">is_chat_model</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#204a87;font-weight:bold">True</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">))</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;正在生成回复...&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#000">streaming_response</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">query_engine</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">query</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#39;我们公司项目管理应该用什么工具&#39;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;回答是：&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 采用流式输出</span>
</span></span><span style="display:flex;"><span><span style="color:#000">streaming_response</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">print_response_stream</span><span style="color:#000;font-weight:bold">()</span>
</span></span></code></pre></div><p><strong>2、RAG程序优化，保存和加载索引</strong></p>
<p>由于创建索引的时间比较长，如果提前创建索引并保存到本地，则可以提升大模型回答的速度。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 将索引保存为本地文件</span>
</span></span><span style="display:flex;"><span><span style="color:#000">index</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">storage_context</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">persist</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;knowledge_base/test&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;索引文件保存到了knowledge_base/test&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 将本地索引文件加载为索引</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">from</span> <span style="color:#000">llama_index.core</span> <span style="color:#204a87;font-weight:bold">import</span> <span style="color:#000">StorageContext</span><span style="color:#000;font-weight:bold">,</span><span style="color:#000">load_index_from_storage</span>
</span></span><span style="display:flex;"><span><span style="color:#000">storage_context</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">StorageContext</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">from_defaults</span><span style="color:#000;font-weight:bold">(</span><span style="color:#000">persist_dir</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#34;knowledge_base/test&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#000">index</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">load_index_from_storage</span><span style="color:#000;font-weight:bold">(</span><span style="color:#000">storage_context</span><span style="color:#000;font-weight:bold">,</span><span style="color:#000">embed_model</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">DashScopeEmbedding</span><span style="color:#000;font-weight:bold">(</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000">model_name</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">DashScopeTextEmbeddingModels</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">TEXT_EMBEDDING_V2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">))</span>
</span></span><span style="display:flex;"><span><span style="color:#204a87">print</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#34;成功从knowledge_base/test路径加载索引&#34;</span><span style="color:#000;font-weight:bold">)</span>
</span></span></code></pre></div><p>封装：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#204a87;font-weight:bold">from</span> <span style="color:#000">chatbot</span> <span style="color:#204a87;font-weight:bold">import</span> <span style="color:#000">rag</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8f5902;font-style:italic"># 引文在前面的步骤中已经建立了索引，因此这里可以直接加载索引。如果需要重建索引，可以增加一行代码：rag.indexing()</span>
</span></span><span style="display:flex;"><span><span style="color:#000">index</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">rag</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">load_index</span><span style="color:#000;font-weight:bold">(</span><span style="color:#000">persist_path</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#4e9a06">&#39;./knowledge_base/test&#39;</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#000">query_engine</span> <span style="color:#ce5c00;font-weight:bold">=</span> <span style="color:#000">rag</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">create_query_engine</span><span style="color:#000;font-weight:bold">(</span><span style="color:#000">index</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">index</span><span style="color:#000;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#000">rag</span><span style="color:#ce5c00;font-weight:bold">.</span><span style="color:#000">ask</span><span style="color:#000;font-weight:bold">(</span><span style="color:#4e9a06">&#39;我们公司项目管理应该用什么工具&#39;</span><span style="color:#000;font-weight:bold">,</span> <span style="color:#000">query_engine</span><span style="color:#ce5c00;font-weight:bold">=</span><span style="color:#000">query_engine</span><span style="color:#000;font-weight:bold">)</span>
</span></span></code></pre></div><h1 id="2-rag工作流程优化">2. RAG工作流程优化</h1>
<p>rag的工作流程如下：</p>
<ol>
<li>
<p>解析与切片</p>
</li>
<li>
<p>向量存储</p>
</li>
<li>
<p>检索召回</p>
</li>
<li>
<p>生成答案</p>
</li>
</ol>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757591798/article/linux/llm/rag.jpg" alt=""></p>
<p>RAG各个环节优化策略</p>
<h2 id="2-1-文档准备阶段">2.1. 文档准备阶段</h2>
<h2 id="2-2-文档解析与切片阶段">2.2. 文档解析与切片阶段</h2>
<h3 id="2-2-1-问题分类及改进策略">2.2.1. 问题分类及改进策略</h3>
<h3 id="2-2-2-借助工具解析pdf到markdown">2.2.2. 借助工具解析pdf到markdown</h3>
<h3 id="2-2-3-使用多种文档切片方法">2.2.3. 使用多种文档切片方法</h3>
<ol>
<li>
<p>Token切片</p>
</li>
<li>
<p>句子切片</p>
</li>
<li>
<p>句子窗口切片</p>
</li>
<li>
<p>语义切片</p>
</li>
<li>
<p>markdown切片</p>
</li>
</ol>
<h2 id="2-3-切片向量化与存储阶段">2.3. 切片向量化与存储阶段</h2>
<p>文档切片后需要建立索引，可以使用嵌入（Embedding）模型将切片向量化，并存储到向量数据库中。</p>
<h3 id="2-3-1-了解embedding与向量化">2.3.1. 了解Embedding与向量化</h3>
<p>Embedding模型将文本转换为高维向量，向量之间夹角越小说明相似度越高。</p>
<h3 id="2-3-2-选择合适的embedding模型">2.3.2. 选择合适的Embedding模型</h3>
<p>不同的Embedding模型对相同文字得到的向量可能完全不同，越新的Embedding模型其表现越好。在实践中，单纯升级Embedding模型就可以显著提升检索质量。</p>
<h3 id="2-3-3-选择合适的向量数据库">2.3.3. 选择合适的向量数据库</h3>
<p>向量存储方案从简单到复杂如下：</p>
<ul>
<li>
<p><code>内存向量存储</code>：优点快速上手，开发测试；缺点数据无法持久化，受限于内存大小。</p>
</li>
<li>
<p><code>本地向量数据库</code>：例如Milvus、Qdrant 等。这些数据库提供了数据持久化和高效检索能力。优点是功能完整、可控性强；缺点是需要自行部署维护。</p>
</li>
<li>
<p><code>云服务向量存储</code>：例如阿里云的向量检索服务（DashVector），向量检索服务 Milvus 版，已有数据库的向量能力。优点是按量付费，成本可控，无需运维。</p>
</li>
</ul>
<p>选择建议：</p>
<ul>
<li>
<p>开发测试选择内存向量存储</p>
</li>
<li>
<p>小规模应用选择本地向量数据库</p>
</li>
<li>
<p>生产环境使用云服务。</p>
</li>
</ul>
<h2 id="2-4-向量召回阶段">2.4. 向量召回阶段</h2>
<h3 id="2-3-1-问题改写">2.3.1. 问题改写</h3>
<ul>
<li>
<p>使用大模型扩充问题</p>
</li>
<li>
<p>将单一查询改为多步查询</p>
</li>
<li>
<p>用假设文档来增强检索</p>
</li>
</ul>
<h3 id="2-3-2-提取标签增强检索">2.3.2. 提取标签增强检索</h3>
<p>在向量检索的基础上我们可以增加标签过滤：</p>
<ol>
<li>
<p>建立索引时，从文档切片中提取结构化标签</p>
</li>
<li>
<p>检索时，从用户问题中提取对应标签进行过滤</p>
</li>
</ol>
<h3 id="2-2-3-重排序">2.2.3. 重排序</h3>
<p>从向量数据库中检索出20条文档切片，通过文本排序模型进行重新排序，删选出最相关的3条信息。</p>
<h2 id="2-5-生成答案阶段">2.5. 生成答案阶段</h2>
<p>大模型生成的答案不及预期，可以通过以下方式解决：</p>
<ol>
<li>
<p>选择合适的大模型</p>
</li>
<li>
<p>充分优化提示词模板：明确要求不编造答案；添加内容分隔标记；根据问题类型调整模板。</p>
</li>
<li>
<p>调整大模型参数</p>
</li>
<li>
<p>调优大模型</p>
</li>
</ol>
<p>参考：</p>
<ul>
<li>
<p><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_2_%E6%89%A9%E5%B1%95%E7%AD%94%E7%96%91%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E7%9F%A5%E8%AF%86%E8%8C%83%E5%9B%B4.ipynb">扩展答疑机器人的知识范围</a></p>
</li>
<li>
<p><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_5_%E4%BC%98%E5%8C%96RAG%E5%BA%94%E7%94%A8%E6%8F%90%E5%8D%87%E9%97%AE%E7%AD%94%E5%87%86%E7%A1%AE%E5%BA%A6.ipynb">优化RAG应用提升问答准确度</a></p>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7b65909556914234bb5eaf029e28201c">5 - 大模型微调</h1>
    
	<blockquote>
<p>本文基于<a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/tree/main">GitHub - AlibabaCloudDocs/aliyun_acp_learning</a>内容整理。</p>
</blockquote>
<h1 id="1-模型如何学习">1. 模型如何学习</h1>
<h2 id="1-1-机器学习-通过数据寻找规律">1.1. 机器学习-通过数据寻找规律</h2>
<p>假设有个规则编写成函数的形式，例如：f(x)=ax。机器学习的目标就是帮助你通过数据（训练集）来尝试找到 a 这些参数值，这一过程被称为训练模型。</p>
<h2 id="1-2-loss-function-cost-function-量化评估模型表现">1.2. Loss Function &amp; Cost Function - 量化评估模型表现</h2>
<p>为了评估f(x)=ax中的a是否合适，我们可以通过损失函数和代价函数来判断。</p>
<h3 id="1-2-1-loss-function-损失函数">1.2.1. Loss Function 损失函数</h3>
<p>你可以用训练集的每一个样本$x_i$对应的实际结果值 $y_i$，与模型预测结果值 $f(x_i)$相减，来评估模型在$x_i，y_i$这一条数据上的表现。这个评估误差的函数被称为 Loss Function（损失函数，或误差函数）：$L(y_i, f(x_i)) =  y_i - ax_i$。因为差值有正负，因此我们通过差值的平方的方式来计算损失函数，$L(y_i, f(x_i)) = (y_i - ax_i)^2$。同时，平方值能够放大误差的影响，有利于你找到最合适的模型参数。</p>
<blockquote>
<p>在实际应用中，对于不同的模型，可能会选择不同的计算方法来作为 Loss Function。</p>
</blockquote>
<h3 id="1-2-2-cost-function-代价函数">1.2.2. Cost Function 代价函数</h3>
<p>为了评估模型在整个训练集上的表现，你可以计算所有样本的损失平均值（即均方误差，Mean Squared Error，MSE）。这种用于评估模型在所有训练样本上的整体表现的函数，被称为 Cost Function（代价函数，或成本函数）。</p>
<p>对于包含 m 个样本的训练集，代价函数可以表示为：$J(a) = \frac{1}{m} \sum_{i=1}^{m} (y_i - ax_i)^2$。</p>
<blockquote>
<p>在实际应用中，对于不同的模型，也可能会选择不同的计算方法来作为 Cost Function。</p>
</blockquote>
<p>有了 Cost Function，寻找模型合适的参数的任务，就可以等效为寻找 Cost Function 最小值（即最优解）的任务。找到 Cost Function 的最小值，意味着该位置的参数 a 取值，就是最合适的模型参数取值。</p>
<blockquote>
<p>实际工作中，我们将代价函数和损失函数统一称为损失函数。</p>
</blockquote>
<h2 id="1-3-梯度下降法">1.3. 梯度下降法</h2>
<p>一种常见的梯度下降算法实现是，先在曲面（或曲线）上随机选择一个起点，然后通过不断小幅度调整参数，最终找到最低点（对应最优参数配置）。</p>
<div style="text-align: center">
<img title="" src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338700/article/linux/llm/llm_upgrade1.gif" alt="" style="width: 400px;margin-left: auto; margin-right: auto" width="386">
<img title="" src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338719/article/linux/llm/llm_upgrade2.gif" alt="" style="width: 400px;margin-left: auto; margin-right: auto" width="387">
</div>
<p>训练模型时，你需要训练程序能自动地不断调整参数，最终让 Cost Function 的值逼近最低点。所以梯度下降算法，需要能自动地控制两点：</p>
<ul>
<li>
<p>调整参数的方向</p>
</li>
<li>
<p>调整参数的幅度</p>
</li>
</ul>
<h3 id="1-3-1-调整参数的方向">1.3.1. 调整参数的方向</h3>
<h3 id="1-3-2-调整参数的幅度">1.3.2. 调整参数的幅度</h3>
<p>如果按照固定的步长调整参数，那么可能导致在最低点附近反复震荡，无法逼近最低点。越接近最低点，斜率越小，因此可以使用当前位置的斜率作为调整的幅度。针对损失函数斜率非常陡峭，也可能反复在最低点震荡，因此可以将斜率再乘以一个系数，我们称之为<code>学习率（Learning rate）</code>。</p>
<ul>
<li>
<p><strong>过低的学习率，会导致找到合适的参数速度变慢，消耗更多的资源和时间。</strong></p>
</li>
<li>
<p><strong>过高的学习率，会导致跳过最优解，最终找不到最低点。</strong></p>
</li>
</ul>
<h2 id="1-4-模型训练用的参数">1.4. 模型训练用的参数</h2>
<h4 id="batch-size">batch size</h4>
<p>较大的batch size会加速训练过程，但对资源消耗更大，过大的batch size可能导致模型泛化性能下降的问题</p>
<h4 id="eval-steps">eval steps</h4>
<p>因为训练集数量大，因此一般不会对训练集完整迭代后进行评估，而是间隔多少个训练步骤后评估，这个间隔步骤称为eval_steps。</p>
<h4 id="epoch">epoch</h4>
<p>对训练集一个完整的迭代称为epoch，</p>
<ul>
<li>
<p>过小的epoch值会导致训练结束还没找到最优参数。</p>
</li>
<li>
<p>过大的epoch值会导致训练时间长以及资源浪费。</p>
</li>
</ul>
<p>寻找合适epoch常用早停法（early stopping）：在训练过程中定期评估模型表现，当模型不再提升则自动停止训练。</p>
<h1 id="2-高效微调技术">2. 高效微调技术</h1>
<h2 id="2-1-预训练和微调">2.1. 预训练和微调</h2>
<p>模型训练的本质就是寻找最合适的参数组合。最开始下载好的模型就是预训练好的参数组合。微调则是在此基数上进一步调整参数，以适应你的目标任务。</p>
<p>预训练与微调的主要区别</p>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>预训练</strong></th>
<th><strong>微调</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>目标</td>
<td>$ $ 学习通用特征</td>
<td>适应特定任务</td>
</tr>
<tr>
<td>数据</td>
<td>大规模通用数据</td>
<td>小规模任务相关数据</td>
</tr>
<tr>
<td>训练方法</td>
<td>自监督/无监督</td>
<td>有监督</td>
</tr>
<tr>
<td>参数更新</td>
<td>所有参数可训练</td>
<td>部分或全部参数可训练</td>
</tr>
<tr>
<td>应用场景</td>
<td>基础模型构建</td>
<td>特定任务优化</td>
</tr>
</tbody>
</table>
<h2 id="2-2-lora微调">2.2. LoRA微调</h2>
<p>LoRA（Low-Rank Adaptation）低秩适应微调是现在最常用的微调方法。</p>
<h1 id="3-微调实战">3. 微调实战</h1>
<p>训练的三套题目：</p>
<ul>
<li>
<p><code>训练集</code>：课程练习题，<code>训练损失</code>越小说明模型在练习册上表现更好。</p>
</li>
<li>
<p><code>验证集</code>：模拟考试题，<code>验证损失</code>越小说明在模拟考试中表现越好。</p>
</li>
<li>
<p><code>测试题</code>：考试真题，模型在测试集上的准确率用于评估模型最终表现。</p>
</li>
</ul>
<p>训练的三个状态：</p>
<ul>
<li>
<p>训练损失不变，甚至变大：说明训练失败。</p>
</li>
<li>
<p>训练损失和验证损失都在下降：说明模型欠拟合，模型学习不充分，在训练集上表现不好。</p>
</li>
<li>
<p>训练损失下降但验证损失上升：说明模型过拟合，模型在训练集表现好（背题），在新数据表现差。</p>
</li>
</ul>
<p>如何调参：</p>
<ol>
<li>
<p>训练损失和验证损失增大，说明训练失败，则调低学习率，慢慢学习。</p>
</li>
<li>
<p>训练欠拟合，说明学习不够，增加学习批大小（batch_size），增加学习次数（epochs）。</p>
</li>
<li>
<p>训练过拟合，说明在背题，降低学习次数（epoch）,增加LoRA的秩。</p>
</li>
<li>
<p>训练成功：训练损失基本不减小，验证损失也不减小甚至还略微升高。</p>
</li>
</ol>
<p>参数：</p>
<ul>
<li>
<p>学习率（learning_rate）：训练失败，调低学习率。</p>
</li>
<li>
<p>LoRA的秩（lora_rank）:训练过拟合，增大lora_rank。</p>
</li>
<li>
<p>数据集学习次数（num_tarin_epochs）:训练欠拟合，增加学习次数。</p>
</li>
<li>
<p>batch_size：训练欠拟合，说明学习不够，增加学习批大小。</p>
</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_7_%E9%80%9A%E8%BF%87%E5%BE%AE%E8%B0%83%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E4%B8%8E%E6%95%88%E7%8E%87.ipynb">通过微调提升模型的准确度与效率</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4fdff5e5273cc1b3e9dfe04c52604d40">6 - 大模型agent概述</h1>
    
	<h1 id="1-为什么需要agent">1. 为什么需要agent</h1>
<p>如果希望答疑机器人能具备这样一种功能：只需说出“帮我请明天的假”，机器人便能自动提交请假申请单。那么就需要让大模型理解用户的意图，并且可以调用相应的API来实现。<strong>通过任务分解和自动化执行，就可以高效的完成复杂的动作，即智能体（Agent）。</strong></p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338374/article/linux/llm/agent.webp" alt=""></p>
<h1 id="2-如何构建agent">2. 如何构建agent</h1>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338373/article/linux/llm/agent2.webp" alt=""></p>
<p>构建一个agent的步骤：</p>
<ol>
<li>
<p>明确目标</p>
</li>
<li>
<p>定义工具函数</p>
</li>
<li>
<p>将工具函数集成到agent</p>
</li>
<li>
<p>尝试对话</p>
</li>
</ol>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338376/article/linux/llm/agent3.png" alt=""></p>
<p>智能体的工作原理核心模块：</p>
<ul>
<li>
<p><strong>工具模块</strong></p>
<p>工具模块负责定义和管理智能体能够使用的各种工具。包括工具的描述、参数以及功能特性。这一模块确保智能体能够理解并有效使用这些工具来完成任务。</p>
</li>
<li>
<p><strong>记忆模块</strong></p>
<p>记忆模块可以分为长期记忆和短期记忆。</p>
<p>长期记忆用于存储持久的信息和经验，帮助智能体进行模式学习、知识积累和个性化服务。</p>
<p>短期记忆则用于临时存储当前任务相关的信息，以支持智能体在任务执行过程中实时调整决策。</p>
</li>
<li>
<p><strong>计划能力</strong></p>
<p>计划能力模块负责任务的规划。通过智能体的决策能力，这部分帮助智能体分解复杂任务，制定具体的行动步骤和策略，确保任务顺利完成。</p>
</li>
<li>
<p><strong>行动能力</strong></p>
<p>行动能力与工具模块紧密配合，确保智能体能够选择合适的工具，并通过容器执行相应的操作。行动能力是智能体实现任务的核心，确保其能够根据既定计划和决策，有效地实施各项任务。</p>
</li>
</ul>
<h1 id="3-大模型意图识别">3. 大模型意图识别</h1>
<blockquote>
<p>待补充</p>
</blockquote>
<h1 id="4-多智能体multi-agent">4. 多智能体Multi-Agent</h1>
<p>多智能体系统通过将任务拆解成多个子任务，并由不同的智能体分别处理这些任务，从而克服了单一智能体无法同时完成多个操作的局限性。每个智能体专注于一个特定任务，像一个团队中的成员，各司其职，最终协作完成整个任务。</p>
<h2 id="4-1-如何设计实现多智能体">4.1. 如何设计实现多智能体</h2>
<p>Multi-Agent系统有多种设计思路，本教程将介绍一个由Planner Agent、若干个负责执行工具函数的Agent，以及一个Summary Agent组成的Multi-Agent系统。</p>
<ul>
<li><strong>Planner Agent规划智能体</strong>： 根据用户的输入内容，选择要将任务分发给哪个Agent或Agent组合完成任务。</li>
<li><strong>执行工具函数的Agent智能体</strong>： 根据Planner Agent分发的任务，执行属于自己的工具函数。</li>
<li><strong>Summary Agent总结智能体</strong>： 根据用户的输入，以及执行工具函数的Agent的输出，生成总结并返回给用户。</li>
</ul>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757338374/article/linux/llm/agent4.png" alt=""></p>
<h1 id="5-多智能体编排">5. 多智能体编排</h1>
<p>在自主构建一个Multi-Agent系统时，虽然能够提供高度的灵活性，但也伴随着一定的工作量和复杂性。对于许多企业来说，快速实现复杂业务逻辑更为重要。因此可以使用智能体流程编排工具，例如：<a href="https://dify.ai/">Dify</a>，<a href="https://help.aliyun.com/zh/model-studio/user-guide/single-agent-application">百炼智能体应用</a>。</p>
<p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1757594989/article/linux/llm/multi-agent.png" alt=""></p>
<p>参考：</p>
<ul>
<li><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_6_%E7%94%A8%E6%8F%92%E4%BB%B6%E6%89%A9%E5%B1%95%E7%AD%94%E7%96%91%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%83%BD%E5%8A%9B%E8%BE%B9%E7%95%8C.ipynb">用插件扩展答疑机器人的能力边界</a></li>
</ul>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/huweihuang/blog.huweihuang.com" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2025 www.huweihuang.com 保留所有权利</small>
        <small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">隐私政策</a></small>
	
		<p class="mt-2"><a href="/about/">About</a></p>
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"
    integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN"
    crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js"
    integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA=="
    crossorigin="anonymous"></script>





<script src='/js/tabpane-persist.js'></script>




















<script src="/js/main.min.91798a335c881f1b6b805085ba4aa22d1dbd2b0b18d105d05189fa104ddae350.js" integrity="sha256-kXmKM1yIHxtrgFCFukqiLR29KwsY0QXQUYn6EE3a41A=" crossorigin="anonymous"></script>



<script src='/js/prism.js'></script>



  </body>
</html>
