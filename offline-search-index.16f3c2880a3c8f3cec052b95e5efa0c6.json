[{"body":"假设在 Kubernetes 集群中一次性调度 1 万个 Pod， 这是一项极具挑战性的任务。如果管理不当，可能会导致调度器瓶颈、API Server 过载，甚至整个集群崩溃。\n本文将探讨优化大规模 Pod 调度的最佳实践与技术手段。\n🚀 面临的挑战 调度器压力大：大量 Pod 同时进入 Pending 状态，调度器处理不过来。 API Server 压力大：高频的 CREATE/GET/LIST 请求可能触发限流。 etcd 延迟增加：写入及状态变化频繁，导致存储后端压力过大。 节点压力不均衡：调度不均可导致部分节点 CPU/内存/磁盘 IO 资源打爆。 网络插件瓶颈：CNI 插件无法处理大量并发的 IP 分配。 1. 调度器优化 🔧 1.1. 控制 Pod 创建速率 不要一次性启动 10,000 个 Pod，而是：\n分批创建：例如每批创建 500–1000 个 Pod。 控制速率：通过脚本或 Job 控制器引入 sleep 等时间间隔。 示例 Shell 脚本： for file in batches/batch_*.yaml; do kubectl apply -f \"$file\" sleep 10 done ⚙️ 1.2. 调优默认调度器 默认调度器（kube-scheduler）在高并发场景下可能成为瓶颈。为了高效调度 1 万个以上的 Pod，可以从以下几个方面进行深入调优：\n✅ 1. 提高并发调度能力 Kubernetes v1.19+ 支持配置并行调度线程数：\napiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler parallelism: 64 推荐值为 CPU 核心数的 2~4 倍（视调度密集度而定）。\n注意：并发过高可能导致内存激增或 etcd 压力过大，建议结合压测评估。\n✅ 2. 启用缓存调度器（Scheduling Queue 优化） 调度器内部维护了 Pending Pod 的优先队列（PriorityQueue）与 Node 信息缓存。\n确保使用优先级调度（PodPriority）可帮助调度器优先处理重要任务。\n配置调度器时可启用 permit 插件阶段，在调度决策前提前控制调度流量。\n✅ 3. 关闭或精简耗时插件 某些默认启用的插件在调度高峰时会带来性能负担：\n插件 类型 说明 VolumeBinding Bind 持久化卷绑定，需访问 API NodeResourcesFit Filter 检查资源是否满足 InterPodAffinity Filter Pod 之间亲和性计算复杂 ⚠️ 优化建议：\n无状态服务建议 关闭 VolumeBinding 插件。\n只使用必要的 Score 插件（如 LeastAllocated、BalancedAllocation）。\n配置方式：\nplugins: score: disabled: - name: \"NodeResourcesBalancedAllocation\" enabled: - name: \"NodeResourcesLeastAllocated\" ✅ 4. 预选节点集范围（节点剪枝） 调度器默认会评估所有可调度节点，1 万 Pod × 1 千节点的组合极其耗时。\n优化方法：\nNodeAffinity：提前通过标签筛掉不符合的节点。\n使用 preFilter 插件 自定义节点集合。\n示例：\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/worker operator: In values: - batch ✅ 5. 启用拓扑感知与亲和性缓存 使用拓扑调度建议：\ntopologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway ✅ 6. 控制调度队列压力（Backoff \u0026 Retry） Pod 多次调度失败会进入 backoff 队列，默认退避时间为：\nInitialBackoff = 1 * time.Second MaxBackoff = 10 * time.Second 调大 MaxBackoff 可减缓高频重试对调度器的压力。\n🧪 调优效果验证建议： 使用 --v=5 级别运行 kube-scheduler，输出调度详细日志。\n观察调度延迟指标（SchedulingLatencySeconds）：\nframework_extension_point_duration_seconds\nscheduler_scheduling_duration_seconds_bucket\n🧩 1.3. 扩展调度器（Scheduling Framework 插件） Kubernetes 支持通过调度框架插件机制自定义调度逻辑。\n✨ 示例：快速 Filter 插件 自定义过滤插件，可只评估部分节点，从而减少调度延迟：\nfunc (f *FastFilterPlugin) Filter(...) *framework.Status { if strings.HasPrefix(node.Name, \"compute-node-\") { return framework.NewStatus(framework.Success) } return framework.NewStatus(framework.Unschedulable) } 🧠 1.4. 使用多个调度器（调度器隔离） 并行部署多个调度器进程：\nspec: schedulerName: batch-scheduler 每类工作负载使用不同调度器进行处理，实现并行调度和资源隔离。\n🛠 1.5. 使用高性能调度系统 Koordinator 支持批量调度、NUMA 感知、QoS 资源分级 与 Kubernetes 调度框架兼容，部署简单 Volcano 面向大规模批处理、AI/ML、HPC 任务调度 支持抢占、任务优先级、依赖关系等 📊 1.6. 监控与验证建议 使用 kubectl get pods -w 实时观察 Pending 状态 关注调度事件 FailedScheduling 跟踪 API Server 指标：QPS、延迟、内存占用 部署 Prometheus + Grafana 进行系统监控与可视化 ✅ 1.7. 总结对比 优化方式 效果 控制 Pod 创建速率 避免控制面组件过载 提高调度器并发度 提升每秒调度吞吐 编写调度器插件 降低单次调度复杂度 多调度器架构 实现任务隔离与并行调度 使用 Koordinator/Volcano 面向 AI/批处理等高负载场景 2. Etcd优化 etcd 是 Kubernetes 控制平面的核心存储引擎，一旦在大规模 Pod 创建、调度过程中出现 写入延迟增加，会直接影响 API Server 性能，进而拖慢调度器和控制器反应速度，甚至引发集群不可用。\n🔧 2.1. 基础配置优化 ✅ 1. 启用自动压缩历史数据 etcd 默认会保留历史版本，随着对象变化增多，存储膨胀，导致延迟升高。\n--auto-compaction-retention=1h # 每小时清理历史 --snapshot-count=10000 # 控制何时触发快照 ✅ 2. 启用 WAL 压缩 压缩 Write-Ahead Log，减少磁盘 I/O 开销：\n--experimental-initial-corrupt-check=true --experimental-compact-hash-check-enabled=true 💽 2.2. 硬件层优化（非常关键） etcd 对 磁盘 IOPS 和延迟 敏感，推荐：\n使用 SSD（NVMe 最佳）\netcd 独立磁盘，避免和 kubelet 或 container runtime 共用\n提升内存（建议 16G+）、CPU 性能（至少 4 核）\n开启 NUMA 亲和配置，减少跨核调度\n🧱 2.3. 集群部署架构优化 ✅ 1. 隔离部署 etcd etcd不要与 kube-apiserver、controller-manager 等组件共节点运行。\n1）etcd 对磁盘 IO、内存和 CPU 的性能非常敏感，特别是磁盘延迟对 etcd 性能和稳定性有显著影响。\nkube-apiserver、controller-manager、scheduler 等组件也会频繁访问 etcd，产生较大 CPU 和内存负载。\n如果它们部署在同一节点，容易导致 资源竞争（尤其是 IO），影响 etcd 的响应能力和稳定性，进而影响整个集群的控制面。\n2） Kubernetes 通常运行在高速局域网内，访问 etcd 的延迟很小\n网络延迟在现代数据中心或云环境中通常是微秒到毫秒级别。\n只要保证 etcd 集群网络稳定，控制面组件即使不在同一个节点也能快速访问。\n3） 不在一个节点，可以避免“局部高负载”导致连锁影响\n如果 kube-apiserver 跟 etcd 同节点，一旦 kube-apiserver 突发流量（比如创建大量资源），会导致 etcd 所在节点资源被占满，从而影响 etcd 响应。\n反之亦然，etcd 的 GC 或 compaction 操作也可能影响 apiserver 的性能。\n✅ 2. 多副本部署（3~5个节点） 避免单点瓶颈，并启用高可用。\n3. kube-apiserver优化 在大量 Pod 同时调度时，API Server 压力大 是造成集群卡顿或异常的核心瓶颈之一，主要表现为：\n创建、更新、查询 Pod 等请求响应变慢\nkubelet、controller-manager 与 API Server 通信超时\netcd QPS 激增、延迟升高，甚至触发熔断\n以下是具体的优化策略，从集群参数、限流、组件解耦等多个层面展开：\n🧱 3.1. 控制请求速率（限流） ✅ 1. 控制客户端创建速率 比如大量 Job/Deployment 控制器、脚本同时发出 kubectl apply 请求：\n解决方法：\n采用 kubectl --wait=false 异步创建\n使用分批 apply 或 sleep 控制速率\n使用 controller（例如自定义 CRD + controller）分批分组管理 pod/job\n⚙️ 3.2. 调优 API Server 参数 在 kube-apiserver 启动参数中：\n✅ 1. 增加最大并发 QPS --max-requests-inflight=4000 # 默认 400，增加吞吐能力 --max-mutating-requests-inflight=2000 # 默认 200，调大写请求容量 ✅ 2. 增加缓存时间与响应窗口 --request-timeout=1m --min-request-timeout=300 3.3. operator优化 如果有开发自定义的operator，则需要对operator的逻辑进行优化。\n✅ 1. 使用 informer 缓存机制（client-go 默认支持） 自定义控制器或调度插件中应使用共享缓存，而非频繁 GET：\ninformer := factory.Core().V1().Pods().Informer() ✅ 2. 减少不必要的 Watch 或频繁 List 请求 调度器插件中不要频繁访问 Pod 列表\n减少 metrics 或审计日志系统对 API 的高频采集\n4. 网络插件优化 Kubernetes 网络插件（CNI）在大规模部署或高并发场景下，若处理能力跟不上，会出现 调度成功但网络不通、服务连接慢、跨节点通信异常 等问题。\n🧭 4.1. 网络瓶颈表现 现象 可能原因 Pod 创建卡住在 ContainerCreating CNI 插件调用超时，网络设备未初始化 跨节点服务访问慢或超时 网络插件转发路径性能不足，iptables/ebpf 累积 集群中 ping 某些 Pod 慢 某些节点流量瓶颈，或者 VXLAN 隧道高延迟 kube-proxy CPU 占满 iptables 规则过多或频繁变更 ✅ 4.2. 选型优化：选择高性能 CNI 插件 插件 性能特点 Cilium eBPF 驱动，无需 iptables，极高性能，支持大规模节点 Calico (BPF 模式) 支持 eBPF 模式，性能更好于传统 iptables Flannel 适用于小规模集群，性能普通，不推荐大集群使用 Multus 支持多网卡/多 CNI，适合边缘场景但调试复杂 💡 推荐使用 Cilium 或 Calico（eBPF 模式），避免使用传统 Flannel/VXLAN。\n⚙️ 4.3. 网络插件参数调优 🔹 Cilium 示例 配置 /etc/cilium/cilium-config：\nenable-bpf-masquerade: \"true\" enable-ipv4-masquerade: \"false\" bpf-lb-map-max: \"65536\" bpf-ct-global-tcp-max: \"524288\" bpf-ct-global-any-max: \"262144\" 并启用 kube-proxy 替代模式（kube-proxy-free）：\nkubeProxyReplacement: \"strict\" 🔄 4.4. 跨节点通信优化 ✅ 1. 减少 VXLAN 封装（或改为 Native Routing） Flannel/Calico VXLAN 模式性能差\n推荐切换为 Calico 的 BGP 模式（路由直达，无封装）\n✅ 2. 使用 Direct Server Return（DSR）+ ECMP 路由 大流量服务部署时，避免中心化转发。\n🔃 4.5. kube-proxy 优化 ✅ 1. 使用 ipvs 模式代替 iptables --proxy-mode=ipvs --ipvs-scheduler=rr 相比 iptables，ipvs 处理服务转发在大规模集群下 CPU 更省、延迟更低。\n✅ 2. 配合 eBPF 替代 kube-proxy（Cilium 推荐） Cilium 的 kube-proxy-replacement=strict 直接使用 BPF 加速服务调度。\n🔧 4.6. 节点系统参数优化 设置节点的内核参数，提升大流量下系统处理能力：\n# 提高 conntrack 表容量 sysctl -w net.netfilter.nf_conntrack_max=262144 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=86400 # 允许更多文件描述符 ulimit -n 1048576 # 调高队列长度 sysctl -w net.core.somaxconn=1024 sysctl -w net.core.netdev_max_backlog=250000 📊 4.7. 监控关键指标 通过 Cilium/Calico 自带 metrics 或 Prometheus 采集：\n指标 说明 cilium_forwarding_latency_seconds 转发延迟 cilium_drop_count_total 数据包被丢弃的原因 iptables_rule_count kube-proxy 中规则数量 conntrack_entries 当前连接跟踪表大小 ✅ 4.8. 总结优化建议表 方向 方案 插件选型 使用 Cilium/Calico eBPF，避免 Flannel 插件配置 优化转发表、连接跟踪表大小 网络架构 BGP 替代 VXLAN，开启 kube-proxy-free 系统内核 调高 conntrack / backlog 等参数 转发模式 使用 ipvs 或 eBPF 加速 kube-proxy 监控排查 开启 drop 分析、BPF 路径追踪 5. 总结 大规模 Pod 调度不仅仅是追求速度，更重要的是在高压下保持系统的稳定性与正确性。需要从各个方面进行集群优化才能承受大规模pod集群的性能压力。本文分别从以下几个方面进行优化：\n调度器及扩展调度器\nETCD优化\nkube-apiserver优化\n网络插件及节点优化\n只要设计合理，Kubernetes 完全可以稳定高效地调度数万个 Pod。\n","categories":"","description":"","excerpt":"假设在 Kubernetes 集群中一次性调度 1 万个 Pod， 这是一项极具挑战性的任务。如果管理不当，可能会导致调度器瓶颈、API …","ref":"/kubernetes-notes/cluster-optimization/massive-pod-scheduling/","tags":["Kubernetes"],"title":"大规模Pod调度优化"},{"body":"1. Flannel简介 Flannel 是一个简单的、易于使用的 Kubernetes 网络插件，用于为容器集群提供网络功能。它主要解决的是 Kubernetes 集群中跨节点容器间通信的问题，通过为每个节点分配一个独立的子网，确保容器之间可以使用虚拟网络进行无障碍通信。\n1.1. Flannel 的特点与优势 易于配置和使用 提供简单的配置文件，易于集成到 Kubernetes 集群中。 支持多种后端（如 VXLAN、host-gw、AWS VPC 等），灵活满足不同环境需求。 跨节点网络通信 为每个节点分配独立的子网，容器之间使用虚拟网络 IP 直接通信，而无需 NAT。 轻量级设计 运行时资源占用少，适合资源有限的环境。 稳定兼容性强 支持多种 Linux 发行版，兼容 Kubernetes 和 Docker，适应广泛的容器化场景。 多后端支持 提供 VXLAN、host-gw、AWS VPC、UDP 等多种网络后端，以适应不同场景和需求。 1.2. 使用场景 中小规模 Kubernetes 集群 Flannel 易于部署和管理，非常适合中小规模集群使用。 跨节点容器通信 在需要容器间无障碍通信的场景中，Flannel 提供可靠的虚拟网络支持。 非高性能敏感的场景 由于 Flannel 使用封包封装技术（如 VXLAN），在性能要求不是特别高的场景中非常适用。 混合云/多云部署 Flannel 的多后端支持和灵活配置，使其在多种基础设施中易于部署。 1.3. Flannel 的局限性 尽管 Flannel 易用且轻量，但它也存在一些不足之处：\n性能限制 使用 VXLAN 或 UDP 后端时，由于封包和解封包操作会消耗额外资源，网络性能可能不如直接路由的方案（如 Calico 的 BGP）。 在高流量或低延迟场景下，Flannel 可能不是最佳选择。 缺乏高级网络功能 不支持网络策略（Network Policy）功能，无法实现细粒度的访问控制。 对于需要复杂网络功能（如流量加密、多租户隔离）的场景，Calico 或 Cilium 是更好的选择。 依赖 etcd Flannel 强依赖于 etcd。如果 etcd 出现故障，可能影响网络管理和子网分配。 需要额外注意 etcd 的高可用性和性能。 运维复杂度随着规模增长 随着集群规模扩大（如 1000+ 节点），Flannel 的资源消耗和配置复杂度可能增加，不如更高性能的网络方案。 2. Flannel 的架构与配置 flannel的架构比较简单，只有每个节点一个的flanneld组件，通过daemonset部署，并没有跟calico或cilium的架构中有中控组件。其他组件则使用k8s的etcd。\nflanneld 组件 每个节点运行一个 flanneld 服务，负责管理该节点的网络配置和数据封包解封包。 etcd 集成 Flannel 使用 etcd 存储网络配置和子网分配信息。 所有节点通过 etcd 协调分配网络资源。 网络后端 Flannel 支持多种后端技术，如 VXLAN、UDP、host-gw 等，可根据需求选择。 Kubernetes 集成 Flannel 通过 Kubernetes 的 CNI 插件接口无缝集成，确保与 Kubernetes 网络需求的兼容性。 2.1. 部署flannel 通过以下的yaml文件可以快速的部署flannel组件\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 部署完成后会生成默认配置：kube-flannel-cfg， 其中默认使用vxlan的后端模式。\napiVersion: v1 kind: ConfigMap data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" // 默认为vxlan的模式 } } 2.2. 节点配置 在/etc/cni/net.d路径下会生成flannel的cni配置。\n{ \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } 同时在节点会生成一个子网配置/var/run/flannel/subnet.env\nFLANNEL_NETWORK=10.244.0.0/16 # 整个集群的Pod网段 FLANNEL_SUBNET=10.244.3.1/24 # 该节点的子网Pod网段 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true 节点会生成cni0和flannel.1的网卡，其中网卡的网段跟该节点的FLANNEL_SUBNET网段一致，如果不一致则需要重建网卡。\nflannel.1：节点网关，10.244.3.0 cni0: 10.244.3.1 FLANNEL_SUBNET=10.244.3.1/24 cni0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1450 inet 10.244.3.1 netmask 255.255.255.0 broadcast 10.244.3.255 inet6 fe80::828:abff:fe83:34ac prefixlen 64 scopeid 0x20\u003clink\u003e ether 0a:28:ab:83:34:ac txqueuelen 1000 (Ethernet) RX packets 16220840959 bytes 2329280828193 (2.3 TB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 16382181068 bytes 43297103465563 (43.2 TB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 flannel.1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1450 inet 10.244.3.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::d875:a3ff:fe8b:1e64 prefixlen 64 scopeid 0x20\u003clink\u003e ether da:75:a3:8b:1e:64 txqueuelen 0 (Ethernet) RX packets 225837271 bytes 33216374045 (33.2 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 204036495 bytes 396891087255 (396.8 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 3. Flannel网络原理 3.1. 原理图 图中docker0等价于cni0, flannel0等价于flannel.1网卡\n关键网卡和路由表的角色\ncni0 网桥 是一个 Linux 网桥，连接同一节点上的所有 Pod 网络接口（veth 对）。 将数据包从 Pod 转发到本地的其他 Pod 或上交给 flannel.1。 flannel.1 网卡 是一个虚拟网卡（VXLAN 接口），用于封装和解封装跨节点的 Pod 数据包。 连接到物理网络，通过封装的方式将数据包发送到目标节点。 路由表 定义了如何转发数据包，包括 Pod 子网的路由规则和默认路由。 Flannel 会在每个节点配置路由表，使得本地 Pod 的子网可以通过 cni0 访问，远程 Pod 的子网通过 flannel.1 访问。 3.2. 数据包路径 3.2.1. 数据包环境 假设有以下环境：\nNode A：子网 10.244.1.0/24，Pod1 的 IP 是 10.244.1.2。 Node B：子网 10.244.2.0/24，Pod2 的 IP 是 10.244.2.3。 两个节点通过物理网络互联。 其中节点路由表信息如下：\nA 节点（Pod IP：10.244.1.2）：\n# 物理机路由 default via \u003c物理机网关\u003e dev bond0 proto static \u003c物理机目标网段\u003e dev bond0 proto kernel scope link src \u003c本机节点IP\u003e # flannel路由 10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink 10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1 #本地子网，直接通过 cni0 处理。 10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink # 远程子网，数据包通过 flannel.1 封装 B节点（Pod IP：10.244.2.3）：\n# 物理机路由 default via \u003c物理机网关\u003e dev bond0 proto static \u003c物理机目标网段\u003e dev bond0 proto kernel scope link src \u003c本机节点IP\u003e # flannel路由 10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink # 远程子网，数据包通过 flannel.1 封装 10.244.2.0/24 dev cni0 proto kernel scope link src 10.244.2.1 #本地子网，直接通过 cni0 处理。 3.2.1. 数据路径 1. 从 Pod 发出的数据包\n(1) Pod1 发送数据包\nPod1 发往 Pod2 的数据包： 源 IP：10.244.1.2 目标 IP：10.244.2.3 数据包通过 Pod1 的 veth 设备发送到本地的 cni0 网桥。 2. cni0 网桥处理\n判断目标 IP 属于哪个子网： 本节点子网（10.244.1.0/24）：直接转发到对应的 veth。 其他子网（10.244.2.0/24）：路由表指向 flannel.1。 在本例中，目标 IP 属于 10.244.2.0/24，因此数据包通过路由规则转发到 flannel.1。 3. flannel.1 网卡封装\n(1) 数据封装\nFlannel 代理（flanneld）会检测目标子网属于远程节点，触发封装流程。 数据包封装为 VXLAN 包，外层 IP 标头： 源 IP：Node A 的物理 IP（例如 192.168.1.1）。 目标 IP：Node B 的物理 IP（例如 192.168.1.2）。 VXLAN Header：标记虚拟网络 ID 和其他信息。 (2) 路由转发\n封装后的数据包通过主机的物理网卡（如 eth0）发送到目标节点。 4. 到达目标节点 (Node B)\n(1) flannel.1 接收数据包\nNode B 的物理网卡接收封装的 VXLAN 数据包，交由 flannel.1。 flannel.1 解封数据包，还原出原始的 Pod 数据包： 源 IP：10.244.1.2 目标 IP：10.244.2.3 (2) 路由到 cni0\n根据路由表，目标子网 10.244.2.0/24 属于本节点，通过 cni0 转发数据包。 cni0 根据目标 IP，找到 Pod2 的 veth 设备。 5. 数据包到达目标 Pod\n数据包最终通过 cni0 网桥送到 Pod2 的 veth 接口，Pod2 接收到来自 Pod1 的通信。 3.2.3. 数据流总结 Pod1 数据包先进入本地的 cni0 网桥。 cni0 网桥通过路由表，发现目标 IP 属于其他子网，交由 flannel.1。 flannel.1 封装数据包，并通过物理网卡发往目标节点。 目标节点的 flannel.1 解封数据包，交给 cni0。 cni0 网桥将数据包转发到目标 Pod 的 veth。 通过这种方式，不同节点的 Pod 实现了透明的互通。\n4. 总结 Flannel是一个非常简单，稳定的CNI插件，其中部署和配置方式都非常简单，网络原理也简单，出现问题排查比较方便。特别适合k8s集群规模不大（1000个节点以内），网络性能要求不是非常严格，且团队中网络相关人员较少且无法支持维护复杂网络插件的团队使用。因为选择方案有一个基本的考虑点是该方案稳定且团队中有人可维护，而Flannel是一个维护成本相对比较低的网络方案。\n参考：\nhttps://github.com/flannel-io/flannel ","categories":"","description":"","excerpt":"1. Flannel简介 Flannel 是一个简单的、易于使用的 Kubernetes 网络插件，用于为容器集群提供网络功能。它主要解决的 …","ref":"/kubernetes-notes/network/flannel/flannel/","tags":["Kubernetes","CNI"],"title":"Flannel介绍"},{"body":"1. Cilium简介 Cilium 是一个开源的容器网络插件（CNI），专为 Kubernetes 和云原生环境设计，基于 eBPF（Extended Berkeley Packet Filter） 实现高性能、可扩展的网络和安全功能。它支持微服务间的细粒度流量控制，能够在 L3/L4/L7 层提供网络策略，同时具有强大的可观测性工具（如 Hubble）以帮助运维人员监控和优化流量。\n1.1. 核心特性 基于 eBPF 的高性能数据平面\neBPF：Cilium 通过 eBPF 在 Linux 内核中直接运行数据包处理逻辑，避免了内核与用户态的频繁切换，大幅提高了性能。\n高效流量转发：支持 BPF 的快速路径优化（Zero-Copy 转发），在高流量环境中表现出色。\n多层网络策略\nL3/L4 策略：基于 IP 和端口的基本流量控制。\nL7 策略：支持应用层协议（如 HTTP、gRPC）的访问控制，可以根据请求路径、方法或内容过滤流量。\n微服务友好：特别适合需要细粒度网络策略的微服务架构。\n可观测性\nHubble：Cilium 内置的可观测性平台，可以实时监控服务间的网络流量、延迟和错误率，帮助开发和运维团队快速定位问题。\n流量路径追踪：支持流量路径的全链路追踪，便于排查网络瓶颈或策略冲突。\n拓展性\n支持自定义 eBPF 程序，用户可以根据业务需求扩展网络功能。\n与其他云原生工具（如 Prometheus、Grafana、Istio）无缝集成。\n跨云和混合云支持\n支持 Kubernetes 集群的多网络环境，例如在跨云和混合云场景中提供统一的网络策略和流量控制。 服务发现与负载均衡\n内置服务负载均衡：提供内核级的流量负载均衡，比传统的 kube-proxy 性能更高。\n服务发现支持：可以与 Kubernetes 的 Service 资源协同工作，自动实现 Pod 间通信。\n1.2. 适用场景 云原生微服务架构 在需要严格流量控制和丰富可观测性的环境中表现尤为出色。 边缘计算 低延迟需求较高的场景，如 CDN 边缘节点和 IoT 环境。 高流量集群 适用于对吞吐量和性能要求极高的生产集群，例如电商、流媒体和金融服务。 多租户隔离 支持多租户网络环境中的强隔离需求。 1.3. Cilium的局限性 虽然 Cilium 在现代 Kubernetes 网络中表现出色，但它也存在一些缺点或需要注意的限制。\n1. 高性能消耗\n内存和 CPU 占用：由于需要在每个节点运行 Cilium Agent 和依赖 eBPF 加载程序，可能对节点资源消耗较高，尤其是在高流量场景下。 资源密集功能：如 Hubble（Cilium 的可观测性工具）可能进一步增加资源使用。 2. 依赖 Linux 内核版本\neBPF 限制：Cilium 依赖 eBPF 技术，对 Linux 内核版本有要求，最低需要 4.19+，部分功能（如高级负载均衡）需要 5.x 或更高版本。 内核升级成本：在某些环境（如老旧系统或企业级环境）中，升级内核可能具有挑战性。 3. 学习曲线陡峭\n复杂性：Cilium 引入了 eBPF 技术，与传统 CNI（如 Calico、Flannel）相比技术更复杂，需要深入理解 eBPF、Linux 内核网络栈和 Cilium 的配置方式。 4. 部署和管理复杂\n高级功能配置繁琐：如替代 kube-proxy 或配置高性能负载均衡，需要了解底层网络和 Kubernetes 的细节。 监控和故障排查难度：eBPF 程序运行在内核中，排查问题时无法直接查看传统用户态日志，需使用专用工具如 bpftool 或 Hubble。 2. Cilium部署 2.1. 部署 部署文档可参考：\nhttps://docs.cilium.io/en/stable/installation/k8s-install-helm/ https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#k8s-install-quick 可以使用helm来部署\n默认值：\n默认的clusterPoolIPv4PodCIDRList是10.0.0.0/8，需要保证pod CIDR跟node的CIDR不冲突。 默认ipam.mode是cluster-pool，可不修改设置。 helm repo add cilium https://helm.cilium.io/ helm repo update # 部署cilium kubectl create ns cilium-system || true helm install cilium cilium/cilium --namespace cilium-system \\ --set ipam.mode=cluster-pool \\ --set ipam.operator.clusterPoolIPv4PodCIDRList=\"10.244.0.0/16\" \\ --set ipam.operator.clusterPoolIPv4MaskSize=24 或者使用Cilium CLI 部署\n# 安装cilium cli curl -L --remote-name https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz tar xzvf cilium-linux-amd64.tar.gz sudo mv cilium /usr/local/bin # 部署cilium套件 kubectl create ns cilium-system || true cilium install --namespace=cilium-system \\ --set ipam.mode=cluster-pool \\ --set ipam.operator.clusterPoolIPv4PodCIDRList=\"10.244.0.0/16\" \\ --set ipam.operator.clusterPoolIPv4MaskSize=24 2.2. 部署检查 $ cilium status --wait --namespace cilium-system /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Hubble: disabled \\__/¯¯\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium-operator Running: 2 cilium Running: 2 Image versions cilium quay.io/cilium/cilium:v1.9.5: 2 cilium-operator quay.io/cilium/operator-generic:v1.9.5: 2 网络连通性测试\n$ cilium connectivity test ℹ️ Monitor aggregation detected, will skip some flow validation steps ✨ [k8s-cluster] Creating namespace for connectivity check... (...) --------------------------------------------------------------------------------------------------------------------- 📋 Test Report --------------------------------------------------------------------------------------------------------------------- ✅ 69/69 tests successful (0 warnings) 2.3. Cilium IPAM 模式 Cilium 支持以下两种常见的 IPAM 模式：\n2.3.1. Cluster-Pool 模式（默认模式）： 由 Cilium 自己管理 Pod IP 地址范围。 可以在部署时指定 CIDR 范围。 高灵活性：支持为不同节点或区域定义独立的 IP 地址池。 动态管理：可以动态调整 IP 地址池大小，适应集群的扩展需求。 无冲突设计：避免因节点增加或删除引起的 IP 地址冲突。 优化性能：减少对 Kubernetes 控制器的依赖，提高网络性能和资源利用率。 依赖 Cilium Operator：需要运行 Cilium Operator 来管理 IP 地址池，增加运维复杂性。 推荐场景\n大型集群（\u003e 500 节点）或超大规模集群。 需要跨区域或多节点池的复杂网络规划。 需要动态扩展 Pod IP 地址范围的集群。 集群运行 Cilium 并需要充分利用其高级功能（如 eBPF 加速、服务网格等）。 2.3.2. Kubernetes 模式： 使用 Kubernetes 自身的 IP 地址分配方式，例如由 kube-controller-manager 通过 --cluster-cidr 进行管理。 Cilium 从 Kubernetes 中获取分配给 Pod 的 IP 地址。 兼容性强：适配大多数 CNI 插件（包括 Cilium），无需额外配置。 灵活性有限：不支持细粒度的 IP 地址池管理，无法为特定节点或区域分配特定的 IP 范围。 推荐场景\n小型或中型集群（\u003c 500 节点）。 网络规划较为简单，无需复杂的 IP 地址管理。 需要快速部署并保持与 Kubernetes 默认行为一致。 3. Cilium架构及组件介绍 3.1. 架构图 参考官网： https://docs.cilium.io/en/stable/overview/component-overview/\n3.2. 核心组件 1. Cilium Agent\n运行在每个 Kubernetes 节点上，是 Cilium 的核心守护进程。 功能： 从 Kubernetes API Server 获取资源（如 Pod、Service 和网络策略）并转换为 eBPF 程序。 在每个节点上管理和加载 eBPF 程序到内核。 实现 L3/L4 和 L7 网络策略，并将策略下发到数据平面。 负责服务发现和负载均衡（取代 kube-proxy）。 2. Cilium CLI\n命令行工具，用于安装、配置和调试 Cilium。 功能： 配置 Cilium 的网络策略。 查看 Cilium 和 Hubble 的运行状态。 调试 eBPF 程序。 3. CNI Plugin\n当在节点上调度或终止pod时，Kubernetes会调用CNI插件（cilium-cni）。它与节点的Cilium API交互，触发必要的数据路径配置，为pod提供网络、负载平衡和网络策略。 4. Cilium Operator\n在 Kubernetes 中运行的控制平面组件。 功能： 处理 IP 地址管理：管理 Pod 的 IP 池。 维护 Cilium Agent 与 Kubernetes 的集成。 处理节点间的拓扑变化（如节点加入或离开）。 3.3. 其他组件 1. eBPF 程序\nCilium 的数据平面核心，运行在 Linux 内核中。 功能： 路由和转发：在节点间处理 Pod 的网络流量。 网络策略：实现 L3/L4 和 L7 的访问控制。 服务负载均衡：提供类似 kube-proxy 的服务转发功能，但性能更高。 监控和可观测性：收集网络流量数据，供 Hubble 或其他工具分析。 2. Hubble\nCilium 的可观测性平台，用于实时监控和分析网络流量。 功能： 流量可视化：展示服务间的流量路径和统计。 流量追踪：捕获和调试网络问题。 延迟和错误率监控。 4. Cilium 的工作流 初始化： Cilium Agent 启动后，与 Kubernetes API Server 建立连接，监听资源变化。 策略配置： 用户定义的 NetworkPolicy 或 CiliumNetworkPolicy 通过 Cilium Agent 传递到 eBPF 程序。 eBPF 程序在内核中直接执行流量控制逻辑。 流量处理： 数据平面通过 eBPF 程序对网络流量进行路由、策略匹配和负载均衡。 监控和分析： eBPF 程序收集流量数据，发送到 Hubble。 Hubble 将数据可视化，供用户监控和调试。 参考：\nhttps://docs.cilium.io/en/stable/ https://docs.cilium.io/en/stable/overview/component-overview/ https://docs.cilium.io/en/stable/network/concepts/ipam/ ","categories":"","description":"","excerpt":"1. Cilium简介 Cilium 是一个开源的容器网络插件（CNI），专为 Kubernetes 和云原生环境设计， …","ref":"/kubernetes-notes/network/cilium/cilium/","tags":["Kubernetes","CNI"],"title":"Cilium介绍"},{"body":"1. Calico简介 Calico 是一个开源的网络和网络安全解决方案，主要用于 Kubernetes 等容器编排系统。它通过提供高效的网络连接和强大的安全控制来满足容器化和微服务架构的需求。Calico 以其灵活性、可扩展性和性能著称，是许多企业和云原生应用的首选网络插件。\n1.1. 主要功能和特点 网络架构：\nL3 路由架构：Calico 基于第三层（L3）网络构建，不依赖传统的覆盖网络（overlay network），使其可以利用 IP 路由来实现跨节点的直接通信。 BGP 支持：Calico 使用 BGP（边界网关协议）来分发和同步路由信息，使得容器和 Pod 能够跨节点直接通信，减少网络延迟，提升性能。 支持多种后端：除了默认的 IP 路由模式，Calico 也支持 VXLAN、IPIP、WireGuard 和 eBPF 后端，适应不同的网络环境和需求。 网络安全：\n网络策略：Calico 支持标准的 Kubernetes NetworkPolicy，用户可以通过策略控制 Pod 之间的通信权限。 GlobalNetworkPolicy：Calico 提供 GlobalNetworkPolicy，可以实现跨命名空间、跨集群的统一策略管理，用于多租户隔离或更高的安全控制。 支持 eBPF：Calico 在启用 eBPF 模式时，可以通过 eBPF 提供更高效的网络数据处理，并支持 L4 层的负载均衡和网络策略控制。 可观测性和可视化：\nCalico 集成了多种可观测性工具，支持 Prometheus 等监控系统，并可以生成网络流量和策略的监控指标，帮助运维人员了解网络状况。 支持与 EFK（Elasticsearch, Fluentd, Kibana）集成，便于可视化网络流量和策略。 高扩展性：\nCalico 可以在不同的基础设施中运行，包括本地数据中心和云平台（AWS、GCP、Azure），并支持与 VM 及裸金属服务器互联。 使用 IP 路由和 BGP，使得 Calico 在大规模集群中也能保持良好的性能和扩展性。 1.2. 适用场景 容器化应用的网络管理：适用于 Kubernetes 等容器编排平台，为容器提供高效、安全的网络连接。 混合云和多集群部署：Calico 支持跨数据中心和云环境的多集群部署，非常适合混合云和多租户环境。 网络安全：对于需要细粒度的网络安全策略控制和多租户隔离的场景，Calico 提供多层次的安全控制和隔离。 1.3. Calico的局限性 calico也存在一些不足和局限性。\n1. 对网络拓扑的依赖\nBGP 配置复杂性： Calico 默认使用 BGP 协议进行路由，如果网络环境中没有现成的 BGP 支持（如非生产环境或网络管理员经验不足），配置可能较复杂。此外，BGP 的管理对部分运维人员有一定门槛。 对底层网络要求较高： Calico 的无隧道设计依赖底层网络的正常运行。如果底层网络不支持高效的路由或不能很好地管理多播流量，可能会影响整体网络性能。 2. 大规模集群性能问题\netcd 负载： 在大规模 Kubernetes 集群中（如几千个节点），Calico 对 etcd 的访问频率较高，这可能会给 etcd 带来较大压力。尽管 Typha 可以缓解部分问题，但依然可能成为瓶颈。 路由表膨胀： 在大规模集群中，BGP 模式下每个节点需要维护大量路由信息（与集群中 Pod 的数量相关），可能导致路由表膨胀并对内存和 CPU 资源产生较大压力。 2. Calico架构图 来自官网\n2.1. Calico 架构概览 Calico 通过 BGP（边界网关协议）在 Kubernetes 集群节点之间进行路由广播，无需使用复杂的隧道协议（如 VXLAN 或 GRE），这使得 Calico 的网络性能较高。它主要由以下组件组成：\ncalico-node：这是 Calico 的核心组件，运行在每个节点上，负责设置路由、管理 IP 地址，并通过 Felix 实现网络策略的执行。 BGP Daemon (Bird)：负责节点之间的路由传播，以实现不同节点 Pod 之间的流量路由。 Felix：作为 Calico 的主代理，负责监控 etcd 中存储的网络策略并将其应用于节点的网络接口，同时负责设置 IP 路由、管理 ACLs 以确保流量符合策略。 Typha：当节点数较多时，可以通过 Typha 组件来减少 etcd 的访问负载。它会将 etcd 中的策略变化缓存起来，并同步给每个节点的 Felix 代理。 etcd：Calico 使用 etcd 作为存储后端，用于存储网络策略、IP 池等配置。也可以使用 Kubernetes 的 API Server 作为存储后端，便于集成。 2.2. Calico 的核心组件 calicoctl：这是 Calico 提供的命令行工具，用于配置和管理 Calico 的资源，比如 IP 池、策略、网络设置等。可以通过该工具查看、创建、更新和删除网络策略。 IPAM (IP Address Management)：Calico 自带的 IP 地址管理模块，可以为集群中的 Pod 自动分配 IP 地址。IPAM 支持灵活的 IP 池设置，可以对不同的节点、命名空间或工作负载分配特定的 IP 范围。 2.3. 工作流程 每个节点上运行的 calico-node 组件会和其他节点进行 BGP 路由信息交换，确保不同节点的 Pod 可以互相通信。 Felix 组件负责将网络策略的定义应用到实际的网络接口上，以确保流量符合预设的策略。 Typha 组件在大规模集群中可以有效地减少 etcd 的压力，帮助 Felix 快速同步网络策略。 2.4. Calico 网络策略 (Network Policy) Calico 支持丰富的网络策略，用于定义不同的 Pod 或服务之间的网络访问规则。网络策略的主要功能包括：\n基于标签的策略：可以根据 Pod 或命名空间的标签来控制网络流量的允许和拒绝。 支持 Egress 和 Ingress 策略：不仅可以控制进入 Pod 的流量，还可以控制 Pod 发出的流量。 灵活的规则定义：支持基于 IP 地址、端口、协议的规则配置，能够精细地控制网络流量。 3. Calico组件及配置 calico的部署可参考：kubeadm-scripts/cni/install-calico.sh\n部署完成后可以在k8s集群中看到以下组件：\n中控组件：calico-kube-controllers\n节点组件：calico-node\ncalico-system calico-kube-controllers-8945657f7-ntbxm 1/1 Running 0 421d calico-system calico-node-2df8c 1/1 Running 0 421d calico-system calico-node-5vq6z 1/1 Running 0 421d calico-system calico-node-dpnkd 1/1 Running 0 421d calico-system calico-node-sms2h 1/1 Running 0 421d calico-system calico-node-w95l2 1/1 Running 0 414d 3.1. calico node进程树 \\_ /usr/local/bin/runsvdir -P /etc/service/enabled \\_ runsv confd | \\_ calico-node -confd \\_ runsv allocate-tunnel-addrs | \\_ calico-node -allocate-tunnel-addrs \\_ runsv monitor-addresses | \\_ calico-node -monitor-addresses \\_ runsv bird | \\_ bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg \\_ runsv felix | \\_ calico-node -felix \\_ runsv cni | \\_ calico-node -monitor-token \\_ runsv node-status-reporter | \\_ calico-node -status-reporter \\_ runsv bird6 \\_ bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg 3.2. calico-kube-controllers进程树 /usr/bin/kube-controllers 3.3. CNI calico二进制 cd /opt/cni/bin |-- calico |-- calico-ipam |-- install 3.4. CNI calico配置 10-calico.conflist\ncd /etc/cni/net.d # cat 10-calico.conflist { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"log_file_path\": \"/var/log/calico/cni/cni.log\", \"datastore_type\": \"kubernetes\", \"nodename\": \"node1\", \"mtu\": 0, \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"snat\": true, \"capabilities\": {\"portMappings\": true} }, { \"type\": \"bandwidth\", \"capabilities\": {\"bandwidth\": true} } ] } calico-kubeconfig\n# Kubeconfig file for Calico CNI plugin. Installed by calico/node. apiVersion: v1 kind: Config clusters: - name: local cluster: server: https://10.96.0.1:443 certificate-authority-data: \"xxx\" users: - name: calico user: token:xxx contexts: - name: calico-context context: cluster: local user: calico 参考：\nhttps://docs.tigera.io/calico/latest/about/\nhttps://docs.tigera.io/calico/latest/reference/architecture/overview\nhttps://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart\nLive Migration from Flannel to Calico\n","categories":"","description":"","excerpt":"1. Calico简介 Calico 是一个开源的网络和网络安全解决方案，主要用于 Kubernetes 等容器编排系统。它通过提供高效的网 …","ref":"/kubernetes-notes/network/calico/calico/","tags":["Kubernetes","CNI"],"title":"Calico介绍"},{"body":"1. knative简介 knative是一个将serverless的能力扩展到k8s中的开源项目。serverless让开发者无需关注容器、镜像、运维等事项，集中精力于开发代码本身，即将代码通过免运维的形式交付给serverless平台。代码会在设定的条件下运行，并自动实现扩缩容。\n2. knative的组件 knative主要包含三个部分：\nbuild: 将代码转换为容器，主要包括\n将源代码从git仓库拉取下来，安装相关的依赖 构建容器镜像 将容器镜像推送到镜像仓库 Serving：创建一个可伸缩的部署。\n配置定义了服务的状态，包括版本管理，每次修改都创建一个新版本部署，并保留旧版本。 灵活的路由控制，可以控制百分比的路由到新版本和旧版本服务。 自动弹性伸缩，可以快速创建上千个实例或快速调整实例数为0。 Eventing：事件触发，通过定义各种事件使用knative自动来完成这些任务，而无需手动编写脚本。\n3. 部署knative 部署knative主要是部署Serving和Eventing两个组件，可以单独部署也可以同时部署。\n3.1. 部署Serving 部署CRD\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-crds.yaml 部署Serving\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-core.yaml 部署HPA autoscaling（可选）\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-hpa.yaml 查看部署结果\n# kgdep -n knative-serving NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/activator 1/1 1 1 107s deployment.apps/autoscaler 1/1 1 1 107s deployment.apps/autoscaler-hpa 1/1 1 1 107s deployment.apps/controller 1/1 1 1 107s deployment.apps/domain-mapping 1/1 1 1 107s deployment.apps/domainmapping-webhook 1/1 1 1 107s deployment.apps/webhook 1/1 1 1 107s 3.2. 部署Eventing 部署CRD\nkubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.10.0/eventing-crds.yaml 部署Eventing\nkubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.10.0/eventing-core.yaml 查看部署结果\n# kgdep -n knative-eventing NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1/1 1 1 2m13s eventing-webhook 1/1 1 1 2m13s pingsource-mt-adapter 0/0 0 0 2m13s 4. 部署knative客户端 wget https://github.com/knative/client/releases/download/knative-v1.10.0/kn-linux-amd64 chmod +x kn-linux-amd64 mv kn-linux-amd64 /usr/bin/kn kn命令：\nkn kn is the command line interface for managing Knative Serving and Eventing resources Find more information about Knative at: https://knative.dev Serving Commands: service Manage Knative services revision Manage service revisions route List and describe service routes domain Manage domain mappings container Manage service's containers (experimental) Eventing Commands: source Manage event sources broker Manage message brokers trigger Manage event triggers channel Manage event channels subscription Manage event subscriptions eventtype Manage eventtypes Other Commands: plugin Manage kn plugins secret Manage secrets completion Output shell completion code version Show the version of this client 5. 创建示例服务 以下通过yaml的方式演示。\nvi hello.yaml\napiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: ghcr.io/knative/helloworld-go:latest ports: - containerPort: 8080 env: - name: TARGET value: \"World\" 创建文件\nkubectl apply -f hello.yaml 查看服务\n# kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default.svc.cluster.local hello-00001 hello-00001 Unknown IngressNotConfigured # kubectl get po NAME READY STATUS RESTARTS AGE hello-00001-deployment-6469df75c-qpp5v 2/2 Running 0 15m 参考：\nhttps://www.ibm.com/topics/knative https://knative.dev/docs/concepts/ https://knative.dev/docs/serving/ ","categories":"","description":"","excerpt":"1. knative简介 knative是一个将serverless的能力扩展到k8s中的开源项目。serverless让开发者无需关注容 …","ref":"/kubernetes-notes/serverless/knative/","tags":["Serverless"],"title":"knative介绍"},{"body":"1. 前言 本规范在 Google Golang 代码规范 的基础上，进行了调整和补充。\n每项规范内容，给出了要求等级，其定义为：\n必须（Mandatory）：用户必须采用； 推荐（Preferable）：用户理应采用，但如有特殊情况，可以不采用； 可选（Optional）：用户可参考，自行决定是否采用； 2. 代码风格 2.1 【必须】格式化 代码都必须用 gofmt 格式化。 2.2 【推荐】换行 建议一行代码不要超过120列，超过的情况，使用合理的换行方法换行。 例外场景： import 模块语句 工具生成代码 struct tag 2.3 【必须】括号和空格 遵循 gofmt 的逻辑。\n运算符和操作数之间要留空格。\n作为输入参数或者数组下标时，运算符和运算数之间不需要空格，紧凑展示。\n2.4 【必须】import 规范 使用 goimports 自动格式化引入的包名，import 规范原则上以 goimports 规则为准。\ngoimports 会自动把依赖包按首字母排序，并对包进行分组管理，通过空行隔开，默认分为本地包（标准库、内部包）、第三方包。\n标准包永远位于最上面的第一组。\n内部包是指不能被外部 import 的包，如 GoPath 模式下的包名或者非域名开头的当前项目的 GoModules 包名。\ngoimports 默认最少分成本地包和第三方包两大类，这两类包必须分开不能放在一起。本地包或者第三方包内部可以继续按实际情况细分不同子类。\n不要使用相对路径引入包：\n// 不要采用这种方式 import ( \"../net\" ) 应该使用完整的路径引入包： import ( \"xxxx.com/proj/net\" ) 包名和 git 路径名不一致时，或者多个相同包名冲突时，使用别名代替，别名命名规范和包命名规范保持一致： // 合理用法：包名和 git 路径名不一致，使用别名 import ( opentracing \"github.com/opentracing/opentracing-go\" ) // 合理用法：多个相同包名冲突，使用别名 import ( \"fmt\" \"os\" \"runtime/trace\" nettrace \"golang.net/x/trace\" ) // 不合理用法：包名和路径名一致，也不存在多包名冲突，不应该使用别名 import ( \"fmt\" \"os\" nettrace \"golang.net/x/trace\" ) 【可选】匿名包的引用建议使用一个新的分组引入，并在匿名包上写上注释说明。 完整示例如下：\nimport ( // standard package \u0026 inner package \"encoding/json\" \"myproject/models\" \"myproject/controller\" \"strings\" // third-party package \"git.obc.im/obc/utils\" \"git.obc.im/dep/beego\" \"git.obc.im/dep/mysql\" opentracing \"github.com/opentracing/opentracing-go\" // anonymous import package // import filesystem storage driver _ \"github.com/org/repo/pkg/storage/filesystem ) 2.5 【必须】错误处理 2.5.1 【必须】error 处理 error 作为函数的值返回，必须对 error 进行处理, 或将返回值赋值给明确忽略。对于 defer xx.Close()可以不用显式处理。\nerror 作为函数的值返回且有多个返回值的时候，error 必须是最后一个参数。\n// 不要采用这种方式 func do() (error, int) { } // 要采用下面的方式 func do() (int, error) { } 错误描述不需要标点结尾。\n采用独立的错误流进行处理。\n// 不要采用这种方式 if err != nil { // error handling } else { // normal code } // 而要采用下面的方式 if err != nil { // error handling return // or continue, etc. } // normal code 如果返回值需要初始化，则采用下面的方式： x, err := f() if err != nil { // error handling return // or continue, etc. } // use x 错误返回的判断独立处理，不与其他变量组合逻辑判断。 // 不要采用这种方式： x, y, err := f() if err != nil || y == nil { return err // 当y与err都为空时，函数的调用者会出现错误的调用逻辑 } // 应当使用如下方式： x, y, err := f() if err != nil { return err } if y == nil { return errors.New(\"some error\") } 【推荐】对于不需要格式化的错误，生成方式为：errors.New(\"xxxx\")。 【推荐】建议go1.13 以上，error 生成方式为：fmt.Errorf(\"module xxx: %w\", err)。 2.5.2 【必须】panic 处理 在业务逻辑处理中禁止使用 panic。\n在 main 包中只有当完全不可运行的情况可使用 panic，例如：文件无法打开，数据库无法连接导致程序无法正常运行。\n对于其它的包，可导出的接口一定不能有 panic；在包内传递错误时，不推荐使用 panic 来传递 error。\n// 不推荐为传递error而在包内使用panic,以下为示例 // PError 包内定义的错误类型 type PError string // Error error接口方法 func (e PError) Error() string { return string(e) } func do(str string) { // ... // 此处的panic用于传递error panic(PError(\"错误信息\")) // ... } // Do 包级访问入口 func Do(str string) error { var err error defer func() { if e := recover(); e != nil { err = e.(PError) } }() do(str) return err } 建议在 main 包中使用 log.Fatal 来记录错误，这样就可以由 log 来结束程序，或者将 panic 抛出的异常记录到日志文件中，方便排查问题。\npanic 捕获只能到 goroutine 最顶层，每个自行启动的 goroutine，必须在入口处捕获 panic，并打印详细堆栈信息或进行其它处理。\n2.5.3 【必须】recover 处理 recover 用于捕获 runtime 的异常，禁止滥用 recover。\n必须在 defer 中使用，一般用来捕获程序运行期间发生异常抛出的 panic 或程序主动抛出的 panic。\npackage main import ( \"log\" ) func main() { defer func() { if err := recover(); err != nil { // do something or record log log.Println(\"exec panic error: \", err) // log.Println(debug.Stack()) } }() getOne() panic(11) // 手动抛出panic } // getOne 模拟slice越界 runtime运行时抛出的panic func getOne() { defer func() { if err := recover(); err != nil { // do something or record log log.Println(\"exec panic error: \", err) // log.Println(debug.Stack()) } }() var arr = []string{\"a\", \"b\", \"c\"} log.Println(\"hello,\", arr[4]) } // 执行结果： // 2020/01/02 17:18:53 exec panic error: runtime error: index out of range // 2020/01/02 17:18:53 exec panic error: 11 2.6 【必须】单元测试 单元测试文件名命名规范为 example_test.go。\n测试用例的函数名称必须以 Test 开头，例如 TestExample。\n如果存在 func Foo，单测函数可以带下划线，为 func Test_Foo。如果存在 func (b *Bar) Foo，单测函数可以为 func TestBar_Foo。下划线不能出现在前面描述情况以外的位置。\n单测文件行数限制是普通文件的2倍，即1600行。单测函数行数限制也是普通函数的2倍，即为160行。圈复杂度、列数限制、 import 分组等其他规范细节和普通文件保持一致。\n由于单测文件内的函数都是不对外的，所有可导出函数可以没有注释，但是结构体定义时尽量不要导出。\n每个重要的可导出函数都要首先编写测试用例，测试用例和正规代码一起提交方便进行回归测试。\n2.7 【必须】类型断言失败处理 type assertion 的单个返回值形式针对不正确的类型将产生 panic。因此，请始终使用 “comma ok” 的惯用法。 // 不要采用这种方式 t := i.(string) // 而要采用下面的方式 t, ok := i.(string) if !ok { // 优雅地处理错误 } 3. 注释 在编码阶段同步写好变量、函数、包注释，注释可以通过 godoc 导出生成文档。 程序中每一个被导出的(大写的)名字，都应该有一个文档注释。 所有注释掉的代码在提交 code review 前都应该被删除，除非添加注释讲解为什么不删除， 并且标明后续处理建议（比如删除计划）。 3.1 【必须】包注释 每个包都应该有一个包注释。\n包如果有多个 go 文件，只需要出现在一个 go 文件中（一般是和包同名的文件）即可，格式为：“// Package 包名 包信息描述”。\n// Package math provides basic constants and mathematical functions. package math // 或者 /* Package template implements data-driven templates for generating textual output such as HTML. .... */ package template 3.2 【必须】结构体注释 每个需要导出的自定义结构体或者接口都必须有注释说明。\n注释对结构进行简要介绍，放在结构体定义的前一行。\n格式为：\"// 结构体名 结构体信息描述\"。\n结构体内的可导出成员变量名，如果是个生僻词，或者意义不明确的词，就必须要给出注释，放在成员变量的前一行或同一行的末尾。\n// User 用户结构定义了用户基础信息 type User struct { Name string Email string // Demographic 族群 Demographic string } 3.3 【必须】方法注释 每个需要导出的函数或者方法（结构体或者接口下的函数称为方法）都必须有注释。注意，如果方法的接收器为不可导出类型，可以不注释，但需要质疑该方法可导出的必要性。\n注释描述函数或方法功能、调用方等信息。\n格式为：\"// 函数名 函数信息描述\"。\n// NewtAttrModel 是属性数据层操作类的工厂方法 func NewAttrModel(ctx *common.Context) *AttrModel { // TODO } 3.4 【必须】变量和常量注释 每个需要导出的常量和变量都必须有注释说明。\n该注释对常量或变量进行简要介绍，放在常量或者变量定义的前一行。\n大块常量或变量定义时，可在前面注释一个总的说明，然后每一行常量的末尾详细注释该常量的定义。\n格式为：\"// 变量名 变量信息描述\"，斜线后面紧跟一个空格。\n// FlagConfigFile 配置文件的命令行参数名 const FlagConfigFile = \"--config\" // 命令行参数 const ( FlagConfigFile1 = \"--config\" // 配置文件的命令行参数名1 FlagConfigFile2 = \"--config\" // 配置文件的命令行参数名2 FlagConfigFile3 = \"--config\" // 配置文件的命令行参数名3 FlagConfigFile4 = \"--config\" // 配置文件的命令行参数名4 ) // FullName 返回指定用户名的完整名称 var FullName = func(username string) string { return fmt.Sprintf(\"fake-%s\", username) } 3.5 【必须】类型注释 每个需要导出的类型定义（type definition）和类型别名（type aliases）都必须有注释说明。\n该注释对类型进行简要介绍，放在定义的前一行。\n格式为：\"// 类型名 类型信息描述\"。\n// StorageClass 存储类型 type StorageClass string // FakeTime 标准库时间的类型别名 type FakeTime = time.Time 4. 命名规范 命名是代码规范中很重要的一部分，统一的命名规范有利于提高代码的可读性，好的命名仅仅通过命名就可以获取到足够多的信息。\n4.1 【推荐】包命名 保持 package 的名字和目录一致。\n尽量采取有意义、简短的包名，尽量不要和标准库冲突。\n包名应该为小写单词，不要使用下划线或者混合大小写，使用多级目录来划分层级。\n包名可谨慎地使用缩写。当缩写是程序员广泛熟知的词时，可以使用缩写。例如：\nstrconv (string conversion) syscall (system call) fmt (formatted I/O) 如果缩写有歧义或不清晰，不用缩写。\n项目名可以通过中划线来连接多个单词。\n简单明了的包命名，如：time、list、http。\n不要使用无意义的包名，如：util、common、misc、global。package名字应该追求清晰且越来越收敛，符合‘单一职责’原则。而不是像common一样，什么都能往里面放，越来越膨胀，让依赖关系变得复杂，不利于阅读、复用、重构。注意，xx/util/encryption这样的包名是允许的。\n4.2 【必须】文件命名 采用有意义，简短的文件名。\n文件名应该采用小写，并且使用下划线分割各个单词。\n4.3 【必须】结构体命名 采用驼峰命名方式，首字母根据访问控制采用大写或者小写。\n结构体名应该是名词或名词短语，如 Customer、WikiPage、Account、AddressParser，它不应是动词。\n避免使用 Data、Info 这类意义太宽泛的结构体名。\n结构体的声明和初始化格式采用多行，例如：\n// User 多行声明 type User struct { Name string Email string } // 多行初始化 u := User{ Name: \"john\", Email: \"john@example.com\", } 4.4 【推荐】接口命名 命名规则基本保持和结构体命名规则一致。\n单个函数的接口名以 er 作为后缀，例如 Reader，Writer。\n// Reader 字节数组读取接口 type Reader interface { // Read 读取整个给定的字节数据并返回读取的长度 Read(p []byte) (n int, err error) } 两个函数的接口名综合两个函数名。\n三个以上函数的接口名，类似于结构体名。\n// Car 小汽车结构申明 type Car interface { // Start ... Start([]byte) // Stop ... Stop() error // Recover ... Recover() } 4.5 【必须】变量命名 变量名必须遵循驼峰式，首字母根据访问控制决定使用大写或小写。\n特有名词时，需要遵循以下规则：\n如果变量为私有，且特有名词为首个单词，则使用小写，如 apiClient； 其他情况都应该使用该名词原有的写法，如 APIClient、repoID、UserID； 错误示例：UrlArray，应该写成 urlArray 或者 URLArray； 详细的专有名词列表可参考这里。 私有全局变量和局部变量规范一致，均以小写字母开头。\n代码生成工具自动生成的代码可排除此规则（如 xxx.pb.go 里面的 Id）。\n变量名更倾向于选择短命名。特别是对于局部变量。 c比lineCount要好，i比sliceIndex要好。基本原则是：变量的使用和声明的位置越远，变量名就需要具备越强的描述性。\n4.6 【必须】常量命名 常量均需遵循驼峰式。 // AppVersion 应用程序版本号定义 const AppVersion = \"1.0.0\" 如果是枚举类型的常量，需要先创建相应类型： // Scheme 传输协议 type Scheme string const ( // HTTP 表示HTTP明文传输协议 HTTP Scheme = \"http\" // HTTPS 表示HTTPS加密传输协议 HTTPS Scheme = \"https\" ) 私有全局常量和局部变量规范一致，均以小写字母开头。 const appVersion = \"1.0.0\" 4.7 【必须】函数命名 函数名必须遵循驼峰式，首字母根据访问控制决定使用大写或小写。 代码生成工具自动生成的代码可排除此规则（如协议生成文件 xxx.pb.go , gotests 自动生成文件 xxx_test.go 里面的下划线）。 5. 控制结构 5.1 【推荐】if if 接受初始化语句，约定如下方式建立局部变量： if err := file.Chmod(0664); err != nil { return err } if 对两个值进行判断时，约定如下顺序：变量在左，常量在右： // 不要采用这种方式 if nil != err { // error handling } // 不要采用这种方式 if 0 == errorCode { // do something } // 而要采用下面的方式 if err != nil { // error handling } // 而要采用下面的方式 if errorCode == 0 { // do something } if 对于bool类型的变量，应直接进行真假判断： var allowUserLogin bool // 不要采用这种方式 if allowUserLogin == true { // do something } // 不要采用这种方式 if allowUserLogin == false { // do something } // 而要采用下面的方式 if allowUserLogin { // do something } // 而要采用下面的方式 if !allowUserLogin { // do something } 5.2\t【推荐】for 采用短声明建立局部变量： sum := 0 for i := 0; i \u003c 10; i++ { sum += 1 } 5.3\t【必须】range 如果只需要第一项（key），就丢弃第二个： for key := range m { if key.expired() { delete(m, key) } } 如果只需要第二项，则把第一项置为下划线： sum := 0 for _, value := range array { sum += value } 5.4\t【必须】switch 要求必须有 default： switch os := runtime.GOOS; os { case \"darwin\": fmt.Println(\"OS X.\") case \"linux\": fmt.Println(\"Linux.\") default: // freebsd, openbsd, // plan9, windows... fmt.Printf(\"%s.\\n\", os) } 5.5 【推荐】return 尽早 return，一旦有错误发生，马上返回： f, err := os.Open(name) if err != nil { return err } defer f.Close() d, err := f.Stat() if err != nil { return err } codeUsing(f, d) 5.6 【必须】goto 业务代码禁止使用 goto，其他框架或底层源码推荐尽量不用。 6. 函数 6.1 【推荐】函数参数 函数返回相同类型的两个或三个参数，或者如果从上下文中不清楚结果的含义，使用命名返回，其它情况不建议使用命名返回。 // Parent1 ... func (n *Node) Parent1() *Node // Parent2 ... func (n *Node) Parent2() (*Node, error) // Location ... func (f *Foo) Location() (lat, long float64, err error) 传入变量和返回变量以小写字母开头。\n参数数量均不能超过5个。\n尽量用值传递，非指针传递。\n传入参数是 map，slice，chan，interface 不要传递指针。\n6.2 【必须】defer 当存在资源管理时，应紧跟 defer 函数进行资源的释放。\n判断是否有错误发生之后，再 defer 释放资源。\nresp, err := http.Get(url) if err != nil { return err } // 如果操作成功，再defer Close() defer resp.Body.Close() 禁止在循环中使用 defer，举例如下： // 不要这样使用 func filterSomething(values []string) { for _, v := range values { fields, err := db.Query(v) // 示例，实际不要这么查询，防止sql注入 if err != nil { // xxx } defer fields.Close() // 继续使用fields } } // 应当使用如下的方式： func filterSomething(values []string) { for _, v := range values { func() { fields, err := db.Query(v) // 示例，实际不要这么查询，防止sql注入 if err != nil { ... } defer fields.Close() // 继续使用fields }() } } 6.3 【推荐】方法的接收器 【推荐】推荐以类名第一个英文首字母的小写作为接收器的命名。\n【推荐】接收器的命名在函数超过20行的时候不要用单字符。\n【必须】命名不能采用 me，this，self 这类易混淆名称。\n6.4 【推荐】代码行数 【必须】文件长度不能超过800行。\n【推荐】函数长度不能超过80行（函数长度为函数签名左括号下一行开始到右括号上一行结束部分的行数，包括代码行，注释行，空行）。\n6.5 【必须】嵌套 嵌套深度不能超过4层： // AddArea 添加成功或出错 func (s *BookingService) AddArea(areas ...string) error { s.Lock() defer s.Unlock() for _, area := range areas { for _, has := range s.areas { if area == has { return srverr.ErrAreaConflict } } s.areas = append(s.areas, area) s.areaOrders[area] = new(order.AreaOrder) } return nil } // 建议调整为这样： // AddArea 添加成功或出错 func (s *BookingService) AddArea(areas ...string) error { s.Lock() defer s.Unlock() for _, area := range areas { if s.HasArea(area) { return srverr.ErrAreaConflict } s.areas = append(s.areas, area) s.areaOrders[area] = new(order.AreaOrder) } return nil } // HasArea ... func (s *BookingService) HasArea(area string) bool { for _, has := range s.areas { if area == has { return true } } return false } 6.6 【推荐】变量声明 变量声明尽量放在变量第一次使用前面，就近原则。 6.7 【必须】魔法数字 如果魔法数字出现超过2次，则禁止使用。 func getArea(r float64) float64 { return 3.14 * r * r } func getLength(r float64) float64 { return 3.14 * 2 * r } 用一个常量代替： // PI ... const PI = 3.14 func getArea(r float64) float64 { return PI * r * r } func getLength(r float64) float64 { return PI * 2 * r } 7. 依赖管理 7.1 【必须】go1.11 以上必须使用 go modules 模式： go mod init git.xxx.com/group/myrepo 7.2 【推荐】代码提交 建议所有不对外开源的工程的 module name 使用 git.xxx.com/group/repo ，方便他人直接引用。\n建议使用 go modules 作为依赖管理的项目不提交 vendor 目录。\n建议使用 go modules 管理依赖的项目， go.sum 文件必须提交，不要添加到 .gitignore 规则中。\n8. 应用服务 8.1 【推荐】应用服务接口建议有 README.md 其中建议包括服务基本描述、使用方法、部署时的限制与要求、基础环境依赖（例如最低 go 版本、最低外部通用包版本）等。 8.2 【必须】应用服务必须要有接口测试。 附：常用工具 go 语言本身在代码规范性这方面也做了很多努力，很多限制都是强制语法要求，例如左大括号不换行，引用的包或者定义的变量不使用会报错，此外 go 还是提供了很多好用的工具帮助我们进行代码的规范。\ngofmt ，大部分的格式问题可以通过 gofmt 解决， gofmt 自动格式化代码，保证所有的 go 代码与官方推荐的格式保持一致，于是所有格式有关问题，都以 gofmt 的结果为准。 goimports ，此工具在 gofmt 的基础上增加了自动删除和引入包。 go vet ，vet 工具可以帮我们静态分析我们的源码存在的各种问题，例如多余的代码，提前 return 的逻辑， struct 的 tag 是否符合标准等。编译前先执行代码静态分析。 golint ，类似 javascript 中的 jslint 的工具，主要功能就是检测代码中不规范的地方。 ","categories":"","description":"","excerpt":"1. 前言 本规范在 Google Golang 代码规范 的基础上，进行了调整和补充。\n每项规范内容，给出了要求等级，其定义为：\n必 …","ref":"/golang-notes/standard/go-style-guide/","tags":["Golang"],"title":"Golang 代码规范"},{"body":"本文主要介绍跟baremetal相关的基本概念\nBMC（Baseboard Management Controller） 在介绍BMC之前需要了解一个概念，即平台管理（platform management）。平台管理表示的是一系列的监视和控制功能，操作的对象是系统硬件。比如通过监视系统的温度，电压，风扇、电源等等，并做相应的调节工作，以保证系统处于健康的状态。同时平台管理还负责记录各种硬件的信息和日志记录，用于提示用户和后续问题的定位。\n以上的这些功能可以集成到一个控制器上来实现，这个控制器被称为基板管理控制器（Baseboard Manager Controller，简称BMC）。\nBMC 是独立于服务器系统之外的小型操作系统，是一个集成在主板上的芯片，也有产品是通过 PCIE 等形式插在主板上，对外表现形式只是一个标准的 RJ45 网口，拥有独立 IP 的固件系统。服务器集群一般使用 BMC 指令进行大规模无人值守操作，包括服务器的远程管理、监控、安装、重启等。\nBMC是一个独立的系统，它不依赖与系统上的其它硬件（比如CPU、内存等），也不依赖与BIOS、OS等。\nBMC通过不同的接口与系统中的其它组件连接。LPC、I2C、SMBUS，Serial等，这些都是比较基本的接口，而IPMI，它是与BMC匹配的接口，所有的BMC都需要实现这种接口。\nBIOS（Basic Input Output System） BIOS（Basic Input Output System），即基础输入输出系统，是刻在主板 ROM 芯片上不可篡改的启动程序，BIOS 负责计算系统自检程序（POST，Power On Self Test）和系统自启动程序，因此是计算机系统启动后的第一道程式。由于不可篡改性，故程序存储在 ROM 芯片中，并且在断电后，依然可以维持原有设置。\nBIOS 主要功能是控制计算机启动后的基本程式，包括硬盘驱动（如装机过程中优先选择 DVD 或者 USB 启动盘），键盘设置，软盘驱动，内存和相关设备。\nIPMI（Intelligent Platform Management Interface） IPMI: 智慧平台管理接口（Intelligent Platform Management Interface）基于硬件的平台管理系统的一组标准化规范，可以集中控制和监视服务器。\nIPMI就是对“平台管理”这个概念的具体的规范定义，该规范定义了“平台管理”的软硬件架构，交互指令，事件格式，数据记录，能力集等。而BMC是IPMI中的一个核心部分，属于IPMI硬件架构。\nIPMI是独立于主机系统 CPU、BIOS/UEFI 和 OS 之外，可独立运行的板上部件，其核心部件即为 BMC。或者说，BMC 与其他组件如 BIOS/UEFI、CPU 等交互，都是经由 IPMI 来完成。在 IPMI 协助下，用户可以远程对关闭的服务器进行启动、重装、挂载 ISO 镜像等。\nRedFish Redfish是一种基于HTTPs服务的管理标准，利用RESTful接口实现设备管理。每个HTTPs操作都以UTF-8编码的JSON的形式，提交或返回一个资源。用于执行带外系统管理（out-of-band systems management），其适用于大规模的服务器。\nRedfish是相当于IPMI规范的一种演化。\n","categories":"","description":"","excerpt":"本文主要介绍跟baremetal相关的基本概念\nBMC（Baseboard Management Controller） 在介绍BMC之前需 …","ref":"/linux-notes/baremetal/bmc/","tags":["裸金属"],"title":"BMC概念"},{"body":"Deployment配置金丝雀发布 金丝雀发布是指控制更新过程中的滚动节奏，通过“暂停”（pause）或“继续”（resume）更新发布操作。通过一小部分的版本发布实例来观察新版本是否有异常，如果没有异常则依次发布剩余的实例。\n1. 设置发版节奏 主要是以下字段的设置：\nmaxSurge：最大发版实例数，可以创建的超出期望 Pod 个数的 Pod 数量。可以是百分比或者是数字。 maxUnavailable：最大不可用实例数。可以是百分比或数字。 minReadySeconds ：新建的 Pod 在没有任何容器崩溃的情况下就绪并被系统视为可用的最短秒数。 默认为 0（Pod 就绪后即被视为可用）。可将该值设置为5-10（秒），防止新起的pod发生crash，进而影响服务的可用性，保证集群在更新过程的稳定性。 # 例如将maxSurge设置为1，maxUnavailable设置为0 $ kubectl patch deployment myapp-deploy -p '{\"spec\": {\"strategy\": {\"rollingUpdate\": {\"maxSurge\": 1, \"maxUnavailable\": 0}}}}' 2. 升级版本并暂停 kubectl set image deployment myapp-deploy myapp=kubernetes/myapp:v3 \u0026\u0026 \\ kubectl rollout pause deployments myapp-deploy 3. 查看升级状态 $ kubectl rollout status deployment myapp-deploy Waiting for deployment \"myapp-deploy\" rollout to finish: 1 out of 3 new replicas have been updated... 4. 恢复继续发版 观察灰度的实例的流量是否正常，如果正常则继续发版，如果不正常则回滚之前的升级。\n$ kubectl rollout resume deployments myapp-deploy 5. 回滚发布 5.1. 回滚上一个版本 kubectl rollout undo deployments myapp-deploy 5.2. 查看历史版本 $ kubectl rollout history deployment myapp-deploy deployment.apps/myapp-deploy REVISION CHANGE-CAUSE 3 \u003cnone\u003e 5 \u003cnone\u003e 6 \u003cnone\u003e 5.3. 回滚指定版本 kubectl rollout undo deployment myapp-deploy --to-revision 3 参考：\nhttps://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/manage-deployment/#canary-deployments https://kubernetes.renkeju.com/chapter_5/5.3.4.Canary_release.html https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/deployment-v1/#DeploymentSpec ","categories":"","description":"","excerpt":"Deployment配置金丝雀发布 金丝雀发布是指控制更新过程中的滚动节奏，通过“暂停”（pause）或“继续”（resume）更新发布操 …","ref":"/kubernetes-notes/operation/deployment/canary-deployment/","tags":["Kubernetes"],"title":"金丝雀发布"},{"body":"1. kube-prometheus-stack简介 kube-prometheus-stack是prometheus监控k8s集群的套件，可以通过helm一键安装，同时带有监控的模板。\n各组件包括\ngrafana kube-state-metrics prometheus alertmanager node-exporter 2. 安装kube-prometheus-stack 执行以下命令\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n prometheus 示例：\n# helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n prometheus NAME: kube-prometheus-stack LAST DEPLOYED: Wed May 17 17:12:24 2023 NAMESPACE: prometheus STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace prometheus get pods -l \"release=kube-prometheus-stack\" Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026 configure Alertmanager and Prometheus instances using the Operator. 3. 查看安装结果 deployment\nkube-prometheus-stack-grafana kube-prometheus-stack-kube-state-metrics kube-prometheus-stack-operator statefulset\nprometheus-kube-prometheus-stack-prometheus alertmanager-kube-prometheus-stack-alertmanager daemonset\nkube-prometheus-stack-prometheus-node-exporter # kg all -n prometheus NAME READY STATUS RESTARTS AGE pod/alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 0 9m34s pod/kube-prometheus-stack-grafana-5bb7689dc8-lgrws 3/3 Running 0 9m35s pod/kube-prometheus-stack-kube-state-metrics-5d6578867c-25xbq 1/1 Running 0 9m35s pod/kube-prometheus-stack-operator-9c5fbdc68-nrn7h 1/1 Running 0 9m35s pod/kube-prometheus-stack-prometheus-node-exporter-8ghd8 1/1 Running 0 48s pod/kube-prometheus-stack-prometheus-node-exporter-brtp9 1/1 Running 0 29s pod/kube-prometheus-stack-prometheus-node-exporter-n4kdp 1/1 Running 0 88s pod/kube-prometheus-stack-prometheus-node-exporter-ttksv 1/1 Running 0 35s pod/prometheus-kube-prometheus-stack-prometheus-0 2/2 Running 0 9m34s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,9094/TCP,9094/UDP 9m34s service/kube-prometheus-stack-alertmanager ClusterIP 10.99.108.180 \u003cnone\u003e 9093/TCP 9m36s service/kube-prometheus-stack-grafana ClusterIP 10.110.62.28 \u003cnone\u003e 80/TCP 9m36s service/kube-prometheus-stack-kube-state-metrics ClusterIP 10.110.105.139 \u003cnone\u003e 8080/TCP 9m35s service/kube-prometheus-stack-operator ClusterIP 10.96.147.204 \u003cnone\u003e 443/TCP 9m36s service/kube-prometheus-stack-prometheus ClusterIP 10.98.235.203 \u003cnone\u003e 9090/TCP 9m36s service/kube-prometheus-stack-prometheus-node-exporter ClusterIP 10.105.99.77 \u003cnone\u003e 9100/TCP 9m36s service/prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 9m34s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/kube-prometheus-stack-prometheus-node-exporter 4 4 4 4 4 \u003cnone\u003e 9m35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/kube-prometheus-stack-grafana 1/1 1 1 9m35s deployment.apps/kube-prometheus-stack-kube-state-metrics 1/1 1 1 9m35s deployment.apps/kube-prometheus-stack-operator 1/1 1 1 9m35s NAME READY AGE statefulset.apps/alertmanager-kube-prometheus-stack-alertmanager 1/1 9m34s statefulset.apps/prometheus-kube-prometheus-stack-prometheus 1/1 9m34s 4. 登录grafana 默认账号密码\n账号：admin 密码：prom-operator 默认账号密码位于secret中，通过base64解码可得上述密码。\nkg secret -n prometheus kube-prometheus-stack-grafana -oyaml apiVersion: v1 data: admin-password: cHJvbS1vcGVyYXRvcg== admin-user: YWRtaW4= 模板内容：\npod数据：\n参考：\nhttps://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack https://github.com/prometheus-operator/kube-prometheus ","categories":"","description":"","excerpt":"1. kube-prometheus-stack简介 kube-prometheus-stack是prometheus监控k8s集群的套件， …","ref":"/kubernetes-notes/monitor/kube-promethus-stack/","tags":["Monitor"],"title":"kube-prometheus-stack的使用"},{"body":" 本文分析yurthub源码，第一部分。\n本文以commit id：180282663457080119a1bc6076cce20c922b5c50， 对应版本tag: v1.2.1 的源码分析yurthub的实现逻辑。\nyurthub是部署在每个边缘节点上用来实现边缘自治的组件。在云边通信正常的情况下实现apiserver的请求转发，断网的情况下通过本地的缓存数据保证节点上容器的正常运行。\n基本架构图：\npkg包中yurthub代码目录结构；\nyurthub ├── cachemanager # cache 管理器， ├── certificate # 证书token管理 ├── filter ├── gc # GCManager ├── healthchecker # cloud apiserver 探火机制 ├── kubernetes # ├── metrics ├── network # 网络iptables配置 ├── otaupdate ├── poolcoordinator ├── proxy # 核心代码，反向代理机制，包括remote proxy和local proxy ├── server # yurthub server ├── storage # 本地存储的实现 ├── tenant ├── transport └── util 1. NewCmdStartYurtHub openyurt的代码风格与k8s的一致，由cmd为入口，pkg为主要的实现逻辑。\n以下是cmd的main函数。\nfunc main() { newRand := rand.New(rand.NewSource(time.Now().UnixNano())) newRand.Seed(time.Now().UnixNano()) cmd := app.NewCmdStartYurtHub(server.SetupSignalContext()) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { panic(err) } } main 函数主要创建 NewCmdStartYurtHub 对象。NewCmd的函数一般都包含以下的几个部分，运行顺序从上到下：\nNewYurtHubOptions：创建option参数对象，主要用于flag参数解析到option的结构体。\nyurtHubOptions.AddFlags(cmd.Flags())：添加AddFlags，设置flag参数信息。\nyurtHubOptions.Validate()：校验flag解析后的option的参数合法性。\nyurtHubCfg, err := config.Complete(yurtHubOptions)：将option的参数转换为config的对象。\nRun(ctx, yurtHubCfg)：基于config执行run函数，运行cmd的核心逻辑。\n// NewCmdStartYurtHub creates a *cobra.Command object with default parameters func NewCmdStartYurtHub(ctx context.Context) *cobra.Command { yurtHubOptions := options.NewYurtHubOptions() cmd := \u0026cobra.Command{ Use: projectinfo.GetHubName(), Short: \"Launch \" + projectinfo.GetHubName(), Long: \"Launch \" + projectinfo.GetHubName(), Run: func(cmd *cobra.Command, args []string) { if yurtHubOptions.Version { fmt.Printf(\"%s: %#v\\n\", projectinfo.GetHubName(), projectinfo.Get()) return } fmt.Printf(\"%s version: %#v\\n\", projectinfo.GetHubName(), projectinfo.Get()) cmd.Flags().VisitAll(func(flag *pflag.Flag) { klog.V(1).Infof(\"FLAG: --%s=%q\", flag.Name, flag.Value) }) if err := yurtHubOptions.Validate(); err != nil { klog.Fatalf(\"validate options: %v\", err) } yurtHubCfg, err := config.Complete(yurtHubOptions) if err != nil { klog.Fatalf(\"complete %s configuration error, %v\", projectinfo.GetHubName(), err) } klog.Infof(\"%s cfg: %#+v\", projectinfo.GetHubName(), yurtHubCfg) if err := Run(ctx, yurtHubCfg); err != nil { klog.Fatalf(\"run %s failed, %v\", projectinfo.GetHubName(), err) } }, } yurtHubOptions.AddFlags(cmd.Flags()) return cmd } 以上flag、option、config的构建函数此处不做分析，以下分析run函数的逻辑。\n2. Run(ctx, yurtHubCfg) Run函数部分主要构建了几个manager，每个manager各司其职，负责对应的逻辑。有的manager在该函数中直接构建后执行manager.run的逻辑。有的则作为参数传入下一级函数中再执行manager.run函数。\n主要包括以下的manager：\ntransportManager\ncloudHealthChecker\nrestConfigMgr\ncacheMgr\ngcMgr\ntenantMgr\nNetworkMgr\n每个manager的实现细节此处暂不做分析。\n此处先贴一下完整源码，避免读者还需要去翻代码。\n代码：/cmd/yurthub/app/start.go\n// Run runs the YurtHubConfiguration. This should never exit func Run(ctx context.Context, cfg *config.YurtHubConfiguration) error { defer cfg.CertManager.Stop() trace := 1 klog.Infof(\"%d. new transport manager\", trace) // 构造NewTransportManager transportManager, err := transport.NewTransportManager(cfg.CertManager, ctx.Done()) if err != nil { return fmt.Errorf(\"could not new transport manager, %w\", err) } trace++ klog.Infof(\"%d. prepare cloud kube clients\", trace) cloudClients, err := createClients(cfg.HeartbeatTimeoutSeconds, cfg.RemoteServers, cfg.CoordinatorServerURL, transportManager) if err != nil { return fmt.Errorf(\"failed to create cloud clients, %w\", err) } trace++ var cloudHealthChecker healthchecker.MultipleBackendsHealthChecker if cfg.WorkingMode == util.WorkingModeEdge { klog.Infof(\"%d. create health checkers for remote servers and pool coordinator\", trace) cloudHealthChecker, err = healthchecker.NewCloudAPIServerHealthChecker(cfg, cloudClients, ctx.Done()) if err != nil { return fmt.Errorf(\"could not new cloud health checker, %w\", err) } } else { klog.Infof(\"%d. disable health checker for node %s because it is a cloud node\", trace, cfg.NodeName) // In cloud mode, cloud health checker is not needed. // This fake checker will always report that the cloud is healthy and pool coordinator is unhealthy. cloudHealthChecker = healthchecker.NewFakeChecker(true, make(map[string]int)) } trace++ klog.Infof(\"%d. new restConfig manager\", trace) restConfigMgr, err := hubrest.NewRestConfigManager(cfg.CertManager, cloudHealthChecker) if err != nil { return fmt.Errorf(\"could not new restConfig manager, %w\", err) } trace++ var cacheMgr cachemanager.CacheManager if cfg.WorkingMode == util.WorkingModeEdge { klog.Infof(\"%d. new cache manager with storage wrapper and serializer manager\", trace) cacheMgr = cachemanager.NewCacheManager(cfg.StorageWrapper, cfg.SerializerManager, cfg.RESTMapperManager, cfg.SharedFactory) } else { klog.Infof(\"%d. disable cache manager for node %s because it is a cloud node\", trace, cfg.NodeName) } trace++ if cfg.WorkingMode == util.WorkingModeEdge { klog.Infof(\"%d. new gc manager for node %s, and gc frequency is a random time between %d min and %d min\", trace, cfg.NodeName, cfg.GCFrequency, 3*cfg.GCFrequency) gcMgr, err := gc.NewGCManager(cfg, restConfigMgr, ctx.Done()) if err != nil { return fmt.Errorf(\"could not new gc manager, %w\", err) } // 直接运行manager gcMgr.Run() } else { klog.Infof(\"%d. disable gc manager for node %s because it is a cloud node\", trace, cfg.NodeName) } trace++ klog.Infof(\"%d. new tenant sa manager\", trace) tenantMgr := tenant.New(cfg.TenantNs, cfg.SharedFactory, ctx.Done()) trace++ var coordinatorHealthCheckerGetter func() healthchecker.HealthChecker = getFakeCoordinatorHealthChecker var coordinatorTransportManagerGetter func() transport.Interface = getFakeCoordinatorTransportManager var coordinatorGetter func() poolcoordinator.Coordinator = getFakeCoordinator if cfg.EnableCoordinator { klog.Infof(\"%d. start to run coordinator\", trace) trace++ coordinatorInformerRegistryChan := make(chan struct{}) // coordinatorRun will register secret informer into sharedInformerFactory, and start a new goroutine to periodically check // if certs has been got from cloud APIServer. It will close the coordinatorInformerRegistryChan if the secret channel has // been registered into informer factory. coordinatorHealthCheckerGetter, coordinatorTransportManagerGetter, coordinatorGetter = coordinatorRun(ctx, cfg, restConfigMgr, cloudHealthChecker, coordinatorInformerRegistryChan) // wait for coordinator informer registry klog.Infof(\"waiting for coordinator informer registry\") \u003c-coordinatorInformerRegistryChan klog.Infof(\"coordinator informer registry finished\") } // Start the informer factory if all informers have been registered cfg.SharedFactory.Start(ctx.Done()) cfg.YurtSharedFactory.Start(ctx.Done()) klog.Infof(\"%d. new reverse proxy handler for remote servers\", trace) // 将之前构造的manager作为参数构建yurtProxyHandler yurtProxyHandler, err := proxy.NewYurtReverseProxyHandler( cfg, cacheMgr, transportManager, cloudHealthChecker, tenantMgr, coordinatorGetter, coordinatorTransportManagerGetter, coordinatorHealthCheckerGetter, ctx.Done()) if err != nil { return fmt.Errorf(\"could not create reverse proxy handler, %w\", err) } trace++ if cfg.NetworkMgr != nil { cfg.NetworkMgr.Run(ctx.Done()) } klog.Infof(\"%d. new %s server and begin to serve\", trace, projectinfo.GetHubName()) // 基于yurtProxyHandler运行一个http server. if err := server.RunYurtHubServers(cfg, yurtProxyHandler, restConfigMgr, ctx.Done()); err != nil { return fmt.Errorf(\"could not run hub servers, %w\", err) } \u003c-ctx.Done() klog.Infof(\"hub agent exited\") return nil } 除了上述的各种manager的构造及运行外，run函数中还构建了yurtProxyHandler，最终执行RunYurtHubServers运行一组不会退出的http server。以下先不对manager的实现做展开，而直接分析RunYurtHubServers的逻辑。RunYurtHubServers的代码在pkg包中。\n3. RunYurtHubServers RunYurtHubServers就是一个传统的http server的运行逻辑，主要包括几个不同类型的http server。http server的运行逻辑可以概括如下：\nhubServerHandler := mux.NewRouter()： 新建路由创建handler\nregisterHandlers(hubServerHandler, cfg, rest)： 注册路由\nYurtHubServerServing.Serve：执行http server.Serve函数启动一个server服务。\nhttp server分为两类：\nyurthub http server: yurthub metrics, healthz的接口。\nyurthub proxy server: 代理kube-apiserver的请求。\n3.1. YurtHubServerServing hubServerHandler := mux.NewRouter() registerHandlers(hubServerHandler, cfg, rest) // start yurthub http server for serving metrics, pprof. if cfg.YurtHubServerServing != nil { if err := cfg.YurtHubServerServing.Serve(hubServerHandler, 0, stopCh); err != nil { return err } } registerHandlers的路由内容如下：\n// registerHandler registers handlers for yurtHubServer, and yurtHubServer can handle requests like profiling, healthz, update token. func registerHandlers(c *mux.Router, cfg *config.YurtHubConfiguration, rest *rest.RestConfigManager) { // register handlers for update join token c.Handle(\"/v1/token\", updateTokenHandler(cfg.CertManager)).Methods(\"POST\", \"PUT\") // register handler for health check c.HandleFunc(\"/v1/healthz\", healthz).Methods(\"GET\") c.Handle(\"/v1/readyz\", readyz(cfg.CertManager)).Methods(\"GET\") // register handler for profile if cfg.EnableProfiling { profile.Install(c) } // register handler for metrics c.Handle(\"/metrics\", promhttp.Handler()) // register handler for ota upgrade c.Handle(\"/pods\", ota.GetPods(cfg.StorageWrapper)).Methods(\"GET\") c.Handle(\"/openyurt.io/v1/namespaces/{ns}/pods/{podname}/upgrade\", ota.HealthyCheck(rest, cfg.NodeName, ota.UpdatePod)).Methods(\"POST\") } 以上路由不做深入分析。\n3.2. YurtHubProxyServerServing YurtHubProxyServerServing主要代理kube-apiserver的转发请求。\n// start yurthub proxy servers for forwarding requests to cloud kube-apiserver if cfg.WorkingMode == util.WorkingModeEdge { proxyHandler = wrapNonResourceHandler(proxyHandler, cfg, rest) } if cfg.YurtHubProxyServerServing != nil { if err := cfg.YurtHubProxyServerServing.Serve(proxyHandler, 0, stopCh); err != nil { return err } } 以下分析yurtProxyHandler的逻辑。\n3.3. NewYurtReverseProxyHandler NewYurtReverseProxyHandler主要创建了http handler 代理所有转发请求。\n1、创建Load Balancer，主要用来转发apiserver的请求。\nlb, err := remote.NewLoadBalancer( yurtHubCfg.LBMode, yurtHubCfg.RemoteServers, localCacheMgr, transportMgr, coordinatorGetter, cloudHealthChecker, yurtHubCfg.FilterManager, yurtHubCfg.WorkingMode, stopCh) 2、创建local Proxy，主要用来转发本地缓存的请求。\n// When yurthub works in Edge mode, we may use local proxy or pool proxy to handle // the request when offline. localProxy = local.NewLocalProxy(localCacheMgr, cloudHealthChecker.IsHealthy, isCoordinatorHealthy, yurtHubCfg.MinRequestTimeout, ) localProxy = local.WithFakeTokenInject(localProxy, yurtHubCfg.SerializerManager) 3、创建yurtReverseProxy\nyurtProxy := \u0026yurtReverseProxy{ resolver: resolver, loadBalancer: lb, cloudHealthChecker: cloudHealthChecker, coordinatorHealtCheckerGetter: coordinatorHealthCheckerGetter, localProxy: localProxy, poolProxy: poolProxy, maxRequestsInFlight: yurtHubCfg.MaxRequestInFlight, isCoordinatorReady: isCoordinatorReady, enablePoolCoordinator: yurtHubCfg.EnableCoordinator, tenantMgr: tenantMgr, workingMode: yurtHubCfg.WorkingMode, } return yurtProxy.buildHandlerChain(yurtProxy), nil 4. yurtReverseProxy yurtReverseProxy主要是作为实现反向代理的结构体。\ntype yurtReverseProxy struct { resolver apirequest.RequestInfoResolver loadBalancer remote.LoadBalancer cloudHealthChecker healthchecker.MultipleBackendsHealthChecker coordinatorHealtCheckerGetter func() healthchecker.HealthChecker localProxy http.Handler poolProxy http.Handler maxRequestsInFlight int tenantMgr tenant.Interface isCoordinatorReady func() bool workingMode hubutil.WorkingMode enablePoolCoordinator bool } 反向代理服务\nfunc (p *yurtReverseProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) { if p.workingMode == hubutil.WorkingModeCloud { p.loadBalancer.ServeHTTP(rw, req) return } switch { case util.IsKubeletLeaseReq(req): p.handleKubeletLease(rw, req) case util.IsEventCreateRequest(req): p.eventHandler(rw, req) case util.IsPoolScopedResouceListWatchRequest(req): p.poolScopedResouceHandler(rw, req) case util.IsSubjectAccessReviewCreateGetRequest(req): p.subjectAccessReviewHandler(rw, req) default: // For resource request that do not need to be handled by pool-coordinator, // handling the request with cloud apiserver or local cache. if p.cloudHealthChecker.IsHealthy() { p.loadBalancer.ServeHTTP(rw, req) } else { p.localProxy.ServeHTTP(rw, req) } } } 核心逻辑：如果是云端apiserver可以访问的通，则通过loadbalaner来转发，否则就通过localproxy来转发读取本地节点的数据。\n5. LoadBalancer LoadBalancer是个本地的负载均衡逻辑，通过轮询的方式去请求cloud的apiserver，当云边网络通信是正常的时候反向代理apiserver的请求，并做本地缓存持久化。断网的时候则读取本地的缓存数据。\nbackends：真实反向代理的后端\nalgo: 处理负载均衡策略，轮询或者按优先级\nlocalCacheMgr: 本地缓存管理的manager\n代码：/pkg/yurthub/proxy/remote/loadbalancer.go\ntype loadBalancer struct { backends []*util.RemoteProxy algo loadBalancerAlgo localCacheMgr cachemanager.CacheManager filterManager *manager.Manager coordinatorGetter func() poolcoordinator.Coordinator workingMode hubutil.WorkingMode stopCh \u003c-chan struct{} } 5.1. NewLoadBalancer NewLoadBalancer构建一个remote的反向代理，主要包含添加romote server proxy和处理负载均衡策略两部分。\n1、添加多个apiserver的地址，创建remote proxy实现反向代理操作。\nbackends := make([]*util.RemoteProxy, 0, len(remoteServers)) for i := range remoteServers { b, err := util.NewRemoteProxy(remoteServers[i], lb.modifyResponse, lb.errorHandler, transportMgr, stopCh) if err != nil { klog.Errorf(\"could not new proxy backend(%s), %v\", remoteServers[i].String(), err) continue } backends = append(backends, b) } 2、处理负载均衡策略：\nvar algo loadBalancerAlgo switch lbMode { case \"rr\": algo = \u0026rrLoadBalancerAlgo{backends: backends, checker: healthChecker} case \"priority\": algo = \u0026priorityLoadBalancerAlgo{backends: backends, checker: healthChecker} default: algo = \u0026rrLoadBalancerAlgo{backends: backends, checker: healthChecker} } 5.2. loadBalancer.ServeHTTP loadBalancer实现ServeHTTP的接口，通过负载均衡策略挑选出一个可用的反向代理backend。再调用backend的ServeHTTP方法实现具体的反向代理操作。\n// pick a remote proxy based on the load balancing algorithm. rp := lb.algo.PickOne() rp.ServeHTTP(rw, req) 5.3. errorHandler 如果请求apiserver失败，当verb=get/list, 则读取cache中的内容。\nfunc (lb *loadBalancer) errorHandler(rw http.ResponseWriter, req *http.Request, err error) { klog.Errorf(\"remote proxy error handler: %s, %v\", hubutil.ReqString(req), err) if lb.localCacheMgr == nil || !lb.localCacheMgr.CanCacheFor(req) { rw.WriteHeader(http.StatusBadGateway) return } ctx := req.Context() if info, ok := apirequest.RequestInfoFrom(ctx); ok { if info.Verb == \"get\" || info.Verb == \"list\" { # 读取cache内容 if obj, err := lb.localCacheMgr.QueryCache(req); err == nil { hubutil.WriteObject(http.StatusOK, obj, rw, req) return } } } rw.WriteHeader(http.StatusBadGateway) } 5.4. QueryCache // QueryCache get runtime object from backend storage for request func (cm *cacheManager) QueryCache(req *http.Request) (runtime.Object, error) { ctx := req.Context() info, ok := apirequest.RequestInfoFrom(ctx) if !ok || info == nil || info.Resource == \"\" { return nil, fmt.Errorf(\"failed to get request info for request %s\", util.ReqString(req)) } if !info.IsResourceRequest { return nil, fmt.Errorf(\"failed to QueryCache for getting non-resource request %s\", util.ReqString(req)) } # 根据verb查询storage中的数据 switch info.Verb { case \"list\": return cm.queryListObject(req) case \"get\", \"patch\", \"update\": return cm.queryOneObject(req) default: return nil, fmt.Errorf(\"failed to QueryCache, unsupported verb %s of request %s\", info.Verb, util.ReqString(req)) } } 5.5. 查询storage中的数据 func (cm *cacheManager) queryOneObject(req *http.Request) (runtime.Object, error) { ... klog.V(4).Infof(\"component: %s try to get key: %s\", comp, key.Key()) obj, err := cm.storage.Get(key) if err != nil { klog.Errorf(\"failed to get obj %s from storage, %v\", key.Key(), err) return nil, err } ... } 目前存储有两种接口实现，一个是本地磁盘存储，一个是etcd存储。以下以磁盘存储为例分析。\n代码：/pkg/yurthub/storage/disk/storage.go\n// Get will get content from the regular file that specified by key. // If key points to a dir, return ErrKeyHasNoContent. func (ds *diskStorage) Get(key storage.Key) ([]byte, error) { if err := utils.ValidateKey(key, storageKey{}); err != nil { return []byte{}, storage.ErrKeyIsEmpty } storageKey := key.(storageKey) if !ds.lockKey(storageKey) { return nil, storage.ErrStorageAccessConflict } defer ds.unLockKey(storageKey) path := filepath.Join(ds.baseDir, storageKey.Key()) buf, err := ds.fsOperator.Read(path) switch err { case nil: return buf, nil case fs.ErrNotExists: return nil, storage.ErrStorageNotFound case fs.ErrIsNotFile: return nil, storage.ErrKeyHasNoContent default: return buf, fmt.Errorf(\"failed to read file at %s, %v\", path, err) } } 6. RemoteProxy RemoteProxy实现一个具体的反向代理操作。\n字段说明：\nreverseProxy：http的ReverseProxy\nremoteServer：apiserver的地址\n代码：/pkg/yurthub/proxy/util/remote.go\n// RemoteProxy is an reverse proxy for remote server type RemoteProxy struct { reverseProxy *httputil.ReverseProxy remoteServer *url.URL currentTransport http.RoundTripper bearerTransport http.RoundTripper upgradeHandler *proxy.UpgradeAwareHandler bearerUpgradeHandler *proxy.UpgradeAwareHandler stopCh \u003c-chan struct{} } 实现ReverseProxy的ServeHTTP接口。\nfunc (rp *RemoteProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) { if httpstream.IsUpgradeRequest(req) { klog.V(5).Infof(\"get upgrade request %s\", req.URL) if isBearerRequest(req) { rp.bearerUpgradeHandler.ServeHTTP(rw, req) } else { rp.upgradeHandler.ServeHTTP(rw, req) } return } rp.reverseProxy.ServeHTTP(rw, req) } 实现错误处理的接口。\nfunc (r *responder) Error(w http.ResponseWriter, req *http.Request, err error) { klog.Errorf(\"failed while proxying request %s, %v\", req.URL, err) http.Error(w, err.Error(), http.StatusInternalServerError) } 7. LocalProxy LocalProxy是一个当云边网络断开的时候，用于处理本地kubelet请求的数据的代理。\n字段说明：\ncacheMgr：主要包含本地cache的一个处理管理器。 代码：/pkg/yurthub/proxy/local/local.go\n// LocalProxy is responsible for handling requests when remote servers are unhealthy type LocalProxy struct { cacheMgr manager.CacheManager isCloudHealthy IsHealthy isCoordinatorReady IsHealthy minRequestTimeout time.Duration } LocalProxy实现ServeHTTP接口，根据不同的k8s请求类型，执行不同的操作：\nwatch：lp.localWatch(w, req)\ncreate：lp.localPost(w, req)\ndelete, deletecollection: localDelete(w, req)\nlist., get, update：lp.localReqCache(w, req)\n// ServeHTTP implements http.Handler for LocalProxy func (lp *LocalProxy) ServeHTTP(w http.ResponseWriter, req *http.Request) { var err error ctx := req.Context() if reqInfo, ok := apirequest.RequestInfoFrom(ctx); ok \u0026\u0026 reqInfo != nil \u0026\u0026 reqInfo.IsResourceRequest { klog.V(3).Infof(\"go into local proxy for request %s\", hubutil.ReqString(req)) switch reqInfo.Verb { case \"watch\": err = lp.localWatch(w, req) case \"create\": err = lp.localPost(w, req) case \"delete\", \"deletecollection\": err = localDelete(w, req) default: // list, get, update err = lp.localReqCache(w, req) } if err != nil { klog.Errorf(\"could not proxy local for %s, %v\", hubutil.ReqString(req), err) util.Err(err, w, req) } } else { klog.Errorf(\"local proxy does not support request(%s), requestInfo: %s\", hubutil.ReqString(req), hubutil.ReqInfoString(reqInfo)) util.Err(apierrors.NewBadRequest(fmt.Sprintf(\"local proxy does not support request(%s)\", hubutil.ReqString(req))), w, req) } } 7.1. localReqCache 当边缘网络断连的时候，kubelet执行get list的操作时，通过localReqCache请求本地缓存的数据，返回给kubelet对应的k8s元数据。\n// localReqCache handles Get/List/Update requests when remote servers are unhealthy func (lp *LocalProxy) localReqCache(w http.ResponseWriter, req *http.Request) error { if !lp.cacheMgr.CanCacheFor(req) { klog.Errorf(\"can not cache for %s\", hubutil.ReqString(req)) return apierrors.NewBadRequest(fmt.Sprintf(\"can not cache for %s\", hubutil.ReqString(req))) } obj, err := lp.cacheMgr.QueryCache(req) if errors.Is(err, storage.ErrStorageNotFound) || errors.Is(err, hubmeta.ErrGVRNotRecognized) { klog.Errorf(\"object not found for %s\", hubutil.ReqString(req)) reqInfo, _ := apirequest.RequestInfoFrom(req.Context()) return apierrors.NewNotFound(schema.GroupResource{Group: reqInfo.APIGroup, Resource: reqInfo.Resource}, reqInfo.Name) } else if err != nil { klog.Errorf(\"failed to query cache for %s, %v\", hubutil.ReqString(req), err) return apierrors.NewInternalError(err) } else if obj == nil { klog.Errorf(\"no cache object for %s\", hubutil.ReqString(req)) return apierrors.NewInternalError(fmt.Errorf(\"no cache object for %s\", hubutil.ReqString(req))) } return util.WriteObject(http.StatusOK, obj, w, req) } 核心代码为：\n查询本地缓存，返回缓存数据。\nobj, err := lp.cacheMgr.QueryCache(req) return util.WriteObject(http.StatusOK, obj, w, req) 总结 yurthub是实现边缘断网自治的核心组件，核心逻辑是kubelet向apiserver的请求会通过yurhub进行转发，如果apiserver的接口可通，则将请求结果返回，并存储到本地，如果接口不可通，则读取本地的数据。\nyurthub本质是一个反向代理的http server, 核心逻辑主要包括 ：\n- proxy: 反向代理的实现 - cachemanager：cache的实现 - storage：本地存储的实现 参考：\nhttps://github.com/openyurtio/openyurt ","categories":"","description":"","excerpt":" 本文分析yurthub源码，第一部分。\n本文以commit …","ref":"/kubernetes-notes/edge/openyurt/code-analysis/yurthub-code-analysis-1/","tags":["OpenYurt"],"title":"OpenYurt之YurtHub源码分析"},{"body":"1. 简介 linux系统上常用tcpdump抓包来分析网络问题。本文基于网络文章整理，主要介绍tcpdump抓包的常用命令及参数。\n以下是数据包在操作系统层面的流程：\n网卡nic -\u003e tcpdump -\u003e iptables(netfilter) -\u003e app -\u003e iptables(netfilter) -\u003e tcpdump -\u003e 网卡nic\n2. tcpdump常用参数及命令 2.1. 指定网卡(-i)和主机(host) tcpdump默认会将IP反向解析为域名，可以用-nn禁止反向解析。\n-i：指定网卡\nhost：指定主机\n-nn：禁止反向解析域名\n-v或-vv：显示抓包的详细信息\n-w: 写入文件（.pcap或.cap），供wireshark分析。\ntcpdump -i any host 192.168.1.1 #-i指定网卡为所有 tcpdump -i eth0 host 192.168.1.1 #-i指定网卡为eth0 tcpdump -i eth0 host 192.168.1.1 -nn -v -w client.pcap # 写入文件 2.2. 指定来源IP或目的IP、网段 src：指定来源IP\ndst: 指定目标IP\nnet: 指定网段\n-s : 指定抓包字节数，-s 0不限字节数，抓完整的包。例如icmp 大小为84字节\nport: 指定端口\nportrange: 指定端口范围\n协议：tcp, udp, icmp\n# 指定源IP tcpdump -nn -i any src host 192.168.1.1 # 指定目标IP tcpdump -nn -i any dst 192.168.1.1 # 指定网段 tcpdump -nn -i any net 192.168.1.1/32 # 指定字节数 tcpdump -nn -i any -s 84 host 192.168.1.1 # 84表示icmp的包 # 指定协议 tcpdump -nn -i any -s 0 icmp #只抓icmp协议 tcpdump -nn -i any -s 60 tcp port 80 #tcp协议，这里只抓60个头部字节 tcpdump -nn -i any -s 0 udp port 22 # udp协议 # 指定端口或范围 tcpdump -nn -i any -s 0 port 22 tcpdump -nn -i any tcp portrange 53-80 2.3. 指定抓包数量、抓包大小、及轮询抓包 -c: 指定抓多少个包\n-W: 最多写入多少个抓包文件，以MB为单位\n-C：写入到抓包文件的大小上限\n-G：参数指定间隔多少秒轮询保存一次文件，通常是以时间格式命令\n# 指定抓2个包 tcpdump -i any -s 0 net 192.168.1.1/32 -c 2 # 指定写入到文件的大小上限为1M tcpdump -i any -s 0 -C 1 -v -w client.pcap # 指定写入到10个抓包文件，每个文件只抓1M，循环写入 tcpdump -i any -s 0 -C 1M -v -W 10 -w client.pcap # 参数指定间隔多少秒轮询保存一次文件，通常是以时间格式命令 tcpdump -nn -i any -s 0 -G 5 -Z root -v -w %m-%d-%H:%M:%S.pcap #每隔五秒保存一次文件 2.4. tcpdump的逻辑表达式(or、and、not) and: 与\nor: 或\nnot: 非\n# 与 tcpdump -nn -i any -s 0 host 192.168.1.1 and icmp # 或 tcpdump -nn -i any -s 0 host 192.168.1.1 or icmp or src net 192.168.1.1/32 # 非 tcpdump -nn -i any -s 0 ! net 172.16.0.0/16 and icmp and ! tcp 3. Flags标记解读 Flags 含义 [S] SYN [.] ACK [S.] SYN、ACK [P.] PUSH [R.] RST [F.] FIN [DF] Don't Fragment(不分片)，当DF=0时，允许分片 [FP.] FIN、PUSH、ACK 参考：\nhttps://www.tcpdump.org/manpages/tcpdump.1.html\n抓包神器TCPDUMP的分析总结-涵盖各大使用场景、高级用法\n","categories":"","description":"","excerpt":"1. 简介 linux系统上常用tcpdump抓包来分析网络问题。本文基于网络文章整理，主要介绍tcpdump抓包的常用命令及参数。\n以下是 …","ref":"/linux-notes/network/tcpdump/","tags":["network"],"title":"tcpdump抓包流程"},{"body":"1. 安装helm curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash 2. 基本概念 Helm是用来管理k8s集群上的软件包。\nChart:代表helm软件包\nRepository：软件包的存放仓库\nRelease:运行在k8s上的一个发布实例。\n3. helm命令 Usage: helm [command] Available Commands: completion generate autocompletion scripts for the specified shell create create a new chart with the given name dependency manage a chart's dependencies env helm client environment information get download extended information of a named release help Help about any command history fetch release history install install a chart lint examine a chart for possible issues list list releases package package a chart directory into a chart archive plugin install, list, or uninstall Helm plugins pull download a chart from a repository and (optionally) unpack it in local directory push push a chart to remote registry login to or logout from a registry repo add, list, remove, update, and index chart repositories rollback roll back a release to a previous revision search search for a keyword in charts show show information of a chart status display the status of the named release template locally render templates test run tests for a release uninstall uninstall a release upgrade upgrade a release verify verify that a chart at the given path has been signed and is valid version print the client version information 4. 常用命令 4.1. helm search helm search hub：从 Artifact Hub 中查找并列出 helm charts。支持模糊匹配。 helm search hub wordpress helm search repo：基于指定仓库进行搜索。 helm repo add brigade https://brigadecore.github.io/charts helm search repo brigade # 列出所有版本 helm search repo apisix -l 4.2. helm install/uninstall helm install \u003crelease_name\u003e \u003cchart_name\u003e # 示例 helm install happy-panda bitnami/wordpress - # uninstall helm uninstall RELEASE_NAME 安装自定义chart\nhelm install -f values.yaml bitnami/wordpress --generate-name # 本地 chart 压缩包 helm install foo foo-0.1.1.tgz # 解压后的 chart 目录 helm install foo path/to/foo # 完整的 URL helm install foo https://example.com/charts/foo-1.2.3.tgz 4.3. helm upgrade helm upgrade happy-panda bitnami/wordpress 4.4. helm rollback helm rollback \u003cRELEASE\u003e [REVISION] [flags] 4.5. helm repo helm repo add dev https://example.com/dev-charts helm repo list helm repo remove 4.6. helm pull 从仓库下载并（可选）在本地目录解压。\nhelm pull [chart URL | repo/chartname] helm pull [chart URL | repo/chartname] --version 5. 创建chart 5.1. 初始化chart helm create mychart 查看生成的文件目录：\nmychart |-- charts # 目录用于存放所依赖的子chart |-- Chart.yaml # 描述这个 Chart 的相关信息、包括名字、描述信息、版本等 |-- templates | |-- deployment.yaml | |-- _helpers.tpl # 模板助手文件，定义的值可在模板中使用 | |-- hpa.yaml | |-- ingress.yaml | |-- NOTES.txt # Chart 部署到集群后的一些信息，例如：如何使用、列出缺省值 | |-- serviceaccount.yaml | |-- service.yaml | `-- tests | `-- test-connection.yaml `-- values.yaml # 模板的值文件，这些值会在安装时应用到 GO 模板生成部署文件 移除默认模板文件，并添加自己的模板文件。\nrm -rf mysubchart/templates/* 5.2. 调试模板 helm lint 是验证chart是否遵循最佳实践的首选工具。 helm template --debug 在本地测试渲染chart模板。 helm install --dry-run --debug：我们已经看到过这个技巧了，这是让服务器渲染模板的好方法，然后返回生成的清单文件。 helm get manifest: 这是查看安装在服务器上的模板的好方法。 参考：\nHelm | 安装Helm Helm | 使用Helm ","categories":"","description":"","excerpt":"1. 安装helm curl …","ref":"/kubernetes-notes/operation/helm/helm-usage/","tags":["Kubernetes"],"title":"helm的使用"},{"body":" 本文主要介绍通过k8s来部署apisix及apisix-ingress-controller，使用apisix作为k8s内Pod互相访问的网关。\n1. 环境准备 1.1. 安装helm 参考：Helm | 安装\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash helm添加仓库\nhelm repo add apisix https://charts.apiseven.com helm repo update 1.2. 安装ETCD 可以提前准备好etcd环境，也可以使用apisix官方的helm命令安装，但是需要存在默认是storageclass来提供pv挂载。\n2. 一键安装全部 helm install apisix apisix/apisix --set gateway.type=NodePort --set ingress-controller.enabled=true --namespace=apisix --create-namespace 或通过以下方式分别安装各组件。\n3. 安装apisix 参考：apisix-helm-chart/apisix.md at master · apache/apisix-helm-chart · GitHub\n3.1. 安装 helm install apisix apisix/apisix --namespace apisix --create-namespace 卸载\nhelm uninstall apisix --namespace apisix 3.2. 修改配置 kubectl edit cm apisix-napisix 可选：\n修改apisix端口。\n修改etcd地址。\n修改admin key值。\n修改日志路径。\n使用etcd存储stream配置或者静态文件存储stream配置\napisix: node_listen: 8000 # APISIX listening port config_center: etcd # etcd: use etcd to store the config value # yaml: fetch the config value from local yaml file `/your_path/conf/apisix.yaml` etcd: host: \"http://foo:2379\" # etcd address admin_key - name: \"admin\" key: newsupersecurekey # 请修改 key 的值 role: admin nginx_config: error_log: /var/log/apisix_error.log http: access_log: /var/log/apisix_access.log access_log_format: \"$time_iso8601|$remote_addr - $remote_user|$http_host|\\\"$request\\\"|$status|$body_bytes_sent|$request_time|\\\"$http_referer\\\"|\\\"$http_user_agent\\\"|$upstream_addr|$upstream_status|$upstream_response_time|\\\"$upstream_scheme://$upstream_host$upstream_uri\\\"\" plugin_attr: log-rotate: interval: 3600 # rotate interval (unit: second) max_kept: 48 # max number of log files will be kept enable_compression: false 4. 安装apisix-ingress-controller 参考：apisix-helm-chart/apisix-ingress-controller.md at master · apache/apisix-helm-chart · GitHub\n4.1. 安装 helm install apisix-ingress-controller apisix/apisix-ingress-controller --namespace apisix --create-namespace 卸载\nhelm uninstall apisix-ingress-controller --namespace apisix 4.2. 修改配置 kubectl edit cm apisix-configmap -napisix 配置\napisix地址\napisix admin key\ndefault_cluster_base_url: http://apisix-admin.apisix.svc.cluster.local:9180/apisix/admin default_cluster_admin_key: \"edd1c9f034335f136f87ad84b625c8f1\" 5. 安装dashboard 5.1. 安装 helm repo add apisix https://charts.apiseven.com helm repo update helm install apisix-dashboard apisix/apisix-dashboard --namespace apisix --create-namespace 卸载\nhelm uninstall apisix-dashboard --namespace apisix 5.2. 修改配置 kubectl edit cm apisix-dashboard -napisix 端口\netcd地址\n登录账号密码\ndata: conf.yaml: |- conf: listen: host: 0.0.0.0 port: 9000 etcd: prefix: \"/apisix\" endpoints: - 10.65.240.210:2379 log: error_log: level: warn file_path: /dev/stderr access_log: file_path: /dev/stdout authentication: secert: secert expire_time: 3600 users: - username: admin password: admin 6. 查看helm安装列表 # helm list -n apisix NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION apisix apisix 1 2022-08-23 16:19:47.678174579 +0800 +08 deployed apisix-0.11.0 2.15.0 apisix-dashboard apisix 1 2022-08-23 20:36:37.55042356 +0800 +08 deployed apisix-dashboard-0.6.0 2.13.0 参考：\nhttps://github.com/apache/apisix-helm-chart\nhttps://apisix.apache.org/zh/docs/apisix/installation-guide/\nhttps://github.com/apache/apisix-ingress-controller/blob/master/install.md\n","categories":"","description":"","excerpt":" 本文主要介绍通过k8s来部署apisix及apisix-ingress-controller，使用apisix作为k8s内Pod互相访问的 …","ref":"/kubernetes-notes/network/gateway/install/","tags":["ApiSix"],"title":"安装APISIX"},{"body":" 本文主要介绍部署openyurt组件到k8s集群中。\n1. 给云端节点和边缘节点打标签 openyurt将k8s节点分为云端节点和边缘节点，云端节点主要运行一些云端的业务，边缘节点运行边缘业务。当与 apiserver 断开连接时，只有运行在边缘自治的节点上的Pod才不会被驱逐。通过打 openyurt.io/is-edge-worker 的标签的方式来区分，false表示云端节点，true表示边缘节点。\n云端组件：\nyurt-controller-manager\nyurt-tunnel-server\n边缘组件：\nyurt-hub\nyurt-tunnel-agent\n1.1. openyurt.io/is-edge-worker节点标签 # 云端节点，值为false kubectl label node us-west-1.192.168.0.87 openyurt.io/is-edge-worker=false # 边缘节点，值为true kubectl label node us-west-1.192.168.0.88 openyurt.io/is-edge-worker=true 1.2. 给边缘节点开启自治模式 kubectl annotate node us-west-1.192.168.0.88 node.beta.openyurt.io/autonomy=true 2. 安装准备 2.1. 调整k8s组件的配置 参考调整k8s组件的配置\n2.2. 部署tunnel-dns wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurt-tunnel-dns.yaml kubectl apply -f yurt-tunnel-dns.yaml 获取clusterIP，作为kube-apiserver的专用nameserver地址。\nkubectl -n kube-system get svc yurt-tunnel-dns -o=jsonpath='{.spec.clusterIP}' 3. 部署openyurt控制面 通过helm来部署控制面，所有helm charts都可以在openyurt-helm 仓库中找到。\n快捷安装可参考脚本：helm-install-openyurt.sh\nhelm repo add openyurt https://openyurtio.github.io/openyurt-helm 3.1. yurt-app-manager helm upgrade --install yurt-app-manager -n kube-system openyurt/yurt-app-manager 3.2. openyurt 在openyurt/openyurt中的组件包括：\nyurt-controller-manager: 防止apiserver在断开连接时驱逐运行在边缘节点上的pod yurt-tunnel-server: 在云端构建云边隧道 yurt-tunnel-agent: 在边缘侧构建云边隧道 由于yurt-tunnel-server默认使用host模式，因此可能存在边缘端的agent无法访问云端的tunnel-server，需要为tunnel-server配置一个可访问的地址。\n# 下载并解压 helm pull openyurt/openyurt --untar # 修改tunnel相关配置 cd openyurt vi values.yaml # 示例： yurtTunnelServer: replicaCount: 1 tolerations: [] parameters: certDnsNames: \"\u003ctunnel server的域名\u003e\" tunnelAgentConnectPort: \u003ctunnel server端口，默认为10262\u003e certIps: \"\" yurtTunnelAgent: replicaCount: 1 tolerations: [] parameters: tunnelserverAddr: \"\u003ctunnel server的地址，包括端口\u003e\" # install helm install openyurt ./openyurt 4. 部署 Yurthub(edge) 在 yurt-controller-manager 启动并正常运行后，以静态 pod 的方式部署 Yurthub。\n为 yurthub 创建全局配置(即RBAC, configmap) wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub-cfg.yaml kubectl apply -f yurthub-cfg.yaml 在边缘节点以static pod方式创建yurthub mkdir -p /etc/kubernetes/manifests/ cd /etc/kubernetes/manifests/ wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub.yaml # 获取bootstrap token kubeadm token create # 假设 apiserver 的地址是 1.2.3.4:6443，bootstrap token 是 07401b.f395accd246ae52d sed -i 's|__kubernetes_master_address__|1.2.3.4:6443|; s|__bootstrap_token__|07401b.f395accd246ae52d|' /etc/kubernetes/manifests/yurthub.yaml 5. 重置 Kubelet 重置 kubelet 服务，让它通过 yurthub 访问apiserver。为 kubelet 服务创建一个新的 kubeconfig 文件来访问apiserver。\nmkdir -p /var/lib/openyurt cat \u003c\u003c EOF \u003e /var/lib/openyurt/kubelet.conf apiVersion: v1 clusters: - cluster: server: http://127.0.0.1:10261 name: default-cluster contexts: - context: cluster: default-cluster namespace: default user: default-auth name: default-context current-context: default-context kind: Config preferences: {} EOF 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nsed -i \"s|KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=\\/etc\\/kubernetes\\/bootstrap-kubelet.conf\\ --kubeconfig=\\/etc\\/kubernetes\\/kubelet.conf|KUBELET_KUBECONFIG_ARGS=--kubeconfig=\\/var\\/lib\\/openyurt\\/kubelet.conf|g\" \\ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 重启kubelet服务\nsystemctl daemon-reload \u0026\u0026 systemctl restart kubelet 6. yurthub部署脚本 根据以上部署步骤，整理部署脚本。需要修改脚本内容的master-addr和token字段。\n#!/bin/bash set -e set -x ### install yurthub ### mkdir -p /etc/kubernetes/manifests/ cd /etc/kubernetes/manifests/ wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub.yaml ### 修改master和token字段 sed -i 's|__kubernetes_master_address__|\u003cmaster-addr\u003e:6443|; s|__bootstrap_token__|\u003ctoken\u003e|' /etc/kubernetes/manifests/yurthub.yaml mkdir -p /var/lib/openyurt cat \u003c\u003c EOF \u003e /var/lib/openyurt/kubelet.conf apiVersion: v1 clusters: - cluster: server: http://127.0.0.1:10261 name: default-cluster contexts: - context: cluster: default-cluster namespace: default user: default-auth name: default-context current-context: default-context kind: Config preferences: {} EOF cp /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.bak sed -i \"s|KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=\\/etc\\/kubernetes\\/bootstrap-kubelet.conf\\ --kubeconfig=\\/etc\\/kubernetes\\/kubelet.conf|KUBELET_KUBECONFIG_ARGS=--kubeconfig=\\/var\\/lib\\/openyurt\\/kubelet.conf|g\" \\ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl daemon-reload \u0026\u0026 systemctl restart kubelet 参考：\nhttps://openyurt.io/zh/docs/installation/manually-setup https://openyurt.io/zh/docs/installation/openyurt-prepare 在存量的K8s节点上安装OpenYurt Node组件 ","categories":"","description":"","excerpt":" 本文主要介绍部署openyurt组件到k8s集群中。\n1. 给云端节点和边缘节点打标签 openyurt将k8s节点分为云端节点和边缘节 …","ref":"/kubernetes-notes/edge/openyurt/install-openyurt/","tags":["OpenYurt"],"title":"OpenYurt部署"},{"body":" 本文基于https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/ 整理。\n1. RBAC介绍 基于角色的访问控制【Role-based access control (RBAC)】是一种基于组织中用户的角色来调节控制对 计算机或网络资源的访问的方法。\nRBAC 鉴权机制使用 rbac.authorization.k8s.io API 组来驱动鉴权决定， 允许你通过 Kubernetes API 动态配置策略。\n要启用 RBAC，在启动 API 服务器时将 --authorization-mode 参数设置为一个逗号分隔的列表并确保其中包含 RBAC。\nkube-apiserver --authorization-mode=Example,RBAC --\u003c其他选项\u003e --\u003c其他选项\u003e 编写自定义CRD控制器或部署其他开源组件时，经常需要给组件配置RBAC权限。\n理解RBAC权限体系，只需要理解以下三个概念对象即可：\n【权限】Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。\n【用户】Subject：被作用者，既可以是“人”，也可以是“机器”，也可以是你在 Kubernetes 里定义的“用户”。\n【授权】RoleBinding：定义了“被作用者”和“角色”的绑定关系。\n一句话理解RBAC，就是将定义的权限与定义的用户之间的关系进行绑定，即授权某个用户某些权限。\n快速授权脚本可以参考：https://github.com/huweihuang/kubeadm-scripts/tree/main/kubeconfig/token\n2. API对象 角色（权限）---角色（权限）绑定---用户（subject）\n集群级别范围 命名空间范围 权限 ClusterRole Role 授权 ClusterRoleBinding RoleBinding 用户 ServiceAccout 以下从权限、用户、授权三个概念进行说明。完成一套授权逻辑，主要分为三个步骤\n创建权限，即创建Role或ClusterRole对象。\n创建用户，即创建ServiceAccount对象。\n分配权限，即创建RoleBinding或ClusterRoleBinding对象。\n3. 权限 RBAC 的 Role 或 ClusterRole 中包含一组代表相关权限的规则。 这些权限是纯粹累加的（不存在拒绝某操作的规则）。\n3.1. 命名空间权限[Role] Role是针对指定namespace的权限，即创建的时候需要指定namespace。\n示例：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\"\"] # \"\" 标明 core API 组 resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 3.2. 集群级别权限[ClusterRole] ClusterRole用于指定集群内的资源：\n集群范围资源（比如节点（Node））\n非资源端点（比如 /healthz）\n跨名字空间访问的名字空间作用域的资源（如 访问所有namespace下的Pod）\n示例：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # \"namespace\" 被忽略，因为 ClusterRoles 不受名字空间限制 name: secret-reader rules: - apiGroups: [\"\"] # 在 HTTP 层面，用来访问 Secret 资源的名称为 \"secrets\" resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] 3.3. 默认权限角色 在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以system:开头。可以通过 kubectl get clusterroles 查看到它们。\n超级用户（Super-User）角色（cluster-admin）、 使用 ClusterRoleBinding 在集群范围内完成授权的角色（cluster-status）、 以及使用 RoleBinding 在特定名字空间中授予的角色（admin、edit、view）。\n默认 ClusterRole 默认 ClusterRoleBinding 描述 cluster-admin system:masters 组 允许超级用户在平台上的任何资源上执行所有操作。 当在 ClusterRoleBinding 中使用时，可以授权对集群中以及所有名字空间中的全部资源进行完全控制。 当在 RoleBinding 中使用时，可以授权控制角色绑定所在名字空间中的所有资源，包括名字空间本身。 admin 无 允许管理员访问权限，旨在使用 RoleBinding 在名字空间内执行授权。如果在 RoleBinding 中使用，则可授予对名字空间中的大多数资源的读/写权限， 包括创建角色和角色绑定的能力。 此角色不允许对资源配额或者名字空间本身进行写操作。 此角色也不允许对 Kubernetes v1.22+ 创建的 Endpoints 进行写操作。 更多信息参阅 “Endpoints 写权限”小节。 edit 无 允许对名字空间的大多数对象进行读/写操作。此角色不允许查看或者修改角色或者角色绑定。 不过，此角色可以访问 Secret，以名字空间中任何 ServiceAccount 的身份运行 Pod， 所以可以用来了解名字空间内所有服务账户的 API 访问级别。 此角色也不允许对 Kubernetes v1.22+ 创建的 Endpoints 进行写操作。 更多信息参阅 “Endpoints 写操作”小节。 view 无 允许对名字空间的大多数对象有只读权限。 它不允许查看角色或角色绑定。此角色不允许查看 Secrets，因为读取 Secret 的内容意味着可以访问名字空间中 ServiceAccount 的凭据信息，进而允许利用名字空间中任何 ServiceAccount 的身份访问 API（这是一种特权提升）。 4. 用户 4.1. ServiceAccount 创建指定namespace的ServiceAccount对象。ServiceAccount可以在pod中被使用。\napiVersion: v1 kind: ServiceAccount metadata: namespace: \u003cNamespace\u003e name: \u003cServiceAccountName\u003e 4.2. Secret 创建secret，绑定serviceaccount，会自动生成token。\nk8s 1.24后的版本不再自动生成secret，绑定后当删除ServiceAccount时会自动删除secret\napiVersion: v1 kind: Secret metadata: name: \u003cSecretName\u003e namespace: \u003cNamespace\u003e annotations: kubernetes.io/service-account.name: \"ServiceAccountName\" type: kubernetes.io/service-account-token 5. 授权 5.1. 授权命名空间权限[RoleBinding] RoleBinding角色绑定（Role Binding）是将角色中定义的权限赋予一个或者一组用户。 它包含若干 主体（用户、组或服务账户）的列表和对这些主体所获得的角色的引用。 RoleBinding 在指定的名字空间中执行授权，而 ClusterRoleBinding 在集群范围执行授权。\n字段说明：\nsubjects：表示权限所授予的用户，包括ServiceAccount，Group，User。\nroleRef：表示权限对应的角色，包括Role，ClusterRole。\n示例：\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ${USER}-rolebinding namespace: ${NAMESPACE} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: kind: ClusterRole name: ${ROLE} apiGroup: rbac.authorization.k8s.io 可以给指定命名空间下的serviceaccount授权其他命名空间的权限。只需要新增RoleBinding在预授权的命名空间下即可。可以理解为可以给一个用户分配多个命名空间的权限。\n#!/bin/bash set -e # 给已存在的用户USER 添加其他NAMESPACE的权限 USER=$1 NAMESPACE=$2 ROLE=$3 ROLE=${ROLE:-edit} ServiceAccountName=\"${USER}-user\" ServiceAccountNS=\"kubernetes-dashboard\" cat\u003c\u003cEOF | kubectl apply -f - kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ${USER}-rolebinding namespace: ${NAMESPACE} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: kind: ClusterRole name: ${ROLE} apiGroup: rbac.authorization.k8s.io EOF 5.2. 授权集群级别权限[ClusterRoleBinding] 要跨整个集群完成访问权限的授予，可以使用一个 ClusterRoleBinding。\n示例：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ${USER} subjects: - kind: ServiceAccount name: ${USER} namespace: ${NAMESPACE} roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ${ROLE} 参考：\n使用 RBAC 鉴权 | Kubernetes 用户认证 | Kubernetes k8s serviceaccount创建后没有生成对应的secret ","categories":"","description":"","excerpt":" 本文基 …","ref":"/kubernetes-notes/operation/access/rbac-auth/","tags":["Kubernetes"],"title":"使用 RBAC 鉴权"},{"body":" 本文由网络资源整理以作记录\n简介 Karmada（Kubernetes Armada）是基于Kubernetes原生API的多集群管理系统。在多云和混合云场景下，Karmada提供可插拔，全自动化管理多集群应用，实现多云集中管理、高可用性、故障恢复和流量调度。\n特性 基于K8s原生API的跨集群应用管理，用户可以方便快捷地将应用从单集群迁移到多集群。 中心式操作和管理Kubernetes集群。 跨集群应用可在多集群上自动扩展，故障转移和负载均衡。 高级的调度策略：区域，可用区，云提供商，集群亲和性/反亲和性。 支持创建分发用户自定义（CustomResourceDefinitions）资源。 框架结构 ETCD：存储Karmada API对象。 Karmada Scheduler：提供高级的多集群调度策略。 Karmada Controller Manager: 包含多个Controller，Controller监听karmada对象并且与成员集群API server进行通信并创建成员集群的k8s对象。 Cluster Controller：成员集群的生命周期管理与对象管理。 Policy Controller：监听PropagationPolicy对象，创建ResourceBinding，配置资源分发策略。 Binding Controller：监听ResourceBinding对象，并创建work对象响应资源清单。 Execution Controller：监听work对象，并将资源分发到成员集群中。 资源分发流程 基本概念\n资源模板（Resource Template）：Karmada使用K8s原生API定义作为资源模板，便于快速对接K8s生态工具链。 分发策略（Propagaion Policy）：Karmada提供独立的策略API，用来配置资源分发策略。 差异化策略（Override Policy）：Karmada提供独立的差异化API，用来配置与集群相关的差异化配置。比如配置不同集群使用不同的镜像。 Karmada资源分发流程图：\n参考：\nhttps://github.com/karmada-io/karmada https://support.huaweicloud.com/productdesc-mcp/mcp_productdesc_0001.html ","categories":"","description":"","excerpt":" 本文由网络资源整理以作记录\n简介 Karmada（Kubernetes Armada）是基于Kubernetes原生API的多集群管理系 …","ref":"/kubernetes-notes/multi-cluster/karmada/karmada-introduction/","tags":["Karmada"],"title":"Karmada介绍"},{"body":" 本文主要由云原生虚拟化：基于 Kubevirt 构建边缘计算实例文章重新整理而成。\n1. kubevirt简介 kubevirt是基于k8s之上，提供了一种通过k8s来编排和管理虚拟机的方式。\n2. 架构图 2.1. 组件说明 分类 组件 部署方式 功能说明 控制面 virt-api deployment 自定义API，开机、关机、重启等，作为apiserver的插件，业务通过k8s apiserver请求virt-api。 virt-controller deployment 管理和监控VMI对象的状态，控制VMI下的pod。 节点侧 virt-handler daemonset 类似kubelet，管理宿主机上的所有虚拟机实例。 virt-launcher virt-handler pod 调用libvirt和qemu创建虚拟机进程。 virt-launcher与libvirt逻辑：\n2.2. 自定义CRD对象 分类 CRD对象 功能说明 虚机 VirtualMachineInstance（VMI） 代表运行的虚拟机实例 VirtualMachine（VM） 虚机对象，提供开机、关机、重启，管理VMI实例，与VMI的关系是1：1 3. 创建虚拟机流程 待补充\n参考：\nhttps://github.com/kubevirt/kubevirt\nArchitecture - KubeVirt User-Guide\nhttps://mp.weixin.qq.com/s/IwA1QcGaooZAL96YjvTqjA\n","categories":"","description":"","excerpt":" 本文主要由云原生虚拟化：基于 Kubevirt 构建边缘计算实例文章重新整理而成。\n1. kubevirt简介 kubevirt是基 …","ref":"/kubernetes-notes/kvm/kubevirt/kubevirt-introduction/","tags":["KubeVirt"],"title":"KubeVirt的介绍"},{"body":" 本文主要分析OCI，CRI，runc，containerd，cri-containerd，dockershim等组件说明及调用关系。\n1. 概述 各个组件调用关系图如下：\n图片来源：https://www.jianshu.com/p/62e71584d1cb\n2. OCI（Open Container Initiative） OCI（Open Container Initiative）即开放的容器运行时规范，目的在于定义一个容器运行时及镜像的相关标准和规范，其中包括\nruntime-spec：容器的生命周期管理，具体参考runtime-spec。 image-spec：镜像的生命周期管理，具体参考image-spec。 实现OCI标准的容器运行时有runc，kata等。\n3. RunC runc(run container)是一个基于OCI标准实现的一个轻量级容器运行工具，用来创建和运行容器。而Containerd是用来维持通过runc创建的容器的运行状态。即runc用来创建和运行容器，containerd作为常驻进程用来管理容器。\nrunc包含libcontainer，包括对namespace和cgroup的调用操作。\n命令参数：\nTo start a new instance of a container: # runc run [ -b bundle ] \u003ccontainer-id\u003e USAGE: runc [global options] command [command options] [arguments...] COMMANDS: checkpoint checkpoint a running container create create a container delete delete any resources held by the container often used with detached container events display container events such as OOM notifications, cpu, memory, and IO usage statistics exec execute new process inside the container init initialize the namespaces and launch the process (do not call it outside of runc) kill kill sends the specified signal (default: SIGTERM) to the container's init process list lists containers started by runc with the given root pause pause suspends all processes inside the container ps ps displays the processes running inside a container restore restore a container from a previous checkpoint resume resumes all processes that have been previously paused run create and run a container spec create a new specification file start executes the user defined process in a created container state output the state of a container update update container resource constraints help, h Shows a list of commands or help for one command 4. Containerd containerd（container daemon）是一个daemon进程用来管理和运行容器，可以用来拉取/推送镜像和管理容器的存储和网络。其中可以调用runc来创建和运行容器。\n4.1. containerd的架构图 4.2. docker与containerd、runc的关系图 更具体的调用逻辑：\n5. CRI（Container Runtime Interface ） CRI即容器运行时接口，主要用来定义k8s与容器运行时的API调用，kubelet通过CRI来调用容器运行时，只要实现了CRI接口的容器运行时就可以对接到k8s的kubelet组件。\n5.1. docker与k8s调用containerd的关系图 5.2. cri-api 5.2.1. runtime service // Runtime service defines the public APIs for remote container runtimes service RuntimeService { // Version returns the runtime name, runtime version, and runtime API version. rpc Version(VersionRequest) returns (VersionResponse) {} // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure // the sandbox is in the ready state on success. rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {} // StopPodSandbox stops any running process that is part of the sandbox and // reclaims network resources (e.g., IP addresses) allocated to the sandbox. // If there are any running containers in the sandbox, they must be forcibly // terminated. // This call is idempotent, and must not return an error if all relevant // resources have already been reclaimed. kubelet will call StopPodSandbox // at least once before calling RemovePodSandbox. It will also attempt to // reclaim resources eagerly, as soon as a sandbox is not needed. Hence, // multiple StopPodSandbox calls are expected. rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {} // RemovePodSandbox removes the sandbox. If there are any running containers // in the sandbox, they must be forcibly terminated and removed. // This call is idempotent, and must not return an error if the sandbox has // already been removed. rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {} // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not // present, returns an error. rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {} // ListPodSandbox returns a list of PodSandboxes. rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {} // CreateContainer creates a new container in specified PodSandbox rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {} // StartContainer starts the container. rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {} // StopContainer stops a running container with a grace period (i.e., timeout). // This call is idempotent, and must not return an error if the container has // already been stopped. // The runtime must forcibly kill the container after the grace period is // reached. rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {} // RemoveContainer removes the container. If the container is running, the // container must be forcibly removed. // This call is idempotent, and must not return an error if the container has // already been removed. rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {} // ListContainers lists all containers by filters. rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {} // ContainerStatus returns status of the container. If the container is not // present, returns an error. rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {} // UpdateContainerResources updates ContainerConfig of the container. rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {} // ReopenContainerLog asks runtime to reopen the stdout/stderr log file // for the container. This is often called after the log file has been // rotated. If the container is not running, container runtime can choose // to either create a new log file and return nil, or return an error. // Once it returns error, new container log file MUST NOT be created. rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {} // ExecSync runs a command in a container synchronously. rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {} // Exec prepares a streaming endpoint to execute a command in the container. rpc Exec(ExecRequest) returns (ExecResponse) {} // Attach prepares a streaming endpoint to attach to a running container. rpc Attach(AttachRequest) returns (AttachResponse) {} // PortForward prepares a streaming endpoint to forward ports from a PodSandbox. rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {} // ContainerStats returns stats of the container. If the container does not // exist, the call returns an error. rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {} // ListContainerStats returns stats of all running containers. rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {} // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} // Status returns the status of the runtime. rpc Status(StatusRequest) returns (StatusResponse) {} } 5.2.2. image service // ImageService defines the public APIs for managing images. service ImageService { // ListImages lists existing images. rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {} // ImageStatus returns the status of the image. If the image is not // present, returns a response with ImageStatusResponse.Image set to // nil. rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {} // PullImage pulls an image with authentication config. rpc PullImage(PullImageRequest) returns (PullImageResponse) {} // RemoveImage removes the image. // This call is idempotent, and must not return an error if the image has // already been removed. rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {} // ImageFSInfo returns information of the filesystem that is used to store images. rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {} } 5.3. cri-containerd 5.3.1. CRI Plugin调用流程 kubelet调用CRI插件，通过CRI Runtime Service接口创建pod cri通过CNI接口创建和配置pod的network namespace cri调用containerd创建sandbox container（pause container ）并将容器放入pod的cgroup和namespace中 kubelet调用CRI插件，通过image service接口拉取镜像，接着通过containerd来拉取镜像 kubelet调用CRI插件，通过runtime service接口运行拉取下来的镜像服务，最后通过containerd来运行业务容器，并将容器放入pod的cgroup和namespace中。 具体参考：https://github.com/containerd/cri/blob/release/1.4/docs/architecture.md\n5.3.2. k8s对runtime调用的演进 由原来通过dockershim调用docker再调用containerd，直接变成通过cri-containerd调用containerd，从而减少了一层docker调用逻辑。\n具体参考：https://github.com/containerd/cri/blob/release/1.4/docs/proposal.md\n5.4. Dockershim 在旧版本的k8s中，由于docker没有实现CRI接口，因此增加一个Dockershim来实现k8s对docker的调用。（shim：垫片，一般用来表示对第三方组件API调用的适配插件，例如k8s使用Dockershim来实现对docker接口的适配调用）\n5.5. CRI-O cri-o与containerd类似，用来实现容器的管理，可替换containerd的使用。\n参考：\nhttps://opencontainers.org/about/overview/ https://github.com/opencontainers/runtime-spec https://github.com/kubernetes/kubernetes/blob/242a97307b34076d5d8f5bbeb154fa4d97c9ef1d/docs/devel/container-runtime-interface.md https://github.com/containerd/containerd/blob/main/docs/cri/architecture.md https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/ ","categories":"","description":"","excerpt":" 本文主要分析OCI，CRI，runc，containerd，cri-containerd，dockershim等组件说明及调用关系。\n1. …","ref":"/kubernetes-notes/runtime/runtime/","tags":["Kubernetes","Runtime"],"title":"Runc和Containerd概述"},{"body":"1. Go modules简介 Go 1.11版本开始支持Go modules方式的依赖包管理功能，官网参考：https://github.com/golang/go/wiki/Modules 。\n2. go mod的使用 项目文件如下：\nhello.go\npackage main import ( \"fmt\" \"rsc.io/quote\" ) func main() { fmt.Println(quote.Hello()) } 操作记录：\n# 安装GO 1.11及以上版本 go version go version go1.12.5 darwin/amd64 # 开启module功能 export GO111MODULE=on # 进入到项目目录 cd /home/gopath/src/hello # 初始化 go mod init go: creating new go.mod: module hello # 编译 go build go: finding rsc.io/quote v1.5.2 go: downloading rsc.io/quote v1.5.2 go: extracting rsc.io/quote v1.5.2 go: finding rsc.io/sampler v1.3.0 go: finding golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: downloading rsc.io/sampler v1.3.0 go: extracting rsc.io/sampler v1.3.0 go: downloading golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: extracting golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c # 不添加vendor目录 go mod tidy -v # 如果添加vendor目录，则执行vendor参数 go mod vendor -v # 命令输出如下: # golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c golang.org/x/text/language golang.org/x/text/internal/tag # rsc.io/quote v1.5.2 rsc.io/quote # rsc.io/sampler v1.3.0 rsc.io/sampler # 文件目录结构 ./ ├── go.mod ├── go.sum ├── hello # 二进制文件 ├── hello.go └── vendor ├── golang.org ├── modules.txt └── rsc.io 3. go mod的相关文件 3.1. go.mod 文件路径：项目根目录下\nmodule hello go 1.12 require rsc.io/quote v1.5.2 3.2. go.sum 文件路径：项目根目录下\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c h1:qgOY6WgZOaTkIIMiVjBQcw93ERBE4m30iBm00nkL0i8= golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ= rsc.io/quote v1.5.2 h1:w5fcysjrx7yqtD/aO+QwRjYZOKnaM9Uh2b40tElTs3Y= rsc.io/quote v1.5.2/go.mod h1:LzX7hefJvL54yjefDEDHNONDjII0t9xZLPXsUe+TKr0= rsc.io/sampler v1.3.0 h1:7uVkIFmeBqHfdjD+gZwtXXI+RODJ2Wc4O7MPEh/QiW4= rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA= 3.3. modules.txt 文件路径：/{project}/vendor/modules.txt\n# golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c golang.org/x/text/language golang.org/x/text/internal/tag # rsc.io/quote v1.5.2 rsc.io/quote # rsc.io/sampler v1.3.0 rsc.io/sampler 4. go mod的帮助信息 go help mod Go mod provides access to operations on modules. Note that support for modules is built into all the go commands, not just 'go mod'. For example, day-to-day adding, removing, upgrading, and downgrading of dependencies should be done using 'go get'. See 'go help modules' for an overview of module functionality. Usage: go mod \u003ccommand\u003e [arguments] The commands are: download download modules to local cache edit edit go.mod from tools or scripts graph print module requirement graph init initialize new module in current directory tidy add missing and remove unused modules vendor make vendored copy of dependencies verify verify dependencies have expected content why explain why packages or modules are needed Use \"go help mod \u003ccommand\u003e\" for more information about a command. 4.1. go mod init go help mod init usage: go mod init [module] Init initializes and writes a new go.mod to the current directory, in effect creating a new module rooted at the current directory. The file go.mod must not already exist. If possible, init will guess the module path from import comments (see 'go help importpath') or from version control configuration. To override this guess, supply the module path as an argument. 4.2. go mod tidy usage: go mod tidy [-v] Tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module's packages and dependencies, and it removes unused modules that don't provide any relevant packages. It also adds any missing entries to go.sum and removes any unnecessary ones. The -v flag causes tidy to print information about removed modules to standard error. 4.3. go mod vendor go help mod vendor usage: go mod vendor [-v] Vendor resets the main module's vendor directory to include all packages needed to build and test all the main module's packages. It does not include test code for vendored packages. The -v flag causes vendor to print the names of vendored modules and packages to standard error. 参考：\nhttps://github.com/golang/go/wiki/Modules https://blog.golang.org/modules2019 https://blog.golang.org/using-go-modules ","categories":"","description":"","excerpt":"1. Go modules简介 Go 1.11版本开始支持Go modules方式的依赖包管理功能，官网参 …","ref":"/golang-notes/introduction/package/go-modules/","tags":["Golang"],"title":"go modules的使用"},{"body":"kubeedge源码分析之cloudcore 本文源码分析基于kubeedge v1.1.0\n本文主要分析cloudcore中CloudCoreCommand的基本流程，具体的cloudhub、edgecontroller、devicecontroller模块的实现逻辑待后续单独文章分析。\n目录结构：\ncloud/cmd/cloudcore\ncloudcore ├── app │ ├── options │ │ └── options.go │ └── server.go # NewCloudCoreCommand、registerModules └── cloudcore.go # main函数 cloudcore部分包含以下模块：\ncloudhub edgecontroller devicecontroller 1. main函数 kubeedge的代码采用cobra命令框架，代码风格与k8s源码风格类似。cmd目录主要为cobra command的基本内容及参数解析，pkg目录包含具体的实现逻辑。\ncloud/cmd/cloudcore/cloudcore.go\nfunc main() { command := app.NewCloudCoreCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 2. NewCloudCoreCommand NewCloudCoreCommand为cobra command的构造函数，该类函数一般包含以下部分：\n构造option 添加Flags 运行Run函数（核心） cloud/cmd/cloudcore/app/server.go\nfunc NewCloudCoreCommand() *cobra.Command { opts := options.NewCloudCoreOptions() cmd := \u0026cobra.Command{ Use: \"cloudcore\", Long: `CloudCore is the core cloud part of KubeEdge, which contains three modules: cloudhub, edgecontroller, and devicecontroller. Cloudhub is a web server responsible for watching changes at the cloud side, caching and sending messages to EdgeHub. EdgeController is an extended kubernetes controller which manages edge nodes and pods metadata so that the data can be targeted to a specific edge node. DeviceController is an extended kubernetes controller which manages devices so that the device metadata/status date can be synced between edge and cloud.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() flag.PrintFlags(cmd.Flags()) // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) registerModules() // start all modules core.Run() }, } fs := cmd.Flags() namedFs := opts.Flags() verflag.AddFlags(namedFs.FlagSet(\"global\")) globalflag.AddGlobalFlags(namedFs.FlagSet(\"global\"), cmd.Name()) for _, f := range namedFs.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStderr(), namedFs, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStdout(), namedFs, cols) }) return cmd } 核心代码：\n// 构造option opts := options.NewCloudCoreOptions() // 执行run函数 registerModules() core.Run() // 添加flags fs.AddFlagSet(f) 3. registerModules 由于kubeedge的代码的大部分模块都采用了基于go-channel的消息通信框架Beehive（待后续单独文章分析），因此在各模块启动之前，需要将该模块注册到beehive的框架中。\n其中cloudcore部分涉及的模块有：\ncloudhub edgecontroller devicecontroller cloud/cmd/cloudcore/app/server.go\n// registerModules register all the modules started in cloudcore func registerModules() { cloudhub.Register() edgecontroller.Register() devicecontroller.Register() } 以下以cloudhub为例说明注册的过程。\ncloudhub结构体主要包含：\ncontext：上下文，用来传递消息上下文 stopChan：go channel通信 beehive框架中的模块需要实现Module接口，因此cloudhub也实现了该接口，其中核心方法为Start，用来启动相应模块的运行。\nvendor/github.com/kubeedge/beehive/pkg/core/module.go\n// Module interface type Module interface { Name() string Group() string Start(c *context.Context) Cleanup() } 以下为cloudHub结构体及注册函数。\ncloud/pkg/cloudhub/cloudhub.go\ntype cloudHub struct { context *context.Context stopChan chan bool } func Register() { core.Register(\u0026cloudHub{}) } 具体的注册实现函数为core.Register，注册过程实际上就是将具体的模块结构体放入一个以模块名为key的map映射中，待后续调用。\nvendor/github.com/kubeedge/beehive/pkg/core/module.go\n// Register register module func Register(m Module) { if isModuleEnabled(m.Name()) { modules[m.Name()] = m //将具体的模块结构体放入一个以模块名为key的map映射中 log.LOGGER.Info(\"module \" + m.Name() + \" registered\") } else { disabledModules[m.Name()] = m log.LOGGER.Info(\"module \" + m.Name() + \" is not register, please check modules.yaml\") } } 4. core.Run CloudCoreCommand命令的Run函数实际上是运行beehive框架中注册的所有模块。\n其中包括两部分逻辑：\n启动运行所有注册模块 监听信号并做优雅清理 vendor/github.com/kubeedge/beehive/pkg/core/core.go\n//Run starts the modules and in the end does module cleanup func Run() { //Address the module registration and start the core StartModules() // monitor system signal and shutdown gracefully GracefulShutdown() } 5. StartModules StartModules获取context上下文，并以goroutine的方式运行所有已注册的模块。其中Start函数即每个模块的具体实现Module接口中的Start方法。不同模块各自定义自己的具体Start方法实现。\ncoreContext := context.GetContext(context.MsgCtxTypeChannel) go module.Start(coreContext) 具体实现如下：\nvendor/github.com/kubeedge/beehive/pkg/core/core.go\n// StartModules starts modules that are registered func StartModules() { coreContext := context.GetContext(context.MsgCtxTypeChannel) modules := GetModules() for name, module := range modules { //Init the module coreContext.AddModule(name) //Assemble typeChannels for send2Group coreContext.AddModuleGroup(name, module.Group()) go module.Start(coreContext) log.LOGGER.Info(\"starting module \" + name) } } 6. GracefulShutdown 当收到相关信号，则执行各个模块实现的Cleanup方法。\nvendor/github.com/kubeedge/beehive/pkg/core/core.go\n// GracefulShutdown is if it gets the special signals it does modules cleanup func GracefulShutdown() { c := make(chan os.Signal) signal.Notify(c, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM, syscall.SIGQUIT, syscall.SIGILL, syscall.SIGTRAP, syscall.SIGABRT) select { case s := \u003c-c: log.LOGGER.Info(\"got os signal \" + s.String()) //Cleanup each modules modules := GetModules() for name, module := range modules { log.LOGGER.Info(\"Cleanup module \" + name) module.Cleanup() } } } 参考：\nhttps://github.com/kubeedge/kubeedge/tree/release-1.1/cloud/cmd/cloudcore https://github.com/kubeedge/kubeedge/tree/release-1.1/vendor/github.com/kubeedge/beehive/pkg/core ","categories":"","description":"","excerpt":"kubeedge源码分析之cloudcore 本文源码分析基于kubeedge v1.1.0\n本文主要分析cloudcore …","ref":"/kubernetes-notes/edge/kubeedge/code-analysis/cloudcore/","tags":["Kubeedge"],"title":"Kubeedge之cloudcore 源码分析"},{"body":"kubeedge源码分析之edgecore 本文源码分析基于kubeedge v1.1.0\n本文主要分析edgecore中EdgeCoreCommand的基本流程，具体的edged、edgehub、metamanager等模块的实现逻辑待后续单独文章分析。\n目录结构：\nedgecore ├── app │ ├── options │ │ └── options.go │ └── server.go # NewEdgeCoreCommand 、registerModules └── edgecore.go # main edgecore模块包含：\nedged edgehub metamanager eventbus servicebus devicetwin edgemesh 1. main函数 main入口函数，仍然是cobra命令框架格式。\nedge/cmd/edgecore/edgecore.go\nfunc main() { command := app.NewEdgeCoreCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 2. NewEdgeCoreCommand NewEdgeCoreCommand与NewCloudCoreCommand一样构造对应的cobra command结构体。\nedge/cmd/edgecore/app/server.go\n// NewEdgeCoreCommand create edgecore cmd func NewEdgeCoreCommand() *cobra.Command { opts := options.NewEdgeCoreOptions() cmd := \u0026cobra.Command{ Use: \"edgecore\", Long: `Edgecore is the core edge part of KubeEdge, which contains six modules: devicetwin, edged, edgehub, eventbus, metamanager, and servicebus. DeviceTwin is responsible for storing device status and syncing device status to the cloud. It also provides query interfaces for applications. Edged is an agent that runs on edge nodes and manages containerized applications and devices. Edgehub is a web socket client responsible for interacting with Cloud Service for the edge computing (like Edge Controller as in the KubeEdge Architecture). This includes syncing cloud-side resource updates to the edge, and reporting edge-side host and device status changes to the cloud. EventBus is a MQTT client to interact with MQTT servers (mosquito), offering publish and subscribe capabilities to other components. MetaManager is the message processor between edged and edgehub. It is also responsible for storing/retrieving metadata to/from a lightweight database (SQLite).ServiceBus is a HTTP client to interact with HTTP servers (REST), offering HTTP client capabilities to components of cloud to reach HTTP servers running at edge. `, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() flag.PrintFlags(cmd.Flags()) // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) registerModules() // start all modules core.Run() }, } fs := cmd.Flags() namedFs := opts.Flags() verflag.AddFlags(namedFs.FlagSet(\"global\")) globalflag.AddGlobalFlags(namedFs.FlagSet(\"global\"), cmd.Name()) for _, f := range namedFs.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStderr(), namedFs, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStdout(), namedFs, cols) }) return cmd } 核心代码：\nopts := options.NewEdgeCoreOptions() registerModules() core.Run() 3. registerModules edgecore仍然采用Beehive通信框架，模块调用前先注册对应的模块。具体参考cloudcore.registerModules处的分析，此处不再展开分析注册流程。此处注册的是edgecore中涉及的组件。\nedge/cmd/edgecore/app/server.go\n// registerModules register all the modules started in edgecore func registerModules() { devicetwin.Register() edged.Register() edgehub.Register() eventbus.Register() edgemesh.Register() metamanager.Register() servicebus.Register() test.Register() dbm.InitDBManager() } 4. core.Run core.Run与cloudcore.run处逻辑一致不再展开分析。\nvendor/github.com/kubeedge/beehive/pkg/core/core.go\n//Run starts the modules and in the end does module cleanup func Run() { //Address the module registration and start the core StartModules() // monitor system signal and shutdown gracefully GracefulShutdown() } 参考：\nhttps://github.com/kubeedge/kubeedge/tree/release-1.1/edge/cmd/edgecore ","categories":"","description":"","excerpt":"kubeedge源码分析之edgecore 本文源码分析基于kubeedge v1.1.0\n本文主要分析edgecore …","ref":"/kubernetes-notes/edge/kubeedge/code-analysis/edgecore/","tags":["Kubeedge"],"title":"Kubeedge之edgecore 源码分析"},{"body":"1. kubebuilder 1.1. 安装kubebuilder # download kubebuilder and install locally. curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder \u0026\u0026 mv kubebuilder /usr/local/bin/ 1.2. kubebuilder命令 Development kit for building Kubernetes extensions and tools. Provides libraries and tools to create new projects, APIs and controllers. Includes tools for packaging artifacts into an installer container. Typical project lifecycle: - initialize a project: kubebuilder init --domain example.com --license apache2 --owner \"The Kubernetes authors\" - create one or more a new resource APIs and add your code to them: kubebuilder create api --group \u003cgroup\u003e --version \u003cversion\u003e --kind \u003cKind\u003e Create resource will prompt the user for if it should scaffold the Resource and / or Controller. To only scaffold a Controller for an existing Resource, select \"n\" for Resource. To only define the schema for a Resource without writing a Controller, select \"n\" for Controller. After the scaffold is written, api will run make on the project. Usage: kubebuilder [command] Available Commands: create Scaffold a Kubernetes API or webhook. edit This command will edit the project configuration help Help about any command init Initialize a new project version Print the kubebuilder version Flags: -h, --help help for kubebuilder Use \"kubebuilder [command] --help\" for more information about a command. 2. 操作步骤 2.1. 初始化 mkdir $GOPATH/src/github.com/huweihuang/operator-example cd $GOPATH/src/github.com/huweihuang/operator-example go mod init github.com/huweihuang/operator-example 2.2. 创建项目 # kubebuilder init --domain github.com --license apache2 --owner \"Hu Weihuang\" Writing scaffold for you to edit... Get controller runtime: $ go get sigs.k8s.io/controller-runtime@v0.5.0 Update go.mod: $ go mod tidy Running make: $ make go: creating new go.mod: module tmp go: finding sigs.k8s.io v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd/controller-gen v0.2.5 /Users/weihuanghu/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\" go fmt ./... go vet ./... go build -o bin/manager main.go Next: define a resource with: $ kubebuilder create api 查看生成文件：\n./ ├── Dockerfile ├── Makefile ├── PROJECT ├── bin │ └── manager ├── config │ ├── certmanager │ │ ├── certificate.yaml │ │ ├── kustomization.yaml │ │ └── kustomizeconfig.yaml │ ├── default │ │ ├── kustomization.yaml │ │ ├── manager_auth_proxy_patch.yaml │ │ ├── manager_webhook_patch.yaml │ │ └── webhookcainjection_patch.yaml │ ├── manager │ │ ├── kustomization.yaml │ │ └── manager.yaml │ ├── prometheus │ │ ├── kustomization.yaml │ │ └── monitor.yaml │ ├── rbac │ │ ├── auth_proxy_client_clusterrole.yaml │ │ ├── auth_proxy_role.yaml │ │ ├── auth_proxy_role_binding.yaml │ │ ├── auth_proxy_service.yaml │ │ ├── kustomization.yaml │ │ ├── leader_election_role.yaml │ │ ├── leader_election_role_binding.yaml │ │ └── role_binding.yaml │ └── webhook │ ├── kustomization.yaml │ ├── kustomizeconfig.yaml │ └── service.yaml ├── go.mod ├── go.sum ├── hack │ └── boilerplate.go.txt └── main.go 2.3. 创建API # kubebuilder create api --group webapp --version v1 --kind Guestbook Create Resource [y/n] y Create Controller [y/n] y Writing scaffold for you to edit... api/v1/guestbook_types.go controllers/guestbook_controller.go Running make: $ make go: creating new go.mod: module tmp go: finding sigs.k8s.io/controller-tools/cmd v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd/controller-gen v0.2.5 go: finding sigs.k8s.io v0.2.5 /Users/weihuanghu/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\" go fmt ./... go vet ./... go build -o bin/manager main.go 查看创建文件\napi └── v1 ├── groupversion_info.go ├── guestbook_types.go └── zz_generated.deepcopy.go controllers ├── guestbook_controller.go └── suite_test.go 查看api/v1/guestbook_types.go\n// GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \"make\" to regenerate code after modifying this file // Quantity of instances // +kubebuilder:validation:Minimum=1 // +kubebuilder:validation:Maximum=10 Size int32 `json:\"size\"` // Name of the ConfigMap for GuestbookSpec's configuration // +kubebuilder:validation:MaxLength=15 // +kubebuilder:validation:MinLength=1 ConfigMapName string `json:\"configMapName\"` // +kubebuilder:validation:Enum=Phone;Address;Name Type string `json:\"alias,omitempty\"` } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run \"make\" to regenerate code after modifying this file // PodName of the active Guestbook node. Active string `json:\"active\"` // PodNames of the standby Guestbook nodes. Standby []string `json:\"standby\"` } // +kubebuilder:object:root=true // +kubebuilder:subresource:status // +kubebuilder:resource:scope=Cluster // Guestbook is the Schema for the guestbooks API type Guestbook struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } 3. troubleshooting 3.1. controller-gen: No such file or directory ➜ operator-example kubebuilder init --domain github.com --license apache2 --owner \"Hu Weihuang\" Writing scaffold for you to edit... Get controller runtime: $ go get sigs.k8s.io/controller-runtime@v0.5.0 Update go.mod: $ go mod tidy Running make: $ make go: creating new go.mod: module tmp go: finding sigs.k8s.io v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd/controller-gen v0.2.5 /Users/weihuanghu/go:/Users/weihuanghu/k8spath/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\" /bin/sh: /Users/weihuanghu/go:/Users/weihuanghu/k8spath/bin/controller-gen: No such file or directory make: *** [generate] Error 127 2020/04/13 14:34:47 failed to initialize project: exit status 2 由于本地存在多个GOPATH的目录，而获取了非当前项目下的GOPATH目录，因此将当前项目所在的GOPATH目录export到GOPATH环境变量中，就可以解决。\nexport GOPATH=\"/path/to/gopath\" 参考：\nhttps://kubernetes.io/zh/docs/concepts/extend-kubernetes/operator/ https://github.com/kubernetes-sigs/kubebuilder https://book.kubebuilder.io/quick-start.html https://operatorhub.io/ https://devops.college/developing-kubernetes-operator-is-now-easy-with-operator-framework-d3194a7428ff ","categories":"","description":"","excerpt":"1. kubebuilder 1.1. 安装kubebuilder # download kubebuilder and install …","ref":"/kubernetes-notes/develop/operator/kubebuilder/","tags":["Operator"],"title":"kubebuilder的使用"},{"body":"1. KubeEdge简介 KubeEdge是基于kubernetes之上将容器化应用的编排能力拓展到边缘主机或边缘设备，在云端和边缘端提供网络通信，应用部署、元数据同步等功能。同时支持MQTT协议，允许开发者在边缘端自定义接入边缘设备。\n2. 功能 边缘计算：提供边缘节点自治能力，边缘节点数据处理能力。 便捷部署：开发者可以开发http或mqtt协议的应用，运行在云端和边缘端。 k8s原生支持：可以通过k8s管理和监控边缘设备和边缘节点。 丰富的应用类型：可以在边缘端部署机器学习、图片识别、事件处理等应用。 3. 组件 3.1. 云端 CloudHub：一个web socket服务器，负责监听云端的更新、缓存及向EdgeHub发送消息。\nEdgeController：一个扩展的k8s控制器，负责管理边缘节点和pod元数据，同步边缘节点的数据，是k8s-apiserver 与EdgeCore的通信桥梁。\nDeviceController：一个扩展的k8s控制器，负责管理节点设备，同步云端和边缘端的设备元数据和状态。\n3.2. 边缘端 EdgeHub：一个web socket客户端，负责云端与边缘端的信息交互，其中包括将云端的资源变更同步到边缘端及边缘端的状态变化同步到云端。 Edged：运行在边缘节点，管理容器化应用的agent，负责pod生命周期的管理，类似kubelet。 EventBus：一个MQTT客户端，与MQTT服务端交互，提供发布/订阅的能力。 ServiceBus：一个HTTP客户端，与HTTP服务端交互。为云组件提供HTTP客户端功能，以访问在边缘运行的HTTP服务器。 DeviceTwin：负责存储设备状态并同步设备状态到云端，同时提供应用的接口查询。 MetaManager：edged和edgehub之间的消息处理器，负责向轻量数据库（SQLite）存储或查询元数据。 4. 架构图 参考：\nhttps://github.com/kubeedge/kubeedge\nhttps://kubeedge.readthedocs.io/en/latest/modules/kubeedge.html\n","categories":"","description":"","excerpt":"1. KubeEdge简介 KubeEdge是基于kubernetes之上将容器化应用的编排能力拓展到边缘主机或边缘设备，在云端和边缘端提供 …","ref":"/kubernetes-notes/edge/kubeedge/kubeedge-arch/","tags":["Kubeedge"],"title":"KubeEdge介绍"},{"body":"Kata-container简介 kata-container通过轻量型虚拟机技术构建一个安全的容器运行时，表现像容器一样，但通硬件虚拟化技术提供强隔离，作为第二层的安全防护。\n特点：\n安全：独立的内核，提供网络、I/O、内存的隔离。 兼容性：支持OCI容器标准，k8s的CRI接口。 性能：兼容虚拟机的安全和容器的轻量特点。 简单：使用标准的接口。 1. kata-container架构 kata-container与传统container的比较\n2. kata-runtime Kata Containers runtime (kata-runtime)通过QEMU*/KVM技术创建了一种轻量型的虚拟机，兼容 OCI runtime specification 标准，支持Kubernetes* Container Runtime Interface (CRI)接口，可替换CRI shim runtime (runc) 通过k8s来创建pod或容器。\n3. shim shim类似Docker的 containerd-shim 或CRI-O的 conmon，主要用来监控和回收容器的进程，kata-shim需要处理所有的容器的IO流(stdout, stdin and stderr)和转发相关信号。\ncontainerd-shim-kata-v2实现了Containerd Runtime V2 (Shim API)，k8s可以通过containerd-shim-kata-v2（替代2N+1个shims[由一个containerd-shim和kata-shim组成]）来创建pod。\n4. kata-agent 在虚拟机内kata-agent作为一个daemon进程运行，并拉起容器的进程。kata-agent使用VIRTIO或VSOCK接口（QEMU在主机上暴露的socket文件）在guest虚拟机中运行gRPC服务器。kata-runtime通过grpc协议与kata-agent通信，向kata-agent发送管理容器的命令。该协议还用于容器和管理引擎（例如Docker Engine）之间传送I / O流（stdout，stderr，stdin）。\n容器内所有的执行命令和相关的IO流都需要通过QEMU在宿主机暴露的virtio-serial或vsock接口，当使用VIRTIO的情况下，每个虚拟机会创建一个Kata Containers proxy (kata-proxy) 来处理命令和IO流。\nkata-agent使用libcontainer 来管理容器的生命周期，复用了runc的部分代码。\n5. kata-proxy kata-proxy提供了 kata-shim 和 kata-runtime 与VM中的kata-agent通信的方式，其中通信方式是使用virtio-serial或vsock，默认是使用virtio-serial。\n6. Hypervisor kata-container通过QEMU/KVM来创建虚拟机给容器运行，可以支持多种hypervisors。\n7. QEMU/KVM 待补充\n参考文档：\nhttps://katacontainers.io/\nhttps://github.com/kata-containers/documentation/blob/master/design/architecture.md\n","categories":"","description":"","excerpt":"Kata-container简介 kata-container通过轻量型虚拟机技术构建一个安全的容器运行时，表现像容器一样，但通硬件虚拟化技 …","ref":"/kubernetes-notes/runtime/kata/kata-container/","tags":["Kubernetes","Runtime"],"title":"Kata容器简介"},{"body":"1. CNI（Container Network Interface） CNI（Container Network Interface）即容器网络接口，通过约定统一的容器网络接口，从而kubelet可以通过这个标准的API来调用不同的网络插件实现不同的网络功能。\nkubelet启动参数--network-plugin=cni来指定CNI插件，kubelet从--cni-conf-dir （默认是 /etc/cni/net.d） 读取文件并使用 该文件中的 CNI 配置来设置各个 Pod 的网络。 CNI 配置文件必须与 CNI 规约 匹配，并且配置所引用的所有所需的 CNI 插件都应存在于 --cni-bin-dir（默认是 /opt/cni/bin）下。如果有多个CNI配置文件，kubelet 将会使用按文件名的字典顺序排列 的第一个作为配置文件。\nCNI规范定义：\n网络配置文件的格式\n容器runtime与CNI插件的通信协议\n基于提供的配置执行网络插件的步骤\n网络插件调用其他功能插件的步骤\n插件返回给runtime结果的数据格式\n2. CNI配置文件格式 CNI配置文件的格式为JSON格式，配置文件的默认路径：/etc/cni/net.d。插件二进制默认的路径为：/opt/cni/bin。\n2.1. 主配置的字段 cniVersion (string)：CNI规范使用的版本，例如版本为0.4.0。\nname (string)：目标网络的名称。\ndisableCheck (boolean)：关闭CHECK操作。\nplugins (list)：CNI插件列表及插件配置。\n2.2. 插件配置字段 根据不同的插件，插件配置所需的字段不同。\n必选字段：\ntype (string)：节点上插件二进制的名称，比如bridge，sriov，macvlan等。 可选字段：\ncapabilities (dictionary)\nipMasq (boolean)：为目标网络配上Outbound Masquerade(地址伪装)，即：由容器内部通过网关向外发送数据包时，对数据包的源IP地址进行修改。\n当我们的容器以宿主机作为网关时，这个参数是必须要设置的。否则，从容器内部发出的数据包就没有办法通过网关路由到其他网段。因为容器内部的IP地址无法被目标网段识别，所以这些数据包最终会被丢弃掉。\nipam (dictionary)：IPAM(IP Adderss Management)即IP地址管理，提供了一系列方法用于对IP和路由进行管理。它对应的是由CNI提供的一组标准IPAM插件，比如像host-local，dhcp，static等。比如文中用到的bridge插件，会调用我们所指定的IPAM插件，实现对网络设备IP地址的分配和管理。**如果是自己开发的ipam插件，则相关的入参可以自己定义和实现。\n以下以host-local为例说明。\ntype：指定所用IPAM插件的名称，在我们的例子里，用的是host-local。 subnet：为目标网络分配网段，包括网络ID和子网掩码，以CIDR形式标记。在我们的例子里为10.15.10.0/24，也就是目标网段为10.15.10.0，子网掩码为255.255.255.0。 routes：用于指定路由规则，插件会为我们在容器的路由表里生成相应的规则。其中，dst表示希望到达的目标网段，以CIDR形式标记。gw对应网关的IP地址，也就是要到达目标网段所要经过的“next hop(下一跳)”。如果省略gw的话，那么插件会自动帮我们选择默认网关。在我们的例子里，gw选择的是默认网关，而dst为0.0.0.0/0则代表“任何网络”，表示数据包将通过默认网关发往任何网络。实际上，这对应的是一条默认路由规则，即：当所有其他路由规则都不匹配时，将选择该路由。 rangeStart：允许分配的IP地址范围的起始值 rangeEnd：允许分配的IP地址范围的结束值 gateway：为网关（也就是我们将要在宿主机上创建的bridge）指定的IP地址。如果省略的话，那么插件会自动从允许分配的IP地址范围内选择起始值作为网关的IP地址。 dns (dictionary, optional)：dns配置\nnameservers (list of strings, optional)\ndomain (string, optional)\nsearch (list of strings, optional)\noptions (list of strings, optional)\n2.3. 配置文件示例 $ mkdir -p /etc/cni/net.d $ cat \u003e/etc/cni/net.d/10-mynet.conf \u003c\u003cEOF { \"cniVersion\": \"1.0.0\", \"name\": \"dbnet\", \"plugins\": [ { \"type\": \"bridge\", // plugin specific parameters \"bridge\": \"cni0\", \"keyA\": [\"some more\", \"plugin specific\", \"configuration\"], \"ipam\": { \"type\": \"host-local\", // ipam specific \"subnet\": \"10.1.0.0/16\", \"gateway\": \"10.1.0.1\", \"routes\": [ {\"dst\": \"0.0.0.0/0\"} ] }, \"dns\": { \"nameservers\": [ \"10.1.0.1\" ] } }, { \"type\": \"tuning\", \"capabilities\": { \"mac\": true }, \"sysctl\": { \"net.core.somaxconn\": \"500\" } }, { \"type\": \"portmap\", \"capabilities\": {\"portMappings\": true} } ] } 3. CNI插件 3.1. 安装插件 安装CNI二进制插件，插件下载地：https://github.com/containernetworking/plugins/releases\n# 下载二进制 wget https://github.com/containernetworking/plugins/releases/download/v1.1.0/cni-plugins-linux-amd64-v1.1.0.tgz # 解压文件 tar -zvxf cni-plugins-linux-amd64-v1.1.0.tgz -C /opt/cni/bin/ # 查看解压文件 # ll -h 总用量 63M -rwxr-xr-x 1 root root 3.7M 2月 24 01:01 bandwidth -rwxr-xr-x 1 root root 4.1M 2月 24 01:01 bridge -rwxr-xr-x 1 root root 9.3M 2月 24 01:01 dhcp -rwxr-xr-x 1 root root 4.2M 2月 24 01:01 firewall -rwxr-xr-x 1 root root 3.7M 2月 24 01:01 host-device -rwxr-xr-x 1 root root 3.1M 2月 24 01:01 host-local -rwxr-xr-x 1 root root 3.8M 2月 24 01:01 ipvlan -rwxr-xr-x 1 root root 3.2M 2月 24 01:01 loopback -rwxr-xr-x 1 root root 3.8M 2月 24 01:01 macvlan -rwxr-xr-x 1 root root 3.6M 2月 24 01:01 portmap -rwxr-xr-x 1 root root 4.0M 2月 24 01:01 ptp -rwxr-xr-x 1 root root 3.4M 2月 24 01:01 sbr -rwxr-xr-x 1 root root 2.7M 2月 24 01:01 static -rwxr-xr-x 1 root root 3.3M 2月 24 01:01 tuning -rwxr-xr-x 1 root root 3.8M 2月 24 01:01 vlan -rwxr-xr-x 1 root root 3.4M 2月 24 01:01 vrf 3.2. 插件分类 参考：https://www.cni.dev/plugins/current/\n分类 插件 说明 main bridge Creates a bridge, adds the host and the container to it ipvlan Adds an ipvlan interface in the container macvlan Creates a new MAC address, forwards all traffic to that to the container ptp Creates a veth pair host-device Moves an already-existing device into a container vlan Creates a vlan interface off a master IPAM dhcp Runs a daemon on the host to make DHCP requests on behalf of a container host-local Maintains a local database of allocated IPs static Allocates static IPv4/IPv6 addresses to containers meta tuning Changes sysctl parameters of an existing interface portmap An iptables-based portmapping plugin. Maps ports from the host’s address space to the container bandwidth Allows bandwidth-limiting through use of traffic control tbf (ingress/egress) sbr A plugin that configures source based routing for an interface (from which it is chained) firewall A firewall plugin which uses iptables or firewalld to add rules to allow traffic to/from the container 4. CNI插件接口 具体可参考：https://github.com/containernetworking/cni/blob/master/SPEC.md#cni-operations\nCNI定义的接口操作有：\nADD：添加容器网络，在容器启动时调用。 DEL：删除容器网络，在容器删除时调用。 CHECK：检查容器网络是否正常。 VERSION：显示插件版本。 这些操作通过CNI_COMMAND环境变量来传递给CNI插件二进制。\n其中环境变量包括：\nCNI_COMMAND：命令操作，包括 ADD, DEL, CHECK, or VERSION。\nCNI_CONTAINERID:容器的ID，有runtime分配，不为空。\nCNI_NETNS:容器的网络命名空间，命名空间路径，例如：/run/netns/[nsname]\nCNI_IFNAME:容器内的网卡名称。\nCNI_ARGS:其他参数。\nCNI_PATH:CNI插件二进制的路径。\n4.1. ADD接口：添加容器网络 在容器的网络命名空间CNI_NETNS中创建CNI_IFNAME网卡设备，或者调整网卡配置。\n必选参数：\nCNI_COMMAND CNI_CONTAINERID CNI_NETNS CNI_IFNAME 可选参数：\nCNI_ARGS CNI_PATH 4.2. DEL接口：删除容器网络 删除容器网络命名空间CNI_NETNS中的容器网卡CNI_IFNAME，或者撤销ADD修改操作。\n必选参数：\nCNI_COMMAND CNI_CONTAINERID CNI_IFNAME 可选参数：\nCNI_NETNS CNI_ARGS CNI_PATH 4.3. CHECK接口：检查容器网络 必选参数：\nCNI_COMMAND CNI_CONTAINERID CNI_NETNS CNI_IFNAME 可选参数：\nCNI_ARGS CNI_PATH 4.4. VERSION接口：输出CNI的版本 参考：\nhttps://www.cni.dev/docs/spec/ https://github.com/containernetworking/cni https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ https://github.com/containernetworking/plugins/tree/master/plugins https://www.cni.dev/plugins/current/ https://cloud.tencent.com/developer/news/600713 配置CNI插件 ","categories":"","description":"","excerpt":"1. CNI（Container Network Interface） CNI（Container Network Interface）即容 …","ref":"/kubernetes-notes/network/cni/cni/","tags":["CNI"],"title":"CNI接口介绍"},{"body":"k8s多集群的思考 1. 为什么需要多集群 1、k8s单集群的承载能力有限。\nKubernetes v1.21 支持的最大节点数为 5000。 更具体地说，Kubernetes旨在适应满足以下所有标准的配置：\n每个节点的 Pod 数量不超过 100 节点数不超过 5000 Pod 总数不超过 150000 容器总数不超过 300000 参考：https://kubernetes.io/zh/docs/setup/best-practices/cluster-large/\n且当节点数量较大时，会出现调度延迟，etcd读写延迟，apiserver负载高等问题，影响服务的正常创建。\n2、分散集群服务风险。\n全部服务都放在一个k8s集群中，当该集群出现异常，短期无法恢复的情况下，则影响全部服务和影响部署。为了避免机房等故障导致单集群异常，建议将k8s的master在分散在延迟较低的不同可用区部署，且在不同region部署多个k8s集群来进行集群级别的容灾。\n3、当前混合云的使用方式和架构\n当前部分公司会存在自建机房+不同云厂商的公有云从而来实现混部云的运营模式，那么自然会引入多集群管理的问题。\n2. 多集群部署需要解决哪些问题 目标：让用户像使用单集群一样来使用多集群。\n扩展集群的边界，服务的边界从单台物理机多个进程，发展到通过k8s集群来管理多台的物理机，再发展到管理多个的k8s集群。服务的边界从物理机发展到集群。\n而多集群管理需要解决以下问题：\n多集群服务的分发部署（deployment、daemonset等） 跨集群自动迁移与调度（当某个集群异常，服务可以在其他集群自动部署） 多集群服务发现，网络通信及负载均衡（service，ingress等） 而多集群服务的网络通信可以由Service mesh等来解决，本文不做重点讨论。\n以上几个问题，可以先从k8s管理节点的思维进行分析\n物理机视角 单集群视角 多集群视角 进程的边界 物理机 k8s集群 多集群 调度单元 进程或线程 容器或pod 工作负载（deployment） 服务的集合 工作负载（deployment） 不同集群工作负载的集合体（workloadGroup） 服务发现 service 不同集群service的集合体 服务迁移 工作负载（deployment）控制器 不同集群工作负载的集合体控制器 服务调度 nodename或者node selector clustername或cluster selector pod的反亲和（相同deployment下的pod不调度在相同节点） workload反亲和（相同workloadGroup分散在不同集群） 2.1. 多集群工作负载的分发 单集群中k8s的调度单元是pod，即一个pod只能跑在一个节点上，一个节点可以运行多个pod，而不同节点上的一组pod是通过一个workload来控制和分发。类似这个逻辑，那么在多集群的视角下，多集群的调度单元是一个集群的workload，一个workload只能跑在一个集群中，一个集群可以运行多个workload。\n那么就需要有一个控制器来管理不同k8s集群的相同workload。例如 workloadGroup。而该workloadGroup在不侵入k8s原生API的情况下，主要包含两个部分。\nworkloadGroup:\n资源模板（Resource Template）：服务的描述（workload） 分发策略（Propagaion Policy）：服务分发的集群（即多个workload应该被分发到哪些集群运行） workload描述的是什么服务运行在什么节点，workloadGroup描述的是什么服务运行在什么集群。\n实现workloadGroup有两种方式：\n一种是自定义API将workloadGroup中的Resource Template和Propagaion Policy合成在一个自定义的对象中，由用户直接指定该workloadGroup信息，从而将不同的workload分发到不同的集群中。 另一种方式是通过一个k8s载体来记录一个具体的workload对象，再由用户指定Propagaion Policy关联该workload对象，从而让控制器自动根据用户指定的Propagaion Policy将workload分发到不同的集群中。 2.2. 跨集群自动迁移与调度 单集群中k8s中通过workload中的nodeselector或者nodename以及亲和性来控制pod运行在哪个节点上。而多集群的视角下，则需要有一个控制器来实现集群级别的调度逻辑，例如clustername，cluster selector，cluster AntiAffinity，从而来自动控制workloadGroup下的workload分散在什么集群上。\n3. 目前的多集群方案 3.1. Kubefed[Federation v2] 简介\n基本思想\n3.2. virtual kubelet 简介\n基本思想\n3.3. Karmada 简介\n基本思想\n参考：\nhttps://kubernetes.io/zh/docs/setup/best-practices/cluster-large/ CoreOS 是如何将 Kubernetes 的性能提高 10 倍的? 当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？ https://github.com/kubernetes-sigs/kubefed https://jimmysong.io/kubernetes-handbook/practice/federation.html https://kubernetes.io/blog/2018/12/12/kubernetes-federation-evolution/ https://zhuanlan.zhihu.com/p/355193315 ","categories":"","description":"","excerpt":"k8s多集群的思考 1. 为什么需要多集群 1、k8s单集群的承载能力有限。\nKubernetes v1.21 …","ref":"/kubernetes-notes/multi-cluster/k8s-multi-cluster-thinking/","tags":["多集群"],"title":"k8s多集群管理的思考"},{"body":"1. Linux简介 严格来讲，Linux（内核）是计算机软件与硬件通信之间的平台，不是真正意义上的操作系统，而一些厂家将Linux内核和GNU软件（系统软件和工具）整合起来，并提供一些安装界面和系统设定与管理工具，就构成一些发行套件（系统），例如：Ubuntu、CentOS、Red Hat、Debian等。\nLinux内核版本\nLinux内核版本一般格式为：x.y.zz-www ，例如：Kernel2.6.15\nx.y：Linux内核主版本号，y若为奇数则表示是测试版 zz：次版本好 www：代表发行号 2. Linux体系结构 Linux体系结构如下：\n几个重要概念：\n内核：内核是操作系统的核心。内核直接与硬件交互，并处理大部分较低层的任务，如内存管理、进程调度、文件管理等。 Shell：Shell是一个处理用户请求的工具，它负责解释用户输入的命令，调用用户希望使用的程序。 命令和工具：日常工作中，你会用到很多系统命令和工具，如cp、mv、cat和grep等。 文件和目录：Linux系统中所有的数据都被存储到文件中，这些文件被分配到各个目录，构成文件系统。 3. 系统操作 3.1. 登录Linux 登录需要输入用户名和密码，用户名和密码是区分大小写。\nlogin : amrood amrood's password: Last login: Sun Jun 14 09:32:32 2009 from 62.61.164.73 $ 3.2. 修改密码 输入password命令后，输入原密码和新密码，确认密码即可。\n$ passwd Changing password for amrood (current) Linux password:****** New Linux password:******* Retype new Linux password:******* passwd: all authentication tokens updated successfully 3.3. 查看当前用户 1、查看自己的用户名\n$ whoami amrood 2、查看当前在线用户\n可以使用users 、who、w命令。\n$ users amrood bablu qadir $ who amrood ttyp0 Oct 8 14:10 (limbo) bablu ttyp2 Oct 4 09:08 (calliope) qadir ttyp4 Oct 8 12:09 (dent) $ w 13:58:53 up 158 days, 22:07, 3 users, load average: 0.72, 0.99, 1.11 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/1 172.16.20.65 13:40 0.00s 0.22s 0.02s w root pts/2 172.16.20.65 Fri15 43:17m 1.04s 1.04s -bash 3.4. 关闭系统 关闭系统可以使用以下命令\n命令 说明 halt 直接关闭系统 init 0 使用预先定义的脚本关闭系统，关闭前可以清理和更新有关信息 init 6 重新启动系统 poweroff 通过断电来关闭系统 reboot 重新启动系统 shutdown 安全关闭系统 一般只有root有关闭系统的权限，普通用户被赋予相应权限也可以关闭系统。\n","categories":"","description":"","excerpt":"1. Linux简介 严格来讲，Linux（内核）是计算机软件与硬件通信之间的平台，不是真正意义上的操作系统，而一些厂家将Linux内核 …","ref":"/linux-notes/file/linux-introduction/","tags":["Linux"],"title":"Linux介绍"},{"body":"1. 判断磁盘是SSD或HDD盘 1、没有使用raid方案\nlsblk -d -o name,rota命令，0表示SSD，1表示HDD\n# lsblk -d -o name,rota NAME ROTA sda 0 sdb 1 sdc 1 2、使用raid方案\n下载工具\nwget https://raw.githubusercontent.com/eLvErDe/hwraid/master/wrapper-scripts/megaclisas-status 执行检测命令\n$ megaclisas-status -- Controller information -- -- ID | H/W Model | RAM | Temp | BBU | Firmware c0 | SAS3508 | 2048MB | 55C | Good | FW: 50.6.3-0109 -- Array information -- -- ID | Type | Size | Strpsz | Flags | DskCache | Status | OS Path | CacheCade |InProgress c0u0 | RAID-1 | 1089G | 256 KB | RA,WB | Default | Optimal | /dev/sda | None |None c0u1 | RAID-5 | 2616G | 256 KB | RA,WB | Default | Optimal | /dev/sdb | None |None -- Disk information -- -- ID | Type | Drive Model | Size | Status | Speed | Temp | Slot ID | LSI ID c0u0p0 | HDD | TOSHIBA AL15SEB120N 080710R0A0LJFDWG | 1.089 TB | Online, Spun Up | 12.0Gb/s | 27C | [134:4] | 0 c0u0p1 | HDD | TOSHIBA AL15SEB120N 080710S0A10SFDWG | 1.089 TB | Online, Spun Up | 12.0Gb/s | 28C | [134:5] | 5 c0u1p0 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002816 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:0] | 2 c0u1p1 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002799 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 30C | [134:1] | 4 c0u1p2 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002805 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:2] | 1 c0u1p3 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002797 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:3] | 3 2. 解决umount target is busy挂载盘卸载不掉问题 问题描述:\n由于有进程占用目录，因此无法umount目录，需要先将占用进程杀死，再umount目录。\n$ umount /data umount: /data: target is busy. 查看目录占用进程：\n# fuser -mv /mnt/ USER PID ACCESS COMMAND /mnt: root kernel mount /mnt root 13830 ..c.. bash 杀死目录占用进程\n# fuser -kv /mnt/ USER PID ACCESS COMMAND /mnt: root kernel mount /mnt root 13830 ..c.. bash # 检查目录占用进程 # fuser -mv /mnt/ # umount /mnt fuser命令参数说明\n-k,--kill kill processes accessing the named file -m,--mount show all processes using the named filesystems or block device -v,--verbose verbose output 3. 删除大文件后磁盘空间没减少 问题描述：\n为了清理磁盘空间，在删除某些大文件后，查看df -h发现磁盘的空间并没减少。\n原因：\n有进程占用了被删除的文件，导致空间没有清理。\n解决方案:\n查看所占用文件的进程，并杀掉该进程。\n# lsof 查看删除文件对应的进程 lsof | grep deleted|grep \"/data/xxxx\" # 杀掉进程 kill -9 {pid} # 查看空间,此时空间已经腾出来 df -h ","categories":"","description":"","excerpt":"1. 判断磁盘是SSD或HDD盘 1、没有使用raid方案\nlsblk -d -o name,rota命令，0表示SSD，1表示HDD\n# …","ref":"/linux-notes/disk/disk-command/","tags":["disk"],"title":"磁盘命令"},{"body":"1. Cobra简介 Cobra是一个cli接口模式的应用程序框架，同时也是生成该框架的命令行工具。用户可以通过help方式快速查看该二进制的使用方式。\nCobra主要包括以下部分\nCommand:一般表示action，即运行的二进制命令服务。同时可以拥有子命令（children commands）。 Args:命令执行相关参数。 Flags:二进制命令的配置参数，可对应配置文件。参数可分为全局参数和子命令参数。参考：pflag library。 2. 安装 通过以下操作可以在$GOPATH/bin安装cobra的二进制命令。\ngo get -u github.com/spf13/cobra/cobra 3. 使用 cobra命令行帮助信息如下：\n# cobra Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobra [command] Available Commands: add Add a command to a Cobra Application help Help about any command init Initialize a Cobra Application Flags: -a, --author string author name for copyright attribution (default \"YOUR NAME\") --config string config file (default is $HOME/.cobra.yaml) -h, --help help for cobra -l, --license string name of license for the project --viper use Viper for configuration (default true) Use \"cobra [command] --help\" for more information about a command. 3.1. cobra init # cobra init --help Initialize (cobra init) will create a new application, with a license and the appropriate structure for a Cobra-based CLI application. * If a name is provided, it will be created in the current directory; * If no name is provided, the current directory will be assumed; * If a relative path is provided, it will be created inside $GOPATH (e.g. github.com/spf13/hugo); * If an absolute path is provided, it will be created; * If the directory already exists but is empty, it will be used. Init will not use an existing directory with contents. Usage: cobra init [name] [flags] Aliases: init, initialize, initialise, create Flags: -h, --help help for init --pkg-name string fully qualified pkg name Global Flags: -a, --author string author name for copyright attribution (default \"YOUR NAME\") --config string config file (default is $HOME/.cobra.yaml) -l, --license string name of license for the project --viper use Viper for configuration (default true) 3.2. cobra add # cobra add --help Add (cobra add) will create a new command, with a license and the appropriate structure for a Cobra-based CLI application, and register it to its parent (default rootCmd). If you want your command to be public, pass in the command name with an initial uppercase letter. Example: cobra add server -\u003e resulting in a new cmd/server.go Usage: cobra add [command name] [flags] Aliases: add, command Flags: -h, --help help for add -p, --parent string variable name of parent command for this command (default \"rootCmd\") Global Flags: -a, --author string author name for copyright attribution (default \"YOUR NAME\") --config string config file (default is $HOME/.cobra.yaml) -l, --license string name of license for the project --viper use Viper for configuration (default true) 参考：\nhttps://github.com/spf13/cobra https://github.com/spf13/cobra/blob/master/cobra/README.md ","categories":"","description":"","excerpt":"1. Cobra简介 Cobra是一个cli接口模式的应用程序框架，同时也是生成该框架的命令行工具。用户可以通过help方式快速查看该二进制 …","ref":"/golang-notes/framework/cobra/cobra-usage/","tags":["Golang"],"title":"cobra 介绍"},{"body":" 本文主要介绍Go的调度模型。\n1. 线程实现模型 线程模型有三类：内核级线程模型、用户级线程模型、混合型线程模型。三者的区别主要在于线程与内核调度实体KSE(Kernel Scheduling Entity)之间的对应关系上。\n内核调度实体KSE指操作系统内核调度器调度的对象实体，是内核调度的最小单元。\n1.1. 线程模型对比 线程模型 用户线程与KSE之间的对应关系 特点 优点 缺点 内核级线程模型 1:1 1条用户线程对应一条内核进程/线程来调度，即以核心态线程实现。 具有和内核线程一致的优点，不同用户线程之间不会互相影响。可以利用多核系统的优势。 在大量线程的情况下，线程的创建、删除、切换的代价更昂贵，影响性能。 用户级线程模型 M:1 N条用户线程只由一条内核进程/线程调度，即以用户态线程实现。 线程的创建、删除和环境切换都很高效。 一旦一个线程发生阻塞，整个进程下的其他线程也会被阻塞。不能利用多核系统的优势。 混合型线程模型 M:N M条用户线程由N条内核线程动态关联。又称两级线程模型 可以快速地执行上下文切换，而且可以利用多核的优势。当某个线程发生阻塞可以调度出CPU关联到可以执行的线程上。目前Go就是采用这种线程模型。 动态关联机制实现复杂，需要用户或runtime自己去实现。 1.2. 线程模型示意图 2. G-P-M调度模型 调度模型:\nG-P-M对应关系：\n2.1. 基本概念 M：machine，代表系统内核进程，用来执行G。（工人） P：processor，代表调度执行的上下文（context），维护了一个本地的goroutine的队列。（小推车） G：goroutine，代表goroutine，即执行的goroutine的数据结构及栈等。（砖头） 2.2. 基本流程 调度的本质是将G尽量均匀合理地安排给M来执行，其中P的作用就是来实现合理安排逻辑。\nP的数量通过 GOMAXPROCS() 来设置，一般等于CPU的核数，对于一次代码执行设置好一般不会变。 P维护了一个本地的G队列（runqueue），包括正在执行和待执行的G，尽量保证所有的P都匹配一个M同时在执行G。 当P本地goroutine队列消费完，会从全局的goroutine队列（global runqueue）中拿goroutine到本地队列。P也会定期检查全局的goroutine队列，避免存在全局的goroutine没有被执行而\"饿死\"的现象。 P和M是动态形式的一对一的关系，P和G是动态形式的一对多的关系。 2.3. 抢占式调度（阻塞） 当goroutine发生阻塞的时候，可以通过P将剩余的G切换给新的M来执行，而不会导致剩余的G无法执行，如果没有M则创建M来匹配P。\n当阻塞的goroutine返回后，进程会尝试获取一个上下文（Context）来执行这个goroutine。一般是先从其他进程中\"偷取\"一个Context，如果\"偷取\"不成功，则将goroutine放入全局的goroutine中。\n2.4. 偷任务 P可以偷任务即goroutine，当某个P的本地G执行完，且全局没有G需要执行的时候，P可以去偷别的P还没有执行完的一半的G来给M执行，提高了G的执行效率。\n参考：\nhttp://morsmachine.dk/go-scheduler Scalable Go Scheduler Design Doc Go Preemptive Scheduler Design Doc k2huang/blogpost/Go并发机制 ","categories":"","description":"","excerpt":" 本文主要介绍Go的调度模型。\n1. 线程实现模型 线程模型有三类：内核级线程模型、用户级线程模型、混合型线程模型。三者的区别主要在于线程与 …","ref":"/golang-notes/principle/go-scheduler/","tags":["Golang"],"title":"Goroutine调度"},{"body":"1. 简介 Virtual Kubelet是 Kubernetes kubelet 的一种实现，作为一种虚拟的kubelet用来连接k8s集群和其他平台的API。这允许k8s的节点由其他提供者（provider）提供支持，这些提供者例如serverless平台（ACI, AWS Fargate）、IoT Edge等。\n一句话概括：Kubernetes API on top, programmable back。\n2. 架构图 3. 功能 virtual kubelet提供一个可以自定义k8s node的依赖库。\n目前支持的功能如下：\n创建、删除、更新 pod 容器的日志、exec命令、metrics 获取pod、pod列表、pod status node的地址、容量、daemon 操作系统 自定义virtual network 4. Providers virtual kubelet提供一个插件式的provider接口，让开发者可以自定义实现传统kubelet的功能。自定义的provider可以用自己的配置文件和环境参数。\n自定义的provider必须提供以下功能：\n提供pod、容器、资源的生命周期管理的功能 符合virtual kubelet提供的API 不直接访问k8s apiserver，定义获取数据的回调机制，例如configmap、secrets 开源的provider\nAlibaba Cloud ECI Provider Azure Container Instances Provider AWS Fargate Provider 5. 自定义provider 创建自定义provider的目录。\ngit clone https://github.com/virtual-kubelet/virtual-kubelet cd virtual-kubelet mkdir providers/my-provider 5.1. PodLifecylceHandler 当pod被k8s创建、更新、删除时，会调用以下方法。\ntype PodLifecycleHandler interface { // CreatePod takes a Kubernetes Pod and deploys it within the provider. CreatePod(ctx context.Context, pod *corev1.Pod) error // UpdatePod takes a Kubernetes Pod and updates it within the provider. UpdatePod(ctx context.Context, pod *corev1.Pod) error // DeletePod takes a Kubernetes Pod and deletes it from the provider. DeletePod(ctx context.Context, pod *corev1.Pod) error // GetPod retrieves a pod by name from the provider (can be cached). GetPod(ctx context.Context, namespace, name string) (*corev1.Pod, error) // GetPodStatus retrieves the status of a pod by name from the provider. GetPodStatus(ctx context.Context, namespace, name string) (*corev1.PodStatus, error) // GetPods retrieves a list of all pods running on the provider (can be cached). GetPods(context.Context) ([]*corev1.Pod, error) } PodLifecycleHandler是被PodController来调用，来管理被分配到node上的pod。\npc, _ := node.NewPodController(podControllerConfig) // \u003c-- instatiates the pod controller pc.Run(ctx) // \u003c-- starts watching for pods to be scheduled on the node 5.2. PodNotifier(optional) PodNotifier是可选实现，该接口主要用来通知virtual kubelet的pod状态变化。如果没有实现该接口，virtual-kubelet会定期检查所有pod的状态。\ntype PodNotifier interface { // NotifyPods instructs the notifier to call the passed in function when // the pod status changes. // // NotifyPods should not block callers. NotifyPods(context.Context, func(*corev1.Pod)) } 5.3. NodeProvider NodeProvider用来通知virtual-kubelet关于node状态的变化，virtual-kubelet会定期检查node是状态并相应地更新k8s。\ntype NodeProvider interface { // Ping checks if the node is still active. // This is intended to be lightweight as it will be called periodically as a // heartbeat to keep the node marked as ready in Kubernetes. Ping(context.Context) error // NotifyNodeStatus is used to asynchronously monitor the node. // The passed in callback should be called any time there is a change to the // node's status. // This will generally trigger a call to the Kubernetes API server to update // the status. // // NotifyNodeStatus should not block callers. NotifyNodeStatus(ctx context.Context, cb func(*corev1.Node)) } NodeProvider是被NodeController调用，来管理k8s中的node对象。\nnc, _ := node.NewNodeController(nodeProvider, nodeSpec) // \u003c-- instantiate a node controller from a node provider and a kubernetes node spec nc.Run(ctx) // \u003c-- creates the node in kubernetes and starts up he controller 5.4. 测试 进入到项目根目录\nmake test 5.5. 示例代码 Azure Container Instances Provider\nhttps://github.com/virtual-kubelet/azure-aci/blob/master/aci.go#L541\nAlibaba Cloud ECI Provider\nhttps://github.com/virtual-kubelet/alibabacloud-eci/blob/master/eci.go#L177\nAWS Fargate Provider\nhttps://github.com/virtual-kubelet/aws-fargate/blob/master/provider.go#L110\n参考：\nhttps://github.com/virtual-kubelet/virtual-kubelet https://virtual-kubelet.io/docs/ ","categories":"","description":"","excerpt":"1. 简介 Virtual Kubelet是 Kubernetes kubelet 的一种实现，作为一种虚拟的kubelet用来连接k8s集 …","ref":"/kubernetes-notes/multi-cluster/virtual-kubelet/virtual-kubelet/","tags":["VirtualKubelet"],"title":"Virtual Kubelet介绍"},{"body":"1. Git是什么 1.1. 概述 Git是分布式版本控制系统，与SVN类似的集中化版本控制系统相比，集中化版本控制系统如果中央服务器宕机则会影响数据和协同开发。\nGit是分布式的版本控制系统，客户端不只是提取最新版本的快照，而且将整个代码仓库镜像复制下来。如果任何协同工作用的服务器发生故障了，也可以用任何一个代码仓库来恢复。而且在协作服务器宕机期间，你也可以提交代码到本地仓库，当协作服务器正常工作后，你再将本地仓库同步到远程仓库。\n1.2. 特性 能够对文件版本控制和多人协作开发 拥有强大的分支特性，所以能够灵活地以不同的工作流协同开发 分布式版本控制系统，即使协作服务器宕机，也能继续提交代码或文件到本地仓库，当协作服务器恢复正常工作时，再将本地仓库同步到远程仓库。 当团队中某个成员完成某个功能时，通过pull request操作来通知其他团队成员，其他团队成员能够review code后再合并代码。 2. 为什么要用Git 能够对文件版本控制和多人协作开发 拥有强大的分支特性，所以能够灵活地以不同的工作流协同开发 分布式版本控制系统，即使协作服务器宕机，也能继续提交代码或文件到本地仓库，当协作服务器恢复正常工作时，再将本地仓库同步到远程仓库。 当团队中某个成员完成某个功能时，通过pull request操作来通知其他团队成员，其他团队成员能够review code后再合并代码。 3. Git 命令思维导图 ","categories":"","description":"","excerpt":"1. Git是什么 1.1. 概述 Git是分布式版本控制系统，与SVN类似的集中化版本控制系统相比，集中化版本控制系统如果中央服务器宕机则 …","ref":"/linux-notes/git/git/","tags":["Git"],"title":"Git介绍"},{"body":"1. Keepalived简介 1.1. 概述 Keepalived一个基于VRRP协议来实现的LVS服务高可用方案，可以利用其来避免单点故障。一个LVS服务会有2台服务器运行Keepalived，一台为主服务器（MASTER），一台为备份服务器（BACKUP），但是对外表现为一个虚拟IP，主服务器会发送特定的消息给备份服务器，当备份服务器收不到这个消息的时候，即主服务器宕机的时候， 备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性。\n1.2. keepalived的作用 Keepalived的作用是检测服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中。\n2. 如何实现Keepalived 2.1. 基于VRRP协议的理解 Keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。\n虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master。这样的话就可以保证路由器的高可用了。\nkeepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式。vrrp模块是来实现VRRP协议的。\n2.2. 基于TCP/IP协议的理解 Layer3,4\u00267工作在IP/TCP协议栈的IP层，TCP层，及应用层,原理分别如下：\nLayer3：\nKeepalived使用Layer3的方式工作式时，Keepalived会定期向服务器群中的服务器发送一个ICMP的数据包（既我们平时用的Ping程序）,如果发现某台服务的IP地址没有激活，Keepalived便报告这台服务器失效，并将它从服务器群中剔除，这种情况的典型例子是某台服务器被非法关机。Layer3的方式是以服务器的IP地址是否有效作为服务器工作正常与否的标准。\nLayer4:\n如果您理解了Layer3的方式，Layer4就容易了。Layer4主要以TCP端口的状态来决定服务器工作正常与否。如web server的服务端口一般是80，如果Keepalived检测到80端口没有启动，则Keepalived将把这台服务器从服务器群中剔除。\nLayer7：\nLayer7就是工作在具体的应用层了，比Layer3,Layer4要复杂一点，在网络上占用的带宽也要大一些。Keepalived将根据用户的设定检查服务器程序的运行是否正常，如果与用户的设定不相符，则Keepalived将把服务器从服务器群中剔除。\n3. Keepalived选举策略 3.1. 选举策略 首先，每个节点有一个初始优先级，由配置文件中的priority配置项指定，MASTER节点的priority应比BAKCUP高。运行过程中keepalived根据vrrp_script的weight设定，增加或减小节点优先级。规则如下：\n“weight”值为正时,脚本检测成功时”weight”值会加到”priority”上,检测失败是不加 主失败: 主priority\u003c备priority+weight之和时会切换\n主成功: 主priority+weight之和\u003e备priority+weight之和时,主依然为主,即不发生切换\n“weight”为负数时,脚本检测成功时”weight”不影响”priority”,检测失败时,Master节点的权值将是“priority“值与“weight”值之差 主失败: 主priotity-abs(weight) \u003c 备priority时会发生切换\n主成功: 主priority \u003e 备priority 不切换\n当两个节点的优先级相同时，以节点发送VRRP通告的IP作为比较对象，IP较大者为MASTER。 3.2. priority和weight的设定 主从的优先级初始值priority和变化量weight设置非常关键，配错的话会导致无法进行主从切换。比如，当MASTER初始值定得太高，即使script脚本执行失败，也比BACKUP的priority + weight大，就没法进行VIP漂移了。\n所以priority和weight值的设定应遵循: abs(MASTER priority - BAKCUP priority) \u003c abs(weight)。一般情况下，初始值MASTER的priority值应该比较BACKUP大，但不能超过weight的绝对值。 另外，当网络中不支持多播(例如某些云环境)，或者出现网络分区的情况，keepalived BACKUP节点收不到MASTER的VRRP通告，就会出现脑裂(split brain)现象，此时集群中会存在多个MASTER节点。\n","categories":"","description":"","excerpt":"1. Keepalived简介 1.1. 概述 Keepalived一个基于VRRP协议来实现的LVS服务高可用方案，可以利用其来避免单点故 …","ref":"/linux-notes/keepalived/keepalived-introduction/","tags":["Keepalived"],"title":"Keepalived简介"},{"body":"vscode快捷键 1. 基本快捷键 1.1. VsCode 快捷键有五种组合方式 Ctrl + Shift + ? : 这种常规组合按钮 Ctrl + V Ctrl +V : 同时依赖一个按键的组合 Shift + V c : 先组合后单键的输入 Ctrl + Click: 键盘 + 鼠标点击 Ctrl + DragMouse : 键盘 + 鼠标拖动 1.2. Ctrl+P 模式 在Ctrl+P下输入\u003e又可以回到主命令框 Ctrl+Shift+P模式。\n在Ctrl+P窗口下还可以\n直接输入文件名，快速打开文件 ? 列出当前可执行的动作 ! 显示Errors或Warnings，也可以Ctrl+Shift+M : 跳转到行数，也可以Ctrl+G直接进入 @ 跳转到symbol（搜索变量或者函数），也可以Ctrl+Shift+O直接进入 @:根据分类跳转symbol，查找属性或函数，也可以Ctrl+Shift+O后输入:进入 # 根据名字查找symbol，也可以Ctrl+T 2. 快捷键分类 2.1. 通用快捷键 快捷键 作用 Ctrl+Shift+P,F1 展示全局命令面板 Ctrl+P 快速打开最近打开的文件 Ctrl+Shift+N 打开新的编辑器窗口 Ctrl+Shift+W 关闭编辑器 2.2. 基础编辑 快捷键 作用 Ctrl + X 剪切 Ctrl + C 复制 Alt + up/down 移动行上下 Shift + Alt up/down 在当前行上下复制当前行 Ctrl + Shift + K 删除行 Ctrl + Enter 在当前行下插入新的一行 Ctrl + Shift + Enter 在当前行上插入新的一行 Ctrl + Shift + | 匹配花括号的闭合处，跳转 Ctrl + ] / [ 行缩进 Home 光标跳转到行头 End 光标跳转到行尾 Ctrl + Home 跳转到页头 Ctrl + End 跳转到页尾 Ctrl + up/down 行视图上下偏移 Alt + PgUp/PgDown 屏视图上下偏移 Ctrl + Shift + [ 折叠区域代码 Ctrl + Shift + ] 展开区域代码 Ctrl + K Ctrl + [ 折叠所有子区域代码 Ctrl + k Ctrl + ] 展开所有折叠的子区域代码 Ctrl + K Ctrl + 0 折叠所有区域代码 Ctrl + K Ctrl + J 展开所有折叠区域代码 Ctrl + K Ctrl + C 添加行注释 Ctrl + K Ctrl + U 删除行注释 Ctrl + / 添加关闭行注释 Shift + Alt +A 块区域注释 Alt + Z 添加关闭词汇包含 2.3. 导航 快捷键 作用 Ctrl + T 列出所有符号 Ctrl + G 跳转行 Ctrl + P 跳转文件 Ctrl + Shift + O 跳转到符号处 Ctrl + Shift + M 打开问题展示面板 F8 跳转到下一个错误或者警告 Shift + F8 跳转到上一个错误或者警告 Ctrl + Shift + Tab 切换到最近打开的文件 Alt + left / right 向后、向前 Ctrl + M 进入用Tab来移动焦点 2.4. 查询与替换 快捷键 作用 Ctrl + F 查询 Ctrl + H 替换 F3 / Shift + F3 查询下一个/上一个 Alt + Enter 选中所有出现在查询中的 Ctrl + D 匹配当前选中的词汇或者行，再次选中-可操作 Ctrl + K Ctrl + D 移动当前选择到下个匹配选择的位置(光标选定) Alt + C / R / W 2.5. 多行光标操作于选择 快捷键 作用 Alt + Click 插入光标-支持多个 Ctrl + Alt + up/down 上下插入光标-支持多个 Ctrl + U 撤销最后一次光标操作 Shift + Alt + I 插入光标到选中范围内所有行结束符 Ctrl + I 选中当前行 Ctrl + Shift + L 选择所有出现在当前选中的行-操作 Ctrl + F2 选择所有出现在当前选中的词汇-操作 Shift + Alt + right 从光标处扩展选中全行 Shift + Alt + left 收缩选择区域 Shift + Alt + (drag mouse) 鼠标拖动区域，同时在多个行结束符插入光标 Ctrl + Shift + Alt + (Arrow Key) 也是插入多行光标的[方向键控制] Ctrl + Shift + Alt + PgUp/PgDown 也是插入多行光标的[整屏生效] Shift + Alt +鼠标选择块 多行 块选择编辑 2.6. 丰富的语言操作 快捷键 作用 Ctrl + Space 输入建议[智能提示] Ctrl + Shift + Space 参数提示 Tab Emmet指令触发/缩进 Shift + Alt + F 格式化代码 Ctrl + K Ctrl + F 格式化选中部分的代码 F12 跳转到定义处 ctrl + - 跳回原处（跳转前位置） Alt + F12 代码片段显示定义 Ctrl + K F12 在其他窗口打开定义处 Ctrl + . 快速修复部分可以修复的语法错误 Shift + F12 显示所有引用 F2 重命名符号 Ctrl + Shift + . / , 替换下个值 Ctrl + K Ctrl + X 移除空白字符 Ctrl + K M 更改页面文档格式 2.7. 编辑器管理 快捷键 作用 Ctrl + F4, Ctrl + W 关闭编辑器 Ctrl + k F 关闭当前打开的文件夹 Ctrl + |切割编辑窗口 Ctrl + 1/2/3 切换焦点在不同的切割窗口 Ctrl + K Ctrl \u003c-/-\u003e 切换焦点在不同的切割窗口 Ctrl + Shift + PgUp/PgDown 切换标签页的位置 Ctrl + K \u003c-/-\u003e 切割窗口位置调换 2.8. 文件管理 快捷键 作用 Ctrl + N 新建文件 Ctrl + O 打开文件 Ctrl + S 保存文件 Ctrl + Shift + S 另存为 Ctrl + K S 保存所有当前已经打开的文件 Ctrl + F4 关闭当前编辑窗口 Ctrl + K Ctrl + W 关闭所有编辑窗口 Ctrl + Shift + T 撤销最近关闭的一个文件编辑窗口 Ctrl + K Enter 保持开启 Ctrl + Shift + Tab 调出最近打开的文件列表，重复按会切换 Ctrl + Tab 与上面一致，顺序不一致 Ctrl + K P 复制当前打开文件的存放路径 Ctrl + K R 打开当前编辑文件存放位置【文件管理器】 Ctrl + K O 在新的编辑器中打开当前编辑的文件 2.9. 显示 快捷键 作用 F11 切换全屏模式 Shift + Alt + 1 切换编辑布局【目前无效】 Ctrl + =/- 放大 / 缩小 Ctrl + B 侧边栏显示隐藏 Ctrl + Shift + E 资源视图和编辑视图的焦点切换 Ctrl + Shift + F 打开全局搜索 Ctrl + Shift + G 打开Git可视管理 Ctrl + Shift + D 打开DeBug面板 Ctrl + Shift + X 打开插件市场面板 Ctrl + Shift + H 在当前文件替换查询替换 Ctrl + Shift + J 开启详细查询 Ctrl + Shift + V 预览Markdown文件【编译后】 Ctrl + K v 在边栏打开渲染后的视图【新建】 2.10. 调试 快捷键 作用 F9 添加解除断点 F5 启动调试、继续 F11 / Shift + F11 单步进入 / 单步跳出 F10 单步跳过 Ctrl + K Ctrl + I 显示悬浮 2.11. 集成终端 快捷键 作用 Ctrl + ` 打开集成终端 Ctrl + Shift + ` 创建一个新的终端 Ctrl + Shift + C 复制所选 Ctrl + Shift + V 复制到当前激活的终端 Shift + PgUp / PgDown 页面上下翻屏 Ctrl + Home / End 滚动到页面头部或尾部 ","categories":"","description":"","excerpt":"vscode快捷键 1. 基本快捷键 1.1. VsCode 快捷键有五种组合方式 Ctrl + Shift + ? : …","ref":"/linux-notes/keymap/vscode-keymap/","tags":["快捷键"],"title":"vscode快捷键"},{"body":"1. kubectl的安装 curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ 安装指定版本的kubectl，例如：v1.9.0\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ 2. 配置k8s集群环境 2.1. 命令行方式 2.1.1 非安全方式 kubectl config set-cluster k8s --server=http://\u003curl\u003e kubectl config set-context \u003cNAMESPACE\u003e --cluster=k8s --namespace=\u003cNAMESPACE\u003e kubectl config use-context \u003cNAMESPACE\u003e 2.1.2 安全方式 kubectl config set-cluster k8s --server=https://\u003curl\u003e --insecure-skip-tls-verify=true kubectl config set-credentials k8s-user --username=\u003cusername\u003e --password=\u003cpassword\u003e kubectl config set-context \u003cNAMESPACE\u003e --cluster=k8s --user=k8s-user --namespace=\u003cNAMESPACE\u003e kubectl config use-context \u003cNAMESPACE\u003e 2.1.3 查询当前配置环境 [root@test ]# kubectl cluster-info Kubernetes master is running at http://192.168.10.3:8081 2.2. 添加配置文件的方式 当没有指定 --kubeconfig参数和$KUBECONFIG的环境变量的时候，会默认读取${HOME}/.kube/config。\n因此创建${HOME}/.kube/config文件，并在``${HOME}/.kube/ssl`目录下创建ca.pem、cert.pem、key.pem文件。\n内容如下：\napiVersion: v1 kind: Config clusters: - name: local cluster: certificate-authority: ./ssl/ca.pem server: https://192.168.10.3:6443 users: - name: kubelet user: client-certificate: ./ssl/cert.pem client-key: ./ssl/key.pem contexts: - context: cluster: local user: kubelet name: kubelet-cluster.local current-context: kubelet-cluster.local 3. kubectl config kubectl config命令说明\n$ kubectl config --help Modify kubeconfig files using subcommands like \"kubectl config set current-context my-context\" The loading order follows these rules: 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used a list of paths (normal path delimitting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Available Commands: current-context Displays the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts rename-context Renames a context from the kubeconfig file. set Sets an individual value in a kubeconfig file set-cluster Sets a cluster entry in kubeconfig set-context Sets a context entry in kubeconfig set-credentials Sets a user entry in kubeconfig unset Unsets an individual value in a kubeconfig file use-context Sets the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file Usage: kubectl config SUBCOMMAND [options] Use \"kubectl \u003ccommand\u003e --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). 4. shell自动补齐 source \u003c(kubectl completion bash) echo \"source \u003c(kubectl completion bash)\" \u003e\u003e ~/.bashrc 如果出现以下报错\n# kubectl自动补齐失败 kubectl _get_comp_words_by_ref : command not found 解决方法：\nyum install bash-completion -y source /etc/profile.d/bash_completion.sh 参考文章：\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl/ ","categories":"","description":"","excerpt":"1. kubectl的安装 curl -LO …","ref":"/kubernetes-notes/operation/kubectl/install-kubectl/","tags":["Kubernetes"],"title":"kubectl安装与配置"},{"body":"1. 虚拟化 借助虚拟化技术，用户能以单个物理硬件系统为基础，创建多个模拟环境或专用资源，并使用一款名为“Hypervisor”（虚拟机监控程序）的软件直接连接到硬件，从而将一个系统划分为不同、单独而安全的环境，即虚拟机 (VM)。\n虚拟化技术可以重新划分IT资源，提高资源的利用率。\n2. 虚拟化的类型 全虚拟化（Full virtualization）\n全虚拟化使用未修改的guest操作系统版本，guest直接与CPU通信，是最快的虚拟化方法。\n半虚拟化（Paravirtualization）\n半虚拟化使用修改过的guest操作系统，guest与hypervisor通信，hypervisor将guest的调用传递给CPU和其他接口。因为通信经过hypervisor，因此比全虚拟化慢。\n3. hypervisor hypervisor又称为 virtual machine monitor (VMM)，是一个创建和运行虚拟机的程序。被 hypervisor 用来运行一个或多个虚拟机的计算机称为宿主机（host machine），这些虚拟机则称为客户机（guest machine）。\n在虚拟化技术中，Hypervisor（虚拟机监视器）是一种软件、固件或硬件，能够创建和运行虚拟机（VM）。Hypervisor 起到物理硬件与虚拟机之间的中介作用，管理和分配物理资源（如 CPU、内存、存储和网络）给虚拟机，并确保每个虚拟机的隔离性和安全性。\nHypervisor 的功能\n虚拟化资源管理： 分配物理硬件资源（CPU、内存、存储、网络）给虚拟机。 管理和调度虚拟机的运行。 虚拟机隔离： 确保虚拟机之间的隔离性，防止一个虚拟机的故障或安全问题影响其他虚拟机。 硬件抽象： 抽象底层硬件，使虚拟机可以运行在不同的硬件平台上，而无需修改虚拟机内的操作系统和应用程序。 资源优化： 动态调整虚拟机资源，提供负载均衡、资源共享和节能功能。 高可用性和容错： 提供虚拟机快照、备份和恢复功能，确保高可用性和数据安全。 支持虚拟机迁移和克隆，方便运维管理。 Hypervisor 的优点\n硬件利用率：通过虚拟化，可以更高效地利用物理硬件资源，减少硬件浪费。 灵活性：虚拟机可以轻松创建、删除和迁移，方便开发、测试和部署。 隔离性：虚拟机之间相互隔离，提高系统安全性和稳定性。 成本效益：减少对物理硬件的需求，降低硬件和维护成本。 4. kvm kvm(Kernel-based Virtual Machine)是Linux内核的虚拟化模块，可以利用Linux内核的功能来作为hypervisor。\nKVM本身不进行模拟，而是暴露一个/dev/kvm接口。\n使用KVM，可以在Linux的镜像上\n5. qemu QEMU（Quick Emulator）是一个开源的硬件虚拟化和仿真器软件。它被广泛用于创建和运行虚拟机，支持多种不同的硬件平台和操作系统。以下是对 QEMU 的详细介绍：\nQEMU 的主要特点\n虚拟化和仿真： 虚拟化：QEMU 能够利用硬件虚拟化技术（如 Intel VT-x 和 AMD-V）来运行虚拟机。通过硬件辅助虚拟化，QEMU 可以提供接近原生性能的虚拟机运行环境。 仿真：QEMU 还可以完全在软件中仿真硬件，无需硬件虚拟化支持。这种模式下，QEMU 能够仿真多种不同的 CPU 架构（如 x86、ARM、MIPS、PowerPC 等），适用于跨平台开发和测试。 多种平台支持： QEMU 支持多种不同的主机平台和目标平台，能够在一个平台上运行不同架构的操作系统。这使得 QEMU 成为跨平台开发和测试的理想工具。 与 KVM 集成： QEMU 可以与内核虚拟机（KVM）集成使用，以提高虚拟化性能。KVM 提供了基于 Linux 内核的高效虚拟化解决方案，而 QEMU 提供了强大的虚拟机管理和仿真功能。 设备仿真： QEMU 提供了丰富的设备仿真功能，包括网络接口、磁盘存储、图形显示、USB 设备等。这些设备仿真使得虚拟机能够模拟真实硬件环境，方便开发和测试。 快照和恢复： QEMU 支持虚拟机快照功能，允许用户在特定时刻保存虚拟机的状态，并在需要时恢复到该状态。这对于调试和测试非常有用。 6. libvirt Libvirt 是一个开源的虚拟化管理框架和工具集，用于管理不同的虚拟化技术，包括 KVM、QEMU、Xen、VMware ESXi、Hyper-V 以及其他一些虚拟化平台。它提供了一个统一的 API，用于创建、管理和监控虚拟机，使得对不同虚拟化技术的操作更加简化和一致。\n6.1. 特点 统一接口 提供了一个统一的 API 和命令行工具，使用户和开发者可以通过一致的接口管理不同的虚拟化技术。 多种虚拟化技术支持： 支持多种虚拟化平台，如 KVM、QEMU、Xen、VMware ESXi、Hyper-V、LXC（Linux 容器）等，提供跨平台的虚拟化管理功能。 管理和监控： 提供强大的管理和监控功能，包括创建、启动、停止、迁移虚拟机，以及管理存储和网络资源。 XML 配置： 使用 XML 配置文件来定义虚拟机和资源，提供灵活和可扩展的配置方式，易于保存和版本控制。 安全性： 支持多种安全机制，如基于 SELinux 的安全策略，确保虚拟机和宿主系统的隔离性和安全性。 网络管理： 提供网络桥接、NAT、虚拟交换机等网络管理功能，支持复杂的网络拓扑配置。 存储管理： 支持多种存储后端，如本地磁盘、网络文件系统（NFS）、iSCSI、LVM 等，方便虚拟机的存储资源管理。 6.2. 主要组件 libvirtd： Libvirt 的守护进程，负责处理客户端请求和管理虚拟机。通过这个守护进程，可以与不同的虚拟化后端进行交互。 virsh： Libvirt 的命令行工具，提供了一系列命令用于管理虚拟机和资源。通过 virsh，用户可以执行创建、启动、停止、迁移虚拟机等操作。 libvirt API： 提供 C 语言的库和 API，同时支持多种编程语言的绑定，如 Python、Perl、Ruby、Java、Go 等，使得开发者可以在不同的编程环境中使用 Libvirt 的功能。 6.3. virsh命令 # 列出所有虚拟机 virsh list --all # 启动虚拟机 virsh start \u003cvm_name\u003e # 关闭虚拟机 virsh shutdown \u003cvm_name\u003e # 定义新的虚拟机 virsh define /path/to/vm.xml # 创建并启动虚拟机 virsh create /path/to/vm.xml # 迁移虚拟机 virsh migrate --live \u003cvm_name\u003e qemu+ssh://destination_host/system 参考：\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_getting_started_guide/chap-virtualization_getting_started-what_is_it https://en.wikipedia.org/wiki/Hypervisor https://www.linux-kvm.org/page/Main_Page http://www.linux-kvm.org/page/Documents https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine https://wiki.qemu.org/Index.html ","categories":"","description":"","excerpt":"1. 虚拟化 借助虚拟化技术，用户能以单个物理硬件系统为基础，创建多个模拟环境或专用资源，并使用一款名为“Hypervisor”（虚拟机监控 …","ref":"/kubernetes-notes/kvm/vm-concept/","tags":["KubeVirt"],"title":"虚拟化相关概念"},{"body":"1. Memcached简介 Memcached是一个开源的，高性能，分布式内存对象缓存系统。\nMemcached是一种基于内存的key-value存储，用来存储小块的任意数据（字符串、对象）。这些数据可以是数据库调用、API调用或者是页面渲染的结果。\n一般的使用目的是，通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web应用的速度、提高可扩展性。\n1.1. 特征 memcached作为高速运行的分布式缓存服务器，具有以下的特点。\n协议简单 基于libevent的事件处理 内置内存存储方式 memcached不互相通信的分布式 2. 安装与运行 2.1. 自动安装 # For Redhat/Fedora yum install -y memcached # For Debian or Ubuntu apt-get install memcached 2.2. 源码安装 安装指定版本的Memcached可以从 https://github.com/memcached/memcached/wiki/ReleaseNotes 地址下载。\n# Memcached depends on libevent yum install libevent-devel # install wget https://memcached.org/latest [you might need to rename the file] tar -zxf memcached-1.x.x.tar.gz cd memcached-1.x.x ./configure --prefix=/usr/local/memcached make \u0026\u0026 make test \u0026\u0026 sudo make install 问题\n如遇以下报错，可再执行make install。\nSignal handled: Interrupt. ok 51 - shutdown ok 52 - stop_server /bin/sh:行3: prove: 未找到命令 make: *** [test] Error 127 2.3. 验证 确认是否安装成功，可执行以下命令\n/usr/local/memcached/bin/memcached -h 2.4. 运行 2.4.1. 前台运行 /usr/local/memcached/bin/memcached -p 11211 -m 64m -vv 2.4.2. 后台运行 /usr/local/memcached/bin/memcached -p 11211 -m 64m -d -c 102400 -t 8 -P /tmp/memcached.pid 2.5. 连接 $ telnet 127.0.0.1 11211 Trying 127.0.0.1... Connected to 127.0.0.1. Escape character is '^]'. set foo 0 0 3 保存命令 bar 数据 STORED 结果 get foo 取得命令 VALUE foo 0 3 数据 bar 数据 END 结束行 quit 退出 3. Memcached运行参数 # /usr/local/memcached/bin/memcached -h memcached 1.5.12 -p, --port=\u003cnum\u003e TCP port to listen on (default: 11211) -U, --udp-port=\u003cnum\u003e UDP port to listen on (default: 0, off) -s, --unix-socket=\u003cfile\u003e UNIX socket to listen on (disables network support) -A, --enable-shutdown enable ascii \"shutdown\" command -a, --unix-mask=\u003cmask\u003e access mask for UNIX socket, in octal (default: 0700) -l, --listen=\u003caddr\u003e interface to listen on (default: INADDR_ANY) -d, --daemon run as a daemon -r, --enable-coredumps maximize core file limit -u, --user=\u003cuser\u003e assume identity of \u003cusername\u003e (only when run as root) -m, --memory-limit=\u003cnum\u003e item memory in megabytes (default: 64 MB) -M, --disable-evictions return error on memory exhausted instead of evicting -c, --conn-limit=\u003cnum\u003e max simultaneous connections (default: 1024) -k, --lock-memory lock down all paged memory -v, --verbose verbose (print errors/warnings while in event loop) -vv very verbose (also print client commands/responses) -vvv extremely verbose (internal state transitions) -h, --help print this help and exit -i, --license print memcached and libevent license -V, --version print version and exit -P, --pidfile=\u003cfile\u003e save PID in \u003cfile\u003e, only used with -d option -f, --slab-growth-factor=\u003cnum\u003e chunk size growth factor (default: 1.25) -n, --slab-min-size=\u003cbytes\u003e min space used for key+value+flags (default: 48) -L, --enable-largepages try to use large memory pages (if available) -D \u003cchar\u003e Use \u003cchar\u003e as the delimiter between key prefixes and IDs. This is used for per-prefix stats reporting. The default is \":\" (colon). If this option is specified, stats collection is turned on automatically; if not, then it may be turned on by sending the \"stats detail on\" command to the server. -t, --threads=\u003cnum\u003e number of threads to use (default: 4) -R, --max-reqs-per-event maximum number of requests per event, limits the requests processed per connection to prevent starvation (default: 20) -C, --disable-cas disable use of CAS -b, --listen-backlog=\u003cnum\u003e set the backlog queue limit (default: 1024) -B, --protocol=\u003cname\u003e protocol - one of ascii, binary, or auto (default) -I, --max-item-size=\u003cnum\u003e adjusts max item size (default: 1mb, min: 1k, max: 128m) -F, --disable-flush-all disable flush_all command -X, --disable-dumping disable stats cachedump and lru_crawler metadump -o, --extended comma separated list of extended options most options have a 'no_' prefix to disable - maxconns_fast: immediately close new connections after limit - hashpower: an integer multiplier for how large the hash table should be. normally grows at runtime. set based on \"STAT hash_power_level\" - tail_repair_time: time in seconds for how long to wait before forcefully killing LRU tail item. disabled by default; very dangerous option. - hash_algorithm: the hash table algorithm default is murmur3 hash. options: jenkins, murmur3 - lru_crawler: enable LRU Crawler background thread - lru_crawler_sleep: microseconds to sleep between items default is 100. - lru_crawler_tocrawl: max items to crawl per slab per run default is 0 (unlimited) - lru_maintainer: enable new LRU system + background thread - hot_lru_pct: pct of slab memory to reserve for hot lru. (requires lru_maintainer) - warm_lru_pct: pct of slab memory to reserve for warm lru. (requires lru_maintainer) - hot_max_factor: items idle \u003e cold lru age * drop from hot lru. - warm_max_factor: items idle \u003e cold lru age * this drop from warm. - temporary_ttl: TTL's below get separate LRU, can't be evicted. (requires lru_maintainer) - idle_timeout: timeout for idle connections - slab_chunk_max: (EXPERIMENTAL) maximum slab size. use extreme care. - watcher_logbuf_size: size in kilobytes of per-watcher write buffer. - worker_logbuf_size: size in kilobytes of per-worker-thread buffer read by background thread, then written to watchers. - track_sizes: enable dynamic reports for 'stats sizes' command. - no_inline_ascii_resp: save up to 24 bytes per item. small perf hit in ASCII, no perf difference in binary protocol. speeds up all sets. - no_hashexpand: disables hash table expansion (dangerous) - modern: enables options which will be default in future. currently: nothing - no_modern: uses defaults of previous major version (1.4.x) 常用参数：\n-d是启动一个守护进程； -m是分配给Memcache使用的内存数量，单位是MB； -u是运行Memcache的用户； -l是监听的服务器IP地址，可以有多个地址； -p是设置Memcache监听的端口，，最好是1024以上的端口； -c是最大运行的并发连接数，默认是1024； -t是线程数，默认为4； -P是设置保存Memcache的pid文件。 参考文章：\nhttps://github.com/memcached/memcached/wiki/Overview https://github.com/memcached/memcached/wiki/Install http://www.runoob.com/memcached/memcached-tutorial.html ","categories":"","description":"","excerpt":"1. Memcached简介 Memcached是一个开源的，高性能，分布式内存对象缓存系统。\nMemcached是一种基于内存 …","ref":"/linux-notes/memcached/memcached/","tags":["Memcached"],"title":"Memcached的使用"},{"body":"1. 基础知识 1.1. 协议 计算机与网络设备要相互通信，必须基于相同的方法。比如，如何探测到通信目标，使用哪种语言通信，如何结束通信等规则要事先确定。\n不同硬件，操作系统之间的通信都需要一种规则，我们将这种事先约定好的规则称之为协议。\n1.2. 地址 地址：在某一范围内确认的唯一标识符，即数据包传到某一个范围，需要有一个明确唯一的目标地址。\n类型 层 地址 说明 端口号 传输层 程序地址 同一个计算机中不同的应用程序 IP地址 网络层 主机地址 识别TCP/IP网络中不同的主机或路由器 MAC地址 数据链路层 物理地址 在同一个数据链路中识别不同的计算机 1.3. 网络构成 构成要素 说明 网卡 连入网络必须使用网卡，又称网络接口卡。 中继器 OSI第1层，物理层上延长网络的设备，将电缆的信号放大传给另一个电缆。 网桥/2层交换机 OSI第2层，数据链路层面上连接两个网络的设备，识别数据帧的内容并转发给相邻的网段，根据MAC地址进行处理。 路由器/3层交换机 OSI第3层，网络层面连接两个网络并对分组报文进行转发，根据IP进行处理。 4-7层交换机 传输层到应用层，以TCP等协议分析收发数据，负载均衡器就是其中一种。 网关 对传输层到应用层的数据进行转换和转发的设备，通常会使用表示层或应用层的网关来处理不同协议之间的翻译和通信，代理服务器（proxy）就是应用网关的一种。 2. OSI与TCP/IP参考模型 2.1. OSI与TCP/IP参考模型图 2.2. OSI参考模型分层说明 2.3. OSI参考模型通信过程 1、打包数据时，每一层在处理上一层传过来的数据时，会在数据上附上当前层的首部信息后传给下一层；\n2、解包数据时，每一层在处理下一层传过来的数据时，会将当前层的首部信息与数据分开，将数据传给上一层。\n3、数据通信过程\n分层 每层的操作 应用层 在数据前面加首部，首部包括数据内容、源地址和目标地址，同时也会处理异常的反馈信息。 表示层 将特有的数据格式转换为通用的数据格式，同时也会加上表示层的首部信息以供解析。 会话层 对何时连接，以何种方式连接，连接多久，何时断开等做记录。同时也会加会话层的首部信息。 传输层 建立连接，断开连接，确认数据是否发送成功和执行失败重发任务。 网络层 负责将数据发到目标地址，也包含首部信息。 数据链路层 通过物理的传输介质实现数据的传输。 物理层 将0/1转换成物理的传输介质，通过MAC地址进行传输。 2.4. TCP/IP应用层协议 2.4.1. 通信模型 2.4.2. 应用层协议说明 应用类型 协议 协议说明 WWW HTTP,HTML 电子邮件 SMTP，MIME 文件传输 FTP 远程登录 TELNET,SSH 网络管理 SNMP,MIB 3. TCP/IP通信过程 3.1. 数据包结构 3.2. 数据打包和解包过程 3.2.1. 包的封装 3.2.2. 发送与接收 3.3. 数据包传输过程 文章：\n《图解TCP/IP》 ","categories":"","description":"","excerpt":"1. 基础知识 1.1. 协议 计算机与网络设备要相互通信，必须基于相同的方法。比如，如何探测到通信目标，使用哪种语言通信，如何结束通信等规 …","ref":"/linux-notes/tcpip/tcpip-basics/","tags":["TCPIP"],"title":"TCPIP基础"},{"body":"1. client-go简介 1.1 client-go说明 ​\tclient-go是一个调用kubernetes集群资源对象API的客户端，即通过client-go实现对kubernetes集群中资源对象（包括deployment、service、ingress、replicaSet、pod、namespace、node等）的增删改查等操作。大部分对kubernetes进行前置API封装的二次开发都通过client-go这个第三方包来实现。\n​\tclient-go官方文档：https://github.com/kubernetes/client-go\n1.2 示例代码 git clone https://github.com/huweihuang/client-go.git cd client-go #保证本地HOME目录有配置kubernetes集群的配置文件 go run client-go.go client-go.go\npackage main import ( \"flag\" \"fmt\" \"os\" \"path/filepath\" \"time\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/clientcmd\" ) func main() { var kubeconfig *string if home := homeDir(); home != \"\" { kubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\") } else { kubeconfig = flag.String(\"kubeconfig\", \"\", \"absolute path to the kubeconfig file\") } flag.Parse() // uses the current context in kubeconfig config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) if err != nil { panic(err.Error()) } // creates the clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } for { pods, err := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{}) if err != nil { panic(err.Error()) } fmt.Printf(\"There are %d pods in the cluster\\n\", len(pods.Items)) time.Sleep(10 * time.Second) } } func homeDir() string { if h := os.Getenv(\"HOME\"); h != \"\" { return h } return os.Getenv(\"USERPROFILE\") // windows } 1.3 运行结果 ➜ go run client-go.go There are 9 pods in the cluster There are 7 pods in the cluster There are 7 pods in the cluster There are 7 pods in the cluster There are 7 pods in the cluster 2. client-go源码分析 client-go源码：https://github.com/kubernetes/client-go\nclient-go源码目录结构\nThe kubernetes package contains the clientset to access Kubernetes API. The discovery package is used to discover APIs supported by a Kubernetes API server. The dynamic package contains a dynamic client that can perform generic operations on arbitrary Kubernetes API objects. The transport package is used to set up auth and start a connection. The tools/cache package is useful for writing controllers. 2.1 kubeconfig kubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\") 获取kubernetes配置文件kubeconfig的绝对路径。一般路径为$HOME/.kube/config。该文件主要用来配置本地连接的kubernetes集群。\nconfig内容如下：\napiVersion: v1 clusters: - cluster: server: http://\u003ckube-master-ip\u003e:8080 name: k8s contexts: - context: cluster: k8s namespace: default user: \"\" name: default current-context: default kind: Config preferences: {} users: [] 2.2 rest.config 通过参数（master的url或者kubeconfig路径）和BuildConfigFromFlags方法来获取rest.Config对象，一般是通过参数kubeconfig的路径。\nconfig, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) BuildConfigFromFlags函数源码\nk8s.io/client-go/tools/clientcmd/client_config.go\n// BuildConfigFromFlags is a helper function that builds configs from a master // url or a kubeconfig filepath. These are passed in as command line flags for cluster // components. Warnings should reflect this usage. If neither masterUrl or kubeconfigPath // are passed in we fallback to inClusterConfig. If inClusterConfig fails, we fallback // to the default config. func BuildConfigFromFlags(masterUrl, kubeconfigPath string) (*restclient.Config, error) { if kubeconfigPath == \"\" \u0026\u0026 masterUrl == \"\" { glog.Warningf(\"Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work.\") kubeconfig, err := restclient.InClusterConfig() if err == nil { return kubeconfig, nil } glog.Warning(\"error creating inClusterConfig, falling back to default config: \", err) } return NewNonInteractiveDeferredLoadingClientConfig( \u0026ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}, \u0026ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: masterUrl}}).ClientConfig() } 2.3 clientset 通过*rest.Config参数和NewForConfig方法来获取clientset对象，clientset是多个client的集合，每个client可能包含不同版本的方法调用。\nclientset, err := kubernetes.NewForConfig(config) 2.3.1 NewForConfig NewForConfig函数就是初始化clientset中的每个client。\nk8s.io/client-go/kubernetes/clientset.go\n// NewForConfig creates a new Clientset for the given config. func NewForConfig(c *rest.Config) (*Clientset, error) { configShallowCopy := *c ... var cs Clientset cs.appsV1beta1, err = appsv1beta1.NewForConfig(\u0026configShallowCopy) ... cs.coreV1, err = corev1.NewForConfig(\u0026configShallowCopy) ... } 2.3.2 clientset的结构体 k8s.io/client-go/kubernetes/clientset.go\n// Clientset contains the clients for groups. Each group has exactly one // version included in a Clientset. type Clientset struct { *discovery.DiscoveryClient admissionregistrationV1alpha1 *admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Client appsV1beta1 *appsv1beta1.AppsV1beta1Client appsV1beta2 *appsv1beta2.AppsV1beta2Client authenticationV1 *authenticationv1.AuthenticationV1Client authenticationV1beta1 *authenticationv1beta1.AuthenticationV1beta1Client authorizationV1 *authorizationv1.AuthorizationV1Client authorizationV1beta1 *authorizationv1beta1.AuthorizationV1beta1Client autoscalingV1 *autoscalingv1.AutoscalingV1Client autoscalingV2beta1 *autoscalingv2beta1.AutoscalingV2beta1Client batchV1 *batchv1.BatchV1Client batchV1beta1 *batchv1beta1.BatchV1beta1Client batchV2alpha1 *batchv2alpha1.BatchV2alpha1Client certificatesV1beta1 *certificatesv1beta1.CertificatesV1beta1Client coreV1 *corev1.CoreV1Client extensionsV1beta1 *extensionsv1beta1.ExtensionsV1beta1Client networkingV1 *networkingv1.NetworkingV1Client policyV1beta1 *policyv1beta1.PolicyV1beta1Client rbacV1 *rbacv1.RbacV1Client rbacV1beta1 *rbacv1beta1.RbacV1beta1Client rbacV1alpha1 *rbacv1alpha1.RbacV1alpha1Client schedulingV1alpha1 *schedulingv1alpha1.SchedulingV1alpha1Client settingsV1alpha1 *settingsv1alpha1.SettingsV1alpha1Client storageV1beta1 *storagev1beta1.StorageV1beta1Client storageV1 *storagev1.StorageV1Client } 2.3.3 clientset.Interface clientset实现了以下的Interface，因此可以通过调用以下方法获得具体的client。例如：\npods, err := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{}) clientset的方法集接口\nk8s.io/client-go/kubernetes/clientset.go\ntype Interface interface { Discovery() discovery.DiscoveryInterface AdmissionregistrationV1alpha1() admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Interface // Deprecated: please explicitly pick a version if possible. Admissionregistration() admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Interface AppsV1beta1() appsv1beta1.AppsV1beta1Interface AppsV1beta2() appsv1beta2.AppsV1beta2Interface // Deprecated: please explicitly pick a version if possible. Apps() appsv1beta2.AppsV1beta2Interface AuthenticationV1() authenticationv1.AuthenticationV1Interface // Deprecated: please explicitly pick a version if possible. Authentication() authenticationv1.AuthenticationV1Interface AuthenticationV1beta1() authenticationv1beta1.AuthenticationV1beta1Interface AuthorizationV1() authorizationv1.AuthorizationV1Interface // Deprecated: please explicitly pick a version if possible. Authorization() authorizationv1.AuthorizationV1Interface AuthorizationV1beta1() authorizationv1beta1.AuthorizationV1beta1Interface AutoscalingV1() autoscalingv1.AutoscalingV1Interface // Deprecated: please explicitly pick a version if possible. Autoscaling() autoscalingv1.AutoscalingV1Interface AutoscalingV2beta1() autoscalingv2beta1.AutoscalingV2beta1Interface BatchV1() batchv1.BatchV1Interface // Deprecated: please explicitly pick a version if possible. Batch() batchv1.BatchV1Interface BatchV1beta1() batchv1beta1.BatchV1beta1Interface BatchV2alpha1() batchv2alpha1.BatchV2alpha1Interface CertificatesV1beta1() certificatesv1beta1.CertificatesV1beta1Interface // Deprecated: please explicitly pick a version if possible. Certificates() certificatesv1beta1.CertificatesV1beta1Interface CoreV1() corev1.CoreV1Interface // Deprecated: please explicitly pick a version if possible. Core() corev1.CoreV1Interface ExtensionsV1beta1() extensionsv1beta1.ExtensionsV1beta1Interface // Deprecated: please explicitly pick a version if possible. Extensions() extensionsv1beta1.ExtensionsV1beta1Interface NetworkingV1() networkingv1.NetworkingV1Interface // Deprecated: please explicitly pick a version if possible. Networking() networkingv1.NetworkingV1Interface PolicyV1beta1() policyv1beta1.PolicyV1beta1Interface // Deprecated: please explicitly pick a version if possible. Policy() policyv1beta1.PolicyV1beta1Interface RbacV1() rbacv1.RbacV1Interface // Deprecated: please explicitly pick a version if possible. Rbac() rbacv1.RbacV1Interface RbacV1beta1() rbacv1beta1.RbacV1beta1Interface RbacV1alpha1() rbacv1alpha1.RbacV1alpha1Interface SchedulingV1alpha1() schedulingv1alpha1.SchedulingV1alpha1Interface // Deprecated: please explicitly pick a version if possible. Scheduling() schedulingv1alpha1.SchedulingV1alpha1Interface SettingsV1alpha1() settingsv1alpha1.SettingsV1alpha1Interface // Deprecated: please explicitly pick a version if possible. Settings() settingsv1alpha1.SettingsV1alpha1Interface StorageV1beta1() storagev1beta1.StorageV1beta1Interface StorageV1() storagev1.StorageV1Interface // Deprecated: please explicitly pick a version if possible. Storage() storagev1.StorageV1Interface } 2.4 CoreV1Client 我们以clientset中的CoreV1Client为例做分析。\n通过传入的配置信息rest.Config初始化CoreV1Client对象。\nk8s.io/client-go/kubernetes/clientset.go\ncs.coreV1, err = corev1.NewForConfig(\u0026configShallowCopy) 2.4.1 corev1.NewForConfig k8s.io/client-go/kubernetes/typed/core/v1/core_client.go\n// NewForConfig creates a new CoreV1Client for the given config. func NewForConfig(c *rest.Config) (*CoreV1Client, error) { config := *c if err := setConfigDefaults(\u0026config); err != nil { return nil, err } client, err := rest.RESTClientFor(\u0026config) if err != nil { return nil, err } return \u0026CoreV1Client{client}, nil } corev1.NewForConfig方法本质是调用了rest.RESTClientFor(\u0026config)方法创建RESTClient对象，即CoreV1Client的本质就是一个RESTClient对象。\n2.4.2 CoreV1Client结构体 以下是CoreV1Client结构体的定义：\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\n// CoreV1Client is used to interact with features provided by the group. type CoreV1Client struct { restClient rest.Interface } CoreV1Client实现了CoreV1Interface的接口，即以下方法，从而对kubernetes的资源对象进行增删改查的操作。\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\n//CoreV1Client的方法 func (c *CoreV1Client) ComponentStatuses() ComponentStatusInterface {...} //ConfigMaps func (c *CoreV1Client) ConfigMaps(namespace string) ConfigMapInterface {...} //Endpoints func (c *CoreV1Client) Endpoints(namespace string) EndpointsInterface {...} func (c *CoreV1Client) Events(namespace string) EventInterface {...} func (c *CoreV1Client) LimitRanges(namespace string) LimitRangeInterface {...} //Namespaces func (c *CoreV1Client) Namespaces() NamespaceInterface {...} //Nodes func (c *CoreV1Client) Nodes() NodeInterface {...} func (c *CoreV1Client) PersistentVolumes() PersistentVolumeInterface {...} func (c *CoreV1Client) PersistentVolumeClaims(namespace string) PersistentVolumeClaimInterface {...} //Pods func (c *CoreV1Client) Pods(namespace string) PodInterface {...} func (c *CoreV1Client) PodTemplates(namespace string) PodTemplateInterface {...} //ReplicationControllers func (c *CoreV1Client) ReplicationControllers(namespace string) ReplicationControllerInterface {...} func (c *CoreV1Client) ResourceQuotas(namespace string) ResourceQuotaInterface {...} func (c *CoreV1Client) Secrets(namespace string) SecretInterface {...} //Services func (c *CoreV1Client) Services(namespace string) ServiceInterface {...} func (c *CoreV1Client) ServiceAccounts(namespace string) ServiceAccountInterface {...} 2.4.3 CoreV1Interface k8s.io/client-go/kubernetes/typed/core/v1/core_client.go\ntype CoreV1Interface interface { RESTClient() rest.Interface ComponentStatusesGetter ConfigMapsGetter EndpointsGetter EventsGetter LimitRangesGetter NamespacesGetter NodesGetter PersistentVolumesGetter PersistentVolumeClaimsGetter PodsGetter PodTemplatesGetter ReplicationControllersGetter ResourceQuotasGetter SecretsGetter ServicesGetter ServiceAccountsGetter } CoreV1Interface中包含了各种kubernetes对象的调用接口，例如PodsGetter是对kubernetes中pod对象增删改查操作的接口。ServicesGetter是对service对象的操作的接口。\n2.4.4 PodsGetter 以下我们以PodsGetter接口为例分析CoreV1Client对pod对象的增删改查接口调用。\n示例中的代码如下：\npods, err := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{}) CoreV1().Pods()\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\nfunc (c *CoreV1Client) Pods(namespace string) PodInterface { return newPods(c, namespace) } newPods()\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// newPods returns a Pods func newPods(c *CoreV1Client, namespace string) *pods { return \u0026pods{ client: c.RESTClient(), ns: namespace, } } CoreV1().Pods()的方法实际上是调用了newPods()的方法，创建了一个pods对象，pods对象继承了rest.Interface接口，即最终的实现本质是RESTClient的HTTP调用。\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// pods implements PodInterface type pods struct { client rest.Interface ns string } pods对象实现了PodInterface接口。PodInterface定义了pods对象的增删改查等方法。\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// PodInterface has methods to work with Pod resources. type PodInterface interface { Create(*v1.Pod) (*v1.Pod, error) Update(*v1.Pod) (*v1.Pod, error) UpdateStatus(*v1.Pod) (*v1.Pod, error) Delete(name string, options *meta_v1.DeleteOptions) error DeleteCollection(options *meta_v1.DeleteOptions, listOptions meta_v1.ListOptions) error Get(name string, options meta_v1.GetOptions) (*v1.Pod, error) List(opts meta_v1.ListOptions) (*v1.PodList, error) Watch(opts meta_v1.ListOptions) (watch.Interface, error) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *v1.Pod, err error) PodExpansion } PodsGetter\nPodsGetter继承了PodInterface的接口。\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// PodsGetter has a method to return a PodInterface. // A group's client should implement this interface. type PodsGetter interface { Pods(namespace string) PodInterface } Pods().List()\npods.List()方法通过RESTClient的HTTP调用来实现对kubernetes的pod资源的获取。\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// List takes label and field selectors, and returns the list of Pods that match those selectors. func (c *pods) List(opts meta_v1.ListOptions) (result *v1.PodList, err error) { result = \u0026v1.PodList{} err = c.client.Get(). Namespace(c.ns). Resource(\"pods\"). VersionedParams(\u0026opts, scheme.ParameterCodec). Do(). Into(result) return } 以上分析了clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{})对pod资源获取的过程，最终是调用RESTClient的方法实现。\n2.5 RESTClient 以下分析RESTClient的创建过程及作用。\nRESTClient对象的创建同样是依赖传入的config信息。\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\nclient, err := rest.RESTClientFor(\u0026config) 2.5.1 rest.RESTClientFor k8s.io/client-go/rest/config.go\n// RESTClientFor returns a RESTClient that satisfies the requested attributes on a client Config // object. Note that a RESTClient may require fields that are optional when initializing a Client. // A RESTClient created by this method is generic - it expects to operate on an API that follows // the Kubernetes conventions, but may not be the Kubernetes API. func RESTClientFor(config *Config) (*RESTClient, error) { ... qps := config.QPS ... burst := config.Burst ... baseURL, versionedAPIPath, err := defaultServerUrlFor(config) ... transport, err := TransportFor(config) ... var httpClient *http.Client if transport != http.DefaultTransport { httpClient = \u0026http.Client{Transport: transport} if config.Timeout \u003e 0 { httpClient.Timeout = config.Timeout } } return NewRESTClient(baseURL, versionedAPIPath, config.ContentConfig, qps, burst, config.RateLimiter, httpClient) } RESTClientFor函数调用了NewRESTClient的初始化函数。\n2.5.2 NewRESTClient k8s.io/client-go/rest/client.go\n// NewRESTClient creates a new RESTClient. This client performs generic REST functions // such as Get, Put, Post, and Delete on specified paths. Codec controls encoding and // decoding of responses from the server. func NewRESTClient(baseURL *url.URL, versionedAPIPath string, config ContentConfig, maxQPS float32, maxBurst int, rateLimiter flowcontrol.RateLimiter, client *http.Client) (*RESTClient, error) { base := *baseURL ... serializers, err := createSerializers(config) ... return \u0026RESTClient{ base: \u0026base, versionedAPIPath: versionedAPIPath, contentConfig: config, serializers: *serializers, createBackoffMgr: readExpBackoffConfig, Throttle: throttle, Client: client, }, nil } 2.5.3 RESTClient结构体 以下介绍RESTClient的结构体定义，RESTClient结构体中包含了http.Client，即本质上RESTClient就是一个http.Client的封装实现。\nk8s.io/client-go/rest/client.go\n// RESTClient imposes common Kubernetes API conventions on a set of resource paths. // The baseURL is expected to point to an HTTP or HTTPS path that is the parent // of one or more resources. The server should return a decodable API resource // object, or an api.Status object which contains information about the reason for // any failure. // // Most consumers should use client.New() to get a Kubernetes API client. type RESTClient struct { // base is the root URL for all invocations of the client base *url.URL // versionedAPIPath is a path segment connecting the base URL to the resource root versionedAPIPath string // contentConfig is the information used to communicate with the server. contentConfig ContentConfig // serializers contain all serializers for underlying content type. serializers Serializers // creates BackoffManager that is passed to requests. createBackoffMgr func() BackoffManager // TODO extract this into a wrapper interface via the RESTClient interface in kubectl. Throttle flowcontrol.RateLimiter // Set specific behavior of the client. If not set http.DefaultClient will be used. Client *http.Client } 2.5.4 RESTClient.Interface RESTClient实现了以下的接口方法：\nk8s.io/client-go/rest/client.go\n// Interface captures the set of operations for generically interacting with Kubernetes REST apis. type Interface interface { GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion } 在调用HTTP方法（Post()，Put()，Get()，Delete() ）时，实际上调用了Verb(verb string)函数。\nk8s.io/client-go/rest/client.go\n// Verb begins a request with a verb (GET, POST, PUT, DELETE). // // Example usage of RESTClient's request building interface: // c, err := NewRESTClient(...) // if err != nil { ... } // resp, err := c.Verb(\"GET\"). // Path(\"pods\"). // SelectorParam(\"labels\", \"area=staging\"). // Timeout(10*time.Second). // Do() // if err != nil { ... } // list, ok := resp.(*api.PodList) // func (c *RESTClient) Verb(verb string) *Request { backoff := c.createBackoffMgr() if c.Client == nil { return NewRequest(nil, verb, c.base, c.versionedAPIPath, c.contentConfig, c.serializers, backoff, c.Throttle) } return NewRequest(c.Client, verb, c.base, c.versionedAPIPath, c.contentConfig, c.serializers, backoff, c.Throttle) } Verb函数调用了NewRequest方法，最后调用Do()方法实现一个HTTP请求获取Result。\n2.6 总结 client-go对kubernetes资源对象的调用，需要先获取kubernetes的配置信息，即$HOME/.kube/config。\n整个调用的过程如下：\nkubeconfig→rest.config→clientset→具体的client(CoreV1Client)→具体的资源对象(pod)→RESTClient→http.Client→HTTP请求的发送及响应\n通过clientset中不同的client和client中不同资源对象的方法实现对kubernetes中资源对象的增删改查等操作，常用的client有CoreV1Client、AppsV1beta1Client、ExtensionsV1beta1Client等。\n3. client-go对k8s资源的调用 创建clientset\n//获取kubeconfig kubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\") //创建config\tconfig, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) //创建clientset clientset, err := kubernetes.NewForConfig(config) //具体的资源调用见以下例子 3.1 deployment //声明deployment对象 var deployment *v1beta1.Deployment //构造deployment对象 //创建deployment deployment, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Create(\u003cdeployment\u003e) //更新deployment deployment, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Update(\u003cdeployment\u003e) //删除deployment err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Delete(\u003cdeployment.Name\u003e, \u0026meta_v1.DeleteOptions{}) //查询deployment deployment, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Get(\u003cdeployment.Name\u003e, meta_v1.GetOptions{}) //列出deployment deploymentList, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch deployment watchInterface, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.2 service //声明service对象 var service *v1.Service //构造service对象 //创建service service, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Create(\u003cservice\u003e) //更新service service, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Update(\u003cservice\u003e) //删除service err := clientset.CoreV1().Services(\u003cnamespace\u003e).Delete(\u003cservice.Name\u003e, \u0026meta_v1.DeleteOptions{}) //查询service service, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Get(\u003cservice.Name\u003e, meta_v1.GetOptions{}) //列出service serviceList, err := clientset.CoreV1().Services(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch service watchInterface, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.3 ingress //声明ingress对象 var ingress *v1beta1.Ingress //构造ingress对象 //创建ingress ingress, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Create(\u003cingress\u003e) //更新ingress ingress, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Update(\u003cingress\u003e) //删除ingress err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Delete(\u003cingress.Name\u003e, \u0026meta_v1.DeleteOptions{}) //查询ingress ingress, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Get(\u003cingress.Name\u003e, meta_v1.GetOptions{}) //列出ingress ingressList, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch ingress watchInterface, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.4 replicaSet //声明replicaSet对象 var replicaSet *v1beta1.ReplicaSet //构造replicaSet对象 //创建replicaSet replicaSet, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Create(\u003creplicaSet\u003e) //更新replicaSet replicaSet, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Update(\u003creplicaSet\u003e) //删除replicaSet err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Delete(\u003creplicaSet.Name\u003e, \u0026meta_v1.DeleteOptions{}) //查询replicaSet replicaSet, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Get(\u003creplicaSet.Name\u003e, meta_v1.GetOptions{}) //列出replicaSet replicaSetList, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch replicaSet watchInterface, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 新版的kubernetes中一般通过deployment来创建replicaSet，再通过replicaSet来控制pod。\n3.5 pod //声明pod对象 var pod *v1.Pod //创建pod pod, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Create(\u003cpod\u003e) //更新pod pod, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Update(\u003cpod\u003e) //删除pod err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Delete(\u003cpod.Name\u003e, \u0026meta_v1.DeleteOptions{}) //查询pod pod, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Get(\u003cpod.Name\u003e, meta_v1.GetOptions{}) //列出pod podList, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch pod watchInterface, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.6 statefulset //声明statefulset对象 var statefulset *v1.StatefulSet //创建statefulset statefulset, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Create(\u003cstatefulset\u003e) //更新statefulset statefulset, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Update(\u003cstatefulset\u003e) //删除statefulset err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Delete(\u003cstatefulset.Name\u003e, \u0026meta_v1.DeleteOptions{}) //查询statefulset statefulset, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Get(\u003cstatefulset.Name\u003e, meta_v1.GetOptions{}) //列出statefulset statefulsetList, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch statefulset watchInterface, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) ​\t通过以上对kubernetes的资源对象的操作函数可以看出，每个资源对象都有增删改查等方法，基本调用逻辑类似。一般二次开发只需要创建deployment、service、ingress三个资源对象即可，pod对象由deployment包含的replicaSet来控制创建和删除。函数调用的入参一般只有NAMESPACE和kubernetesObject两个参数，部分操作有Options的参数。在创建前，需要对资源对象构造数据，可以理解为编辑一个资源对象的yaml文件，然后通过kubectl create -f xxx.yaml来创建对象。\n参考文档:\nhttps://github.com/kubernetes/client-go ","categories":"","description":"","excerpt":"1. client-go简介 1.1 client-go说明 ​\tclient-go是一个调用kubernetes集群资源对象API的客户 …","ref":"/kubernetes-notes/develop/client-go/","tags":["源码分析","Kubernetes"],"title":"client-go的使用及源码分析"},{"body":"1. 查看系统Event事件 kubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e 该命令可以显示Pod创建时的配置定义、状态等信息和最近的Event事件，事件信息可用于排错。例如当Pod状态为Pending，可通过查看Event事件确认原因，一般原因有几种：\n没有可用的Node可调度 开启了资源配额管理并且当前Pod的目标节点上恰好没有可用的资源 正在下载镜像（镜像拉取耗时太久）或镜像下载失败。 kubectl describe还可以查看其它k8s对象：NODE,RC,Service,Namespace,Secrets。\n1.1. Pod kubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e 以下是容器的启动命令非阻塞式导致容器挂掉，被k8s频繁重启所产生的事件。\nkubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e Events: FirstSeen LastSeen Count From SubobjectPath Reason Message ───────── ──────── ───── ──── ───────────── ────── ─────── 7m 7m 1 {scheduler } Scheduled Successfully assigned yangsc-1-0-0-index0 to 10.8.216.19 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Pulled Container image \"gcr.io/kube-system/pause:0.8.0\" already present on machine 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Created Created with docker id 84f133c324d0 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Started Started with docker id 84f133c324d0 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 3f9f82abb145 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 3f9f82abb145 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id fb112e4002f4 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id fb112e4002f4 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 613b119d4474 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 613b119d4474 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 25cb68d1fd3d 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 25cb68d1fd3d 5m 5m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 7d9ee8610b28 5m 5m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 7d9ee8610b28 3m 3m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 88b9e8d582dd 3m 3m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 88b9e8d582dd 7m 1m 7 {kubelet 10.8.216.19} containers{yangsc0} Pulling Pulling image \"gcr.io/test/tcp-hello:1.0.0\" 1m 1m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 089abff050e7 1m 1m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 089abff050e7 7m 1m 7 {kubelet 10.8.216.19} containers{yangsc0} Pulled Successfully pulled image \"gcr.io/test/tcp-hello:1.0.0\" 6m 7s 34 {kubelet 10.8.216.19} containers{yangsc0} Backoff Back-off restarting failed docker container 1.2. NODE kubectl describe node 10.8.216.20 [root@FC-43745A-10 ~]# kubectl describe node 10.8.216.20 Name: 10.8.216.20 Labels: kubernetes.io/hostname=10.8.216.20,namespace/bcs-cc=true,namespace/myview=true CreationTimestamp: Mon, 17 Apr 2017 11:32:52 +0800 Phase: Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ──── ────── ───────────────── ────────────────── ────── ─────── Ready True Fri, 18 Aug 2017 09:38:33 +0800 Tue, 02 May 2017 17:40:58 +0800 KubeletReady kubelet is posting ready status OutOfDisk False Fri, 18 Aug 2017 09:38:33 +0800 Mon, 17 Apr 2017 11:31:27 +0800 KubeletHasSufficientDisk kubelet has sufficient disk space available Addresses: 10.8.216.20,10.8.216.20 Capacity: cpu: 32 memory: 67323039744 pods: 40 System Info: Machine ID: 723bafc7f6764022972b3eae1ce6b198 System UUID: 4C4C4544-0042-4210-8044-C3C04F595631 Boot ID: da01f2e3-987a-425a-9ca7-1caaec35d1e5 Kernel Version: 3.10.0-327.28.3.el7.x86_64 OS Image: CentOS Linux 7 (Core) Container Runtime Version: docker://1.13.1 Kubelet Version: v1.1.1-xxx2-13.1+79c90c68bfb72f-dirty Kube-Proxy Version: v1.1.1-xxx2-13.1+79c90c68bfb72f-dirty ExternalID: 10.8.216.20 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits ───────── ──── ──────────── ────────── ─────────────── ───────────── bcs-cc bcs-cc-api-0-0-1364-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) bcs-cc bcs-cc-api-0-0-1444-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) fw fw-demo2-0-0-1519-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) myview myview-api-0-0-1362-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) myview myview-api-0-0-1442-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) qa-ts-dna ts-dna-console3-0-0-1434-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) Allocated resources: (Total limits may be over 100%, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md) CPU Requests CPU Limits Memory Requests Memory Limits ──────────── ────────── ─────────────── ───────────── 6 (18%) 6 (18%) 25769803776 (38%) 25769803776 (38%) No events. 1.3. RC kubectl describe rc mytest-1-0-0 --namespace=test [root@FC-43745A-10 ~]# kubectl describe rc mytest-1-0-0 --namespace=test Name: mytest-1-0-0 Namespace: test Image(s): gcr.io/test/mywebcalculator:1.0.1 Selector: app=mytest,appVersion=1.0.0 Labels: app=mytest,appVersion=1.0.0,env=ts,zone=inner Replicas: 1 current / 1 desired Pods Status: 1 Running / 0 Waiting / 0 Succeeded / 0 Failed No volumes. Events: FirstSeen LastSeen Count From SubobjectPath Reason Message ───────── ──────── ───── ──── ───────────── ────── ─────── 20h 19h 9 {replication-controller } FailedCreate Error creating: Pod \"mytest-1-0-0-index0\" is forbidden: limited to 10 pods 20h 17h 7 {replication-controller } FailedCreate Error creating: pods \"mytest-1-0-0-index0\" already exists 20h 17h 4 {replication-controller } SuccessfulCreate Created pod: mytest-1-0-0-index0 1.4. NAMESPACE kubectl describe namespace test [root@FC-43745A-10 ~]# kubectl describe namespace test Name: test Labels: \u003cnone\u003e Status: Active Resource Quotas Resource Used Hard --- --- --- cpu 5 20 memory 1342177280 53687091200 persistentvolumeclaims 0 10 pods 4 10 replicationcontrollers 8 20 resourcequotas 1 1 secrets 3 10 services 8 20 No resource limits. 1.5. Service kubectl describe service xxx-containers-1-1-0 --namespace=test [root@FC-43745A-10 ~]# kubectl describe service xxx-containers-1-1-0 --namespace=test Name: xxx-containers-1-1-0 Namespace: test Labels: app=xxx-containers,appVersion=1.1.0,env=ts,zone=inner Selector: app=xxx-containers,appVersion=1.1.0 Type: ClusterIP IP: 10.254.46.42 Port: port-dna-tcp-35913 35913/TCP Endpoints: 10.0.92.17:35913 Port: port-l7-tcp-8080 8080/TCP Endpoints: 10.0.92.17:8080 Session Affinity: None No events. 2. 查看容器日志 1、查看指定pod的日志\nkubectl logs \u003cpod_name\u003e kubectl logs -f \u003cpod_name\u003e #类似tail -f的方式查看 2、查看上一个pod的日志\nkubectl logs -p \u003cpod_name\u003e 3、查看指定pod中指定容器的日志\nkubectl logs \u003cpod_name\u003e -c \u003ccontainer_name\u003e 4、kubectl logs --help\n[root@node5 ~]# kubectl logs --help Print the logs for a container in a pod. If the pod has only one container, the container name is optional. Usage: kubectl logs [-f] [-p] POD [-c CONTAINER] [flags] Aliases: logs, log Examples: # Return snapshot logs from pod nginx with only one container $ kubectl logs nginx # Return snapshot of previous terminated ruby container logs from pod web-1 $ kubectl logs -p -c ruby web-1 # Begin streaming the logs of the ruby container in pod web-1 $ kubectl logs -f -c ruby web-1 # Display only the most recent 20 lines of output in pod nginx $ kubectl logs --tail=20 nginx # Show all logs from pod nginx written in the last hour $ kubectl logs --since=1h nginx 3. 查看k8s服务日志 3.1. journalctl 在Linux系统上systemd系统来管理kubernetes服务，并且journal系统会接管服务程序的输出日志，可以通过systemctl status 或journalctl -u -f来查看kubernetes服务的日志。\n其中kubernetes组件包括：\nk8s组件 涉及日志内容 备注 kube-apiserver kube-controller-manager Pod扩容相关或RC相关 kube-scheduler Pod扩容相关或RC相关 kubelet Pod生命周期相关：创建、停止等 etcd 3.2. 日志文件 也可以通过指定日志存放目录来保存和查看日志\n--logtostderr=false：不输出到stderr --log-dir=/var/log/kubernetes:日志的存放目录 --alsologtostderr=false:设置为true表示日志输出到文件也输出到stderr --v=0:glog的日志级别 --vmodule=gfs*=2,test*=4：glog基于模块的详细日志级别 4. 常见问题 4.1. Pod状态一直为Pending kubectl describe \u003cpod_name\u003e --namespace=\u003cNAMESPACE\u003e 查看该POD的事件。\n正在下载镜像但拉取不下来（镜像拉取耗时太久）[一般都是该原因] 没有可用的Node可调度 开启了资源配额管理并且当前Pod的目标节点上恰好没有可用的资源 解决方法：\n查看该POD所在宿主机与镜像仓库之间的网络是否有问题，可以手动拉取镜像 删除POD实例，让POD调度到别的宿主机上 4.2. Pod创建后不断重启 kubectl get pods中Pod状态一会running，一会不是，且RESTARTS次数不断增加。\n一般原因为容器启动命令不是阻塞式命令，导致容器运行后马上退出。\n非阻塞式命令：\n本身CMD指定的命令就是非阻塞式命令 将服务启动方式设置为后台运行 解决方法：\n1、将命令改为阻塞式命令（前台运行），例如：zkServer.sh start-foreground\n2、java运行程序的启动脚本将 nohup xxx \u0026的nobup和\u0026去掉，例如：\nnohup JAVA_HOME/bin/java JAVA_OPTS -cp $CLASSPATH com.cnc.open.processor.Main \u0026 改为：\nJAVA_HOME/bin/java JAVA_OPTS -cp $CLASSPATH com.cnc.open.processor.Main 文章参考《Kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. 查看系统Event事件 kubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e  …","ref":"/kubernetes-notes/operation/kubernetes-troubleshooting/","tags":["Kubernetes"],"title":"Kubernetes集群问题排查"},{"body":"1. Kubernetes的总架构图 2. Kubernetes各个组件介绍 2.1 kube-master[控制节点] master的工作流程图\nKubecfg将特定的请求，比如创建Pod，发送给Kubernetes Client。 Kubernetes Client将请求发送给API server。 API Server根据请求的类型，比如创建Pod时storage类型是pods，然后依此选择何种REST Storage API对请求作出处理。 REST Storage API对的请求作相应的处理。 将处理的结果存入高可用键值存储系统Etcd中。 在API Server响应Kubecfg的请求后，Scheduler会根据Kubernetes Client获取集群中运行Pod及Minion/Node信息。 依据从Kubernetes Client获取的信息，Scheduler将未分发的Pod分发到可用的Minion/Node节点上。 2.1.1 API Server[资源操作入口] 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。\n第一，是为了保证集群状态访问的安全。\n第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。\n作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。\n更多API Server信息请参考：Kubernetes核心原理（一）之API Server\n2.1.2 Controller Manager[内部管理控制中心] 实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 更多Controller Manager信息请参考：Kubernetes核心原理（二）之Controller Manager\n2.1.3 Scheduler[集群分发调度器] Scheduler收集和分析当前Kubernetes集群中所有Minion/Node节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 Scheduler也监测Minion/Node节点信息，由于会频繁查找Minion/Node节点，Scheduler会缓存一份最新的信息在本地。 最后，Scheduler在分发Pod到指定的Minion/Node节点后，会把Pod相关的信息Binding写回API Server。 更多Scheduler信息请参考：Kubernetes核心原理（三）之Scheduler\n2.2 kube-node[服务节点] kubelet结构图\n2.2.1 Kubelet[节点上的Pod管家] 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理\n定时上报本Node的状态信息给API Server。\nkubelet是Master API Server和Minion/Node之间的桥梁，接收Master API Server分配给它的commands和work，通过kube-apiserver间接与Etcd集群交互，读取配置信息。\n具体的工作如下：\n设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。\n同步Pod的状态、同步Pod的状态、从cAdvisor获取container info、 pod info、 root info、 machine info。\n在容器中运行命令、杀死容器、删除Pod的所有容器。\n更多Kubelet信息请参考：Kubernetes核心原理（四）之Kubelet\n2.2.2 Proxy[负载均衡、路由转发] Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Minion/Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion/Node上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 2.2.3 kubectl[集群管理命令行工具集] 通过客户端的kubectl命令集操作，API Server响应对应的命令结果，从而达到对kubernetes集群的管理。 参考文章：\nhttps://yq.aliyun.com/articles/47308?spm=5176.100240.searchblog.19.jF7FFa\n","categories":"","description":"","excerpt":"1. Kubernetes的总架构图 2. Kubernetes各个组件介绍 2.1 kube-master[控制节点] master的工作 …","ref":"/kubernetes-notes/concepts/architecture/kubernetes-architecture/","tags":["Kubernetes"],"title":"Kubernetes总架构图"},{"body":"1. API Server简介 k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。\nkubernetes API Server的功能：\n提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; 是资源配额控制的入口； 拥有完备的集群安全机制. kube-apiserver工作原理图\n2. 如何访问kubernetes API k8s通过kube-apiserver这个进程提供服务，该进程运行在单个k8s-master节点上。默认有两个端口。\n2.1. 本地端口 该端口用于接收HTTP请求； 该端口默认值为8080，可以通过API Server的启动参数“--insecure-port”的值来修改默认值； 默认的IP地址为“localhost”，可以通过启动参数“--insecure-bind-address”的值来修改该IP地址； 非认证或授权的HTTP请求通过该端口访问API Server。 2.2. 安全端口 该端口默认值为6443，可通过启动参数“--secure-port”的值来修改默认值； 默认IP地址为非本地（Non-Localhost）网络端口，通过启动参数“--bind-address”设置该值； 该端口用于接收HTTPS请求； 用于基于Tocken文件或客户端证书及HTTP Base的认证； 用于基于策略的授权； 默认不启动HTTPS安全访问控制。 2.3. 访问方式 Kubernetes REST API可参考https://kubernetes.io/docs/api-reference/v1.6/\n2.3.1. curl curl localhost:8080/api curl localhost:8080/api/v1/pods curl localhost:8080/api/v1/services curl localhost:8080/api/v1/replicationcontrollers 2.3.2. Kubectl Proxy Kubectl Proxy代理程序既能作为API Server的反向代理，也能作为普通客户端访问API Server的代理。通过master节点的8080端口来启动该代理程序。\nkubectl proxy --port=8080 \u0026\n具体见kubectl proxy --help\n[root@node5 ~]# kubectl proxy --help To proxy all of the kubernetes api and nothing else, use: kubectl proxy --api-prefix=/ To proxy only part of the kubernetes api and also some static files: kubectl proxy --www=/my/files --www-prefix=/static/ --api-prefix=/api/ The above lets you 'curl localhost:8001/api/v1/pods'. To proxy the entire kubernetes api at a different root, use: kubectl proxy --api-prefix=/custom/ The above lets you 'curl localhost:8001/custom/api/v1/pods' Usage: kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags] Examples: # Run a proxy to kubernetes apiserver on port 8011, serving static content from ./local/www/ $ kubectl proxy --port=8011 --www=./local/www/ # Run a proxy to kubernetes apiserver on an arbitrary local port. # The chosen port for the server will be output to stdout. $ kubectl proxy --port=0 # Run a proxy to kubernetes apiserver, changing the api prefix to k8s-api # This makes e.g. the pods api available at localhost:8011/k8s-api/v1/pods/ $ kubectl proxy --api-prefix=/k8s-api Flags: --accept-hosts=\"^localhost$,^127//.0//.0//.1$,^//[::1//]$\": Regular expression for hosts that the proxy should accept. --accept-paths=\"^/.*\": Regular expression for paths that the proxy should accept. --api-prefix=\"/\": Prefix to serve the proxied API under. --disable-filter[=false]: If true, disable request filtering in the proxy. This is dangerous, and can leave you vulnerable to XSRF attacks, when used with an accessible port. -p, --port=8001: The port on which to run the proxy. Set to 0 to pick a random port. --reject-methods=\"POST,PUT,PATCH\": Regular expression for HTTP methods that the proxy should reject. --reject-paths=\"^/api/.*/exec,^/api/.*/run\": Regular expression for paths that the proxy should reject. -u, --unix-socket=\"\": Unix socket on which to run the proxy. -w, --www=\"\": Also serve static files from the given directory under the specified prefix. -P, --www-prefix=\"/static/\": Prefix to serve static files under, if static file directory is specified. Global Flags: --alsologtostderr[=false]: log to standard error as well as files --api-version=\"\": The API version to use when talking to the server --certificate-authority=\"\": Path to a cert. file for the certificate authority. --client-certificate=\"\": Path to a client key file for TLS. --client-key=\"\": Path to a client key file for TLS. --cluster=\"\": The name of the kubeconfig cluster to use --context=\"\": The name of the kubeconfig context to use --insecure-skip-tls-verify[=false]: If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure. --kubeconfig=\"\": Path to the kubeconfig file to use for CLI requests. --log-backtrace-at=:0: when logging hits line file:N, emit a stack trace --log-dir=\"\": If non-empty, write log files in this directory --log-flush-frequency=5s: Maximum number of seconds between log flushes --logtostderr[=true]: log to standard error instead of files --match-server-version[=false]: Require server version to match client version --namespace=\"\": If present, the namespace scope for this CLI request. --password=\"\": Password for basic authentication to the API server. -s, --server=\"\": The address and port of the Kubernetes API server --stderrthreshold=2: logs at or above this threshold go to stderr --token=\"\": Bearer token for authentication to the API server. --user=\"\": The name of the kubeconfig user to use --username=\"\": Username for basic authentication to the API server. --v=0: log level for V logs --vmodule=: comma-separated list of pattern=N settings for file-filtered logging 2.3.3. kubectl客户端 命令行工具kubectl客户端，通过命令行参数转换为对API Server的REST API调用，并将调用结果输出。\n命令格式：kubectl [command] [options]\n具体可参考k8s常用命令\n2.3.4. 编程方式调用 使用场景：\n1、运行在Pod里的用户进程调用kubernetes API,通常用来实现分布式集群搭建的目标。\n2、开发基于kubernetes的管理平台，比如调用kubernetes API来完成Pod、Service、RC等资源对象的图形化创建和管理界面。可以使用kubernetes提供的Client Library。\n具体可参考https://github.com/kubernetes/client-go。\n3. 通过API Server访问Node、Pod和Service k8s API Server最主要的REST接口是资源对象的增删改查，另外还有一类特殊的REST接口—k8s Proxy API接口，这类接口的作用是代理REST请求，即kubernetes API Server把收到的REST请求转发到某个Node上的kubelet守护进程的REST端口上，由该kubelet进程负责响应。\n3.1. Node相关接口 关于Node相关的接口的REST路径为：/api/v1/proxy/nodes/{name}，其中{name}为节点的名称或IP地址。\n/api/v1/proxy/nodes/{name}/pods/ #列出指定节点内所有Pod的信息 /api/v1/proxy/nodes/{name}/stats/ #列出指定节点内物理资源的统计信息 /api/v1/prxoy/nodes/{name}/spec/ #列出指定节点的概要信息 这里获取的Pod信息来自Node而非etcd数据库，两者时间点可能存在偏差。如果在kubelet进程启动时加--enable-debugging-handles=true参数，那么kubernetes Proxy API还会增加以下接口：\n/api/v1/proxy/nodes/{name}/run #在节点上运行某个容器 /api/v1/proxy/nodes/{name}/exec #在节点上的某个容器中运行某条命令 /api/v1/proxy/nodes/{name}/attach #在节点上attach某个容器 /api/v1/proxy/nodes/{name}/portForward #实现节点上的Pod端口转发 /api/v1/proxy/nodes/{name}/logs #列出节点的各类日志信息 /api/v1/proxy/nodes/{name}/metrics #列出和该节点相关的Metrics信息 /api/v1/proxy/nodes/{name}/runningpods #列出节点内运行中的Pod信息 /api/v1/proxy/nodes/{name}/debug/pprof #列出节点内当前web服务的状态，包括CPU和内存的使用情况 3.2. Pod相关接口 /api/v1/proxy/namespaces/{namespace}/pods/{name}/{path:*} #访问pod的某个服务接口 /api/v1/proxy/namespaces/{namespace}/pods/{name} #访问Pod #以下写法不同，功能一样 /api/v1/namespaces/{namespace}/pods/{name}/proxy/{path:*} #访问pod的某个服务接口 /api/v1/namespaces/{namespace}/pods/{name}/proxy #访问Pod 3.3. Service相关接口 /api/v1/proxy/namespaces/{namespace}/services/{name} Pod的proxy接口的作用：在kubernetes集群之外访问某个pod容器的服务（HTTP服务），可以用Proxy API实现，这种场景多用于管理目的，比如逐一排查Service的Pod副本，检查哪些Pod的服务存在异常问题。\n4. 集群功能模块之间的通信 kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信，集群内各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，通过API Server提供的REST接口（GET/LIST/WATCH方法）来实现，从而实现各模块之间的信息交互。\n4.1. kubelet与API Server交互 每个Node节点上的kubelet定期就会调用API Server的REST接口报告自身状态，API Server接收这些信息后，将节点状态信息更新到etcd中。kubelet也通过API Server的Watch接口监听Pod信息，从而对Node机器上的POD进行管理。\n监听信息 kubelet动作 新的POD副本被调度绑定到本节点 执行POD对应的容器的创建和启动逻辑 POD对象被删除 删除本节点上相应的POD容器 修改POD信息 修改本节点的POD容器 4.2. kube-controller-manager与API Server交互 kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口，实时监控Node的信息，并做相应处理。\n4.3. kube-scheduler与API Server交互 Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，它会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑。调度成功后将Pod绑定到目标节点上。\n4.4. 特别说明 为了缓解各模块对API Server的访问压力，各功能模块都采用缓存机制来缓存数据，各功能模块定时从API Server获取指定的资源对象信息（LIST/WATCH方法），然后将信息保存到本地缓存，功能模块在某些情况下不直接访问API Server，而是通过访问缓存数据来间接访问API Server。\n参考《kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. API Server简介 k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch …","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-api-server/","tags":["Kubernetes"],"title":"Kubernetes核心原理（一）之API Server"},{"body":"1. Pod是什么（what） 1.1. Pod概念 Pod是kubernetes集群中最小的部署和管理的基本单元，协同寻址，协同调度。 Pod是一个或多个容器的集合，是一个或一组服务（进程）的抽象集合。 Pod中可以共享网络和存储（可以简单理解为一个逻辑上的虚拟机，但并不是虚拟机）。 Pod被创建后用一个UID来唯一标识，当Pod生命周期结束，被一个等价Pod替代，UID将重新生成。 1.1.1. Pod与Docker Docker是目前Pod最常用的容器环境，但仍支持其他容器环境。 Pod是一组被模块化的拥有共享命名空间和共享存储卷的容器，但并没有共享PID 命名空间（即同个Pod的不同容器中进程的PID是独立的，互相看不到非自己容器的进程）。 1.1.2. Pod中容器的运行方式 只运行一个单独的容器 即one-container-per-Pod模式，是最常用的模式，可以把这样的Pod看成单独的一个容器去管理。\n运行多个强关联的容器 即sidecar模式，Pod 封装了一组紧耦合、共享资源、协同寻址的容器，将这组容器作为一个管理单元。\n1.2. Pod管理多个容器 Pod是一组紧耦合的容器的集合，Pod内的容器作为一个整体以Pod形式进行协同寻址，协同调度、协同管理。相同Pod内的容器共享网络和存储。\n1.2.1. 网络 每个Pod被分配了唯一的IP地址，该Pod内的所有容器共享一个网络空间，包括IP和端口。 同个Pod不同容器之间通过localhost通信，Pod内端口不能冲突。 不同Pod之间的通信则通过IP+端口的形式来访问到Pod内的具体服务（容器）。 1.2.2. 存储 可以在Pod中创建共享存储卷的方式来实现不同容器之间数据共享。 2. 为什么需要Pod(why) 2.1. 管理需求 Pod 是一种模式的抽象：互相协作的多个进程（容器）共同形成一个完整的服务。以一个或多个容器的方式组合成一个整体，作为管理的基本单元，通过Pod可以方便部署、水平扩展，协同调度等。\n2.2. 资源共享和通信 Pod作为多个紧耦合的容器的集合，通过共享网络和存储的方式来简化紧耦合容器之间的通信，从这个角度，可以将Pod简单理解为一个逻辑上的“虚拟机”。而不同的Pod之间的通信则通过Pod的IP和端口的方式。\n2.3. Pod设计的优势 调度器和控制器的可拔插性。 将Pod 的生存期从 controller 中剥离出来，从而减少相互影响。 高可用--在终止和删除 Pod 前，需要提前生成替代 Pod。 集群级别的功能和 Kubelet（Pod Controller） 级别的功能组合更加清晰。 3. Pod的使用(how) Pod一般是通过各种不同类型的Controller对Pod进行管理和控制，包括自我恢复（例如Pod因异常退出，则会再起一个相同的Pod替代该Pod，而该Pod则会被清除）。也可以不通过Controller单独创建一个Pod，但一般很少这么操作，因为这个Pod是一个孤立的实体，并不会被Controller管理。\n3.1. Controller Controller是kubernetes中用于对Pod进行管理的控制器，通过该控制器让Pod始终维持在一个用户原本设定或期望的状态。如果节点宕机或者Pod因其他原因死亡，则会在其他节点起一个相同的Pod来替代该Pod。\n常用的Controller有：\nDeployment StatefulSet DaemonSet Controller是通过用户提供的Pod模板来创建和控制Pod。\n3.2. Pod模板 Pod模板用来定义Pod的各种属性，Controller通过Pod模板来生成对应的Pod。\nPod模板类似一个饼干模具，通过模具已经生成的饼干与原模具已经没有关系，即对原模具的修改不会影响已经生成的饼干，只会对通过修改后的模具生成的饼干有影响。这种方式可以更加方便地控制和管理Pod。\n4. Pod的终止 用户发起一个删除Pod的请求，系统会先发送TERM信号给每个容器的主进程，如果在宽限期（默认30秒）主进程没有自主终止运行，则系统会发送KILL信号给该进程，接着Pod将被删除。\n4.1. Pod终止的流程 用户发送一个删除 Pod 的命令， 并使用默认的宽限期（30s)。 把 API server 上的 pod 的时间更新成 Pod 与宽限期一起被认为 “dead” 之外的时间点。 使用客户端的命令，显示出的Pod的状态为 terminating。 （与第3步同时发生）Kubelet 发现某一个 Pod 由于时间超过第2步的设置而被标志成 terminating 状态时， Kubelet 将启动一个停止进程。 如果 pod 已经被定义成一个 preStop hook，这会在 pod 内部进行调用。如果宽限期已经过期但 preStop 锚依然还在运行，将调用第2步并在原来的宽限期上加一个小的时间窗口（2 秒钟）。 把 Pod 里的进程发送到 TERM 信号。 （与第3步同时发生），Pod 被从终端的服务列表里移除，同时也不再被 replication controllers 看做时一组运行中的 pods。 在负载均衡（比如说 service proxy）会将它们从轮询中移除前， Pods 这种慢关闭的方式可以继续为流量提供服务。 当宽期限过期时， 任何还在 Pod 里运行的进程都会被 SIGKILL 杀掉。 Kubelet 通过在 API server 把宽期限设置成0(立刻删除)的方式完成删除 Pod的过程。 这时 Pod 在 API 里消失，也不再能被用户看到。 4.2. 强制删除Pod 强制删除Pod是指从k8s集群状态和Etcd中立刻删除对应的Pod数据，API Server不会等待kubelet的确认信息。被强制删除后，即可重新创建一个相同名字的Pod。\n删除默认的宽限期是30秒，通过将宽限期设置为0的方式可以强制删除Pod。\n通过kubectl delete 命令后加--force和--grace-period=0的参数强制删除Pod。\nkubectl delete pod \u003cpod_name\u003e --namespace=\u003cnamespace\u003e --force --grace-period=0 4.3. Pod特权模式 特权模式是指让Pod中的进程具有访问宿主机系统设备或使用网络栈操作等的能力，例如编写网络插件和卷插件。\n通过将container spec中的SecurityContext设置为privileged即将该容器赋予了特权模式。特权模式的使用要求k8s版本高于v1.1。\n参考文章：\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-overview/ https://kubernetes.io/docs/concepts/workloads/pods/pod/ ","categories":"","description":"","excerpt":"1. Pod是什么（what） 1.1. Pod概念 Pod是kubernetes集群中最小的部署和管理的基本单元，协同寻址，协同调度。 …","ref":"/kubernetes-notes/concepts/pod/pod/","tags":["Kubernetes"],"title":"Pod介绍"},{"body":"初识Go语言 1. 概述 一个在语言层面实现了并发机制的类C通用型编程语言。\n2. Go关键字（25个） 类别 关键字 说明 程序声明 package，import 包的声明和导入 声明与定义 var，const 变量和常量的声明 type 用于定义类型 复合数据类型 struct 定义结构体，类似java中的class interface 定义接口 map 定义键值对 func 定义函数和方法 chan 定义管道，并发中channel通信 并发编程 go 并发编程 select 用于选择不同类型通信 流程语句 for；if，else；switch，case 循环语句；条件语句；选择语句 break，continue，fallthrough，default，goto 跳转语句等 return 函数返回值 defer 延迟函数，用于return前释放资源 range 用于读取slice，map，channel容器类数据 3. Go语言命令 Usage：go command [arguments]\n分类 命令 说明 build compile packages and dependencies clean remove object files doc show documentation for package or symbol env print Go environment information fix run go tool fix on packages fmt run gofmt on package sources generate generate Go files by processing source get download and install packages and dependencies install compile and install packages and dependencies list list packages run compile and run Go program test test packages tool run specified go tool version print Go version vet run go tool vet on packages ","categories":"","description":"","excerpt":"初识Go语言 1. 概述 一个在语言层面实现了并发机制的类C通用型编程语言。\n2. Go关键字（25个） …","ref":"/golang-notes/introduction/golang/","tags":["Golang"],"title":"Golang介绍"},{"body":"1.变量 1.1变量声明 //1、单变量声明,类型放在变量名之后，可以为任意类型 var 变量名 类型 var v1,v2,v3 string //多变量同类型声明 //2、多变量声明 var { v1 int v2 []int } 1.2变量初始化 //1、使用关键字var，声明变量类型并赋值 var v1 int=10 //2、使用关键字var，直接对变量赋值，go可以自动推导出变量类型 var v2=10 //3、直接使用“：=”对变量赋值，不使用var，两者同时使用会语法冲突，推荐使用 v3:=10 1.3变量赋值 //1、声明后再变量赋值 var v int v=10 //2、多重赋值，经常使用在函数的多返回值中，err,v=func(arg) i，j=j,i //两者互换，并不需要引入中间变量 1.4匿名变量 //Go中所有声明后的变量都需要调用到，当出现函数多返回值，并且部分返回值不需要使用时，可以使用匿名变量丢弃该返回值 func GetName()(firstName,lastName,nickName string){ return \"May\",\"Chan\",\"Make\" } _,_,nickName:=GetName() //使用匿名变量丢弃部分返回值 2.常量 ​\tGo语言中，常量是编译时期就已知且不可变的值，常量可以是数值类型（整型、浮点型、复数类型）、布尔类型、字符串类型。\n2.1字面常量 //字面常量(literal)指程序中硬编码的常量 3.14 “foo” true 2.2常量定义 //1、可以限定常量类型，但非必需 const Pi float64 = 3.14 //2、无类型常量和字面常量一样 const zero=0.0 //3、多常量赋值 const( size int64=1024 eof=-1 ) //4、常量的多重赋值，类似变量的多重赋值 const u,v float32=0,3 const a,b,c=3,4,\"foo\" //无类型常量的多重赋值 //5、常量赋值是编译期行为，可以赋值为一个编译期运算的常量表达式 const mask=1\u003c\u003c3 2.3预定义常量 //预定义常量：true、false、iota //iota：可修改常量，在每次const出现时被重置为0，在下一个const出现前，每出现一次iota，其代表的值自动增1。 const( //iota重置为0 c0=iota //c0==0 c1=iota //c1==1 c2=iota //c2==2 ) //两个const赋值语句一样可以省略后一个 const( //iota重置为0 c0=iota //c0==0 c1 //c1==1 c2 //c2==2 ) 2.4枚举 枚举指一系列相关常量。\nconst( Sunday=iota //Sunday==0,以此类推 Monday Tuesday Wednesday Thursday Friday Saturday //大写字母开头表示包外可见 numberOfDays //小写字母开头表示包内私有 ) ","categories":"","description":"","excerpt":"1.变量 1.1变量声明 //1、单变量声明,类型放在变量名之后，可以为任意类型 var 变量名 类型 var v1,v2,v3 …","ref":"/golang-notes/basis/var-const/","tags":["Golang"],"title":"变量与常量"},{"body":" 面向对象编程 ​ 把一组数据结构和处理它们的方法组成对象（object），把相同行为的对象归纳为类（class），通过类的封装（encapsulation）隐藏内部细节，通过继承（inheritance）实现类的特化（specialization）[方法的重写，子类不同于父类的特性]／泛化（generalization）[共性，子类都拥有父类的特性]，通过多态（polymorphism）实现基于对象类型的动态分派（dynamic dispatch）。\n面对对象思想 ​ 面向对象思想是对现实世界事物的抽象，系统中一切事物皆为对象；对象是属性及其操作的封装体；对象可按其性质划分为类，对象成为类的实例；实例关系和继承关系是对象之间的静态关系；消息传递是对象之间动态联系的唯一形式，也是计算的唯一形式；方法是消息的序列。\n（一）类型系统[类的声明] 类型系统：\n一组基本类型构成的“基本类型集合”； “基本类型集合”上定义的一系列组合、运算、转换方法。 类型系统包括基础类型（byte、int、bool、float等）；复合类型（数组、结构体、指针等）；可以指向任何对象的类型（Any类型，类似Java的Object类型）；值语义和引用语义；面向对象类型；接口。Go大多数类型为值语义，可以给任何类型添加方法（包括内置类型，不包括指针类型）。Any类型是空接口即interface{}。\n1.方法 1、为类型添加方法[类方法声明]，方法即为有接收者的函数 func (对象名 对象类型) 函数名(参数列表) (返回值列表) 可随时为某个对象添加方法即为某个方法添加归属对象（receiver），以方法为中心 在Go语言中没有隐藏的this指针，即显示传递，形参即为this，例如以下的形参为a。\ntype Integer int func (a Integer) Less(b Integer) bool{ //表示a这个对象定义了Less这个方法，a可以为任意类型 return a\u003cb } //类型基于值传递，如果要修改值需要传递指针 func (a *Integer) Add(b Integer){ *a+=b //通过指针传递来改变值 } 2.值语义和引用语义 值类型：b的修改并不会影响a的值\n引用类型：b的修改会影响a的值\nGo大多类型为值语义，包括基本类型：byte，int，string等；复合类型：数组，结构体(struct)，指针等\n//2、值语义和引用语义 b=a b.Modify() //值类型 var a=[3]int{1,2,3} b:=a b[1]++ fmt.Println(a,b) //a=[1,2,3] b=[1,3,3] //引用类型 a:=[3]int{1,2,3} b:=\u0026a //b指向a,即为a的地址，对b指向的值改变实际上就是对a的改变（数组本身就是一种地址指向） b[1]++ fmt.Println(a,*b) //a=[1,3,3] b=[1,3,3] //*b,取地址指向的值 3.结构体 3、结构体[类属性的声明] struct的功能类似Java的class，可实现嵌套组合(类似继承的功能) struct实际上就是一种复合类型，只是对类中的属性进行定义赋值，并没有对方法进行定义，方法可以随时定义绑定到该类的对象上，更具灵活性。可利用嵌套组合来实现类似继承的功能避免代码重复。\ntype Rect struct{ //定义矩形类 x,y float64 //类型只包含属性，并没有方法 width,height float64 } func (r *Rect) Area() float64{ //为Rect类型绑定Area的方法，*Rect为指针引用可以修改传入参数的值 return r.width*r.height //方法归属于类型，不归属于具体的对象，声明该类型的对象即可调用该类型的方法 } （二）初始化[实例化对象] 数据初始化的内建函数new()与make()，二者都是用来分配空间。区别如下:\nnew() func new(Type) *Type 内置函数 new 分配空间。传递给new 函数的是一个类型，不是一个值。返回值是指向这个新分配的零值的指针 make() func make(Type, size IntegerType) Type 内建函数 make 分配并且初始化 一个 slice, 或者 map 或者 chan 对象。 并且只能是这三种对象。 和 new 一样，第一个参数是 类型，不是一个值。 但是make 的返回值就是这个类型（即使一个引用类型），而不是指针。 具体的返回值，依赖具体传入的类型。 //创建实例 rect1:=new(Rect) //new一个对象 rect2:=\u0026Rect{} //为赋值默认值，bool默认值为false，int默认为零值0，string默认为空字符串 rect3:=\u0026Rect{0,0,100,200} //取地址并赋值,按声明的变量顺序依次赋值 rect4:=\u0026Rect{width:100,height:200} //按变量名赋值不按顺序赋值 //构造函数：没有构造参数的概念，通常由全局的创建函数NewXXX来实现构造函数的功能 func NewRect(x,y,width,height float64) *Rect{ return \u0026Rect{x,y,width,height} //利用指针来改变传入参数的值达到类似构造参数的效果 } //方法的重载,Go不支持方法的重载（函数同名，参数不同） //v …interface{}表示参数不定的意思，其中v是slice类型，及声明不定参数，可以传入任意参数，实现类似方法的重载 func (poem *Poem) recite(v ...interface{}) { fmt.Println(v) } （三）匿名组合[继承] ​ 组合，即方法代理，例如A包含B，即A通过消息传递的形式代理了B的方法，而不需要重复写B的方法。\n​ 继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。继承主要为了代码复用，继承也可以扩展已存在的代码模块（类）。\n​ 严格来讲，继承是“a kind of ”，即子类是父类的一种，例如student是person的一种；组合是“a part of”，即父类是子类中的一部分，例如眼睛是头部的一部分。\n//1、匿名组合的方式实现了类似Java继承的功能，可以实现多继承 type Base struct{ Name string } func (base *Base) Foo(){...} //Base的Foo()方法 func (base *Base) Bar(){...} //Base的Bar()方法 type Foo struct{ Base //通过组合的方式声明了基类，即继承了基类 ... } func (foo *Foo) Bar(){ foo.Base.Bar() //并改写了基类的方法，该方法实现时先调用基类的Bar()方法 ... //如果没有改写即为继承，调用foo.Foo()和调用foo.Base.Foo()的作用的一样的 } //修改内存布局 type Foo struct{ ... //其他成员信息 Base } //以指针方式组合 type Foo struct{ *Base //以指针方式派生，创建Foo实例时，需要外部提供一个Base类实例的指针 ... } //名字冲突问题,组合内外如果出现名字重复问题，只会访问到最外层，内层会被隐藏，不会报错，即类似java中方法覆盖/重写。 type X struct{ Name string } type Y struct{ X //Y.X.Name会被隐藏，内层会被隐藏 Name string //只会访问到Y.Name，只会调用外层属性 } （四）可见性[封装] ​ 封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。\n​ 封装的本质或目的其实程序对信息(数据)的控制力。封装分为两部分：该隐藏的隐藏，该暴露的暴露。封装可以隐藏实现细节，使得代码模块化。\n​ Go中用大写字母开头来表示public，可以包外访问；小写字母开头来表示private，只能包内访问；访问性是包级别非类型级别 ​ 如果可访问性是类型一致的，可以加friend关键字表示朋友关系可互相访问彼此的私有成员(属性和方法)\ntype Rect struct{ X,Y float64 Width,Height float64 //字母大写开头表示该属性可以由包外访问到 } func (r *Rect) area() float64{ //字母小写开头表示该方法只能包内调用 return r.Width*r.Height } （五）接口[多态] ​ 多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。\n​ 简而言之，就是允许将子类类型的指针赋值给父类类型的指针。\n​ 即一个引用变量倒底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。不修改程序代码就可以改变程序运行时所绑定的具体代码，让程序可以选择多个运行状态，这就是多态性。多态分为编译时多态（静态多态）和运行时多态（动态多态），编译时多态一般通过方法重载实现，运行时多态一般通过方法重写实现。\n5.1接口概念 ​ 接口即一组方法的集合，定义了对象的一组行为，方法包含实际的代码。换句话说，一个接口就是定义（规范或约束），而方法就是实现，接口的作用应该是将定义与实现分离，降低耦合度。习惯用“er”结尾来命名，例如“Reader”。接口与对象的关系是多对多，即一个对象可以实现多个接口，一个接口也可以被多个对象实现。\n​ 接口是Go语言整个类型系统的基石，其他语言的接口是不同组件之间的契约的存在，对契约的实现是强制性的，必须显式声明实现了该接口，这类接口称之为“侵入式接口”。而Go语言的接口是隐式存在，只要实现了该接口的所有函数则代表已经实现了该接口，并不需要显式的接口声明。\n接口的比喻 ​ 你的电脑上只有一个USB接口。这个USB接口可以接MP3，数码相机，摄像头，鼠标，键盘等。。。所有的上述硬件都可以公用这个接口，有很好的扩展性，该USB接口定义了一种规范，只要实现了该规范，就可以将不同的设备接入电脑，而设备的改变并不会对电脑本身有什么影响（低耦合）。\n面向接口编程 ​ 接口表示调用者和设计者的一种约定，在多人合作开发同一个项目时，事先定义好相互调用的接口可以大大提高开发的效率。接口是用类来实现的，实现接口的类必须严格按照接口的声明来实现接口提供的所有功能。有了接口，就可以在不影响现有接口声明的情况下，修改接口的内部实现，从而使兼容性问题最小化。 ​ 当其他设计者调用了接口后，就不能再随意更改接口的定义，否则项目开发者事先的约定就失去了意义。但是可以在类中修改相应的代码，完成需要改动的内容。\n5.2非侵入式接口 非侵入式接口：一个类只需要实现了接口要求的所有函数就表示实现了该接口，并不需要显式声明\ntype File struct{ //类的属性 } //File类的方法 func (f *File) Read(buf []byte) (n int,err error) func (f *File) Write(buf []byte) (n int,err error) func (f *File) Seek(off int64,whence int) (pos int64,err error) func (f *File) Close() error //接口1：IFile type IFile interface{ Read(buf []byte) (n int,err error) Write(buf []byte) (n int,err error) Seek(off int64,whence int) (pos int64,err error) Close() error } //接口2：IReader type IReader interface{ Read(buf []byte) (n int,err error) } //接口赋值,File类实现了IFile和IReader接口，即接口所包含的所有方法 var file1 IFile = new(File) var file2 IReader = new(File) 5.3接口赋值 只要类实现了该接口的所有方法，即可将该类赋值给这个接口，接口主要用于多态化方法。即对接口定义的方法，不同的实现方式。\n接口赋值： 1）将对象实例赋值给接口\ntype IUSB interface{ //定义IUSB的接口方法 } //方法定义在类外，绑定该类，以下为方便，备注写在类中 type MP3 struct{ //实现IUSB的接口，具体实现方式是MP3的方法 } type Mouse struct{ //实现IUSB的接口，具体实现方式是Mouse的方法 } //接口赋值给具体的对象实例MP3 var usb IUSB =new(MP3) usb.Connect() usb.Close() //接口赋值给具体的对象实例Mouse var usb IUSB =new(Mouse) usb.Connect() usb.Close() 2）将接口赋值给另一个接口\n只要两个接口拥有相同的方法列表（与次序无关），即是两个相同的接口，可以相互赋值 接口赋值只需要接口A的方法列表是接口B的子集（即假设接口A中定义的所有方法，都在接口B中有定义），那么B接口的实例可以赋值给A的对象。反之不成立，即子接口B包含了父接口A，因此可以将子接口的实例赋值给父接口。 即子接口实例实现了子接口的所有方法，而父接口的方法列表是子接口的子集，则子接口实例自然实现了父接口的所有方法，因此可以将子接口实例赋值给父接口。 type Writer interface{ //父接口 Write(buf []byte) (n int,err error) } type ReadWriter interface{ //子接口 Read(buf []byte) (n int,err error) Write(buf []byte) (n int,err error) } var file1 ReadWriter=new(File) //子接口实例 var file2 Writer=file1 //子接口实例赋值给父接口 5.4接口查询 若要在 switch 外判断一个接口类型是否实现了某个接口，可以使用“逗号 ok ”。\nvalue, ok := Interfacevariable.(implementType)\n其中 Interfacevariable 是接口变量（接口值），implementType 为实现此接口的类型，value 返回接口变量实际类型变量的值，如果该类型实现了此接口返回 true。\n//判断file1接口指向的对象实例是否是File类型 var file1 Writer=... if file5,ok:=file1.(File);ok{ ... } 5.5接口类型查询 在 Go 中，要判断传递给接口值的变量类型，可以在使用 type switch 得到。(type)只能在 switch 中使用。\n// 另一个实现了 I 接口的 R 类型 type R struct { i int } func (p *R) Get() int { return p.i } func (p *R) Put(v int) { p.i = v } func f(p I) { switch t := p.(type) { // 判断传递给 p 的实际类型 case *S: // 指向 S 的指针类型 case *R: // 指向 R 的指针类型 case S: // S 类型 case R: // R 类型 default: //实现了 I 接口的其他类型 } } 5.6接口组合 //接口组合类似类型组合，只不过只包含方法，不包含成员变量 type ReadWriter interface{ //接口组合，避免代码重复 Reader //接口Reader Writer //接口Writer } 5.7Any类型[空接口] 每种类型都能匹配到空接口：interface{}。空接口类型对方法没有任何约束（因为没有方法），它能包含任意类型，也可以实现到其他接口类型的转换。如果传递给该接口的类型变量实现了转换后的接口则可以正常运行，否则出现运行时错误。\n//interface{}即为可以指向任何对象的Any类型，类似Java中的Object类 var v1 interface{}=struct{X int}{1} var v2 interface{}=\"abc\" func DoSomething(v interface{}) { //该函数可以接收任何类型的参数，因为任何类型都实现了空接口 // ... } 5.8接口的代码示例 //接口animal type Animal interface { Speak() string } //Dog类实现animal接口 type Dog struct { } func (d Dog) Speak() string { return \"Woof!\" } //Cat类实现animal接口 type Cat struct { } func (c Cat) Speak() string { return \"Meow!\" } //Llama实现animal接口 type Llama struct { } func (l Llama) Speak() string { return \"?????\" } //JavaProgrammer实现animal接口 type JavaProgrammer struct { } func (j JavaProgrammer) Speak() string { return \"Design patterns!\" } //主函数 func main() { animals := []Animal{Dog{}, Cat{}, Llama{}, JavaProgrammer{}} //利用接口实现多态 for _, animal := range animals { fmt.Println(animal.Speak()) //打印不同实现该接口的类的方法返回值 } } ","categories":"","description":"","excerpt":" 面向对象编程 ​ 把一组数据结构和处理它们的方法组成对象（object），把相同行为的对象归纳为类（class），通过类的封 …","ref":"/golang-notes/oop/golang-object-oriented-programming/","tags":["Golang"],"title":"Golang系列（二）之面向对象编程"},{"body":"（一）并发基础 1.概念 并发意味着程序在运行时有多个执行上下文，对应多个调用栈。\n并发与并行的区别：\n并发的主流实现模型：\n实现模型 说明 特点 多进程 操作系统层面的并发模式 处理简单，互不影响，但开销大 多线程 系统层面的并发模式 有效，开销较大，高并发时影响效率 基于回调的非阻塞/异步IO 多用于高并发服务器开发中 编程复杂，开销小 协程 用户态线程，不需要操作系统抢占调度，寄存于线程中 编程简单，结构简单，开销极小，但需要语言的支持 共享内存系统：线程之间采用共享内存的方式通信，通过加锁来避免死锁或资源竞争。\n消息传递系统：将线程间共享状态封装在消息中，通过发送消息来共享内存，而非通过共享内存来通信。\n2.协程 执行体是个抽象的概念，在操作系统中分为三个级别：进程（process），进程内的线程（thread），进程内的协程（coroutine，轻量级线程）。协程的数量级可达到上百万个，进程和线程的数量级最多不超过一万个。Go语言中的协程叫goroutine，Go标准库提供的调用操作，IO操作都会出让CPU给其他goroutine，让协程间的切换管理不依赖系统的线程和进程，不依赖CPU的核心数量。\n3.并发通信 并发编程的难度在于协调，协调需要通过通信，并发通信模型分为共享数据和消息。共享数据即多个并发单元保持对同一个数据的引用，数据可以是内存数据块，磁盘文件，网络数据等。数据共享通过加锁的方式来避免死锁和资源竞争。Go语言则采取消息机制来通信，每个并发单元是独立的个体，有独立的变量，不同并发单元间这些变量不共享，每个并发单元的输入输出只通过消息的方式。\n（二）goroutine 1. go关键字 //定义调用体 func Add(x,y int){ z:=x+y fmt.Println(z) } //go关键字执行调用，即会产生一个goroutine并发执行 //当函数返回时，goroutine自动结束，如果有返回值,返回值会自动被丢弃 go Add(1,1) //并发执行 func main(){ for i:=0;i\u003c10;i++{//主函数启动了10个goroutine，然后返回，程序退出，并不会等待其他goroutine结束 go Add(i,i) //所以需要通过channel通信来保证其他goroutine可以顺利执行 } } 2. sync.WaitGroup sync.WaitGroup用来实现启动一组goroutine，并等待任务做完再结束goroutine。\n使用方法是：\nwg.Add()：main协程通过调用 wg.Add(delta int) 设置worker协程的个数，然后创建worker协程； wg.Done()：worker协程执行结束以后，都要调用 wg.Done()，表示做完任务，goroutine减1； wg.Wait() ：main协程调用 wg.Wait() 且被block，直到所有worker协程全部执行结束后返回。 针对可能panic的goroutine，可以使用defer wg.Done()来结束goroutine。 示例：\npackage main import ( \"fmt\" \"sync\" ) func main() { var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { fmt.Println(i) wg.Done() }(i) } wg.Wait() } 输出如下，随机输出0到9的数字\n9 5 6 7 8 1 0 3 4 2 3. sync.Map Go 语言原生 map 并不是线程安全的，对它进行并发读写操作的时候，需要加锁。sync.map 则是一种并发安全的 map，可以使用在并发读写map的场景中。\nsync.Map常见操作：\n写入：m.Store(\"1\", 18) 读取：age, ok := m.Load(\"1\") 删除：m.Delete(\"1\") 遍历：m.Range(func(key, value interface{}) bool{} 存在则读取否则写入：\tm.LoadOrStore(\"2\", 100) package main import ( \"fmt\" \"sync\" ) func main() { var m sync.Map // 1. 写入 m.Store(\"1\", 18) m.Store(\"2\", 20) // 2. 读取 age, ok := m.Load(\"1\") fmt.Println(age, ok) // 3. 遍历 m.Range(func(key, value interface{}) bool { name := key.(string) age := value.(int) fmt.Println(name, age) return true }) // 4. 删除 m.Delete(\"1\") age, ok = m.Load(\"1\") fmt.Println(age, ok) // 5. 如果key存在则读取，否则写入给定的值 m.LoadOrStore(\"2\", 100) age, _ = m.Load(\"2\") fmt.Println(age) } 示例：\n3.1. 不加锁的map并发读写 以下使用线程不安全的map，进行并发写入，就会出现并发报错。可以通过加锁来解决并发问题，但更推荐使用sync.Map来实现。\n示例1：\npackage main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { // 生成随机种子 rand.Seed(time.Now().Unix()) sm := make(map[int]int) var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { for j := 0; j \u003c= 9; j++ { r := rand.Intn(100) // 生成0-99的随机数 sm[j] = r // 同时对map进行并发写入 } wg.Done() }(i) } wg.Wait() // 打印map中的值 fmt.Println(sm) } 输出异常：\nmap不能并发写入。\nfatal error: concurrent map writes 3.2. 加锁的并发读写 示例2：\npackage main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { // 生成随机种子 rand.Seed(time.Now().Unix()) sm := \u0026SafeMap{ Map: make(map[int]int), } var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { for j := 0; j \u003c= 9; j++ { r := rand.Intn(100) // 生成0-99的随机数 sm.Set(i, r) // 同时对map进行并发写入 } wg.Done() }(i) } wg.Wait() // 打印map中的值 fmt.Println(sm.Map) } // SafeMap type SafeMap struct { Map map[int]int lock sync.RWMutex // 加锁 } // Set func (m *SafeMap) Set(key, value int) { m.lock.Lock() defer m.lock.Unlock() m.Map[key] = value } // Get func (m *SafeMap) Get(key int) int { return m.Map[key] } 正常输出\nmap[0:52 1:16 2:86 3:50 4:97 5:38 6:54 7:75 8:26 9:32] 3.3. 使用sync.Map并发读写 示例3：\n以下使用线程安全的sync.Map来实现对map的值进行并发的读写。\npackage main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { // 生成随机种子 rand.Seed(time.Now().Unix()) // 使用线程安全的sync.Map var sm sync.Map var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { for j := 0; j \u003c= 9; j++ { r := rand.Intn(100) // 生成0-99的随机数 sm.Store(j, r) } wg.Done() }(i) } wg.Wait() // 打印map中的值 sm.Range(func(k, v interface{}) bool { fmt.Println(k, v) return true }) } 正常输出：\n2 92 4 5 8 48 9 6 0 50 1 64 6 27 7 86 3 59 5 57 （三）channel ​ channel就像管道的形式，是goroutine之间的通信方式，是进程内的通信方式，跨进程通信建议用分布式系统的方法来解决，例如Socket或http等通信协议。channel是类型相关，即一个channel只能传递一种类型的值，在声明时指定。\n1、基本语法 1）channel的声明 //1、channel声明，声明一个管道chanName，该管道可以传递的类型是ElementType //管道是一种复合类型，[chan ElementType],表示可以传递ElementType类型的管道[类似定语从句的修饰方法] var chanName chan ElementType var ch chan int //声明一个可以传递int类型的管道 var m map[string] chan bool //声明一个map，值的类型为可以传递bool类型的管道 2）初始化 //2、初始化ch:=make(chan int) //make一般用来声明一个复合类型，参数为复合类型的属性 3）管道读写 //3、管道写入,把值想象成一个球，\"\u003c-\"的方向，表示球的流向，ch即为管道 //写入时，当管道已满（管道有缓冲长度）则会导致程序堵塞，直到有goroutine从中读取出值 ch \u003c- value //管道读取，\"\u003c-\"表示从管道把球倒出来赋值给一个变量 //当管道为空，读取数据会导致程序阻塞，直到有goroutine写入值 value:= \u003c-ch 4）select //4、每个case必须是一个IO操作，面向channel的操作，只执行其中的一个case操作，一旦满足则结束select过程 //面向channel的操作无非三种情况：成功读出；成功写入；即没有读出也没有写入 select{ case \u003c-chan1: //如果chan1读到数据，则进行该case处理语句 case chan2\u003c-1: //如果成功向chan2写入数据，则进入该case处理语句 default: //如果上面都没有成功，则进入default处理流程 } 2、缓冲和超时机制 1）缓冲机制 //1、缓冲机制：为管道指定空间长度，达到类似消息队列的效果 c:=make(chan int,1024) //第二个参数为缓冲区大小，与切片的空间大小类似 //通过range关键字来实现依次读取管道的数据，与数组或切片的range使用方法类似 for i :=range c{ fmt.Println(\"Received:\",i) } 2）超时机制 //2、超时机制：利用select只要一个case满足，程序就继续执行而不考虑其他case的情况的特性实现超时机制 timeout:=make(chan bool,1) //设置一个超时管道 go func(){ time.Sleep(1e9) //设置超时时间，等待一分钟 timeout\u003c-true //一分钟后往管道放一个true的值 }() // select { case \u003c-ch: //如果读到数据，则会结束select过程 //从ch中读取数据 case \u003c-timeout: //如果前面的case没有调用到，必定会读到true值，结束select，避免永久等待 //一直没有从ch中读取到数据，但从timeout中读取到了数据 } 3、channel的传递 //1、channel的传递，来实现Linux系统中管道的功能，以插件的方式增加数据处理的流程 type PipeData struct{ value int handler func(int) int //handler是属性？ next chan int //可以把[chan int]看成一个整体，表示放int类型的管道 } func handler(queue chan *PipeData){ //queue是一个存放*PipeDate类型的管道，可改变管道里的数据块内容 for data:=range queue{ //data的类型就是管道存放定义的类型，即PipeData data.next \u003c- data.handler(data.value) //该方法实现将PipeData的value值存放到next的管道中 } } 4、单向channel //2、单向channel：只能用于接收或发送数据，是对channel的一种使用限制 //单向channel的声明 var ch1 chan int //正常channel，可读写 var ch2 chan\u003c- int //单向只写channel [chan\u003c- int]看成一个整体，表示流入管道 var ch3 \u003c-chan int //单向只读channel [\u003c-chan int]看成一个整体，表示流出管道 //管道类型强制转换 ch4:=make(chan int) //ch4为双向管道 ch5:=\u003c-chan int(ch4) //把[\u003c-chan int]看成单向只读管道类型，对ch4进行强制类型转换 ch6:=chan\u003c- int(ch4) //把[chan\u003c- int]看成单向只写管道类型，对ch4进行强制类型转换 func Parse(ch \u003c-chan int){ //最小权限原则 for value:=range ch{ fmt.Println(\"Parsing value\",value) } } 5、关闭channel //3、关闭channel，使用内置函数close()函数即可 close(ch) //判断channel是否关闭 x,ok:=\u003c-ch //ok==false表示channel已经关闭 if !ok { //如果channel关闭，ok==false，!ok==true //执行体 } （四）多核并行化与同步锁 1、多核并行化 //多核并行化 runtime.GOMAXPROCS(16) //设置环境变量GOMAXPROCS的值来控制使用多少个CPU核心 runtime.NumCPU() //来获取核心数 //出让时间片 runtime.Gosched() //在每个goroutine中控制何时出让时间片给其他goroutine 2、同步锁 //同步锁 sync.Mutex //单读单写：占用Mutex后，其他goroutine只能等到其释放该Mutex sync.RWMutex //单写多读：会阻止写，不会阻止读 RLock() //读锁 Lock() //写锁 RUnlock() //解锁（读锁） Unlock() //解锁（写锁） //全局唯一性操作 //once的Do方法保证全局只调用指定函数(setup)一次，其他goroutine在调用到此函数是会阻塞，直到once调用结束才继续 once.Do(setup) ","categories":"","description":"","excerpt":"（一）并发基础 1.概念 并发意味着程序在运行时有多个执行上下文，对应多个调用栈。\n并发与并行的区别：\n并发的主流实现模型： …","ref":"/golang-notes/concurrency/golang-concurrent-programming/","tags":["Golang"],"title":"Golang系列（三）之并发编程"},{"body":"1. 部署 1.1. 使用安装包的方式 rpm -ivh nginx-xxx.rpm\n1.2. 使用源代码安装 1.2.1. 下载源码包 wget http://blob.wae.haplat.net/nginx/nginx-1.9.13.tar.gz 1.2.2. 创建临时目录并解压源码包 mkdir $HOME/build cd $HOME/build \u0026\u0026 tar zxvf nginx-\u003cversion-number\u003e.tar.gz 1.2.3. 编译并安装 cd $HOME/build/nginx-\u003cversion-number\u003e ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ ... #\u003c更多配置项见以下说明\u003e make \u0026\u0026 make install 1.2.4. 配置项 1.2.4.1. 通用配置项 配置选项 说明 --prefix= nginx安装的根路径，所有其他的路径都要依赖与该选项 --sbin-path= nginx二进制文件的路径，如果没有指定则会依赖于--prefix --conf-path= 如果在命令行中没有指定配置文件，则通过该配置项去查找配置文件 --error-log-path= 指定错误文件的路径 --pid-path= 指定的文件将会写入nginx master进程的pid，通常在/var/run下 --lock-path= 共享存储器互斥锁文件的路径 --user= worker进程运行的用户 --group= worker进程运行的组 --with-file-aio 启动异步I/O --with-debug 启用调试日志，生产环境不推荐配置 1.2.4.2. 优化配置项 配置选项 说明 --with-cc= 如果想设置一个不在默认PATH下的C编译器 --with-cpp= 设置C预处理器的相应路径 --with-cc-opt= 指定必要的include文件路径 --with-ld-opt= 包含连接器库的路径和运行路径 --with-cpu-opt= 通过该选项为特定的CPU构建nginx 1.2.4.3. http模块的配置项 配置选项 说明 --without-http-cache 在使用upstream模块时，nginx能够配置本地缓存内容，该选项可以禁用缓存 --with-http_perl_module nginx配置能够扩展使用perl代码。该项启用这个模块，但会降低性能 --with-perl_modules_path= 对于额外嵌入的perl模块，该选项指定该perl解析器的路径 --with-perl= 如果在默认的路径中找不到perl则指定perl（5.6版本以上）的路径 --http-log-path= http访问日志的默认路径 --http-client-body-temp-path= 从客户端收到请求后，该项用于作为请求体临时存放的目录 --http-proxy-temp-path= 在使用代理后，通过该项设置存放临时文件路径 --http-fastcgi-temp-path= 设置FastCGI临时文件的目录 --http-uwsgi-temp-path= 设置uWSGI临时文件的目录 --http-scgi-temp-path= 设置SCGI临时文件的目录 1.2.4.4. 其他模块额外配置项 默认没有安装这些模块，可以通过--with-_module来启用相应的模块功能。\n配置选项 说明 --with-http_ssl_module 如果需要对流量进行加密，可以使用该选项，再URLs中开始部分将会是https(需要OpenSSL库) --with-http_realip_module 如果nginx在七层负载均衡器或者其他设备之后，它们将Http头中的客户端IP地址传递，则需要启用该模块，再多个客户处于一个IP地址的情况下使用 --with-http_addition_module 该模块作为输出过滤器，使能够在请求经过一个location前或后时在该location本身添加内容 --with-http_xslt_module 该模块用于处理XML响应转换，基于一个或多个XSLT格式 --with-http_image_filter_module 该模块被作为图像过滤器使用，在将图像投递到客户之前进行处理（需要libgd库） --with-http_geoip_module 使用该模块，能够设置各种变量以便在配置文件中的区段使用，基于地理位置查找客户端IP地址 --with-http_sub_module 该模块实现替代过滤，在响应中用一个字符串替代另一个字符串 --with-heep_dav_module 启用这个模块将激活使用WebDAV的配置指令。 --with-http_flv_module 如果需要提供Flash流媒体视频文件，那么该模块将会提供伪流媒体 --with-http_mp4_module 这个模块支持H.264/AAC文件伪流媒体 --with-http_gzip_static_module 当被调用的资源没有.gz结尾格式的文件时，如果想支持发送预压缩版本的静态文件，那么使用该模块 --with-http_gunzip_module 对于不支持gzip编码的客户，该模块用于为客户解压缩预压缩内容 --with-http_random_index_module 如果你想提供从一个目录中随机选择文件的索引文件，那么该模块需要激活 --with-http_secure_link_module 该模块提供一种机制，它会将一个哈希值链接到一个URL中，因此只有那些使用正确密码能够计算链接 --with-http_stub_status_module 启用这个模块后会收集Nginx自身的状态信息。输出的状态信息可以使用RRDtool或类似的东西绘制成图 2. 配置 配置文件一般为/etc/nginx/nginx.conf或/usr/local/nginx/conf/nginx.conf。\n2.1. 基本配置格式 \u003csection\u003e{ \u003cdirective\u003e \u003cparameters\u003e; } 每一个指令行由分号结束，大括号{}表示一个新的上下文。\n2.2. Nginx全局配置参数 全局配置指令\n模块 配置项 说明 main模块 user 配置worker进程的用户和组，如果忽略group，则group等于指定的用户的所属组 worker_processes 指定worker进程的启动数量，可将其设置为可用的CPU内核数，若为auto为自动检测 error_log 所有错误的写入文件，第二个参数指定错误的级别（debug，info，notice，warn，error，crit，alert，emerg） pid 设置主进程IP的文件 events模块 use 用于设置使用什么样的连接方法 worker_connections 用于配置一个工作进程能够接受的并发连接最大数。包括客户连接和向上游服务器的连接。 2.3. 使用include文件 include文件可以在任何地方以增强配置文件的可读性，使用include文件要确保被包含文件自身正确的nginx语法，即配置指令和块，然后指定这些文件的路径。\ninclude /etc/nginx/mime.types;\n若使用通配符则表示通配的多个文件，若没有给定全路径则依据主配置文件路径进行搜索。\ninclude /etc/nginx/conf.d/*.conf\n测试配置文件(包括include的配置文件)语法：\nnginx -t -c {path-to-nginx.conf}\n2.4. 配置说明 2.4.1. main模块 #main模块类似main函数包含其他子模块，非模块配置项(包括模块内)分号结尾，子模块配置花括号结尾 user nobady; #一般按默认设置 pid /var/run/nginx.pid; #进程标识符存放路径，一般按默认设置 worker_processes auto; #nginx对外提供web服务时的worder进程数，可将其设置为可用的CPU内核数，auto为自动检测 worker_rlimit_nofile 100000; # 更改worker进程的最大打开文件数限制 error_log logs/error.log info; #错误日志存放路径 keepalive_timeout 60; #keepalive_timeout 60; events{ #见events模块 } http{ #见http模块 server{ ... location /{ } } } mail{ #见mail模块 } 2.4.2. events模块 events { worker_connections 2048; #设置可由一个worker进程同时打开的最大连接数 multi_accept on; #告诉nginx收到一个新连接通知后接受尽可能多的连接 use epoll; #设置用于复用客户端线程的轮询方法。Linux 2.6+：使用epoll；*BSD：使用kqueue。 } 2.4.3. http模块 http { #http模块 server { #server模块，http服务上的虚拟主机， server 当做对应一个域名进行的配置 listen 80; #配置监听端口 server_name www.linuxidc.com; #配置访问域名 access_log logs/linuxidc.access.log main; #指定日志文件的存放路径 index index.html; #默认访问页面 root /var/www/androidj.com/htdocs; # root 是指将本地的一个文件夹作为所有 url 请求的根路径 upstream backend { #反向代理的后端机器，实现负载均衡 ip_hash; #指明了我们均衡的方式是按照用户的 ip 地址进行分配 server backend1.example.com; server backend2.example.com; server backend3.example.com; server backend4.example.com; } location / { #location 是在一个域名下对更精细的路径进行配置 proxy_pass http://backend; #反向代理到后端机器 } } server { listen 80; server_name www.Androidj.com; access_log logs/androidj.access.log main; location / { index index.html; root /var/www/androidj.com/htdocs; } } } 2.4.4. mail模块 mail { auth_http 127.0.0.1:80/auth.php; pop3_capabilities \"TOP\" \"USER\"; imap_capabilities \"IMAP4rev1\" \"UIDPLUS\"; server { listen 110; protocol pop3; proxy on; } server { listen 25; protocol smtp; proxy on; smtp_auth login plain; xclient off; } } ","categories":"","description":"","excerpt":"1. 部署 1.1. 使用安装包的方式 rpm -ivh nginx-xxx.rpm\n1.2. 使用源代码安装 1.2.1. …","ref":"/linux-notes/nginx/install-nginx/","tags":["Nginx"],"title":"Nginx的部署与配置"},{"body":"1. Go中的测试框架 Go语言中自带有一个轻量级的测试框架testing和自带的go test命令来实现单元测试和性能测试，testing框架和其他语言中的测试框架类似，你可以基于这个框架写针对相应函数的测试用例，也可以基于该框架写相应的压力测试用例。\n2. 单元测试原则 文件名必须是_test.go结尾的，这样在执行go test的时候才会执行到相应的代码 你必须import testing这个包 所有的测试用例函数必须是Test开头 测试用例会按照源代码中写的顺序依次执行 测试函数TestXxx()的参数是testing.T，我们可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T),Xxx部分可以为任意的字母数字的组合，但是首字母不能是小写字母[a-z]，例如Testintdiv是错误的函数名。 函数中通过调用testing.T的Error, Errorf, FailNow, Fatal, FatalIf方法，说明测试不通过，调用Log方法用来记录测试的信息。 3 测试常用命令 # 测试整个目录 go test -v ./pkg/... ./cmd/... -coverprofile cover.out # 测试某个文件 go test -v file_test.go file.go # 测试某个函数 go test -v -test.run TestFunction 4. 示例 4.1. 源文件getest.go package gotest import ( \"errors\" ) func Division(a, b float64) (float64, error) { if b == 0 { return 0, errors.New(\"除数不能为0\") } return a / b, nil } 4.2. 测试文件gotest_test.go func Test_Division_2(t *testing.T) { if _, e := Division(6, 0); e == nil { //try a unit test on function t.Error(\"Division did not work as expected.\") // 如果不是如预期的那么就报错 } else { t.Log(\"one test passed.\", e) //记录一些你期望记录的信息 } } 5. 压力测试 压力测试用来检测函数(方法）的性能，和编写单元功能测试的方法类似。\n压力测试用例必须遵循如下格式，其中XXX可以是任意字母数字的组合，但是首字母不能是小写字母 func BenchmarkXXX(b *testing.B) { ... } go test不会默认执行压力测试的函数，如果要执行压力测试需要带上参数-test.bench，语法:-test.bench=\"test_name_regex\",例如go test -test.bench=\".*\"表示测试全部的压力测试函数 在压力测试用例中,请记得在循环体内使用testing.B.N,以使测试可以正常的运行 文件名也必须以_test.go结尾 5.1. 示例 package gotest import ( \"testing\" ) func Benchmark_Division(b *testing.B) { for i := 0; i \u003c b.N; i++ { //use b.N for looping Division(4, 5) } } func Benchmark_TimeConsumingFunction(b *testing.B) { b.StopTimer() //调用该函数停止压力测试的时间计数 //做一些初始化的工作,例如读取文件数据,数据库连接之类的, //这样这些时间不影响我们测试函数本身的性能 b.StartTimer() //重新开始时间 for i := 0; i \u003c b.N; i++ { Division(4, 5) } } 执行测试命令\ngo test -file webbench_test.go -test.bench=\".*\" ","categories":"","description":"","excerpt":"1. Go中的测试框架 Go语言中自带有一个轻量级的测试框架testing和自带的go test命令来实现单元测试和性能测试，testing …","ref":"/golang-notes/test/test/","tags":["Golang"],"title":"单元测试"},{"body":"1. beego的使用 1.1. beego的安装 go get github.com/astaxie/beego 1.2. beego的升级 1、直接升级\ngo get -u github.com/astaxie/beego 2、源码下载升级\n用户访问 https://github.com/astaxie/beego ,下载源码，然后覆盖到 $GOPATH/src/github.com/astaxie/beego 目录，然后通过本地执行安装就可以升级了：\ngo install github.com/astaxie/beego 2. beego的架构 beego 是一个快速开发 Go 应用的 HTTP 框架，他可以用来快速开发 API、Web 及后端服务等各种应用，是一个 RESTful 的框架。\n2.1. beego架构图 beego 是基于八大独立的模块构建的，是一个高度解耦的框架。\n可以使用 cache 模块来做你的缓存逻辑；使用日志模块来记录你的操作信息；使用 config 模块来解析你各种格式的文件。\n2.2. beego执行逻辑 参考：\nhttps://beego.me/docs/intro/ https://beego.me/docs/install/ ","categories":"","description":"","excerpt":"1. beego的使用 1.1. beego的安装 go get github.com/astaxie/beego 1.2. beego的升 …","ref":"/golang-notes/web/beego/beego-introduction/","tags":["Golang"],"title":"Beego 介绍"},{"body":"1. http包建立web服务器 package main import ( \"fmt\" \"log\" \"net/http\" \"strings\" ) func sayhelloName(w http.ResponseWriter, r *http.Request) { r.ParseForm() fmt.Println(r.Form) fmt.Println(\"path\", r.URL.Path) fmt.Println(\"scheme\", r.URL.Scheme) fmt.Println(r.Form[\"url_long\"]) for k, v := range r.Form { fmt.Println(\"key:\", k) fmt.Println(\"val:\", strings.Join((v), \"\")) } fmt.Println(w, \"hello world\") } func main() { http.HandleFunc(\"/\", sayhelloName) err := http.ListenAndServe(\":9090\", nil) if err != nil { log.Fatal(\"ListenAndServe:\", err) } } 2. http包的运行机制 相关源码位于：/src/net/http/server.go\n服务端的几个概念\nRequest：用户请求的信息，用来解析用户的请求信息，包括post，get，Cookie，url等信息。 Response:服务器需要反馈给客户端的信息。 Conn：用户的每次请求链接。 Handle:处理请求和生成返回信息的处理逻辑。 Go实现web服务的流程\n创建Listen Socket，监听指定的端口，等待客户端请求到来。 Listen Socket接受客户端的请求，得到Client Socket，接下来通过Client Socket与客户端通信。 处理客户端请求，首先从Client Socket读取HTTP请求的协议头，如果是POST方法，还可能要读取客户端提交的数据，然后交给相应的handler处理请求，handler处理完，将数据通过Client Socket返回给客户端。 2.1. http包执行流程图 2.2. 注册路由[HandleFunc] http.HandlerFunc类型默认实现了ServeHTTP的接口。\n// The HandlerFunc type is an adapter to allow the use of // ordinary functions as HTTP handlers. If f is a function // with the appropriate signature, HandlerFunc(f) is a // Handler that calls f. type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } // HandleFunc registers the handler function for the given pattern // in the DefaultServeMux. // The documentation for ServeMux explains how patterns are matched. func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } ... // HandleFunc registers the handler function for the given pattern. func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { mux.Handle(pattern, HandlerFunc(handler)) } Handle\n// Handle registers the handler for the given pattern. // If a handler already exists for pattern, Handle panics. func (mux *ServeMux) Handle(pattern string, handler Handler) { mux.mu.Lock() defer mux.mu.Unlock() if pattern == \"\" { panic(\"http: invalid pattern \" + pattern) } if handler == nil { panic(\"http: nil handler\") } if mux.m[pattern].explicit { panic(\"http: multiple registrations for \" + pattern) } mux.m[pattern] = muxEntry{explicit: true, h: handler, pattern: pattern} if pattern[0] != '/' { mux.hosts = true } // Helpful behavior: // If pattern is /tree/, insert an implicit permanent redirect for /tree. // It can be overridden by an explicit registration. n := len(pattern) if n \u003e 0 \u0026\u0026 pattern[n-1] == '/' \u0026\u0026 !mux.m[pattern[0:n-1]].explicit { // If pattern contains a host name, strip it and use remaining // path for redirect. path := pattern if pattern[0] != '/' { // In pattern, at least the last character is a '/', so // strings.Index can't be -1. path = pattern[strings.Index(pattern, \"/\"):] } url := \u0026url.URL{Path: path} mux.m[pattern[0:n-1]] = muxEntry{h: RedirectHandler(url.String(), StatusMovedPermanently), pattern: pattern} } } 2.3. 如何监听端口 通过ListenAndServe来监听，底层实现：初始化一个server对象，调用net.Listen(\"tcp\",addr)，也就是底层用TCP协议搭建了一个服务，监听设置的端口。然后调用srv.Serve(net.Listener)函数，这个函数处理接收客户端的请求信息。这个函数里起了一个for循环，通过Listener接收请求，创建conn，开一个goroutine，把请求的数据当作参数给conn去服务：go c.serve()，即每次请求都是在新的goroutine中去服务，利于高并发。\nsrc/net/http/server.go\n// ListenAndServe always returns a non-nil error. func ListenAndServe(addr string, handler Handler) error { server := \u0026Server{Addr: addr, Handler: handler} return server.ListenAndServe() } ... // ListenAndServe listens on the TCP network address srv.Addr and then // calls Serve to handle requests on incoming connections. // Accepted connections are configured to enable TCP keep-alives. // If srv.Addr is blank, \":http\" is used. // ListenAndServe always returns a non-nil error. func (srv *Server) ListenAndServe() error { addr := srv.Addr if addr == \"\" { addr = \":http\" } ln, err := net.Listen(\"tcp\", addr) if err != nil { return err } return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)}) } 2.4. 如何接收客户端的请求 srv.Serve\n// Serve accepts incoming connections on the Listener l, creating a // new service goroutine for each. The service goroutines read requests and // then call srv.Handler to reply to them. // Serve always returns a non-nil error. func (srv *Server) Serve(l net.Listener) error { defer l.Close() if fn := testHookServerServe; fn != nil { fn(srv, l) } var tempDelay time.Duration // how long to sleep on accept failure if err := srv.setupHTTP2(); err != nil { return err } for { rw, e := l.Accept() if e != nil { if ne, ok := e.(net.Error); ok \u0026\u0026 ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u003e max { tempDelay = max } srv.logf(\"http: Accept error: %v; retrying in %v\", e, tempDelay) time.Sleep(tempDelay) continue } return e } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve() } } 关键代码：\nc := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve() newConn\n// Create new connection from rwc. func (srv *Server) newConn(rwc net.Conn) *conn { c := \u0026conn{ server: srv, rwc: rwc, } if debugServerConnections { c.rwc = newLoggingConn(\"server\", c.rwc) } return c } 2.5. 如何分配handler conn先解析request：c.readRequest()，获取相应的handler:handler:=c.server.Handler，即ListenAndServe的第二个参数，因为值为nil，所以默认handler=DefaultServeMux。该变量是一个路由器，用来匹配url跳转到其相应的handle函数。其中http.HandleFunc(\"/\",sayhelloName)即注册了请求“/”的路由规则，当uri为“/”时，路由跳转到函数sayhelloName。DefaultServeMux会调用ServeHTTP方法，这个方法内部调用sayhelloName本身，最后写入response的信息反馈给客户端。\n2.5.1. c.serve() // Serve a new connection. func (c *conn) serve() { ... for { w, err := c.readRequest() ... serverHandler{c.server}.ServeHTTP(w, w.req) .. } } 2.5.2. c.readRequest() // Read next request from connection. func (c *conn) readRequest() (w *response, err error) { if c.hijacked() { return nil, ErrHijacked } if d := c.server.ReadTimeout; d != 0 { c.rwc.SetReadDeadline(time.Now().Add(d)) } if d := c.server.WriteTimeout; d != 0 { defer func() { c.rwc.SetWriteDeadline(time.Now().Add(d)) }() } c.r.setReadLimit(c.server.initialReadLimitSize()) c.mu.Lock() // while using bufr if c.lastMethod == \"POST\" { // RFC 2616 section 4.1 tolerance for old buggy clients. peek, _ := c.bufr.Peek(4) // ReadRequest will get err below c.bufr.Discard(numLeadingCRorLF(peek)) } req, err := readRequest(c.bufr, keepHostHeader) c.mu.Unlock() if err != nil { if c.r.hitReadLimit() { return nil, errTooLarge } return nil, err } c.lastMethod = req.Method c.r.setInfiniteReadLimit() hosts, haveHost := req.Header[\"Host\"] if req.ProtoAtLeast(1, 1) \u0026\u0026 (!haveHost || len(hosts) == 0) { return nil, badRequestError(\"missing required Host header\") } if len(hosts) \u003e 1 { return nil, badRequestError(\"too many Host headers\") } if len(hosts) == 1 \u0026\u0026 !validHostHeader(hosts[0]) { return nil, badRequestError(\"malformed Host header\") } for k, vv := range req.Header { if !validHeaderName(k) { return nil, badRequestError(\"invalid header name\") } for _, v := range vv { if !validHeaderValue(v) { return nil, badRequestError(\"invalid header value\") } } } delete(req.Header, \"Host\") req.RemoteAddr = c.remoteAddr req.TLS = c.tlsState if body, ok := req.Body.(*body); ok { body.doEarlyClose = true } w = \u0026response{ conn: c, req: req, reqBody: req.Body, handlerHeader: make(Header), contentLength: -1, } w.cw.res = w w.w = newBufioWriterSize(\u0026w.cw, bufferBeforeChunkingSize) return w, nil } 2.5.3. ServeHTTP(w, w.req) func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == \"*\" \u0026\u0026 req.Method == \"OPTIONS\" { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } 2.5.4. DefaultServeMux type ServeMux struct { mu sync.RWMutex m map[string]muxEntry hosts bool // whether any patterns contain hostnames } type muxEntry struct { explicit bool h Handler pattern string } // NewServeMux allocates and returns a new ServeMux. func NewServeMux() *ServeMux { return \u0026ServeMux{m: make(map[string]muxEntry)} } // DefaultServeMux is the default ServeMux used by Serve. var DefaultServeMux = NewServeMux() handler接口的定义\ntype Handler interface { ServeHTTP(ResponseWriter, *Request) } 2.5.5. ServeMux.ServeHTTP // ServeHTTP dispatches the request to the handler whose // pattern most closely matches the request URL. func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \"*\" { if r.ProtoAtLeast(1, 1) { w.Header().Set(\"Connection\", \"close\") } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) h.ServeHTTP(w, r) } mux.Handler(r)\n// Handler returns the handler to use for the given request, // consulting r.Method, r.Host, and r.URL.Path. It always returns // a non-nil handler. If the path is not in its canonical form, the // handler will be an internally-generated handler that redirects // to the canonical path. // // Handler also returns the registered pattern that matches the // request or, in the case of internally-generated redirects, // the pattern that will match after following the redirect. // // If there is no registered handler that applies to the request, // Handler returns a ``page not found'' handler and an empty pattern. func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) { if r.Method != \"CONNECT\" { if p := cleanPath(r.URL.Path); p != r.URL.Path { _, pattern = mux.handler(r.Host, p) url := *r.URL url.Path = p return RedirectHandler(url.String(), StatusMovedPermanently), pattern } } return mux.handler(r.Host, r.URL.Path) } // handler is the main implementation of Handler. // The path is known to be in canonical form, except for CONNECT methods. func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), \"\" } return } 2.6. http连接处理流程图 3. http的执行流程总结 1、首先调用Http.HandleFunc，按如下顺序执行：\n调用了DefaultServerMux的HandleFunc。 调用了DefaultServerMux的Handle。 往DefaultServerMux的map[string] muxEntry中增加对应的handler和路由规则。 2、调用http.ListenAndServe(\":9090\",nil)，按如下顺序执行：\n实例化Server。 调用Server的ListenAndServe()。 调用net.Listen(\"tcp\",addr)监听端口。 启动一个for循环，在循环体中Accept请求。 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve()。 读取每个请求的内容w,err:=c.readRequest()。 判断handler是否为空，如果没有设置handler，handler默认设置为DefaultServeMux。 调用handler的ServeHttp。 根据request选择handler，并且进入到这个handler的ServeHTTP, mux.handler(r).ServeHTTP(w,r) 选择handler 判断是否有路由能满足这个request（循环遍历ServeMux的muxEntry）。 如果有路由满足，调用这个路由handler的ServeHttp。 如果没有路由满足，调用NotFoundHandler的ServeHttp。 4. 自定义路由 Go支持外部实现路由器，ListenAndServe的第二个参数就是配置外部路由器，它是一个Handler接口。即外部路由器实现Hanlder接口。\nHandler接口：\ntype Handler interface { ServeHTTP(ResponseWriter, *Request) } 自定义路由\npackage main import ( \"fmt\" \"net/http\" ) type MyMux struct{ } func (p *MyMux) ServeHTTP(w http.ResponseWriter,r *http.Request){ if r.URL.Path==\"/\"{ sayhelloName(w,r) return } http.NotFound(w,r) return } func sayhelloName(w http.ResponseWriter,r *http.Request){ fmt.Fprintln(w,\"Hello myroute\") } func main() { mux:=\u0026MyMux{} http.ListenAndServe(\":9090\",mux) } 文章参考：\n《Go web编程》\n","categories":"","description":"","excerpt":"1. http包建立web服务器 package main import ( \"fmt\" \"log\" \"net/http\" …","ref":"/golang-notes/web/golang-http-execution-flow/","tags":["Golang"],"title":"Http包源码分析"},{"body":"JSON处理 JSON是一种轻量级的数据交换语言。\n1. 解析JSON[Unmarshal(data []byte, v interface{})] 1.1. Unmarshal源码 /src/encoding/json/decode.go\nfunc Unmarshal(data []byte, v interface{}) error { // Check for well-formedness. // Avoids filling out half a data structure // before discovering a JSON syntax error. var d decodeState err := checkValid(data, \u0026d.scan) if err != nil { return err } d.init(data) return d.unmarshal(v) } ... func (d *decodeState) unmarshal(v interface{}) (err error) { defer func() { if r := recover(); r != nil { if _, ok := r.(runtime.Error); ok { panic(r) } err = r.(error) } }() rv := reflect.ValueOf(v) if rv.Kind() != reflect.Ptr || rv.IsNil() { return \u0026InvalidUnmarshalError{reflect.TypeOf(v)} } d.scan.reset() // We decode rv not rv.Elem because the Unmarshaler interface // test must be applied at the top level of the value. d.value(rv) return d.savedError } 1.2. 解析到结构体 package main import ( \"encoding/json\" \"fmt\" ) type Server struct { ServerName string ServerIP string } type Serverslice struct { Servers []Server } func main() { var s Serverslice str := `{\"servers\": [{\"serverName\":\"Shanghai_VPN\",\"serverIP\":\"127.0.0.1\"}, {\"serverName\":\"Beijing_VPN\",\"serverIP\":\"127.0.0.2\"}]}` err:=json.Unmarshal([]byte(str), \u0026s) if err!=nil{ fmt.Println(err) } fmt.Println(s) } 说明\nJSON格式与结构体一一对应，Unmarshal方法即将JSON文本转换成结构体。只会匹配结构体中的可导出字段，即首字母大写字段（类似java的public），匹配规则如下：json的key为Foo为例\n先查找struct tag中含有Foo的可导出的struct字段（首字母大写） 其次查找字段名为Foo的可导出字段。 最后查找类似FOO或者FoO这类除首字母外，其他大小写不敏感的可导出字段。 1.3. 解析到interface 2. 生成JSON[Marshal(v interface{})] 2.1. Marshal源码 /src/encoding/json/encode.go\nfunc Marshal(v interface{}) ([]byte, error) { e := \u0026encodeState{} err := e.marshal(v) if err != nil { return nil, err } return e.Bytes(), nil } ... func (e *encodeState) marshal(v interface{}) (err error) { defer func() { if r := recover(); r != nil { if _, ok := r.(runtime.Error); ok { panic(r) } if s, ok := r.(string); ok { panic(s) } err = r.(error) } }() e.reflectValue(reflect.ValueOf(v)) return nil } 2.2. 使用方法 package main import ( \"encoding/json\" \"fmt\" ) type Server struct { ServerName string `json:\"serverName,string\"` ServerIP string `json:\"serverIP,omitempty\"` } type Serverslice struct { Servers []Server `json:\"servers\"` } func main() { var s Serverslice s.Servers = append(s.Servers, Server{ServerName: \"Shanghai_VPN\", ServerIP: \"127.0.0.1\"}) s.Servers = append(s.Servers, Server{ServerName: \"Beijing_VPN\", ServerIP: \"127.0.02\"}) b, err := json.Marshal(s) if err != nil { fmt.Println(\"JSON ERR:\", err) } fmt.Println(string(b)) } 2.3. 说明 Marshal方法将结构体转换成json文本，匹配规则如下：\n如果字段的tag是“-”，那么该字段不会输出到JSON。 如果tag中带有自定义名称，那么该自定义名称会出现在JSON字段名中。例如例子中的“serverName” 如果tag中带有“omitempty”选项，那么如果该字段值为空，就不会输出到JSON中。 如果字段类型是bool,string,int,int64等，而tag中带有“，string”选项，那么这个字段在输出到JSON的时候会把该字段对应的值转换成JSON字符串。 注意事项：\nMarshal只有在转换成功的时候才会返回数据，JSON对象只支持string作为key，如果要编码一个map,那么必须是map[string]T这种类型。（T为任意类型） Channel,complex和function不能被编码成JSON。 嵌套的数据不能编码，会进入死循环。 指针在编码时会输出指针指向的内容，而空指针会输出null。 ","categories":"","description":"","excerpt":"JSON处理 JSON是一种轻量级的数据交换语言。\n1. 解析JSON[Unmarshal(data []byte, v …","ref":"/golang-notes/text/json/","tags":["Golang"],"title":"Json处理"},{"body":"Pod的配置管理 Kubernetes v1.2的版本提供统一的集群配置管理方案–ConfigMap。\n1. ConfigMap：容器应用的配置管理 使用场景：\n生成为容器内的环境变量。 设置容器启动命令的启动参数（需设置为环境变量）。 以Volume的形式挂载为容器内部的文件或目录。 ConfigMap以一个或多个key:value的形式保存在kubernetes系统中供应用使用，既可以表示一个变量的值（例如：apploglevel=info），也可以表示完整配置文件的内容（例如：server.xml=\u003c?xml...\u003e...）。\n可以通过yaml配置文件或者使用kubectl create configmap命令的方式创建ConfigMap。\n2. 创建ConfigMap 2.1. 通过yaml文件方式 cm-appvars.yaml\napiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: info appdatadir: /var/data 常用命令\nkubectl create -f cm-appvars.yaml\nkubectl get configmap\nkubectl describe configmap cm-appvars\nkubectl get configmap cm-appvars -o yaml\n2.2. 通过kubectl命令行方式 通过kubectl create configmap创建，使用参数--from-file或--from-literal指定内容，可以在一行中指定多个参数。\n1）通过--from-file参数从文件中进行创建，可以指定key的名称，也可以在一个命令行中创建包含多个key的ConfigMap。\nkubectl create configmap NAME --from-file=[key=]source --from-file=[key=]source\n2）通过--from-file参数从目录中进行创建，该目录下的每个配置文件名被设置为key，文件内容被设置为value。\nkubectl create configmap NAME --from-file=config-files-dir\n3）通过--from-literal从文本中进行创建，直接将指定的key=value创建为ConfigMap的内容。\nkubectl create configmap NAME --from-literal=key1=value1 --from-literal=key2=value2\n容器应用对ConfigMap的使用有两种方法：\n通过环境变量获取ConfigMap中的内容。 通过Volume挂载的方式将ConfigMap中的内容挂载为容器内部的文件或目录。 2.3. 通过环境变量的方式 ConfigMap的yaml文件:cm-appvars.yaml\napiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: info appdatadir: /var/data Pod的yaml文件：cm-test-pod.yaml\napiVersion: v1 kind: Pod metadata: name: cm-test-pod spec: containers: - name: cm-test image: busybox command: [\"/bin/sh\",\"-c\",\"env|grep APP\"] env: - name: APPLOGLEVEL valueFrom: configMapKeyRef: name: cm-appvars key: apploglevel - name: APPDATADIR valueFrom: configMapKeyRef: name: cm-appvars key: appdatadir 创建命令：\nkubectl create -f cm-test-pod.yaml\nkubectl get pods --show-all\nkubectl logs cm-test-pod\n3. 使用ConfigMap的限制条件 ConfigMap必须在Pod之前创建 ConfigMap也可以定义为属于某个Namespace。只有处于相同Namespace中的Pod可以引用它。 kubelet只支持可以被API Server管理的Pod使用ConfigMap。静态Pod无法引用。 在Pod对ConfigMap进行挂载操作时，容器内只能挂载为“目录”，无法挂载为文件。 参考文章\n《Kubernetes权威指南》 ","categories":"","description":"","excerpt":"Pod的配置管理 Kubernetes v1.2的版本提供统一的集群配置管理方案–ConfigMap。\n1. ConfigMap：容器应用的 …","ref":"/kubernetes-notes/concepts/configmap/pod-configmap/","tags":["Kubernetes"],"title":"ConfigMap"},{"body":"1. Docker的网络基础 1.1. Network Namespace 不同的网络命名空间中，协议栈是独立的，完全隔离，彼此之间无法通信。同一个网络命名空间有独立的路由表和独立的Iptables/Netfilter来提供包的转发、NAT、IP包过滤等功能。\n1.1.1. 网络命名空间的实现 将与网络协议栈相关的全局变量变成一个Net Namespace变量的成员，然后在调用协议栈函数中加入一个Namepace参数。\n1.1.2. 网络命名空间的操作 1、创建网络命名空间\nip netns add name\n2、在命名空间内执行命令\nip netns exec name command\n3、进入命名空间\nip netns exec name bash\n2. Docker的网络实现 2.1. 容器网络 Docker使用Linux桥接，在宿主机虚拟一个Docker容器网桥(docker0)，Docker启动一个容器时会根据Docker网桥的网段分配给容器一个IP地址，称为Container-IP，同时Docker网桥是每个容器的默认网关。因为在同一宿主机内的容器都接入同一个网桥，这样容器之间就能够通过容器的Container-IP直接通信。\nDocker网桥是宿主机虚拟出来的，并不是真实存在的网络设备，外部网络是无法寻址到的，这也意味着外部网络无法通过直接Container-IP访问到容器。如果容器希望外部访问能够访问到，可以通过映射容器端口到宿主主机（端口映射），即docker run创建容器时候通过 -p 或 -P 参数来启用，访问容器的时候就通过[宿主机IP]:[容器端口]访问容器。\n2.2. 4类网络模式 Docker网络模式 配置 说明 host模式 --net=host 容器和宿主机共享Network namespace。 container模式 --net=container:NAME_or_ID 容器和另外一个容器共享Network namespace。 kubernetes中的pod就是多个容器共享一个Network namespace。 none模式 --net=none 容器有独立的Network namespace，但并没有对其进行任何网络设置，如分配veth pair 和网桥连接，配置IP等。 bridge模式 --net=bridge（默认为该模式） 桥接模式 3. Docker网络模式 3.1. bridge桥接模式 在bridge模式下，Docker可以使用独立的网络栈。实现方式是父进程在创建子进程的时候通过传入CLONE_NEWNET的参数创建出一个网络命名空间。\n实现步骤：\nDocker Daemon首次启动时会创建一个虚拟网桥docker0，地址通常为172.x.x.x开头，在私有的网络空间中给这个网络分配一个子网。 由Docker创建处理的每个容器，都会创建一个虚拟以太设备对（veth pair），一端关联到网桥，另一端使用Namespace技术映射到容器内的eth0设备，然后从网桥的地址段内给eth0接口分配一个IP地址。 一般情况，宿主机IP与docker0 IP、容器IP是不同的IP段，默认情况，外部看不到docker0和容器IP，对于外部来说相当于docker0和容器的IP为内网IP。\n3.1.1. 外部网络访问Docker容器 外部访问docker容器可以通过端口映射(NAT)的方式，Docker使用NAT的方式将容器内部的服务与宿主机的某个端口port_1绑定。\n外部访问容器的流程如下：\n外界网络通过宿主机的IP和映射的端口port_1访问。 当宿主机收到此类请求，会通过DNAT将请求的目标IP即宿主机IP和目标端口即映射端口port_1替换成容器的IP和容器的端口port_0。 由于宿主机上可以识别容器IP，所以宿主机将请求发给veth pair。 veth pair将请求发送给容器内部的eth0，由容器内部的服务进行处理。 3.1.2. Docker容器访问外部网络 docker容器访问外部网络的流程：\ndocker容器向外部目标IP和目标端口port_2发起请求，请求报文中的源IP为容器IP。\n请求通过容器内部的eth0到veth pair的另一端docker0网桥。\ndocker0网桥通过数据报转发功能将请求转发到宿主机的eth0。\n宿主机处理请求时通过SNAT将请求中的源IP换成宿主机eth0的IP。\n处理后的报文通过请求的目标IP发送到外部网络。\n3.1.3. 缺点 使用NAT的方式可能会带来性能的问题，影响网络传输效率。\n3.2. host模式 host模式并没有给容器创建一个隔离的网络环境，而是和宿主机共用一个网络命名空间，容器使用宿主机的eth0和外界进行通信，同样容器也共用宿主机的端口资源，即分配端口可能存在与宿主机已分配的端口冲突的问题。\n实现的方式即父进程在创建子进程的时候不传入CLONE_NEWNET的参数，从而和宿主机共享一个网络空间。\nhost模式没有通过NAT的方式进行转发因此性能上相对较好，但是不存在网络隔离性，可能产生端口冲突的问题。\n3.3. container模式 container模式即docker容器可以使用其他容器的网络命名空间，即和其他容器处于同一个网络命名空间。\n步骤：\n查找其他容器的网络命名空间。 新创建的容器的网络命名空间使用其他容器的网络命名空间。 通过和其他容器共享网络命名空间的方式，可以让不同的容器之间处于相同的网络命名空间，可以直接通过localhost的方式进行通信，简化了强关联的多个容器之间的通信问题。\nk8s中的pod的概念就是通过一组容器共享一个网络命名空间来达到pod内部的不同容器可以直接通过localhost的方式进行通信。\n3.4. none模式 none模式即不为容器创建任何的网络环境，用户可以根据自己的需要手动去创建不同的网络定制配置。\n参考：\n《Docker源码分析》 ","categories":"","description":"","excerpt":"1. Docker的网络基础 1.1. Network Namespace 不同的网络命名空间中，协议栈是独立的，完全隔离，彼此之间无法通 …","ref":"/kubernetes-notes/network/docker-network/","tags":["Kubernetes"],"title":"Docker网络"},{"body":"1. Master 集群的控制节点，负责整个集群的管理和控制，kubernetes的所有的命令基本都是发给Master，由它来负责具体的执行过程。\n1.1. Master的组件 kube-apiserver：资源增删改查的入口 kube-controller-manager：资源对象的大总管 kube-scheduler：负责资源调度（Pod调度） etcd Server:kubernetes的所有的资源对象的数据保存在etcd中。 2. Node Node是集群的工作负载节点，默认情况kubelet会向Master注册自己，一旦Node被纳入集群管理范围，kubelet会定时向Master汇报自身的情报，包括操作系统，Docker版本，机器资源情况等。\n如果Node超过指定时间不上报信息，会被Master判断为“失联”，标记为Not Ready，随后Master会触发Pod转移。\n2.1. Node的组件 kubelet:Pod的管家，与Master通信 kube-proxy：实现kubernetes Service的通信与负载均衡机制的重要组件 Docker：容器的创建和管理 2.2. Node相关命令 kubectl get nodes\nkuebctl describe node {node_name}\n2.3. describe命令的Node信息 Node基本信息：名称、标签、创建时间等 Node当前的状态，Node启动后会进行自检工作，磁盘是否满，内存是否不足，若都正常则切换为Ready状态。 Node的主机地址与主机名 Node上的资源总量：CPU,内存，最大可调度Pod数量等 Node可分配资源量：当前Node可用于分配的资源量 主机系统信息：主机唯一标识符UUID，Linux kernel版本号，操作系统，kubernetes版本，kubelet与kube-proxy版本 当前正在运行的Pod列表及概要信息 已分配的资源使用概要，例如资源申请的最低、最大允许使用量占系统总量的百分比 Node相关的Event信息。 3. Pod Pod是Kubernetes中操作的基本单元。每个Pod中有个根容器(Pause容器)，Pause容器的状态代表整个容器组的状态，其他业务容器共享Pause的IP，即Pod IP，共享Pause挂载的Volume，这样简化了同个Pod中不同容器之间的网络问题和文件共享问题。\nKubernetes集群中，同宿主机的或不同宿主机的Pod之间要求能够TCP/IP直接通信，因此采用虚拟二层网络技术来实现，例如Flannel，Openvswitch(OVS)等，这样在同个集群中，不同的宿主机的Pod IP为不同IP段的IP，集群中的所有Pod IP都是唯一的，不同Pod之间可以直接通信。 Pod有两种类型：普通Pod和静态Pod。静态Pod即不通过K8S调度和创建，直接在某个具体的Node机器上通过具体的文件来启动。普通Pod则是由K8S创建、调度，同时数据存放在ETCD中。 Pod IP和具体的容器端口（ContainnerPort）组成一个具体的通信地址，即Endpoint。一个Pod中可以存在多个容器，可以有多个端口，Pod IP一样，即有多个Endpoint。 Pod Volume是定义在Pod之上，被各个容器挂载到自己的文件系统中，可以用分布式文件系统实现后端存储功能。 Pod中的Event事件可以用来排查问题，可以通过kubectl describe pod xxx 来查看对应的事件。 每个Pod可以对其能使用的服务器上的计算资源设置限额，一般为CPU和Memory。K8S中一般将千分之一个的CPU配置作为最小单位，用m表示，是一个绝对值，即100m对于一个Core的机器还是48个Core的机器都是一样的大小。Memory配额也是个绝对值，单位为内存字节数。 资源配额的两个参数 Requests:该资源的最小申请量，系统必须满足要求。 Limits:该资源最大允许使用量，当超过该量，K8S会kill并重启Pod。 4. Label Label是一个键值对，可以附加在任何对象上，比如Node,Pod,Service,RC等。Label和资源对象是多对多的关系，即一个Label可以被添加到多个对象上，一个对象也可以定义多个Label。 Label的作用主要用来实现精细的、多维度的资源分组管理，以便进行资源分配，调度，配置，部署等工作。 Label通俗理解就是“标签”，通过标签来过滤筛选指定的对象，进行具体的操作。k8s通过Label Selector(标签选择器)来筛选指定Label的资源对象，类似SQL语句中的条件查询（WHERE语句）。 Label Selector有基于等式和基于集合的两种表达方式，可以多个条件进行组合使用。 基于等式：name=redis-slave（匹配name=redis-slave的资源对象）;env!=product(匹配所有不具有标签env=product的资源对象) 基于集合：name in (redis-slave,redis-master);name not in (php-frontend)（匹配所有不具有标签name=php-frontend的资源对象） 使用场景\nkube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本数，从而实现副本数始终保持预期数目。 kube-proxy进程通过Service的Label Selector来选择对应Pod，自动建立每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制。 kube-scheduler实现Pod定向调度：对Node定义特定的Label，并且在Pod定义文件中使用NodeSelector标签调度策略。 5. Replication Controller(RC) RC是k8s系统中的核心概念，定义了一个期望的场景。\n主要包括：\nPod期望的副本数（replicas） 用于筛选目标Pod的Label Selector 用于创建Pod的模板（template） RC特性说明：\nPod的缩放可以通过以下命令实现：kubectl scale rc redis-slave --replicas=3 删除RC并不会删除该RC创建的Pod，可以将副本数设置为0，即可删除对应Pod。或者通过kubectl stop /delete命令来一次性删除RC和其创建的Pod。 改变RC中Pod模板的镜像版本可以实现滚动升级（Rolling Update）。具体操作见https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/ Kubernetes1.2以上版本将RC升级为Replica Set，它与当前RC的唯一区别在于Replica Set支持基于集合的Label Selector(Set-based selector)，而旧版本RC只支持基于等式的Label Selector(equality-based selector)。 Kubernetes1.2以上版本通过Deployment来维护Replica Set而不是单独使用Replica Set。即控制流为：Delpoyment→Replica Set→Pod。即新版本的Deployment+Replica Set替代了RC的作用。 6. Deployment Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。\n使用场景\n创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。 检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。 更新Deployment以创建新的Pod(例如镜像升级的场景)。 如果当前Deployment不稳定，回退到上一个Deployment版本。 挂起或恢复一个Deployment。 可以通过kubectl describe deployment来查看Deployment控制的Pod的水平拓展过程。\n7. Horizontal Pod Autoscaler(HPA) Horizontal Pod Autoscaler(HPA)即Pod横向自动扩容，与RC一样也属于k8s的资源对象。\nHPA原理：通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否针对性调整Pod的副本数。\nPod负载度量指标：\nCPUUtilizationPercentage：Pod所有副本自身的CPU利用率的平均值。即当前Pod的CPU使用量除以Pod Request的值。 应用自定义的度量指标，比如服务每秒内响应的请求数（TPS/QPS）。 8. Service(服务) 8.1. Service概述 Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。\n8.2. kubernetes的服务发现机制 主要通过kube-dns这个组件来进行DNS方式的服务发现。\n8.3. 外部系统访问Service的问题 IP类型 说明 Node IP Node节点的IP地址 Pod IP Pod的IP地址 Cluster IP Service的IP地址 8.3.1. Node IP NodeIP是集群中每个节点的物理网卡IP地址，是真实存在的物理网络，kubernetes集群之外的节点访问kubernetes内的某个节点或TCP/IP服务的时候，需要通过NodeIP进行通信。\n8.3.2. Pod IP Pod IP是每个Pod的IP地址，是Docker Engine根据docker0网桥的IP段地址进行分配的，是一个虚拟二层网络，集群中一个Pod的容器访问另一个Pod中的容器，是通过Pod IP进行通信的，而真实的TCP/IP流量是通过Node IP所在的网卡流出的。\n8.3.3. Cluster IP Service的Cluster IP是一个虚拟IP，只作用于Service这个对象，由kubernetes管理和分配IP地址（来源于Cluster IP地址池）。 Cluster IP无法被ping通，因为没有一个实体网络对象来响应。 Cluster IP结合Service Port组成的具体通信端口才具备TCP/IP通信基础，属于kubernetes集群内，集群外访问该IP和端口需要额外处理。 k8s集群内Node IP 、Pod IP、Cluster IP之间的通信采取k8s自己的特殊的路由规则，与传统IP路由不同。 8.3.4. 外部访问Kubernetes集群 通过宿主机与容器端口映射的方式进行访问，例如：Service定位文件如下：\n可以通过任意Node的IP 加端口访问该服务。也可以通过Nginx或HAProxy来设置负载均衡。\n9. Volume(存储卷) 9.1. Volume的功能 Volume是Pod中能够被多个容器访问的共享目录，可以让容器的数据写到宿主机上或者写文件到网络存储中 可以实现容器配置文件集中化定义与管理，通过ConfigMap资源对象来实现。 9.2. Volume的特点 k8s中的Volume与Docker的Volume相似，但不完全相同。\nk8s上Volume定义在Pod上，然后被一个Pod中的多个容器挂载到具体的文件目录下。 k8s的Volume与Pod生命周期相关而不是容器是生命周期，即容器挂掉，数据不会丢失但是Pod挂掉，数据则会丢失。 k8s中的Volume支持多种类型的Volume：Ceph、GlusterFS等分布式系统。 9.3. Volume的使用方式 先在Pod上声明一个Volume，然后容器引用该Volume并Mount到容器的某个目录。\n9.4. Volume类型 9.4.1. emptyDir emptyDir Volume是在Pod分配到Node时创建的，初始内容为空，无须指定宿主机上对应的目录文件，由K8S自动分配一个目录，当Pod被删除时，对应的emptyDir数据也会永久删除。\n作用：\n临时空间，例如程序的临时文件，无须永久保留 长时间任务的中间过程CheckPoint的临时保存目录 一个容器需要从另一个容器中获取数据的目录（即多容器共享目录） 说明：\n目前用户无法设置emptyVolume的使用介质，如果kubelet的配置使用硬盘则emptyDir将创建在该硬盘上。\n9.4.2. hostPath hostPath是在Pod上挂载宿主机上的文件或目录。\n作用：\n容器应用日志需要持久化时，可以使用宿主机的高速文件系统进行存储 需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统。 注意点：\n在不同的Node上具有相同配置的Pod可能会因为宿主机上的目录或文件不同导致对Volume上目录或文件的访问结果不一致。 如果使用了资源配额管理，则kubernetes无法将hostPath在宿主机上使用的资源纳入管理。 9.4.3. gcePersistentDisk 表示使用谷歌公有云提供的永久磁盘（Persistent Disk ,PD）存放Volume的数据，它与EmptyDir不同，PD上的内容会被永久保存。当Pod被删除时，PD只是被卸载时，但不会被删除。需要先创建一个永久磁盘，才能使用gcePersistentDisk。\n使用gcePersistentDisk的限制条件：\nNode(运行kubelet的节点)需要是GCE虚拟机。 虚拟机需要与PD存在于相同的GCE项目中和Zone中。 10. Persistent Volume Volume定义在Pod上，属于“计算资源”的一部分，而Persistent Volume和Persistent Volume Claim是网络存储，简称PV和PVC，可以理解为k8s集群中某个网络存储中对应的一块存储。\nPV是网络存储，不属于任何Node，但可以在每个Node上访问。 PV不是定义在Pod上，而是独立于Pod之外定义。 PV常见类型：GCE Persistent Disks、NFS、RBD等。 PV是有状态的对象，状态类型如下：\nAvailable:空闲状态 Bound:已经绑定到某个PVC上 Released:对应的PVC已经删除，但资源还没有回收 Failed:PV自动回收失败 11. Namespace Namespace即命名空间，主要用于多租户的资源隔离，通过将资源对象分配到不同的Namespace上，便于不同的分组在共享资源的同时可以被分别管理。\nk8s集群启动后会默认创建一个“default”的Namespace。可以通过kubectl get namespaecs查看。\n可以通过kubectl config use-context namespace配置当前k8s客户端的环境，通过kubectl get pods获取当前namespace的Pod。或者通过kubectl get pods --namespace=NAMESPACE来获取指定namespace的Pod。\nNamespace yaml文件的定义\n12. Annotation(注解) Annotation与Label类似，也使用key/value的形式进行定义，Label定义元数据（Metadata）,Annotation定义“附加”信息。\n通常Annotation记录信息如下：\nbuild信息，release信息，Docker镜像信息等。 日志库、监控库等。 参考《Kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. Master 集群的控制节点，负责整个集群的管理和控制，kubernetes的所有的命令基本都是发给Master，由它来负责具体的执行 …","ref":"/kubernetes-notes/concepts/object/kubernetes-basic-concepts/","tags":["Kubernetes"],"title":"Kubernetes基本概念"},{"body":"1. redis是什么？（what） Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它支持字符串、哈希表、列表、集合、有序集合，位图，hyperloglogs等数据类型。内置复制、Lua脚本、LRU收回、事务以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动分区。\nRedis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。\n2. 为什么使用redis？（why） 2.1. redis的特点 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 2.2. redis的优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 2.3. redis与其他key-value存储有什么不同 Redis有着更为复杂的数据结构并且提供对他们的原子性操作，Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，应为数据量不能大于硬件内存。 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。 在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 3. 如何使用redis？（how） 3.1. redis的数据类型 数据类型 概念 常用命令 String(字符串) key-value型 SET ，GET Hash(哈希) field-value,适用于存储对象类型（对象名-对象属性值） HMSET，HEGTALL List(列表) string类型的有序列表，按照插入顺序排序 lpush，lrange Set(集合) string类型的无序集合 sadd，smembers zset(sorted set：有序集合) string类型元素的集合,且不允许重复的成员。每个元素关联一个double值来进行排序，double值可以重复但元素不能重复。 zadd，ZRANGEBYSCORE 3.2. redis常用命令 ","categories":"","description":"","excerpt":"1. redis是什么？（what） Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它 …","ref":"/linux-notes/redis/redis-introduction/","tags":["Redis"],"title":"Redis介绍"},{"body":" etcdctl的v3版本与v2版本使用命令有所不同，本文介绍etcdctl v3版本的命令工具的使用方式。\n1. etcdctl的安装 etcdctl的二进制文件可以在 github.com/coreos/etcd/releases 选择对应的版本下载，例如可以执行以下install_etcdctl.sh的脚本，修改其中的版本信息。\n#!/bin/bash ETCD_VER=v3.3.4 ETCD_DIR=etcd-download DOWNLOAD_URL=https://github.com/coreos/etcd/releases/download # Download mkdir ${ETCD_DIR} cd ${ETCD_DIR} wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz tar -xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz # install cd etcd-${ETCD_VER}-linux-amd64 cp etcdctl /usr/local/bin/ 2. etcdctl V3 使用etcdctlv3的版本时，需设置环境变量ETCDCTL_API=3。\nexport ETCDCTL_API=3 # 或者在`/etc/profile`文件中添加环境变量 vi /etc/profile ... export ETCDCTL_API=3 ... source /etc/profile # 或者在命令执行前加 ETCDCTL_API=3 ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS member list 查看当前etcdctl的版本信息etcdctl version。\n[root@k8s-dbg-master-1 etcd]# etcdctl version etcdctl version: 3.3.4 API version: 3.3 更多命令帮助可以查询etcdctl —help。\n[root@k8s-dbg-master-1 etcd]# etcdctl --help NAME: etcdctl - A simple command line client for etcd3. USAGE: etcdctl VERSION: 3.3.4 API VERSION: 3.3 COMMANDS: get\tGets the key or a range of keys put\tPuts the given key into the store del\tRemoves the specified key or range of keys [key, range_end) txn\tTxn processes all the requests in one transaction compaction\tCompacts the event history in etcd alarm disarm\tDisarms all alarms alarm list\tLists all alarms defrag\tDefragments the storage of the etcd members with given endpoints endpoint health\tChecks the healthiness of endpoints specified in `--endpoints` flag endpoint status\tPrints out the status of endpoints specified in `--endpoints` flag endpoint hashkv\tPrints the KV history hash for each endpoint in --endpoints move-leader\tTransfers leadership to another etcd cluster member. watch\tWatches events stream on keys or prefixes version\tPrints the version of etcdctl lease grant\tCreates leases lease revoke\tRevokes leases lease timetolive\tGet lease information lease list\tList all active leases lease keep-alive\tKeeps leases alive (renew) member add\tAdds a member into the cluster member remove\tRemoves a member from the cluster member update\tUpdates a member in the cluster member list\tLists all members in the cluster snapshot save\tStores an etcd node backend snapshot to a given file snapshot restore\tRestores an etcd member snapshot to an etcd directory snapshot status\tGets backend snapshot status of a given file make-mirror\tMakes a mirror at the destination etcd cluster migrate\tMigrates keys in a v2 store to a mvcc store lock\tAcquires a named lock elect\tObserves and participates in leader election auth enable\tEnables authentication auth disable\tDisables authentication user add\tAdds a new user user delete\tDeletes a user user get\tGets detailed information of a user user list\tLists all users user passwd\tChanges password of user user grant-role\tGrants a role to a user user revoke-role\tRevokes a role from a user role add\tAdds a new role role delete\tDeletes a role role get\tGets detailed information of a role role list\tLists all roles role grant-permission\tGrants a key to a role role revoke-permission\tRevokes a key from a role check perf\tCheck the performance of the etcd cluster help\tHelp about any command OPTIONS: --cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --command-timeout=5s\ttimeout for short running command (excluding dial timeout) --debug[=false]\tenable client-side debug logging --dial-timeout=2s\tdial timeout for client connections -d, --discovery-srv=\"\"\tdomain name to query for SRV records describing cluster endpoints --endpoints=[127.0.0.1:2379]\tgRPC endpoints --hex[=false]\tprint byte strings as hex encoded strings --insecure-discovery[=true]\taccept insecure SRV records describing cluster endpoints --insecure-skip-tls-verify[=false]\tskip server certificate verification --insecure-transport[=true]\tdisable transport security for client connections --keepalive-time=2s\tkeepalive time for client connections --keepalive-timeout=6s\tkeepalive timeout for client connections --key=\"\"\tidentify secure client using this TLS key file --user=\"\"\tusername[:password] for authentication (prompt if password is not supplied) -w, --write-out=\"simple\"\tset the output format (fields, json, protobuf, simple, table) 3. etcdctl 常用命令 3.1. 指定etcd集群 HOST_1=10.240.0.17 HOST_2=10.240.0.18 HOST_3=10.240.0.19 ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379 etcdctl --endpoints=$ENDPOINTS member list 如果etcd设置了证书访问，则需要添加证书相关参数：\nETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS --cacert=\u003cca-file\u003e --cert=\u003ccert-file\u003e --key=\u003ckey-file\u003e \u003ccommand\u003e 参数说明如下：\n--cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --key=\"\"\tidentify secure client using this TLS key file --endpoints=[127.0.0.1:2379]\tgRPC endpoints 可以自定义alias命令\n# alias 命令，避免每次需要输入证书参数 alias ectl='ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS --cacert=\u003cca-file\u003e --cert=\u003ccert-file\u003e --key=\u003ckey-file\u003e' # 直接使用别名执行命令 ectl \u003ccommand\u003e 3.2. 增删改查 1、增\netcdctl --endpoints=$ENDPOINTS put foo \"Hello World!\" 2、查\netcdctl --endpoints=$ENDPOINTS get foo etcdctl --endpoints=$ENDPOINTS --write-out=\"json\" get foo 基于相同前缀查找\netcdctl --endpoints=$ENDPOINTS put web1 value1 etcdctl --endpoints=$ENDPOINTS put web2 value2 etcdctl --endpoints=$ENDPOINTS put web3 value3 etcdctl --endpoints=$ENDPOINTS get web --prefix 列出所有的key\netcdctl --endpoints=$ENDPOINTS get / --prefix --keys-only 3、删\netcdctl --endpoints=$ENDPOINTS put key myvalue etcdctl --endpoints=$ENDPOINTS del key etcdctl --endpoints=$ENDPOINTS put k1 value1 etcdctl --endpoints=$ENDPOINTS put k2 value2 etcdctl --endpoints=$ENDPOINTS del k --prefix 3.3. 集群状态 集群状态主要是etcdctl endpoint status 和etcdctl endpoint health两条命令。\netcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status +------------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +------------------+------------------+---------+---------+-----------+-----------+------------+ | 10.240.0.17:2379 | 4917a7ab173fabe7 | 3.0.0 | 45 kB | true | 4 | 16726 | | 10.240.0.18:2379 | 59796ba9cd1bcd72 | 3.0.0 | 45 kB | false | 4 | 16726 | | 10.240.0.19:2379 | 94df724b66343e6c | 3.0.0 | 45 kB | false | 4 | 16726 | +------------------+------------------+---------+---------+-----------+-----------+------------+ etcdctl --endpoints=$ENDPOINTS endpoint health 10.240.0.17:2379 is healthy: successfully committed proposal: took = 3.345431ms 10.240.0.19:2379 is healthy: successfully committed proposal: took = 3.767967ms 10.240.0.18:2379 is healthy: successfully committed proposal: took = 4.025451ms 3.4. 集群成员 跟集群成员相关的命令如下：\nmember add\tAdds a member into the cluster member remove\tRemoves a member from the cluster member update\tUpdates a member in the cluster member list\tLists all members in the cluster 例如 etcdctl member list列出集群成员的命令。\netcdctl --endpoints=http://172.16.5.4:12379 member list -w table +-----------------+---------+-------+------------------------+-----------------------------------------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | +-----------------+---------+-------+------------------------+-----------------------------------------------+ | c856d92a82ba66a | started | etcd0 | http://172.16.5.4:2380 | http://172.16.5.4:2379,http://172.16.5.4:4001 | +-----------------+---------+-------+------------------------+-----------------------------------------------+ 4. etcdctl get 使用etcdctl {command} --help可以查看具体命令的帮助信息。\n# etcdctl get --help NAME: get - Gets the key or a range of keys USAGE: etcdctl get [options] \u003ckey\u003e [range_end] OPTIONS: --consistency=\"l\"\tLinearizable(l) or Serializable(s) --from-key[=false]\tGet keys that are greater than or equal to the given key using byte compare --keys-only[=false]\tGet only the keys --limit=0\tMaximum number of results --order=\"\"\tOrder of results; ASCEND or DESCEND (ASCEND by default) --prefix[=false]\tGet keys with matching prefix --print-value-only[=false]\tOnly write values when using the \"simple\" output format --rev=0\tSpecify the kv revision --sort-by=\"\"\tSort target; CREATE, KEY, MODIFY, VALUE, or VERSION GLOBAL OPTIONS: --cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --command-timeout=5s\ttimeout for short running command (excluding dial timeout) --debug[=false]\tenable client-side debug logging --dial-timeout=2s\tdial timeout for client connections --endpoints=[127.0.0.1:2379]\tgRPC endpoints --hex[=false]\tprint byte strings as hex encoded strings --insecure-skip-tls-verify[=false]\tskip server certificate verification --insecure-transport[=true]\tdisable transport security for client connections --key=\"\"\tidentify secure client using this TLS key file --user=\"\"\tusername[:password] for authentication (prompt if password is not supplied) -w, --write-out=\"simple\"\tset the output format (fields, json, protobuf, simple, table) 文章参考：\nhttps://coreos.com/etcd/docs/latest/demo.html\n","categories":"","description":"","excerpt":" etcdctl的v3版本与v2版本使用命令有所不同，本文介绍etcdctl v3版本的命令工具的使用方式。\n1. etcdctl …","ref":"/kubernetes-notes/etcd/etcdctl/etcdctl-v3/","tags":["Etcd"],"title":"etcdctl-V3"},{"body":"1. Etcd是什么（what） etcd is a distributed, consistent key-value store for shared configuration and service discovery, with a focus on being:\nSecure: automatic TLS with optional client cert authentication[可选的SSL客户端证书认证：支持https访问 ] Fast: benchmarked 10,000 writes/sec[单实例每秒 1000 次写操作] Reliable: properly distributed using Raft[使用Raft保证一致性] etcd是一个分布式、一致性的键值存储系统，主要用于配置共享和服务发现。[以上内容来自etcd官网]\n2. 为什么使用Etcd（why） 2.1. Etcd的优势 简单。使用Go语言编写部署简单；使用HTTP作为接口使用简单；使用Raft算法保证强一致性让用户易于理解。 数据持久化。etcd默认数据一更新就进行持久化。 安全。etcd支持SSL客户端安全认证。 3. 如何实现Etcd架构（how） 3.1. Etcd的相关名词解释 Raft：etcd所采用的保证分布式系统强一致性的算法。 Node：一个Raft状态机实例。 Member： 一个etcd实例。它管理着一个Node，并且可以为客户端请求提供服务。 Cluster：由多个Member构成可以协同工作的etcd集群。 Peer：对同一个etcd集群中另外一个Member的称呼。 Client： 向etcd集群发送HTTP请求的客户端。 WAL：预写式日志，etcd用于持久化存储的日志格式。 snapshot：etcd防止WAL文件过多而设置的快照，存储etcd数据状态。 Proxy：etcd的一种模式，为etcd集群提供反向代理服务。 Leader：Raft算法中通过竞选而产生的处理所有数据提交的节点。 Follower：竞选失败的节点作为Raft中的从属节点，为算法提供强一致性保证。 Candidate：当Follower超过一定时间接收不到Leader的心跳时转变为Candidate开始竞选。【候选人】 Term：某个节点成为Leader到下一次竞选时间，称为一个Term。【任期】 Index：数据项编号。Raft中通过Term和Index来定位数据。 3.2. Etcd的架构图 一个用户的请求发送过来，会经由HTTP Server转发给Store进行具体的事务处理，如果涉及到节点的修改，则交给Raft模块进行状态的变更、日志的记录，然后再同步给别的etcd节点以确认数据提交，最后进行数据的提交，再次同步。\n1、HTTP Server: 用于处理用户发送的API请求以及其它etcd节点的同步与心跳信息请求。\n2、Raft: Raft强一致性算法的具体实现，是etcd的核心。\n3、WAL: Write Ahead Log（预写式日志），是etcd的数据存储方式，用于系统提供原子性和持久性的一系列技术。除了在内存中存有所有数据的状态以及节点的索引以外，etcd就通过WAL进行持久化存储。WAL中，所有的数据提交前都会事先记录日志。\nEntry[日志内容]:\n负责存储具体日志的内容。\nSnapshot[快照内容]:\nSnapshot是为了防止数据过多而进行的状态快照，日志内容发生变化时保存Raft的状态。\n4、Store: 用于处理etcd支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是etcd对用户提供的大多数API功能的具体实现。\n","categories":"","description":"","excerpt":"1. Etcd是什么（what） etcd is a distributed, consistent key-value store for …","ref":"/kubernetes-notes/etcd/etcd-introduction/","tags":["Etcd"],"title":"Etcd介绍"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/framework/cobra/","tags":"","title":"Cobra命令框架"},{"body":" confd的源码参考：https://github.com/kelseyhightower/confd\n本文分析的confd的版本是v0.16.0，代码参考：https://github.com/kelseyhightower/confd/tree/v0.16.0。\n1. Main confd的入口函数 Main 函数，先解析参数，如果是打印版本信息的参数，则执行打印版本的命令。\nfunc main() { flag.Parse() if config.PrintVersion { fmt.Printf(\"confd %s (Git SHA: %s, Go Version: %s)\\n\", Version, GitSHA, runtime.Version()) os.Exit(0) } ... } 其中版本信息记录在https://github.com/kelseyhightower/confd/blob/v0.16.0/version.go#L3\nconst Version = \"0.16.0\" 1.1. initConfig 初始化配置文件。\nif err := initConfig(); err != nil { log.Fatal(err.Error()) } initConfig函数对基本的配置内容做初始化，当没有指定后端存储的时候，设置默认存储。\n// initConfig initializes the confd configuration by first setting defaults, // then overriding settings from the confd config file, then overriding // settings from environment variables, and finally overriding // settings from flags set on the command line. // It returns an error if any. func initConfig() error { _, err := os.Stat(config.ConfigFile) if os.IsNotExist(err) { log.Debug(\"Skipping confd config file.\") } else { log.Debug(\"Loading \" + config.ConfigFile) configBytes, err := ioutil.ReadFile(config.ConfigFile) if err != nil { return err } _, err = toml.Decode(string(configBytes), \u0026config) if err != nil { return err } } // Update config from environment variables. processEnv() if config.SecretKeyring != \"\" { kr, err := os.Open(config.SecretKeyring) if err != nil { log.Fatal(err.Error()) } defer kr.Close() config.PGPPrivateKey, err = ioutil.ReadAll(kr) if err != nil { log.Fatal(err.Error()) } } if config.LogLevel != \"\" { log.SetLevel(config.LogLevel) } if config.SRVDomain != \"\" \u0026\u0026 config.SRVRecord == \"\" { config.SRVRecord = fmt.Sprintf(\"_%s._tcp.%s.\", config.Backend, config.SRVDomain) } // Update BackendNodes from SRV records. if config.Backend != \"env\" \u0026\u0026 config.SRVRecord != \"\" { log.Info(\"SRV record set to \" + config.SRVRecord) srvNodes, err := getBackendNodesFromSRV(config.SRVRecord) if err != nil { return errors.New(\"Cannot get nodes from SRV records \" + err.Error()) } switch config.Backend { case \"etcd\": vsm := make([]string, len(srvNodes)) for i, v := range srvNodes { vsm[i] = config.Scheme + \"://\" + v } srvNodes = vsm } config.BackendNodes = srvNodes } if len(config.BackendNodes) == 0 { switch config.Backend { case \"consul\": config.BackendNodes = []string{\"127.0.0.1:8500\"} case \"etcd\": peerstr := os.Getenv(\"ETCDCTL_PEERS\") if len(peerstr) \u003e 0 { config.BackendNodes = strings.Split(peerstr, \",\") } else { config.BackendNodes = []string{\"http://127.0.0.1:4001\"} } case \"etcdv3\": config.BackendNodes = []string{\"127.0.0.1:2379\"} case \"redis\": config.BackendNodes = []string{\"127.0.0.1:6379\"} case \"vault\": config.BackendNodes = []string{\"http://127.0.0.1:8200\"} case \"zookeeper\": config.BackendNodes = []string{\"127.0.0.1:2181\"} } } // Initialize the storage client log.Info(\"Backend set to \" + config.Backend) if config.Watch { unsupportedBackends := map[string]bool{ \"dynamodb\": true, \"ssm\": true, } if unsupportedBackends[config.Backend] { log.Info(fmt.Sprintf(\"Watch is not supported for backend %s. Exiting...\", config.Backend)) os.Exit(1) } } if config.Backend == \"dynamodb\" \u0026\u0026 config.Table == \"\" { return errors.New(\"No DynamoDB table configured\") } config.ConfigDir = filepath.Join(config.ConfDir, \"conf.d\") config.TemplateDir = filepath.Join(config.ConfDir, \"templates\") return nil } 1.2. storeClient log.Info(\"Starting confd\") storeClient, err := backends.New(config.BackendsConfig) if err != nil { log.Fatal(err.Error()) } 根据配置文件中的存储后端类型构造一个存储后端的client，其中主要调用的函数为backends.New(config.BackendsConfig)。\n当没有设置存储后端时，默认为etcd。\nif config.Backend == \"\" { config.Backend = \"etcd\" } backendNodes := config.BackendNodes 当存储后端为file类型的处理。\nif config.Backend == \"file\" { log.Info(\"Backend source(s) set to \" + strings.Join(config.YAMLFile, \", \")) } else { log.Info(\"Backend source(s) set to \" + strings.Join(backendNodes, \", \")) } 最后再根据不同类型的存储后端，调用不同的存储后端构建函数，本文只分析redis类型的存储后端。\nswitch config.Backend { case \"consul\": return consul.New(config.BackendNodes, config.Scheme, config.ClientCert, config.ClientKey, config.ClientCaKeys, config.BasicAuth, config.Username, config.Password, ) case \"etcd\": // Create the etcd client upfront and use it for the life of the process. // The etcdClient is an http.Client and designed to be reused. return etcd.NewEtcdClient(backendNodes, config.ClientCert, config.ClientKey, config.ClientCaKeys, config.BasicAuth, config.Username, config.Password) case \"etcdv3\": return etcdv3.NewEtcdClient(backendNodes, config.ClientCert, config.ClientKey, config.ClientCaKeys, config.BasicAuth, config.Username, config.Password) case \"zookeeper\": return zookeeper.NewZookeeperClient(backendNodes) case \"rancher\": return rancher.NewRancherClient(backendNodes) case \"redis\": return redis.NewRedisClient(backendNodes, config.ClientKey, config.Separator) case \"env\": return env.NewEnvClient() case \"file\": return file.NewFileClient(config.YAMLFile, config.Filter) case \"vault\": vaultConfig := map[string]string{ \"app-id\": config.AppID, \"user-id\": config.UserID, \"role-id\": config.RoleID, \"secret-id\": config.SecretID, \"username\": config.Username, \"password\": config.Password, \"token\": config.AuthToken, \"cert\": config.ClientCert, \"key\": config.ClientKey, \"caCert\": config.ClientCaKeys, \"path\": config.Path, } return vault.New(backendNodes[0], config.AuthType, vaultConfig) case \"dynamodb\": table := config.Table log.Info(\"DynamoDB table set to \" + table) return dynamodb.NewDynamoDBClient(table) case \"ssm\": return ssm.New() } return nil, errors.New(\"Invalid backend\") 其中redis类型的存储后端调用了NewRedisClient方法来构造redis的client。\ncase \"redis\": return redis.NewRedisClient(backendNodes, config.ClientKey, config.Separator) 其中涉及三个参数：\nbackendNodes：redis的节点地址。 ClientKey：redis的密码。 Separator：查找redis键的分隔符，该参数只用在redis类型。 NewRedisClient函数方法如下：\n// NewRedisClient returns an *redis.Client with a connection to named machines. // It returns an error if a connection to the cluster cannot be made. func NewRedisClient(machines []string, password string, separator string) (*Client, error) { if separator == \"\" { separator = \"/\" } log.Debug(fmt.Sprintf(\"Redis Separator: %#v\", separator)) var err error clientWrapper := \u0026Client{machines: machines, password: password, separator: separator, client: nil, pscChan: make(chan watchResponse), psc: redis.PubSubConn{Conn: nil} } clientWrapper.client, _, err = tryConnect(machines, password, true) return clientWrapper, err } 1.3. processor stopChan := make(chan bool) doneChan := make(chan bool) errChan := make(chan error, 10) var processor template.Processor switch { case config.Watch: processor = template.WatchProcessor(config.TemplateConfig, stopChan, doneChan, errChan) default: processor = template.IntervalProcessor(config.TemplateConfig, stopChan, doneChan, errChan, config.Interval) } go processor.Process() 当开启watch参数的时候，则构造WatchProcessor，否则构造IntervalProcessor，最后起一个goroutine。\ngo processor.Process() 这块的逻辑在本文第二部分析。\n1.4. signalChan signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM) for { select { case err := \u003c-errChan: log.Error(err.Error()) case s := \u003c-signalChan: log.Info(fmt.Sprintf(\"Captured %v. Exiting...\", s)) close(doneChan) case \u003c-doneChan: os.Exit(0) } } 2. Process type Processor interface { Process() } Processor是一个接口类型，主要的实现体有：\nintervalProcessor：默认的实现体，即没有添加watch参数。 watchProcessor：添加watch参数的实现体。 2.1. intervalProcessor type intervalProcessor struct { config Config stopChan chan bool doneChan chan bool errChan chan error interval int } intervalProcessor根据config内容和几个channel构造一个intervalProcessor。\nfunc IntervalProcessor(config Config, stopChan, doneChan chan bool, errChan chan error, interval int) Processor { return \u0026intervalProcessor{config, stopChan, doneChan, errChan, interval} } 2.1.1. intervalProcessor.Process func (p *intervalProcessor) Process() { defer close(p.doneChan) for { ts, err := getTemplateResources(p.config) if err != nil { log.Fatal(err.Error()) break } process(ts) select { case \u003c-p.stopChan: break case \u003c-time.After(time.Duration(p.interval) * time.Second): continue } } } 通过解析config内容获取TemplateResources，其中核心函数为process(ts)，然后执行t.process()，该函数中会调用t.sync()。t.process()的具体逻辑后文分析。\nfunc process(ts []*TemplateResource) error { var lastErr error for _, t := range ts { if err := t.process(); err != nil { log.Error(err.Error()) lastErr = err } } return lastErr } 2.2. watchProcessor type watchProcessor struct { config Config stopChan chan bool doneChan chan bool errChan chan error wg sync.WaitGroup } watchProcessor根据config内容和几个channel构造一个watchProcessor。\nfunc WatchProcessor(config Config, stopChan, doneChan chan bool, errChan chan error) Processor { var wg sync.WaitGroup return \u0026watchProcessor{config, stopChan, doneChan, errChan, wg} } 2.2.1. watchProcessor.Process func (p *watchProcessor) Process() { defer close(p.doneChan) ts, err := getTemplateResources(p.config) if err != nil { log.Fatal(err.Error()) return } for _, t := range ts { t := t p.wg.Add(1) go p.monitorPrefix(t) } p.wg.Wait() } watchProcessor.Process方法实现了Processor接口中定义的方法，通过解析config内容获取TemplateResources，再遍历TemplateResources执行monitorPrefix，有多少个TemplateResources就运行多少个monitorPrefix的goroutine。\n2.2.2. monitorPrefix func (p *watchProcessor) monitorPrefix(t *TemplateResource) { defer p.wg.Done() keys := util.AppendPrefix(t.Prefix, t.Keys) for { index, err := t.storeClient.WatchPrefix(t.Prefix, keys, t.lastIndex, p.stopChan) if err != nil { p.errChan \u003c- err // Prevent backend errors from consuming all resources. time.Sleep(time.Second * 2) continue } t.lastIndex = index if err := t.process(); err != nil { p.errChan \u003c- err } } } 先对配置文件中的prefix和keys参数进行拼接。\nkeys := util.AppendPrefix(t.Prefix, t.Keys) AppendPrefix函数如下：\nfunc AppendPrefix(prefix string, keys []string) []string { s := make([]string, len(keys)) for i, k := range keys { s[i] = path.Join(prefix, k) } return s } 接着再执行storeClient的WatchPrefix方法，因为storeClient是一个接口，对应不同类型的存储后端，WatchPrefix的实现逻辑也不同，本文分析的存储类型为redis。\nindex, err := t.storeClient.WatchPrefix(t.Prefix, keys, t.lastIndex, p.stopChan) if err != nil { p.errChan \u003c- err // Prevent backend errors from consuming all resources. time.Sleep(time.Second * 2) continue } storeClient.WatchPrefix主要是获取lastIndex的值，这个值在t.process()中使用。\nt.lastIndex = index if err := t.process(); err != nil { p.errChan \u003c- err } 2.3. TemplateResource.process 无论是否加watch参数，即intervalProcessor和watchProcessor最终都会调用到TemplateResource.process这个函数，而这个函数中的核心函数为t.sync()。\n// process is a convenience function that wraps calls to the three main tasks // required to keep local configuration files in sync. First we gather vars // from the store, then we stage a candidate configuration file, and finally sync // things up. // It returns an error if any. func (t *TemplateResource) process() error { if err := t.setFileMode(); err != nil { return err } if err := t.setVars(); err != nil { return err } if err := t.createStageFile(); err != nil { return err } if err := t.sync(); err != nil { return err } return nil } 2.3.1. setFileMode setFileMode设置文件的权限，如果没有在配置文件指定mode参数则默认为0644，否则根据配置文件中指定的mode来设置文件权限。\n// setFileMode sets the FileMode. func (t *TemplateResource) setFileMode() error { if t.Mode == \"\" { if !util.IsFileExist(t.Dest) { t.FileMode = 0644 } else { fi, err := os.Stat(t.Dest) if err != nil { return err } t.FileMode = fi.Mode() } } else { mode, err := strconv.ParseUint(t.Mode, 0, 32) if err != nil { return err } t.FileMode = os.FileMode(mode) } return nil } 2.3.2. setVars setVars将后端存储中最新的值拿出来暂存到内存中供后续进程使用。其中根据不同的后端，storeClient.GetValues的逻辑可能不同，但通过接口的方式可以让不同的存储后端实现不同的获取值的方法。\n// setVars sets the Vars for template resource. func (t *TemplateResource) setVars() error { var err error log.Debug(\"Retrieving keys from store\") log.Debug(\"Key prefix set to \" + t.Prefix) result, err := t.storeClient.GetValues(util.AppendPrefix(t.Prefix, t.Keys)) if err != nil { return err } log.Debug(\"Got the following map from store: %v\", result) t.store.Purge() for k, v := range result { t.store.Set(path.Join(\"/\", strings.TrimPrefix(k, t.Prefix)), v) } return nil } 2.3.3. createStageFile createStageFile通过src的template文件和最新内存中的变量数据生成StageFile，该文件在sync中和目标文件进行比较，看是否有修改。即StageFile实际上是根据后端存储生成的最新的配置文件，如果这份配置文件跟当前的配置文件不同，表明后端存储的数据被更新了需要重新生成一份新的配置文件。\n// createStageFile stages the src configuration file by processing the src // template and setting the desired owner, group, and mode. It also sets the // StageFile for the template resource. // It returns an error if any. func (t *TemplateResource) createStageFile() error { log.Debug(\"Using source template \" + t.Src) if !util.IsFileExist(t.Src) { return errors.New(\"Missing template: \" + t.Src) } log.Debug(\"Compiling source template \" + t.Src) tmpl, err := template.New(filepath.Base(t.Src)).Funcs(t.funcMap).ParseFiles(t.Src) if err != nil { return fmt.Errorf(\"Unable to process template %s, %s\", t.Src, err) } // create TempFile in Dest directory to avoid cross-filesystem issues temp, err := ioutil.TempFile(filepath.Dir(t.Dest), \".\"+filepath.Base(t.Dest)) if err != nil { return err } if err = tmpl.Execute(temp, nil); err != nil { temp.Close() os.Remove(temp.Name()) return err } defer temp.Close() // Set the owner, group, and mode on the stage file now to make it easier to // compare against the destination configuration file later. os.Chmod(temp.Name(), t.FileMode) os.Chown(temp.Name(), t.Uid, t.Gid) t.StageFile = temp return nil } 2.3.4. sync if err := t.sync(); err != nil { return err } t.sync()是执行confd核心功能的函数，将配置文件通过模板的方式自动生成，并执行检查命令和reload命令。该部分逻辑在本文第三部分分析。\n3. sync sync通过比较源文件和目标文件的差别，如果不同则重新生成新的配置，当设置了check_cmd和reload_cmd的时候，会执行check_cmd指定的检查命令，如果都没有问题则执行reload_cmd中指定的reload命令。\n3.1. IsConfigChanged IsConfigChanged比较源文件和目标文件是否相等，其中比较内容包括：Uid、Gid、Mode、Md5。只要其中任意值不同则认为两个文件不同。\n// IsConfigChanged reports whether src and dest config files are equal. // Two config files are equal when they have the same file contents and // Unix permissions. The owner, group, and mode must match. // It return false in other cases. func IsConfigChanged(src, dest string) (bool, error) { if !IsFileExist(dest) { return true, nil } d, err := FileStat(dest) if err != nil { return true, err } s, err := FileStat(src) if err != nil { return true, err } if d.Uid != s.Uid { log.Info(fmt.Sprintf(\"%s has UID %d should be %d\", dest, d.Uid, s.Uid)) } if d.Gid != s.Gid { log.Info(fmt.Sprintf(\"%s has GID %d should be %d\", dest, d.Gid, s.Gid)) } if d.Mode != s.Mode { log.Info(fmt.Sprintf(\"%s has mode %s should be %s\", dest, os.FileMode(d.Mode), os.FileMode(s.Mode))) } if d.Md5 != s.Md5 { log.Info(fmt.Sprintf(\"%s has md5sum %s should be %s\", dest, d.Md5, s.Md5)) } if d.Uid != s.Uid || d.Gid != s.Gid || d.Mode != s.Mode || d.Md5 != s.Md5 { return true, nil } return false, nil } 如果文件发生改变则执行check_cmd命令（有配置的情况下），重新生成配置文件，并执行reload_cmd命令（有配置的情况下）。\nif ok { log.Info(\"Target config \" + t.Dest + \" out of sync\") if !t.syncOnly \u0026\u0026 t.CheckCmd != \"\" { if err := t.check(); err != nil { return errors.New(\"Config check failed: \" + err.Error()) } } log.Debug(\"Overwriting target config \" + t.Dest) err := os.Rename(staged, t.Dest) if err != nil { if strings.Contains(err.Error(), \"device or resource busy\") { log.Debug(\"Rename failed - target is likely a mount. Trying to write instead\") // try to open the file and write to it var contents []byte var rerr error contents, rerr = ioutil.ReadFile(staged) if rerr != nil { return rerr } err := ioutil.WriteFile(t.Dest, contents, t.FileMode) // make sure owner and group match the temp file, in case the file was created with WriteFile os.Chown(t.Dest, t.Uid, t.Gid) if err != nil { return err } } else { return err } } if !t.syncOnly \u0026\u0026 t.ReloadCmd != \"\" { if err := t.reload(); err != nil { return err } } log.Info(\"Target config \" + t.Dest + \" has been updated\") } else { log.Debug(\"Target config \" + t.Dest + \" in sync\") } 3.2. check check检查暂存的配置文件即stageFile，该文件是由最新的后端存储中的数据生成的。\nif !t.syncOnly \u0026\u0026 t.CheckCmd != \"\" { if err := t.check(); err != nil { return errors.New(\"Config check failed: \" + err.Error()) } } t.check()只是执行配置文件中checkcmd参数指定的命令而已，根据是否执行成功来返回报错。当check命令产生错误的是，则直接return报错，不再执行重新生成配置文件和``reload`的操作了。\n// check executes the check command to validate the staged config file. The // command is modified so that any references to src template are substituted // with a string representing the full path of the staged file. This allows the // check to be run on the staged file before overwriting the destination config // file. // It returns nil if the check command returns 0 and there are no other errors. func (t *TemplateResource) check() error { var cmdBuffer bytes.Buffer data := make(map[string]string) data[\"src\"] = t.StageFile.Name() tmpl, err := template.New(\"checkcmd\").Parse(t.CheckCmd) if err != nil { return err } if err := tmpl.Execute(\u0026cmdBuffer, data); err != nil { return err } return runCommand(cmdBuffer.String()) } check会通过模板解析的方式解析出checkcmd中的{{.src}}部分，并用stageFile来替代。即check的命令是拉取最新后端存储的数据形成临时配置文件（stageFile），并通过指定的checkcmd来检查最新的临时配置文件是否合法，如果合法则替换会新的配置文件，否则返回错误。\n3.3. Overwriting 将staged文件命名为Dest文件的名字，读取staged文件中的内容并将它写入到Dest文件中，该过程实际上就是重新生成一份新的配置文件。staged文件的生成逻辑在函数createStageFile中。\nlog.Debug(\"Overwriting target config \" + t.Dest) err := os.Rename(staged, t.Dest) if err != nil { if strings.Contains(err.Error(), \"device or resource busy\") { log.Debug(\"Rename failed - target is likely a mount. Trying to write instead\") // try to open the file and write to it var contents []byte var rerr error contents, rerr = ioutil.ReadFile(staged) if rerr != nil { return rerr } err := ioutil.WriteFile(t.Dest, contents, t.FileMode) // make sure owner and group match the temp file, in case the file was created with WriteFile os.Chown(t.Dest, t.Uid, t.Gid) if err != nil { return err } } else { return err } } 3.4. reload 如果没有指定syncOnly参数并且指定了ReloadCmd则执行reload操作。\nif !t.syncOnly \u0026\u0026 t.ReloadCmd != \"\" { if err := t.reload(); err != nil { return err } } 其中t.reload()实现如下：\n// reload executes the reload command. // It returns nil if the reload command returns 0. func (t *TemplateResource) reload() error { return runCommand(t.ReloadCmd) } t.reload()和t.check()都调用了runCommand函数：\n// runCommand is a shared function used by check and reload // to run the given command and log its output. // It returns nil if the given cmd returns 0. // The command can be run on unix and windows. func runCommand(cmd string) error { log.Debug(\"Running \" + cmd) var c *exec.Cmd if runtime.GOOS == \"windows\" { c = exec.Command(\"cmd\", \"/C\", cmd) } else { c = exec.Command(\"/bin/sh\", \"-c\", cmd) } output, err := c.CombinedOutput() if err != nil { log.Error(fmt.Sprintf(\"%q\", string(output))) return err } log.Debug(fmt.Sprintf(\"%q\", string(output))) return nil } 4. redisClient.WatchPrefix redisClient.WatchPrefix是当用户设置了watch参数的时候，并且存储后端为redis，则会调用到redis的watch机制。其中redisClient.WatchPrefix是redis存储类型的时候实现了StoreClient接口的WatchPrefix方法。\n// The StoreClient interface is implemented by objects that can retrieve // key/value pairs from a backend store. type StoreClient interface { GetValues(keys []string) (map[string]string, error) WatchPrefix(prefix string, keys []string, waitIndex uint64, stopChan chan bool) (uint64, error) } StoreClient是对后端存储类型的抽象，常用的后端存储类型有Etcd和Redis等，不同的后端存储类型GetValues和WatchPrefix的具体实现不同，本文主要分析Redis类型的watch机制。\n4.1. WatchPrefix WatchPrefix的调用函数在monitorPrefix的部分，具体参考：\nfunc (p *watchProcessor) monitorPrefix(t *TemplateResource) { defer p.wg.Done() keys := util.AppendPrefix(t.Prefix, t.Keys) for { index, err := t.storeClient.WatchPrefix(t.Prefix, keys, t.lastIndex, p.stopChan) if err != nil { p.errChan \u003c- err // Prevent backend errors from consuming all resources. time.Sleep(time.Second * 2) continue } t.lastIndex = index if err := t.process(); err != nil { p.errChan \u003c- err } } } redis的watch主要通过pub-sub的机制，即WatchPrefix会根据传入的prefix起一个sub的监听机制，而在写入redis的数据的同时需要执行redis的publish操作，channel为符合prefix的值，value为给定命令之一，实际上是给定命令之一，具体是什么命令并没有关系，则会触发watch机制，从而自动更新配置，给定的命令列表如下：\n\"del\", \"append\", \"rename_from\", \"rename_to\", \"expire\", \"set\", \"incrby\", \"incrbyfloat\", \"hset\", \"hincrby\", \"hincrbyfloat\", \"hdel\" sub监听的key的格式如下：\n__keyspace@0__:{prefix}/* 如果只是写入redis数据而没有自动执行publish的操作，并不会触发redis的watch机制来自动更新配置。但是如果使用etcd，则etcd的watch机制，只需要用户写入或更新数据就可以自动触发更新配置。\nWatchPrefix源码如下：\nfunc (c *Client) WatchPrefix(prefix string, keys []string, waitIndex uint64, stopChan chan bool) (uint64, error) { if waitIndex == 0 { return 1, nil } if len(c.pscChan) \u003e 0 { var respChan watchResponse for len(c.pscChan) \u003e 0 { respChan = \u003c-c.pscChan } return respChan.waitIndex, respChan.err } go func() { if c.psc.Conn == nil { rClient, db, err := tryConnect(c.machines, c.password, false); if err != nil { c.psc = redis.PubSubConn{Conn: nil} c.pscChan \u003c- watchResponse{0, err} return } c.psc = redis.PubSubConn{Conn: rClient}\tgo func() { defer func() { c.psc.Close() c.psc = redis.PubSubConn{Conn: nil} }() for { switch n := c.psc.Receive().(type) { case redis.PMessage: log.Debug(fmt.Sprintf(\"Redis Message: %s %s\\n\", n.Channel, n.Data)) data := string(n.Data) commands := [12]string{\"del\", \"append\", \"rename_from\", \"rename_to\", \"expire\", \"set\", \"incrby\", \"incrbyfloat\", \"hset\", \"hincrby\", \"hincrbyfloat\", \"hdel\"} for _, command := range commands { if command == data { c.pscChan \u003c- watchResponse{1, nil} break } } case redis.Subscription: log.Debug(fmt.Sprintf(\"Redis Subscription: %s %s %d\\n\", n.Kind, n.Channel, n.Count)) if n.Count == 0 { c.pscChan \u003c- watchResponse{0, nil} return } case error: log.Debug(fmt.Sprintf(\"Redis error: %v\\n\", n)) c.pscChan \u003c- watchResponse{0, n} return } } }() c.psc.PSubscribe(\"__keyspace@\" + strconv.Itoa(db) + \"__:\" + c.transform(prefix) + \"*\") } }() select { case \u003c-stopChan: c.psc.PUnsubscribe() return waitIndex, nil case r := \u003c- c.pscChan: return r.waitIndex, r.err } } 5. 总结 confd的作用是通过将配置存放到存储后端，来自动触发更新配置的功能，其中常用的后端有Etcd和Redis等。 不同的存储后端，watch机制不同，例如Etcd只需要更新key便可以触发自动更新配置的操作，而redis除了更新key还需要执行publish的操作。 可以通过配置check_cmd来校验配置文件是否正确，如果配置文件非法则不会执行自动更新配置和reload的操作，但是当存储后端存入的非法数据，会导致每次校验都是失败的，即使后面新增的配置部分是合法的，所以需要有机制来控制存入存储后端的数据始终是合法的。 参考：\nhttps://github.com/kelseyhightower/confd/tree/v0.16.0 ","categories":"","description":"","excerpt":" confd的源码参考：https://github.com/kelseyhightower/confd\n本文分析的confd的版本 …","ref":"/golang-notes/code/confd-code-analysis/","tags":["Golang"],"title":"confd源码分析"},{"body":"1. 编译CSI CephFS plugin CSI CephFS plugin用来提供CephFS存储卷和挂载存储卷，源码参考：https://github.com/ceph/ceph-csi 。\n1.1. 编译二进制 $ make cephfsplugin 1.2. 编译Docker镜像 $ make image-cephfsplugin 2. 配置项 2.1. 命令行参数 Option Default value Description --endpoint unix://tmp/csi.sock CSI endpoint, must be a UNIX socket --drivername csi-cephfsplugin name of the driver (Kubernetes: provisioner field in StorageClass must correspond to this value) --nodeid empty This node’s ID --volumemounter empty default volume mounter. Available options are kernel and fuse. This is the mount method used if volume parameters don’t specify otherwise. If left unspecified, the driver will first probe for ceph-fuse in system’s path and will choose Ceph kernel client if probing failed. 2.2. volume参数 Parameter Required Description monitors yes Comma separated list of Ceph monitors (e.g. 192.168.100.1:6789,192.168.100.2:6789,192.168.100.3:6789) mounter no Mount method to be used for this volume. Available options are kernel for Ceph kernel client and fuse for Ceph FUSE driver. Defaults to “default mounter”, see command line arguments. provisionVolume yes Mode of operation. BOOL value. If true, a new CephFS volume will be provisioned. If false, an existing CephFS will be used. pool for provisionVolume=true Ceph pool into which the volume shall be created rootPath for provisionVolume=false Root path of an existing CephFS volume csiProvisionerSecretName, csiNodeStageSecretName for Kubernetes name of the Kubernetes Secret object containing Ceph client credentials. Both parameters should have the same value csiProvisionerSecretNamespace, csiNodeStageSecretNamespace for Kubernetes namespaces of the above Secret objects 2.3. provisionVolume 2.3.1. 管理员密钥认证 当provisionVolume=true时，必要的管理员认证参数如下：\nadminID: ID of an admin client adminKey: key of the admin client 2.3.2. 普通用户密钥认证 当provisionVolume=false时，必要的用户认证参数如下：\nuserID: ID of a user client userKey: key of a user client 参考文章：\nhttps://github.com/ceph/ceph-csi/blob/master/docs/deploy-cephfs.md ","categories":"","description":"","excerpt":"1. 编译CSI CephFS plugin CSI CephFS plugin用来提供CephFS存储卷和挂载存储卷，源码参 …","ref":"/kubernetes-notes/storage/csi/ceph/csi-cephfs-plugin/","tags":["CSI"],"title":"csi-cephfs-plugin"},{"body":" 本文主要分析csi-provisioner的源码，关于开发一个Dynamic Provisioner，具体可参考nfs-client-provisioner的源码分析\n1. Dynamic Provisioner 1.1. Provisioner Interface 开发Dynamic Provisioner需要实现Provisioner接口，该接口有两个方法，分别是：\nProvision：创建存储资源，并且返回一个PV对象。 Delete：移除对应的存储资源，但并没有删除PV对象。 1.2. 开发provisioner的步骤 写一个provisioner实现Provisioner接口（包含Provision和Delete的方法）。 通过该provisioner构建ProvisionController。 执行ProvisionController的Run方法。 2. CSI Provisioner CSI Provisioner的源码可参考：https://github.com/kubernetes-csi/external-provisioner。\n2.1. Main 函数 2.1.1. 读取环境变量 源码如下：\nvar ( provisioner = flag.String(\"provisioner\", \"\", \"Name of the provisioner. The provisioner will only provision volumes for claims that request a StorageClass with a provisioner field set equal to this name.\") master = flag.String(\"master\", \"\", \"Master URL to build a client config from. Either this or kubeconfig needs to be set if the provisioner is being run out of cluster.\") kubeconfig = flag.String(\"kubeconfig\", \"\", \"Absolute path to the kubeconfig file. Either this or master needs to be set if the provisioner is being run out of cluster.\") csiEndpoint = flag.String(\"csi-address\", \"/run/csi/socket\", \"The gRPC endpoint for Target CSI Volume\") connectionTimeout = flag.Duration(\"connection-timeout\", 10*time.Second, \"Timeout for waiting for CSI driver socket.\") volumeNamePrefix = flag.String(\"volume-name-prefix\", \"pvc\", \"Prefix to apply to the name of a created volume\") volumeNameUUIDLength = flag.Int(\"volume-name-uuid-length\", -1, \"Truncates generated UUID of a created volume to this length. Defaults behavior is to NOT truncate.\") showVersion = flag.Bool(\"version\", false, \"Show version.\") provisionController *controller.ProvisionController version = \"unknown\" ) func init() { var config *rest.Config var err error flag.Parse() flag.Set(\"logtostderr\", \"true\") if *showVersion { fmt.Println(os.Args[0], version) os.Exit(0) } glog.Infof(\"Version: %s\", version) ...\t}\t通过init函数解析相关参数，其实provisioner指明为PVC提供PV的provisioner的名字，需要和StorageClass对象中的provisioner字段一致。\n2.1.2. 获取clientset对象 源码如下：\n// get the KUBECONFIG from env if specified (useful for local/debug cluster) kubeconfigEnv := os.Getenv(\"KUBECONFIG\") if kubeconfigEnv != \"\" { glog.Infof(\"Found KUBECONFIG environment variable set, using that..\") kubeconfig = \u0026kubeconfigEnv } if *master != \"\" || *kubeconfig != \"\" { glog.Infof(\"Either master or kubeconfig specified. building kube config from that..\") config, err = clientcmd.BuildConfigFromFlags(*master, *kubeconfig) } else { glog.Infof(\"Building kube configs for running in cluster...\") config, err = rest.InClusterConfig() } if err != nil { glog.Fatalf(\"Failed to create config: %v\", err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create client: %v\", err) } // snapclientset.NewForConfig creates a new Clientset for VolumesnapshotV1alpha1Client snapClient, err := snapclientset.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create snapshot client: %v\", err) } csiAPIClient, err := csiclientset.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create CSI API client: %v\", err) } 通过读取对应的k8s的配置，创建clientset对象，用来执行k8s对应的API，其中主要包括对PV和PVC等对象的创建删除等操作。\n2.1.3. k8s版本校验 // The controller needs to know what the server version is because out-of-tree // provisioners aren't officially supported until 1.5 serverVersion, err := clientset.Discovery().ServerVersion() if err != nil { glog.Fatalf(\"Error getting server version: %v\", err) } 获取了k8s的版本信息，因为provisioners的功能在k8s 1.5及以上版本才支持。\n2.1.4. 连接 csi socket // Generate a unique ID for this provisioner timeStamp := time.Now().UnixNano() / int64(time.Millisecond) identity := strconv.FormatInt(timeStamp, 10) + \"-\" + strconv.Itoa(rand.Intn(10000)) + \"-\" + *provisioner // Provisioner will stay in Init until driver opens csi socket, once it's done // controller will exit this loop and proceed normally. socketDown := true grpcClient := \u0026grpc.ClientConn{} for socketDown { grpcClient, err = ctrl.Connect(*csiEndpoint, *connectionTimeout) if err == nil { socketDown = false continue } time.Sleep(10 * time.Second) } 在Provisioner会停留在初始化状态，直到csi socket连接成功才正常运行。如果连接失败，会暂停10秒后重试，其中涉及以下2个参数：\ncsiEndpoint：CSI Volume的gRPC地址，默认通过为/run/csi/socket。 connectionTimeout：连接CSI driver socket的超时时间，默认为10秒。 2.1.5. 构造csi-Provisioner对象 // Create the provisioner: it implements the Provisioner interface expected by // the controller csiProvisioner := ctrl.NewCSIProvisioner(clientset, csiAPIClient, *csiEndpoint, *connectionTimeout, identity, *volumeNamePrefix, *volumeNameUUIDLength, grpcClient, snapClient) provisionController = controller.NewProvisionController( clientset, *provisioner, csiProvisioner, serverVersion.GitVersion, ) 通过参数clientset, csiAPIClient, csiEndpoint, connectionTimeout, identity, volumeNamePrefix, volumeNameUUIDLength, grpcClient, snapClient构造csi-Provisioner对象。\n通过csiProvisioner构造ProvisionController对象。\n2.1.6. 运行ProvisionController func main() { provisionController.Run(wait.NeverStop) } ProvisionController实现了具体的PV和PVC的相关逻辑，Run方法以常驻进程的方式运行。\n2.2. Provision和Delete方法 2.2.1. Provision方法 csiProvisioner的Provision方法具体源码参考：https://github.com/kubernetes-csi/external-provisioner/blob/master/pkg/controller/controller.go#L336\nProvision方法用来创建存储资源，并且返回一个PV对象。其中入参是VolumeOptions，用来指定PV对象的相关属性。\n1、构造PV相关属性\npvName, err := makeVolumeName(p.volumeNamePrefix, fmt.Sprintf(\"%s\", options.PVC.ObjectMeta.UID), p.volumeNameUUIDLength) if err != nil { return nil, err } 2、构造CSIPersistentVolumeSource相关属性\ndriverState, err := checkDriverState(p.grpcClient, p.timeout, needSnapshotSupport) if err != nil { return nil, err } ... // Resolve controller publish, node stage, node publish secret references controllerPublishSecretRef, err := getSecretReference(controllerPublishSecretNameKey, controllerPublishSecretNamespaceKey, options.Parameters, pvName, options.PVC) if err != nil { return nil, err } nodeStageSecretRef, err := getSecretReference(nodeStageSecretNameKey, nodeStageSecretNamespaceKey, options.Parameters, pvName, options.PVC) if err != nil { return nil, err } nodePublishSecretRef, err := getSecretReference(nodePublishSecretNameKey, nodePublishSecretNamespaceKey, options.Parameters, pvName, options.PVC) if err != nil { return nil, err } ... volumeAttributes := map[string]string{provisionerIDKey: p.identity} for k, v := range rep.Volume.Attributes { volumeAttributes[k] = v } ... fsType := \"\" for k, v := range options.Parameters { switch strings.ToLower(k) { case \"fstype\": fsType = v } } if len(fsType) == 0 { fsType = defaultFSType } 3、创建CSI CreateVolumeRequest\n// Create a CSI CreateVolumeRequest and Response req := csi.CreateVolumeRequest{ Name: pvName, Parameters: options.Parameters, VolumeCapabilities: volumeCaps, CapacityRange: \u0026csi.CapacityRange{ RequiredBytes: int64(volSizeBytes), }, } ... glog.V(5).Infof(\"CreateVolumeRequest %+v\", req) rep := \u0026csi.CreateVolumeResponse{} ... opts := wait.Backoff{Duration: backoffDuration, Factor: backoffFactor, Steps: backoffSteps} err = wait.ExponentialBackoff(opts, func() (bool, error) { ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() rep, err = p.csiClient.CreateVolume(ctx, \u0026req) if err == nil { // CreateVolume has finished successfully return true, nil } if status, ok := status.FromError(err); ok { if status.Code() == codes.DeadlineExceeded { // CreateVolume timed out, give it another chance to complete glog.Warningf(\"CreateVolume timeout: %s has expired, operation will be retried\", p.timeout.String()) return false, nil } } // CreateVolume failed , no reason to retry, bailing from ExponentialBackoff return false, err }) if err != nil { return nil, err } if rep.Volume != nil { glog.V(3).Infof(\"create volume rep: %+v\", *rep.Volume) } respCap := rep.GetVolume().GetCapacityBytes() if respCap \u003c volSizeBytes { capErr := fmt.Errorf(\"created volume capacity %v less than requested capacity %v\", respCap, volSizeBytes) delReq := \u0026csi.DeleteVolumeRequest{ VolumeId: rep.GetVolume().GetId(), } delReq.ControllerDeleteSecrets = provisionerCredentials ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() _, err := p.csiClient.DeleteVolume(ctx, delReq) if err != nil { capErr = fmt.Errorf(\"%v. Cleanup of volume %s failed, volume is orphaned: %v\", capErr, pvName, err) } return nil, capErr } Provison方法核心功能是调用p.csiClient.CreateVolume(ctx, \u0026req)。\n4、构造PV对象\npv := \u0026v1.PersistentVolume{ ObjectMeta: metav1.ObjectMeta{ Name: pvName, }, Spec: v1.PersistentVolumeSpec{ PersistentVolumeReclaimPolicy: options.PersistentVolumeReclaimPolicy, AccessModes: options.PVC.Spec.AccessModes, Capacity: v1.ResourceList{ v1.ResourceName(v1.ResourceStorage): bytesToGiQuantity(respCap), }, // TODO wait for CSI VolumeSource API PersistentVolumeSource: v1.PersistentVolumeSource{ CSI: \u0026v1.CSIPersistentVolumeSource{ Driver: driverState.driverName, VolumeHandle: p.volumeIdToHandle(rep.Volume.Id), FSType: fsType, VolumeAttributes: volumeAttributes, ControllerPublishSecretRef: controllerPublishSecretRef, NodeStageSecretRef: nodeStageSecretRef, NodePublishSecretRef: nodePublishSecretRef, }, }, }, } if driverState.capabilities.Has(PluginCapability_ACCESSIBILITY_CONSTRAINTS) { pv.Spec.NodeAffinity = GenerateVolumeNodeAffinity(rep.Volume.AccessibleTopology) } glog.Infof(\"successfully created PV %+v\", pv.Spec.PersistentVolumeSource) return pv, nil Provision方法只是通过VolumeOptions参数来构建PV对象，并没有执行具体PV的创建或删除的操作。\n不同类型的Provisioner的，一般是PersistentVolumeSource类型和参数不同，例如csi-provisioner对应的PersistentVolumeSource为CSI，并且需要传入CSI相关的参数：\nDriver VolumeHandle FSType VolumeAttributes ControllerPublishSecretRef NodeStageSecretRef NodePublishSecretRef 2.2.2. Delete方法 csiProvisioner的delete方法具体源码参考：https://github.com/kubernetes-csi/external-provisioner/blob/master/pkg/controller/controller.go#L606\nfunc (p *csiProvisioner) Delete(volume *v1.PersistentVolume) error { if volume == nil || volume.Spec.CSI == nil { return fmt.Errorf(\"invalid CSI PV\") } volumeId := p.volumeHandleToId(volume.Spec.CSI.VolumeHandle) _, err := checkDriverState(p.grpcClient, p.timeout, false) if err != nil { return err } req := csi.DeleteVolumeRequest{ VolumeId: volumeId, } // get secrets if StorageClass specifies it storageClassName := volume.Spec.StorageClassName if len(storageClassName) != 0 { if storageClass, err := p.client.StorageV1().StorageClasses().Get(storageClassName, metav1.GetOptions{}); err == nil { // Resolve provision secret credentials. // No PVC is provided when resolving provision/delete secret names, since the PVC may or may not exist at delete time. provisionerSecretRef, err := getSecretReference(provisionerSecretNameKey, provisionerSecretNamespaceKey, storageClass.Parameters, volume.Name, nil) if err != nil { return err } credentials, err := getCredentials(p.client, provisionerSecretRef) if err != nil { return err } req.ControllerDeleteSecrets = credentials } } ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() _, err = p.csiClient.DeleteVolume(ctx, \u0026req) return err } Delete方法主要是调用了p.csiClient.DeleteVolume(ctx, \u0026req)方法。\n2.3. 总结 csi provisioner实现了Provisioner接口，其中包含Provison和Delete两个方法:\nProvision：调用csiClient.CreateVolume方法，同时构造并返回PV对象。 Delete：调用csiClient.DeleteVolume方法。 csi provisioner的核心方法都调用了csi-client相关方法。\n3. csi-client csi client的相关代码参考：https://github.com/container-storage-interface/spec/blob/master/lib/go/csi/v0/csi.pb.go\n3.1. 构造csi-client 3.1.1. 构造grpcClient // Provisioner will stay in Init until driver opens csi socket, once it's done // controller will exit this loop and proceed normally. socketDown := true grpcClient := \u0026grpc.ClientConn{} for socketDown { grpcClient, err = ctrl.Connect(*csiEndpoint, *connectionTimeout) if err == nil { socketDown = false continue } time.Sleep(10 * time.Second) } 通过连接csi socket，连接成功才构造可用的grpcClient。\n3.1.2. 构造csi-client 通过grpcClient构造csi-client。\n// Create the provisioner: it implements the Provisioner interface expected by // the controller csiProvisioner := ctrl.NewCSIProvisioner(clientset, csiAPIClient, *csiEndpoint, *connectionTimeout, identity, *volumeNamePrefix, *volumeNameUUIDLength, grpcClient, snapClient) NewCSIProvisioner\n// NewCSIProvisioner creates new CSI provisioner func NewCSIProvisioner(client kubernetes.Interface, csiAPIClient csiclientset.Interface, csiEndpoint string, connectionTimeout time.Duration, identity string, volumeNamePrefix string, volumeNameUUIDLength int, grpcClient *grpc.ClientConn, snapshotClient snapclientset.Interface) controller.Provisioner { csiClient := csi.NewControllerClient(grpcClient) provisioner := \u0026csiProvisioner{ client: client, grpcClient: grpcClient, csiClient: csiClient, csiAPIClient: csiAPIClient, snapshotClient: snapshotClient, timeout: connectionTimeout, identity: identity, volumeNamePrefix: volumeNamePrefix, volumeNameUUIDLength: volumeNameUUIDLength, } return provisioner } NewControllerClient\ncsiClient := csi.NewControllerClient(grpcClient) ... type controllerClient struct { cc *grpc.ClientConn } func NewControllerClient(cc *grpc.ClientConn) ControllerClient { return \u0026controllerClient{cc} } 3.2. csiClient.CreateVolume csi provisoner中调用csiClient.CreateVolume代码如下：\nopts := wait.Backoff{Duration: backoffDuration, Factor: backoffFactor, Steps: backoffSteps} err = wait.ExponentialBackoff(opts, func() (bool, error) { ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() rep, err = p.csiClient.CreateVolume(ctx, \u0026req) if err == nil { // CreateVolume has finished successfully return true, nil } if status, ok := status.FromError(err); ok { if status.Code() == codes.DeadlineExceeded { // CreateVolume timed out, give it another chance to complete glog.Warningf(\"CreateVolume timeout: %s has expired, operation will be retried\", p.timeout.String()) return false, nil } } // CreateVolume failed , no reason to retry, bailing from ExponentialBackoff return false, err }) CreateVolumeRequest的构造：\n// Create a CSI CreateVolumeRequest and Response req := csi.CreateVolumeRequest{ Name: pvName, Parameters: options.Parameters, VolumeCapabilities: volumeCaps, CapacityRange: \u0026csi.CapacityRange{ RequiredBytes: int64(volSizeBytes), }, } ... req.VolumeContentSource = volumeContentSource ... req.AccessibilityRequirements = requirements ... req.ControllerCreateSecrets = provisionerCredentials 具体的Create实现方法如下：\n其中csiClient是个接口类型\n具体代码参考controllerClient.CreateVolume\nfunc (c *controllerClient) CreateVolume(ctx context.Context, in *CreateVolumeRequest, opts ...grpc.CallOption) (*CreateVolumeResponse, error) { out := new(CreateVolumeResponse) err := grpc.Invoke(ctx, \"/csi.v0.Controller/CreateVolume\", in, out, c.cc, opts...) if err != nil { return nil, err } return out, nil } 3.3. csiClient.DeleteVolume csi provisoner中调用csiClient.DeleteVolume代码如下：\nfunc (p *csiProvisioner) Delete(volume *v1.PersistentVolume) error { ... req := csi.DeleteVolumeRequest{ VolumeId: volumeId, } // get secrets if StorageClass specifies it ... ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() _, err = p.csiClient.DeleteVolume(ctx, \u0026req) return err } DeleteVolumeRequest的构造：\nreq := csi.DeleteVolumeRequest{ VolumeId: volumeId, } ... req.ControllerDeleteSecrets = credentials 将构造的DeleteVolumeRequest传给DeleteVolume方法。\n具体的Delete实现方法如下：\n具体代码参考：controllerClient.DeleteVolume\nfunc (c *controllerClient) DeleteVolume(ctx context.Context, in *DeleteVolumeRequest, opts ...grpc.CallOption) (*DeleteVolumeResponse, error) { out := new(DeleteVolumeResponse) err := grpc.Invoke(ctx, \"/csi.v0.Controller/DeleteVolume\", in, out, c.cc, opts...) if err != nil { return nil, err } return out, nil } 4. ProvisionController.Run 自定义的provisioner实现了Provisoner接口的Provision和Delete方法，这两个方法主要对后端存储做创建和删除操作，并没有对PV对象进行创建和删除操作。\nPV对象的相关操作具体由ProvisionController中的provisionClaimOperation和deleteVolumeOperation具体执行，同时调用了具体provisioner的Provision和Delete两个方法来对存储数据做处理。\nfunc main() { provisionController.Run(wait.NeverStop) } 这块代码逻辑可参考：nfs-client-provisioner 源码分析\n参考文章：\nhttps://github.com/kubernetes-csi/external-provisioner https://github.com/container-storage-interface/spec https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md https://github.com/container-storage-interface/spec/blob/master/spec.md ","categories":"","description":"","excerpt":" 本文主要分析csi-provisioner的源码，关于开发一个Dynamic Provisioner，具体可参 …","ref":"/kubernetes-notes/develop/csi/csi-provisioner/","tags":["源码分析"],"title":"csi-provisioner源码分析"},{"body":"Docker学习笔记 详见：Docker学习笔记\n","categories":"","description":"","excerpt":"Docker学习笔记 详见：Docker学习笔记\n","ref":"/kubernetes-notes/runtime/docker/docker-notes/","tags":"","title":"Docker学习笔记"},{"body":" 本文主要记录一些Golang相关的资源链接和书籍\n1. 官方文档 1.1. 官网 https://golang.org/\nhttps://golang.org/doc/\n1.2. 基础 A Tour of Go\nEffective Go\nFrequently Asked Questions (FAQ)\nCodeReviewComments\nUber Go Style Guide | 中文版\n1.3. 补充 Diagnostics\nThe Go Wiki\nLanguage Specification\nThe Go Memory Model\nGo Playground\nawesome-go.com\n1.4. The Go Blog Share Memory by Communicating Defer, Panic, and Recover Go Slices: usage and internals Profiling Go Programs 2. 书籍 《The Go Programming Language》\n《Mastering Go》\n《Go语言实战》\n3. GitHub上优秀的Go项目 ","categories":"","description":"","excerpt":" 本文主要记录一些Golang相关的资源链接和书籍\n1. 官方文档 1.1. 官网 https://golang.org/ …","ref":"/golang-notes/summary/go-resource/","tags":["Golang"],"title":"Golang资源"},{"body":" 本文基于《Kubernetes源码剖析》整理，结合k8s v1.22.0代码分析\n概述 k8s声明式API的思想，以资源描述对象为中心，声明对象的spec，通过系统维持status状态始终是用户声明的资源描述spec，具体可以参考理解k8s资源对象。k8s是一个完全以资源为中心的系统，本质是一个资源控制系统--注册、管理、调度资源并维护资源的状态。k8s将资源进行再次分组和版本化，形成Group(资源组)、Version(资源版本)、Resource(资源)【其中kind表示资源种类】\n资源描述对象 资源描述对象必须的声明元素如下：\napiVersion：kubernetes API的版本，包含\u003cgroup\u003e/\u003cversion\u003e kind：kubernetes对象的类型 metadata：唯一标识该对象的元数据，包括name，UID，可选的namespace spec：标识对象的详细信息，不同对象的spec的格式不同，可以嵌套其他对象的字段。 说明：\n\u003cgroup\u003e/\u003cversion\u003e/\u003cresource\u003e表示唯一一种资源对象，例如apps/v1/deployment。\n每种资源对象都有增删改查的操作方法。具体可包含8类操作接口：\n增：create\n删：delete，deletecollection\n改：update，patch\n查：get，list，watch\n除了内置的k8s资源对象，可用CRD自定义资源对象，在k8s系统中使用。 示例：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 资源对象分类及代码目录 资源对象通用结构 ","categories":"","description":"","excerpt":" 本文基于《Kubernetes源码剖析》整理，结合k8s v1.22.0代码分析\n概述 k8s声明式API的思想，以资源描述对象为中心，声 …","ref":"/k8s-source-code-analysis/summary/resource-object/","tags":["源码分析"],"title":"k8s核心数据结构分析"},{"body":"1. k8s知识体系 以下整理了k8s涉及的相关知识体系。\n思维导图：https://www.processon.com/view/link/5d7f7d08e4b03461a3a937e2\n2. k8s重点开源项目 TODO\n","categories":"","description":"","excerpt":"1. k8s知识体系 以下整理了k8s涉及的相关知识体系。\n思维导 …","ref":"/kubernetes-notes/paas/k8s/","tags":["Kubernetes"],"title":"k8s知识体系"},{"body":"问题描述 write /proc/self/attr/keycreate: permission denied 具体报错：\nkuberuntime_manager.go:758] createPodSandbox for pod \"ecc-hostpath-provisioner-8jbhf_kube-system(b8050fd3-4ffe-11eb-a82e-c6090b53405b)\" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"ecc-hostpath-provisioner-8jbhf\": Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"write /proc/self/attr/keycreate: permission denied\\\"\": unknown 解决办法 SELINUX未设置成disabled\n# 将SELINUX设置成disabled setenforce 0 # 临时生效 # 永久生效，但需重启，配合上述命令可以不用立即重启 sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config # 查看SELinux状态 $ /usr/sbin/sestatus -v SELinux status: disabled $ getenforce Disabled ","categories":"","description":"","excerpt":"问题描述 write /proc/self/attr/keycreate: permission denied 具体报错： …","ref":"/kubernetes-notes/trouble-shooting/node/keycreate-permission-denied/","tags":["问题排查"],"title":"keycreate permission denied"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/kubeedge/","tags":"","title":"KubeEdge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/architecture/","tags":"","title":"kubernetes架构"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/","tags":"","title":"Kubernetes学习笔记"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/","tags":"","title":"Kubernetes源码分析"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/file/","tags":"","title":"Linux 文件系统"},{"body":"1. 通过容器的方式部署 mkdir -p ~/data/mysql docker run --name my-mysql -v ~/data/mysql:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 2. 通过k8s deployment的方式部署 部署的注意事项：\nmysql数据的持久化：数据卷映射和固定宿主机\n设置账号密码及服务端口\napiVersion: apps/v1 kind: Deployment metadata: name: mysql namespace: default spec: replicas: 1 strategy: type: Recreate # 先删除旧 Pod，再启动新 Pod selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:8.0 env: - name: MYSQL_ROOT_PASSWORD value: \"rootpass\" - name: MYSQL_USER value: \"user\" - name: MYSQL_PASSWORD value: \"userpass\" ports: - containerPort: 3306 # MySQL 默认端口 hostPort: 13306 # 直接映射到宿主机端口 volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql # MySQL 数据目录 volumes: - name: mysql-storage hostPath: path: /data/mysql # 宿主机上的 MySQL 存储目录 type: DirectoryOrCreate nodeName: mysql-node ","categories":"","description":"","excerpt":"1. 通过容器的方式部署 mkdir -p ~/data/mysql docker run --name my-mysql -v …","ref":"/linux-notes/mysql/deploy-mysql/","tags":["Mysql"],"title":"Mysql服务部署"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析kube-apiserver中cmd部分的代码，即NewAPIServerCommand相关的代码。更多具体的逻辑待后续文章分析。\nkube-apiserver的cmd部分目录代码结构如下：\nkube-apiserver ├── apiserver.go # kube-apiserver的main入口 └── app ├── aggregator.go ├── apiextensions.go ├── options # 初始化kube-apiserver使用到的option │ ├── options.go # 包括：NewServerRunOptions、Flags等 │ ├── options_test.go │ └── validation.go ├── server.go # 包括：NewAPIServerCommand、Run、CreateServerChain、Complete等 1. Main 此部分代码位于cmd/kube-apiserver/apiserver.go\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewAPIServerCommand(server.SetupSignalHandler()) // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"error: %v\\n\", err) os.Exit(1) } } 核心代码：\n// 初始化APIServerCommand command := app.NewAPIServerCommand(server.SetupSignalHandler()) // 执行Execute err := command.Execute() 2. NewAPIServerCommand 此部分的代码位于/cmd/kube-apiserver/app/server.go\nNewAPIServerCommand即Cobra命令行框架的构造函数，主要包括三部分：\n构造option 添加Flags 执行Run函数 完整代码如下：\n此部分代码位于cmd/kube-apiserver/app/server.go\n// NewAPIServerCommand creates a *cobra.Command object with default parameters func NewAPIServerCommand(stopCh \u003c-chan struct{}) *cobra.Command { s := options.NewServerRunOptions() cmd := \u0026cobra.Command{ Use: \"kube-apiserver\", Long: `The Kubernetes API server validates and configures data for the api objects which include pods, services, replicationcontrollers, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.`, RunE: func(cmd *cobra.Command, args []string) error { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) // set default options completedOptions, err := Complete(s) if err != nil { return err } // validate options if errs := completedOptions.Validate(); len(errs) != 0 { return utilerrors.NewAggregate(errs) } return Run(completedOptions, stopCh) }, } fs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := apiserverflag.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStderr(), namedFlagSets, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStdout(), namedFlagSets, cols) }) return cmd } 核心代码：\n// 构造option s := options.NewServerRunOptions() // 添加flags fs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } // set default options completedOptions, err := Complete(s) // Run Run(completedOptions, stopCh) 3. NewServerRunOptions NewServerRunOptions基于默认的参数构造ServerRunOptions结构体。ServerRunOptions是apiserver运行的配置信息。具体结构体定义如下。\n3.1. ServerRunOptions 其中主要的配置如下：\nGenericServerRunOptions Etcd SecureServing KubeletConfig ... // ServerRunOptions runs a kubernetes api server. type ServerRunOptions struct { GenericServerRunOptions *genericoptions.ServerRunOptions Etcd *genericoptions.EtcdOptions SecureServing *genericoptions.SecureServingOptionsWithLoopback InsecureServing *genericoptions.DeprecatedInsecureServingOptionsWithLoopback Audit *genericoptions.AuditOptions Features *genericoptions.FeatureOptions Admission *kubeoptions.AdmissionOptions Authentication *kubeoptions.BuiltInAuthenticationOptions Authorization *kubeoptions.BuiltInAuthorizationOptions CloudProvider *kubeoptions.CloudProviderOptions StorageSerialization *kubeoptions.StorageSerializationOptions APIEnablement *genericoptions.APIEnablementOptions AllowPrivileged bool EnableLogsHandler bool EventTTL time.Duration KubeletConfig kubeletclient.KubeletClientConfig KubernetesServiceNodePort int MaxConnectionBytesPerSec int64 ServiceClusterIPRange net.IPNet // TODO: make this a list ServiceNodePortRange utilnet.PortRange SSHKeyfile string SSHUser string ProxyClientCertFile string ProxyClientKeyFile string EnableAggregatorRouting bool MasterCount int EndpointReconcilerType string ServiceAccountSigningKeyFile string } 3.2. NewServerRunOptions NewServerRunOptions初始化配置结构体。\n// NewServerRunOptions creates a new ServerRunOptions object with default parameters func NewServerRunOptions() *ServerRunOptions { s := ServerRunOptions{ GenericServerRunOptions: genericoptions.NewServerRunOptions(), Etcd: genericoptions.NewEtcdOptions(storagebackend.NewDefaultConfig(kubeoptions.DefaultEtcdPathPrefix, nil)), SecureServing: kubeoptions.NewSecureServingOptions(), InsecureServing: kubeoptions.NewInsecureServingOptions(), Audit: genericoptions.NewAuditOptions(), Features: genericoptions.NewFeatureOptions(), Admission: kubeoptions.NewAdmissionOptions(), Authentication: kubeoptions.NewBuiltInAuthenticationOptions().WithAll(), Authorization: kubeoptions.NewBuiltInAuthorizationOptions(), CloudProvider: kubeoptions.NewCloudProviderOptions(), StorageSerialization: kubeoptions.NewStorageSerializationOptions(), APIEnablement: genericoptions.NewAPIEnablementOptions(), EnableLogsHandler: true, EventTTL: 1 * time.Hour, MasterCount: 1, EndpointReconcilerType: string(reconcilers.LeaseEndpointReconcilerType), KubeletConfig: kubeletclient.KubeletClientConfig{ Port: ports.KubeletPort, ReadOnlyPort: ports.KubeletReadOnlyPort, PreferredAddressTypes: []string{ // --override-hostname string(api.NodeHostName), // internal, preferring DNS if reported string(api.NodeInternalDNS), string(api.NodeInternalIP), // external, preferring DNS if reported string(api.NodeExternalDNS), string(api.NodeExternalIP), }, EnableHttps: true, HTTPTimeout: time.Duration(5) * time.Second, }, ServiceNodePortRange: kubeoptions.DefaultServiceNodePortRange, } s.ServiceClusterIPRange = kubeoptions.DefaultServiceIPCIDR // Overwrite the default for storage data format. s.Etcd.DefaultStorageMediaType = \"application/vnd.kubernetes.protobuf\" return \u0026s } 3.3. Complete 当kube-apiserver的flags被解析后，调用Complete完成默认配置。\n此部分代码位于cmd/kube-apiserver/app/server.go\n// Should be called after kube-apiserver flags parsed. func Complete(s *options.ServerRunOptions) (completedServerRunOptions, error) { var options completedServerRunOptions // set defaults if err := s.GenericServerRunOptions.DefaultAdvertiseAddress(s.SecureServing.SecureServingOptions); err != nil { return options, err } if err := kubeoptions.DefaultAdvertiseAddress(s.GenericServerRunOptions, s.InsecureServing.DeprecatedInsecureServingOptions); err != nil { return options, err } serviceIPRange, apiServerServiceIP, err := master.DefaultServiceIPRange(s.ServiceClusterIPRange) if err != nil { return options, fmt.Errorf(\"error determining service IP ranges: %v\", err) } s.ServiceClusterIPRange = serviceIPRange if err := s.SecureServing.MaybeDefaultWithSelfSignedCerts(s.GenericServerRunOptions.AdvertiseAddress.String(), []string{\"kubernetes.default.svc\", \"kubernetes.default\", \"kubernetes\"}, []net.IP{apiServerServiceIP}); err != nil { return options, fmt.Errorf(\"error creating self-signed certificates: %v\", err) } if len(s.GenericServerRunOptions.ExternalHost) == 0 { if len(s.GenericServerRunOptions.AdvertiseAddress) \u003e 0 { s.GenericServerRunOptions.ExternalHost = s.GenericServerRunOptions.AdvertiseAddress.String() } else { if hostname, err := os.Hostname(); err == nil { s.GenericServerRunOptions.ExternalHost = hostname } else { return options, fmt.Errorf(\"error finding host name: %v\", err) } } glog.Infof(\"external host was not specified, using %v\", s.GenericServerRunOptions.ExternalHost) } s.Authentication.ApplyAuthorization(s.Authorization) // Use (ServiceAccountSigningKeyFile != \"\") as a proxy to the user enabling // TokenRequest functionality. This defaulting was convenient, but messed up // a lot of people when they rotated their serving cert with no idea it was // connected to their service account keys. We are taking this oppurtunity to // remove this problematic defaulting. if s.ServiceAccountSigningKeyFile == \"\" { // Default to the private server key for service account token signing if len(s.Authentication.ServiceAccounts.KeyFiles) == 0 \u0026\u0026 s.SecureServing.ServerCert.CertKey.KeyFile != \"\" { if kubeauthenticator.IsValidServiceAccountKeyFile(s.SecureServing.ServerCert.CertKey.KeyFile) { s.Authentication.ServiceAccounts.KeyFiles = []string{s.SecureServing.ServerCert.CertKey.KeyFile} } else { glog.Warning(\"No TLS key provided, service account token authentication disabled\") } } } if s.Etcd.StorageConfig.DeserializationCacheSize == 0 { // When size of cache is not explicitly set, estimate its size based on // target memory usage. glog.V(2).Infof(\"Initializing deserialization cache size based on %dMB limit\", s.GenericServerRunOptions.TargetRAMMB) // This is the heuristics that from memory capacity is trying to infer // the maximum number of nodes in the cluster and set cache sizes based // on that value. // From our documentation, we officially recommend 120GB machines for // 2000 nodes, and we scale from that point. Thus we assume ~60MB of // capacity per node. // TODO: We may consider deciding that some percentage of memory will // be used for the deserialization cache and divide it by the max object // size to compute its size. We may even go further and measure // collective sizes of the objects in the cache. clusterSize := s.GenericServerRunOptions.TargetRAMMB / 60 s.Etcd.StorageConfig.DeserializationCacheSize = 25 * clusterSize if s.Etcd.StorageConfig.DeserializationCacheSize \u003c 1000 { s.Etcd.StorageConfig.DeserializationCacheSize = 1000 } } if s.Etcd.EnableWatchCache { glog.V(2).Infof(\"Initializing cache sizes based on %dMB limit\", s.GenericServerRunOptions.TargetRAMMB) sizes := cachesize.NewHeuristicWatchCacheSizes(s.GenericServerRunOptions.TargetRAMMB) if userSpecified, err := serveroptions.ParseWatchCacheSizes(s.Etcd.WatchCacheSizes); err == nil { for resource, size := range userSpecified { sizes[resource] = size } } s.Etcd.WatchCacheSizes, err = serveroptions.WriteWatchCacheSizes(sizes) if err != nil { return options, err } } // TODO: remove when we stop supporting the legacy group version. if s.APIEnablement.RuntimeConfig != nil { for key, value := range s.APIEnablement.RuntimeConfig { if key == \"v1\" || strings.HasPrefix(key, \"v1/\") || key == \"api/v1\" || strings.HasPrefix(key, \"api/v1/\") { delete(s.APIEnablement.RuntimeConfig, key) s.APIEnablement.RuntimeConfig[\"/v1\"] = value } if key == \"api/legacy\" { delete(s.APIEnablement.RuntimeConfig, key) } } } options.ServerRunOptions = s return options, nil } 3. AddFlagSet AddFlagSet主要的作用是通过外部传入的flag的具体值，解析的时候传递给option的结构体，最终给apiserver使用。\n其中NewAPIServerCommand关于AddFlagSet的相关代码如下：\nfs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } 3.1. Flags Flags完整代码如下：\n此部分代码位于cmd/kube-apiserver/app/options/options.go\n// Flags returns flags for a specific APIServer by section name func (s *ServerRunOptions) Flags() (fss apiserverflag.NamedFlagSets) { // Add the generic flags. s.GenericServerRunOptions.AddUniversalFlags(fss.FlagSet(\"generic\")) s.Etcd.AddFlags(fss.FlagSet(\"etcd\")) s.SecureServing.AddFlags(fss.FlagSet(\"secure serving\")) s.InsecureServing.AddFlags(fss.FlagSet(\"insecure serving\")) s.InsecureServing.AddUnqualifiedFlags(fss.FlagSet(\"insecure serving\")) // TODO: remove it until kops stops using `--address` s.Audit.AddFlags(fss.FlagSet(\"auditing\")) s.Features.AddFlags(fss.FlagSet(\"features\")) s.Authentication.AddFlags(fss.FlagSet(\"authentication\")) s.Authorization.AddFlags(fss.FlagSet(\"authorization\")) s.CloudProvider.AddFlags(fss.FlagSet(\"cloud provider\")) s.StorageSerialization.AddFlags(fss.FlagSet(\"storage\")) s.APIEnablement.AddFlags(fss.FlagSet(\"api enablement\")) s.Admission.AddFlags(fss.FlagSet(\"admission\")) // Note: the weird \"\"+ in below lines seems to be the only way to get gofmt to // arrange these text blocks sensibly. Grrr. fs := fss.FlagSet(\"misc\") fs.DurationVar(\u0026s.EventTTL, \"event-ttl\", s.EventTTL, \"Amount of time to retain events.\") fs.BoolVar(\u0026s.AllowPrivileged, \"allow-privileged\", s.AllowPrivileged, \"If true, allow privileged containers. [default=false]\") fs.BoolVar(\u0026s.EnableLogsHandler, \"enable-logs-handler\", s.EnableLogsHandler, \"If true, install a /logs handler for the apiserver logs.\") // Deprecated in release 1.9 fs.StringVar(\u0026s.SSHUser, \"ssh-user\", s.SSHUser, \"If non-empty, use secure SSH proxy to the nodes, using this user name\") fs.MarkDeprecated(\"ssh-user\", \"This flag will be removed in a future version.\") // Deprecated in release 1.9 fs.StringVar(\u0026s.SSHKeyfile, \"ssh-keyfile\", s.SSHKeyfile, \"If non-empty, use secure SSH proxy to the nodes, using this user keyfile\") fs.MarkDeprecated(\"ssh-keyfile\", \"This flag will be removed in a future version.\") fs.Int64Var(\u0026s.MaxConnectionBytesPerSec, \"max-connection-bytes-per-sec\", s.MaxConnectionBytesPerSec, \"\"+ \"If non-zero, throttle each user connection to this number of bytes/sec. \"+ \"Currently only applies to long-running requests.\") fs.IntVar(\u0026s.MasterCount, \"apiserver-count\", s.MasterCount, \"The number of apiservers running in the cluster, must be a positive number. (In use when --endpoint-reconciler-type=master-count is enabled.)\") fs.StringVar(\u0026s.EndpointReconcilerType, \"endpoint-reconciler-type\", string(s.EndpointReconcilerType), \"Use an endpoint reconciler (\"+strings.Join(reconcilers.AllTypes.Names(), \", \")+\")\") // See #14282 for details on how to test/try this option out. // TODO: remove this comment once this option is tested in CI. fs.IntVar(\u0026s.KubernetesServiceNodePort, \"kubernetes-service-node-port\", s.KubernetesServiceNodePort, \"\"+ \"If non-zero, the Kubernetes master service (which apiserver creates/maintains) will be \"+ \"of type NodePort, using this as the value of the port. If zero, the Kubernetes master \"+ \"service will be of type ClusterIP.\") fs.IPNetVar(\u0026s.ServiceClusterIPRange, \"service-cluster-ip-range\", s.ServiceClusterIPRange, \"\"+ \"A CIDR notation IP range from which to assign service cluster IPs. This must not \"+ \"overlap with any IP ranges assigned to nodes for pods.\") fs.Var(\u0026s.ServiceNodePortRange, \"service-node-port-range\", \"\"+ \"A port range to reserve for services with NodePort visibility. \"+ \"Example: '30000-32767'. Inclusive at both ends of the range.\") // Kubelet related flags: fs.BoolVar(\u0026s.KubeletConfig.EnableHttps, \"kubelet-https\", s.KubeletConfig.EnableHttps, \"Use https for kubelet connections.\") fs.StringSliceVar(\u0026s.KubeletConfig.PreferredAddressTypes, \"kubelet-preferred-address-types\", s.KubeletConfig.PreferredAddressTypes, \"List of the preferred NodeAddressTypes to use for kubelet connections.\") fs.UintVar(\u0026s.KubeletConfig.Port, \"kubelet-port\", s.KubeletConfig.Port, \"DEPRECATED: kubelet port.\") fs.MarkDeprecated(\"kubelet-port\", \"kubelet-port is deprecated and will be removed.\") fs.UintVar(\u0026s.KubeletConfig.ReadOnlyPort, \"kubelet-read-only-port\", s.KubeletConfig.ReadOnlyPort, \"DEPRECATED: kubelet port.\") fs.DurationVar(\u0026s.KubeletConfig.HTTPTimeout, \"kubelet-timeout\", s.KubeletConfig.HTTPTimeout, \"Timeout for kubelet operations.\") fs.StringVar(\u0026s.KubeletConfig.CertFile, \"kubelet-client-certificate\", s.KubeletConfig.CertFile, \"Path to a client cert file for TLS.\") fs.StringVar(\u0026s.KubeletConfig.KeyFile, \"kubelet-client-key\", s.KubeletConfig.KeyFile, \"Path to a client key file for TLS.\") fs.StringVar(\u0026s.KubeletConfig.CAFile, \"kubelet-certificate-authority\", s.KubeletConfig.CAFile, \"Path to a cert file for the certificate authority.\") // TODO: delete this flag in 1.13 repair := false fs.BoolVar(\u0026repair, \"repair-malformed-updates\", false, \"deprecated\") fs.MarkDeprecated(\"repair-malformed-updates\", \"This flag will be removed in a future version\") fs.StringVar(\u0026s.ProxyClientCertFile, \"proxy-client-cert-file\", s.ProxyClientCertFile, \"\"+ \"Client certificate used to prove the identity of the aggregator or kube-apiserver \"+ \"when it must call out during a request. This includes proxying requests to a user \"+ \"api-server and calling out to webhook admission plugins. It is expected that this \"+ \"cert includes a signature from the CA in the --requestheader-client-ca-file flag. \"+ \"That CA is published in the 'extension-apiserver-authentication' configmap in \"+ \"the kube-system namespace. Components receiving calls from kube-aggregator should \"+ \"use that CA to perform their half of the mutual TLS verification.\") fs.StringVar(\u0026s.ProxyClientKeyFile, \"proxy-client-key-file\", s.ProxyClientKeyFile, \"\"+ \"Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver \"+ \"when it must call out during a request. This includes proxying requests to a user \"+ \"api-server and calling out to webhook admission plugins.\") fs.BoolVar(\u0026s.EnableAggregatorRouting, \"enable-aggregator-routing\", s.EnableAggregatorRouting, \"Turns on aggregator routing requests to endpoints IP rather than cluster IP.\") fs.StringVar(\u0026s.ServiceAccountSigningKeyFile, \"service-account-signing-key-file\", s.ServiceAccountSigningKeyFile, \"\"+ \"Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.)\") return fss } 4. Run Run以常驻的方式运行apiserver。\n主要内容如下：\n构造一个聚合的server结构体。 执行PrepareRun。 最终执行Run。 此部分代码位于cmd/kube-apiserver/app/server.go\n// Run runs the specified APIServer. This should never exit. func Run(completeOptions completedServerRunOptions, stopCh \u003c-chan struct{}) error { // To help debugging, immediately log version glog.Infof(\"Version: %+v\", version.Get()) server, err := CreateServerChain(completeOptions, stopCh) if err != nil { return err } return server.PrepareRun().Run(stopCh) } 4.1. CreateServerChain 构造聚合的Server。\n基本流程如下：\n首先生成config对象，包括kubeAPIServerConfig、apiExtensionsConfig。 再通过config生成server对象，包括apiExtensionsServer、kubeAPIServer。 执行apiExtensionsServer、kubeAPIServer的PrepareRun部分。 生成聚合的config对象aggregatorConfig。 基于aggregatorConfig、kubeAPIServer、apiExtensionsServer生成聚合的serveraggregatorServer。 此部分代码位于cmd/kube-apiserver/app/server.go\n// CreateServerChain creates the apiservers connected via delegation. func CreateServerChain(completedOptions completedServerRunOptions, stopCh \u003c-chan struct{}) (*genericapiserver.GenericAPIServer, error) { nodeTunneler, proxyTransport, err := CreateNodeDialer(completedOptions) if err != nil { return nil, err } kubeAPIServerConfig, insecureServingInfo, serviceResolver, pluginInitializer, admissionPostStartHook, err := CreateKubeAPIServerConfig(completedOptions, nodeTunneler, proxyTransport) if err != nil { return nil, err } // If additional API servers are added, they should be gated. apiExtensionsConfig, err := createAPIExtensionsConfig(*kubeAPIServerConfig.GenericConfig, kubeAPIServerConfig.ExtraConfig.VersionedInformers, pluginInitializer, completedOptions.ServerRunOptions, completedOptions.MasterCount) if err != nil { return nil, err } apiExtensionsServer, err := createAPIExtensionsServer(apiExtensionsConfig, genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, admissionPostStartHook) if err != nil { return nil, err } // otherwise go down the normal path of standing the aggregator up in front of the API server // this wires up openapi kubeAPIServer.GenericAPIServer.PrepareRun() // This will wire up openapi for extension api server apiExtensionsServer.GenericAPIServer.PrepareRun() // aggregator comes last in the chain aggregatorConfig, err := createAggregatorConfig(*kubeAPIServerConfig.GenericConfig, completedOptions.ServerRunOptions, kubeAPIServerConfig.ExtraConfig.VersionedInformers, serviceResolver, proxyTransport, pluginInitializer) if err != nil { return nil, err } aggregatorServer, err := createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers) if err != nil { // we don't need special handling for innerStopCh because the aggregator server doesn't create any go routines return nil, err } if insecureServingInfo != nil { insecureHandlerChain := kubeserver.BuildInsecureHandlerChain(aggregatorServer.GenericAPIServer.UnprotectedHandler(), kubeAPIServerConfig.GenericConfig) if err := insecureServingInfo.Serve(insecureHandlerChain, kubeAPIServerConfig.GenericConfig.RequestTimeout, stopCh); err != nil { return nil, err } } return aggregatorServer.GenericAPIServer, nil } 4.2. PrepareRun PrepareRun主要执行一些API安装操作。\n此部分的代码位于vendor/k8s.io/apiserver/pkg/server/genericapiserver.go\n// PrepareRun does post API installation setup steps. func (s *GenericAPIServer) PrepareRun() preparedGenericAPIServer { if s.swaggerConfig != nil { routes.Swagger{Config: s.swaggerConfig}.Install(s.Handler.GoRestfulContainer) } if s.openAPIConfig != nil { routes.OpenAPI{ Config: s.openAPIConfig, }.Install(s.Handler.GoRestfulContainer, s.Handler.NonGoRestfulMux) } s.installHealthz() // Register audit backend preShutdownHook. if s.AuditBackend != nil { s.AddPreShutdownHook(\"audit-backend\", func() error { s.AuditBackend.Shutdown() return nil }) } return preparedGenericAPIServer{s} } 4.3. preparedGenericAPIServer.Run preparedGenericAPIServer.Run运行一个安全的http server。具体的实现逻辑待后续文章分析。\n此部分代码位于vendor/k8s.io/apiserver/pkg/server/genericapiserver.go\n// Run spawns the secure http server. It only returns if stopCh is closed // or the secure port cannot be listened on initially. func (s preparedGenericAPIServer) Run(stopCh \u003c-chan struct{}) error { err := s.NonBlockingRun(stopCh) if err != nil { return err } \u003c-stopCh err = s.RunPreShutdownHooks() if err != nil { return err } // Wait for all requests to finish, which are bounded by the RequestTimeout variable. s.HandlerChainWaitGroup.Wait() return nil } 核心函数：\nerr := s.NonBlockingRun(stopCh) preparedGenericAPIServer.Run主要是调用NonBlockingRun函数，最终运行一个http server。该部分逻辑待后续文章分析。\n5. 总结 NewAPIServerCommand采用了Cobra命令行框架，该框架使用主要包含以下部分：\n构造option参数，提供给执行主体(例如 本文的server)作为配置参数使用。 添加Flags，主要用来通过传入的flags参数最终解析成option中使用的结构体属性。 执行Run函数，执行主体的运行逻辑部分（核心部分）。 其中Run函数的主要内容如下：\n构造一个聚合的server结构体。 执行PrepareRun。 最终执行preparedGenericAPIServer.Run。 preparedGenericAPIServer.Run主要是调用NonBlockingRun函数，最终运行一个http server。NonBlockingRun的具体逻辑待后续文章再单独分析。\n参考：\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-apiserver https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/server.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/aggregator.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/options/options.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析kube-apiserver中cmd部分的代码， …","ref":"/k8s-source-code-analysis/kube-apiserver/newapiservercommand/","tags":["源码分析"],"title":"kube-apiserver源码分析（一）之 NewAPIServerCommand"},{"body":"1. 简介 NVIDIA device plugin 通过k8s daemonset的方式部署到每个k8s的node节点上，实现了Kubernetes device plugin的接口。\n提供以下功能：\n暴露每个节点的GPU数量给集群 跟踪GPU的健康情况 使在k8s的节点可以运行GPU容器 2. 要求 NVIDIA drivers ~= 384.81 nvidia-docker version \u003e 2.0 (see how to install and it's prerequisites) docker configured with nvidia as the default runtime. Kubernetes version \u003e= 1.10 3. 使用 3.1. 安装NVIDIA drivers和nvidia-docker 提供GPU节点的机器，准备工作如下\n安装NVIDIA drivers ~= 384.81 安装nvidia-docker version \u003e 2.0 3.2. 配置docker runtime 配置nvidia runtime作为GPU节点的默认runtime。\n修改文件/etc/docker/daemon.json，增加以下runtime内容。\n{ \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } 3.3. 部署nvidia-device-plugin $ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml nvidia-device-plugin的daemonset yaml文件如下：\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-device-plugin-daemonset namespace: kube-system spec: selector: matchLabels: name: nvidia-device-plugin-ds updateStrategy: type: RollingUpdate template: metadata: # This annotation is deprecated. Kept here for backward compatibility # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: name: nvidia-device-plugin-ds spec: tolerations: # This toleration is deprecated. Kept here for backward compatibility # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ - key: CriticalAddonsOnly operator: Exists - key: nvidia.com/gpu operator: Exists effect: NoSchedule # Mark this pod as a critical add-on; when enabled, the critical add-on # scheduler reserves resources for critical add-on pods so that they can # be rescheduled after a failure. # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: \"system-node-critical\" containers: - image: nvidia/k8s-device-plugin:1.0.0-beta4 name: nvidia-device-plugin-ctr securityContext: allowPrivilegeEscalation: false capabilities: drop: [\"ALL\"] volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins 3.4. 运行GPU任务 创建一个GPU的pod，pod的资源类型指定为nvidia.com/gpu。\napiVersion: v1 kind: Pod metadata: name: gpu-pod spec: containers: - name: cuda-container image: nvidia/cuda:9.0-devel resources: limits: nvidia.com/gpu: 2 # requesting 2 GPUs - name: digits-container image: nvidia/digits:6.0 resources: limits: nvidia.com/gpu: 2 # requesting 2 GPUs 4. 构建和运行nvidia-device-plugin 4.1. docker方式 4.1.1. 编译 直接拉取dockerhub的镜像 $ docker pull nvidia/k8s-device-plugin:1.0.0-beta4 拉取代码构建镜像 $ docker build -t nvidia/k8s-device-plugin:1.0.0-beta4 https://github.com/NVIDIA/k8s-device-plugin.git#1.0.0-beta4 修改nvidia-device-plugin后构建镜像 $ git clone https://github.com/NVIDIA/k8s-device-plugin.git \u0026\u0026 cd k8s-device-plugin $ git checkout 1.0.0-beta4 $ docker build -t nvidia/k8s-device-plugin:1.0.0-beta4 . 4.1.2. 运行 docker本地运行 $ docker run --security-opt=no-new-privileges --cap-drop=ALL --network=none -it -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins nvidia/k8s-device-plugin:1.0.0-beta4 daemonset运行 $ kubectl create -f nvidia-device-plugin.yml 4.2. 非docker方式 4.2.1. 编译 $ C_INCLUDE_PATH=/usr/local/cuda/include LIBRARY_PATH=/usr/local/cuda/lib64 go build 4.2.2. 本地运行 $ ./k8s-device-plugin 参考：\nhttps://github.com/NVIDIA/k8s-device-plugin k8s-device-plugin gpu-support ","categories":"","description":"","excerpt":"1. 简介 NVIDIA device plugin 通过k8s daemonset的方式部署到每个k8s的node节点上，实现 …","ref":"/kubernetes-notes/runtime/gpu/nvidia-device-plugin/","tags":["Kubernetes"],"title":"nvidia-device-plugin介绍"},{"body":"Pod创建基本流程图 Pod创建完整流程图 图片来源：https://fuckcloudnative.io/posts/what-happens-when-k8s/\n参考:\nhttps://fuckcloudnative.io/posts/what-happens-when-k8s/ ","categories":"","description":"","excerpt":"Pod创建基本流程图 Pod创建完整流程图 图片来 …","ref":"/kubernetes-notes/principle/flow/pod-flow/","tags":["Kubernetes"],"title":"Pod创建流程"},{"body":"1. shell简介 shell是用户和Linux内核之间的一层代理，解释用户输入的命令，传递给内核。\nshell是一种脚本语言（解释性语言）。\nShell既是一种命令语言，又是一种程序设计语言。作为命令语言，它交互式地解释和执行用户输入的命令；作为程序设计语言，它定义了各种变量和参数，并提供了许多在高级语言中才具有的控制结构，包括循环和分支。\nShell有两种执行命令的方式：\n交互式（Interactive）：解释执行用户的命令，用户输入一条命令，Shell就解释执行一条。 批处理（Batch）：用户事先写一个Shell脚本(Script)，其中有很多条命令，让Shell一次把这些命令执行完，而不必一条一条地敲命令。 Shell脚本和编程语言很相似，也有变量和流程控制语句，但Shell脚本是解释执行的，不需要编译，Shell程序从脚本中一行一行读取并执行这些命令，相当于一个用户把脚本中的命令一行一行敲到Shell提示符下执行。\nUnix/Linux上常见的Shell脚本解释器有bash、sh、csh、ksh等，习惯上把它们称作一种Shell。\n程序设计语言可以分为两类：编译型语言和解释型语言。\n1.1. 编译型语言 很多传统的程序设计语言，例如Fortran、Ada、Pascal、C、C++和Java，都是编译型语言。这类语言需要预先将我们写好的源代码(source code)转换成目标代码(object code)，这个过程被称作“编译”。运行程序时，直接读取目标代码(object code)。由于编译后的目标代码(object code)非常接近计算机底层，因此执行效率很高，这是编译型语言的优点。\n编译型语言多半运作于底层，所处理的是字节、整数、浮点数或是其他机器层级的对象，往往实现一个简单的功能需要大量复杂的代码。\n1.2. 解释型语言 有的语言（例如： Shell、JavaScript、Python、PHP等）需要一边执行一边翻译，不会产生任何可执行文件，用户需要拿到源码才能运行程序。程序运行后会即时翻译，翻译一部分执行一部分，并不用等所有代码翻译完。\n这个过程叫解释，这类语言叫解释型语言或脚本语言，完成解释过程的软件叫解释器。\n解释型语言也被称作“脚本语言”。因为每次执行程序都多了编译的过程，因此效率有所下降。\n使用脚本编程语言的好处是，它们多半运行在比编译型语言还高的层级，能够轻易处理文件与目录之类的对象；缺点是它们的效率通常不如编译型语言。\n脚本编程语言的例子有awk、Perl、Python、Ruby与Shell。\n2. 常见的Shell类型 shell类型 说明 sh sh 是 UNIX 上的标准 shell，很多 UNIX 版本都配有 sh。 bash bash shell 是 Linux 的默认 shell，bash 兼容 sh，但并不完全一致。 csh 语法有点类似C语言。 ... 2.1. 查看shell $ cat /etc/shells /bin/sh /bin/bash /sbin/nologin /usr/bin/sh /usr/bin/bash /usr/sbin/nologin /bin/tcsh /bin/csh 查看默认shell\n$ echo $SHELL /bin/bash sh 一般被 bash 代替，/bin/sh往往是指向/bin/bash的符号链接。\n$ ls -l /bin/sh lrwxrwxrwx. 1 root root 4 Mar 8 2018 /bin/sh -\u003e bash 3. 使用shell场景 之所以要使用Shell脚本是基于：\n简单性：Shell是一个高级语言；通过它，你可以简洁地表达复杂的操作。 可移植性：使用POSIX所定义的功能，可以做到脚本无须修改就可在不同的系统上执行。 开发容易：可以在短时间内完成一个功能强大又妤用的脚本。 但是，考虑到Shell脚本的命令限制和效率问题，下列情况一般不使用Shell：\n资源密集型的任务，尤其在需要考虑效率时（比如，排序，hash等等）。 需要处理大任务的数学操作，尤其是浮点运算，精确运算，或者复杂的算术运算（这种情况一般使用C++或FORTRAN 来处理）。 有跨平台（操作系统）移植需求（一般使用C 或Java）。 复杂的应用，在必须使用结构化编程的时候（需要变量的类型检查，函数原型，等等）。 对于影响系统全局性的关键任务应用。 对于安全有很高要求的任务，比如你需要一个健壮的系统来防止入侵、破解、恶意破坏等等。 项目由连串的依赖的各个部分组成。 需要大规模的文件操作。 需要多维数组的支持。 需要数据结构的支持，比如链表或数等数据结构。 需要产生或操作图形化界面 GUI。 需要直接操作系统硬件。 需要 I/O 或socket 接口。 需要使用库或者遗留下来的老代码的接口。 私人的、闭源的应用（shell 脚本把代码就放在文本文件中，全世界都能看到）。 4. shell脚本 打开文本编辑器，新建一个文件，扩展名为sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用php写shell 脚本，扩展名就用php好了。\n输入一些代码：\n#!/bin/bash echo \"Hello World !\" “#!” 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种Shell。echo命令用于向窗口输出文本。\n5. 运行shell 运行Shell脚本有两种方法。\n5.1. 作为可执行程序 将上面的代码保存为test.sh，并 cd 到相应目录：\nchmod +x ./test.sh #使脚本具有执行权限 ./test.sh #执行脚本 注意，一定要写成./test.sh，而不是test.sh。运行其它二进制的程序也一样，直接写test.sh，linux系统会去PATH里寻找有没有叫test.sh的，而只有/bin, /sbin, /usr/bin，/usr/sbin等在PATH里，你的当前目录通常不在PATH里，所以写成test.sh是会找不到命令的，要用./test.sh告诉系统说，就在当前目录找。\n5.2. 作为解释器参数 这种运行方式是，直接运行解释器\n# 使用 sh 解释器 sh test.sh # 使用 bash 解释器 bash test.sh 这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。\n参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. shell简介 shell是用户和Linux内核之间的一层代理，解释用户输入的命令，传递给内核。\nshell是一种脚本语言（解释性语 …","ref":"/linux-notes/shell/shell-introduction/","tags":["Shell"],"title":"Shell简介"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/summary/","tags":"","title":"summary"},{"body":"1. volume概述 容器上的文件生命周期同容器的生命周期一致，即容器挂掉之后，容器将会以最初镜像中的文件系统内容启动，之前容器运行时产生的文件将会丢失。 Pod的volume的生命周期同Pod的生命周期一致，当Pod被删除的时候，对应的volume才会被删除。即Pod中的容器重启时，之前的文件仍可以保存。 容器中的进程看到的是由其 Docker 镜像和卷组成的文件系统视图。\nPod volume的使用方式\nPod 中的每个容器都必须独立指定每个卷的挂载位置，需要给Pod配置volume相关参数。\nPod的volume关键字段如下：\nspec.volumes：提供怎样的数据卷 spec.containers.volumeMounts：挂载到容器的什么路径 2. volume类型 2.1. emptyDir 1、特点\n会创建emptyDir对应的目录，默认为空（如果该目录原来有文件也会被重置为空） Pod中的不同容器可以在目录中读写相同文件（即Pod中的不同容器可以通过该方式来共享文件） 当Pod被删除，emptyDir 中的数据将被永久删除，如果只是Pod挂掉该数据还会保留 2、使用场景\n不同容器之间共享文件（例如日志采集等）\n暂存空间，例如用于基于磁盘的合并排序\n用作长时间计算崩溃恢复时的检查点\n3、示例\napiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {} 2.2. hostPath 1、特点\n会将宿主机的目录或文件挂载到Pod中 2、使用场景\n运行需要访问 Docker 内部的容器；使用 /var/lib/docker 的 hostPath\n在容器中运行 cAdvisor；使用 /dev/cgroups 的 hostPath\n其他使用到宿主机文件的场景\nhostPath的type字段\n值 行为 空字符串（默认）用于向后兼容，这意味着在挂载 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定的路径上没有任何东西存在，那么将根据需要在那里创建一个空目录，权限设置为 0755，与 Kubelet 具有相同的组和所有权。 Directory 给定的路径下必须存在目录 FileOrCreate 如果在给定的路径上没有任何东西存在，那么会根据需要创建一个空文件，权限设置为 0644，与 Kubelet 具有相同的组和所有权。 File 给定的路径下必须存在文件 Socket 给定的路径下必须存在 UNIX 套接字 CharDevice 给定的路径下必须存在字符设备 BlockDevice 给定的路径下必须存在块设备 注意事项\n由于每个节点上的文件都不同，具有相同配置的 pod 在不同节点上的行为可能会有所不同 当 Kubernetes 按照计划添加资源感知调度时，将无法考虑 hostPath 使用的资源 在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root 身份运行进程，或修改主机上的文件权限以便写入 hostPath 卷 3、示例\napiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: Directory 2.2.1. 同步文件变化到容器内 hostPath的挂载方式可以挂载目录和文件两种格式，如果使用文件挂载的方式，通过简单的vi等命令修改宿主机的文件，并不会实时同步到容器内的映射文件。而需要对容器进行重启的操作才可以把文件的修改内容同步到文件中，但生产的容器一般不建议执行重启的操作。因此我们可以通过以下的方式来避免这个问题的发生。\n以上文件不同步的本质原因是容器在初次挂载的时候使用了宿主机的文件的inode number进行标识，而vi等操作会导致文件的inode number发生变化，所以当宿主机文件的inode number发生变化，容器内并不会发生变化，为了保持文件内容一致，则需要保持修改文件的同时文件的inode number不变。那么我们可以使用 cat 或echo 命令覆盖文件的内容则inode number不会发生变化。\n示例：\ncontainers: volumeMounts: - mountPath: /etc/hosts name: hosts readOnly: true volumes: - hostPath: path: /etc/hosts type: FileOrCreate name: hosts 例如，以上的案例是通过挂载宿主机的/etc/hosts文件来映射到容器，如果想修改宿主机的hosts文件来同步容器内的hosts文件，可以通过以下的方式:\n# 查看文件的inode ls -i /etc/hosts 39324780 /etc/hosts # 追加记录 echo \"1.1.1.1 xxx.com\" \u003e\u003e /etc/hosts # 替换内容 sed 's/1.1.1.1/2.2.2.2/g' /etc/hosts \u003e temp.txt cat temp.txt \u003e /etc/hosts # 查看宿主机和容器内的inode号都没有发生变化 # crictl exec -it 20891de31a4a6 sh /var/www/html # ls -i /etc/hosts 39324780 /etc/hosts 2.3. configMap configMap提供了一种给Pod注入配置文件的方式，配置文件内容存储在configMap对象中，如果Pod使用configMap作为volume的类型，需要先创建configMap的对象。\n示例\napiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: test image: busybox volumeMounts: - name: config-vol mountPath: /etc/config volumes: - name: config-vol configMap: name: log-config items: - key: log_level path: log_level 2.4. cephfs cephfs的方式将Pod的存储挂载到ceph集群中，通过外部存储的方式持久化Pod的数据（即当Pod被删除数据仍可以存储在ceph集群中），前提是先部署和维护好一个ceph集群。\n示例\napiVersion: v1 kind: Pod metadata: name: cephfs spec: containers: - name: cephfs-rw image: kubernetes/pause volumeMounts: - mountPath: \"/mnt/cephfs\" name: cephfs volumes: - name: cephfs cephfs: monitors: - 10.16.154.78:6789 - 10.16.154.82:6789 - 10.16.154.83:6789 # by default the path is /, but you can override and mount a specific path of the filesystem by using the path attribute # path: /some/path/in/side/cephfs user: admin secretFile: \"/etc/ceph/admin.secret\" readOnly: true 更多可参考 CephFS 示例。\n2.5. nfs nfs的方式类似cephfs，即将Pod数据存储到NFS集群中，具体可参考NFS示例。\n2.6. persistentVolumeClaim persistentVolumeClaim 卷用于将PersistentVolume挂载到容器中。PersistentVolumes 是在用户不知道特定云环境的细节的情况下“声明”持久化存储（例如 GCE PersistentDisk 或 iSCSI 卷）的一种方式。\n参考文章：\nhttps://kubernetes.io/docs/concepts/storage/volumes/ ","categories":"","description":"","excerpt":"1. volume概述 容器上的文件生命周期同容器的生命周期一致，即容器挂掉之后，容器将会以最初镜像中的文件系统内容启动，之前容器运行时产生 …","ref":"/kubernetes-notes/storage/volume/volume/","tags":["Kubernetes"],"title":"Volume介绍"},{"body":" 本文主要描述个人使用vscode中的常用插件和配置\n1. 常用插件 插件名称 说明 Atom One Dark Theme 代码风格主题 JetBrains Icon Theme Icon主题 GitLens 显示某行提交记录 Partial Diff 通过剪切板diff对比 Go/Python 编程语言插件 2. 常用快捷键 详细快捷键参考：vscode快捷键\n功能 快捷键 函数跳转和引用 （command+单击）或者F12 回到上次跳回原处（跳转前位置） ctrl + - 查看接口的实现方法 打开terminal终端 Ctrl + ` 多行 块选择编辑 Shift + option +鼠标选择块 3. 配置多窗口显示 vscode默认打开一个新项目就需要打开新的窗口，为了避免很多项目同时浏览而打开太多窗口，因此设置在同一个窗口中可以显示和快速切换多个项目。\n1、左下角点击配置按钮，选中 settings。\n2、搜索 window.native 选中此选项。\n3、重启并合并窗口。\n4. 配置远程开发 4.1. 本地文件传远程开发 安装sftp插件\n创建sftp配置文件\n创建.vscode目录，在目录下创建sftp.json文件，内容如下：\n{ \"name\": \"ip\", \"host\": \"ip\", \"protocol\": \"sftp\", \"port\": 22, \"username\": \"root\", \"privateKeyPath\": \"~/.ssh/id_rsa\", \"remotePath\": \"/home/go/src/projectname\", \"uploadOnSave\": true, \"ignore\": [ \".git\", \".vscode\", \".idea\", \".DS_Store\", \"node_modules\" ], \"watcher\": { \"files\": \"/home/go/src/github.com/projectname/*\", \"autoUpload\": true, \"autoDelete\": true } } 4.2. 远程文件传本地开发 在远程设备上git clone代码，然后在vscode上安装remote的插件，通过remote插件连接远程开发机并打开代码目录。即可进行远程开发。\n通过vscode在远程机器上面安装相关的代码插件即可实现代码跳转等操作。\n","categories":"","description":"","excerpt":" 本文主要描述个人使用vscode中的常用插件和配置\n1. 常用插件 插件名称 说明 Atom One Dark Theme …","ref":"/linux-notes/ide/vscode/","tags":["IDE"],"title":"vscode使用配置"},{"body":"1. Ubuntu安装containerd 以下以Ubuntu为例\n说明：安装containerd与安装docker流程基本一致，差别在于不需要安装docker-ce\ncontainerd: apt-get install -y containerd.io docker: apt-get install docker-ce docker-ce-cli containerd.io 1. 卸载旧版本 sudo apt-get remove docker docker-engine docker.io containerd runc 如果需要删除镜像及容器数据则执行以下命令\nsudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd 2. 准备包环境 1、更新apt，允许使用https。\nsudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 2、添加docker官方GPG key。\nsudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg 3、设置软件仓库源\necho \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null 3. 安装containerd # 安装containerd sudo apt-get update sudo apt-get install -y containerd.io # 如果是安装docker则执行： sudo apt-get install docker-ce docker-ce-cli containerd.io # 查看运行状态 systemctl enable containerd systemctl status containerd 安装指定版本\n# 查看版本 apt-cache madison containerd # sudo apt-get install containerd=\u003cVERSION\u003e 4. 修改配置 在 Linux 上，containerd 的默认 CRI 套接字是 /run/containerd/containerd.sock。\n1、生成默认配置\ncontainerd config default \u003e /etc/containerd/config.toml 2、修改CgroupDriver为systemd\nk8s官方推荐使用systemd类型的CgroupDriver。\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true 3、重启containerd\nsystemctl restart containerd 2. 离线二进制安装containerd 把containerd、runc、cni-plugins、nerdctl二进制下载到本地，再上传到对应服务器，解压文件到对应目录，修改containerd配置文件，启动containerd。\n#!/bin/bash set -e ContainerdVersion=$1 ContainerdVersion=${ContainerdVersion:-1.6.6} RuncVersion=$2 RuncVersion=${RuncVersion:-1.1.3} CniVersion=$3 CniVersion=${CniVersion:-1.1.1} NerdctlVersion=$4 NerdctlVersion=${NerdctlVersion:-0.21.0} CrictlVersion=$5 CrictlVersion=${CrictlVersion:-1.24.2} echo \"--------------install containerd--------------\" wget https://github.com/containerd/containerd/releases/download/v${ContainerdVersion}/containerd-${ContainerdVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-${ContainerdVersion}-linux-amd64.tar.gz echo \"--------------install containerd service--------------\" wget https://raw.githubusercontent.com/containerd/containerd/681aaf68b7dcbe08a51c3372cbb8f813fb4466e0/containerd.service mv containerd.service /lib/systemd/system/ mkdir -p /etc/containerd/ containerd config default \u003e /etc/containerd/config.toml sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml echo \"--------------install runc--------------\" wget https://github.com/opencontainers/runc/releases/download/v${RuncVersion}/runc.amd64 chmod +x runc.amd64 mv runc.amd64 /usr/local/bin/runc echo \"--------------install cni plugins--------------\" wget https://github.com/containernetworking/plugins/releases/download/v${CniVersion}/cni-plugins-linux-amd64-v${CniVersion}.tgz rm -fr /opt/cni/bin mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v${CniVersion}.tgz echo \"--------------install nerdctl--------------\" wget https://github.com/containerd/nerdctl/releases/download/v${NerdctlVersion}/nerdctl-${NerdctlVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local/bin nerdctl-${NerdctlVersion}-linux-amd64.tar.gz echo \"--------------install crictl--------------\" wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v${CrictlVersion}/crictl-v${CrictlVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local/bin crictl-v${CrictlVersion}-linux-amd64.tar.gz cat \u003e /etc/crictl.yaml \u003c\u003c \\EOF runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 2 debug: false pull-image-on-create: false EOF # 启动containerd服务 systemctl daemon-reload systemctl enable contaienrd systemctl restart contaienrd 3. Containerd配置代理 由于节点到k8s官方仓库网络不通，或者设备处于内网，可以通过配置http_proxy代理的方式来拉取镜像。\nvi /lib/systemd/system/containerd.service # 添加代理环境变量 [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] Environment=\"HTTP_PROXY=http://squid:3128/\" # 添加环境变量代理 Environment=\"HTTPS_PROXY=http://squid:3128/\" # 添加环境变量代理 ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target # 重启服务 systemctl daemon-reload systemctl restart containerd 参考：\nhttps://github.com/containerd/containerd\nhttps://github.com/containerd/containerd/blob/main/docs/getting-started.md\nhttps://docs.docker.com/engine/install/ubuntu/\nhttps://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd\ncontainerd/containerd.service at main · containerd/containerd · GitHub\nGitHub - containerd/nerdctl: containerd ctl GitHub - kubernetes-sigs/cri-tools: CLI and validation tools for Kubelet Container Runtime Interface (CRI) .\n","categories":"","description":"","excerpt":"1. Ubuntu安装containerd 以下以Ubuntu为例\n说明：安装containerd与安装docker流程基本一致，差别在于不 …","ref":"/kubernetes-notes/runtime/containerd/install-containerd/","tags":["Containerd"],"title":"安装Containerd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/etcd/install/","tags":"","title":"部署Etcd集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/setup/installer/","tags":"","title":"部署k8s集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/storage/volume/","tags":"","title":"存储卷概念"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/principle/component/","tags":"","title":"核心组件"},{"body":"本文主要介绍如何通过Ollama和OpenWebUI来搭建一个本地私有化运行的大模型工具。私有化大模型的构建主要用于解决数据的安全性问题，对于大部分私有数据不适合通过外部的大模型网站来上传和分析。\n1. Ollama 与 OpenWebUI 介绍 1.1. Ollama简介 Ollama 是一个 本地运行的 AI 大模型管理工具，可以让你在本地 快速拉取、管理和运行 各种开源大语言模型（如 LLaMA、Mistral、deepseek 等），而无需依赖云端 API。它的主要特点包括：\n简易安装：支持 macOS、Linux 和 Windows（WSL）。 本地推理：在本地设备上直接运行 LLM，保护数据隐私。 模型管理：可以像使用 Docker 一样 ollama run llama2 轻松拉取和运行模型。 自定义模型：支持通过 Modelfile 进行微调和定制。 支持 API：可以通过 Python、Node.js 等语言调用 Ollama 提供的本地 REST API。 Ollama 适用于本地 AI 代理、嵌入式 AI 应用、隐私保护的智能助手等场景。你可以用它来运行大语言模型，而无需自己搭建复杂的推理环境。\n1.2. OpenWebUI简介 Open-WebUI 是一个 开源的 Web 用户界面，用于管理和使用本地或远程的大语言模型（LLM），比如 Ollama、OpenAI、Gemini 等。它的主要特点包括：\n友好的 Web 界面：提供 ChatGPT 类似的对话 UI，方便交互。 支持多种后端：可以连接 Ollama、OpenAI API、本地 LLM 等。 多用户支持：适用于团队协作。 对话历史管理：可保存和管理聊天记录。 插件和自定义功能：支持扩展，适用于不同应用场景。 它可以让本地 LLM 变得更加易用，适合个人、企业部署本地 AI 助手。\n2. 部署ollama 2.1. 脚本安装ollama curl -fsSL https://ollama.com/install.sh | sh 输出\n\u003e\u003e\u003e Installing ollama to /usr/local \u003e\u003e\u003e Downloading Linux amd64 bundle ######################################################################## 100.0% \u003e\u003e\u003e Creating ollama user... \u003e\u003e\u003e Adding ollama user to render group... \u003e\u003e\u003e Adding ollama user to video group... \u003e\u003e\u003e Adding current user to ollama group... \u003e\u003e\u003e Creating ollama systemd service... \u003e\u003e\u003e Enabling and starting ollama service... Created symlink /etc/systemd/system/default.target.wants/ollama.service -\u003e /etc/systemd/system/ollama.service. \u003e\u003e\u003e The Ollama API is now available at 127.0.0.1:11434. \u003e\u003e\u003e Install complete. Run \"ollama\" from the command line. WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode. 默认服务监听的地址为：127.0.0.1:11434\n2.2. 查看ollama服务状态 systemctl status ollama * ollama.service - Ollama Service Loaded: loaded (/etc/systemd/system/ollama.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2025-02-07 17:21:55 +08; 23s ago Main PID: 53472 (ollama) Tasks: 10 Memory: 10.3M CGroup: /system.slice/ollama.service `-53472 /usr/local/bin/ollama serve 查看ollama命令\n# ollama --help Large language model runner Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \"ollama [command] --help\" for more information about a command. 2.3. 拉取一个大模型 可以在 https://ollama.com/search 网站上，选择一个所需要的大模型，例如deepseek-r1:7b。\n# 下载指定的大模型，例如deepseek # ollama pull deepseek-r1:7b pulling manifest pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 387 B pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 148 B pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 487 B verifying sha256 digest writing manifest success # ollama list NAME ID SIZE MODIFIED deepseek-r1:7b 0a8c26691023 4.7 GB 12 minutes ago 2.4. 运行大模型 # ollama run deepseek-r1:7b \u003e\u003e\u003e 你是谁 \u003cthink\u003e \u003c/think\u003e 您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。 \u003e\u003e\u003e /bye # 查看正在运行的模型 # ollama ps NAME ID SIZE PROCESSOR UNTIL deepseek-r1:7b 0a8c26691023 5.5 GB 100% CPU 3 minutes from now 2.5. 修改ollama服务地址和目录 2.5.1. 修改ollama服务地址 ollama服务默认监听127.0.0.1, 如果要修改监听地址，则可以添加Environment=\"OLLAMA_HOST=0.0.0.0:11434\"。\nvi /etc/systemd/system/ollama.service [Unit] Description=Ollama Service After=network-online.target [Service] Environment=\"OLLAMA_HOST=0.0.0.0:11434\" # 增加环境变量 ExecStart=/usr/local/bin/ollama serve User=ollama Group=ollama Restart=always RestartSec=3 Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\" [Install] WantedBy=default.target # 重启服务 systemctl daemon-reload systemctl restart ollama systemctl status ollama 2.5.2. 修改ollama数据目录 参考：ollama/docs/faq.md\n默认存储目录\nmacOS: ~/.ollama/models Linux: /usr/share/ollama/.ollama/models Windows: C:\\Users\\%username%\\.ollama\\models 以linux系统为例，修改默认的存储目录：\ndir=\"/data/ollama/models\" # 创建目录并分配权限 mkdir -p /data/ollama/models sudo chown -R ollama:ollama /data/ollama/models # 添加环境变量OLLAMA_MODELS vi /etc/systemd/system/ollama.service [Service] Environment=\"OLLAMA_MODELS=/data/ollama/models\" # 重启服务 systemctl daemon-reload systemctl restart ollama systemctl status ollama # 迁移数据 cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models sudo chown -R ollama:ollama /data/ollama/models 3. 部署open-webui 3.1. 单独部署open-webui 如果已经部署了ollama服务，可以通过以下命令单独部署open-webui，修改OLLAMA_BASE_URL为ollama的服务地址。如果使用host-network，默认服务监听端口为8080。\ndocker run -d --network=host -e OLLAMA_BASE_URL=http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 环境变量\nOLLAMA_BASE_URL:http://OLLAMA_HOST:11434 : 设置ollama服务的地址 HF_HUB_OFFLINE: \"1\"：设置模型为离线的环境 ENABLE_OPENAI_API: \"false\"：设置关闭openai的接口 访问open-webui服务：\n访问http://服务器IP:8080，注册用户名密码然后登录。就可以使用本地的大模型服务。\n3.2. 部署open-webui和ollama服务 如果不想单独部署ollama，可以通过open-webui:ollama镜像，同时部署open-webui和ollama，两个服务集成在同一个镜像中。\n# 下载镜像 docker pull ghcr.io/open-webui/open-webui:ollama # 运行open-webui:ollama docker run -d -p 3000:8080 -v ollama:/root/.ollama -v ollama-open-webui:/app/backend/data --name ollama-open-webui --restart always ghcr.io/open-webui/open-webui:ollama 查看服务\n# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ghcr.io/open-webui/open-webui ollama 29d60b4958c8 4 days ago 8.02GB # docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3175fc20c608 ghcr.io/open-webui/open-webui:ollama \"bash start.sh\" 16 minutes ago Up 16 minutes (healthy) 0.0.0.0:3000-\u003e8080/tcp ollama-open-webui 登录容器下载大模型文件\n# 登录容器 # docker exec -it 3175fc20c608 bash # 下载指定的大模型 # ollama pull deepseek-r1:7b pulling manifest pulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB pulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 387 B pulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB pulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 148 B pulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 487 B verifying sha256 digest writing manifest success # ollama list NAME ID SIZE MODIFIED deepseek-r1:7b 0a8c26691023 4.7 GB 12 minutes ago 则可以访问所部属服务器的地址和端口来访问open-webui的服务。\n3.3. 构建本地知识库 3.3.1. 自定义文件分析 可以通过页面上传本地的知识库文件，让AI回答关于自定义文件中的内容。\n例如：我通过文件自定义了内容，提问张飞的电话号码，则可以通过文章中的内容来回答。\n其中自定义文档的内容如下：\n同样可以上传其他文件来构建一个本地大模型知识库。然后借助大模型来查询和分析数据内容。\n3.3.2. 本地化数据存储 其中open-webui的本地化数据存储在容器内的/app/backend/data/目录下。\n/app/backend/data# ls -l total 236 drwxr-xr-x 7 root root 4096 Feb 11 10:48 cache drwxr-xr-x 2 root root 4096 Feb 18 06:11 uploads drwxr-xr-x 3 root root 4096 Feb 18 06:11 vector_db -rw-r--r-- 1 root root 229376 Feb 18 06:38 webui.db # 可以从uploads目录看到上传的本地文件 /app/backend/data/uploads# cat 117e6f99-0657-40d1-ab6f-1bea81e78053_ollama-docs.md 张飞的电话号码是u987438274 曹操的电话号码是123456 关羽的电话号码是5352345 3.4. FAQ 1）open-webui页面无法选择模型 问题：\n当单独部署open-webui，可能会遇到open-webui页面无法选择模型具体的现象如下：\nopen-webui日志报错：\nINFO [open_webui.routers.ollama] get_all_models() ERROR [open_webui.routers.ollama] Connection error: Cannot connect to host 1.1.1.1:11434 ssl:default [Connect call failed ('1.1.1.1', 11434)] 原因：\n按官网命令使用端口映射的网络模式，如果OLLAMA_BASE_URL配置为127.0.0.1则访问不到单独部署的ollama服务，如果改用具体的ollama的IP也可能存在访问失败的问题。\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 解决方案：\ndocker网络模式改为host-network的网络模式\ndocker run -d --network=host -e OLLAMA_BASE_URL=http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 2）数据目录没权限permission denied 如果用户修改了ollama的models的存储目录，出现ollama服务重启失败，或者pull model数据报错\n# 修改ollama的model目录后ollama服务重启报错 Error: mkdir /data/ollama: permission denied # 迁移model数据后出现没权限，因为使用了root命令执行 cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models # ollama pull deepseek-r1:70b writing manifest Error: open /data/ollama/models/manifests/registry.ollama.ai/library/deepseek-r1/70b: permission denied 原因：\nollama默认使用的用户名是 ollama，因此需要给目录添加用户的权限，例如：目录创建和model文件迁移是通过root或其他用户执行的。\nsudo chown -R ollama:ollama /data/ollama/models 4. 总结 本文主要介绍了ollama和open-webui的部署，从而搭建一个本地化私有的大模型工具，所有的数据都存储在本地。可以通过上传文件来分析本地的数据，类似构建本地大模型知识库。\n不过本地大模型的响应速度依赖于大模型本身和本地的资源，包括cpu和gpu，没有gpu资源也可以运行。在资源较小的情况下，大模型回答问题的速度比较慢。如果完全需要离线的大模型分析数据，在资源受限的情况下需要再进一步做优化才能得到比较好的体验。\n参考：\nhttps://ollama.com/download/linux https://github.com/ollama/ollama/blob/main/docs/faq.md https://docs.openwebui.com/getting-started/quick-start https://github.com/open-webui/open-webui#troubleshooting ","categories":"","description":"","excerpt":"本文主要介绍如何通过Ollama和OpenWebUI来搭建一个本地私有化运行的大模型工具。私有化大模型的构建主要用于解决数据的安全性问题，对 …","ref":"/linux-notes/llm/build-ollama-openwebui/","tags":["大模型"],"title":"基于Ollama构建本地大模型"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/trouble-shooting/node/","tags":"","title":"节点问题"},{"body":"1. 镜像仓库的基本操作 1.1. 登录镜像仓库 docker login -u \u003cusername\u003e -p \u003cpassword\u003e \u003cregistry-addr\u003e 1.2. 拉取镜像 docker pull https://registry.xxx.com/dev/nginx:latest 1.3. 推送镜像 docker push https://registry.xxx.com/dev/nginx:latest 1.4. 重命名镜像 docker tag \u003cold-image\u003e \u003cnew-image\u003e 2. docker.xxx.com镜像仓库 使用docker.xxx.com镜像仓库。\n2.1. 所有节点配置insecure-registries #cat /etc/docker/daemon.json { \"data-root\": \"/data/docker\", \"debug\": false, \"insecure-registries\": [ ... \"docker.xxx.com:8080\" ], ... } 2.2. 所有节点配置/var/lib/kubelet/config.json 具体参考：configuring-nodes-to-authenticate-to-a-private-registry\n在某个节点登录docker.xxx.com:8080镜像仓库，会更新 $HOME/.docker/config.json 检查$HOME/.docker/config.json是否有该镜像仓库的auth信息。 #cat ~/.docker/config.json { \"auths\": { \"docker.xxx.com:8080\": { \"auth\": \"\u003c此处为凭证信息\u003e\" } }, \"HttpHeaders\": { \"User-Agent\": \"Docker-Client/18.09.9 (linux)\" } } 将$HOME/.docker/config.json拷贝到所有的Node节点上的/var/lib/kubelet/config.json。 # 获取所有节点的IP nodes=$(kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type==\"ExternalIP\")]}{.address} {end}') # 拷贝到所有节点 for n in $nodes; do scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json; done 2.3. 创建docker.xxx.com镜像的pod 指定镜像为：docker.xxx.com:8080/public/2048:latest\n完整pod.yaml\napiVersion: apps/v1beta2 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" generation: 1 labels: k8s-app: dockeroa-hub qcloud-app: dockeroa-hub name: dockeroa-hub namespace: test spec: progressDeadlineSeconds: 600 replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dockeroa-hub qcloud-app: dockeroa-hub strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: k8s-app: dockeroa-hub qcloud-app: dockeroa-hub spec: containers: - image: docker.xxx.com:8080/public/2048:latest imagePullPolicy: Always name: game resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always nodeName: 192.168.1.1 schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 查看pod状态\n#kgpoowide -n game NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES docker-oa-757bbbddb5-h6j7m 1/1 Running 0 14m 192.168.2.51 192.168.1.1 \u003cnone\u003e \u003cnone\u003e docker-oa-757bbbddb5-jp5dw 1/1 Running 0 14m 192.168.1.32 192.168.1.2 \u003cnone\u003e \u003cnone\u003e docker-oa-757bbbddb5-nlw9f 1/1 Running 0 14m 192.168.0.43 192.168.1.3 \u003cnone\u003e \u003cnone\u003e 参考：\nhttps://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-registry ","categories":"","description":"","excerpt":"1. 镜像仓库的基本操作 1.1. 登录镜像仓库 docker login -u \u003cusername\u003e -p \u003cpassword\u003e …","ref":"/kubernetes-notes/operation/registry/config-private-registry/","tags":["Kubernetes"],"title":"配置私有镜像仓库"},{"body":"1. 安装etcdadm 在Releases · kubernetes-sigs/etcdadm · GitHub中选择需要部署的版本，示例如下：\nwget https://github.com/kubernetes-sigs/etcdadm/releases/download/v0.1.5/etcdadm-linux-amd64 mv etcdadm-linux-amd64 /usr/bin/etcdadm chmod +x /usr/bin/etcdadm 2. 部署etcd集群 2.1. init etcd的版本可以在 Releases · etcd-io/etcd · GitHub 中查询。\netcdadm init --name \u003cnode1\u003e --version=3.5.4 2.2. 上传证书到其他机器 # 登录node2 node3 mkdir -p /etc/etcd/pki # 将node1的/etc/etcd/pki/ca.* 拷贝到node2 node3 /etc/etcd/pki/ scp /etc/etcd/pki/ca.* node2:/etc/etcd/pki/ 2.3. join etcdadm join https://\u003cnode1\u003e:2379 --name=\u003cnode2\u003e --version=3.5.4 etcdadm join https://\u003cnode1\u003e:2379 --name=\u003cnode3\u003e --version=3.5.4 3. 查看集群状态 设置etcdctl环境变量\n# 添加endpoint cat \u003e\u003e /etc/etcd/etcdctl.env \u003c\u003c EOF export ETCDCTL_ENDPOINTS=node1:2379,node2:2379,node2:2379 EOF # 拷贝命令脚本到/usr/bin/ cp /opt/bin/etcdctl /opt/bin/etcdctl.sh /usr/bin/ 查看集群状态\n$ etcdctl.sh endpoint status -w table +--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | node1:2379 | 5fe84cb4a0ef4e69 | 3.5.4 | 20 kB | true | false | 3 | 13 | 13 | | | node2:2379 | cb8d48da0ea9b8c0 | 3.5.4 | 20 kB | false | false | 3 | 13 | 13 | | | node3:2379 | fafa80c55eebeffa | 3.5.4 | 20 kB | false | false | 3 | 13 | 13 | | +--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 4. Etcd启动配置文件 systemd service\n# cat /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos/etcd Conflicts=etcd-member.service Conflicts=etcd2.service [Service] EnvironmentFile=/etc/etcd/etcd.env ExecStart=/opt/bin/etcd Type=notify TimeoutStartSec=0 Restart=on-failure RestartSec=5s LimitNOFILE=65536 Nice=-10 IOSchedulingClass=best-effort IOSchedulingPriority=2 MemoryLow=200M [Install] WantedBy=multi-user.target /etc/etcd/etcd.env\nETCD_NAME=etcd01 # Initial cluster configuration ETCD_INITIAL_CLUSTER=etcd01=https://node1:2380 ETCD_INITIAL_CLUSTER_TOKEN=88ad6def ETCD_INITIAL_CLUSTER_STATE=new # Peer configuration ETCD_INITIAL_ADVERTISE_PEER_URLS=https://node1:2380 ETCD_LISTEN_PEER_URLS=https://node1:2380 ETCD_CLIENT_CERT_AUTH=true ETCD_PEER_CERT_FILE=/etc/etcd/pki/peer.crt ETCD_PEER_KEY_FILE=/etc/etcd/pki/peer.key ETCD_PEER_TRUSTED_CA_FILE=/etc/etcd/pki/ca.crt # Client/server configuration ETCD_ADVERTISE_CLIENT_URLS=https://node1:2379 ETCD_LISTEN_CLIENT_URLS=https://node1:2379,https://127.0.0.1:2379 ETCD_PEER_CLIENT_CERT_AUTH=true ETCD_CERT_FILE=/etc/etcd/pki/server.crt ETCD_KEY_FILE=/etc/etcd/pki/server.key ETCD_TRUSTED_CA_FILE=/etc/etcd/pki/ca.crt # Other ETCD_DATA_DIR=/var/lib/etcd ETCD_STRICT_RECONFIG_CHECK=true GOMAXPROCS=48 参考：\nGitHub - kubernetes-sigs/etcdadm\nClustering Guide | etcd\n","categories":"","description":"","excerpt":"1. 安装etcdadm 在Releases · kubernetes-sigs/etcdadm · GitHub中选择需要部署的版本，示例 …","ref":"/kubernetes-notes/etcd/install/install-etcd-by-etcdadm/","tags":"","title":"使用etcdadm部署Etcd集群"},{"body":" 本文为基于kubeadm搭建生产环境级别高可用的k8s集群。\n1. 环境准备 1.0. master硬件配置 参考：\nMaster节点规格\n高可靠推荐配置 - 容器服务 ACK - 阿里云\nKubernetes集群Master节点上运行着etcd、kube-apiserver、kube-controller等核心组件，对于Kubernetes集群的稳定性有着至关重要的影响，对于生产环境的集群，必须慎重选择Master规格。Master规格跟集群规模有关，集群规模越大，所需要的Master规格也越高。\n说明 ：可从多个角度衡量集群规模，例如节点数量、Pod数量、部署频率、访问量。这里简单的认为集群规模就是集群里的节点数量。\n对于常见的集群规模，可以参见如下的方式选择Master节点的规格（对于测试环境，规格可以小一些。下面的选择能尽量保证Master负载维持在一个较低的水平上）。\n节点规模 Master规格 磁盘 1~5个节点 4核8 GB（不建议2核4 GB） 6~20个节点 4核16 GB 21~100个节点 8核32 GB 100~200个节点 16核64 GB 1000个节点 32核128GB 1T SSD 注意事项：\n由于Etcd的性能瓶颈，Etcd的数据存储盘尽量选择SSD磁盘。\n为了实现多机房容灾，可将三台master分布在一个可用区下三个不同机房。（机房之间的网络延迟在10毫秒及以下级别）\n申请LB来做master节点的负载均衡实现高可用，LB作为apiserver的访问地址。\n1.1. 设置防火墙端口策略 生产环境设置k8s节点的iptables端口访问规则。\n1.1.1. master节点端口配置 协议 方向 端口范围 目的 使用者 TCP 入站 6443 Kubernetes API server 所有 TCP 入站 2379-2380 etcd server client API kube-apiserver, etcd TCP 入站 10250 Kubelet API 自身, 控制面 TCP 入站 10259 kube-scheduler 自身 TCP 入站 10257 kube-controller-manager 自身 1.1.2. worker节点端口配置 协议 方向 端口范围 目的 使用者 TCP 入站 10250 Kubelet API 自身, 控制面 TCP 入站 30000-32767 NodePort Services 所有 添加防火墙iptables规则\nmaster节点开放6443、2379、2380端口。\niptables -A INPUT -p tcp -m multiport --dports 6443,2379,2380,10250 -j ACCEPT 1.2. 关闭​​swap​​分区 [root@master ~]#swapoff -a [root@master ~]# [root@master ~]# free -m total used free shared buff/cache available Mem: 976 366 135 6 474 393 Swap: 0 0 0 # swap 一栏为0，表示已经关闭了swap 1.3. 开启br_netfilter和bridge-nf-call-iptables 参考：https://imroc.cc/post/202105/why-enable-bridge-nf-call-iptables/\n# 设置加载br_netfilter模块 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 开启bridge-nf-call-iptables ，设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system 2. 安装容器运行时 在所有主机上安装容器运行时，推荐使用containerd为runtime。以下分别是containerd与docker的安装命令。\n2.1. Containerd 1、参考：安装containerd\n# for ubuntu apt install -y containerd.io 2、生成默认配置\ncontainerd config default \u003e /etc/containerd/config.toml 3、修改CgroupDriver为systemd\nk8s官方推荐使用systemd类型的CgroupDriver。\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true 4、重启containerd\nsystemctl restart containerd 2.2. Docker # for ubuntu apt install -y docker.io 官方建议配置cgroupdriver为systemd。\n# 修改docker进程管理器 vi /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } systemctl daemon-reload \u0026\u0026 systemctl restart docker docker info | grep -i cgroup 2.3. Container Socket 运行时 Unix 域套接字 Containerd unix:///var/run/containerd/containerd.sock CRI-O unix:///var/run/crio/crio.sock Docker Engine (使用 cri-dockerd) unix:///var/run/cri-dockerd.sock 3. 安装kubeadm,kubelet,kubectl 安装脚本可参考仓库：https://github.com/huweihuang/kubeadm-scripts.git\n在所有主机上安装kubeadm，kubelet，kubectl。最好版本与需要安装的k8s的版本一致。\n# 以Ubuntu系统为例 # 安装仓库依赖 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl # use google registry sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list # or use aliyun registry curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - tee /etc/apt/sources.list.d/kubernetes.list \u003c\u003cEOF deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF # 安装指定版本的kubeadm, kubelet, kubectl apt-get update apt-get install -y kubelet=1.24.2-00 kubeadm=1.24.2-00 kubectl=1.24.2-00 # 查询有哪些版本 apt-cache madison kubeadm 离线下载安装\n#!/bin/bash Version=${Version:-1.24.2} wget https://dl.k8s.io/release/v${Version}/bin/linux/amd64/kubeadm wget https://dl.k8s.io/release/v${Version}/bin/linux/amd64/kubelet wget https://dl.k8s.io/release/v${Version}/bin/linux/amd64/kubectl chmod +x kubeadm kubelet kubectl cp kubeadm kubelet kubectl /usr/bin/ # add kubelet serivce cat \u003e /lib/systemd/system/kubelet.service \u003c\u003c EOF [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/home/ Wants=network-online.target After=network-online.target [Service] ExecStart=/usr/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target EOF mkdir -p /etc/systemd/system/kubelet.service.d cat \u003e /etc/systemd/system/kubelet.service.d/10-kubeadm.conf \u003c\u003c \\EOF # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS EOF systemctl daemon-reload systemctl enable kubelet systemctl restart kubelet 4. 配置kubeadm config 参考：\nkubeadm Configuration (v1beta3) | Kubernetes kubeadm Configuration (v1beta2) | Kubernetes 4.1. 配置项说明 4.1.1. 配置类型 kubeadm config支持以下几类配置。\napiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration 可以使用以下命令打印init和join的默认配置。\nkubeadm config print init-defaults kubeadm config print join-defaults 4.1.2. Init配置 kubeadm init配置中只有InitConfiguration 和 ClusterConfiguration 是必须的。\nInitConfiguration:\napiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration bootstrapTokens: ... nodeRegistration: ... bootstrapTokens nodeRegistration criSocket：runtime的socket name：节点名称 localAPIEndpoint advertiseAddress：apiserver的广播IP bindPort：k8s控制面安全端口 ClusterConfiguration:\napiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration networking: ... etcd: ... apiServer: extraArgs: ... extraVolumes: ... ... networking:\npodSubnet：Pod CIDR范围 serviceSubnet： service CIDR范围 dnsDomain etcd:\ndataDir：Etcd的数据存储目录 apiserver\ncertSANs：设置额外的apiserver的域名签名证书 imageRepository：镜像仓库\ncontrolPlaneEndpoint：控制面LB的域名\nkubernetesVersion：k8s版本\n4.2. Init配置示例 在master节点生成默认配置，并修改配置参数。\nkubeadm config print init-defaults \u003e kubeadm-config.yaml 修改配置内容\napiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 # 修改为apiserver的IP 或者去掉localAPIEndpoint则会读取默认IP。 bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: node taints: null --- apiServer: certSANs: - lb.k8s.domain # 添加额外的apiserver的域名 - \u003cvip/lb_ip\u003e timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} # 默认为coredns etcd: local: dataDir: /data/etcd # 修改etcd的存储盘目录 imageRepository: k8s.gcr.io # 修改镜像仓库地址 controlPlaneEndpoint: lb.k8s.domain # 修改控制面域名 kind: ClusterConfiguration kubernetesVersion: 1.24.0 # k8s 版本 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # 设置pod的IP范围 scheduler: {} --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd # 设置为systemd 安装完成后可以查看kubeadm config\nkubectl get cm -n kube-system kubeadm-config -oyaml 5. 安装Master控制面 提前拉取镜像：\nkubeadm config images pull 5.1. 安装master sudo kubeadm init --config kubeadm-config.yaml --upload-certs --node-name \u003cnodename\u003e 部署参数说明：\n--control-plane-endpoint：指定控制面(kube-apiserver)的IP或DNS域名地址。\n--apiserver-advertise-address：kube-apiserver的IP地址。\n--pod-network-cidr：pod network范围，控制面会自动给每个节点分配CIDR。\n--service-cidr：service的IP范围，default \"10.96.0.0/12\"。\n--kubernetes-version：指定k8s的版本。\n--image-repository：指定k8s镜像仓库地址。\n--upload-certs ：标志用来将在所有控制平面实例之间的共享证书上传到集群。\n--node-name：hostname-override，作为节点名称。\n执行完毕会输出添加master和添加worker的命令如下：\n... You can now join any number of control-plane node by running the following command on each as a root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 5.2. 添加其他master 添加master和添加worker的差别在于添加master多了--control-plane 参数来表示添加类型为master。\nkubeadm join \u003ccontrol-plane-endpoint\u003e:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e \\ --control-plane --certificate-key \u003ccertificate-key\u003e \\ --node-name \u003cnodename\u003e 6. 添加Node节点 kubeadm join \u003ccontrol-plane-endpoint\u003e:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e \\ --cri-socket /run/containerd/containerd.sock \\ --node-name \u003cnodename\u003e 7. 安装网络插件 ## 如果安装之后node的状态都改为ready，即为成功 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kubectl apply -f ./kube-flannel.yml kubectl get nodes 如果Pod CIDR的网段不是10.244.0.0/16，则需要加flannel配置中的网段更改为与Pod CIDR的网段一致。\n7.1. 问题 Warning FailedCreatePodSandBox 4m6s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"300d9b570cc1e23b6335c407b8e7d0ef2c74dc2fe5d7a110678c2dc919c62edf\": plugin type=\"flannel\" failed (add): failed to delegate add: failed to set bridge addr: \"cni0\" already has an IP address different from 10.244.3.1/24 原因：\n宿主机节点有cni0网卡，且网卡的IP段与flannel的CIDR网段不同，因此需要删除该网卡，让其重建。\n查看cni0网卡\n# ifconfig cni0 |grep -w inet inet 10.244.5.1 netmask 255.255.255.0 broadcast 10.244.116.255 查看flannel配置\n# cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.116.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true 发现cni0 IP与FLANNEL_SUBNET网段不一致，因此删除cni0重建。\n解决：\nifconfig cni0 down ip link delete cni0 8. 部署dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml 镜像： kubernetesui/dashboard:v2.5.0\n默认端口：8443\n登录页面需要填入token或kubeconfig\n参考：dashboard/creating-sample-user\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 创建用户\nkubectl -n kubernetes-dashboard create token admin-user 9. 重置部署 # kubeadm重置 kubeadm reset # 清空数据目录 rm -fr /data/etcd rm -fr /etc/kubernetes rm -fr ~/.kube/ 删除flannel\nifconfig cni0 down ip link delete cni0 ifconfig flannel.1 down ip link delete flannel.1 rm -rf /var/lib/cni/ rm -f /etc/cni/net.d/* 10. 问题排查 10.1. kubeadm token过期 问题描述:\n添加节点时报以下错误：\n[discovery] The cluster-info ConfigMap does not yet contain a JWS signature for token ID \"abcdef\", will try again 原因：token过期，初始化token后会在24小时候会被master删除。\n解决办法：\n# 重新生成token kubeadm token create --print-join-command kubeadm token list # kubeadm token create oumnnc.aqlxuvdbntlvzoiv # 重新生成hash openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 基于新生成的token重新添加节点。\n10.2. 修改kubeadm join的master IP或端口 kubeadm join命令会去kube-public命名空间获取名为cluster-info的ConfigMap。如果需要修改kubeadm join使用的master的IP或端口，则需要修改cluster-info的configmap。\n# 查看cluster-info kubectl -n kube-public get configmaps cluster-info -o yaml # 修改cluster-info kubectl -n kube-public edit configmaps cluster-info 修改配置文件中的server字段\nclusters: - cluster: certificate-authority-data: xxx server: https://lb.k8s.domain:36443 name: \"\" 执行kubeadm join的命令时指定新修改的master地址。\n10.3. conntrack not found [preflight] Some fatal errors occurred: [ERROR FileExisting-conntrack]: conntrack not found in system path 解决方法：\napt -y install conntrack 10.4. Kubelet: unable to determine runtime API version Error: failed to run Kubelet: unable to determine runtime API version: rpc error:code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix: missing address\" 解决方法：\n检查kubelet的启动参数，可以用二进制直接添加参数debug\n# 查看启动参数是否遗漏，比如10-kubeadm.conf 文件参数缺失 systemctl cat --no-pager kubelet cat /lib/systemd/system/kubelet.service cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 参考：\n利用 kubeadm 创建高可用集群 | Kubernetes 使用 kubeadm 创建集群 | Kubernetes 高可用拓扑选项 | Kubernetes kubeadm init | Kubernetes v1.24.2|kubeadm|v1beta3 Installing kubeadm | Kubernetes Ports and Protocols | Kubernetes 容器运行时 | Kubernetes https://github.com/Mirantis/cri-dockerd 配置 cgroup 驱动|Kubernetes GitHub: flannel is a network fabric for containers 部署和访问 Kubernetes 仪表板（Dashboard） | Kubernetes ","categories":"","description":"","excerpt":" 本文为基于kubeadm搭建生产环境级别高可用的k8s集群。\n1. 环境准备 1.0. master硬件配置 参考：\nMaster …","ref":"/kubernetes-notes/setup/installer/install-k8s-by-kubeadm/","tags":["Kubernetes"],"title":"使用kubeadm部署生产环境kubernetes集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/summary/","tags":"","title":"语言概述"},{"body":"源码整体结构图 ","categories":"","description":"","excerpt":"源码整体结构图 ","ref":"/k8s-source-code-analysis/kube-controller-manager/controller-manager-xmind/","tags":"","title":"controller-manager 源码思维导图"},{"body":"源码整体结构图 ","categories":"","description":"","excerpt":"源码整体结构图 ","ref":"/k8s-source-code-analysis/kube-scheduler/scheduler-xmind/","tags":"","title":"kube-scheduler 源码思维导图"},{"body":"源码整体结构图 ","categories":"","description":"","excerpt":"源码整体结构图 ","ref":"/k8s-source-code-analysis/kubelet/kubelet-xmind/","tags":"","title":"kubelet 源码思维导图"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/paas/","tags":"","title":"云原生体系"},{"body":"1. NodeSelector 1.1. 概念 如果需要限制Pod到指定的Node上运行，则可以给Node打标签并给Pod配置NodeSelector。\n1.2. 使用方式 1.2.1. 给Node打标签 # get node的name kubectl get nodes # 设置Label kubectl label nodes \u003cnode-name\u003e \u003clabel-key\u003e=\u003clabel-value\u003e # 例如 kubectl label nodes node-1 disktype=ssd # 查看Node的Label kubectl get nodes --show-labels # 删除Node的label kubectl label node \u003cnode-name\u003e \u003clabel-key\u003e- 1.2.2. 给Pod设置NodeSelector apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd # 对应Node的Label 1.3. 亲和性（Affinity）和反亲和性（Anti-affinity） 待补充\n2. Taint 和 Toleration 2.1. 概念 nodeSelector可以通过打标签的形式让Pod被调度到指定的Node上，Taint 则相反，它使节点能够排斥一类特定的Pod，除非Pod被指定了toleration的标签。（taint即污点，Node被打上污点；只有容忍[toleration]这些污点的Pod才可能被调度到该Node）。\n2.2. 使用方式 2.2.1. kubectl taint # 给节点增加一个taint，它的key是\u003ckey\u003e，value是\u003cvalue\u003e，effect是NoSchedule。 kubectl taint nodes \u003cnode_name\u003e \u003ckey\u003e=\u003cvalue\u003e:NoSchedule 只有拥有和这个taint相匹配的toleration的pod才能够被分配到 node_name 这个节点。\n例如，在 PodSpec 中定义 pod 的 toleration：\ntolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\" tolerations: - key: \"key\" operator: \"Exists\" effect: \"NoSchedule\" 2.2.2. 匹配规则： 一个 toleration 和一个 taint 相“匹配”是指它们有一样的 key 和 effect ，并且：\n如果 operator 是 Exists （此时 toleration 不能指定 value） 如果 operator 是 Equal ，则它们的 value 应该相等 特殊情况：\n如果一个 toleration 的 key 为空且 operator 为 Exists ，表示这个 toleration 与任意的 key 、 value 和 effect 都匹配，即这个 toleration 能容忍任意 taint。\ntolerations: - operator: \"Exists\" 如果一个 toleration 的 effect 为空，则 key 值与之相同的相匹配 taint 的 effect 可以是任意值。\ntolerations: - key: \"key\" operator: \"Exists\" 一个节点可以设置多个taint，一个pod也可以设置多个toleration。Kubernetes 处理多个 taint 和 toleration 的过程就像一个过滤器：从一个节点的所有 taint 开始遍历，过滤掉那些 pod 中存在与之相匹配的 toleration 的 taint。余下未被过滤的 taint 的 effect 值决定了 pod 是否会被分配到该节点，特别是以下情况：\n如果未被过滤的 taint 中存在一个以上 effect 值为 NoSchedule 的 taint，则 Kubernetes 不会将 pod 分配到该节点。 如果未被过滤的 taint 中不存在 effect 值为 NoSchedule 的 taint，但是存在 effect 值为 PreferNoSchedule 的 taint，则 Kubernetes 会尝试将 pod 分配到该节点。 如果未被过滤的 taint 中存在一个以上 effect 值为 NoExecute 的 taint，则 Kubernetes 不会将 pod 分配到该节点（如果 pod 还未在节点上运行），或者将 pod 从该节点驱逐（如果 pod 已经在节点上运行）。 2.2.3. effect的类型 NoSchedule：只有拥有和这个 taint 相匹配的 toleration 的 pod 才能够被分配到这个节点。\nPreferNoSchedule：系统会尽量避免将 pod 调度到存在其不能容忍 taint 的节点上，但这不是强制的。\nNoExecute ：任何不能忍受这个 taint 的 pod 都会马上被驱逐，任何可以忍受这个 taint 的 pod 都不会被驱逐。Pod可指定属性 tolerationSeconds 的值，表示pod 还能继续在节点上运行的时间。\ntolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" tolerationSeconds: 3600 2.3. 使用场景 2.3.1. 专用节点 kubectl taint nodes \u003cnodename\u003e dedicated=\u003cgroupName\u003e:NoSchedule 先给Node添加taint，然后给Pod添加相对应的 toleration，则该Pod可调度到taint的Node，也可调度到其他节点。\n如果想让Pod只调度某些节点且某些节点只接受对应的Pod，则需要在Node上添加Label（例如：dedicated=groupName），同时给Pod的nodeSelector添加对应的Label。\n2.3.2. 特殊硬件节点 如果某些节点配置了特殊硬件（例如CPU），希望不使用这些特殊硬件的Pod不被调度该Node，以便保留必要资源。即可给Node设置taint和label，同时给Pod设置toleration和label来使得这些Node专门被指定Pod使用。\n# kubectl taint kubectl taint nodes nodename special=true:NoSchedule # 或者 kubectl taint nodes nodename special=true:PreferNoSchedule 2.3.3. 基于taint驱逐 effect 值 NoExecute ，它会影响已经在节点上运行的 pod，即根据策略对Pod进行驱逐。\n如果 pod 不能忍受effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐 如果 pod 能够忍受effect 值为 NoExecute 的 taint，但是在 toleration 定义中没有指定 tolerationSeconds，则 pod 还会一直在这个节点上运行。 如果 pod 能够忍受effect 值为 NoExecute 的 taint，而且指定了 tolerationSeconds，则 pod 还能在这个节点上继续运行这个指定的时间长度。 参考：\nhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/\nhttps://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n","categories":"","description":"","excerpt":"1. NodeSelector 1.1. 概念 如果需要限制Pod到指定的Node上运行，则可以给Node打标签并给Pod配 …","ref":"/kubernetes-notes/operation/node/nodeselector-and-taint/","tags":["Kubernetes"],"title":"指定节点调度与隔离"},{"body":"资源配额（ResourceQuota） ResourceQuota对象用来定义某个命名空间下所有资源的使用限额，其实包括：\n计算资源的配额 存储资源的配额 对象数量的配额 如果集群的总容量小于命名空间的配额总额，可能会产生资源竞争。这时会按照先到先得来处理。 资源竞争和配额的更新都不会影响已经创建好的资源。\n1. 启动资源配额 Kubernetes 的众多发行版本默认开启了资源配额的支持。当在apiserver的--admission-control配置中添加ResourceQuota参数后，便启用了。 当一个命名空间中含有ResourceQuota对象时，资源配额将强制执行。\n2. 计算资源配额 可以在给定的命名空间中限制可以请求的计算资源（compute resources）的总量。\n资源名称 描述 cpu 非终止态的所有pod, cpu请求总量不能超出此值。 limits.cpu 非终止态的所有pod， cpu限制总量不能超出此值。 limits.memory 非终止态的所有pod, 内存限制总量不能超出此值。 memory 非终止态的所有pod, 内存请求总量不能超出此值。 requests.cpu 非终止态的所有pod, cpu请求总量不能超出此值。 requests.memory 非终止态的所有pod, 内存请求总量不能超出此值。 3. 存储资源配额 可以在给定的命名空间中限制可以请求的存储资源（storage resources）的总量。\n资源名称 描述 requests.storage 所有PVC, 存储请求总量不能超出此值。 persistentvolumeclaims 命名空间中可以存在的PVC（persistent volume claims）总数。 .storageclass.storage.k8s.io/requests.storage 和该存储类关联的所有PVC, 存储请求总和不能超出此值。 .storageclass.storage.k8s.io/persistentvolumeclaims 和该存储类关联的所有PVC，命名空间中可以存在的PVC（persistent volume claims）总数。 4. 对象数量的配额 资源名称 描述 congfigmaps 命名空间中可以存在的配置映射的总数。 persistentvolumeclaims 命名空间中可以存在的PVC总数。 pods 命名空间中可以存在的非终止态的pod总数。如果一个pod的status.phase 是 Failed, Succeeded, 则该pod处于终止态。 replicationcontrollers 命名空间中可以存在的rc总数。 resourcequotas 命名空间中可以存在的资源配额（resource quotas）总数。 services 命名空间中可以存在的服务总数量。 services.loadbalancers 命名空间中可以存在的服务的负载均衡的总数量。 services.nodeports 命名空间中可以存在的服务的主机接口的总数量。 secrets 命名空间中可以存在的secrets的总数量。 例如：可以定义pod的限额来避免某用户消耗过多的Pod IPs。\n5. 限额的作用域 作用域 描述 Terminating 匹配 spec.activeDeadlineSeconds \u003e= 0 的pod NotTerminating 匹配 spec.activeDeadlineSeconds is nil 的pod BestEffort 匹配具有最佳服务质量的pod NotBestEffort 匹配具有非最佳服务质量的pod 6. request和limit 当分配计算资源时，每个容器可以为cpu或者内存指定一个请求值和一个限度值。可以配置限额值来限制它们中的任何一个值。 如果指定了requests.cpu 或者 requests.memory的限额值，那么就要求传入的每一个容器显式的指定这些资源的请求。如果指定了limits.cpu或者limits.memory，那么就要求传入的每一个容器显式的指定这些资源的限度。\n7. 查看和设置配额 # 创建namespace $ kubectl create namespace myspace # 创建resourcequota $ cat \u003c\u003cEOF \u003e compute-resources.yaml apiVersion: v1 kind: ResourceQuota metadata: name: compute-resources spec: hard: pods: \"4\" requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi EOF $ kubectl create -f ./compute-resources.yaml --namespace=myspace # 查询resourcequota $ kubectl get quota --namespace=myspace NAME AGE compute-resources 30s # 查询resourcequota的详细信息 $ kubectl describe quota compute-resources --namespace=myspace Name: compute-resources Namespace: myspace Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi pods 0 4 requests.cpu 0 1 requests.memory 0 1Gi 8. 配额和集群容量 资源配额对象与集群容量无关，它们以绝对单位表示。即增加节点的资源并不会增加已经配置的namespace的资源。\n参考文章：\nhttps://kubernetes.io/docs/concepts/policy/resource-quotas/ ","categories":"","description":"","excerpt":"资源配额（ResourceQuota） ResourceQuota对象用来定义某个命名空间下所有资源的使用限额，其实包括：\n计算资源的配额  …","ref":"/kubernetes-notes/resource/resource-quota/","tags":["Kubernetes"],"title":"资源配额"},{"body":"1. VXLAN简介 VXLAN（Virtual Extensible LAN）是一种网络虚拟化技术，旨在解决传统二层网络扩展的局限性，尤其是在数据中心大规模部署中。它通过隧道技术将二层以太网帧封装在三层UDP包中，实现了跨三层网络的二层网络延展。\n1.1. VXLAN的基本概念 目的：解决二层网络扩展的问题，例如VLAN的数量限制（传统VLAN ID 只能支持4096（2的12次方）个）。 封装协议：VXLAN将二层以太网帧封装为UDP数据包（即VXLAN隧道）。 VXLAN网络标识： VXLAN使用24位的VXLAN网络标识（VNI，Virtual Network Identifier），支持多达16,777,216（2的24次方）个虚拟网络。 每个VNI对应一个虚拟的二层广播域（类似于传统的VLAN）。 1.2.VXLAN应用场景 多租户数据中心：为不同租户提供逻辑隔离的虚拟网络。 混合云和跨数据中心连接：扩展二层网络到不同位置的数据中心。 容器网络：在Kubernetes等平台上，用VXLAN构建跨节点的Pod网络。 1.3. VXLAN的优点 可扩展性： 支持大量虚拟网络（16M）。 跨三层网络扩展二层网络，适用于大规模数据中心。 网络隔离：通过VNI实现网络隔离，适合多租户场景。 灵活性：VXLAN在IP网络中运行，不依赖底层的物理拓扑。 1.4. VXLAN的局限性 性能开销：封装和解封装增加了CPU负载，尤其是在软件实现中。 复杂性：需要额外配置VTEP和三层网络，维护成本较高。 MTU问题：VXLAN封装增加了数据包长度，可能需要调整网络的MTU（通常为1600字节或更大）。 2. VXLAN的原理 2.1. VXLAN的关键组成 VTEP（VXLAN Tunnel Endpoint） ： VTEP是VXLAN隧道的起点和终点，用于封装和解封装VXLAN数据包。 通常运行在物理交换机或虚拟机主机的网卡上。 包括两个接口： 本地接口：连接到二层网络。 隧道接口：连接到三层网络。 VXLAN头 ： VXLAN头插入到原始以太网帧和UDP头之间。 VXLAN头包含VNI等信息，用于区分不同的虚拟网络。 UDP头 ： VXLAN数据包封装在UDP中，以便通过三层网络传输。 默认使用UDP端口号4789。 确保iptables规则中该UDP端口是放开的。 2.2. VXLAN的工作原理 VXLAN通过以下步骤实现跨三层网络的二层通信：\n1. 数据包封装\nVTEP捕获本地虚拟机（VM）的以太网帧。 VTEP在帧上封装： 添加VXLAN头，用于标识VNI。 添加UDP头，便于三层网络传输。 添加外层IP头和MAC头，用于在三层网络中寻址。 2. 数据包传输\n封装后的数据包通过三层网络传输到目标VTEP。 传输过程中使用三层网络的路由功能，可以跨越不同的子网。 3. 数据包解封装\n目标VTEP接收到VXLAN数据包后，解析外层IP头。 检查VNI，将数据包解封装回原始二层以太网帧。 将解封装后的帧转发到目标虚拟机。 3. VLAN 和 VXLAN 的区别 特性 VLAN VXLAN 定义 二层网络分段技术，通过 802.1Q 标准实现。 二层覆盖网络技术，通过三层网络扩展二层网络。 标准 IEEE 802.1Q IETF RFC 7348 隔离方式 通过 12 位 VLAN ID 标记帧，实现二层广播域隔离。 通过 24 位 VXLAN ID（VNI）实现隔离。 支持网络数量 最多 4096 个 VLAN 超过 1600 万个虚拟网络 封装方式 在以太网帧中添加 4 字节 VLAN Tag。 在以太网帧外封装 UDP，增加 IP 和 VXLAN 标头。 网络边界 限于局域网，依赖物理拓扑。 基于三层网络，支持跨地域、跨数据中心连接。 性能 性能高，硬件交换机对 VLAN 支持成熟。 封装和解封装增加开销，但灵活性更强。 适用场景 中小型网络，局域网内的简单隔离需求。 大规模云环境，多租户数据中心，跨地域网络。 复杂性 配置简单，维护容易。 配置复杂，需要支持 VXLAN 的设备。 硬件依赖 广泛支持，几乎所有交换机都支持。 需要支持 VXLAN 的设备或软件实现。 广播域扩展 广播域较大，不适合大规模网络。 通过三层网络扩展二层广播域。 4. Flannel VXLAN 的基本原理 Flannel 是 Kubernetes 中常用的网络插件之一，用于实现容器跨节点的网络通信。它支持多种网络后端，其中 VXLAN 后端 是一种常用的选择，利用 VXLAN 隧道实现不同节点的容器网络互通。\nFlannel 使用 VXLAN 创建一个虚拟的二层网络，把位于不同节点上的容器子网连接起来。这些子网统一组成一个逻辑上的扁平网络，使得容器可以使用 Pod IP 直接互通。\n在 VXLAN 模式下：\n每个节点分配一个独立的子网（例如 /24），该子网中的 IP 地址分配给该节点上的 Pod。 VXLAN 隧道用于在不同节点之间封装和传输数据包。 4.1. Flannel VXLAN 的关键组件 etcd / Kubernetes API： Flannel 使用 etcd 或 Kubernetes API 作为存储，保存每个节点的子网分配信息。 例如，节点 A 的子网是 10.1.1.0/24，节点 B 的子网是 10.1.2.0/24。 flanneld 进程： 每个节点运行 flanneld，负责： 从 etcd 获取子网信息。 配置 VXLAN 设备。 管理路由规则。 VXLAN 设备： Flannel 在每个节点上创建一个 VXLAN 虚拟网络接口（如 flannel.1）。 通过这个接口，将数据包封装到 VXLAN 隧道中。 4.2. Flannel VXLAN 的通信流程 4.2.1. Pod 到 Pod 通信示例 假设 Pod1 在节点 A，Pod2 在节点 B，Pod1 的 IP 为 10.1.1.2，Pod2 的 IP 为 10.1.2.3：\n数据包生成： Pod1 想要与 Pod2 通信，发送一个 IP 数据包，目标地址是 10.1.2.3。 节点路由查找： 节点 A 的路由表根据目标 IP (10.1.2.3)，发现其子网 10.1.2.0/24属于节点 B。 数据包被转发到 VXLAN 设备 flannel.1。 VXLAN 封装： 在 flannel.1上，数据包被封装： 原始 IP 数据包被作为 VXLAN 的有效载荷。 VXLAN 头部和外层 UDP/IP 头部被添加。 外层 IP 头的目标地址是节点 B 的物理 IP 地址。 跨网络传输： 封装后的数据包通过底层三层网络（通常是宿主机的物理网卡）发送到节点 B。 解封装和转发： 节点 B 的 VXLAN 设备接收到数据包后，解封装外层头部，还原出原始 IP 数据包。 节点 B 根据路由规则将数据包转发给 Pod2。 4.2.2. 路由和 ARP 的处理 路由表：\nFlannel 会在每个节点配置路由规则，将目标子网与相应的 VXLAN 隧道设备关联。\n例如：\n10.1.2.0/24 via 192.168.1.2 dev flannel.1 ARP 处理：\nVXLAN 需要知道远程节点的物理 IP 地址。 Flannel 在 VXLAN 模式下通过 etcd 或 Kubernetes API 维护节点的 IP 映射关系，而不是依赖传统的 ARP。 5. Flannel VXLAN报文解析 通过vxlan隧道传输需要封vxlan包和解vxlan包，以下描述vxlan报文内容。\n5.1. VXLAN 报文结构 Flannel 的 VXLAN 报文可以分为以下几个主要部分：\n字段 描述 外层以太网头 用于传输 VXLAN 报文的物理网络的 MAC 地址。 外层 IP 头 用于三层传输，包含源 IP（本地物理机）和目的 IP（目标物理机）。 UDP 头 用于封装 VXLAN 流量，通常使用 VXLAN 的默认端口 4789。 VXLAN 标头 包含 VXLAN 的关键信息，例如 VNI（虚拟网络标识）。 内层以太网头 原始的以太网帧头，用于容器或 Pod 之间通信。 内层数据（Payload） 实际的应用层数据，例如 HTTP、TCP 或 ICMP 数据。 示例：\n+-----------------------------+ | 外层以太网头 | MAC 源地址 | MAC 目的地址 | 类型 | +-----------------------------+ | 外层 IP 头 | 源 IP 地址 | 目的 IP 地址 | 协议| +-----------------------------+ | UDP 头 | 源端口 | 目的端口 | 长度 | +-----------------------------+ | VXLAN 标头 | Flag | Reserved | VNI | +-----------------------------+ | 内层以太网头 | 源 MAC 地址 | 目的 MAC 地址 | +-----------------------------+ | 内层数据（Payload） | 实际的数据内容 | +-----------------------------+ 5.2. 报文详细解析 1. 外层以太网头\n作用：用于承载 VXLAN 报文的实际网络传输。 内容： 源 MAC 地址：发送报文的物理网卡的 MAC 地址。 目的 MAC 地址：目标主机的物理网卡的 MAC 地址。 2. 外层 IP 头\n作用：在三层网络中将 VXLAN 报文路由到目标主机。 内容： 源 IP 地址：发送报文的物理主机的 IP 地址。 目的 IP 地址：目标主机的 IP 地址。 协议类型：UDP。 3. UDP 头\n作用：封装 VXLAN 数据。 内容： 源端口：动态分配的随机端口。 目的端口：VXLAN 的端口，例如 8472（可以在 Flannel 配置中自定义）。 长度和校验和：用于确保 UDP 报文的完整性。 4. VXLAN 标头\n作用：标识虚拟网络以及对 VXLAN 流量进行必要的控制。 格式： Flag：8 位，标识是否启用 VXLAN 功能，通常为 0x08。 VNI（Virtual Network Identifier）：24 位，标识 VXLAN 所属的虚拟网络。 Reserved：用于对齐和扩展，通常为 0。 5. 内层以太网头\n作用：封装原始的以太网帧，用于容器或 Pod 之间的二层通信。 内容： 源 MAC 地址：发送容器或 Pod 的 MAC 地址。 目的 MAC 地址：目标容器或 Pod 的 MAC 地址。 6. 内层数据（Payload）\n作用：实际的用户数据。 内容： 数据类型：可以是 IP 数据包（例如 TCP、UDP 或 ICMP），也可能是 ARP 等协议。 5.3. 报文的捕获 可以通过命令ip -d link show flannel.1查看vxlan id（VNI） ，vxlan端口，vxlan基于的物理网卡。\n例如，以下查询到vxlan id(VNI)为1，vxlan端口为8472，基于的物理网卡是bond0。\n# ip -d link show flannel.1 10: flannel.1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether xxxxxx brd xxxxx promiscuity 0 minmtu 68 maxmtu 65535 vxlan id 1 local xxxxxxx dev bond0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 使用 tcpdump，在物理网卡上捕获 VXLAN 报文， 协议是UDP，添加vxlan的端口。\ntcpdump -i bond0 udp port 8472 -vv 6. 如何避免VXLAN冲突 在同一台物理机上如果存在其他设备（例如：弹性外网EIP设备）使用了VXLAN的隧道技术，可能与Flannel的VXLAN设备存在冲突，以下是冲突的可能性、原因及解决方法：\n6.1. 可能存在的冲突 1. UDP 端口冲突\nVXLAN 通常使用 UDP 端口 4789 进行封装传输。如果弹性外网 IP 的 VXLAN 实现和 Flannel （默认端口8472）都使用相同的 UDP 端口，那么会导致端口冲突，使得其中一个功能失效。 2. VNI (VXLAN Network Identifier) 冲突\nVXLAN 的每个虚拟网络通过 VNI（24 位）进行区分。 如果弹性外网 IP 和 Flannel 的 VXLAN 使用了相同的 VNI，则可能导致 VXLAN 隧道之间的隔离性失效，造成数据包混乱。 3. 路由或设备名冲突\n两者都会在物理机上创建 VXLAN 设备（如 vxlan0 或 flannel.1），如果设备名称相同，可能导致配置混乱。 路由表可能同时存在与 VXLAN 隧道相关的规则，如果路由目标网络有重叠，可能导致流量被错误转发。 6.2. 冲突的解决方法 1. 避免 UDP 端口冲突\n确认弹性外网 IP 的 VXLAN 端口号。 自定义Flannel的VXLAN 端口号。确保两者使用不同的端口号。 2. 避免 VNI 冲突\n确认弹性外网 IP 的 VXLAN 实现是否允许指定 VNI。如果允许，则为两者设置不同的 VNI 范围。 Flannel VNI 通常由其内部管理，自动分配，但可以在配置文件中指定范围或固定值。 3. 避免设备名冲突\nFlannel 默认设备名是 flannel.1，一般非flannel的设备不会设置为该名称。 确保弹性外网 IP 的 VXLAN 设备名不同，或显式指定设备名称。 4. 路由隔离\n仔细检查路由表，确保两者的目标子网范围没有重叠。 如果要修改flannel的冲突参数，可以修改配置文件。\nkubectl edit cm -n kube-flannel kube-flannel-cfg\n{ \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\", \"VNI\": 1, # 默认值 \"Port\": 8472 # 默认值 } } 6.3. 查看设备的VXLAN 可以使用以下命令列出所有 VXLAN 接口：\n# ip link show type vxlan # 输出 6: vxlan0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 ... 7: flannel.1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 ... 可以查看 VXLAN 接口的配置信息：\n# ip -d link show \u003cvxlan_interface\u003e 6: vxlan0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 06:42:ae:ff:fe:90 brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 local 192.168.1.10 dev eth0 port 4789 0 ... # vxlan id 42：表示 VXLAN 的 VNI（Virtual Network Identifier）是 42。 # port 4789：表示 VXLAN 使用的 UDP 端口是 4789（默认端口）。 如果 VXLAN 接口绑定到了桥接设备（bridge），可以通过 bridge 命令查询详细信息。\nbridge fdb show dev \u003cvxlan_interface\u003e # 输出 00:00:00:00:00:13 dst x.x.x.x self permanent 7. 总结 本文主要介绍了VXLAN的基本概念和原理，报文解析以及在Flannel中的使用。除了Flannel外，在Calico和Cilium的网络插件中也涉及到VXLAN的使用，基本原理类似，大同小异。通过本文的介绍，帮助读者对于其他场景下使用VXLAN的方式也能够快速理解，并且可以快速排查VXLAN相关的网络问题。\n参考：\nhttps://datatracker.ietf.org/doc/html/rfc7348 https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md ","categories":"","description":"","excerpt":"1. VXLAN简介 VXLAN（Virtual Extensible LAN）是一种网络虚拟化技术，旨在解决传统二层网络扩展的局限性，尤其 …","ref":"/kubernetes-notes/network/flannel/vxlan/","tags":["Kubernetes","CNI"],"title":"VXLAN原理介绍"},{"body":"1. kruise-rollout简介 金丝雀发布是逐步将流量导向新版本的应用，以最小化风险。具体过程包括：\n部署新版本的 Pod。 将一部分流量分配到新版本（通常比例很小）。 逐步增加新版本的流量比例，直到完成全量发布。 Kruise Rollouts 是一个 Bypass(旁路) 组件，提供 高级渐进式交付功能 。可以通过Kruise Rollouts插件来实现金丝雀发布的能力。\n组件 Kruise Rollouts 核心概念 增强现有的工作负载 架构 Bypass 插拔和热切换 是 发布类型 多批次、金丝雀、A/B测试、全链路灰度 工作负载类型 Deployment、StatefulSet、CloneSet、Advanced StatefulSet、Advanced DaemonSet 流量类型 Ingress、GatewayAPI、CRD（需要 Lua 脚本） 迁移成本 无需迁移工作负载和Pods HPA 兼容性 是 2. 安装kruise-rollout 2.1. helm安装kruise-rollout helm repo add openkruise https://openkruise.github.io/charts/ helm repo update kubectl create ns openkruise helm install kruise-rollout openkruise/kruise-rollout -n openkruise # 升级到指定版本 helm upgrade kruise-rollout openkruise/kruise-rollout --version 0.5.0 查看部署结果\n通过helm和kubectl的命令可以看到创建了几个crd和kruise-rollout-controller-manager的pod来提供rollout的功能。\n# helm list -A NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION kruise-rollout\topenkruise\t1 2024-11-24 16:52:36.067327844 +0800 +08\tdeployed\tkruise-rollout-0.5.0\t0.5.0 # kubectl get po -n kruise-rollout NAME READY STATUS RESTARTS AGE kruise-rollout-controller-manager-875654888-rpxds 1/1 Running 0 87s kruise-rollout-controller-manager-875654888-w75xj 1/1 Running 0 87s # kubectl get crd |grep kruise batchreleases.rollouts.kruise.io 2024-11-24T08:52:36Z rollouthistories.rollouts.kruise.io 2024-11-24T08:52:36Z rollouts.rollouts.kruise.io 2024-11-24T08:52:36Z trafficroutings.rollouts.kruise.io 2024-11-24T08:52:36Z 2.2. 安装kubectl-kruise kubectl-kruise用于执行rollout发版和回退等操作的二进制命令。\nkubectl-kruise rollout approve：执行下一批版本发布 kubectl-kruise rollout undo：回退全部发版批次 wget https://github.com/openkruise/kruise-tools/releases/download/v1.1.7/kubectl-kruise-linux-amd64-v1.1.7.tar.gz tar -zvxf kubectl-kruise-linux-amd64-v1.1.7.tar.gz mv linux-amd64/kubectl-kruise /usr/local/bin/ 3. 使用指南 先部署一个k8s deployment对象，例如部署10个nginx:1.26的容器。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx namespace: default spec: replicas: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: nginx spec: containers: - image: nginx:1.26 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP 3.1. 创建发布策略(rollout对象) 我们以使用多批次灰度发布为例。\nworkloadRef定义作用于哪个deployment对象。 strategy定义发布策略。 以下是发布策略示例描述：\n在第一批次：只升级 1个Pod； 在第二批次：升级 50% 的 Pods，即 5个已更新的Pod； 在第三批次：升级 100% 的 Pods，即 10个已更新的Pod。 $ kubectl apply -f - \u003c\u003cEOF apiVersion: rollouts.kruise.io/v1beta1 kind: Rollout metadata: name: rollouts-nginx namespace: default spec: workloadRef: apiVersion: apps/v1 kind: Deployment name: nginx strategy: canary: enableExtraWorkloadForCanary: false steps: - replicas: 1 - replicas: 50% - replicas: 100% EOF 3.2. 升级发布版本(发布第一批次) 升级nginx:1.26到nginx:1.27\nkubectl set image deployment nginx nginx=nginx:1.27 查看灰度结果：\n# kubectl get rs -L pod-template-hash -w NAME DESIRED CURRENT READY AGE POD-TEMPLATE-HASH nginx-68556bc579 1 1 1 8s 68556bc579 # nginx:1.27 nginx-d7f5d89c9 9 9 9 24m d7f5d89c9 # nginx:1.26 查看rollout状态：\nrollout对象的状态主要描述了当前处于哪个rollout阶段及总状态和子阶段状态的信息。\n# kubectl get rollout rollouts-nginx -oyaml apiVersion: rollouts.kruise.io/v1beta1 kind: Rollout metadata: name: rollouts-nginx namespace: default spec: disabled: false strategy: canary: steps: - pause: {} replicas: 1 - pause: {} replicas: 50% - pause: {} replicas: 100% workloadRef: apiVersion: apps/v1 kind: Deployment name: nginx status: canaryStatus: canaryReadyReplicas: 1 canaryReplicas: 1 canaryRevision: 5c9556986d currentStepIndex: 1 currentStepState: StepPaused # 当前步骤状态 lastUpdateTime: \"2024-11-24T11:52:56Z\" message: BatchRelease is at state Ready, rollout-id , step 1 observedWorkloadGeneration: 36 podTemplateHash: d7f5d89c9 rolloutHash: 77cxd69w47b7bwddwv2w7vxvb4xxdbwcx9x289vw69w788w4w6z4x8dd4vbz2zbw stableRevision: 68556bc579 conditions: - lastTransitionTime: \"2024-11-24T11:52:47Z\" lastUpdateTime: \"2024-11-24T11:52:47Z\" message: Rollout is in Progressing reason: InRolling status: \"True\" type: Progressing message: Rollout is in step(1/3), and you need manually confirm to enter the next step # rollout信息 observedGeneration: 2 phase: Progressing # 整体状态 3.3. 发布第二批次 kubectl-kruise rollout approve rollout/rollouts-nginx -n default 查看灰度结果：\n# kubectl get rs -L pod-template-hash -w NAME DESIRED CURRENT READY AGE POD-TEMPLATE-HASH nginx-68556bc579 5 5 5 7m24s 68556bc579 # nginx:1.27 nginx-d7f5d89c9 5 5 5 32m d7f5d89c9 # nginx:1.26 # kubectl get po NAME READY STATUS RESTARTS AGE nginx-68556bc579-24lpl 1/1 Running 0 7m34s # nginx:1.27 第一批次 nginx-68556bc579-2dph8 1/1 Running 0 14s # nginx:1.27 第二批次 nginx-68556bc579-57pqt 1/1 Running 0 14s # nginx:1.27 第二批次 nginx-68556bc579-879s9 1/1 Running 0 14s # nginx:1.27 第二批次 nginx-68556bc579-fwt52 1/1 Running 0 14s # nginx:1.27 第二批次 nginx-d7f5d89c9-5fbfp 1/1 Running 0 30m # 其余为第三批次 nginx-d7f5d89c9-gkz9p 1/1 Running 0 30m nginx-d7f5d89c9-jhxwl 1/1 Running 0 30m nginx-d7f5d89c9-vrqfz 1/1 Running 0 30m nginx-d7f5d89c9-zk2sj 1/1 Running 0 30m 3.4. 发布第三批次 kubectl-kruise rollout approve rollout/rollouts-nginx -n default 查看结果：\n# kubectl get rs -L pod-template-hash -w NAME DESIRED CURRENT READY AGE POD-TEMPLATE-HASH nginx-68556bc579 10 10 10 12m 68556bc579 # nginx:1.27 nginx-d7f5d89c9 0 0 0 37m d7f5d89c9 3.5. 发布回滚 该回滚会把全部批次的pod都回滚到第一批发版前的版本。\nkubectl-kruise rollout undo rollout/rollouts-nginx -n default 4. k8s对象变更 kruise-rollout的原理主要还是劫持Deployment和ReplicaSet的操作，例如在升级deployment的时候会把deployment原先的strategy策略放在annotations中暂存，再增加paused=true的参数暂停deployment的发布。在rollout全部结束后再恢复原先的deployment参数，主要包括paused和strategy。\n以下是rollout中间阶段的deployment信息：\napiVersion: apps/v1 kind: Deployment metadata: annotations: batchrelease.rollouts.kruise.io/control-info: '{\"apiVersion\":\"rollouts.kruise.io/v1beta1\",\"kind\":\"BatchRelease\",\"name\":\"rollouts-nginx\",\"uid\":\"8f29624b-ab77-49d1-abbe-aa92bb3998e2\",\"controller\":true,\"blockOwnerDeletion\":true}' deployment.kubernetes.io/revision: \"5\" rollouts.kruise.io/deployment-extra-status: '{\"updatedReadyReplicas\":1,\"expectedUpdatedReplicas\":1}' rollouts.kruise.io/deployment-strategy: '{\"rollingStyle\":\"Partition\",\"rollingUpdate\":{\"maxUnavailable\":\"25%\",\"maxSurge\":\"25%\"},\"partition\":1}' # deployment原本的strategy策略 rollouts.kruise.io/in-progressing: '{\"rolloutName\":\"rollouts-nginx\"}' generation: 36 labels: app: nginx rollouts.kruise.io/controlled-by-advanced-deployment-controller: \"true\" rollouts.kruise.io/stable-revision: 68556bc579 rollouts.kruise.io/workload-type: deployment name: nginx namespace: default spec: paused: true # 增加了paused参数 progressDeadlineSeconds: 600 replicas: 10 revisionHistoryLimit: 10 5. Rollout流量灰度 5.1. 添加pod 版本标签 为了适配Apisix或Nginx等k8s流量网关的灰度逻辑，我们通过k8s service的方式来动态获取Pod的IP（即endpoint）,其中包括全量pod，灰度pod和非灰度pod。因此我们在每次发版的deployment给pod加上所属的版本标签。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx namespace: default spec: replicas: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: nginx version: \"1.26\" # 增加pod对应的版本标签，用于service联动 spec: containers: - image: nginx:1.26 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP 5.2. 添加灰度pod的service 创建三个service来对应全量pod，灰度pod和非灰度pod。\n全量pod的service\napiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: selector: app: nginx # 全量pod的标签 ports: - name: http protocol: TCP port: 80 targetPort: 80 灰度pod的service\napiVersion: v1 kind: Service metadata: name: nginx-canary namespace: default labels: version: canary spec: selector: app: nginx version: \"1.27\" # 增加灰度版本的标签，对应灰度的pod ports: - name: http protocol: TCP port: 80 targetPort: 80 非灰度pod的service\napiVersion: v1 kind: Service metadata: name: nginx-stable namespace: default spec: selector: app: nginx version: \"1.26\" # 增加非灰度版本的标签，对应非灰度的pod ports: - name: http protocol: TCP port: 80 targetPort: 80 5.3. 动态获取灰度的endpoint 通过以上service的配置，可以在灰度的过程中动态获取灰度pod的endpoint，然后动态集成到流量网关中。\n全量pod的service：在流量灰度前和流量灰度完成后可以把流量调度到这个service下的pod。 灰度pod的service：可以设置把灰度流量调度到canary的service的pod。 非灰度pod的service：可以把非灰度流量调度到stable的service的pod。 以下是灰度前后endpoint的变化：\n# kubectl get endpoints # 灰度前：canary的pod为0 nginx 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 2d nginx-canary \u003cnone\u003e 2d nginx-stable 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 47h # 灰度中：canary的pod增加 nginx 10.0.1.202:80,10.0.1.203:80,10.0.1.204:80 + 7 more... 2d \u003cnone\u003e nginx-canary 10.0.1.5:80 2d version=canary nginx-stable 10.0.1.202:80,10.0.1.203:80,10.0.1.204:80 + 6 more... 47h version=stable # 灰度完成：stable的pod为0 NAME ENDPOINTS AGE nginx 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 2d nginx-canary 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 2d nginx-stable \u003cnone\u003e 47h 6. 总结 k8s的deployment默认支持金丝雀发布，可以通过kubectl rollout的命令实现，通过配置deployment中的maxSurge和maxUnavailable来控制发版的节奏，可以通过以下命令来控制发版批次：\nkubectl rollout pause：暂停发版 kubectl rollout resume：继续发版 kubectl rollout undo：回滚版本 但相对来讲，控制粒度没有那么精确，而OpenKruise的Rollouts插件提供了一种更加轻便，可控的发版工具，个人认为最大的优势是配置简单，且无需迁移工作负载和Pods，可以更好的集成到现有的容器平台中。因此可以通过该工具来实现金丝雀发版（多批次发版），实现灰度的能力。\n参考：\nhttps://openkruise.io/zh/rollouts/introduction https://openkruise.io/zh/rollouts/installation https://openkruise.io/zh/rollouts/user-manuals/basic-usage ","categories":"","description":"","excerpt":"1. kruise-rollout简介 金丝雀发布是逐步将流量导向新版本的应用，以最小化风险。具体过程包括：\n部署新版本的 Pod。 将一部 …","ref":"/kubernetes-notes/operation/deployment/kruise-rollout/","tags":["Kubernetes"],"title":"Kruise Rollout发布"},{"body":"本文主要描述常用的CNI插件的选型，主要包括cilium，calico，flannel三种插件的对比。\n1. 技术特点对比 特性 Cilium Calico Flannel 数据面技术 eBPF 加速 基于 iptables（支持 eBPF） vxlan、host-gw、ipip 等隧道技术 转发效率 高（内核级加速，直通流量） 高（支持原生路由） 中（隧道技术增加开销） 可扩展性 优（支持高级 L7 策略） 优（支持原生路由和简单网络策略） 较低（以 L3 网络为主） 延迟 低（无需额外隧道或规则） 低（无隧道或 eBPF 模式） 较高（隧道封装增加延迟） 吞吐量 高（eBPF 高效转发） 中（依赖 iptables 或 eBPF） 较低（隧道开销显著） 2. 性能指标对比 性能指标 Cilium Calico Flannel 吞吐量 高（eBPF 高效转发） 中-高（取决于模式） 较低（隧道封装损耗较大） 延迟 低（直接路由模式最佳） 较低（非隧道模式表现良好） 较高（隧道增加延迟） CPU使用 较高（eBPF 和可观测性功能） 中（iptables/eBPF 开销） 低（简单架构） 内存使用 较高（功能丰富） 中 低 3. 测试数据示例 以下是根据典型测试场景总结的指标（单位为吞吐量 Mbps 和延迟 ms）：\n测试场景 Cilium Calico Flannel 吞吐量 (单节点) ~9,000 Mbps ~8,500 Mbps ~6,000 Mbps 吞吐量 (跨节点) ~8,000 Mbps ~7,500 Mbps ~5,000 Mbps 延迟 (单节点) ~0.2 ms ~0.3 ms ~1.0 ms 延迟 (跨节点) ~0.4 ms ~0.5 ms ~2.0 ms 2020年测试数据：\n数据来源：Benchmark results of Kubernetes network plugins (CNI) over 10Gbit/s network (Updated: August 2020)\n[2024年]单位带宽消耗的CPU和内存数据：\n数据来源：Benchmark results of Kubernetes network plugins (CNI) over 40Gbit/s network -2024\n以上可以看出cilium单位消耗的CPU和内存相比于flannel高。\n4. 网络性能分析 4.1. Cilium 吞吐量：基于 eBPF 的数据面技术，可直接在 Linux 内核中高效转发流量，减少上下文切换。使用直连路由模式（--tunnel=disabled）时，进一步减少封装开销。 延迟：支持 Sidecar-less 的服务网格架构，能够降低服务间通信延迟。 资源消耗：由于其高级功能（如 Hubble 可观测性和 L7 策略），在 CPU 和内存使用上高于其他插件。 4.2. Calico 吞吐量：非隧道模式下，基于 BGP 的原生路由性能接近裸机水平；启用 eBPF 模式时，性能进一步提升。 延迟：表现良好，但在复杂网络策略下可能增加延迟。 资源消耗：资源使用适中，适合大多数生产环境。 4.3. Flannel 吞吐量：由于采用 VXLAN、IPIP 等隧道封装方式，其性能通常不如 Cilium 和 Calico。 延迟：封装和解封装的额外操作导致延迟增加。 资源消耗：架构简单，资源使用最低，适合资源有限的小型集群。 5. 业务场景选型 1. Cilium：适合高性能与安全需求的场景\n适用场景： 微服务架构：Cilium 的 eBPF 技术支持 L7 数据包过滤、服务可观测性和无 Sidecar 服务网格，非常适合复杂微服务环境。 边缘计算：在边缘节点上，需要低延迟和高吞吐量，Cilium 的直连路由模式（--tunnel=disabled）非常高效。 多云和混合云：支持多种高级网络功能，如网络策略的灵活配置和透明的加密。 局限性： 部署复杂度相对较高，对 Linux 内核版本有要求（推荐 5.3+）。 资源消耗比 Flannel 高。 2. Calico：适合大规模、灵活策略的企业集群\n适用场景： 大规模 Kubernetes 集群：基于 BGP 的网络路由能够高效扩展，适合公有云和企业级大规模集群。 注重安全策略：支持丰富的网络安全策略，并提供对接 eBPF 的能力，兼顾性能和策略灵活性。 混合部署：Calico 可以在非 Kubernetes 工作负载中实现一致的网络策略。 局限性： 默认基于 iptables 的实现在高负载下性能可能不如 Cilium 的 eBPF 数据面。 网络策略复杂度较高时，可能增加运维工作量。 3. Flannel：适合轻量级和资源有限的集群\n适用场景： 小型集群：Flannel 架构简单，资源使用少，适合轻量化的 Kubernetes 部署。 测试环境：性能需求较低的开发和测试环境中，可以快速搭建和运行。 边缘计算（非高性能）：对网络性能要求较低的小型边缘节点可以使用。 局限性： 在吞吐量和延迟上不如 Cilium 和 Calico，尤其是需要大量隧道封装时。 缺乏高级网络功能，例如复杂的网络策略和观测能力。 4. 推荐选择总结\n场景类型 推荐插件 原因 高性能微服务架构 Cilium 提供 eBPF 技术，支持复杂策略和低延迟网络 大规模企业集群 Calico 稳定、灵活，适合多样化和大规模 Kubernetes 部署 资源受限环境 Flannel 简单易用，资源消耗低 边缘计算 Cilium/Flannel Cilium 适合高性能需求，Flannel 适合轻量级节点 混合云/多云 Cilium/Calico Cilium 支持透明加密和现代架构，Calico 提供灵活网络策略支持 参考：\nhttps://docs.cilium.io/en/latest/operations/performance/benchmark/ https://cilium.io/blog/2018/12/03/cni-performance/ https://cilium.io/blog/2021/05/11/cni-benchmark/ Benchmark results of Kubernetes network plugins (CNI) over 40Gbit/s network -2024 Benchmark results of Kubernetes network plugins (CNI) over 10Gbit/s network (Updated: August 2020) ","categories":"","description":"","excerpt":"本文主要描述常用的CNI插件的选型，主要包括cilium，calico，flannel三种插件的对比。\n1. …","ref":"/kubernetes-notes/network/cni/cni-plugin-comparison/","tags":["CNI"],"title":"CNI插件选型"},{"body":"开发一个k8s operator组件主要会用到以下几个仓库或工具：\nkubebuilder/operator-sdk：主要用于创建CRD对象。\ncontroller-manager：主要用于实现operator的controller逻辑。\n本文以kubebuilder的工具和controller-manager example为例。\n1. 准备工具环境及创建项目 安装kubebuilder工具\ncurl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder \u0026\u0026 mv kubebuilder /usr/local/bin/ 创建operator项目目录\n$ kubebuilder init --domain example.com --license apache2 --owner \"Your Name\" --repo github.com/example/my-operator 将 example.com 替换为你的域名，Your Name 替换为你的名字，github.com/example/my-operator 替换为你的 GitHub 仓库。\n2. 创建CRD对象 $ kubebuilder create api --group mygroup --version v1alpha1 --kind MyResource 这将在 api/v1alpha1 文件夹中创建一个名为 myresource_types.go 的文件，其中定义了 MyResource 资源的规范和状态。\n编辑 api/v1alpha1/myresource_types.go 文件，添加自定义资源的规范和状态字段。\n3. 实现控制器逻辑 在 controllers/myresource_controller.go 文件中实现控制器逻辑。该部分是不同的CRD实现自定义逻辑的核心部分。\n主要包含以下几个部分：\n定义ReconcileMyResource结构体。\n实现func (r *ReconcileMyResource) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, error)函数接口。\n定义finalizer逻辑。\n定义reconcile逻辑。\npackage controllers import ( \"context\" \"github.com/go-logr/logr\" \"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\" \"sigs.k8s.io/controller-runtime/pkg/controller/inject\" \"sigs.k8s.io/controller-runtime/pkg/reconcile\" myv1alpha1 \"github.com/example/my-operator/api/v1alpha1\" corev1 \"k8s.io/api/core/v1\" \"k8s.io/apimachinery/pkg/api/errors\" \"k8s.io/apimachinery/pkg/runtime\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"sigs.k8s.io/controller-runtime/pkg/controller\" logf \"sigs.k8s.io/controller-runtime/pkg/log\" \"sigs.k8s.io/controller-runtime/pkg/log/zap\" \"sigs.k8s.io/controller-runtime/pkg/manager\" \"sigs.k8s.io/controller-runtime/pkg/reconcile\" \"sigs.k8s.io/controller-runtime/pkg/source\" ) var log = logf.Log.WithName(\"controller_myresource\") // Add creates a new MyResource Controller and adds it to the Manager. The Manager will set fields on the Controller // and Start it when the Manager is Started. func Add(mgr manager.Manager) error { return add(mgr, newReconciler(mgr)) } // newReconciler returns a new reconcile.Reconciler func newReconciler(mgr manager.Manager) reconcile.Reconciler { return \u0026ReconcileMyResource{client: mgr.GetClient(), scheme: mgr.GetScheme()} } // blank assignment to verify that ReconcileMyResource implements reconcile.Reconciler var _ reconcile.Reconciler = \u0026ReconcileMyResource{} // ReconcileMyResource reconciles a MyResource object type ReconcileMyResource struct { client client.Client scheme *runtime.Scheme } // Reconcile reads that state of the cluster for a MyResource object and makes changes based on the state read // and what is in the MyResource.Spec func (r *ReconcileMyResource) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, error) { reqLogger := log.WithValues(\"Namespace\", request.Namespace, \"Name\", request.Name) reqLogger.Info(\"Reconciling MyResource\") // Fetch the MyResource instance myresource := \u0026myv1alpha1.MyResource{} err := r.client.Get(ctx, request.NamespacedName, myresource) if err != nil { if errors.IsNotFound(err) { // Request object not found, could have been deleted after reconcile request. // Owned objects are automatically garbage collected. For additional cleanup logic use finalizers. // Return and don't requeue return reconcile.Result{}, nil } // Error reading the object - requeue the request. return reconcile.Result{}, err } // Add finalizer logic here // Add reconcile logic here return reconcile.Result{}, nil } 4. 注册控制器 在 main.go 文件中注册控制器，主要包含以下几个步骤\nctrl.SetLogger(zap.New())：注册日志工具\nctrl.NewManager：创建一个manager，必要时可设置选主逻辑。\nctrl.NewControllerManagedBy(mgr)：预计mgr创建manager controller并注册Reconciler。\nmgr.Start(ctrl.SetupSignalHandler()): 运行mgr。\nfunc main() { // set logger ctrl.SetLogger(zap.New()) // Set up a Manager mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: myv1alpha1.SchemeBuilder.Scheme, MetricsBindAddress: *metricsHost, Port: 9443, LeaderElection: *enableLeaderElection, LeaderElectionID: \"my-operator-lock\", }) if err != nil { setupLog.Error(err, \"unable to start manager\") os.Exit(1) } // in a real controller, we'd create a new scheme for this err = api.AddToScheme(mgr.GetScheme()) if err != nil { setupLog.Error(err, \"unable to add scheme\") os.Exit(1) } // Create a new Reconciler r := \u0026ReconcileMyResource{ client: m.GetClient(), scheme: m.GetScheme(), } // Create a new Controller and register the Reconciler err = ctrl.NewControllerManagedBy(mgr). For(\u0026myv1alpha1.MyResource{}). // 自定义crd Complete(r) // 注册Reconciler if err != nil { setupLog.Error(err, \"unable to create controller\") os.Exit(1) } err = ctrl.NewWebhookManagedBy(mgr). For(\u0026api.ChaosPod{}). Complete() if err != nil { setupLog.Error(err, \"unable to create webhook\") os.Exit(1) } setupLog.Info(\"starting manager\") if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \"problem running manager\") os.Exit(1) } } 参考：\nImplementing a controller - The Kubebuilder Book\ncontroller-runtime源码分析 | 李乾坤的博客\nKubebuilder - SDK for building Kubernetes APIs using CRDs\noperator-framework/operator-sdk: SDK for building Kubernetes applications.\nhttps://blog.hdls.me/16500403497126.html\nkubebuilder-cronjob_controller\n","categories":"","description":"","excerpt":"开发一个k8s operator组件主要会用到以下几个仓库或工具：\nkubebuilder/operator-sdk：主要用于创建CRD对 …","ref":"/kubernetes-notes/develop/operator/develop-operator/","tags":["Operator"],"title":"如何开发一个Operator"},{"body":" 本文以commit id：180282663457080119a1bc6076cce20c922b5c50， 对应版本tag: v1.2.1 的源码分析tunnel-server的实现逻辑。\n1. Tunnel-server简介 云与边一般位于不同网络平面，同时边缘节点普遍位于防火墙内部，采用云(中心)边协同架构，将导致原生 K8s 系统的运维监控能力面临如下挑战:\nK8s 原生运维能力缺失(如 kubectl logs/exec 等无法执行) 社区主流监控运维组件无法工作(如 Prometheus/metrics-server ) 在 OpenYurt 中，引入了专门的组件 YurtTunnel 负责解决云边通信问题。反向通道是解决跨网络通信的一种常见方式，而 YurtTunnel 的本质也是一个反向通道。 它是一个典型的C/S结构的组件，由部署于云端的 YurtTunnelServer 和部署于边缘节点上的 YurtTunnelAgent组成。\n本文主要分析tunnel-server的代理逻辑。\n以下是基本架构图：\npkg中的目录结构：\nyurttunnel ├── agent # tunnel agent代码 ├── constants # 常量值 ├── handlerwrapper ├── informers ├── kubernetes # k8s clientset工具包 ├── server # 核心代码： tunnel server逻辑 ├── trafficforward # iptables和dns操作 └── util 2. NewYurttunnelServerCommand main函数入口：\nfunc main() { cmd := app.NewYurttunnelServerCommand(stop) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { klog.Fatalf(\"%s failed: %s\", projectinfo.GetServerName(), err) } } 以下是NewYurttunnelServerCommand构造函数，常见的三件套，不做展开：\n读取参数：serverOptions.AddFlags(cmd.Flags())\n生成配置：cfg.Complete()\n执行Run函数：核心逻辑\nfunc NewYurttunnelServerCommand(stopCh \u003c-chan struct{}) *cobra.Command { serverOptions := options.NewServerOptions() cmd := \u0026cobra.Command{ Use: \"Launch \" + projectinfo.GetServerName(), Short: projectinfo.GetServerName() + \" sends requests to \" + projectinfo.GetAgentName(), RunE: func(c *cobra.Command, args []string) error { ... cfg, err := serverOptions.Config() if err != nil { return err } if err := Run(cfg.Complete(), stopCh); err != nil { return err } return nil }, Args: cobra.NoArgs, } serverOptions.AddFlags(cmd.Flags()) return cmd } 3. Run(cfg.Complete(), stopCh) Run函数最终是运行一个tunnelserver的常驻进程。在之前会做一些controller或manager的准备工作。\n其中包括：\nDNS controller\nIP table manager\ncertificate manager\nRegisterInformersForTunnelServer\n首先是构建并运行上述的manager或controller， 源码中的注释也大概描述了主要流程：\n注册tunnel所需的SharedInformerFactory\n运行dns controller\n运行ip table manager\n给tunnel server创建certificate manager\n给tunnel agent 创建certificate manager\n创建handler wrappers\n生成TLS 证书\n运行tunnel server\n以下是部分代码，已删除无关紧要的代码：\n// run starts the yurttunel-server func Run(cfg *config.CompletedConfig, stopCh \u003c-chan struct{}) error { var wg sync.WaitGroup // register informers that tunnel server need informers.RegisterInformersForTunnelServer(cfg.SharedInformerFactory) // 0. start the DNS controller if cfg.EnableDNSController { dnsController, err := dns.NewCoreDNSRecordController(...) go dnsController.Run(stopCh) } // 1. start the IP table manager if cfg.EnableIptables { iptablesMgr, err := iptables.NewIptablesManagerWithIPFamily(...) wg.Add(1) go iptablesMgr.Run(stopCh, \u0026wg) } // 2. create a certificate manager for the tunnel server certManagerFactory := certfactory.NewCertManagerFactory(cfg.Client) ips, dnsNames, err := getTunnelServerIPsAndDNSNamesBeforeInformerSynced(cfg.Client, stopCh) serverCertMgr, err := certManagerFactory.New(...) serverCertMgr.Start() // 3. create a certificate manager for the tunnel proxy client tunnelProxyCertMgr, err := certManagerFactory.New(...) tunnelProxyCertMgr.Start() // 4. create handler wrappers mInitializer := initializer.NewMiddlewareInitializer(cfg.SharedInformerFactory) wrappers, err := wraphandler.InitHandlerWrappers(mInitializer, cfg.IsIPv6()) // after all of informers are configured completed, start the shared index informer cfg.SharedInformerFactory.Start(stopCh) // 5. waiting for the certificate is generated _ = wait.PollUntil(5*time.Second, func() (bool, error) { // keep polling until the certificate is signed if serverCertMgr.Current() != nil \u0026\u0026 tunnelProxyCertMgr.Current() != nil { return true, nil } klog.Infof(\"waiting for the master to sign the %s certificate\", projectinfo.GetServerName()) return false, nil }, stopCh) // 6. generate the TLS configuration based on the latest certificate tlsCfg, err := certmanager.GenTLSConfigUseCurrentCertAndCertPool(serverCertMgr.Current, cfg.RootCert, \"server\") proxyClientTlsCfg, err := certmanager.GenTLSConfigUseCurrentCertAndCertPool(tunnelProxyCertMgr.Current, cfg.RootCert, \"client\") // 7. start the server ts := server.NewTunnelServer( cfg.EgressSelectorEnabled, cfg.InterceptorServerUDSFile, cfg.ListenAddrForMaster, cfg.ListenInsecureAddrForMaster, cfg.ListenAddrForAgent, cfg.ServerCount, tlsCfg, proxyClientTlsCfg, wrappers, cfg.ProxyStrategy) if err := ts.Run(); err != nil { return err } // 8. start meta server util.RunMetaServer(cfg.ListenMetaAddr) \u003c-stopCh wg.Wait() return nil } 3. TunnelServer anpTunnelServer实现了TunnelServer的接口，以下分析TunnelServer.Run的部分。\nrun部分主要运行了三个server\nproxyServer： 主要是重定向apiserver的请求到tunnel server\nMasterServer：\nAgentServer：主要运行一个grpc server与tunnel agent连接，即云边反向隧道\n代码参考：/pkg/yurttunnel/server/anpserver.go\n// Run runs the yurttunnel-server func (ats *anpTunnelServer) Run() error { proxyServer := anpserver.NewProxyServer(uuid.New().String(), []anpserver.ProxyStrategy{anpserver.ProxyStrategy(ats.proxyStrategy)}, ats.serverCount, \u0026anpserver.AgentTokenAuthenticationOptions{}) // 1. start the proxier proxierErr := runProxier( \u0026anpserver.Tunnel{Server: proxyServer}, ats.egressSelectorEnabled, ats.interceptorServerUDSFile, ats.tlsCfg) wrappedHandler, err := wh.WrapHandler( NewRequestInterceptor(ats.interceptorServerUDSFile, ats.proxyClientTlsCfg), ats.wrappers, ) // 2. start the master server masterServerErr := runMasterServer( wrappedHandler, ats.egressSelectorEnabled, ats.serverMasterAddr, ats.serverMasterInsecureAddr, ats.tlsCfg) // 3. start the agent server agentServerErr := runAgentServer(ats.tlsCfg, ats.serverAgentAddr, proxyServer) return nil } 4. runAgentServer runAgentServer主要运行一个grpc server与edge端的agent形成grpc连接。\n// runAgentServer runs a grpc server that handles connections from the yurttunel-agent // NOTE agent server is responsible for managing grpc connection yurttunel-server // and yurttunnel-agent, and the proxy server is responsible for redirecting requests // to corresponding yurttunel-agent func runAgentServer(tlsCfg *tls.Config, agentServerAddr string, proxyServer *anpserver.ProxyServer) error { serverOption := grpc.Creds(credentials.NewTLS(tlsCfg)) ka := keepalive.ServerParameters{ // Ping the client if it is idle for `Time` seconds to ensure the // connection is still active Time: constants.YurttunnelANPGrpcKeepAliveTimeSec * time.Second, // Wait `Timeout` second for the ping ack before assuming the // connection is dead Timeout: constants.YurttunnelANPGrpcKeepAliveTimeoutSec * time.Second, } grpcServer := grpc.NewServer(serverOption, grpc.KeepaliveParams(ka)) anpagent.RegisterAgentServiceServer(grpcServer, proxyServer) listener, err := net.Listen(\"tcp\", agentServerAddr) klog.Info(\"start handling connection from agents\") if err != nil { return fmt.Errorf(\"fail to listen to agent on %s: %w\", agentServerAddr, err) } go grpcServer.Serve(listener) return nil } 5. Interceptor Interceptor（请求拦截器）拦截kube-apiserver的请求转发给tunnel，通过tunnel请求kubelet。\n流程图：\n代码地址：/pkg/yurttunnel/server/interceptor.go\n// NewRequestInterceptor creates a interceptor object that intercept request from kube-apiserver func NewRequestInterceptor(udsSockFile string, cfg *tls.Config) *RequestInterceptor { if len(udsSockFile) == 0 || cfg == nil { return nil } cfg.InsecureSkipVerify = true contextDialer := func(addr string, header http.Header, isTLS bool) (net.Conn, error) { klog.V(4).Infof(\"Sending request to %q.\", addr) proxyConn, err := net.Dial(\"unix\", udsSockFile) if err != nil { return nil, fmt.Errorf(\"dialing proxy %q failed: %w\", udsSockFile, err) } var connectHeaders string for _, h := range supportedHeaders { if v := header.Get(h); len(v) != 0 { connectHeaders = fmt.Sprintf(\"%s\\r\\n%s: %s\", connectHeaders, h, v) } } fmt.Fprintf(proxyConn, \"CONNECT %s HTTP/1.1\\r\\nHost: localhost%s\\r\\n\\r\\n\", addr, connectHeaders) br := newBufioReader(proxyConn) defer putBufioReader(br) res, err := http.ReadResponse(br, nil) if err != nil { proxyConn.Close() return nil, fmt.Errorf(\"reading HTTP response from CONNECT to %s via proxy %s failed: %w\", addr, udsSockFile, err) } if res.StatusCode != 200 { proxyConn.Close() return nil, fmt.Errorf(\"proxy error from %s while dialing %s, code %d: %v\", udsSockFile, addr, res.StatusCode, res.Status) } // if the request scheme is https, setup a tls connection over the // proxy tunnel (i.e. interceptor \u003c--tls--\u003e kubelet) if isTLS { tlsTunnelConn := tls.Client(proxyConn, cfg) if err := tlsTunnelConn.Handshake(); err != nil { proxyConn.Close() return nil, fmt.Errorf(\"fail to setup TLS handshake through the Tunnel: %w\", err) } klog.V(4).Infof(\"successfully setup TLS connection to %q with headers: %s\", addr, connectHeaders) return tlsTunnelConn, nil } klog.V(2).Infof(\"successfully setup connection to %q with headers: %q\", addr, connectHeaders) return proxyConn, nil } return \u0026RequestInterceptor{ contextDialer: contextDialer, } } 参考：\nhttps://openyurt.io/zh/docs/core-concepts/yurttunnel ","categories":"","description":"","excerpt":" 本文以commit id：180282663457080119a1bc6076cce20c922b5c50， 对应版本tag: …","ref":"/kubernetes-notes/edge/openyurt/code-analysis/tunnel-server-code-analysis/","tags":["OpenYurt"],"title":"OpenYurt之TunnelServer源码分析"},{"body":"安装openyurt，为了适配边缘场景，需要对k8s组件进行调整。其中包括：\nkube-apiserver\nkube-controller-manager\nkube-proxy\nCoreDNS\n1. kube-apiserver 为了实现云边通信，即用户可以正常使用kubectl exec/logs的功能来登录或查看边缘容器的信息。需要将kube-apiserver访问kubelet的地址调整为hostname优先。\n$ vi /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 kind: Pod ... spec: dnsPolicy: \"None\" # 1. dnsPolicy修改为None dnsConfig: # 2. 增加dnsConfig配置 nameservers: - 1.2.3.4 # 使用yurt-tunnel-dns service的clusterIP替换 searches: - kube-system.svc.cluster.local - svc.cluster.local - cluster.local options: - name: ndots value: \"5\" containers: - command: - kube-apiserver ... - --kubelet-preferred-address-types=Hostname,InternalIP,ExternalIP # 3. 把Hostname放在第一位 ... 2. kube-controller-manager 禁用默认的 nodelifecycle 控制器，当节点断连时不驱逐pod。\nnodelifecycle控制器主要用来根据node的status及lease的更新时间来决定是否要驱逐节点上的pod。为了让 yurt-controller-mamanger 能够正常工作，因此需要禁用controller的驱逐功能。\nvim /etc/kubernetes/manifests/kube-controller-manager.yaml # 在--controllers=*,bootstrapsigner,tokencleaner后面添加,-nodelifecycle # 即参数为： --controllers=*,bootstrapsigner,tokencleaner,-nodelifecycle # 如果kube-controller-manager是以static pod部署，修改yaml文件后会自动重启。 3. CoreDNS 将coredns从deployment部署改为daemonset部署。\n将deployment的coredns副本数调整为0。\nkubectl scale --replicas=0 deployment/coredns -n kube-system 创建daemonset的coredns。\nwget https://raw.githubusercontent.com/huweihuang/kubeadm-scripts/main/openyurt/yurt-tunnel/coredns.ds.yaml kubectl apply -f 支持流量拓扑：\n# 利用openyurt实现endpoint过滤 kubectl annotate svc kube-dns -n kube-system openyurt.io/topologyKeys='openyurt.io/nodepool' 4. kube-proxy 云边端场景下，边缘节点间很有可能无法互通，因此需要endpoints基于nodepool进行拓扑。直接将kube-proxy的kubeconfig配置删除，将apiserver请求经过yurthub即可解决服务拓扑问题。\nkubectl edit cm -n kube-system kube-proxy 示例：\napiVersion: v1 data: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 bindAddressHardFail: false clientConnection: acceptContentTypes: \"\" burst: 0 contentType: \"\" #kubeconfig: /var/lib/kube-proxy/kubeconfig.conf \u003c-- 删除这个配置 qps: 0 clusterCIDR: 100.64.0.0/10 configSyncPeriod: 0s // 省略 参考：\nhttps://openyurt.io/zh/docs/installation/openyurt-prepare/ ","categories":"","description":"","excerpt":"安装openyurt，为了适配边缘场景，需要对k8s组件进行调整。其中包括：\nkube-apiserver …","ref":"/kubernetes-notes/edge/openyurt/update-k8s-for-openyurt/","tags":["OpenYurt"],"title":"OpenYurt 安装相关Kubernetes配置调整"},{"body":"本文主要介绍三种方式来创建apisix的路由规则。需要提前创建好k8s service作为路由的后端标识来关联endpoints。\n0. 创建k8s service apiVersion: v1 kind: Service metadata: name: {APP}-service namespace: {NAMESPACE} spec: selector: k8s-app: {APP} ports: - name: http protocol: TCP port: 80 targetPort: 9376 路由规则主要包括：\nhosts：域名\npaths：访问路径\nbackends:\nserviceName\nservicePort\n1. 使用ApisixRoute创建路由规则 使用ApisixRoute自定义CRD创建路由规则，具体参考：reference。\n示例：\n# httpbin-route.yaml apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: httpserver-route spec: http: - name: rule1 match: hosts: - local.httpbin.org paths: - /* backends: - serviceName: httpbin servicePort: 80 在k8s中创建ApisixRoute。\nkubectl apply -f httpbin-route.yaml 2. 使用ingress创建路由规则 使用k8s ingress来创建路由规则，示例如下：\n# httpbin-ingress.yaml # Note use apiVersion is networking.k8s.io/v1, so please make sure your # Kubernetes cluster version is v1.19.0 or higher. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: httpserver-ingress spec: # apisix-ingress-controller is only interested in Ingress # resources with the matched ingressClass name, in our case, # it's apisix. ingressClassName: apisix rules: - host: local.httpbin.org http: paths: - backend: service: name: httpbin port: number: 80 path: / pathType: Prefix 创建k8s ingress。\nkubectl apply -f httpbin-ingress.yaml 3. 使用Admin API创建路由规则 直接调用Admin API或者使用dashboard创建路由规则。\n3.1. 一键创建路由和upstream curl \"http://127.0.0.1:9180/apisix/admin/routes/1\" -H \"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\" -X PUT -d ' { \"methods\": [\"GET\"], \"host\": \"example.com\", \"uri\": \"/anything/*\", \"upstream\": { \"type\": \"roundrobin\", \"nodes\": { \"httpbin.org:80\": 1 } } }' 3.2. 分开创建路由和upstream 推荐使用分开创建路由和upstream。\n创建upstream\ncurl \"http://127.0.0.1:9180/apisix/admin/upstreams/1\" -H \"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\" -X PUT -d ' { \"type\": \"roundrobin\", \"nodes\": { \"httpbin.org:80\": 1 } }' 创建路由绑定upstream\ncurl \"http://127.0.0.1:9180/apisix/admin/routes/1\" -H \"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\" -X PUT -d ' { \"uris\": [\"/get\",\"/list\"], \"host\": \"httpbin.org\", \"upstream_id\": \"1\" }' 删除upstream\nDELETE /apisix/admin/upstreams/{id}\n删除route\nDELETE /apisix/admin/routes/{id}\n4. 验证路由规则 基于上述的方式，apisix-ingress-controller会调用apisix admin的接口自动创建routes和upstreams两个信息存入etcd，通过业务域名访问apisix就可以访问到具体的pod。\n服务调用\n将业务域名解析到apisix的IP上（如果是物理机部署可以是VIP，或者k8s部署的clusterIP）。\n访问业务域名：\ncurl -v http://local.httpbin.org 5. 查看etcd中的路由规则 etcdctl get /apisix --prefix 5.1. routes /apisix/routes/\n通过ingress创建的routes\n{ \"host\": \"local.httpbin.org\", \"create_time\": 1661251916, \"name\": \"ing_default_httpserver-ingress_37a4f3ae\", \"status\": 1, \"uris\": [ \"\\/\", \"\\/*\" ], \"upstream_id\": \"5ce57b8e\", \"labels\": { \"managed-by\": \"apisix-ingress-controller\" }, \"priority\": 0, \"desc\": \"Created by apisix-ingress-controller, DO NOT modify it manually\", \"update_time\": 1661397119, \"id\": \"148730bb\" } 通过ApisixRoute创建的routes\n{ \"priority\": 0, \"create_time\": 1661397584, \"name\": \"default_httpserver-route_rule1\", \"status\": 1, \"uris\": [ \"\\/*\" ], \"upstream_id\": \"5ce57b8e\", \"hosts\": [ \"local.httpbin.org\" ], \"labels\": { \"managed-by\": \"apisix-ingress-controller\" }, \"desc\": \"Created by apisix-ingress-controller, DO NOT modify it manually\", \"update_time\": 1661397584, \"id\": \"add8e28c\" } 5.2. upstreams /apisix/upstreams/5ce57b8e\n相同的backend，使用ingress或ApisixRoute创建后生成的upstreams相同。\n{ \"scheme\": \"http\", \"pass_host\": \"pass\", \"name\": \"default_httpbin_80\", \"nodes\": [ { \"host\": \"10.244.3.167\", \"priority\": 0, \"port\": 80, \"weight\": 100 } ], \"type\": \"roundrobin\", \"labels\": { \"managed-by\": \"apisix-ingress-controller\" }, \"hash_on\": \"vars\", \"create_time\": 1661251916, \"id\": \"5ce57b8e\", \"update_time\": 1661397584, \"desc\": \"Created by apisix-ingress-controller, DO NOT modify it manually\" } 参考：\nhttps://apisix.apache.org/zh/docs/ingress-controller/getting-started/\nhttps://apisix.apache.org/zh/docs/ingress-controller/tutorials/index/\nhttps://apisix.apache.org/zh/docs/ingress-controller/tutorials/proxy-the-httpbin-service/\nhttps://apisix.apache.org/zh/docs/ingress-controller/tutorials/proxy-the-httpbin-service-with-ingress/\n","categories":"","description":"","excerpt":"本文主要介绍三种方式来创建apisix的路由规则。需要提前创建好k8s service作为路由的后端标识来关联endpoints。\n0. 创 …","ref":"/kubernetes-notes/network/gateway/apisix-ingress-controller-usage/","tags":["ApiSix"],"title":"创建路由"},{"body":"1. 安装kubevirt 1.1. 修改镜像仓库 针对私有环境，需要将所需镜像上传到自己的镜像仓库中。\n涉及的镜像组件有\nvirt-operator virt-api virt-controller virt-launcher 重命名镜像脚本如下:\n#!/bin/bash # kubevirt组件版本 version=$1 # 私有镜像仓库 registry=$2 # 私有镜像仓库的namespace namespace=$3 kubevirtRegistry=\"quay.io/kubevirt\" readonly APPLIST=( virt-operator virt-api virt-controller virt-launcher ) for app in \"${APPLIST[@]}\"; do # 拉取镜像 docker pull ${kubevirtRegistry}/${app}:${version} # 重命名 docker tag ${kubevirtRegistry}/${app}:${version} ${registry}/${namespace}/${app}:${version} # 推送镜像 docker push ${registry}/${namespace}/${app}:${version} done echo \"重新命名成功\" 1.2. 部署virt-operator 通过kubevirt operator安装kubevirt相关组件，选择指定版本，下载kubevirt-operator.yaml和kubevirt-cr.yaml文件，并创建k8s相关对象。\n如果是私有镜像仓库，则需要将kubevirt-operator.yaml文件中镜像的名字替换为私有镜像仓库的地址，并提前按步骤1推送所需镜像到私有镜像仓库。\n# Pick an upstream version of KubeVirt to install $ export RELEASE=v0.52.0 # Deploy the KubeVirt operator $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml # Create the KubeVirt CR (instance deployment request) which triggers the actual installation $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml # wait until all KubeVirt components are up $ kubectl -n kubevirt wait kv kubevirt --for condition=Available 1.3. 部署virtctl virtctl用来启动和关闭虚拟机。\nVERSION=$(kubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt -o=jsonpath=\"{.status.observedKubeVirtVersion}\") ARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed 's/x86_64/amd64/') || windows-amd64.exe echo ${ARCH} curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-${ARCH} chmod +x virtctl sudo install virtctl /usr/local/bin 2. kubevirt部署产物 通过手动部署virt-operator，会自动部署以下组件\n组件 部署方式 副本数 virt-api deployment 2 virt-controller deployment 2 virt-handler daemonset - 具体参考:\n#kg all -n kubevirt NAME READY STATUS RESTARTS AGE pod/virt-api-5fb5cffb7f-hgjjh 1/1 Running 0 23h pod/virt-api-5fb5cffb7f-jcp7x 1/1 Running 0 23h pod/virt-controller-844cd4f58c-h8vsx 1/1 Running 0 23h pod/virt-controller-844cd4f58c-vlxqs 1/1 Running 0 23h pod/virt-handler-lb5ft 1/1 Running 0 23h pod/virt-handler-mtr4d 1/1 Running 0 22h pod/virt-handler-sxd2t 1/1 Running 0 23h pod/virt-operator-8595f577cd-b9txg 1/1 Running 0 23h pod/virt-operator-8595f577cd-p2f69 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubevirt-operator-webhook ClusterIP 10.254.159.81 \u003cnone\u003e 443/TCP 23h service/kubevirt-prometheus-metrics ClusterIP 10.254.7.231 \u003cnone\u003e 443/TCP 23h service/virt-api ClusterIP 10.254.244.139 \u003cnone\u003e 443/TCP 23h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/virt-handler 3 3 3 3 3 kubernetes.io/os=linux 23h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/virt-api 2/2 2 2 23h deployment.apps/virt-controller 2/2 2 2 23h deployment.apps/virt-operator 2/2 2 2 23h NAME DESIRED CURRENT READY AGE replicaset.apps/virt-api-5fb5cffb7f 2 2 2 23h replicaset.apps/virt-controller-844cd4f58c 2 2 2 23h replicaset.apps/virt-operator-8595f577cd 2 2 2 23h NAME AGE PHASE kubevirt.kubevirt.io/kubevirt 23h Deployed 3. 创建虚拟机 通过vm.yaml创建虚拟机\n# 下载vm.yaml wget https://kubevirt.io/labs/manifests/vm.yaml # 创建虚拟机 kubectl apply -f https://kubevirt.io/labs/manifests/vm.yaml vm.yaml文件\napiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: testvm spec: running: false template: metadata: labels: kubevirt.io/size: small kubevirt.io/domain: testvm spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default masquerade: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userDataBase64: SGkuXG4= 查看虚拟机\nkubectl get vms kubectl get vms -o yaml testvm 启动或暂停虚拟机\n# 启动虚拟机 virtctl start testvm # 关闭虚拟机 virtctl stop testvm # 进入虚拟机 virtctl console testvm 删除虚拟机\nkubectl delete vm testvm 参考：\nInstallation - KubeVirt User-Guide KubeVirt quickstart with Minikube | KubeVirt.io Use KubeVirt | KubeVirt.io ","categories":"","description":"","excerpt":"1. 安装kubevirt 1.1. 修改镜像仓库 针对私有环境，需要将所需镜像上传到自己的镜像仓库中。 …","ref":"/kubernetes-notes/kvm/kubevirt/kubevirt-installation/","tags":["KubeVirt"],"title":"KubeVirt的使用"},{"body":"1. 文件系统 文件系统就是分区或磁盘上的所有文件的逻辑集合。文件系统不仅包含着文件中的数据而且还有文件系统的结构，所有Linux 用户和程序看到的文件、目录、软连接及文件保护信息等都存储在其中。\n不同Linux发行版本之间的文件系统差别很少，主要表现在系统管理的特色工具以及软件包管理方式的不同，文件目录结构基本上都是一样的。\next2 ： 早期linux中常用的文件系统； ext3 ： ext2的升级版，带日志功能； RAMFS ： 内存文件系统，速度很快； iso9660：光盘或光盘镜像； NFS ： 网络文件系统，由SUN发明，主要用于远程文件共享； MS-DOS ： MS-DOS文件系统； FAT ： Windows XP 操作系统采用的文件系统； NTFS ： Windows NT/XP 操作系统采用的文件系统。 2. 分区与目录 文件系统位于磁盘分区中；一个硬盘可以有多个分区，也可以只有一个分区；一个分区只能包含一个文件系统。\nLinux文件系统与Windows有较大的差别：\nWindows的文件结构是多个并列的树状结构，最顶部的是不同的磁盘（分区），如 C、D、E、F等。\nLinux的文件结构是单个的树状结构，根目录是“/”，其他目录都要位于根目录下。\n每次安装系统的时候我们都会进行分区，Linux下磁盘分区和目录的关系如下：\n任何一个分区都必须对应到某个目录上，才能进行读写操作，称为“挂载”。 被挂载的目录可以是根目录，也可以是其他二级、三级目录，任何目录都可以是挂载点。 目录是逻辑上的区分。分区是物理上的区分。 根目录是所有Linux的文件和目录所在的地方，需要挂载上一个磁盘分区。 下图是常见的目录和分区的对应关系：\n更详细的目录路径功能如下：\n为什么要分区，如何分区？\n可以把不同资料，分别放入不同分区中管理，降低风险。 大硬盘搜索范围大，效率低。 /home、/var、/usr/local 经常是单独分区，因为经常会操作，容易产生碎片。 为了便于定位和查找，Linux中的每个目录一般都存放特定类型的文件，下表列出了各种Linux发行版本的常见目录：\n目录 说明 / 根目录，只能包含目录，不能包含具体文件。 /bin 存放可执行文件。很多命令就对应/bin目录下的某个程序，例如 ls、cp、mkdir。/bin目录对所有用户有效。 /dev 硬件驱动程序。例如声卡、磁盘驱动等，还有如 /dev/null、/dev/console、/dev/zero、/dev/full 等文件。 /etc 主要包含系统配置文件和用户、用户组配置文件。 /lib 主要包含共享库文件，类似于Windows下的DLL；有时也会包含内核相关文件。 /boot 系统启动文件，例如Linux内核、引导程序等。 /home 用户工作目录（主目录），每个用户都会分配一个目录。 /mnt 临时挂载文件系统。这个目录一般是用于存放挂载储存设备的挂载目录的，例如挂载CD-ROM的cdrom目录。 /proc 操作系统运行时，进程（正在运行中的程序）信息及内核信息（比如cpu、硬盘分区、内存信息等）存放在这里。/proc目录伪装的文件系统proc的挂载目录，proc并不是真正的文件系统。 /tmp 临时文件目录，系统重启后不会被保存。 /usr /user目下的文件比较混杂，包含了管理命令、共享文件、库文件等，可以被很多用户使用。 /var 主要包含一些可变长度的文件，会经常对数据进行读写，例如日志文件和打印队列里的文件。 /sbin 和 /bin 类似，主要包含可执行文件，不过一般是系统管理所需要的，不是所有用户都需要。 3. 常用文件管理命令 你可以通过下面的命令来管理文件：\nCommand Description cat filename 查看文件内容。 cd dirname 改变所在目录。 cp file1 file2 复制文件或目录。 file filename 查看文件类型(binary, text, etc)。 find filename dir 搜索文件或目录。 head filename 显示文件的开头，与tail命令相对。 less filename 查看文件的全部内容，可以分页显示，比more命令要强大。 ls dirname 遍历目录下的文件或目录。 mkdir dirname 创建目录。 more filename 查看文件的全部内容，可以分页显示。 mv file1 file2 移动文件或重命名。 pwd 显示用户当前所在目录。 rm filename 删除文件。 rmdir dirname 删除目录。 tail filename 显示文件的结尾，与head命令相对。 touch filename 文件不存在时创建一个空文件，存在时修改文件时间戳。 whereis filename 查看文件所在位置。 which filename 如果文件在环境变量PATH中有定义，那么显示文件位置。 3.1. df命令 管理磁盘分区时经常会使用 df (disk free) 命令，df -k 命令可以用来查看磁盘空间的使用情况（以千字节计），例如：\n$df -k Filesystem 1K-blocks Used Available Use% Mounted on /dev/vzfs 10485760 7836644 2649116 75% / /devices 0 0 0 0% /devices 每一列的含义如下：\n列 说明 Filesystem 代表文件系统对应的设备文件的路径名（一般是硬盘上的分区）。 kbytes 分区包含的数据块（1024字节）的数目。 used 已用空间。 avail 可用空间。 capacity 已用空间的百分比。 Mounted on 文件系统挂载点。 某些目录（例如 /devices）的 kbytes、used、avail 列为0，use列为0%，这些都是特殊（或虚拟）文件系统，即使位于根目录下，也不占用硬盘空间。\n你可以结合 -h (human readable) 选项将输出信息格式化，让人更易阅读。\n3.2. du 命令 du (disk usage) 命令可以用来查看特定目录的空间使用情况。\ndu 命令会显示每个目录所占用数据块。根据系统的不同，一个数据块可能是 512 字节或 1024 字节。举例如下：\n$du /etc 10 /etc/cron.d 126 /etc/default 6 /etc/dfs ... 结合 -h 选项可以让信息显示的更加清晰：\n$du -h /etc 5k /etc/cron.d 63k /etc/default 3k /etc/dfs ... 4. 挂载文件系统 挂载是指将一个硬件设备（例如硬盘、U盘、光盘等）对应到一个已存在的目录上。 若要访问设备中的文件，必须将文件挂载到一个已存在的目录上， 然后通过访问这个目录来访问存储设备。\n这样就为用户提供了统一的接口，屏蔽了硬件设备的细节。Linux将所有的硬件设备看做文件，对硬件设备的操作等同于对文件的操作。\n注意：挂载目录可以不为空，但挂载后这个目录下以前的内容将不可用。\n需要知道的是，光盘、软盘、其他操作系统使用的文件系统的格式与linux使用的文件系统格式是不一样的，挂载需要确认Linux是否支持所要挂载的文件系统格式。\n查看当前系统所挂载的硬件设备可以使用 mount 命令：\n$ mount /dev/vzfs on / type reiserfs (rw,usrquota,grpquota) proc on /proc type proc (rw,nodiratime) devpts on /dev/pts type devpts (rw) 一般约定，/mnt 为临时挂载目录，例如挂载CD-ROM、远程网络设备、软盘等。 也可以通过mount命令来挂载文件系统，语法为：\nmount -t file_system_type device_to_mount directory_to_mount_to 例如：\n将 CD-ROM 挂载到 /mnt/cdrom 目录。\n$ mount -t iso9660 /dev/cdrom /mnt/cdrom 注意：file_system_type用来指定文件系统类型，通常可以不指定，Linux会自动正确选择文件系统类型。\n挂载文件系统后，就可以通过 cd、cat 等命令来操作对应文件。\n可以通过 umount 命令来卸载文件系统。例如，卸载 cdrom：\n$ umount /dev/cdrom 不过，大部分现代的Linux系统都有自动挂载卸载功能，unmount 命令较少用到。\n5. 用户和群组配额 用户和群组配额可以让管理员为每个用户或群组分配固定的磁盘空间。 管理员有两种方式来分配磁盘空间：\n软限制：如果用户超过指定的空间，会有一个宽限期，等待用户释放空间。 硬限制：没有宽限期，超出指定空间立即禁止操作。 下面的命令可以用来管理配额：\n命令 说明 quota 显示磁盘使用情况以及每个用户组的配额。 edquota 编辑用户和群组的配额。 quotacheck 查看文件系统的磁盘使用情况，创建、检查并修复配额文件。 setquota 设置配额。 quotaon 开启用户或群组的配额功能。 quotaoff 关闭用户或群组的配额功能。 repquota 打印指定文件系统的配额。 参考：\nhttp://c.biancheng.net/cpp/linux/ ","categories":"","description":"","excerpt":"1. 文件系统 文件系统就是分区或磁盘上的所有文件的逻辑集合。文件系统不仅包含着文件中的数据而且还有文件系统的结构，所有Linux 用户和程 …","ref":"/linux-notes/file/linux-file-system/","tags":["Linux"],"title":"Linux文件系统"},{"body":" 本文由网络内容整理而成的笔记\n1. LVM简介 LVM是逻辑盘卷管理（Logical Volume Manager）的简称，它是Linux环境下对磁盘分区进行管理的一种机制，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。\n优点：\n可以灵活分配和管理磁盘空间\n可以对分区进行动态的扩容\n可以增加新的磁盘到lvm中\n2. LVM核心概念 LVM概念图：\nPV（Physical Volume）物理卷 磁盘分区后（还未格式化为文件系统）使用 pvcreate 命令可以将硬盘分区创建为 pv，此分区的 systemID 为8e，即为 LVM 格式的系统标识符。 VG（Volume Group）卷组 将多个 PV 组合起来，使用 vgcreate 命令创建成卷组。卷组包含了多个 PV，相当于重新整合了多个分区后得到的硬盘。虽然 VG 整合了多个 PV，但是创建 VG 时会将所有空间根据指定 PE 大小划分为多个 PE，在 LVM 模式下的存储都是以 PE 为单元，类似于文件系统的 Block。 PE（Physical Extend）物理存储单元 PE 是 VG 中的存储单元。实际存储的数据都是在 PE 存储。 LV（Logical Volume）逻辑卷 如果说VG是整合分区为硬盘，那么 LV 就是把这个硬盘重新的分区，只不过该分区是通过 VG 来划分的。VG 中有很多 PE 单元，可以指定将多少 PE 划分给一个 LV，也可以直接指定大小来划分。划分 LV 后就相当于划分了分区，只需要对 LV 进行格式化即可变成普通的文件系统。 LE（Logical extent）逻辑存储单元 LE 则是逻辑存储单元，即 LV 中的逻辑存储单元，和 PE 的大小一样。从 VG 中划分 LV，实际上是从 VG 中划分 VG 中的 PE，只不过划分 LV 后它不在称为 PE，而是 LE。 3. LVM原理 LVM 之所以能够伸缩容量，实现的方法就是讲 LV 里空闲的 PE 移出，或向 LV 中添加空闲的 PE。\n4. 格式化为LVM盘 4.1. fdisk格式化2T以下磁盘 # 使用fdisk进行盘的格式化 fdisk /dev/vdb # 以下是交互输出结果 Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0xadfbfcb4. Command (m for help): n # 新建分区 Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p # 待定主分区 Partition number (1-4, default 1): 1 # 序号 First sector (2048-1048575999, default 2048): # 直接回车 Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-1048575999, default 1048575999): # 直接回车 Using default value 1048575999 Partition 1 of type Linux and of size 500 GiB is set Command (m for help): p # 确认分区情况 Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xadfbfcb4 Device Boot Start End Blocks Id System /dev/vdb1 2048 1048575999 524286976 83 Linux Command (m for help): t # 选择系统id Selected partition 1 Hex code (type L to list all codes): 8e # 8e指定的是使用LVM Changed type of partition 'Linux' to 'Linux LVM' Command (m for help): w # 保存 The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. 4.2. parted格式化2T以上磁盘 # parted /dev/sdk GNU Parted 3.1 使用 /dev/sdk Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) mktable 新的磁盘标签类型？ gpt (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name 标志 (parted) mkpart 分区名称？ []? 文件系统类型？ [ext2]? 起始点？ 0g 结束点？ 4000G (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name 标志 1 1049kB 4000GB 4000GB (parted) toggle 1 lvm (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name 标志 1 1049kB 4000GB 4000GB lvm (parted) quit 信息: You may need to update /etc/fstab. 5. LVM操作 # pvcreate如果提示命令不存在，则需要安装lvm2 yum install lvm2 -y 5.1. 创建物理卷（PV） # pvcreate /dev/nvme1n1p1 /dev/nvme2n1p1 Physical volume \"/dev/nvme1n1p1\" successfully created. Physical volume \"/dev/nvme2n1p1\" successfully created. # 使用pvs或者 pvdisplay 查看结果 # pvs PV VG Fmt Attr PSize PFree /dev/nvme1n1p1 lvm2 --- 931.51g 931.51g /dev/nvme2n1p1 lvm2 --- 931.51g 931.51g 5.2. 创建卷组（VG） # vgcreate vgdata /dev/nvme1n1p1 /dev/nvme2n1p1 Volume group \"vgdata\" successfully created # 使用vgs 查看vg, vgdisplay的信息 # lsblk查看 # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 931.5G 0 disk /pcdn_data/storage1_ssd nvme2n1 259:2 0 931.5G 0 disk └─nvme2n1p1 259:5 0 931.5G 0 part └─vgdata-data 251:2 0 1.8T 0 lvm /vgdata nvme1n1 259:1 0 931.5G 0 disk └─nvme1n1p1 259:4 0 931.5G 0 part └─vgdata-data 251:2 0 1.8T 0 lvm /vgdata 5.3. 创建逻辑卷（LV） # lvcreate -L 后面是大小， -n 后面是逻辑卷名称， vgdata对应上面的卷组 # lvcreate -L 1.8T -n data vgdata Rounding up size to full physical extent 1.80 TiB Logical volume \"data\" created. # 使用lvdisplay 查看结果 5.4. 格式化文件系统及挂载 # 查看磁盘信息 # fdisk -l 磁盘 /dev/mapper/vgdata-data：1979.1 GB, 1979124285440 字节，3865477120 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 # 格式化成xfs, /dev/vgdata/data为上面 LV Path mkfs.xfs /dev/vgdata/data # mount mkdir -p /data mount /dev/vgdata/data /data 5.5. LVM扩容 LVM最大的优势就是其可伸缩性，伸缩性有更加偏重与扩容。扩容的实质是将 VG 中的空闲 PE 添加到 LV 中，所以只要 VG 中有空闲的 PE，就可以进行扩容。即使没有空闲 PE，也可以添加PV，将PV加入到VG中增加空闲PE。\n扩容的两个关键步骤：\n（1）使用 lvextend 或 lvresize 添加更多的 PE 或容量到 LV\n（2）使用 resize2fs命令（xfs 使用 xfs_growfs）将 LV 增加后的容量添加到对应的文件系统中(此过程是修改文件系统而非LVM内容)\n6. LVM相关命令 6.1. 管理 PV 功能 命令 创建 PV pvcreate 扫描并列出所有 PV pvscan 列出 PV 属性 pvdisplay {name|size} 移除 PV pvremove 移动 PV 中的数据 pvmove 6.2. 管理 VG 功能 命令 创建 VG vgcreate 扫描并列出所有 VG vgscan 列出 VG 属性信息 vgdisplay 移除（删除）VG vgremove 从 VG 中移除 PV vgreduce 将 PV 添加到 VG 中 vgextend 修改 VG 属性 vgchange 6.3. 管理 LV 功能 命令 创建 LV lvcreate 扫描并列出所有 LV lvscan 列出 LV 属性信息 lvdisplay 移除 LV lvremove 缩小 LV 容量 lvreduce/lvresize 增大 LV 容量 lvextend/lvresize 调整 LV 容量 lvresize lvcreate命令\n一般用法：lvcreate [-L size(M/G) | -l PEnum] -n lv_name vg_name\n选项：\n-L：根据大小创建 LV，即分配多少空间给此 LV\n-l：根据 PE 的数量来创建 LV，即分配多少个 PE 给此 LV\n-n：指定 LV 名称\n参考：\nLinux下使用lvm将多块盘合并 | Z.S.K.'s Records\n100个Linux命令(5)-LVM - 云+社区 - 腾讯云\nLVM数据卷 - 容器服务 ACK - 阿里云\n","categories":"","description":"","excerpt":" 本文由网络内容整理而成的笔记\n1. LVM简介 LVM是逻辑盘卷管理（Logical Volume Manager）的简称，它是Linux …","ref":"/linux-notes/disk/lvm-usage/","tags":["disk"],"title":"LVM的使用"},{"body":"类型 1. 基础类型 1.1. 布尔类型 //布尔类型的关键字为bool,值为true或false，不可写为0或1 var v1 bool v1=true //接受表达式判断赋值，不支持自动或强制类型转换 v2:=(1==2) 1.2. 整型 //1、类型表示 //int和int32为不同类型，不会自动类型转换需要强制类型转换 //强制类型转换需注意精度损失（浮点数→整数），值溢出（大范围→小范围） var v2 int32 v1:=64 v2=int32(v1) //2、数值运算,支持“+,-,*,/和%” 5%3 //求余 //3、比较运算,“\u003c,\u003e,==,\u003e=,\u003c=,!=” //不同类型不能进行比较例如int和int8，但可以与字面常量（literal）进行比较 var i int32 var j int64 i,j=1,2 if i==j //编译错误，不同类型不能进行比较 if i==1 || j==2 //编译通过，可以与字面常量（literal）进行比较 //4、位运算 //Go(^x)取反与C语言(~x)不同，其他类似，具体见下表 1.3. 浮点型 //1、浮点型分为float32(类似C中的float)，float64(类似C中的double) var f1 float32 f1=12 //不加小数点，被推导为整型 f2:=12.0 //加小数点，被推导为float64 f1=float32(f2) //需要执行强制转换 //2、浮点数的比较 //浮点数不是精确的表达方式，不能直接使用“==”来判断是否相等，可以借用math的包math.Fdim 1.4. 复数类型 //1、复数的表示 var v1 complex64 v1=3.2+12i //v1 v2 v3 表示为同一个数 v2:=3.2+12i v3:=complex(3.2,12) //2、实部与虚部 //z=complex(x,y),通过内置函数实部x=real(z),虚部y=imag(z) 1.5. 字符串 //声明与赋值 var str string str=\"hello world\" 1.6. 字符类型 //1、byte，即uint8的别名 //2、rune，即Unicode 1.7. 错误类型（error） 2. 复合类型 2.1. 数组(array) 数组表示同一类型数据，数组长度定义后就不可更改，长度是数组内的一个内置常量，可通过len()来获取。\n//1、创建数组 var array1 [5]int //声明：var 变量名 类型 var array2 [5]int=[5]int{1,2,3,4,5} //初始化 array3：=[5]int{1,2,3,4,5} //直接用“：=”赋值 [3][5]int //二维数组 [3]*float //指针数组 //2、元素访问 for i,v:=range array{ //第一个返回值为数组下标，第二个为元素的值 } //3、值类型 //数组在Go中作为一个值类型，值类型在赋值和函数参数传递时，只复制副本，因此在函数体中并不能改变数组的内容，需用指针来改变数组的值。 2.2. 切片(slice) ​\t数组在定义了长度后无法改变，且作为值类型在传递时产生副本，并不能改变数组元素的值。因此切片的功能弥补了这个不足，切片类似指向数组的一个指针。可以抽象为三个变量：指向数组的指针；切片中元素的个数(len函数)；已分配的存储空间(cap函数)。\n//1、创建切片 //a)基于数组创建 var myArray [5]int=[5]{1,2,3,4,5} var mySlice []int=myArray[first:last] slice1=myArray[:] //基于数组所有元素创建 slice2=myArray[:3] //基于前三个元素创建 slice3=myArray[3:] //基于第3个元素开始后的所有元素创建 //b)直接创建 slice1:=make([]int,5) //元素初始值为0，初始个数为5 slice2:=make([]int,5,10) //元素初始值为0，初始个数为5，预留个数为10 slice3:=[]int{1,2,3,4,5} //初始化赋值 //c)基于切片创建 oldSlice:=[]int{1,2,3,4,5} newSlice:=oldSlice[:3] //基于切片创建，不能超过原切片的存储空间(cap函数的值) //2、元素遍历 for i,v:=range slice{ //与数组的方式一致，使用range来遍历 //第一个返回值(i)为索引，第二个为元素的值(v) } //3、动态增减元素 //切片分存储空间(cap)和元素个数(len)，当存储空间小于实际的元素个数，会重新分配一块原空间2倍的内存块，并将原数据复制到该内存块中，合理的分配存储空间可以以空间换时间，降低系统开销。 //添加元素 newSlice:=append(oldSlice,1,2,3) //直接将元素加进去，若存储空间不够会按上述方式扩容。 newSlice1:=append(oldSlice1,oldSlice2...) //将oldSlice2的元素打散后加到oldSlice1中，三个点不可省略。 //4、内容复制 //copy()函数可以复制切片，如果切片大小不一样，按较小的切片元素个数进行复制 slice1:=[]int{1,2,3,4,5} slice2:=[]int{6,7,8} copy(slice2,slice1) //只会复制slice1的前三个元素到slice2中 copy(slice1,slice1) //只会复制slice2的三个元素到slice1中的前三个位置 2.3. 键值对(map) map是一堆键值对的未排序集合。\n//1、先声明后创建再赋值 var map1 map[键类型] 值类型 //创建 map1=make(map[键类型] 值类型) map1=make(map[键类型] 值类型 存储空间) //赋值 map1[key]=value // 直接创建 m2 := make(map[string]string) // 然后赋值 m2[\"a\"] = \"aa\" m2[\"b\"] = \"bb\" // 初始化 + 赋值一体化 m3 := map[string]string{ \"a\": \"aa\", \"b\": \"bb\", } //2、元素删除 //delete()函数删除对应key的键值对，如果key不存在，不会报错；如果value为nil，则会抛出异常(panic)。 delete(map1,key) //3、元素查找 value,ok:=myMap[key] if ok{//如果找到 //处理找到的value值 } //遍历 for key,value:=range myMap{ //处理key或value } map可以用来判断一个值是否在切片或数组中。\n// 判断某个类型（假如为myType）的值是否在切片或数组（假如为myList）中 // 构造一个map,key的类型为myType,value为bool型 myMap := make(map[myType]bool) myList := []myType{value1, value2} // 将切片中的值存为map中的key（因为key不能重复）,map的value都为true for _, value := range myList { myMap[value] = true } // 判断valueX是否在myList中，即判断其是否在myMap的key中 if _, ok := myMap[valueX]; ok { // 如果valueX 在myList中，执行后续操作 } 2.4. 指针(pointer) 具体参考Go语言指针详解\n2.5. 结构体(struct) 具体参考Go面向对象编程之结构体\n2.6. 接口(interface) 具体参考Go面向对象编程之接口\n2.7. 通道(chan) 具体参考Go并发编程之channel\n","categories":"","description":"","excerpt":"类型 1. 基础类型 1.1. 布尔类型 //布尔类型的关键字为bool,值为true或false，不可写为0或1 var v1 bool …","ref":"/golang-notes/basis/data-types/","tags":["Golang"],"title":"数据类型"},{"body":" 本文以cobra-demo为例介绍cobra添加命令的具体使用操作。\n0. cobra-demo cobra-demo编译二进制执行的结果如下。具体代码参考：https://github.com/huweihuang/cobra-demo\n$ ./cobra-demo A longer description that spans multiple lines and likely contains examples and usage of using your application. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobra-demo [command] Available Commands: config A brief description of your command help Help about any command server A brief description of your command Flags: --config string config file (default is $HOME/.cobra-demo.yaml) -h, --help help for cobra-demo -t, --toggle Help message for toggle Use \"cobra-demo [command] --help\" for more information about a command. 1. cobra init cobra init 的具体使用参考 init\n1.1. cobra init --pkg-name cobra init \u003ccobra-demo\u003e --pkg-name github.com/huweihuang/cobra-demo -a 'author name \u003cemail\u003e' 执行以上命令，创建的文件目录结构如下：\n./ ├── LICENSE ├── cmd │ ├── root.go └── main.go 1.2. main.go package main import \"github.com/huweihuang/cobra-demo/cmd\" func main() { cmd.Execute() } 1.3. cmd/root.go package cmd import ( \"fmt\" \"os\" \"github.com/spf13/cobra\" homedir \"github.com/mitchellh/go-homedir\" \"github.com/spf13/viper\" ) var cfgFile string // rootCmd represents the base command when called without any subcommands var rootCmd = \u0026cobra.Command{ Use: \"cobra-demo\", Short: \"A brief description of your application\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your application. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, // Uncomment the following line if your bare application // has an action associated with it: //\tRun: func(cmd *cobra.Command, args []string) { }, } // Execute adds all child commands to the root command and sets flags appropriately. // This is called by main.main(). It only needs to happen once to the rootCmd. func Execute() { if err := rootCmd.Execute(); err != nil { fmt.Println(err) os.Exit(1) } } func init() { cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings. // Cobra supports persistent flags, which, if defined here, // will be global for your application. rootCmd.PersistentFlags().StringVar(\u0026cfgFile, \"config\", \"\", \"config file (default is $HOME/.cobra-demo.yaml)\") // Cobra also supports local flags, which will only run // when this action is called directly. rootCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\") } // initConfig reads in config file and ENV variables if set. func initConfig() { if cfgFile != \"\" { // Use config file from the flag. viper.SetConfigFile(cfgFile) } else { // Find home directory. home, err := homedir.Dir() if err != nil { fmt.Println(err) os.Exit(1) } // Search config in home directory with name \".cobra-demo\" (without extension). viper.AddConfigPath(home) viper.SetConfigName(\".cobra-demo\") } viper.AutomaticEnv() // read in environment variables that match // If a config file is found, read it in. if err := viper.ReadInConfig(); err == nil { fmt.Println(\"Using config file:\", viper.ConfigFileUsed()) } } 2. cobra add cobra add 的具体使用参考 add\n2.1. cobra add command cobra add serve -a 'author name \u003cemail\u003e' cobra add config -a 'author name \u003cemail\u003e' cobra add create -p 'configCmd' -a 'author name \u003cemail\u003e'# 在父命令config命令下创建子命令create,若没有指定-p,默认的父命令为rootCmd。 执行以上命令，创建的文件目录结构如下：\n./ ├── LICENSE ├── cmd │ ├── config.go # rootCmd的子命令 │ ├── create.go # config的子命令 │ ├── root.go # 默认父命令 │ └── server.go # rootCmd的子命令 └── main.go 2.2. cmd/config.go package cmd import ( \"fmt\" \"github.com/spf13/cobra\" ) // configCmd represents the config command var configCmd = \u0026cobra.Command{ Use: \"config\", Short: \"A brief description of your command\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"config called\") }, } func init() { rootCmd.AddCommand(configCmd) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags which will work for this command // and all subcommands, e.g.: // configCmd.PersistentFlags().String(\"foo\", \"\", \"A help for foo\") // Cobra supports local flags which will only run when this command // is called directly, e.g.: // configCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\") } 2.3. cmd/create.go create为config的子命令。\npackage cmd import ( \"fmt\" \"github.com/spf13/cobra\" ) // createCmd represents the create command var createCmd = \u0026cobra.Command{ Use: \"create\", Short: \"A brief description of your command\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"create called\") }, } func init() { configCmd.AddCommand(createCmd) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags which will work for this command // and all subcommands, e.g.: // createCmd.PersistentFlags().String(\"foo\", \"\", \"A help for foo\") // Cobra supports local flags which will only run when this command // is called directly, e.g.: // createCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\") } 参考：\nhttps://github.com/spf13/cobra https://github.com/spf13/cobra/blob/master/cobra/README.md ","categories":"","description":"","excerpt":" 本文以cobra-demo为例介绍cobra添加命令的具体使用操作。\n0. cobra-demo cobra-demo编译二进制执行的结果 …","ref":"/golang-notes/framework/cobra/cobra-command/","tags":["Golang"],"title":"cobra command"},{"body":"1. 简介 iptables是一个设置防火墙（netfilter）规则的命令工具。网络规则包括源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等，当数据包与规则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。配置防火墙的主要工作就是添加、修改和删除这些规则。\n2. 基本概念 2.1. 链(Chain) 网络设置的”关卡“一般有多个网络规则，称为链。\nINPUT\nOUTPUT\nFORWORD\nPREROUTING\nPOSTROUTING\n2.2. 表 具有相同功能的规则的集合叫做”表”。iptables定义了四类表。\nfilter表：负责过滤功能，防火墙；内核模块：iptables_filter\nnat表：network address translation，网络地址转换功能；内核模块：iptable_nat\nmangle表：拆解报文，做出修改，并重新封装 的功能；iptable_mangle\nraw表：关闭nat表上启用的连接追踪机制；iptable_raw\n2.3. 表和链的关系 PREROUTING的规则可以存在于：raw表，mangle表，nat表。\nINPUT的规则可以存在于：mangle表，filter表。\nFORWARD的规则可以存在于：mangle表，filter表。\nOUTPUT的规则可以存在于：raw表mangle表，nat表，filter表。\nPOSTROUTING的规则可以存在于：mangle表，nat表。\n3. 规则匹配条件 基本匹配条件\n源地址Source IP\n目标地址 Destination IP\n扩展匹配条件\n源端口Source Port,\n目标端口Destination Port\n处理操作\nACCEPT：允许数据包通过。\nDROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。\nREJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。\nSNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。\nMASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。\nDNAT：目标地址转换。\nREDIRECT：在本机做端口映射。l\n4. 数据包经过防火墙的流程 图片来自：https://www.zsythink.net/archives/1199\n到本机某进程的报文：PREROUTING –\u003e INPUT\n由本机转发的报文：PREROUTING –\u003e FORWARD –\u003e POSTROUTING\n由本机的某进程发出报文（通常为响应报文）：OUTPUT –\u003e POSTROUTING\n参考：\nIPtables-朱双印博客\niptables详解（1）：iptables概念-朱双印博客\n","categories":"","description":"","excerpt":"1. 简介 iptables是一个设置防火墙（netfilter）规则的命令工具。网络规则包括源地址、目的地址、传输协议（ …","ref":"/linux-notes/network/iptables/","tags":["iptables"],"title":"iptables介绍"},{"body":"1. kubectl命令介绍 kubectl的命令语法\nkubectl [command] [TYPE] [NAME] [flags] 其中command，TYPE，NAME，和flags分别是：\ncommand: 指定要在一个或多个资源进行操作，例如create，get，describe，delete。\nTYPE：指定资源类型。资源类型区分大小写，您可以指定单数，复数或缩写形式。例如，以下命令产生相同的输出：\nkubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAME：指定资源的名称。名称区分大小写。如果省略名称，则会显示所有资源的详细信息,比如$ kubectl get pods。\n按类型和名称指定多种资源：\n* 要分组资源，如果它们都是相同的类型：`TYPE1 name1 name2 name\u003c#\u003e`.\u003cbr/\u003e 例: `$ kubectl get pod example-pod1 example-pod2` * 要分别指定多种资源类型: `TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE\u003c#\u003e/name\u003c#\u003e`.\u003cbr/\u003e 例: `$ kubectl get pod/example-pod1 replicationcontroller/example-rc1` flags：指定可选标志。例如，您可以使用-s或--serverflags来指定Kubernetes API服务器的地址和端口。\n更多命令介绍：\n[root@node5 ~]# kubectl kubectl controls the Kubernetes cluster manager. Find more information at https://github.com/kubernetes/kubernetes. Basic Commands (Beginner): create Create a resource from a file or from stdin. expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service run Run a particular image on the cluster set Set specific features on objects run-container Run a particular image on the cluster. This command is deprecated, use \"run\" instead Basic Commands (Intermediate): get Display one or many resources explain Documentation of resources edit Edit a resource on the server delete Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands: rollout Manage the rollout of a resource rolling-update Perform a rolling update of the given ReplicationController scale Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job autoscale Auto-scale a Deployment, ReplicaSet, or ReplicationController Cluster Management Commands: certificate Modify certificate resources. cluster-info Display cluster info top Display Resource (CPU/Memory/Storage) usage. cordon Mark node as unschedulable uncordon Mark node as schedulable drain Drain node in preparation for maintenance taint Update the taints on one or more nodes Troubleshooting and Debugging Commands: describe Show details of a specific resource or group of resources logs Print the logs for a container in a pod attach Attach to a running container exec Execute a command in a container port-forward Forward one or more local ports to a pod proxy Run a proxy to the Kubernetes API server cp Copy files and directories to and from containers. auth Inspect authorization Advanced Commands: apply Apply a configuration to a resource by filename or stdin patch Update field(s) of a resource using strategic merge patch replace Replace a resource by filename or stdin convert Convert config files between different API versions Settings Commands: label Update the labels on a resource annotate Update the annotations on a resource completion Output shell completion code for the specified shell (bash or zsh) Other Commands: api-versions Print the supported API versions on the server, in the form of \"group/version\" config Modify kubeconfig files help Help about any command plugin Runs a command-line plugin version Print the client and server version information Use \"kubectl \u003ccommand\u003e --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). 2. 操作的常用资源对象 Node Podes Replication Controllers Services Namespace Deployment StatefulSet 具体对象类型及缩写：\n* all * certificatesigningrequests (aka 'csr') * clusterrolebindings * clusterroles * componentstatuses (aka 'cs') * configmaps (aka 'cm') * controllerrevisions * cronjobs * customresourcedefinition (aka 'crd') * daemonsets (aka 'ds') * deployments (aka 'deploy') * endpoints (aka 'ep') * events (aka 'ev') * horizontalpodautoscalers (aka 'hpa') * ingresses (aka 'ing') * jobs * limitranges (aka 'limits') * namespaces (aka 'ns') * networkpolicies (aka 'netpol') * nodes (aka 'no') * persistentvolumeclaims (aka 'pvc') * persistentvolumes (aka 'pv') * poddisruptionbudgets (aka 'pdb') * podpreset * pods (aka 'po') * podsecuritypolicies (aka 'psp') * podtemplates * replicasets (aka 'rs') * replicationcontrollers (aka 'rc') * resourcequotas (aka 'quota') * rolebindings * roles * secrets * serviceaccounts (aka 'sa') * services (aka 'svc') * statefulsets (aka 'sts') * storageclasses (aka 'sc') 3. kubectl命令分类[command] 3.1 增 1）create:[Create a resource by filename or stdin]\n2）run:[ Run a particular image on the cluster]\n3）apply:[Apply a configuration to a resource by filename or stdin]\n4）proxy:[Run a proxy to the Kubernetes API server ]\n3.2 删 1）delete:[Delete resources ]\n3.3 改 1）scale:[Set a new size for a Replication Controller]\n2）exec:[Execute a command in a container]\n3）attach:[Attach to a running container]\n4）patch:[Update field(s) of a resource by stdin]\n5）edit:[Edit a resource on the server]\n6） label:[Update the labels on a resource]\n7）annotate:[Auto-scale a replication controller]\n8）replace:[Replace a resource by filename or stdin]\n9）config:[config modifies kubeconfig files]\n3.4 查 1）get:[Display one or many resources]\n2）describe:[Show details of a specific resource or group of resources]\n3）log:[Print the logs for a container in a pod]\n4）cluster-info:[Display cluster info]\n5） version:[Print the client and server version information]\n6）api-versions:[Print the supported API versions]\n4. Pod相关命令 4.1 查询Pod kubectl get pod -o wide --namespace=\u003cNAMESPACE\u003e 4.2 进入Pod kubectl exec -it \u003cPodName\u003e /bin/bash --namespace=\u003cNAMESPACE\u003e # 进入Pod中指定容器 kubectl exec -it \u003cPodName\u003e -c \u003cContainerName\u003e /bin/bash --namespace=\u003cNAMESPACE\u003e 4.3 删除Pod kubectl delete pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e # 强制删除Pod，当Pod一直处于Terminating状态 kubectl delete pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e --force --grace-period=0 # 删除某个namespace下某个类型的所有对象 kubectl delete deploy --all --namespace=test 4.4 日志查看 $ 查看运行容器日志 kubectl logs \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e $ 查看上一个挂掉的容器日志 kubectl logs \u003cPodName\u003e -p --namespace=\u003cNAMESPACE\u003e 5. 常用命令 5.1. Node隔离与恢复 说明：Node设置隔离之后，原先运行在该Node上的Pod不受影响，后续的Pod不会调度到被隔离的Node上。\n1. Node隔离\n# cordon命令 kubectl cordon \u003cNodeName\u003e # 或者 kubectl patch node \u003cNodeName\u003e -p '{\"spec\":{\"unschedulable\":true}}' 2. Node恢复\n# uncordon kubectl uncordon \u003cNodeName\u003e # 或者 kubectl patch node \u003cNodeName\u003e -p '{\"spec\":{\"unschedulable\":false}}' 5.2. kubectl label 1. 固定Pod到指定机器\nkubectl label node \u003cNodeName\u003e namespace/\u003cNAMESPACE\u003e=true 2. 取消Pod固定机器\nkubectl label node \u003cNodeName\u003e namespace/\u003cNAMESPACE\u003e- 5.3. 升级镜像 # 升级镜像 kubectl set image deployment/nginx nginx=nginx:1.15.12 -n nginx # 查看滚动升级情况 kubectl rollout status deployment/nginx -n nginx 5.4. 调整资源值 # 调整指定容器的资源值 kubectl set resources sts nginx-0 -c=agent --limits=memory=512Mi -n nginx 5.5. 调整readiness probe # 批量查看readiness probe timeoutSeconds kubectl get statefulset -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.template.spec.containers[0].readinessProbe.timeoutSeconds}{\"\\n\"}{end}' # 调整readiness probe timeoutSeconds参数 kubectl patch statefulset nginx-sts --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/readinessProbe/timeoutSeconds\", \"value\":5}]' -n nginx 5.6. 调整tolerations属性 kubectl patch statefulset nginx-sts --patch '{\"spec\": {\"template\": {\"spec\": {\"tolerations\": [{\"effect\": \"NoSchedule\",\"key\": \"dedicated\",\"operator\": \"Equal\",\"value\": \"nginx\"}]}}}}' -n nginx 5.7. 查看所有节点的IP kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.addresses[0].address}{\"\\n\"}{end}' 5.8. 查看当前k8s组件leader节点 当k8s集群高可用部署的时候，kube-controller-manager和kube-scheduler只能一个服务处于实际逻辑运行状态，通过参数--leader-elect=true来开启选举操作。以下提供查询leader节点的命令。\n$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml apiVersion: v1 kind: Endpoints metadata: annotations: control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\"xxx.xxx.xxx.xxx_6537b938-7f5a-11e9-8487-00220d338975\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-05-26T02:03:18Z\",\"renewTime\":\"2019-05-26T02:06:08Z\",\"leaderTransitions\":1}' creationTimestamp: \"2019-05-26T01:52:39Z\" name: kube-controller-manager namespace: kube-system resourceVersion: \"1965\" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: f1755fc5-7f58-11e9-b4c4-00220d338975 以上表示\"holderIdentity\":\"xxx.xxx.xxx.xxx为kube-controller-manager的leader节点。\n同理，可以通过以下命令查看kube-scheduler的leader节点。\nkubectl get endpoints kube-scheduler --namespace=kube-system -o yaml 5.9. 修改副本数 kubectl scale deployment.v1.apps/nginx-deployment --replicas=10 5.10. 批量删除pod kubectl get po -n default |grep Evicted |awk '{print $1}' |xargs -I {} kubectl delete po {} -n default 5.11. 各种查看命令 # 不使用外部工具来输出解码后的 Secret kubectl get secret my-secret -o go-template='{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\n\"}}{{$v|base64decode}}{{\"\\n\\n\"}}{{end}}' # 列出事件（Events），按时间戳排序 kubectl get events --sort-by=.metadata.creationTimestamp 5.12. 拷贝文件 从pod拷贝到本地\n注意事项：\npod的目录是workdir的相对路径，可以将文件拷贝到workdir下再拷贝出来\n文件绝对路径前面不能加 /\n文件目标位置不能为文件夹，必须为文件路径\nkubectl cp -n \u003cns\u003e -c \u003ccontainer\u003e \u003cpod_name\u003e:\u003c与workdir的相对路径\u003e \u003c本地路径文件名\u003e # 示例： # 将pod workdir下的prometheus.env.yaml文件拷贝到本地 kubectl cp -n prometheus -c prometheus prometheus-0:prometheus.env.yaml ./prometheus.env.yaml 从本地拷贝到pod\n注意事项：\n如果没有加路径，默认拷贝到pod内workdir路径。 kubectl cp \u003c本地路径文件名\u003e -n \u003cns\u003e -c \u003ccontainer\u003e \u003cpod_name\u003e:\u003c与workdir的相对路径\u003e # 示例： kubectl cp ./prometheus.env.yaml -n prometheus -c prometheus prometheus-0:prometheus.env.yaml 5.13. 强制删除namespace 如果强制删除ns失败，可以使用以下命令删除，将以下的calico-system改为需要删除的namespace。\nkubectl get namespaces calico-system -o json \\ | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" \\ | kubectl replace --raw /api/v1/namespaces/calico-system/finalize -f - 5.14. edit status 参考：\nhttps://github.com/ulucinar/kubectl-edit-status https://krew.sigs.k8s.io/docs/user-guide/setup/install/ 1、通过二进制安装\nversion=v0.3.0 wget https://github.com/ulucinar/kubectl-edit-status/releases/download/${version}/kubectl-edit-status_${version}_linux_amd64.tar.gz tar -zvxf kubectl-edit-status_${version}_linux_amd64.tar.gz cp kubectl-edit_status /usr/bin/ 2、通过krew安装edit-status\n安装krew\n( set -x; cd \"$(mktemp -d)\" \u0026\u0026 OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" \u0026\u0026 ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" \u0026\u0026 KREW=\"krew-${OS}_${ARCH}\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" \u0026\u0026 tar zxvf \"${KREW}.tar.gz\" \u0026\u0026 ./\"${KREW}\" install krew ) export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" 通过krew安装edit-status\n说明：edit-status一般在该status是终止状态时可以修改，如果非终止状态可能被operator recycle状态覆盖。\nkubectl krew update kubectl krew install edit-status 6. kubectl日志级别 Kubectl 日志输出详细程度是通过 -v 或者 --v 来控制的，参数后跟一个数字表示日志的级别。 Kubernetes 通用的日志习惯和相关的日志级别在 这里 有相应的描述。\n详细程度 描述 --v=0 用于那些应该 始终 对运维人员可见的信息，因为这些信息一般很有用。 --v=1 如果您不想要看到冗余信息，此值是一个合理的默认日志级别。 --v=2 输出有关服务的稳定状态的信息以及重要的日志消息，这些信息可能与系统中的重大变化有关。这是建议大多数系统设置的默认日志级别。 --v=3 包含有关系统状态变化的扩展信息。 --v=4 包含调试级别的冗余信息。 --v=5 跟踪级别的详细程度。 --v=6 显示所请求的资源。 --v=7 显示 HTTP 请求头。 --v=8 显示 HTTP 请求内容。 --v=9 显示 HTTP 请求内容而且不截断内容。 参考文章：\nhttps://kubernetes.io/docs/reference/kubectl/overview/ https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/ https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/ ","categories":"","description":"","excerpt":"1. kubectl命令介绍 kubectl的命令语法\nkubectl [command] [TYPE] [NAME] [flags] 其 …","ref":"/kubernetes-notes/operation/kubectl/kubectl-commands/","tags":["Kubernetes"],"title":"kubectl命令使用"},{"body":"virtual-kubelet --help #./virtual-kubelet --help virtual-kubelet implements the Kubelet interface with a pluggable backend implementation allowing users to create kubernetes nodes without running the kubelet. This allows users to schedule kubernetes workloads on nodes that aren't running Kubernetes. Usage: virtual-kubelet [flags] virtual-kubelet [command] Available Commands: help Help about any command providers Show the list of supported providers version Show the version of the program Flags: --cluster-domain string kubernetes cluster-domain (default is 'cluster.local') (default \"cluster.local\") --disable-taint disable the virtual-kubelet node taint --enable-node-lease use node leases (1.13) for node heartbeats --full-resync-period duration how often to perform a full resync of pods between kubernetes and the provider (default 1m0s) -h, --help help for virtual-kubelet --klog.alsologtostderr log to standard error as well as files --klog.log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --klog.log_dir string If non-empty, write log files in this directory --klog.log_file string If non-empty, use this log file --klog.log_file_max_size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --klog.logtostderr log to standard error instead of files (default true) --klog.skip_headers If true, avoid header prefixes in the log messages --klog.skip_log_headers If true, avoid headers when opening log files --klog.stderrthreshold severity logs at or above this threshold go to stderr (default 2) --klog.v Level number for the log level verbosity --klog.vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging --kubeconfig string kube config file to use for connecting to the Kubernetes API server (default \"/root/.kube/config\") --log-level string set the log level, e.g. \"debug\", \"info\", \"warn\", \"error\" (default \"info\") --metrics-addr string address to listen for metrics/stats requests (default \":10255\") --namespace string kubernetes namespace (default is 'all') --nodename string kubernetes node name (default \"virtual-kubelet\") --os string Operating System (Linux/Windows) (default \"Linux\") --pod-sync-workers int set the number of pod synchronization workers (default 10) --provider string cloud provider --provider-config string cloud provider configuration file --startup-timeout duration How long to wait for the virtual-kubelet to start --trace-exporter strings sets the tracing exporter to use, available exporters: [jaeger ocagent] --trace-sample-rate string set probability of tracing samples --trace-service-name string sets the name of the service used to register with the trace exporter (default \"virtual-kubelet\") --trace-tag map add tags to include with traces in key=value form Use \"virtual-kubelet [command] --help\" for more information about a command. ","categories":"","description":"","excerpt":"virtual-kubelet --help #./virtual-kubelet --help virtual-kubelet …","ref":"/kubernetes-notes/multi-cluster/virtual-kubelet/virtual-kubelet-cmd/","tags":["VirtualKubelet"],"title":"Virtual Kubelet命令"},{"body":"eclipse快捷键 1. 快捷键 1.1. 编辑 作用域 功能 快捷键 全局 查找并替换 Ctrl+F 文本编辑器 查找上一个 Ctrl+Shift+K 文本编辑器 查找下一个 Ctrl+K 文本编辑器 删除当前行 Ctrl+D 文本编辑器 当前行的下一行插入空行 Shift+Enter 文本编辑器 当前行插入空行 Ctrl+Shift+Enter 文本编辑器 定位到最后编辑的位置 Ctrl+Q 全局 恢复上一个选择 Alt+Shift+↓ 全局 快速修正 Ctrl+1 全局 内容辅助（代码提示） Alt+/ 全局 全部选中 Ctrl+A 全局 删除 Delete 全局 上下文信息 Alt+/Alt+Shift+?Ctrl+Shift+Space Java编辑器 显示工具提示描述 F2 Java编辑器 选择封装元素 Alt+Shift+↑ Java编辑器 增量选择上一个同级元素 Alt+Shift+← Java编辑器 增量选择下一个同级元素 Alt+Shift+→ 文本编辑器 增量查找 Ctrl+J 文本编辑器 增量逆向查找 Ctrl+Shift+J java编辑器 自动生成get set方法 Alt+Shift+s 再按 r java编辑器 列出所有实现此接口的类 ctrl+T 1.2. 查看 作用域 功能 快捷键 全局 放大 Ctrl+= 全局 缩小 Ctrl+- 1.3. 窗口 作用域 功能 快捷键 全局 激活编辑器 F12 全局 关闭所有编辑器 Ctrl+Shift+W 全局 上一个编辑器 Ctrl+Shift+F6 全局 上一个视图 Ctrl+Shift+F7 全局 上一个透视图 Ctrl+Shift+F8 全局 下一个编辑器 Ctrl+F6 全局 下一个视图 Ctrl+F7 全局 下一个透视图 Ctrl+F8 文本编辑器 关闭当前窗口 Ctrl+W 全局 显示视图菜单 Ctrl+F10 全局 显示系统菜单 Alt+- 1.4. 导航 作用域 功能 快捷键 Java编辑器 打开结构 Ctrl+F3 全局 打开类型 Ctrl+Shift+T 全局 打开类型层次结构 F4 全局 打开声明 F3 全局 打开外部javadoc Shift+F2 全局 打开资源 Ctrl+Shift+R 全局 后退历史记录 Alt+← 全局 前进历史记录 Alt+→ 全局 上一个 Ctrl+, 全局 下一个 Ctrl+. Java编辑器 显示大纲 Ctrl+O 全局 在层次结构中打开类型 Ctrl+Shift+H 全局 转至匹配的括号 Ctrl+Shift+P 全局 转至上一个编辑位置 Ctrl+Q Java编辑器 转至上一个成员 Ctrl+Shift+↑ Java编辑器 转至下一个成员 Ctrl+Shift+↓ 文本编辑器 转至行 Ctrl+L 2. 搜索 作用域 功能 快捷键 全局 出现在文件中 Ctrl+Shift+U 全局 查找目标文件 ctrl+shift+R 全局 打开搜索对话框 Ctrl+H 全局 工作区中的声明 Ctrl+G 全局 工作区中的引用 Ctrl+Shift+G 工作区域的类 查看某一个类的继承类或者实现类 ctrl+T 3. 文本编辑 作用域 功能 快捷键 文本编辑器 改写切换 Insert 文本编辑器 上滚行 Ctrl+↑ 文本编辑器 下滚行 Ctrl+↓ 4. 文件 作用域 功能 快捷键 全局 保存 Ctrl+S 全局 打印 Ctrl+P 全局 关闭 Ctrl+F4 全局 全部保存 Ctrl+Shift+S 全局 全部关闭 Ctrl+Shift+F4 全局 属性 Alt+Enter 全局 新建 Ctrl+N 5. 项目 作用域 功能 快捷键 全局 全部构建 Ctrl+B 5.1. 源代码 作用域 功能 快捷键 Java编辑器 格式化 Ctrl+Shift+F Java编辑器 添加/取消注释 Ctrl+/ Java编辑器 添加导入 Ctrl+Shift+M Java编辑器 组织导入 Ctrl+Shift+O Java编辑器 使用try/catch块来包围 未设置，太常用了，所以在这里列出，建议自己设置。也可以使用Ctrl+1自动修正。Alt+Shift+z（就可以吧） Java编辑器 将所选区域字母设置为小写 Ctrl+Shift+Y Java编辑器 将所选区域字母设置为大写 Ctrl+Shift+X Java编辑器 方法添加注释 Alt+Shift+J 5.2. 运行 作用域 功能 快捷键 全局 单步返回 F7 全局 单步执行 F6 全局 单步跳入 F5 全局 单步跳入选择 Ctrl+F5 全局 调试上次启动 F11 全局 继续 F8 全局 使用过滤器单步执行 Shift+F5 全局 添加/去除断点 Ctrl+Shift+B 全局 显示 Ctrl+D 全局 运行上次启动 Ctrl+F11 全局 运行至行 Ctrl+R 全局 执行 Ctrl+U 5.3. 重构 作用域 功能 快捷键 全局 撤销重构 Alt+Shift+Z 全局 抽取方法 Alt+Shift+M 全局 抽取局部变量 Alt+Shift+L 全局 内联 Alt+Shift+I 全局 移动 Alt+Shift+V 全局 重命名 Alt+Shift+R 全局 重做 Alt+Shift+Y ","categories":"","description":"","excerpt":"eclipse快捷键 1. 快捷键 1.1. 编辑 作用域 功能 快捷键 全局 查找并替换 Ctrl+F …","ref":"/linux-notes/keymap/eclipse-keymap/","tags":["快捷键"],"title":"eclipse快捷键"},{"body":"1. Git常用命令 分类 子类 git command zsh alias 分支 查看当前分支 git branch gb 创建新分支,仍停留在当前分支 git branch 创建并切换到新分支 git checkout -b gcb 切换分支 git checkout 合并分支 git checkout #切换到要合并的分支git merge –no-ff #合并指定分支到当前分支 提交 查看状态 git status gst 查看修改部分 git diff --color gd 添加文件到暂存区 git add --all 提交本地仓库 git commit -m \"\" 推送到指定分支 git push -u origin 查看提交日志 git log - 2. git rebase 如果信息修改无法生效，设置永久环境变量：export EDITOR=vim\n帮助信息：\n# Rebase 67da308..6ef692b onto 67da308 (1 command) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out 2.1. 合并多余提交记录 #以交互的方式进行rebase git rebase -i master #合并多余提交记录：s, squash = use commit, but meld into previous commit pick 6ef692b FIX: Fix parsing docker image version error s 3df667y FIX: the second push s 3fds95t FIX: the third push 保存退出 # 进入修改交互界面 删除需要删除的提交记录，保存退出 #查看提交记录是否已被修改 git log #最后强制提交到分支 git commit --force -u origin fix/add-unit-test-for-global-role-revoking 2.2. 修改提交记录 #以交互的方式进行rebase git rebase -i master #修改提交记录：e, edit = use commit, but stop for amending e 6ef692b FIX: Fix parsing docker image version error e 5ty697u FIX: Fix parsing docker image version error #保存退出 git commit --amend #修改提交记录内容，保存退出 git rebase --continue git commit --amend #修改下一条提交记录，保存退出 git rebase --continue git status # 查看状态提示 #最后强制提交到分支 git commit --force -u origin fix/add-unit-test-for-global-role-revoking #查看提交记录是否已被修改 git log 3. git设置忽略特殊文件 3.1. 忽略文件的原则 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 3.2. 设置的方法 在项目的workdir 下编辑 .gitignore 文件，文件的路径填写为workdir的相对路径。\n.idea/ #IDE的配置文件 _build/ server/server #二进制文件 3.3. gitignore 不生效解决方法 原因是.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交：\ngit rm -r --cached . git add . git commit -m 'update .gitignore' 4. Git分支重命名 假设分支名称为oldName 想要修改为 newName\n1. 本地分支重命名(还没有推送到远程)\ngit branch -m oldName newName 2. 远程分支重命名 (已经推送远程-假设本地分支和远程对应分支名称相同) a. 重命名远程分支对应的本地分支\ngit branch -m oldName newName b. 删除远程分支\ngit push --delete origin oldName c. 上传新命名的本地分支\ngit push origin newName d.把修改后的本地分支与远程分支关联\ngit branch --set-upstream-to origin/newName 5. 代码冲突 git checkout master git pull git checkout \u003cbranch\u003e git rebase -i master fix conflict git rebase --continue git push --force -u origin \u003cbranch\u003e 6. 修改历史提交的用户信息 1、克隆并进入你的仓库\ngit clone --bare https://github.com/user/repo.git cd repo.git 2、创建以下脚本，例如命名为rename.sh\n#!/bin/sh git filter-branch --env-filter ' OLD_EMAIL=\"your-old-email@example.com\" #修改参数为你的旧提交邮箱 CORRECT_NAME=\"Your Correct Name\" #修改参数为你新的用户名 CORRECT_EMAIL=\"your-correct-email@example.com\" #修改参数为你新的邮箱名 if [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_COMMITTER_NAME=\"$CORRECT_NAME\" export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\" fi if [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_AUTHOR_NAME=\"$CORRECT_NAME\" export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\" fi ' --tag-name-filter cat -- --branches --tags 3、执行脚本\nchmod +x rename.sh sh rename.sh 4、查看新 Git 历史有没有错误。\n#可以看到提交记录的用户信息已经修改为新的用户信息 git log 5、确认提交内容，重新提交（可以先把rename.sh移除掉）\ngit push --force --tags origin 'refs/heads/*' 7. 撤销已经push的提交 # 本地仓库回退到某一版本 git reset -hard \u003ccommit-id\u003e # 强制 PUSH，此时远程分支已经恢复成指定的 commit 了 git push origin master --force ","categories":"","description":"","excerpt":"1. Git常用命令 分类 子类 git command zsh alias 分支 查看当前分支 git branch gb 创建新分支,仍 …","ref":"/linux-notes/git/git-common-cmd/","tags":["Git"],"title":"Git常用命令"},{"body":"1. Keepalived的安装 1.1. yum install方式 yum install -y keepalived 1.2. 安装包编译方式 更多安装包参考：http://www.keepalived.org/download.html\nwget http://www.keepalived.org/software/keepalived-2.0.7.tar.gz tar zxvf keepalived-2.0.7.tar.gz cd keepalived-2.0.7 ./configure --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc --mandir=/usr/share make \u0026\u0026 make install 2. 常用配置 keepalived配置文件路径：/etc/keepalived/keepalived。\n2.1. MASTER（主机配置） global_defs { router_id proxy-keepalived } vrrp_script check_nginx { script \"/etc/keepalived/scripts/check_nginx.sh\" interval 3 weight 2 } vrrp_instance VI_1 { state BACKUP interface eth2 virtual_router_id 15 priority 100 advert_int 1 authentication { auth_type PASS auth_pass xxx } track_script { check_nginx } virtual_ipaddress { 180.101.115.139 218.98.38.29 } nopreempt notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" } 2.2. BACKUP（备机配置） global_defs { router_id proxy-keepalived } vrrp_script check_nginx { script \"/etc/keepalived/scripts/check_nginx.sh\" interval 3 weight 2 } vrrp_instance VI_1 { state BACKUP interface eth2 virtual_router_id 15 priority 99 advert_int 1 authentication { auth_type PASS auth_pass xxx } track_script { check_nginx } virtual_ipaddress { 180.101.115.139 218.98.38.29 } nopreempt notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" } 3. 注意事项 1、指定Nginx健康检测脚本：/etc/keepalived/scripts/check_nginx.sh\n2、主备配置差别主要为（建议这么配置）：\n以下两种方式的配置，当其中一台机器keepalived挂掉后会自动VIP切到另一台机器，当挂掉机器keepalived恢复后不会抢占VIP，该方式可以避免机器恢复再次切VIP所带来的影响。\n主机:(state BACKUP;priority 100)\n备机：(state BACKUP;priority 99)\n非抢占：nopreempt\n或者：\n主机:(state MASTER;priority 100)\n备机：(state BACKUP;priority 100)\n默认抢占\n3、指定VIP\nvirtual_ipaddress { 180.101.115.139 218.98.38.29 } 4、可以指定为非抢占：nopreempt，即priority高不会抢占已经绑定VIP的机器。\n5、制定绑定IP的网卡： interface eth2\n6、可以指定keepalived状态变化通知\nnotify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" 7、virtual_router_id 15值，主备值一致，但建议不应与集群中其他Nginx机器上的相同，如果同一个网段配置的virtual_router_id 重复则会报错，选择一个不重复的0~255之间的值，可以用以下命令查看已存在的vrid。\ntcpdump -nn -i any net 224.0.0.0/8 4. 常用脚本 4.1. Nginx健康检测脚本 在Nginx配置目录下（/etc/nginx/conf.d/）增加health.conf的配置文件,该配置文件用于配置Nginx health的接口。\nserver { listen 80 default_server; server_name localhost; default_type text/html; return 200 'Health'; } Nginx健康检测脚本：/etc/keepalived/scripts/check_nginx.sh\n4.1.1. 检查接口调用是否为200 #!/bin/sh set -x timeout=30 #指定默认30秒没返回200则为非健康，该值可根据实际调整 if [ -n ${timeout} ];then httpcode=`curl -sL -w %{http_code} -m ${timeout} http://localhost -o /dev/null` else httpcode=`curl -sL -w %{http_code} http://localhost -o /dev/null` fi if [ ${httpcode} -ne 200 ];then echo `date`': nginx is not healthy, return http_code is '${httpcode} \u003e\u003e /etc/keeperalived/keepalived.log killall keepalived exit 1 else exit 0 fi 4.1.2. 检查Nginx进程是否运行 #!/bin/sh if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then echo \"$(date) nginx pid not found\"\u003e\u003e/etc/keepalived/keepalived.log killall keepalived fi 4.2. Keepalived状态通知脚本 #!/bin/bash set -x warn_receiver=$1 ip=$(ifconfig bond0|grep inet |awk '{print $2}') warningInfo=\"${ip}_keepalived_changed_status_to_$1\" warn-report --user admin --key=xxxx --target=${warn_receiver} ${warningInfo} echo $(date) $1 \u003e\u003e /etc/keepalived/status 说明：\nip获取本机IP，本例中IP获取是bond0的IP，不同机器网卡名称不同需要修改为对应网卡名称。 告警工具根据自己指定。 ","categories":"","description":"","excerpt":"1. Keepalived的安装 1.1. yum install方式 yum install -y keepalived 1.2. 安装包 …","ref":"/linux-notes/keepalived/install-keepalived/","tags":["Keepalived"],"title":"Keepalived安装与配置"},{"body":"1. Raft协议[分布式一致性算法] raft算法中涉及三种角色，分别是：\nfollower: 跟随者 candidate: 候选者，选举过程中的中间状态角色 leader: 领导者 2. 过程 2.1. 选举 有两个timeout来控制选举，第一个是election timeout，该时间是节点从follower到成为candidate的时间，该时间是150到300毫秒之间的随机值。另一个是heartbeat timeout。\n当某个节点经历完election timeout成为candidate后，开启新的一个选举周期，他向其他节点发起投票请求（Request Vote），如果接收到消息的节点在该周期内还没投过票则给这个candidate投票，然后节点重置他的election timeout。 当该candidate获得大部分的选票，则可以当选为leader。 leader就开始发送append entries给其他follower节点，这个消息会在内部指定的heartbeat timeout时间内发出，follower收到该信息则响应给leader。 这个选举周期会继续，直到某个follower没有收到心跳，并成为candidate。 如果某个选举周期内，有两个candidate同时获得相同多的选票，则会等待一个新的周期重新选举。 2.2. 同步 当选举过程结束，选出了leader，则leader需要把所有的变更同步的系统中的其他节点，该同步也是通过发送Append Entries的消息的方式。\n首先一个客户端发送一个更新给leader，这个更新会添加到leader的日志中。 然后leader会在给follower的下次心跳探测中发送该更新。 一旦大多数follower收到这个更新并返回给leader，leader提交这个更新，然后返回给客户端。 2.3. 网络分区 当发生网络分区的时候，在不同分区的节点接收不到leader的心跳，则会开启一轮选举，形成不同leader的多个分区集群。 当客户端给不同leader的发送更新消息时，不同分区集群中的节点个数小于原先集群的一半时，更新不会被提交，而节点个数大于集群数一半时，更新会被提交。 当网络分区恢复后，被提交的更新会同步到其他的节点上，其他节点未提交的日志会被回滚并匹配新leader的日志，保证全局的数据是一致的。 参考：\nhttp://thesecretlivesofdata.com/raft/ https://raft.github.io/raft.pdf https://raft.github.io/ ","categories":"","description":"","excerpt":"1. Raft协议[分布式一致性算法] raft算法中涉及三种角色，分别是：\nfollower: 跟随者 candidate: 候选者，选举 …","ref":"/kubernetes-notes/etcd/raft/","tags":["Etcd"],"title":"Raft算法"},{"body":"1. IP基础 TCP/IP的心脏是互联网层，这一层主要有IP和ICMP两个协议组成，在OSI参考模型中为第三层（网络层）。网络层的主要作用是实现终端节点之间的通信（点对点通信）。\n1.1. 网络层与数据链路层的关系 1.2. IP寻址 IP地址用于在“连接到网络中的所有主机中识别出进行通信的目标地址”。因此TCP/IP通信中所有主机或路由器必须设定自己的IP地址（每块网卡至少配置一个或以上的IP地址）。\n1.3. 路由控制 路由控制是指将分组数据发送到最终目标地址的功能。\nIP数据包类似快递中的包裹，送货车类似数据链路，包裹依赖送货车承载转运，而一辆送货车只能将包裹送到某个区间内，由新的快递点安排新的送货车来进行下一区间的运输。\n1.3.1. 路由控制表 为了将数据包发给目标主机，所有主机都维护一张路由控制表（Routing Table）,该表记录IP数据在下一步应该发给哪个路由器。IP包根据这个路由表在各个数据链路上传输。\n1.4. 数据链路的抽象化 IP是实现多个数据链路之间通信的协议。对不同数据链路的相异特性进行抽象化也是IP的重要作用之一。不同数据链路最大的区别在于它们各自的最大传输单位（MTU）不同，类似快递包裹有各自的大小限制。当数据包过大时，IP进行分片处理，即将大的IP包分成多个较小的IP包，当到目标地址后再被组合起来传给上一层。\n1.5. IP是面向无连接型 IP发包之前不需要提前与目标建立连接。采用面向无连接的原因：为了简化和提速。面向连接型需要提前建立连接会降低处理速度。IP只负责将数据发给目标主机，但途中可能会发生丢包、错位、数据量翻倍等问题。TCP则是面向连接的协议，负责保证对端主机确实收到数据。\n2. IP地址 在TCP/IP通信中，用IP地址识别主机和路由器。\n2.1. IP地址的定义 IP地址（IPv4地址）由32位正整数来表示。IP地址在计算机内部以二进制方式被处理，但习惯将32位的IP地址以8位为一组，分成4组，每组以“.”隔开，转换成10进制来表示。IPv4地址为32位，最多允许43亿台计算机连接网络。\n实际上，IP地址并非根据主机台数来分配而是每一台主机上的每一块网卡都得设置IP地址，一块网卡可以设置一个或以上个IP,路由器通常会配置两个以上的网卡。\n2.2. IP地址由网络和主机两部分标识组成 IP地址由“网络地址”和“主机地址”两部分组成。\n网络标识在数据链路的每个段配置不同的值，必须保证相互连接的每个段的地址不重复，相同段内连接的主机必须有相同的网络地址。主机标识则不允许同一个网段内重复出现。在某一范围内，IP地址需具有唯一性。\nIP包被转发到某个路由器时，是利用目标IP地址的网络标识进行路由，即使不看主机地址，由网络地址则可判断是否是该网段内的主机。\n2.3. IP地址的分类 IP地址分为A、B、C、D四类。\nIP地址类别 地址开头 网络地址 主机地址 范围 一个网段内主机地址个数 备注 A类地址 0 第1-8位 后24位 0.0.0.0~127.0.0.0 2^24-2=16777214 B类地址 10 第1-16位 后16位 128.0.0.0~191.255.0.0 2^16-2=65534 C类地址 110 第1-24位 后8位 192.0.0.0~239.255.255.0 2^8-2=254 D类地址 1110 第1-32位 没有主机地址 224.0.0.0~239.255.255.255 常用于多播 注意：同一个网段中的主机地址分配，主机地址全为0表示对应的网络地址，主机地址全为1通常用于广播地址。因此一个网段内主机的个数去掉2个（例如2^8-2=254）。\n2.4. 子网隐码 用1表示网络地址的范围，用0表示主机地址的访问。因此A、B、C类可表示为\nIP类别 表示 A类 255.0.0.0 B类 255.255.0.0 C类 255.255.255.0 按照以上的组合方式IP有点浪费，因此产生子网隐码的分类方法减少这种浪费。\n引入子网后，IP地址由两种识别码组成：IP地址本身+表示网络地址的子网隐码。即将A,B,C类中的主机地址拆成网络部分和主机部分，重新分配网络地址和主机地址。子网隐码同样是用1表示网络地址的范围，用0表示主机地址的访问。\n2.4.1. 子网隐码的表示方法 表示方法 地址 子网隐码 备注 数字 IP地址 172.20.100.52 255.255.255.192 网络地址 172.20.100.0 255.255.255.192 广播地址 172.20.100.63 255.255.255.192 “/26”，表示前26位为网络地址 IP地址 172.20.100.52/26 网络地址 172.20.100.0/26 广播地址 172.20.100.63/26 3. 路由控制 发送数据包除了有目标IP地址外，还需要指明路由器和主机的信息，即路由控制表。\n路由控制表的形成方式有两种：\n1、静态路由\n由管理员手动设置\n2、动态路由\n路由器与其他路由器相互交互信息时自动刷新。为了让动态路由及时刷新路由表，在网络上互联的路由器之间需设置路由协议，保证正常读取路由控制信息。\n3.1. IP地址和路由控制 IP地址的网络地址部分用于进行路由控制。路由控制表中记录着网络地址与下一步应该发送至路由器的地址。在发送IP包时，首先确认IP包首部中的目标地址，再从路由控制表中找到与该地址具有相同网络地址的记录，根据该记录将IP包转发给相应的下一个路由器。如果存在多条相同网络地址的记录，则选择最为吻合的网络地址（相同位数最多）。例如：172.20.100.52的网络地址与172.20.0/16和172.20.100.0/24都匹配，则选择匹配最长的172.20.100.0/24。\n3.1.1. 默认路由 默认路由一般标记为0.0.0.0/0或default。当路由表中没有任何一个地址与之匹配的记录，则使用默认路由。\n3.1.2. 主机路由 “IP地址/32”也被称为主机路由，即整个IP地址的所有位都参与路由。进行主机路由意味着基于主机上网卡配置的IP地址本身而不是基于该地址的网络地址进行路由。一般用于不希望通过网络地址路由的情况。使用主机路由会导致路由表膨大，路由负荷增加，网络性能下降。\n3.1.3. 环回地址 环回地址是在同一台计算机上程序之间进行网络通信时所使用的一个默认地址。即IP地址为127.0.0.1，主机名为localhost。\n3.2. 路由控制表的聚合 路由信息的聚合可以有效的减少路由表的条目。路由表越大，管理它所需要的内存和CPU就越多，查找路由表的时间越长，导致转发IP包性能下降。要构建高性能网络就需要尽可能减少路由表的大小。\n4. IP首部信息 IP进行通信时，需要在数据前面加入IP首部信息，IP首部包含着用于IP协议进行发包控制时所有的必要信息。\n4.1. IPv4首部 字段 说明 大小 版本 标识IP首部的版本号，IPv4，即版本号为4 4比特 首部长度 表示IP首部的大小，单位为4字节 4比特 区分服务 表示服务质量 8比特 DSCP段与ECN段 DSCP用来进行质量控制，值越大优先度越高；ECN用来报告网络拥堵情况 2比特 总长度 表示IP首部与数据部分合起来的总字节数 16比特 标识 用于分片重组，同一个分片标识值相同，不同分片的标识值不同 16比特 标志 表示包被分片的相关信息 3比特 片偏移 用来标识被分片的每一个分段相对于原始数据的位置。 13比特 生存时间（TTL） 本意为包的生存期限，一般表示可以中转多少个路由器，每经过一个路由器TTL减1，直到变为0则丢弃该包 8比特 协议 表示IP首部的下一个首部隶属于哪个协议。 8比特 首部校验和 IP首部校验和，用来确保IP数据报不被破坏。 16比特 源地址 发送端IP地址 32比特 目标地址 接收端IP地址 32比特 可选项 安全级别、源路径、路径记录、时间戳 填充 填补物，调整大小使用 数据 存入数据 4.2. IPv6首部 字段 说明 版本 IPv6,版本为6 通信量类 相当于IPv4的TOS（Type Of Service）字段 流标号 用于服务质量控制 有效载荷长度 包的数据部分 下一个首部 相当于IPv4的协议字段 跳数限制 Hop Limit，同IPv4的TTL,表示可通过的路由器个数 源地址 发送端的IP地址 目标地址 接收端的IP地址 参考\n《图解TCP/IP》 ","categories":"","description":"","excerpt":"1. IP基础 TCP/IP的心脏是互联网层，这一层主要有IP和ICMP两个协议组成，在OSI参考模型中为第三层（网络层）。网络层的主要作用 …","ref":"/linux-notes/tcpip/ip/","tags":["TCPIP"],"title":"IP协议"},{"body":"1. Memcached 命令 1.1. 存储命令 1.1.1. 常用命令 命令 说明 set 新增或更新 add 新增 replace 替换 append 在后面追加 prepend 在前面追加 cas 检查并设置 以上几个命令语法格式相似，以set为例：\nset key flags exptime bytes [noreply] value 参数说明如下：\n**key：**键值 key-value 结构中的 key，用于查找缓存值。 flags：可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。 exptime：在缓存中保存键值对的时间长度（以秒为单位，0 表示永远） bytes：在缓存中存储的字节数 noreply（可选）： 该参数告知服务器不需要返回数据 value：存储的值（始终位于第二行）（可直接理解为key-value结构中的value） 实例：\nkey → runoob flag → 0 exptime → 900 (以秒为单位) bytes → 9 (数据存储的字节数) value → memcached set runoob 0 900 9 memcached STORED get runoob VALUE runoob 0 9 memcached END 输出：\n如果数据设置成功，则输出：\nSTORED 输出信息说明：\nSTORED：保存成功后输出。 ERROR：在保存失败后输出。 1.1.2. cas命令 Memcached CAS（Check-And-Set 或 Compare-And-Swap） 命令用于执行一个\"检查并设置\"的操作。\n它仅在当前客户端最后一次取值后，该key 对应的值没有被其他客户端修改的情况下， 才能够将值写入。\n检查是通过cas_token参数进行的， 这个参数是Memcach指定给已经存在的元素的一个唯一的64位值。\n语法：\n比以上命令多了一个unique_cas_token\ncas key flags exptime bytes unique_cas_token [noreply] value 参数说明如下：\nkey：键值 key-value 结构中的 key，用于查找缓存值。 flags：可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。 exptime：在缓存中保存键值对的时间长度（以秒为单位，0 表示永远） bytes：在缓存中存储的字节数 unique_cas_token通过 gets 命令获取的一个唯一的64位值。 noreply（可选）： 该参数告知服务器不需要返回数据 value：存储的值（始终位于第二行）（可直接理解为key-value结构中的value） unique_cas_token通过gets命令获取。\n1.2. 查找命令 命令 说明 get 获取一个或多个key gets 获取一个或多个cas token delete 删除已存在的key incr/decr 对已存在的 key(键) 的数字值进行自增或自减操作 1.3. 统计命令 命令 说明 stats 用于返回统计信息例如 PID(进程号)、版本号、连接数等。 stats items 用于显示各个 slab 中 item 的数目和存储时长(最后一次访问距离现在的秒数)。 stats slabs 用于显示各个slab的信息，包括chunk的大小、数目、使用情况等。 stats sizes 用于显示所有item的大小和个数。 flush_all 用于清理缓存中的所有 key=\u003evalue(键=\u003e值) 对。 参考文章：\nhttp://www.runoob.com/memcached/memcached-tutorial.html ","categories":"","description":"","excerpt":"1. Memcached 命令 1.1. 存储命令 1.1.1. 常用命令 命令 说明 set 新增或更新 add 新增 replace 替 …","ref":"/linux-notes/memcached/memcached-cmd/","tags":["Memcached"],"title":"Memcached命令"},{"body":"1. kubernetes对象概述 kubernetes中的对象是一些持久化的实体，可以理解为是对集群状态的描述或期望。\n包括：\n集群中哪些node上运行了哪些容器化应用 应用的资源是否满足使用 应用的执行策略，例如重启策略、更新策略、容错策略等。 kubernetes的对象是一种意图（期望）的记录，kubernetes会始终保持预期创建的对象存在和集群运行在预期的状态下。\n操作kubernetes对象（增删改查）需要通过kubernetes API，一般有以下几种方式：\nkubectl命令工具 Client library的方式，例如 client-go 2. Spec and Status 每个kubernetes对象的结构描述都包含spec和status两个部分。\nspec：该内容由用户提供，描述用户期望的对象特征及集群状态。 status：该内容由kubernetes集群提供和更新，描述kubernetes对象的实时状态。 任何时候，kubernetes都会控制集群的实时状态status与用户的预期状态spec一致。\n例如：当你定义Deployment的描述文件，指定集群中运行3个实例，那么kubernetes会始终保持集群中运行3个实例，如果任何实例挂掉，kubernetes会自动重建新的实例来保持集群中始终运行用户预期的3个实例。\n3. 对象描述文件 当你要创建一个kubernetes对象的时候，需要提供该对象的描述信息spec，来描述你的对象在kubernetes中的预期状态。\n一般使用kubernetes API来创建kubernetes对象，其中spec信息可以以JSON的形式存放在request body中，也可以以.yaml文件的形式通过kubectl工具创建。\n例如，以下为Deployment对象对应的yaml文件：\napiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 执行kubectl create的命令\n#create command kubectl create -f https://k8s.io/docs/user-guide/nginx-deployment.yaml --record #output deployment \"nginx-deployment\" created 4. 必须字段 在对象描述文件.yaml中，必须包含以下字段。\napiVersion：kubernetes API的版本 kind：kubernetes对象的类型 metadata：唯一标识该对象的元数据，包括name，UID，可选的namespace spec：标识对象的详细信息，不同对象的spec的格式不同，可以嵌套其他对象的字段。 文章参考：\nhttps://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/\n","categories":"","description":"","excerpt":"1. kubernetes对象概述 kubernetes中的对象是一些持久化的实体，可以理解为是对集群状态的描述或期望。\n包括：\n集群中哪 …","ref":"/kubernetes-notes/concepts/object/understanding-kubernetes-objects/","tags":["Kubernetes"],"title":"理解kubernetes对象"},{"body":"[编者的话]\n目前很多的容器云平台通过Docker及Kubernetes等技术提供应用运行平台，从而实现运维自动化，快速部署应用、弹性伸缩和动态调整应用环境资源，提高研发运营效率。\n从宏观到微观（从抽象到具体）的思路来理解：云计算→PaaS→ App Engine→XAE[XXX App Engine] （XAE泛指一类应用运行平台，例如GAE、SAE、BAE等）。\n本文简要介绍了与容器云相关的几个重要概念：PaaS、App Engine、Dokcer、Kubernetes。\n1. PaaS概述 1.1. PaaS概念 PaaS(Platform as a service)，平台即服务，指将软件研发的平台（或业务基础平台）作为一种服务，以SaaS的模式提交给用户。 PaaS是云计算服务的其中一种模式，云计算是一种按使用量付费的模式的服务，类似一种租赁服务，服务可以是基础设施计算资源（IaaS），平台（PaaS），软件（SaaS）。租用IT资源的方式来实现业务需要，如同水力、电力资源一样，计算、存储、网络将成为企业IT运行的一种被使用的资源，无需自己建设，可按需获得。 PaaS的实质是将互联网的资源服务化为可编程接口，为第三方开发者提供有商业价值的资源和服务平台。简而言之，IaaS就是卖硬件及计算资源，PaaS就是卖开发、运行环境，SaaS就是卖软件。 1.2. IaaS/PaaS/SaaS说明 类型 说明 比喻 例子 IaaS:Infrastructure-as-a-Service(基础设施即服务) 提供的服务是计算基础设施 地皮，需要自己盖房子 Amazon EC2（亚马逊弹性云计算） PaaS: Platform-as-a-Service(平台即服务) 提供的服务是软件研发的平台或业务基础平台 商品房，需要自己装修 GAE（谷歌开发者平台） SaaS: Software-as-a-Service(软件即服务) 提供的服务是运行在云计算基础设施上的应用程序 酒店套房，可以直接入住 谷歌的Gmail邮箱 1.3. PaaS的特点（三种层次） 特点 说明 平台即服务 PaaS提供的服务就是个基础平台，一个环境，而不是具体的应用 平台及服务 不仅提供平台，还提供对该平台的技术支持、优化等服务 平台级服务 “平台级服务”即强大稳定的平台和专业的技术支持团队，保障应用的稳定使用 2. App Engine概述 2.1. App Engine概念 App Engine是PaaS模式的一种实现方式，App Engine将应用运行所需的 IT 资源和基础设施以服务的方式提供给用户，包括了中间件服务、资源管理服务、弹性调度服务、消息服务等多种服务形式。App Engine的目标是对应用提供完整生命周期（包括设计、开发、测试和部署等阶段）的支持，从而减少了用户在购置和管理应用生命周期内所必须的软硬件以及部署应用和IT 基础设施的成本，同时简化了以上工作的复杂度。常见的App Engine有：GAE(Google App Engine)，SAE(Sina App Engine)，BAE(Baidu App Engine)。\nApp Engine利用虚拟化与自动化技术实现快速搭建部署应用运行环境和动态调整应用运行时环境资源这两个目标。一方面实现即时部署以及快速回收，降低了环境搭建时间，避免了手工配置错误，快速重复搭建环境，及时回收资源， 减少了低利用率硬件资源的空置。另一方面，根据应用运行时的需求对应用环境进行动态调整，实现了应用平台的弹性扩展和自优化，减少了非高峰时硬件资源的空置。\n简而言之，App Engine主要目标是：Easy to maintain(维护), Easy to scale(扩容), Easy to build(构建)。\n2.2. 架构设计 2.3. 组成模块说明 组成模块 模块说明 App Router[流量接入层] 接收用户请求，并转发到不同的App Runtime。 App Runtime[应用运行层] 应用运行环境，为各个应用提供基本的运行引擎，从而让app能够运行起来。 Services[基础服务层] 各个通用基础服务，主要是对主流的服务提供通用的接入，例如数据库等。 Platform Control[平台控制层] 整个平台的控制中心，实现业务调度，弹性扩容、资源审计、集群管理等相关工作。 Manage System[管理界面层] 提供友好可用的管理操作界面方便平台管理员来控制管理整个平台。 Platform Support[平台支持层] 为应用提供相关的支持，比如应用监控、问题定位、分布式日志重建、统计分析等。 Log Center[日志中心] 实时收集相关应用及系统的日志（日志收集），提供实时计算和分析平台（日志处理）。 Code Center[代码中心] 完成代码存储、部署上线相关的工作。 3. 容器云平台技术栈 功能组成部分 使用工具 应用载体 Docker 编排工具 Kubernetes 配置数据 Etcd 网络管理 Flannel 存储管理 Ceph 底层实现 Linux内核的Namespace[资源隔离]和CGroups[资源控制] Namespace[资源隔离] Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于某个特定的Namespace。每个namespace下的资源对于其他namespace下的资源都是透明，不可见的。 CGroups[资源控制] CGroup（control group）是将任意进程进行分组化管理的Linux内核功能。CGroup本身是提供将进程进行分组化管理的功能和接口的基础结构，I/O或内存的分配控制等具体的资源管理功能是通过这个功能来实现的。CGroups可以限制、记录、隔离进程组所使用的物理资源（包括：CPU、memory、IO等），为容器实现虚拟化提供了基本保证。CGroups本质是内核附加在程序上的一系列钩子（hooks），通过程序运行时对资源的调度触发相应的钩子以达到资源追踪和限制的目的。 4. Docker概述 更多详情请参考：Docker整体架构图\n4.1. Docker介绍 Docker - Build, Ship, and Run Any App, Anywhere Docker是一种Linux容器工具集，它是为“构建（Build）、交付（Ship）和运行（Run）”分布式应用而设计的。 Docker相当于把应用以及应用所依赖的环境完完整整地打成了一个包，这个包拿到哪里都能原生运行。因此可以在开发、测试、运维中保证环境的一致性。 Docker的本质：Docker=LXC(Namespace+CGroups)+Docker Images，即在Linux内核的Namespace[资源隔离]和CGroups[资源控制]技术的基础上通过镜像管理机制来实现轻量化设计。 4.2. Docker的基本概念 4.2.1. 镜像 Docker 镜像就是一个只读的模板，可以把镜像理解成一个模子（模具），由模子（镜像）制作的成品（容器）都是一样的（除非在生成时加额外参数），修改成品（容器）本身并不会对模子（镜像）产生影响（除非将成品提交成一个模子），容器重启时，即由模子（镜像）重新制作成一个成品（容器），与其他由该模子制作成的成品并无区别。\n例如：一个镜像可以包含一个完整的 ubuntu 操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器。Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户可以直接从其他人那里下载一个已经做好的镜像来直接使用。\n4.2.2. 容器 Docker 利用容器来运行应用。容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。\n4.2.3. 仓库 仓库是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。\n4.3. Docker的优势 容器的快速轻量\n容器的启动，停止和销毁都是以秒或毫秒为单位的，并且相比传统的虚拟化技术，使用容器在CPU、内存，网络IO等资源上的性能损耗都有同样水平甚至更优的表现。\n一次构建，到处运行\n当将容器固化成镜像后，就可以非常快速地加载到任何环境中部署运行。而构建出来的镜像打包了应用运行所需的程序、依赖和运行环境， 这是一个完整可用的应用集装箱，在任何环境下都能保证环境一致性。\n完整的生态链\n容器技术并不是Docker首创，但是以往的容器实现只关注于如何运行，而Docker站在巨人的肩膀上进行整合和创新，特别是Docker镜像的设计，完美地解决了容器从构建、交付到运行，提供了完整的生态链支持。\n5. Kubernetes概述 更多详情请参考：Kubernetes总架构图\n5.1. Kubernetes介绍 Kubernetes是Google开源的容器集群管理系统。它构建Docker技术之上，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等整一套功能，本质上可看作是基于容器技术的Micro-PaaS平台，即第三代PaaS的代表性项目。\n5.2. Kubernetes的基本概念 5.2.1. Pod Pod是若干个相关容器的组合，是一个逻辑概念，Pod包含的容器运行在同一个宿主机上，这些容器使用相同的网络命名空间、IP地址和端口，相互之间能通过localhost来发现和通信，共享一块存储卷空间。在Kubernetes中创建、调度和管理的最小单位是Pod。一个Pod一般只放一个业务容器和一个用于统一网络管理的网络容器。\n5.2.2. Replication Controller Replication Controller是用来控制管理Pod副本(Replica，或者称实例)，Replication Controller确保任何时候Kubernetes集群中有指定数量的Pod副本在运行，如果少于指定数量的Pod副本，Replication Controller会启动新的Pod副本，反之会杀死多余的以保证数量不变。另外Replication Controller是弹性伸缩、滚动升级的实现核心。\n5.2.3. Service Service是真实应用服务的抽象，定义了Pod的逻辑集合和访问这个Pod集合的策略，Service将代理Pod对外表现为一个单一访问接口，外部不需要了解后端Pod如何运行，这给扩展或维护带来很大的好处，提供了一套简化的服务代理和发现机制。\n5.2.4. Label Label是用于区分Pod、Service、Replication Controller的Key/Value键值对，实际上Kubernetes中的任意API对象都可以通过Label进行标识。每个API对象可以有多个Label，但是每个Label的Key只能对应一个Value。Label是Service和Replication Controller运行的基础，它们都通过Label来关联Pod，相比于强绑定模型，这是一种非常好的松耦合关系。\n5.2.5. Node Kubernets属于主从的分布式集群架构，Kubernets Node（简称为Node，早期版本叫做Minion）运行并管理容器。Node作为Kubernetes的操作单元，将用来分配给Pod（或者说容器）进行绑定，Pod最终运行在Node上，Node可以认为是Pod的宿主机。\n5.3. Kubernetes架构 ","categories":"","description":"","excerpt":"[编者的话]\n目前很多的容器云平台通过Docker及Kubernetes等技术提供应用运行平台，从而实现运维自动化，快速部署应用、弹性伸缩和 …","ref":"/kubernetes-notes/concepts/architecture/paas-based-on-docker-and-kubernetes/","tags":["Kubernetes"],"title":" 基于Docker及Kubernetes技术构建容器云（PaaS）平台"},{"body":"1. install-go.sh #!/bin/bash set -x set -e # default version VERSION=$1 VERSION=${VERSION:-1.14.6} PLATFORM=$2 PLATFORM=${PLATFORM:-linux} GOROOT=\"/usr/local/go\" GOPATH=$HOME/gopath GO_DOWNLOAD_URL=\"https://golang.org/dl\" # download and install case ${PLATFORM} in \"linux\") wget ${GO_DOWNLOAD_URL}/go${VERSION}.${PLATFORM}-amd64.tar.gz tar -C /usr/local -xzf go${VERSION}.${PLATFORM}-amd64.tar.gz ;; \"mac\") PLATFORM=\"darwin\" wget ${GO_DOWNLOAD_URL}/go${VERSION}.${PLATFORM}-amd64.tar.gz tar -C /usr/local -xzf go${VERSION}.${PLATFORM}-amd64.tar.gz ;; *) echo \"platform not found\" ;; esac # set golang env cat \u003e\u003e $HOME/.bashrc \u003c\u003c EOF # Golang env export GOROOT=/usr/local/go export GOPATH=\\$HOME/gopath export PATH=\\$PATH:\\$GOROOT/bin:\\$GOPATH/bin EOF source $HOME/.bashrc # mkdir gopath mkdir -p $GOPATH/src $GOPATH/pkg $GOPATH/bin 2. 安装 chmod +x install-go.sh ./install-go.sh 1.14.6 linux 更多版本号可参考：https://golang.org/dl/\n参考：\nhttps://golang.org/doc/install ","categories":"","description":"","excerpt":"1. install-go.sh #!/bin/bash set -x set -e # default version …","ref":"/golang-notes/introduction/install/","tags":["Golang"],"title":"Golang安装"},{"body":"1. 指针的概念 概念 说明 变量 是一种占位符，用于引用计算机的内存地址。可理解为内存地址的标签 指针 表示内存地址，表示地址的指向。指针是一个指向另一个变量内存地址的值 \u0026 取地址符，例如：{指针}:=\u0026{变量} * 取值符，例如：{变量}:=*{指针} 2. 内存地址说明 2.1. 内存定义 计算机的内存 RAM 可以把它想象成一些有序的盒子，一个接一个的排成一排，每一个盒子或者单元格都被一个唯一的数字标记依次递增，这个数字就是该单元格的地址，也就是内存的地址。 硬件角度：内存是CPU沟通的桥梁，程序运行在内存中。\n逻辑角度：内存是一块具备随机访问能力，支持读写操作，用来存放程序及程序运行中产生的数据的区域。\n概念 比喻 内存 一层楼层 内存块 楼层中的一个房间 变量名 房间的标签，例如：总经理室 指针 房间的具体地址（门牌号），例如：总经理室地址是2楼201室 变量值 房间里的具体存储物 指针地址 指针的地址：存储指针内存块的地址 2.2. 内存单位和编址 2.2.1. 内存单位 单位 说明 位（bit） 计算机中最小的数据单位，每一位的状态只能是0或1 字节（Byte） 1Byte=8bit，是内存基本的计量单位 字 “字”由若干个字节构成，字的位数叫字长，不同档次的机器有不同的字长 KB 1KB=1024Byte，即1024个字节 MB 1MB=1024KB GB 1GB=1024MB 2.2.2. 内存编址 计算机中的内存按字节编址，每个地址的存储单元可以存放一个字节的数据，CPU通过内存地址获取指令和数据，并不关心这个地址所代表的空间在什么位置，内存地址和地址指向的空间共同构成了一个内存单元。\n2.2.3. 内存地址 内存地址通常用16进制的数据表示，例如0x0ffc1。\n3.变量与指针运算理解 编写一段程序，检索出值并存储在地址为 200 的一个块内存中，将其乘以 3，并将结果存储在地址为 201 的另一块内存中\n3.1.本质 检索出内存地址为 200 的值，并将其存储在 CPU 中 将存储在 CPU 中的值乘以 3 将 CPU 中存储的结果，写入地址为 201 的内存块中 3.2.基于变量的理解 获取变量 a 中存储的值，并将其存储在 CPU 中 将其乘以 3 将结果保存在变量 b 中 var a = 6 var b = a * 3 3.3.基于指针的理解 func main() { a := 200 b := \u0026a *b++ fmt.Println(a) } 以上函数对a进行+1操作，具体理解如下：\n1.a:=200\n2. b := \u0026a\n*3. b++\n4. 指针的使用 4.1. 方法中的指针 方法即为有接受者的函数，接受者可以是类型的实例变量或者是类型的实例指针变量。但两种效果不同。\n1、类型的实例变量\nfunc main(){ person := Person{\"vanyar\", 21} fmt.Printf(\"person\u003c%s:%d\u003e\\n\", person.name, person.age) person.sayHi() person.ModifyAge(210) person.sayHi() } type Person struct { name string age int } func (p Person) sayHi() { fmt.Printf(\"SayHi -- This is %s, my age is %d\\n\",p.name, p.age) } func (p Person) ModifyAge(age int) { fmt.Printf(\"ModifyAge\") p.age = age } //输出结果 person\u003cvanyar:21\u003e SayHi -- This is vanyar, my age is 21 ModifyAgeSayHi -- This is vanyar, my age is 21 尽管 ModifyAge 方法修改了其age字段，可是方法里的p是person变量的一个副本，修改的只是副本的值。下一次调用sayHi方法的时候，还是person的副本，因此修改方法并不会生效。\n即实例变量的方式并不会改变接受者本身的值。\n2、类型的实例指针变量\nfunc (p *Person) ChangeAge(age int) { fmt.Printf(\"ModifyAge\") p.age = age } Go会根据Person的示例类型，转换成指针类型再拷贝，即 person.ChangeAge会变成 (\u0026person).ChangeAge。\n指针类型的接受者，如果实例对象是值，那么go会转换成指针，然后再拷贝，如果本身就是指针对象，那么就直接拷贝指针实例。因为指针都指向一处值，就能修改对象了。\n5. 零值与nil(空指针) 变量声明而没有赋值，默认为零值，不同类型零值不同，例如字符串零值为空字符串；\n指针声明而没有赋值，默认为nil，即该指针没有任何指向。当指针没有指向的时候，不能对(*point)进行操作包括读取，否则会报空指针异常。\nfunc main(){ // 声明一个指针变量 aPot 其类型也是 string var aPot *string fmt.Printf(\"aPot: %p %#v\\n\", \u0026aPot, aPot) // 输出 aPot: 0xc42000c030 (*string)(nil) *aPot = \"This is a Pointer\" // 报错： panic: runtime error: invalid memory address or nil pointer dereference } 解决方法即给该指针分配一个指向,即初始化一个内存，并把该内存地址赋予指针变量，例如：\n// 声明一个指针变量 aPot 其类型也是 string var aPot *string fmt.Printf(\"aPot: %p %#v\\n\", \u0026aPot, aPot) // 输出 aPot: 0xc42000c030 (*string)(nil) aPot = \u0026aVar *aPot = \"This is a Pointer\" fmt.Printf(\"aVar: %p %#v \\n\", \u0026aVar, aVar) // 输出 aVar: 0xc42000e240 \"This is a Pointer\" fmt.Printf(\"aPot: %p %#v %#v \\n\", \u0026aPot, aPot, *aPot) // 输出 aPot: 0xc42000c030 (*string)(0xc42000e240) \"This is a Pointer\" 或者通过new开辟一个内存，并返回这个内存的地址。\nvar aNewPot *int aNewPot = new(int) *aNewPot = 217 fmt.Printf(\"aNewPot: %p %#v %#v \\n\", \u0026aNewPot, aNewPot, *aNewPot) // 输出 aNewPot: 0xc42007a028 (*int)(0xc42006e1f0) 217 6. 总结 Golang提供了指针用于操作数据内存，并通过引用来修改变量。 只声明未赋值的变量，golang都会自动为其初始化为零值，基础数据类型的零值比较简单，引用类型和指针的零值都为nil，nil类型不能直接赋值，因此需要通过new开辟一个内存，或者通过make初始化数据类型，或者两者配合，然后才能赋值。 指针也是一种类型，不同于一般类型，指针的值是地址，这个地址指向其他的内存，通过指针可以读取其所指向的地址所存储的值。 函数方法的接受者，也可以是指针变量。无论普通接受者还是指针接受者都会被拷贝传入方法中，不同在于拷贝的指针，其指向的地方都一样，只是其自身的地址不一样。 参考：\nhttp://www.jianshu.com/p/d23f78a3922b\nhttp://www.jianshu.com/p/44b9429d7bef\n","categories":"","description":"","excerpt":"1. 指针的概念 概念 说明 变量 是一种占位符，用于引用计算机的内存地址。可理解为内存地址的标签 指针 表示内存地址，表示地址的指向。指针 …","ref":"/golang-notes/oop/pointer/","tags":["Golang"],"title":"Golang 指针"},{"body":"1. 反向代理简介 Nginx可以作为反向代理，接收客户端的请求，并向上游服务器发起新的请求。该请求可以根据客户端请求的URI，客户机参数或其他逻辑进行拆分，原始URL中的任何部分可以以这种方式进行转换。\n1.1. 代理模块指令 指令 说明 proxy_connect_timeout Nginx从接受到请求到连接至上游服务器的最长等待时间 proxy_cookie_domain 替代从上游服务器来的Set-Cookie头的域domain proxy_cookie_path 替代从上游服务器来的Set-Cookie头的path属性 proxy_headers_hash_bucket_size 头名字的最大值 proxy_headers_hash_max_size 从上游服务器接收到头的总大小 proxy_hide_header 不应该传递给客户端头的列表 proxy_http_version 用于通上游服务器通信的Http协议版本 proxy_ignore_client_abort 如果设置为ON，那么客户端放弃连接后，nginx将不会放弃同上游服务器的连接 proxy_ignore_headers 当处理来自上游服务器的响应时，设置哪些头可以被忽略 proxy_intercept_errors 如果启用该选项，Nginx将会显示配置的error_page错误，而不是来自于上游服务器的直接响应 proxy_max_temp_file_size 在写入内存缓冲区时响应与内存不匹配时使用时，给出溢出文件的最大值 proxy_pass 指定请求被传递到的上游服务器，格式为URL proxy_pass_header 覆盖掉在proxy_hide_header指令中设置的头，允许这些头传递到客户端 proxy_pass_request_body 如果设置为off，将会阻止请求体传递到客户端 proxy_pass_request_headers 如果设置为on,则阻止请求头发送到上游服务器 proxy_read_timeout 给出连接关闭前从上游服务器两次成功的读操作耗时，如果上游服务器处理请求比较慢，那么该值需设置较高些 proxy_redirect 重写来自于上游服务器的Location和Refresh头 proxy_send_timeout 给出连接关闭前从上游服务器两次成功的写操作耗时，如果上游服务器处理请求比较慢，那么该值需设置较高些 proxy_set_body 发送到上游服务器的请求体可能会被该指令的设置值修改 proxy_set_header 重写发送到上游服务器头的内容，也可以通过将某种头的值设置为空字符，而不是发送某种头的方法实现 proxy_temp_file_write_size 在同一时间内限制缓冲到一个临时文件的数据量，以使得Nginx不会过长地阻止单个请求 proxy_temp_path 设定临时文件的缓冲，用于缓冲从上游服务器来的文件，可以设定目录的层次 1.2. upstream模块 upstream指令将会启用一个新的配置区域，在该区域定义了一组上游服务器，这些服务器可以被设置为不同的权重（权重高的服务器将会被Nginx传递越多的连接）。\n指令 说明 ip_hash 通过IP地址的哈希值确保客户端均匀地连接所有的服务器，键值基于C类地址 keepalive 每一个worker进程缓存的到上游服务器的连接数。再使用Http连接时，proxy_http_verison设置1.1，并将proxy_set_header设置为Connection \"\" least_conn 激活负载均衡算法，将请求发送到活跃连接数最少的那台服务器 server 为upstream定义一个服务器地址（带有端口号的域名、IP地址，或者是UNIX套接字）和一个可选的参数，参数如下：weight：设置一个服务器的优先级优于其他服务器。max_fails：设置在fail_timeout时间之内尝试对一个服务器连接的最大次数，如果超过这个次数，那么就会被标记为down。fail_timeout：在这个指定的时间内服务器必须提供响应，如果在这个时间内没有收到响应，那么服务器就会被标记为down状态。backup：一旦其他服务器宕机，那么有该标记的机器就会接收请求。down：标记为一个服务器不再接受任何请求。 1.2.1. 负载均衡算法 upstream模块能够使用轮询、IP hash和最少连接数三种负载均衡算法之一来选择哪个上游服务器将会被在下一步中连接。\n1.2.1.1. 轮询 默认情况使用轮询，不需要配置指令来设置，该算法选择下一个服务器，基于先前选择，再配置文件中哪一个是下一个服务器，以及每个服务器的负载。轮询算法是基于在队列中谁是下一个的原理确保将访问量均匀的分配给每一个上游服务器。\n1.2.1.2. IP 哈希 通过ip_hash指令激活使用，从而将某些IP地址映射到同一个上游服务器。\n1.2.1.3. 最少连接数 通过least_conn指令启用，该算法通过选择一个活跃的最少连接数服务器，然后将负载均匀分配给上游服务器。如果上游服务器的处理器能力不同，那么可以为server指令使用weight来指示说明。该算法将考虑到不同服务器的加权最小连接数。\n2. Upstream服务器类型 上游服务器是Ngixn代理连接的一个服务器，可以是物理机或虚拟机。\n2.1. 单个upstream服务器 指令try_files(包括http core模块内)意味着按顺序尝试，直到找到一个匹配为止。Nginx将会投递与客户端给定URI匹配的任何文件，如果没有找到任何配置文件，将会把请求代理到Apache作进一步处理。\n2.2. 多个upstream服务器 Nginx将会通过轮询的方式将连续请求传递给3个上游服务器。这样应用程序不会过载。\n3. 负载均衡特别说明 如果客户端希望总是访问同一个上游服务器，可以使用ip_hash指令； 如果请求响应时间长短不一，可以使用least_conn指令； 默认为轮询。 ","categories":"","description":"","excerpt":"1. 反向代理简介 Nginx可以作为反向代理，接收客户端的请求，并向上游服务器发起新的请求。该请求可以根据客户端请求的URI，客户机参数或 …","ref":"/linux-notes/nginx/nginx-proxy/","tags":["Nginx"],"title":"Nginx作为反向代理"},{"body":"1. bee工具 bee工具用来进行beego项目的创建、热编译、开发、测试、和部署。\n安装:\ngo get github.com/beego/bee 配置：\n安装完之后，bee可执行文件默认存放在$GOPATH/bin里面，所以要把$GOPATH/bin添加到环境变量中。\n2. bee命令 Bee is a tool for managing beego framework. Usage: bee command [arguments] The commands are: new create an application base on beego framework run run the app which can hot compile pack compress an beego project api create an api application base on beego framework bale packs non-Go files to Go source files version show the bee \u0026 beego version generate source code generator migrate run database migrations 说明：\n2.1. new 在 $GOPATH/src的目录下执行bee new \u003cappname\u003e，会在当前目录下生成以下文件：\nmyproject ├── conf │ └── app.conf ├── controllers │ └── default.go ├── main.go ├── models ├── routers │ └── router.go ├── static │ ├── css │ ├── img │ └── js ├── tests │ └── default_test.go └── views └── index.tpl 2.2. run 必须在$GOPATH/src/appname下执行bee run，默认监听8080端口：http://localhost:8080/。\n2.3. api api 命令就是用来创建 API 应用，生成以下文件：和 Web 项目相比，少了 static 和 views 目录，多了一个 test 模块，用来做单元测试。\napiproject ├── conf │ └── app.conf ├── controllers │ └── object.go │ └── user.go ├── docs │ └── doc.go ├── main.go ├── models │ └── object.go │ └── user.go ├── routers │ └── router.go └── tests └── default_test.go 2.4. pack pack 目录用来发布应用的时候打包，会把项目打包成 zip 包(apiproject.tar.gz)，这样我们部署的时候直接把打包之后的项目上传，解压就可以部署了：\n2.5. generate 用来自动化的生成代码的，包含了从数据库一键生成model，还包含了scaffold。\n2.6. migrate 这个命令是应用的数据库迁移命令，主要是用来每次应用升级，降级的SQL管理。\n参考：\nhttps://beego.me/docs/install/bee.md ","categories":"","description":"","excerpt":"1. bee工具 bee工具用来进行beego项目的创建、热编译、开发、测试、和部署。\n安装:\ngo get …","ref":"/golang-notes/web/beego/bee/","tags":["Golang"],"title":"Bee 工具使用"},{"body":"1. GDB简介 GDB是FSF(自由软件基金会)发布的一个强大的类UNIX系统下的程序调试工具。使用GDB可以做如下事情：\n启动程序，可以按照开发者的自定义要求运行程序。 可让被调试的程序在开发者设定的调置的断点处停住。（断点可以是条件表达式） 当程序被停住时，可以检查此时程序中所发生的事。 动态的改变当前程序的执行环境。 目前支持调试Go程序的GDB版本必须大于7.1。\n编译Go程序的时候需要注意以下几点\n传递参数-ldflags \"-s\"，忽略debug的打印信息 传递-gcflags \"-N -l\" 参数，这样可以忽略Go内部做的一些优化，聚合变量和函数等优化，这样对于GDB调试来说非常困难，所以在编译的时候加入这两个参数避免这些优化。 2. 常用命令 2.1. list 简写命令l，用来显示源代码，默认显示十行代码，后面可以带上参数显示的具体行，例如：list 15，显示十行代码，其中第15行在显示的十行里面的中间，如下所示。\n10\ttime.Sleep(2 * time.Second) 11\tc \u003c- i 12\t} 13\tclose(c) 14\t} 15\t16\tfunc main() { 17\tmsg := \"Starting main\" 18\tfmt.Println(msg) 19\tbus := make(chan int) 2.2. break 简写命令 b,用来设置断点，后面跟上参数设置断点的行数，例如b 10在第十行设置断点。\n2.3. delete 简写命令 d,用来删除断点，后面跟上断点设置的序号，这个序号可以通过info breakpoints获取相应的设置的断点序号，如下是显示的设置断点序号。\nNum Type Disp Enb Address What 2 breakpoint keep y 0x0000000000400dc3 in main.main at /home/xiemengjun/gdb.go:23 breakpoint already hit 1 time 2.4. backtrace 简写命令 bt,用来打印执行的代码过程，如下所示：\n#0 main.main () at /home/xiemengjun/gdb.go:23 #1 0x000000000040d61e in runtime.main () at /home/xiemengjun/go/src/pkg/runtime/proc.c:244 #2 0x000000000040d6c1 in schedunlock () at /home/xiemengjun/go/src/pkg/runtime/proc.c:267 #3 0x0000000000000000 in ?? () 2.5. info info命令用来显示信息，后面有几种参数，我们常用的有如下几种：\n1、 info locals\n显示当前执行的程序中的变量值\n2、 info breakpoints\n显示当前设置的断点列表\n3、 info goroutines\n显示当前执行的goroutine列表，如下代码所示,带*的表示当前执行的\n* 1 running runtime.gosched * 2 syscall runtime.entersyscall 3 waiting runtime.gosched 4 runnable runtime.gosched 2.6. print 简写命令p，用来打印变量或者其他信息，后面跟上需要打印的变量名，当然还有一些很有用的函数$len()和$cap()，用来返回当前string、slices或者maps的长度和容量。\n2.7. whatis 用来显示当前变量的类型，后面跟上变量名，例如whatis msg,显示如下：\ntype = struct string 2.8. next 简写命令 n,用来单步调试，跳到下一步，当有断点之后，可以输入n跳转到下一步继续执行\n2.9. coutinue 简称命令 c，用来跳出当前断点处，后面可以跟参数N，跳过多少次断点\n2.10. set variable 该命令用来改变运行过程中的变量值，格式如：set variable \u003cvar\u003e=\u003cvalue\u003e\n3. 调试过程 3.1. 示例代码 package main import ( \"fmt\" \"time\" ) func counting(c chan\u003c- int) { for i := 0; i \u003c 10; i++ { time.Sleep(2 * time.Second) c \u003c- i } close(c) } func main() { msg := \"Starting main\" fmt.Println(msg) bus := make(chan int) msg = \"starting a gofunc\" go counting(bus) for count := range bus { fmt.Println(\"count:\", count) } } 3.2. 调试步骤 编译文件，生成可执行文件gdbfile:\ngo build -gcflags \"-N -l\" gdbfile.go 通过gdb命令启动调试：\ngdb gdbfile 启动之后首先看看这个程序是不是可以运行起来，只要输入run命令回车后程序就开始运行，程序正常的话可以看到程序输出如下，和我们在命令行直接执行程序输出是一样的：\n(gdb) run Starting program: /home/xiemengjun/gdbfile Starting main count: 0 count: 1 count: 2 count: 3 count: 4 count: 5 count: 6 count: 7 count: 8 count: 9 [LWP 2771 exited] [Inferior 1 (process 2771) exited normally] 好了，现在我们已经知道怎么让程序跑起来了，接下来开始给代码设置断点：\n(gdb) b 23 Breakpoint 1 at 0x400d8d: file /home/xiemengjun/gdbfile.go, line 23. (gdb) run Starting program: /home/xiemengjun/gdbfile Starting main [New LWP 3284] [Switching to LWP 3284] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) 上面例子b 23表示在第23行设置了断点，之后输入run开始运行程序。现在程序在前面设置断点的地方停住了，我们需要查看断点相应上下文的源码，输入list就可以看到源码显示从当前停止行的前五行开始：\n(gdb) list 18 fmt.Println(msg) 19 bus := make(chan int) 20 msg = \"starting a gofunc\" 21 go counting(bus) 22 for count := range bus { 23 fmt.Println(\"count:\", count) 24 } 25 } 现在GDB在运行当前的程序的环境中已经保留了一些有用的调试信息，我们只需打印出相应的变量，查看相应变量的类型及值：\n(gdb) info locals count = 0 bus = 0xf840001a50 (gdb) p count $1 = 0 (gdb) p bus $2 = (chan int) 0xf840001a50 (gdb) whatis bus type = chan int 接下来该让程序继续往下执行，请继续看下面的命令\n(gdb) c Continuing. count: 0 [New LWP 3303] [Switching to LWP 3303] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) (gdb) c Continuing. count: 1 [Switching to LWP 3302] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) 每次输入c之后都会执行一次代码，又跳到下一次for循环，继续打印出来相应的信息。设想目前需要改变上下文相关变量的信息，跳过一些过程，并继续执行下一步，得出修改后想要的结果：\n(gdb) info locals count = 2 bus = 0xf840001a50 (gdb) set variable count=9 (gdb) info locals count = 9 bus = 0xf840001a50 (gdb) c Continuing. count: 9 [Switching to LWP 3302] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) 最后稍微思考一下，前面整个程序运行的过程中到底创建了多少个goroutine，每个goroutine都在做什么：\n(gdb) info goroutines * 1 running runtime.gosched * 2 syscall runtime.entersyscall 3 waiting runtime.gosched 4 runnable runtime.gosched (gdb) goroutine 1 bt #0 0x000000000040e33b in runtime.gosched () at /home/xiemengjun/go/src/pkg/runtime/proc.c:927 #1 0x0000000000403091 in runtime.chanrecv (c=void, ep=void, selected=void, received=void) at /home/xiemengjun/go/src/pkg/runtime/chan.c:327 #2 0x000000000040316f in runtime.chanrecv2 (t=void, c=void) at /home/xiemengjun/go/src/pkg/runtime/chan.c:420 #3 0x0000000000400d6f in main.main () at /home/xiemengjun/gdbfile.go:22 #4 0x000000000040d0c7 in runtime.main () at /home/xiemengjun/go/src/pkg/runtime/proc.c:244 #5 0x000000000040d16a in schedunlock () at /home/xiemengjun/go/src/pkg/runtime/proc.c:267 #6 0x0000000000000000 in ?? () 通过查看goroutines的命令我们可以清楚地了解goruntine内部是怎么执行的，每个函数的调用顺序已经明明白白地显示出来了。\n参考《Go Web编程》\n","categories":"","description":"","excerpt":"1. GDB简介 GDB是FSF(自由软件基金会)发布的一个强大的类UNIX系统下的程序调试工具。使用GDB可以做如下事情：\n启动程序，可以 …","ref":"/golang-notes/test/gdb/","tags":["Golang"],"title":"GDB调试"},{"body":"1. Controller Manager简介 Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。\n每个Controller通过API Server提供的接口实时监控整个集群的每个资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将系统状态修复到“期望状态”。\n2. Replication Controller 为了区分，将资源对象Replication Controller简称RC,而本文中是指Controller Manager中的Replication Controller，称为副本控制器。副本控制器的作用即保证集群中一个RC所关联的Pod副本数始终保持预设值。\n只有当Pod的重启策略是Always的时候（RestartPolicy=Always），副本控制器才会管理该Pod的操作（创建、销毁、重启等）。 RC中的Pod模板就像一个模具，模具制造出来的东西一旦离开模具，它们之间就再没关系了。一旦Pod被创建，无论模板如何变化，也不会影响到已经创建的Pod。 Pod可以通过修改label来脱离RC的管控，该方法可以用于将Pod从集群中迁移，数据修复等调试。 删除一个RC不会影响它所创建的Pod，如果要删除Pod需要将RC的副本数属性设置为0。 不要越过RC创建Pod，因为RC可以实现自动化控制Pod，提高容灾能力。 2.1. Replication Controller的职责 确保集群中有且仅有N个Pod实例，N是RC中定义的Pod副本数量。 通过调整RC中的spec.replicas属性值来实现系统扩容或缩容。 通过改变RC中的Pod模板来实现系统的滚动升级。 2.2. Replication Controller使用场景 使用场景 说明 使用命令 重新调度 当发生节点故障或Pod被意外终止运行时，可以重新调度保证集群中仍然运行指定的副本数。 弹性伸缩 通过手动或自动扩容代理修复副本控制器的spec.replicas属性，可以实现弹性伸缩。 kubectl scale 滚动更新 创建一个新的RC文件，通过kubectl 命令或API执行，则会新增一个新的副本同时删除旧的副本，当旧副本为0时，删除旧的RC。 kubectl rolling-update 滚动升级，具体可参考kubectl rolling-update --help,官方文档：https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/\n3. Node Controller kubelet在启动时会通过API Server注册自身的节点信息，并定时向API Server汇报状态信息，API Server接收到信息后将信息更新到etcd中。\nNode Controller通过API Server实时获取Node的相关信息，实现管理和监控集群中的各个Node节点的相关控制功能。流程如下\n1、Controller Manager在启动时如果设置了--cluster-cidr参数，那么为每个没有设置Spec.PodCIDR的Node节点生成一个CIDR地址，并用该CIDR地址设置节点的Spec.PodCIDR属性，防止不同的节点的CIDR地址发生冲突。\n2、具体流程见以上流程图。\n3、逐个读取节点信息，如果节点状态变成非“就绪”状态，则将节点加入待删除队列，否则将节点从该队列删除。\n4. ResourceQuota Controller 资源配额管理确保指定的资源对象在任何时候都不会超量占用系统物理资源。\n支持三个层次的资源配置管理：\n1）容器级别：对CPU和Memory进行限制\n2）Pod级别：对一个Pod内所有容器的可用资源进行限制\n3）Namespace级别：包括\nPod数量 Replication Controller数量 Service数量 ResourceQuota数量 Secret数量 可持有的PV（Persistent Volume）数量 说明：\nk8s配额管理是通过Admission Control（准入控制）来控制的； Admission Control提供两种配额约束方式：LimitRanger和ResourceQuota； LimitRanger作用于Pod和Container； ResourceQuota作用于Namespace上，限定一个Namespace里的各类资源的使用总额。 ResourceQuota Controller流程图：\n5. Namespace Controller 用户通过API Server可以创建新的Namespace并保存在etcd中，Namespace Controller定时通过API Server读取这些Namespace信息。\n如果Namespace被API标记为优雅删除（即设置删除期限，DeletionTimestamp）,则将该Namespace状态设置为“Terminating”,并保存到etcd中。同时Namespace Controller删除该Namespace下的ServiceAccount、RC、Pod等资源对象。\n6. Endpoint Controller Service、Endpoint、Pod的关系：\nEndpoints表示了一个Service对应的所有Pod副本的访问地址，而Endpoints Controller负责生成和维护所有Endpoints对象的控制器。它负责监听Service和对应的Pod副本的变化。\n如果监测到Service被删除，则删除和该Service同名的Endpoints对象； 如果监测到新的Service被创建或修改，则根据该Service信息获得相关的Pod列表，然后创建或更新Service对应的Endpoints对象。 如果监测到Pod的事件，则更新它对应的Service的Endpoints对象。 kube-proxy进程获取每个Service的Endpoints，实现Service的负载均衡功能。\n7. Service Controller Service Controller是属于kubernetes集群与外部的云平台之间的一个接口控制器。Service Controller监听Service变化，如果是一个LoadBalancer类型的Service，则确保外部的云平台上对该Service对应的LoadBalancer实例被相应地创建、删除及更新路由转发表。\n参考《Kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. Controller Manager简介 Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod …","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-controller-manager/","tags":["Kubernetes"],"title":"Kubernetes核心原理（二）之Controller Manager"},{"body":"1. kubernetes网络模型 1.1. 基础原则 每个Pod都拥有一个独立的IP地址，而且假定所有Pod都在一个可以直接连通的、扁平的网络空间中，不管是否运行在同一Node上都可以通过Pod的IP来访问。 k8s中Pod的IP是最小粒度IP。同一个Pod内所有的容器共享一个网络堆栈，该模型称为IP-per-Pod模型。 Pod由docker0实际分配的IP，Pod内部看到的IP地址和端口与外部保持一致。同一个Pod内的不同容器共享网络，可以通过localhost来访问对方的端口，类似同一个VM内的不同进程。 IP-per-Pod模型从端口分配、域名解析、服务发现、负载均衡、应用配置等角度看，Pod可以看作是一台独立的VM或物理机。 1.2. k8s对集群的网络要求 所有容器都可以不用NAT的方式同别的容器通信。 所有节点都可以在不同NAT的方式下同所有容器通信，反之亦然。 容器的地址和别人看到的地址是同一个地址。 以上的集群网络要求可以通过第三方开源方案实现，例如flannel。\n1.3. 网络架构图 1.4. k8s集群IP概念汇总 由集群外部到集群内部：\nIP类型 说明 Proxy-IP 代理层公网地址IP，外部访问应用的网关服务器。[实际需要关注的IP] Service-IP Service的固定虚拟IP，Service-IP是内部，外部无法寻址到。 Node-IP 容器宿主机的主机IP。 Container-Bridge-IP 容器网桥（docker0）IP，容器的网络都需要通过容器网桥转发。 Pod-IP Pod的IP，等效于Pod中网络容器的Container-IP。 Container-IP 容器的IP，容器的网络是个隔离的网络空间。 2. kubernetes的网络实现 k8s网络场景\n容器与容器之间的直接通信。 Pod与Pod之间的通信。 Pod到Service之间的通信。 集群外部与内部组件之间的通信。 2.1. Pod网络 Pod作为kubernetes的最小调度单元，Pod是容器的集合，是一个逻辑概念，Pod包含的容器都运行在同一个宿主机上，这些容器将拥有同样的网络空间，容器之间能够互相通信，它们能够在本地访问其它容器的端口。 实际上Pod都包含一个网络容器，它不做任何事情，只是用来接管Pod的网络，业务容器通过加入网络容器的网络从而实现网络共享。Pod网络本质上还是容器网络，所以Pod-IP就是网络容器的Container-IP。\n一般将容器云平台的网络模型打造成一个扁平化网络平面，在这个网络平面内，Pod作为一个网络单元同Kubernetes Node的网络处于同一层级。\n2.2. Pod内部容器之间的通信 同一个Pod之间的不同容器因为共享同一个网络命名空间，所以可以直接通过localhost直接通信。\n2.3. Pod之间的通信 2.3.1. 同Node的Pod之间的通信 同一个Node内，不同的Pod都有一个全局IP，可以直接通过Pod的IP进行通信。Pod地址和docker0在同一个网段。\n在pause容器启动之前，会创建一个虚拟以太网接口对（veth pair），该接口对一端连着容器内部的eth0 ，一端连着容器外部的vethxxx，vethxxx会绑定到容器运行时配置使用的网桥bridge0上，从该网络的IP段中分配IP给容器的eth0。\n当同节点上的Pod-A发包给Pod-B时，包传送路线如下：\npod-a的eth0—\u003epod-a的vethxxx—\u003ebridge0—\u003epod-b的vethxxx—\u003epod-b的eth0 因为相同节点的bridge0是相通的，因此可以通过bridge0来完成不同pod直接的通信，但是不同节点的bridge0是不通的，因此不同节点的pod之间的通信需要将不同节点的bridge0给连接起来。\n2.3.2. 不同Node的Pod之间的通信 不同的Node之间，Node的IP相当于外网IP，可以直接访问，而Node内的docker0和Pod的IP则是内网IP，无法直接跨Node访问。需要通过Node的网卡进行转发。\n所以不同Node之间的通信需要达到两个条件：\n对整个集群中的Pod-IP分配进行规划，不能有冲突（可以通过第三方开源工具来管理，例如flannel）。 将Node-IP与该Node上的Pod-IP关联起来，通过Node-IP再转发到Pod-IP。 不同节点的Pod之间的通信需要将不同节点的bridge0给连接起来。连接不同节点的bridge0的方式有好几种，主要有overlay和underlay，或常规的三层路由。\n不同节点的bridge0需要不同的IP段，保证Pod IP分配不会冲突，节点的物理网卡eth0也要和该节点的网桥bridge0连接。因此，节点a上的pod-a发包给节点b上的pod-b，路线如下：\n节点a上的pod-a的eth0—\u003epod-a的vethxxx—\u003e节点a的bridge0—\u003e节点a的eth0—\u003e 节点b的eth0—\u003e节点b的bridge0—\u003epod-b的vethxxx—\u003epod-b的eth0 1. Pod间实现通信\n例如：Pod1和Pod2（同主机），Pod1和Pod3(跨主机)能够通信\n实现：因为Pod的Pod-IP是Docker网桥分配的，Pod-IP是同Node下全局唯一的。所以将不同Kubernetes Node的 Docker网桥配置成不同的IP网段即可。\n2. Node与Pod间实现通信\n例如：Node1和Pod1/ Pod2(同主机)，Pod3(跨主机)能够通信\n实现：在容器集群中创建一个覆盖网络(Overlay Network)，联通各个节点，目前可以通过第三方网络插件来创建覆盖网络，比如Flannel和Open vSwitch等。\n不同节点间的Pod访问也可以通过calico形成的Pod IP的路由表来解决。\n2.4. Service网络 Service的就是在Pod之间起到服务代理的作用，对外表现为一个单一访问接口，将请求转发给Pod，Service的网络转发是Kubernetes实现服务编排的关键一环。Service都会生成一个虚拟IP，称为Service-IP， Kuberenetes Porxy组件负责实现Service-IP路由和转发，在容器覆盖网络之上又实现了虚拟转发网络。\nKubernetes Porxy实现了以下功能：\n转发访问Service的Service-IP的请求到Endpoints(即Pod-IP)。 监控Service和Endpoints的变化，实时刷新转发规则。 负载均衡能力。 3. 开源的网络组件 3.1. Flannel 具体参考Flannel介绍\n参考《Kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. kubernetes网络模型 1.1. 基础原则 每个Pod都拥有一个独立的IP地址，而且假定所有Pod都在一个可以直接连通的、扁平的 …","ref":"/kubernetes-notes/network/kubernetes-network/","tags":["Kubernetes"],"title":"K8S网络"},{"body":"1. 概述 1.1. cAdvisor cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，cAdvisor集成在Kubelet中，当kubelet启动时会自动启动cAdvisor，即一个cAdvisor仅对一台Node机器进行监控。kubelet的启动参数--cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器Node_IP:port访问。项目主页：http://github.com/google/cadvisor。\n1.2. Heapster 是对集群中的各个Node、Pod的资源使用数据进行采集，通过访问每个Node上Kubelet的API，再通过Kubelet调用cAdvisor的API来采集该节点上所有容器的性能数据。由Heapster进行数据汇聚，保存到后端存储系统中，例如InfluxDB，Google Cloud Logging等。项目主页为：https://github.com/kubernetes/heapster。\n1.3. InfluxDB 是分布式时序数据库（每条记录带有时间戳属性），主要用于实时数据采集、事件跟踪记录、存储时间图表、原始数据等。提供REST API用于数据的存储和查询。项目主页为http://InfluxDB.com。\n1.4. Grafana 通过Dashboard将InfluxDB的时序数据展现成图表形式，便于查看集群运行状态。项目主页为http://Grafana.org。\n1.5. 总体架构图 其中当前Kubernetes中，Heapster、InfluxDB、Grafana均以Pod的形式启动和运行。Heapster与Master需配置安全连接。\n2. 部署与使用 2.1. cAdvisor kubelet的启动参数--cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器Node_IP:port访问。也提供了REST API供客户端远程调用，API返回的格式为JSON，可以采用URL访问：http://hostname:port/api/version/request/\n例如：http://14.152.49.100:4194/api/v1.3/machine 获取主机信息。\n2.2. Service 2.2.1. heapster-service heapster-service.yaml\napiVersion:v1 kind:Service metadata: label: kubenetes.io/cluster-service:\"true\" kubernetes.io/name:Heapster name:heapster namespace:kube-system spec: ports: - port:80 targetPort:8082 selector: k8s-app:heapster 2.2.2. influxdb-service influxdb-service.yaml\napiVersion:v1 kind:Service metadata: label:null name:monitoring-InfluxDB namespace:kube-system spec: type:Nodeport ports: - name:http port:80 targetPort:8083 - name:api port:8086 targetPort:8086 Nodeport:8086 selector: name:influxGrafana 2.2.3. grafana-service grafana-service.yaml\napiVersion:v1 kind:Service metadata: label: kubenetes.io/cluster-service:\"true\" kubernetes.io/name:monitoring-Grafana name:monitoring-Grafana namespace:kube-system spec: type:Nodeport ports: port:80 targetPort:8080 Nodeport:8085 selector: name:influxGrafana 使用type=NodePort将InfluxDB和Grafana暴露在Node的端口上，以便通过浏览器进行访问。\n2.2.4. 创建service kubectl create -f heapster-service.yaml kubectl create -f InfluxDB-service.yaml kubectl create -f Grafana-service.yaml 2.3. ReplicationController 2.3.1. influxdb-grafana-controller influxdb-grafana-controller-v3.yaml\napiVersion:v1 kind:ReplicationController metadata: name:monitoring-influxdb-grafana-v3 namespace:kube-system labels: k8s-app:influxGrafana version:v3 kubernetes.io/cluster-service:\"true spec: replicas:1 selector: k8s-app:influxGrafana version:v3 template: metadata: labels: k8s-app:influxGrafana version:v3 kubernetes.io/cluster-service:\"true spec: containers: - image:gcr.io/google_containers/heapster_influxdb:v0.5 name:influxdb resources: limits: cpu:100m memory:500Mi requests: cpu:100m memory:500Mi ports: - containerPort:8083 - containerPort:8086 volumeMounts: -name:influxdb-persistent-storage mountPath:/data - image:grc.io/google_containers/heapster_grafana:v2.6.0-2 name:grafana resources: limits: cpu:100m memory:100Mi requests: cpu:100m memory:100Mi env: - name:INFLUXDB_SERVICE_URL value:http://monitoring-influxdb:8086 - name:GF_AUTH_BASIC_ENABLED value:\"false\" - name:GF_AUTH_ANONYMOUS_ENABLED value:\"true\" - name:GF_AUTH_ANONYMOUS_ORG_ROLE value:Admin - name:GF_SERVER_ROOT_URL value:/api/v1/proxy/namespace/kube-system/services/monitoring-grafana/ volumeMounts: - name:grafana-persistent-storage mountPath:/var volumes: - name:influxdb-persistent-storage emptyDir{} - name:grafana-persistent-storage emptyDir{} 2.3.2. heapster-controller heapster-controller.yaml\napiVersion:v1 kind:ReplicationController metadata: labels: k8s-app:heapster name:heapster version:v6 name:heapster namespace:kube-system spec: replicas:1 selector: name:heapster k8s-app:heapster version:v6 template: metadata: labels: k8s-app:heapster version:v6 spec: containers: - image:gcr.io/google_containers/heapster:v0.17.0 name:heapster command: - /heapster - --source=kubernetes:http://192.168.1.128:8080?inClusterConfig=flase\u0026kubeletHttps=true\u0026useServiceAccount=true\u0026auth= - --sink=InfluxDB:http://monitoring-InfluxDB:8086 Heapster设置启动参数说明：\n1、–source\n配置监控来源，本例中表示从k8s-Master获取各个Node的信息。在URL的参数部分，修改kubeletHttps、inClusterConfig、useServiceAccount的值。\n2、–sink\n配置后端的存储系统，本例中使用InfluxDB。URL中主机名的地址是InfluxDB的Service名字，需要DNS服务正常工作，如果没有配置DNS服务可使用Service的ClusterIP地址。\n2.3.3. 创建ReplicationController kubelet create -f InfluxDB-Grafana-controller.yaml kubelet create -f heapster-controller.yaml 3. 查看界面及数据 3.1. InfluxDB 访问任意一台Node机器的30083端口。\n3.2. Grafana 访问任意一台Node机器的30080端口。\n4. 容器化部署 4.1. 拉取镜像 docker pull influxdb:latest docker pull cadvisor:latest docker pull grafana:latest docker pull heapster:latest 4.2. 运行容器 4.2.1. influxdb #influxdb docker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --volume=/opt/data/influxdb:/data --name influxsrv influxdb:latest 4.2.2. cadvisor #cadvisor docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --link influxsrv:influxsrv --name=cadvisor cadvisor:latest -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 4.2.3. grafana #grafana docker run -d -p 3000:3000 -e INFLUXDB_HOST=influxsrv -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=cadvisor -e INFLUXDB_USER=root -e INFLUXDB_PASS=root --link influxsrv:influxsrv --name grafana grafana:latest 4.2.4. heapster docker run -d -p 8082:8082 --net=host heapster:canary --source=kubernetes:http://`k8s-server-ip`:8080?inClusterConfig=false/\u0026useServiceAccount=false --sink=influxdb:http://`influxdb-ip`:8086 4.3. 访问 在浏览器输入IP:PORT\n","categories":"","description":"","excerpt":"1. 概述 1.1. cAdvisor cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用 …","ref":"/kubernetes-notes/monitor/kubernetes-cluster-monitoring/","tags":["Monitor"],"title":"Kubernetes集群监控"},{"body":"1. 系统管理 1.1. 连接mysql 快速部署docker mysql\ndocker pull mysql:5.7 启动MySQL\nmkdir -p ~/data/mysql docker run --name my-mysql -v ~/data/mysql:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 格式： mysql -h主机地址 -u用户名 －p用户密码\n#连接本地 mysql -h\u003clocalhost/127.0.0.1\u003e -P \u003cPORT\u003e -u用户名 －p用户密码 #连接远程 mysql -h \u003cmysql地址\u003e -P \u003cPORT\u003e -u \u003cuser\u003e -p \u003cpassword\u003e \u003cdb_name\u003e # 使用mycli, apt install -y mycli mycli -h \u003cmysql地址\u003e -P \u003cPORT\u003e -u \u003cuser\u003e -p \u003cpassword\u003e \u003cdb_name\u003e #退出连接 exit 1.2. 备份数据库 1.导出整个数据库\n导出文件默认是存在mysql\\bin目录下\n#1）备份单个数据库 mysqldump -u 用户名 -p 数据库名 \u003e 导出的文件名 mysqldump -u user_name -p123456 database_name \u003e outfile_name.sql #2）同时备份多个数据库，例如database1_name，database2_name mysqldump -u user_name -p123456 --databases database1_name database2_name \u003e outfile_name.sql #3）备份全部数据库 mysqldump -u user_name -p123456 --all-databases \u003e outfile_name.sql 2.导出一个表\nmysqldump -u 用户名 -p 数据库名 表名\u003e 导出的文件名 mysqldump -u user_name -p database_name table_name \u003e outfile_name.sql 3.导出一个数据库结构\nmysqldump -u user_name -p -d –add-drop-table database_name \u003e outfile_name.sql -d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table 4.带语言参数导出\nmysqldump -uroot -p –default-character-set=latin1 –set-charset=gbk –skip-opt database_name \u003e outfile_name.sql 5、导入数据库\n#1）多个个数据库 mysql -u root –p \u003c [备份文件的保存路径] 或者source [备份文件的保存路径] #2）单个数据库 mysql -uroot –p database_name \u003c [备份文件的保存路径] 或者source [备份文件的保存路径] 1.3. 用户管理 #创建用户 create user '用户名'@'IP地址' identified by '密码'; #删除用户 drop user '用户名'@'IP地址'; delete from user where user='用户名' and host='localhost'; #修改用户 rename user '用户名'@'IP地址'; to '新用户名'@'IP地址';; #修改密码 set password for '用户名'@'IP地址' = Password('新密码') mysqladmin -u用户名 -p旧密码 password 新密码 1.4. 权限管理 1.4.1. grant 1、grant 权限 on 数据库对象 to 用户\n数据库对象的格式为\u003cdatabase\u003e.\u003ctable\u003e。\u003cdatabase\u003e.*：表示授权数据库对象该数据库的所有表；*.*：表示授权数据库对象为所有数据库的所有表。\ngrant all privileges on . to \u003cuser\u003e@'\u003cip\u003e' identified by '\u003cpasswd\u003e';如果\u003cip\u003e为'%'表示不限制IP。 2、撤销权限：\nrevoke all on . from \u003cuser\u003e@\u003cip\u003e; 1.4.2. 普通数据库用户 查询、插入、更新、删除 数据库中所有表数据的权利\ngrant select, insert, update, delete on testdb.* to \u003cuser\u003e@'\u003cip\u003e'; 1.4.3. DBA 用户 #1、授权 grant all privileges on . to \u003cdba\u003e@'\u003cip\u003e' identified by '\u003cpasswd\u003e'; #2、刷新系统权限 flush privileges; 1.4.4. 查看用户权限 #查看当前用户（自己）权限 show grants; #查看指定MySQL 用户权限 show grants for \u003cuser\u003e@\u003clocalhost\u003e; #查看user和host select user,host from mysql.user order by user; 1.4.5. 权限列表 权限 说明 网站使用账户是否给予 Select 可对其下所有表进行查询 建议给予 Insert 可对其下所有表进行插入 建议给予 Update 可对其下所有表进行更新 建议给予 Delete 可对其下所有表进行删除 建议给予 Create 可在此数据库下创建表或索引 建议给予 Drop 可删除此数据库及数据库下所有表 不建议给予 Grant 赋予权限选项 不建议给予 References 未来MySQL特性的占位符 不建议给予 Index 可对其下所有表进行索引 建议给予 Alter 可对其下所有表进行更改 建议给予 Create_tmp_table 创建临时表 不建议给予 Lock_tables 可对其下所有表进行锁定 不建议给予 Create_view 可在此数据下创建视图 建议给予 Show_view 可在此数据下查看视图 建议给予 Create_routine 可在此数据下创建存储过程 不建议给予 Alter_routine 可在此数据下更改存储过程 不建议给予 Execute 可在此数据下执行存储过程 不建议给予 Event 可在此数据下创建事件调度器 不建议给予 Trigger 可在此数据下创建触发器 不建议给予 1.4.6.查看主从关系 #登录主机 show slave hosts; #登录从机 show slave status; ","categories":"","description":"","excerpt":"1. 系统管理 1.1. 连接mysql 快速部署docker mysql\ndocker pull mysql:5.7 启动MySQL …","ref":"/linux-notes/mysql/system-manage/","tags":["Mysql"],"title":"Mysql常用命令之系统管理"},{"body":"1. Pod的基本用法 1.1. 说明 Pod实际上是容器的集合，在k8s中对运行容器的要求为：容器的主程序需要一直在前台运行，而不是后台运行。应用可以改造成前台运行的方式，例如Go语言的程序，直接运行二进制文件；java语言则运行主类；tomcat程序可以写个运行脚本。或者通过supervisor的进程管理工具，即supervisor在前台运行，应用程序由supervisor管理在后台运行。具体可参考supervisord。 当多个应用之间是紧耦合的关系时，可以将多个应用一起放在一个Pod中，同个Pod中的多个容器之间互相访问可以通过localhost来通信（可以把Pod理解成一个虚拟机，共享网络和存储卷）。 1.2. Pod相关命令 操作 命令 说明 创建 kubectl create -f frontend-localredis-pod.yaml 查询Pod运行状态 kubectl get pods --namespace=\u003cNAMESPACE\u003e 查询Pod详情 kebectl describe pod \u003cPOD_NAME\u003e --namespace=\u003cNAMESPACE\u003e 该命令常用来排查问题，查看Event事件 删除 kubectl delete pod \u003cPOD_NAME\u003e ;kubectl delete pod --all 更新 kubectl replace pod.yaml - 2. Pod的定义文件 apiVersion: v1 kind: Pod metadata: name: string namaspace: string labels: - name: string annotations: - name: string spec: containers: - name: string images: string imagePullPolice: [Always | Never | IfNotPresent] command: [string] args: [string] workingDir: string volumeMounts: - name: string mountPath: string readOnly: boolean ports: - name: string containerPort: int hostPort: int protocol: string env: - name: string value: string resources: limits: cpu: string memory: string requests: cpu: string memory: string livenessProbe: exec: command: [string] httpGet: path: string port: int host: string scheme: string httpHeaders: - name: string value: string tcpSocket: port: int initialDelaySeconds: number timeoutSeconds: number periodSeconds: number successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] nodeSelector: object imagePullSecrets: - name: string hostNetwork: false volumes: - name: string emptyDir: {} hostPath: path: string secret: secretName: string items: - key: string path: string configMap: name: string items: - key: string path: string 3. 静态pod 静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod。它们不能通过API Server进行管理，无法与ReplicationController、Deployment或DaemonSet进行关联，并且kubelet也无法对其健康检查。\n静态Pod总是由kubelet创建，并且总在kubelet所在的Node上运行。\n创建静态Pod的方式：\n3.1. 通过配置文件方式 需要设置kubelet的启动参数“–config”，指定kubelet需要监控的配置文件所在目录，kubelet会定期扫描该目录，并根据该目录的.yaml或.json文件进行创建操作。静态Pod无法通过API Server删除（若删除会变成pending状态），如需删除该Pod则将yaml或json文件从这个目录中删除。\n例如：\n配置目录为/etc/kubelet.d/，配置启动参数：--config=/etc/kubelet.d/，该目录下放入static-web.yaml。\napiVersion: v1 kind: Pod metadata: name: static-web labels: name: static-web spec: containers: - name: static-web image: nginx ports: - name: web containerPort: 80 参考文章\n《Kubernetes权威指南》 ","categories":"","description":"","excerpt":"1. Pod的基本用法 1.1. 说明 Pod实际上是容器的集合，在k8s中对运行容器的要求为：容器的主程序需要一直在前台运行，而不是后台运 …","ref":"/kubernetes-notes/concepts/pod/pod-definition/","tags":["Kubernetes"],"title":"Pod定义文件"},{"body":"1. etcdctl介绍 etcdctl是一个命令行的客户端，它提供了一下简洁的命令，可理解为命令工具集，可以方便我们在对服务进行测试或者手动修改数据库内容。etcdctl与其他xxxctl的命令原理及操作类似（例如kubectl，systemctl）。\n用法：etcdctl [global options] command [command options][args...]\n2. Etcd常用命令 2.1. 数据库操作命令 etcd 在键的组织上采用了层次化的空间结构（类似于文件系统中目录的概念），数据库操作围绕对键值和目录的 CRUD [增删改查]（符合 REST 风格的一套操作：Create, Read, Update, Delete）完整生命周期的管理。\n具体的命令选项参数可以通过 etcdctl command --help来获取相关帮助。\n2.1.1. 对象为键值 set[增:无论是否存在]:etcdctl set key value mk[增:必须不存在]:etcdctl mk key value rm[删]:etcdctl rm key update[改]:etcdctl update key value get[查]:etcdctl get key 2.1.2. 对象为目录 setdir[增:无论是否存在]:etcdctl setdir dir mkdir[增:必须不存在]: etcdctl mkdir dir rmdir[删]:etcdctl rmdir dir updatedir[改]:etcdctl updatedir dir ls[查]:etcdclt ls 2.2. 非数据库操作命令 backup[备份 etcd 的数据] etcdctl backup\nwatch[监测一个键值的变化，一旦键值发生更新，就会输出最新的值并退出] etcdctl watch key\nexec-watch[监测一个键值的变化，一旦键值发生更新，就执行给定命令] etcdctl exec-watch key --sh -c \"ls\"\nmember[通过 list、add、remove、update 命令列出、添加、删除 、更新etcd 实例到 etcd 集群中] etcdctl member list；etcdctl member add 实例；etcdctl member remove 实例；etcdctl member update 实例。\netcdctl cluster-health[检查集群健康状态] 2.3. 常用配置参数 设置配置文件，默认为/etc/etcd/etcd.conf。\n配置参数 参数说明 配置参数 参数说明 -name 节点名称 -data-dir 保存日志和快照的目录，默认为当前工作目录，指定节点的数据存储目录 -addr 公布的ip地址和端口。 默认为127.0.0.1:2379 -bind-addr 用于客户端连接的监听地址，默认为-addr配置 -peers 集群成员逗号分隔的列表，例如 127.0.0.1:2380,127.0.0.1:2381 -peer-addr 集群服务通讯的公布的IP地址，默认为 127.0.0.1:2380. -peer-bind-addr 集群服务通讯的监听地址，默认为-peer-addr配置 -wal-dir 指定节点的was文件的存储目录，若指定了该参数，wal文件会和其他数据文件分开存储 -listen-client-urls -listen-peer-urls 监听URL，用于与其他节点通讯 -initial-advertise-peer-urls 告知集群其他节点url. -advertise-client-urls 告知客户端url, 也就是服务的url -initial-cluster-token 集群的ID -initial-cluster 集群中所有节点 -initial-cluster-state -initial-cluster-state=new 表示从无到有搭建etcd集群 -discovery-srv 用于DNS动态服务发现，指定DNS SRV域名 -discovery 用于etcd动态发现，指定etcd发现服务的URL [https://discovery.etcd.io/],用环境变量表示 ","categories":"","description":"","excerpt":"1. etcdctl介绍 etcdctl是一个命令行的客户端，它提供了一下简洁的命令，可理解为命令工具集，可以方便我们在对服务进行测试或者手 …","ref":"/kubernetes-notes/etcd/etcdctl/etcdctl-v2/","tags":["Etcd"],"title":"etcdctl-V2"},{"body":" 以下主要介绍PaaS平台设计架构中使用到的方法论，统称为12-Factor(要素)\n简介 软件通常会作为一种服务来交付，即软件即服务(SaaS)。12-Factor原则为构建SaaS应用提供了以下的方法论：\n使用标准化流程自动配置，减少开发者的学习成本。 和操作系统解耦，使其可以在各个系统间提供最大的移植性。 适合部署在现代的云计算平台上，从而在服务器和系统管理方面节省资源。 将开发环境与生产环境的差异降至最低，并使用持续交付实施敏捷开发。 可以在工具、架构和开发流程不发生明显变化的前提下实现拓展 该理论适应于任何语言和后端服务(数据库、消息队列、缓存等)开发的应用程序。\n1. 基准代码 一份基准代码，多份部署\n应用代码使用版本控制系统来管理，常用的有Git、SVN等。一份用来跟踪代码所有修订版本的数据库称为代码库。\n1.1. 一份基准代码 基准代码和应用之间总是保持一一对应的关系：\n一旦有多个基准代码，则不能称之为一个应用，而是一个分布式系统。分布式系统中的每个组件都是一个应用，每个应用都可以使用12-Factor原则进行开发。 多个应用共享一份基准代码有悖于12-Factor原则。解决方法是将共享的代码拆成独立的类库，通过依赖管理去使用它们。 1.2. 多份部署 每个应用只对应一份基准代码，但可以同时存在多份的部署，每份部署相当于运行了一个应用的实例。\n多份部署的区别在于：\n可以存在不同的配置文件对应不同的环境。例如开发环境、预发布环境、生产环境等。 可以使用不同的版本。例如开发环境的版本可能高于预发布环境版本，还没同步到预发布环境版本，同理，预发布环境版本可能高于生产环境版本。 2. 依赖 显式声明依赖关系\n大多数的编程语言都会提供一个包管理系统或工具，其中包含所有的依赖库，例如Golang的vendor目录存放了该应用的所有依赖包。\n12-Factor原则下的应用会通过依赖清单来显式确切地声明所有的依赖项。在运行工程中通过依赖隔离工具来保证应用不会去调用系统中存在但依赖清单中未声明的依赖项。\n显式声明依赖项的优点在于可以简化环境配置流程，开发者关注应用的基准代码，而依赖库则由依赖库管理工具来管理和配置。例如，Golang中的包管理工具dep等。\n3. 配置 在环境中存储配置\n通常，应用的配置在不同的发布环境中(例如：开发、预发布、生产环境)会有很大的差异，其中包括：\n数据库、Redis等后端服务的配置 每份部署特有的配置，例如域名 第三方服务的证书等 12-Factor原则要求代码和配置严格分离，而不应该通过代码常量的形式写在代理里面。配置在不同的部署环境中存在大幅差异，但是代码却是完全一致的。\n判断一个应用是否正确地将配置排除在代码外，可以看应用的基准代码是否可以立即开源而不担心暴露敏感信息。\n12-Factor原则建议将应用的配置存储在环境变量中，环境变量可以方便在不同的部署环境中修改，而不侵入原有的代码。(例如，k8s的大部分代码配置是通过环境变量的方式来传入的)。\n12-Factor应用中，环境变量的粒度要足够小且相对独立。当应用需要拓展时，可以平滑过渡。\n4. 后端服务 把后端服务当作附加资源\n后端服务指程序运行时所需要通过网络调用的各种服务，例如：数据库（MySQL，CouchDB），消息/队列系统（RabbitMQ，Beanstalkd），SMTP 邮件发送服务（Postfix），以及缓存系统（Memcached）。\n其中可以根据管理对象分为本地服务(例如本地数据库)和第三方服务(例如Amason S3)。对于12-Factor应用来说都是附加资源，没有区别对待，当其中一份后端服务失效后，可以通过切换到原先备份的后端服务中，而不需要修改代码(但可能需要修改配置)。12-Factor应用与后端服务保持松耦合的关系。\n5. 构建，发布，运行 严格分离构建和运行\n基准代码转化成一份部署需要经过三个阶段：\n构建阶段：指代码转化为可执行包的过程。构建过程会使用指定版本的代码，获取依赖项，编译生成二进制文件和资源文件。 发布阶段：将构建的结果与当前部署所需的配置结合，并可以在运行环境中使用。 运行阶段（运行时）：指针对指定的发布版本在执行环境中启动一系列应用程序的进程。 12-Factor应用严格区分构建、发布、运行三个步骤，每一个发布版本对应一个唯一的发布ID，可以使用时间戳或递增的版本序列号。\n如果需要修改则需要产生一个新的发布版本，如果需要回退，则回退到之前指定的发布版本。\n新代码部署之前，由开发人员触发构建操作，构建阶段可以相对复杂一些，方便错误信息可以展示出来得到妥善处理。运行阶段可以人为触发或自动运行，运行阶段应该保持尽可能少的模块。\n6. 进程 以一个或多个无状态进程运行应用\n12-Factor应有的进程必须是无状态且无共享的，任何需要持久化的数据存储在后端服务中，例如数据库。\n内存区域和磁盘空间可以作为进程的缓存，12-Factor应用不需要关注这些缓存的持久化，而是允许其丢失，例如重启的时候。\n进程的二进制文件应该在构建阶段执行编译而不是运行阶段。\n当应用使用到粘性Session，即将用户的session数据缓存到进程的内存中，将同一用户的后续请求路由到同一个进程。12-Factor应用反对这种处理方式，而是建议将session的数据保存在redis/memcached带有过期时间的缓存中。\n7. 端口绑定 通过端口绑定提供服务\n应用通过端口绑定来提供服务，并监听发送至该端口的请求。端口绑定的方式意味着一个应用也可以成为另一个应用的后端服务，例如提供某些API请求。\n8. 并发 通过进程模型进行扩展\n12-Factor应用中，开发人员可以将不同的工作分配给不同类型进程，例如HTTP请求由web进程来处理，常驻的后台工作由worker进程来处理（k8s的设计中就经常用不同类型的manager来处理不同的任务）。\n12-Factor应用的进程具备无共享、水平分区的特性，使得水平扩展较为容易。\n12-Factor应用的进程不需要守护进程或是写入PID文件，而是通过进程管理器（例如 systemd）来管理输出流，响应崩溃的进程，以及处理用户触发的重启或关闭超级进程的操作。\n9. 易处理 快速启动和优雅终止可最大化健壮性\n12-Factor应用的进程是易处理的，即它们可以快速的开启或停止，这样有利于快速部署迭代和弹性伸缩实例。\n进程应该追求最小的启动时间，这样可以敏捷发布，增加健壮性，当出现问题可以快速在别的机器部署一个实例。\n进程一旦接收到终止信号(SIGTERM)就会优雅终止。优雅终止指停止监听服务的端口，拒绝所有新的请求，并继续执行当前已接收的请求，然后退出。\n进程还需在面对突然挂掉的情况下保持健壮性，例如通过任务队列的方式来解决进程突然挂掉而没有完成处理的事情，所以应该设计为任务执行是幂等的，可以被重复执行，重复执行的结果是一致的。\n10. 开发环境与线上环境等价 尽可能的保持开发，预发布，线上环境相同\n不同的发布环境可能存在以下差异：\n时间差异：开发到部署的周期较长。 人员差异：开发人员只负责开发，运维人员只负责部署。分工过于隔离。 工具差异：不同环境的配置和运行环境，使用的后端类型可能存在不同。 应尽量缩小本地与线上的差异，缩短上线周期，开发运维一体化，保证开发环境与线上运行的环境一致（例如，可以通过Docker容器的方式）。\n11. 日志 把日志当作事件流\n日志应该是事件流的汇总。12-Factor应用本身不考虑存储自己的日志输出流，不去写或管理日志文件，而是通过标准输出（stdout）的方式。\n日志的标准输出流可以通过其他组件截获，整合其他的日志输出流，一并发给统一的日志中心处理，用于查看或存档。例如：日志收集开源工具Fluentd。\n截获的日志流可以输出至文件，或者在终端实时查看。最重要的是可以发送到Splunk这样的日志索引及分析系统，提供后续的分析统计及监控告警等功能。例如：\n找出过去一段时间的特殊事件。 图形化一个大规模的趋势，如每分钟的请求量。 根据用户定义的条件触发告警，如每分钟报错数超过某个警戒线。 12. 管理进程 后台管理任务当作一次性进程运行\n开发人员经常需要执行一些管理或维护应用的一次性任务，一次性管理进程应该和常驻进程使用相同的运行环境，开发人员可以通过ssh方式来执行一次性脚本或任务。\n参考：\nhttps://12factor.net/ ","categories":"","description":"","excerpt":" 以下主要介绍PaaS平台设计架构中使用到的方法论，统称为12-Factor(要素)\n简介 软件通常会作为一种服务来交付，即软件即服 …","ref":"/kubernetes-notes/paas/12-factor/","tags":["Kubernetes"],"title":"12 Factor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/web/beego/","tags":"","title":"Beego Web框架"},{"body":"问题描述 机器内核版本较低，kubelet启动异常，报错如下：\nFailed to start ContainerManager failed to initialize top level QOS containers: failed to update top level Burstable QOS cgroup : failed to set supported cgroup subsystems for cgroup [kubepods burstable]: Failed to find subsystem mount for required subsystem: pids 原因分析 低版本内核的cgroup不支持pids资源的功能，\ncat /proc/cgroups #subsys_name\thierarchy\tnum_cgroups\tenabled cpuset\t5\t6\t1 cpu\t2\t76\t1 cpuacct\t2\t76\t1 memory\t4\t76\t1 devices\t10\t76\t1 freezer\t7\t6\t1 net_cls\t3\t6\t1 blkio\t8\t76\t1 perf_event\t9\t6\t1 hugetlb\t6\t6\t1 正常机器的cgroup\nroot@host:~# cat /proc/cgroups #subsys_name\thierarchy\tnum_cgroups\tenabled cpuset\t5\t17\t1 cpu\t7\t80\t1 cpuacct\t7\t80\t1 memory\t12\t80\t1 devices\t10\t80\t1 freezer\t2\t17\t1 net_cls\t4\t17\t1 blkio\t8\t80\t1 perf_event\t6\t17\t1 hugetlb\t11\t17\t1 pids\t3\t80\t1 # 此处支持pids资源 oom\t9\t1\t1 解决方案 1、升级内核版本，使得cgroup支持pids资源。\n或者\n2、将kubelet的启动参数添加 SupportPodPidsLimit=false,SupportNodePidsLimit=false\nvi /etc/systemd/system/kubelet.service # 添加 kubelet 启动参数 --feature-gates=... ,SupportPodPidsLimit=false,SupportNodePidsLimit=false \\ systemctl daemon-reload \u0026\u0026 systemctl restart kubelet.service 文档参考：\nKubernetes 1.14 稳定性改进中的进程ID限制\nhttps://blog.csdn.net/qq_38900565/article/details/100707025\nhttps://adoyle.me/Today-I-Learned/k8s/k8s-deployment.html\n","categories":"","description":"","excerpt":"问题描述 机器内核版本较低，kubelet启动异常，报错如下：\nFailed to start ContainerManager …","ref":"/kubernetes-notes/trouble-shooting/node/cgroup-pid-error/","tags":["问题排查"],"title":"Cgroup不支持pid资源"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/containerd/","tags":"","title":"Containerd"},{"body":"crictl #!/bin/bash CrictlVersion=$1 CrictlVersion=${CrictlVersion:-1.24.2} echo \"--------------install crictl--------------\" wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v${CrictlVersion}/crictl-v${CrictlVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local/bin nerdctl-${NerdctlVersion}-linux-amd64.tar.gz 设置配置文件\ncat \u003e /etc/crictl.yaml \u003c\u003c \\EOF runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 2 debug: false pull-image-on-create: false EOF 参考：\n使用 crictl 对 Kubernetes 节点进行调试 | Kubernetes\nhttps://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.mdCrictlVersion=${CrictlVersion:-1.24.2}\n","categories":"","description":"","excerpt":"crictl #!/bin/bash CrictlVersion=$1 …","ref":"/kubernetes-notes/runtime/containerd/containerd-ctl/","tags":["Containerd"],"title":"Containerd命令工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/storage/csi/","tags":"","title":"CSI"},{"body":"1. dep简介 dep是一个golang项目的包管理工具，一般只需要2-3个命令就可以将go依赖包自动下载并归档到vendor的目录中。dep官网参考：https://github.com/golang/dep\n2. dep安装 go get -u github.com/golang/dep/cmd/dep 3. dep使用 #进入到项目目录 cd /home/gopath/src/demo #dep初始化，初始化配置文件Gopkg.toml dep init #dep加载依赖包，自动归档到vendor目录 dep ensure # 最终会生成vendor目录，Gopkg.toml和Gopkg.lock的文件 4. dep的配置文件 Gopkg.toml记录依赖包列表。\n# Gopkg.toml example # # Refer to https://golang.github.io/dep/docs/Gopkg.toml.html # for detailed Gopkg.toml documentation. # # required = [\"github.com/user/thing/cmd/thing\"] # ignored = [\"github.com/user/project/pkgX\", \"bitbucket.org/user/project/pkgA/pkgY\"] # # [[constraint]] # name = \"github.com/user/project\" # version = \"1.0.0\" # # [[constraint]] # name = \"github.com/user/project2\" # branch = \"dev\" # source = \"github.com/myfork/project2\" # # [[override]] # name = \"github.com/x/y\" # version = \"2.4.0\" # # [prune] # non-go = false # go-tests = true # unused-packages = true ignored = [\"demo\"] [[constraint]] name = \"github.com/BurntSushi/toml\" version = \"0.3.0\" [prune] go-tests = true unused-packages = true 5. dep-help 更多dep的命令帮助参考dep。\n$ dep Dep is a tool for managing dependencies for Go projects Usage: \"dep [command]\" Commands: init Set up a new Go project, or migrate an existing one status Report the status of the project's dependencies ensure Ensure a dependency is safely vendored in the project prune Pruning is now performed automatically by dep ensure. version Show the dep version information Examples: dep init set up a new project dep ensure install the project's dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project Use \"dep help [command]\" for more information about a command. ","categories":"","description":"","excerpt":"1. dep简介 dep是一个golang项目的包管理工具，一般只需要2-3个命令就可以将go依赖包自动下载并归档到vendor的目录 …","ref":"/golang-notes/introduction/package/dep-usage/","tags":["Golang"],"title":"dep的使用"},{"body":"Goland配置引用mod目录索引 在preferences-Go-Go module下，启用go模块集成，配置环境变量如下，点击应用。\nGOPROXY=https://goproxy.cn,direct ","categories":"","description":"","excerpt":"Goland配置引用mod目录索引 在preferences-Go-Go module下，启用go模块集成，配置环境变量如下，点击应用。 …","ref":"/linux-notes/ide/goland/","tags":["IDE"],"title":"Goland配置"},{"body":"golangci-lint是一种go linter的工具，支持快速，可配置多种linter参数的功能。在go项目中使用golangci-lint可以帮助多人团队开发的代码风格及质量的一致性，同时也可以帮助开发者提高代码质量。可以结合Golang 代码规范配置golangci-lint的参数。\n本文主要介绍该工具的使用及常见推荐配置。\n1. golangci-lint安装 golangci-lint推荐在Makefile文件中配置安装和执行命令，参考如下Makefile内容：\n## golangci-lint的安装及命令 GOLANGCI_LINT = $(shell pwd)/bin/golangci-lint GOLANGCI_LINT_VERSION ?= v1.54.2 golangci-lint: @[ -f $(GOLANGCI_LINT) ] || { \\ set -e ;\\ curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(shell dirname $(GOLANGCI_LINT)) $(GOLANGCI_LINT_VERSION) ;\\ } .PHONY: lint lint: golangci-lint ## Run golangci-lint linter \u0026 yamllint $(GOLANGCI_LINT) run --timeout=10m .PHONY: lint-fix lint-fix: golangci-lint ## Run golangci-lint linter and perform fixes $(GOLANGCI_LINT) run --fix ## 配置在build阶段执行lint命令 ##@ Build .PHONY: build build: fmt vet lint ## Build manager binary. bash hack/build.sh 可以在makefile中集成golangci-lint，同时集成到代码CI的流程中，在代码提交和merge前自动执行golangci-lint的操作。\n通过执行make lint的命令即可运行golangci-lint。\n2. 配置参数说明 基础检测（推荐所有项目开启） 这些 linters 检查常见的代码错误和潜在问题：\ngovet: [默认开启]Go 内置的静态分析工具，用于检查可能导致运行时错误的问题。 gosimple: [默认开启]提示可以简化的代码。 staticcheck: [默认开启]检测潜在错误、不良代码风格以及性能优化建议。 errcheck: [默认开启] 检查未处理的错误。 ineffassign: [默认开启] 检测无效的变量赋值。 unused: [默认开启]检测未使用的代码（变量、函数等）。 代码风格和一致性 这些 linters 确保代码风格一致，易于维护：\ngofmt: 强制格式化代码。 goimports: 格式化代码并管理导入的包。 typecheck: 检查类型错误。 misspell: 检测并修复拼写错误。 stylecheck: 提供风格建议，确保代码符合 Go 的约定。 dupl: 检测代码中的重复部分。 wsl: 确保 if 语句等块代码之间有适当的空行。 复杂度和性能 用于优化代码的可读性和性能：\ngocyclo: 检测代码的圈复杂度，确保函数复杂度不会过高（默认阈值为 30）。 funlen: 检查函数和文件的长度（如上所述）。 megacheck: 组合了 gosimple, unused, 和 staticcheck。 prealloc: 检测在大型切片中是否提前分配了容量。 安全性 检测安全问题，适合对代码安全性有要求的项目：\nnakedret: 检查是否有裸返回值，可能导致混淆或错误。 gosec: 检测常见的安全问题，例如 SQL 注入、弱密码等。 maligned: 检测结构体字段排列是否影响内存对齐。 3. golangci.yml推荐配置 可以通过.golangci.yml配置文件来配置详细的lint参数，在项目的根目录下创建该文件。常用推荐配置如下：\n为了可以精准控制开启的类型，可以把disable-all设置为true，然后再根据团队需要逐步添加enable的类型，避免开启检查过多影响开发效率。\nlinters：主要配置开启或关闭的检查类型。 linters-settings：针对linters中的具体检查类型配置该类型的参数。 run: timeout: \"10m\" linters: disable-all: true enable: # basic - govet - staticcheck - errcheck - ineffassign - gosimple - unused # style - gofmt - goimports - misspell - stylecheck - dupl - wsl - goconst # complexity - funlen - gocyclo - lll # security - gosec linters-settings: funlen: # Checks the number of lines in a function. lines: 80 # Checks the number of statements in a function. statements: 40 # Ignore comments when counting lines. ignore-comments: true lines-in-file: 800 gocyclo: # Minimal code complexity to report. # Default: 30 (but we recommend 10-20) min-complexity: 15 lll: # Max line length, lines longer will be reported. # Default: 120. line-length: 120 dupl: # Tokens count to trigger issue. # Default: 150 threshold: 100 output: format: colored-line-number print-issued-lines: true print-config: true 参考：\nhttps://golangci-lint.run/ https://golangci-lint.run/usage/configuration/ https://golangci-lint.run/usage/linters/ ","categories":"","description":"","excerpt":"golangci-lint是一种go linter的工具，支持快速，可配置多种linter参数的功能。在go项目中使 …","ref":"/golang-notes/standard/golangci-lint/","tags":"","title":"golangci-lint的使用"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/","tags":"","title":"Golang学习笔记"},{"body":" 本文基于《Kubernetes源码剖析》整理，结合k8s v1.22.0代码分析\n概述 k8s基于Etcd作为存储，Etcd是分布式的KV存储集群，Etcd中存储了k8s的元数据、事件数据、状态数据等，数据前缀为/registry下，具体的各类对象的key可以参考Etcd中的k8s数据。\nEtcd作为k8s唯一存储，兼具了MySQL存储元数据和消息队列存储任务事件的功能。\nEtcd存储架构设计 k8s对etcd的操作进行了分层封装，从上(k8s)到下(etcd)，分别为：\nRESTStorage（k8s对象操作封装） RegistryStore（BeforeFunc,AfterFunc） Storage.Interface（增删改查方法封装） CacherStorage（Etcd缓存层） UnderlyingStorage（与Etcd交互） RESTStorage存储接口 每种k8s资源实现RESTStorage接口，接口代码如下：\nkubernetes/staging/src/k8s.io/apiserver/pkg/registry/rest/rest.go\n// Storage is a generic interface for RESTful storage services. // Resources which are exported to the RESTful API of apiserver need to implement this interface. It is expected // that objects may implement any of the below interfaces. type Storage interface { // New returns an empty object that can be used with Create and Update after request data has been put into it. // This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) New() runtime.Object } k8s基于etcd相关封装代码主要在/pkg/registry目录下。\n其中每种资源对于storage接口的实现定义在/pkg/registry/\u003c资源组\u003e/\u003c资源\u003e/storage/storage.go。以下以deployment为例串联k8s中关于etcd的调用流程，调用顺序从上(k8s)到下(etcd)。\nDeploymentStorage kubernetes/pkg/registry/apps/deployment/storage/storage.go\n// DeploymentStorage includes dummy storage for Deployments and for Scale subresource. type DeploymentStorage struct { Deployment *REST Status *StatusREST Scale *ScaleREST Rollback *RollbackREST } // REST implements a RESTStorage for Deployments. type REST struct { *genericregistry.Store categories []string } // StatusREST implements the REST endpoint for changing the status of a deployment type StatusREST struct { store *genericregistry.Store } // ScaleREST implements a Scale for Deployment. type ScaleREST struct { store *genericregistry.Store } // RollbackREST implements the REST endpoint for initiating the rollback of a deployment type RollbackREST struct { store *genericregistry.Store } ","categories":"","description":"","excerpt":" 本文基于《Kubernetes源码剖析》整理，结合k8s v1.22.0代码分析\n概述 k8s基于Etcd作为存储，Etcd是分布式的KV …","ref":"/k8s-source-code-analysis/summary/etcd-storage/","tags":["源码分析"],"title":"k8s中Etcd存储的实现"},{"body":"1. 配置文件路径 默认的配置文件位于/usr/share/defaults/kata-containers/configuration.toml，如果/etc/kata-containers/configuration.toml的配置文件存在，则会替代默认的配置文件。\n查看配置文件的路径命令如下：\n# kata-runtime --kata-show-default-config-paths /etc/kata-containers/configuration.toml /usr/share/defaults/kata-containers/configuration.toml 指定自定义配置文件运行\nkata-runtime --kata-config=/some/where/configuration.toml ... 2. kata-env 查看runtime使用到的环境参数，\nkata-runtime kata-env 输出内容如下：\n[Meta] Version = \"1.0.23\" [Runtime] Debug = false Trace = false DisableGuestSeccomp = true DisableNewNetNs = false Path = \"/usr/bin/kata-runtime\" [Runtime.Version] Semver = \"1.7.2\" Commit = \"9b9282693cfbcf70d442916bea56771cc6fc3afe\" OCI = \"1.0.1-dev\" [Runtime.Config] Path = \"/usr/share/defaults/kata-containers/configuration.toml\" [Hypervisor] MachineType = \"pc\" Version = \"QEMU emulator version 2.11.0\\nCopyright (c) 2003-2017 Fabrice Bellard and the QEMU Project developers\" Path = \"/usr/bin/qemu-lite-system-x86_64\" BlockDeviceDriver = \"virtio-scsi\" EntropySource = \"/dev/urandom\" Msize9p = 8192 MemorySlots = 10 Debug = false UseVSock = false SharedFS = \"virtio-9p\" [Image] Path = \"/usr/share/kata-containers/kata-containers-image_centos_1.7.2_agent_20190702.img\" [Kernel] Path = \"/usr/share/kata-containers/vmlinuz-4.19.28.42-6.1.container\" Parameters = \"init=/usr/lib/systemd/systemd systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket systemd.mask=systemd-journald.service systemd.mask=systemd-journald.socket systemd.mask=systemd-journal-flush.service systemd.mask=systemd-journald-dev-log.socket systemd.mask=systemd-udevd.service systemd.mask=systemd-udevd.socket systemd.mask=systemd-udev-trigger.service systemd.mask=systemd-udevd-kernel.socket systemd.mask=systemd-udevd-control.socket systemd.mask=systemd-timesyncd.service systemd.mask=systemd-update-utmp.service systemd.mask=systemd-tmpfiles-setup.service systemd.mask=systemd-tmpfiles-cleanup.service systemd.mask=systemd-tmpfiles-cleanup.timer systemd.mask=tmp.mount systemd.mask=systemd-random-seed.service systemd.mask=systemd-coredump@.service\" [Initrd] Path = \"\" [Proxy] Type = \"kataProxy\" Version = \"kata-proxy version 1.7.2-a56df7c\" Path = \"/usr/libexec/kata-containers/kata-proxy\" Debug = false [Shim] Type = \"kataShim\" Version = \"kata-shim version 1.7.2-2ea178c\" Path = \"/usr/libexec/kata-containers/kata-shim\" Debug = false [Agent] Type = \"kata\" Debug = false Trace = false TraceMode = \"\" TraceType = \"\" [Host] Kernel = \"4.14.105-1-tlinux3-0008\" Architecture = \"amd64\" VMContainerCapable = true SupportVSocks = true [Host.Distro] Name = \"Tencent tlinux\" Version = \"2.2\" [Host.CPU] Vendor = \"GenuineIntel\" Model = \"Intel(R) Xeon(R) CPU X3440 @ 2.53GHz\" [Netmon] Version = \"kata-netmon version 1.7.2\" Path = \"/usr/libexec/kata-containers/kata-netmon\" Debug = false Enable = false 3. configuration.toml # Copyright (c) 2017-2019 Intel Corporation # # SPDX-License-Identifier: Apache-2.0 # # XXX: WARNING: this file is auto-generated. # XXX: # XXX: Source file: \"cli/config/configuration-qemu.toml.in\" # XXX: Project: # XXX: Name: Kata Containers # XXX: Type: kata [hypervisor.qemu] path = \"/usr/bin/qemu-lite-system-x86_64\" kernel = \"/usr/share/kata-containers/vmlinuz.container\" image = \"/usr/share/kata-containers/kata-containers.img\" machine_type = \"pc\" # Optional space-separated list of options to pass to the guest kernel. # For example, use `kernel_params = \"vsyscall=emulate\"` if you are having # trouble running pre-2.15 glibc. # # WARNING: - any parameter specified here will take priority over the default # parameter value of the same name used to start the virtual machine. # Do not set values here unless you understand the impact of doing so as you # may stop the virtual machine from booting. # To see the list of default parameters, enable hypervisor debug, create a # container and look for 'default-kernel-parameters' log entries. kernel_params = \"\" # Path to the firmware. # If you want that qemu uses the default firmware leave this option empty firmware = \"\" # Machine accelerators # comma-separated list of machine accelerators to pass to the hypervisor. # For example, `machine_accelerators = \"nosmm,nosmbus,nosata,nopit,static-prt,nofw\"` machine_accelerators=\"\" # Default number of vCPUs per SB/VM: # unspecified or 0 --\u003e will be set to 1 # \u003c 0 --\u003e will be set to the actual number of physical cores # \u003e 0 \u003c= number of physical cores --\u003e will be set to the specified number # \u003e number of physical cores --\u003e will be set to the actual number of physical cores default_vcpus = 1 # Default maximum number of vCPUs per SB/VM: # unspecified or == 0 --\u003e will be set to the actual number of physical cores or to the maximum number # of vCPUs supported by KVM if that number is exceeded # \u003e 0 \u003c= number of physical cores --\u003e will be set to the specified number # \u003e number of physical cores --\u003e will be set to the actual number of physical cores or to the maximum number # of vCPUs supported by KVM if that number is exceeded # WARNING: Depending of the architecture, the maximum number of vCPUs supported by KVM is used when # the actual number of physical cores is greater than it. # WARNING: Be aware that this value impacts the virtual machine's memory footprint and CPU # the hotplug functionality. For example, `default_maxvcpus = 240` specifies that until 240 vCPUs # can be added to a SB/VM, but the memory footprint will be big. Another example, with # `default_maxvcpus = 8` the memory footprint will be small, but 8 will be the maximum number of # vCPUs supported by the SB/VM. In general, we recommend that you do not edit this variable, # unless you know what are you doing. default_maxvcpus = 0 # Bridges can be used to hot plug devices. # Limitations: # * Currently only pci bridges are supported # * Until 30 devices per bridge can be hot plugged. # * Until 5 PCI bridges can be cold plugged per VM. # This limitation could be a bug in qemu or in the kernel # Default number of bridges per SB/VM: # unspecified or 0 --\u003e will be set to 1 # \u003e 1 \u003c= 5 --\u003e will be set to the specified number # \u003e 5 --\u003e will be set to 5 default_bridges = 1 # Default memory size in MiB for SB/VM. # If unspecified then it will be set 2048 MiB. default_memory = 2048 # # Default memory slots per SB/VM. # If unspecified then it will be set 10. # This is will determine the times that memory will be hotadded to sandbox/VM. #memory_slots = 10 # The size in MiB will be plused to max memory of hypervisor. # It is the memory address space for the NVDIMM devie. # If set block storage driver (block_device_driver) to \"nvdimm\", # should set memory_offset to the size of block device. # Default 0 #memory_offset = 0 # Disable block device from being used for a container's rootfs. # In case of a storage driver like devicemapper where a container's # root file system is backed by a block device, the block device is passed # directly to the hypervisor for performance reasons. # This flag prevents the block device from being passed to the hypervisor, # 9pfs is used instead to pass the rootfs. disable_block_device_use = false # Shared file system type: # - virtio-9p (default) # - virtio-fs shared_fs = \"virtio-9p\" # Path to vhost-user-fs daemon. virtio_fs_daemon = \"/usr/bin/virtiofsd-x86_64\" # Default size of DAX cache in MiB virtio_fs_cache_size = 1024 # Cache mode: # # - none # Metadata, data, and pathname lookup are not cached in guest. They are # always fetched from host and any changes are immediately pushed to host. # # - auto # Metadata and pathname lookup cache expires after a configured amount of # time (default is 1 second). Data is cached while the file is open (close # to open consistency). # # - always # Metadata, data, and pathname lookup are cached in guest and never expire. virtio_fs_cache = \"always\" # Block storage driver to be used for the hypervisor in case the container # rootfs is backed by a block device. This is virtio-scsi, virtio-blk # or nvdimm. block_device_driver = \"virtio-scsi\" # Specifies cache-related options will be set to block devices or not. # Default false #block_device_cache_set = true # Specifies cache-related options for block devices. # Denotes whether use of O_DIRECT (bypass the host page cache) is enabled. # Default false #block_device_cache_direct = true # Specifies cache-related options for block devices. # Denotes whether flush requests for the device are ignored. # Default false #block_device_cache_noflush = true # Enable iothreads (data-plane) to be used. This causes IO to be # handled in a separate IO thread. This is currently only implemented # for SCSI. # enable_iothreads = false # Enable pre allocation of VM RAM, default false # Enabling this will result in lower container density # as all of the memory will be allocated and locked # This is useful when you want to reserve all the memory # upfront or in the cases where you want memory latencies # to be very predictable # Default false #enable_mem_prealloc = true # Enable huge pages for VM RAM, default false # Enabling this will result in the VM memory # being allocated using huge pages. # This is useful when you want to use vhost-user network # stacks within the container. This will automatically # result in memory pre allocation #enable_hugepages = true # Enable swap of vm memory. Default false. # The behaviour is undefined if mem_prealloc is also set to true #enable_swap = true # This option changes the default hypervisor and kernel parameters # to enable debug output where available. This extra output is added # to the proxy logs, but only when proxy debug is also enabled. # # Default false #enable_debug = true # Disable the customizations done in the runtime when it detects # that it is running on top a VMM. This will result in the runtime # behaving as it would when running on bare metal. # #disable_nesting_checks = true # This is the msize used for 9p shares. It is the number of bytes # used for 9p packet payload. #msize_9p = 8192 # If true and vsocks are supported, use vsocks to communicate directly # with the agent and no proxy is started, otherwise use unix # sockets and start a proxy to communicate with the agent. # Default false #use_vsock = true # VFIO devices are hotplugged on a bridge by default. # Enable hotplugging on root bus. This may be required for devices with # a large PCI bar, as this is a current limitation with hotplugging on # a bridge. This value is valid for \"pc\" machine type. # Default false #hotplug_vfio_on_root_bus = true # If host doesn't support vhost_net, set to true. Thus we won't create vhost fds for nics. # Default false #disable_vhost_net = true # # Default entropy source. # The path to a host source of entropy (including a real hardware RNG) # /dev/urandom and /dev/random are two main options. # Be aware that /dev/random is a blocking source of entropy. If the host # runs out of entropy, the VMs boot time will increase leading to get startup # timeouts. # The source of entropy /dev/urandom is non-blocking and provides a # generally acceptable source of entropy. It should work well for pretty much # all practical purposes. #entropy_source= \"/dev/urandom\" # Path to OCI hook binaries in the *guest rootfs*. # This does not affect host-side hooks which must instead be added to # the OCI spec passed to the runtime. # # You can create a rootfs with hooks by customizing the osbuilder scripts: # https://github.com/kata-containers/osbuilder # # Hooks must be stored in a subdirectory of guest_hook_path according to their # hook type, i.e. \"guest_hook_path/{prestart,postart,poststop}\". # The agent will scan these directories for executable files and add them, in # lexicographical order, to the lifecycle of the guest container. # Hooks are executed in the runtime namespace of the guest. See the official documentation: # https://github.com/opencontainers/runtime-spec/blob/v1.0.1/config.md#posix-platform-hooks # Warnings will be logged if any error is encountered will scanning for hooks, # but it will not abort container execution. #guest_hook_path = \"/usr/share/oci/hooks\" [factory] # VM templating support. Once enabled, new VMs are created from template # using vm cloning. They will share the same initial kernel, initramfs and # agent memory by mapping it readonly. It helps speeding up new container # creation and saves a lot of memory if there are many kata containers running # on the same host. # # When disabled, new VMs are created from scratch. # # Note: Requires \"initrd=\" to be set (\"image=\" is not supported). # # Default false #enable_template = true # Specifies the path of template. # # Default \"/run/vc/vm/template\" #template_path = \"/run/vc/vm/template\" # The number of caches of VMCache: # unspecified or == 0 --\u003e VMCache is disabled # \u003e 0 --\u003e will be set to the specified number # # VMCache is a function that creates VMs as caches before using it. # It helps speed up new container creation. # The function consists of a server and some clients communicating # through Unix socket. The protocol is gRPC in protocols/cache/cache.proto. # The VMCache server will create some VMs and cache them by factory cache. # It will convert the VM to gRPC format and transport it when gets # requestion from clients. # Factory grpccache is the VMCache client. It will request gRPC format # VM and convert it back to a VM. If VMCache function is enabled, # kata-runtime will request VM from factory grpccache when it creates # a new sandbox. # # Default 0 #vm_cache_number = 0 # Specify the address of the Unix socket that is used by VMCache. # # Default /var/run/kata-containers/cache.sock #vm_cache_endpoint = \"/var/run/kata-containers/cache.sock\" [proxy.kata] path = \"/usr/libexec/kata-containers/kata-proxy\" # If enabled, proxy messages will be sent to the system log # (default: disabled) #enable_debug = true [shim.kata] path = \"/usr/libexec/kata-containers/kata-shim\" # If enabled, shim messages will be sent to the system log # (default: disabled) #enable_debug = true # If enabled, the shim will create opentracing.io traces and spans. # (See https://www.jaegertracing.io/docs/getting-started). # # Note: By default, the shim runs in a separate network namespace. Therefore, # to allow it to send trace details to the Jaeger agent running on the host, # it is necessary to set 'disable_new_netns=true' so that it runs in the host # network namespace. # # (default: disabled) #enable_tracing = true [agent.kata] # If enabled, make the agent display debug-level messages. # (default: disabled) #enable_debug = true # Enable agent tracing. # # If enabled, the default trace mode is \"dynamic\" and the # default trace type is \"isolated\". The trace mode and type are set # explicity with the `trace_type=` and `trace_mode=` options. # # Notes: # # - Tracing is ONLY enabled when `enable_tracing` is set: explicitly # setting `trace_mode=` and/or `trace_type=` without setting `enable_tracing` # will NOT activate agent tracing. # # - See https://github.com/kata-containers/agent/blob/master/TRACING.md for # full details. # # (default: disabled) #enable_tracing = true # #trace_mode = \"dynamic\" #trace_type = \"isolated\" [netmon] # If enabled, the network monitoring process gets started when the # sandbox is created. This allows for the detection of some additional # network being added to the existing network namespace, after the # sandbox has been created. # (default: disabled) #enable_netmon = true # Specify the path to the netmon binary. path = \"/usr/libexec/kata-containers/kata-netmon\" # If enabled, netmon messages will be sent to the system log # (default: disabled) #enable_debug = true [runtime] # If enabled, the runtime will log additional debug messages to the # system log # (default: disabled) #enable_debug = true # # Internetworking model # Determines how the VM should be connected to the # the container network interface # Options: # # - bridged # Uses a linux bridge to interconnect the container interface to # the VM. Works for most cases except macvlan and ipvlan. # # - macvtap # Used when the Container network interface can be bridged using # macvtap. # # - none # Used when customize network. Only creates a tap device. No veth pair. # # - tcfilter # Uses tc filter rules to redirect traffic from the network interface # provided by plugin to a tap interface connected to the VM. # internetworking_model=\"tcfilter\" # disable guest seccomp # Determines whether container seccomp profiles are passed to the virtual # machine and applied by the kata agent. If set to true, seccomp is not applied # within the guest # (default: true) disable_guest_seccomp=true # If enabled, the runtime will create opentracing.io traces and spans. # (See https://www.jaegertracing.io/docs/getting-started). # (default: disabled) #enable_tracing = true # If enabled, the runtime will not create a network namespace for shim and hypervisor processes. # This option may have some potential impacts to your host. It should only be used when you know what you're doing. # `disable_new_netns` conflicts with `enable_netmon` # `disable_new_netns` conflicts with `internetworking_model=bridged` and `internetworking_model=macvtap`. It works only # with `internetworking_model=none`. The tap device will be in the host network namespace and can connect to a bridge # (like OVS) directly. # If you are using docker, `disable_new_netns` only works with `docker run --net=none` # (default: false) #disable_new_netns = true # Enabled experimental feature list, format: [\"a\", \"b\"]. # Experimental features are features not stable enough for production, # They may break compatibility, and are prepared for a big version bump. # Supported experimental features: # 1. \"newstore\": new persist storage driver which breaks backward compatibility, #\texpected to move out of experimental in 2.0.0. # (default: []) experimental=[] 参考：\nhttps://github.com/kata-containers/runtime#configuration ","categories":"","description":"","excerpt":"1. 配置文件路径 默认的配置文件位 …","ref":"/kubernetes-notes/runtime/kata/kata-container-conf/","tags":["Kubernetes"],"title":"kata配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kube-apiserver/","tags":"","title":"kube-apiserver"},{"body":"本文主要说明如何使用kubeadm来升级k8s集群。\n1. 版本注意事项 假设k8s的版本格式为x.y.z，那么使用kubeadm最多只能升级到y+1版本，或者是当前y版本的最新版本。例如你k8s集群的版本为1.24.x，那么你最大版本只能下载1.25.x的kubeadm来升级版本。\n因此升级前需要执行以下命令来验证可升级的版本。\nkubeadm upgrade plan 1.1. 版本跨度过大 如果出现以下报错，说明升级的版本跨度过大。\n# kubeadm的版本为v1.26.7 ./kubeadm version kubeadm version: \u0026version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:22:13Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"} # 当前k8s集群版本1.24.2 版本跨度过大。 ./kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [upgrade/config] FATAL: this version of kubeadm only supports deploying clusters with the control plane version \u003e= 1.25.0. Current version: v1.24.2 To see the stack trace of this error execute with --v=5 or higher 1.2. 可升级的版本计划 可升级的版本计划如下\n当前的k8s版本为1.24.2，可升级的版本是1.24.16或1.25.12，其中etcd升级的版本为3.5.6-0。\n./kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.24.2 [upgrade/versions] kubeadm version: v1.25.12 I0815 21:41:34.096199 2934255 version.go:256] remote version is much newer: v1.27.4; falling back to: stable-1.25 [upgrade/versions] Target version: v1.25.12 [upgrade/versions] Latest version in the v1.24 series: v1.24.16 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT TARGET kubelet 4 x v1.24.2 v1.24.16 Upgrade to the latest version in the v1.24 series: COMPONENT CURRENT TARGET kube-apiserver v1.24.2 v1.24.16 kube-controller-manager v1.24.2 v1.24.16 kube-scheduler v1.24.2 v1.24.16 kube-proxy v1.24.2 v1.24.16 CoreDNS v1.8.6 v1.9.3 etcd 3.5.3-0 3.5.6-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.24.16 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT TARGET kubelet 4 x v1.24.2 v1.25.12 Upgrade to the latest stable version: COMPONENT CURRENT TARGET kube-apiserver v1.24.2 v1.25.12 kube-controller-manager v1.24.2 v1.25.12 kube-scheduler v1.24.2 v1.25.12 kube-proxy v1.24.2 v1.25.12 CoreDNS v1.8.6 v1.9.3 etcd 3.5.3-0 3.5.6-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.25.12 _____________________________________________________________________ The table below shows the current state of component configs as understood by this version of kubeadm. Configs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually upgrade to is denoted in the \"PREFERRED VERSION\" column. API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED kubeproxy.config.k8s.io v1alpha1 v1alpha1 no kubelet.config.k8s.io v1beta1 v1beta1 no _____________________________________________________________________ 2. 版本升级步骤 2.1. 准备工作 下载指定版本的kubeadm二进制\n查看升级计划：kubeadm upgrade plan\n2.2. 升级master节点 2.2.1. 升级第一个master节点 kubeadm upgrade apply v1.25.x -f 升级结束会查看都以下输出：\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.25.x\". Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. 2.2.2. 手动升级你的 CNI 驱动插件。 你的容器网络接口（CNI）驱动应该提供了程序自身的升级说明。 参阅插件页面查找你的 CNI 驱动， 并查看是否需要其他升级步骤。\n如果 CNI 驱动作为 DaemonSet 运行，则在其他控制平面节点上不需要此步骤。\n2.2.3. 升级其他master节点 下载指定版本的kubeadm组件。使用以下命令升级，注意区别于第一个master的升级命令。\nkubeadm upgrade node 2.2.4. 升级master的kubelet组件 将节点标记为不可调度并驱逐所有负载，准备节点的维护：\n# 将 \u003cnode-to-drain\u003e 替换为你要腾空的控制面节点名称 kubectl drain \u003cnode-to-drain\u003e --ignore-daemonsets 下载kubelet和kubectl\n# 用最新的补丁版本替换 1.27.x-00 中的 x apt-mark unhold kubelet kubectl \u0026\u0026 \\ apt-get update \u0026\u0026 apt-get install -y kubelet=1.27.x-00 kubectl=1.27.x-00 \u0026\u0026 \\ apt-mark hold kubelet kubectl 重启kubelet\nsudo systemctl daemon-reload sudo systemctl restart kubelet 解除节点保护\n# 将 \u003cnode-to-uncordon\u003e 替换为你的节点名称 kubectl uncordon \u003cnode-to-uncordon\u003e 2.3. 升级worker节点 同 2.2.4. 升级master的kubelet组件的步骤，worker节点只需要升级kubelet。\n3. 升级版本回滚 kubeadm升级过程中会把相关目录备份到/etc/kubernetes/tmp目录，备份内容如下：\ntmp/ ├── kubeadm-backup-etcd-2023-08-16-14-50-50 │ └── etcd └── kubeadm-backup-manifests-2023-08-16-14-50-50 ├── etcd.yaml ├── kube-apiserver.yaml ├── kube-controller-manager.yaml └── kube-scheduler.yaml 如果 kubeadm upgrade 失败并且没有回滚，例如由于执行期间节点意外关闭， 你可以再次运行 kubeadm upgrade。 此命令是幂等的，并最终确保实际状态是你声明的期望状态。\n要从故障状态恢复，你还可以运行 kubeadm upgrade apply --force 而无需更改集群正在运行的版本。\n在升级期间，kubeadm 向 /etc/kubernetes/tmp 目录下的如下备份文件夹写入数据：\nkubeadm-backup-etcd-\u003cdate\u003e-\u003ctime\u003e kubeadm-backup-manifests-\u003cdate\u003e-\u003ctime\u003e kubeadm-backup-etcd 包含当前控制面节点本地 etcd 成员数据的备份。 如果 etcd 升级失败并且自动回滚也无法修复，则可以将此文件夹中的内容复制到 /var/lib/etcd 进行手工修复。如果使用的是外部的 etcd，则此备份文件夹为空。\nkubeadm-backup-manifests 包含当前控制面节点的静态 Pod 清单文件的备份版本。 如果升级失败并且无法自动回滚，则此文件夹中的内容可以复制到 /etc/kubernetes/manifests 目录实现手工恢复。 如果由于某些原因，在升级前后某个组件的清单未发生变化，则 kubeadm 也不会为之生成备份版本。\n4. 问题排查 在升级k8s 从1.25.12到1.26.7的过程中，遇到master节点的服务起不来，报错如下：\n\"CreatePodSandbox for pod failed\" err=\"open /run/systemd/resolve/resolv.conf: no such file or directory\" pod=\"kube-system/kube-apiserver\" 现象主要是静态pod起不来，包括etcd等。\n具体解决方法参考：open /run/systemd/resolve/resolv.conf\n# 查看以下的resolv.conf是否存在 cat /var/lib/kubelet/config.yaml | grep resolvConf /run/systemd/resolve/resolv.conf # 如果不存在，检查systemd-resolved是否正常运行， systemctl status systemd-resolved # 如果没有运行，则运行该服务 systemctl start systemd-resolved # 或者新建文件/run/systemd/resolve/resolv.conf，并将其他master的文件拷贝过来。 参考：\n升级 kubeadm 集群 | Kubernetes 升级 Linux 节点 | Kubernetes 安装扩展（Addon） | Kubernetes ","categories":"","description":"","excerpt":"本文主要说明如何使用kubeadm来升级k8s集群。\n1. 版本注意事项 假设k8s的版本格式为x.y.z，那么使用kubeadm最多只能升 …","ref":"/kubernetes-notes/setup/kubeadm-upgrade/","tags":["kubeadm"],"title":"kubeadm升级k8s集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/kubectl/","tags":"","title":"kubectl工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/kubeedge/code-analysis/","tags":"","title":"KubeEdge源码分析"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/object/","tags":"","title":"kubernetes对象"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/kvm/kubevirt/","tags":"","title":"KubeVirt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/","tags":"","title":"Linux学习笔记"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-controller-manager 部分的代码。\n本文主要分析 kubernetes/cmd/kube-controller-manager部分，该部分主要涉及各种类型的controller的参数解析，及初始化，例如 deployment controller 和statefulset controller。并没有具体controller运行的详细逻辑，该部分位于kubernetes/pkg/controller模块，待后续文章分析。\nkube-controller-manager的cmd部分代码目录结构如下：\nkube-controller-manager ├── app │ ├── apps.go # 包含:startDeploymentController、startReplicaSetController、startStatefulSetController、startDaemonSetController │ ├── autoscaling.go # startHPAController │ ├── batch.go # startJobController、startCronJobController │ ├── bootstrap.go │ ├── certificates.go │ ├── cloudproviders.go │ ├── config │ │ └── config.go # config: controller manager执行的上下文 │ ├── controllermanager.go # 包含:NewControllerManagerCommand、Run、NewControllerInitializers、StartControllers等 │ ├── core.go # startServiceController、startNodeIpamController、startPersistentVolumeBinderController、startNamespaceController等 │ ├── options # 包含不同controller的option参数 │ │ ├── attachdetachcontroller.go │ │ ├── csrsigningcontroller.go │ │ ├── daemonsetcontroller.go # DaemonSetControllerOptions │ │ ├── deploymentcontroller.go # DeploymentControllerOptions │ │ ├── deprecatedcontroller.go │ │ ├── endpointcontroller.go │ │ ├── garbagecollectorcontroller.go │ │ ├── hpacontroller.go │ │ ├── jobcontroller.go │ │ ├── namespacecontroller.go # NamespaceControllerOptions │ │ ├── nodeipamcontroller.go │ │ ├── nodelifecyclecontroller.go │ │ ├── options.go # KubeControllerManagerOptions、NewKubeControllerManagerOptions │ │ ├── persistentvolumebindercontroller.go │ │ ├── podgccontroller.go │ │ ├── replicasetcontroller.go # ReplicaSetControllerOptions │ │ ├── replicationcontroller.go │ │ ├── resourcequotacontroller.go │ │ ├── serviceaccountcontroller.go │ │ └── ttlafterfinishedcontroller.go └── controller-manager.go # main入口函数 1. Main函数 kube-controller-manager的入口函数Main函数，仍然是采用统一的代码风格，使用Cobra命令行框架。\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewControllerManagerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 核心代码：\n// 初始化命令行结构体 command := app.NewControllerManagerCommand() // 执行Execute err := command.Execute() 2. NewControllerManagerCommand 该部分代码位于：kubernetes/cmd/kube-controller-manager/app/controllermanager.go\n// NewControllerManagerCommand creates a *cobra.Command object with default parameters func NewControllerManagerCommand() *cobra.Command { ... cmd := \u0026cobra.Command{ Use: \"kube-controller-manager\", Long: `The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } if err := Run(c.Complete(), wait.NeverStop); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } ... } 构建一个*cobra.Command对象，然后执行Run函数。\n2.1. NewKubeControllerManagerOptions s, err := options.NewKubeControllerManagerOptions() if err != nil { glog.Fatalf(\"unable to initialize command options: %v\", err) } 初始化controllerManager的参数，其中主要包括了各种controller的option，例如DeploymentControllerOptions:\n// DeploymentControllerOptions holds the DeploymentController options. type DeploymentControllerOptions struct { ConcurrentDeploymentSyncs int32 DeploymentControllerSyncPeriod metav1.Duration } 具体代码如下：\n// NewKubeControllerManagerOptions creates a new KubeControllerManagerOptions with a default config. func NewKubeControllerManagerOptions() (*KubeControllerManagerOptions, error) { componentConfig, err := NewDefaultComponentConfig(ports.InsecureKubeControllerManagerPort) if err != nil { return nil, err } s := KubeControllerManagerOptions{ Generic: cmoptions.NewGenericControllerManagerConfigurationOptions(componentConfig.Generic), KubeCloudShared: cmoptions.NewKubeCloudSharedOptions(componentConfig.KubeCloudShared), AttachDetachController: \u0026AttachDetachControllerOptions{ ReconcilerSyncLoopPeriod: componentConfig.AttachDetachController.ReconcilerSyncLoopPeriod, }, CSRSigningController: \u0026CSRSigningControllerOptions{ ClusterSigningCertFile: componentConfig.CSRSigningController.ClusterSigningCertFile, ClusterSigningKeyFile: componentConfig.CSRSigningController.ClusterSigningKeyFile, ClusterSigningDuration: componentConfig.CSRSigningController.ClusterSigningDuration, }, DaemonSetController: \u0026DaemonSetControllerOptions{ ConcurrentDaemonSetSyncs: componentConfig.DaemonSetController.ConcurrentDaemonSetSyncs, }, DeploymentController: \u0026DeploymentControllerOptions{ ConcurrentDeploymentSyncs: componentConfig.DeploymentController.ConcurrentDeploymentSyncs, DeploymentControllerSyncPeriod: componentConfig.DeploymentController.DeploymentControllerSyncPeriod, }, DeprecatedFlags: \u0026DeprecatedControllerOptions{ RegisterRetryCount: componentConfig.DeprecatedController.RegisterRetryCount, }, EndpointController: \u0026EndpointControllerOptions{ ConcurrentEndpointSyncs: componentConfig.EndpointController.ConcurrentEndpointSyncs, }, GarbageCollectorController: \u0026GarbageCollectorControllerOptions{ ConcurrentGCSyncs: componentConfig.GarbageCollectorController.ConcurrentGCSyncs, EnableGarbageCollector: componentConfig.GarbageCollectorController.EnableGarbageCollector, }, HPAController: \u0026HPAControllerOptions{ HorizontalPodAutoscalerSyncPeriod: componentConfig.HPAController.HorizontalPodAutoscalerSyncPeriod, HorizontalPodAutoscalerUpscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerUpscaleForbiddenWindow, HorizontalPodAutoscalerDownscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleForbiddenWindow, HorizontalPodAutoscalerDownscaleStabilizationWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleStabilizationWindow, HorizontalPodAutoscalerCPUInitializationPeriod: componentConfig.HPAController.HorizontalPodAutoscalerCPUInitializationPeriod, HorizontalPodAutoscalerInitialReadinessDelay: componentConfig.HPAController.HorizontalPodAutoscalerInitialReadinessDelay, HorizontalPodAutoscalerTolerance: componentConfig.HPAController.HorizontalPodAutoscalerTolerance, HorizontalPodAutoscalerUseRESTClients: componentConfig.HPAController.HorizontalPodAutoscalerUseRESTClients, }, JobController: \u0026JobControllerOptions{ ConcurrentJobSyncs: componentConfig.JobController.ConcurrentJobSyncs, }, NamespaceController: \u0026NamespaceControllerOptions{ NamespaceSyncPeriod: componentConfig.NamespaceController.NamespaceSyncPeriod, ConcurrentNamespaceSyncs: componentConfig.NamespaceController.ConcurrentNamespaceSyncs, }, NodeIPAMController: \u0026NodeIPAMControllerOptions{ NodeCIDRMaskSize: componentConfig.NodeIPAMController.NodeCIDRMaskSize, }, NodeLifecycleController: \u0026NodeLifecycleControllerOptions{ EnableTaintManager: componentConfig.NodeLifecycleController.EnableTaintManager, NodeMonitorGracePeriod: componentConfig.NodeLifecycleController.NodeMonitorGracePeriod, NodeStartupGracePeriod: componentConfig.NodeLifecycleController.NodeStartupGracePeriod, PodEvictionTimeout: componentConfig.NodeLifecycleController.PodEvictionTimeout, }, PersistentVolumeBinderController: \u0026PersistentVolumeBinderControllerOptions{ PVClaimBinderSyncPeriod: componentConfig.PersistentVolumeBinderController.PVClaimBinderSyncPeriod, VolumeConfiguration: componentConfig.PersistentVolumeBinderController.VolumeConfiguration, }, PodGCController: \u0026PodGCControllerOptions{ TerminatedPodGCThreshold: componentConfig.PodGCController.TerminatedPodGCThreshold, }, ReplicaSetController: \u0026ReplicaSetControllerOptions{ ConcurrentRSSyncs: componentConfig.ReplicaSetController.ConcurrentRSSyncs, }, ReplicationController: \u0026ReplicationControllerOptions{ ConcurrentRCSyncs: componentConfig.ReplicationController.ConcurrentRCSyncs, }, ResourceQuotaController: \u0026ResourceQuotaControllerOptions{ ResourceQuotaSyncPeriod: componentConfig.ResourceQuotaController.ResourceQuotaSyncPeriod, ConcurrentResourceQuotaSyncs: componentConfig.ResourceQuotaController.ConcurrentResourceQuotaSyncs, }, SAController: \u0026SAControllerOptions{ ConcurrentSATokenSyncs: componentConfig.SAController.ConcurrentSATokenSyncs, }, ServiceController: \u0026cmoptions.ServiceControllerOptions{ ConcurrentServiceSyncs: componentConfig.ServiceController.ConcurrentServiceSyncs, }, TTLAfterFinishedController: \u0026TTLAfterFinishedControllerOptions{ ConcurrentTTLSyncs: componentConfig.TTLAfterFinishedController.ConcurrentTTLSyncs, }, SecureServing: apiserveroptions.NewSecureServingOptions().WithLoopback(), InsecureServing: (\u0026apiserveroptions.DeprecatedInsecureServingOptions{ BindAddress: net.ParseIP(componentConfig.Generic.Address), BindPort: int(componentConfig.Generic.Port), BindNetwork: \"tcp\", }).WithLoopback(), Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(), Authorization: apiserveroptions.NewDelegatingAuthorizationOptions(), } s.Authentication.RemoteKubeConfigFileOptional = true s.Authorization.RemoteKubeConfigFileOptional = true s.Authorization.AlwaysAllowPaths = []string{\"/healthz\"} s.SecureServing.ServerCert.CertDirectory = \"/var/run/kubernetes\" s.SecureServing.ServerCert.PairName = \"kube-controller-manager\" s.SecureServing.BindPort = ports.KubeControllerManagerPort gcIgnoredResources := make([]kubectrlmgrconfig.GroupResource, 0, len(garbagecollector.DefaultIgnoredResources())) for r := range garbagecollector.DefaultIgnoredResources() { gcIgnoredResources = append(gcIgnoredResources, kubectrlmgrconfig.GroupResource{Group: r.Group, Resource: r.Resource}) } s.GarbageCollectorController.GCIgnoredResources = gcIgnoredResources return \u0026s, nil } 2.2. AddFlagSet 添加参数及帮助函数。\nfs := cmd.Flags() namedFlagSets := s.Flags(KnownControllers(), ControllersDisabledByDefault.List()) for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := apiserverflag.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStderr(), namedFlagSets, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStdout(), namedFlagSets, cols) }) 3. Run 此部分的代码位于cmd/kube-controller-manager/app/controllermanager.go\n基于KubeControllerManagerOptions运行controllerManager，不退出。\n// Run runs the KubeControllerManagerOptions. This should never exit. func Run(c *config.CompletedConfig, stopCh \u003c-chan struct{}) error { ... run := func(ctx context.Context) { ... controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) if err != nil { glog.Fatalf(\"error building controller context: %v\", err) } saTokenControllerInitFunc := serviceAccountTokenControllerStarter{rootClientBuilder: rootClientBuilder}.startServiceAccountTokenController if err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil { glog.Fatalf(\"error starting controllers: %v\", err) } controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) select {} } ... } Run函数涉及的核心代码如下：\n// 创建controller的context controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) // 启动各种controller err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux) 其中StartControllers中的入参NewControllerInitializers初始化了各种controller。\n3.1. CreateControllerContext CreateControllerContext构建了各种controller所需的资源的上下文，各种controller在启动时，入参为该context，具体参考initFn(ctx)。\n// CreateControllerContext creates a context struct containing references to resources needed by the // controllers such as the cloud provider and clientBuilder. rootClientBuilder is only used for // the shared-informers client and token controller. func CreateControllerContext(s *config.CompletedConfig, rootClientBuilder, clientBuilder controller.ControllerClientBuilder, stop \u003c-chan struct{}) (ControllerContext, error) { versionedClient := rootClientBuilder.ClientOrDie(\"shared-informers\") sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)()) // If apiserver is not running we should wait for some time and fail only then. This is particularly // important when we start apiserver and controller manager at the same time. if err := genericcontrollermanager.WaitForAPIServer(versionedClient, 10*time.Second); err != nil { return ControllerContext{}, fmt.Errorf(\"failed to wait for apiserver being healthy: %v\", err) } // Use a discovery client capable of being refreshed. discoveryClient := rootClientBuilder.ClientOrDie(\"controller-discovery\") cachedClient := cacheddiscovery.NewMemCacheClient(discoveryClient.Discovery()) restMapper := restmapper.NewDeferredDiscoveryRESTMapper(cachedClient) go wait.Until(func() { restMapper.Reset() }, 30*time.Second, stop) availableResources, err := GetAvailableResources(rootClientBuilder) if err != nil { return ControllerContext{}, err } cloud, loopMode, err := createCloudProvider(s.ComponentConfig.KubeCloudShared.CloudProvider.Name, s.ComponentConfig.KubeCloudShared.ExternalCloudVolumePlugin, s.ComponentConfig.KubeCloudShared.CloudProvider.CloudConfigFile, s.ComponentConfig.KubeCloudShared.AllowUntaggedCloud, sharedInformers) if err != nil { return ControllerContext{}, err } ctx := ControllerContext{ ClientBuilder: clientBuilder, InformerFactory: sharedInformers, ComponentConfig: s.ComponentConfig, RESTMapper: restMapper, AvailableResources: availableResources, Cloud: cloud, LoopMode: loopMode, Stop: stop, InformersStarted: make(chan struct{}), ResyncPeriod: ResyncPeriod(s), } return ctx, nil } 核心代码为NewSharedInformerFactory。\n// 创建SharedInformerFactory sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)()) // 赋值给ControllerContext ctx := ControllerContext{ InformerFactory: sharedInformers, } SharedInformerFactory提供了公共的k8s对象的informers。\n// SharedInformerFactory provides shared informers for resources in all known // API group versions. type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh \u003c-chan struct{}) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Coordination() coordination.Interface Core() core.Interface Events() events.Interface Extensions() extensions.Interface Networking() networking.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Settings() settings.Interface Storage() storage.Interface } 3.2. NewControllerInitializers NewControllerInitializers定义了各种controller的类型和其对于的启动函数，例如deployment``、statefulset、replicaset、replicationcontroller、namespace等。\n// NewControllerInitializers is a public map of named controller groups (you can start more than one in an init func) // paired to their InitFunc. This allows for structured downstream composition and subdivision. func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { controllers := map[string]InitFunc{} controllers[\"endpoint\"] = startEndpointController controllers[\"replicationcontroller\"] = startReplicationController controllers[\"podgc\"] = startPodGCController controllers[\"resourcequota\"] = startResourceQuotaController controllers[\"namespace\"] = startNamespaceController controllers[\"serviceaccount\"] = startServiceAccountController controllers[\"garbagecollector\"] = startGarbageCollectorController controllers[\"daemonset\"] = startDaemonSetController controllers[\"job\"] = startJobController controllers[\"deployment\"] = startDeploymentController controllers[\"replicaset\"] = startReplicaSetController controllers[\"horizontalpodautoscaling\"] = startHPAController controllers[\"disruption\"] = startDisruptionController controllers[\"statefulset\"] = startStatefulSetController controllers[\"cronjob\"] = startCronJobController controllers[\"csrsigning\"] = startCSRSigningController controllers[\"csrapproving\"] = startCSRApprovingController controllers[\"csrcleaner\"] = startCSRCleanerController controllers[\"ttl\"] = startTTLController controllers[\"bootstrapsigner\"] = startBootstrapSignerController controllers[\"tokencleaner\"] = startTokenCleanerController controllers[\"nodeipam\"] = startNodeIpamController if loopMode == IncludeCloudLoops { controllers[\"service\"] = startServiceController controllers[\"route\"] = startRouteController // TODO: volume controller into the IncludeCloudLoops only set. // TODO: Separate cluster in cloud check from node lifecycle controller. } controllers[\"nodelifecycle\"] = startNodeLifecycleController controllers[\"persistentvolume-binder\"] = startPersistentVolumeBinderController controllers[\"attachdetach\"] = startAttachDetachController controllers[\"persistentvolume-expander\"] = startVolumeExpandController controllers[\"clusterrole-aggregation\"] = startClusterRoleAggregrationController controllers[\"pvc-protection\"] = startPVCProtectionController controllers[\"pv-protection\"] = startPVProtectionController controllers[\"ttl-after-finished\"] = startTTLAfterFinishedController return controllers } 3.3. StartControllers func StartControllers(ctx ControllerContext, startSATokenController InitFunc, controllers map[string]InitFunc, unsecuredMux *mux.PathRecorderMux) error { ... for controllerName, initFn := range controllers { if !ctx.IsControllerEnabled(controllerName) { glog.Warningf(\"%q is disabled\", controllerName) continue } time.Sleep(wait.Jitter(ctx.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter)) glog.V(1).Infof(\"Starting %q\", controllerName) debugHandler, started, err := initFn(ctx) if err != nil { glog.Errorf(\"Error starting %q\", controllerName) return err } if !started { glog.Warningf(\"Skipping %q\", controllerName) continue } if debugHandler != nil \u0026\u0026 unsecuredMux != nil { basePath := \"/debug/controllers/\" + controllerName unsecuredMux.UnlistedHandle(basePath, http.StripPrefix(basePath, debugHandler)) unsecuredMux.UnlistedHandlePrefix(basePath+\"/\", http.StripPrefix(basePath, debugHandler)) } glog.Infof(\"Started %q\", controllerName) } return nil } 核心代码：\nfor controllerName, initFn := range controllers { debugHandler, started, err := initFn(ctx) } 启动各种controller，controller的启动函数在NewControllerInitializers中定义了，例如：\n// deployment controllers[\"deployment\"] = startDeploymentController // statefulset controllers[\"statefulset\"] = startStatefulSetController 3.4. InformerFactory.Start InformerFactory实际上是SharedInformerFactory，具体的实现逻辑在client-go中的informer的实现机制。\ncontrollerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) 3.4.1. SharedInformerFactory SharedInformerFactory是一个informer工厂的接口定义。\n// SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh \u003c-chan struct{}) InformerFor(obj runtime.Object, newFunc NewInformerFunc) cache.SharedIndexInformer } 3.4.2. sharedInformerFactory.Start Start方法初始化各种类型的informer\n// Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh \u003c-chan struct{}) { f.lock.Lock() defer f.lock.Unlock() for informerType, informer := range f.informers { if !f.startedInformers[informerType] { go informer.Run(stopCh) f.startedInformers[informerType] = true } } } 3.4.3. sharedIndexInformer.Run sharedIndexInformer.Run具体运行了sharedIndexInformer的实现逻辑，该部分待后续对informer机制做专题分析。\nfunc (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) cfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) } 4. initFn(ctx) initFn实际调用的就是各种类型的controller，代码位于kubernetes/cmd/kube-controller-manager/app/apps.go，本文以startStatefulSetController和startDeploymentController为例，controller中实际调用的函数逻辑位于kubernetes/pkg/controller中，待后续分析。\n4.1. startStatefulSetController func startStatefulSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"statefulsets\"}] { return nil, false, nil } go statefulset.NewStatefulSetController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Apps().V1().StatefulSets(), ctx.InformerFactory.Core().V1().PersistentVolumeClaims(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.ClientBuilder.ClientOrDie(\"statefulset-controller\"), ).Run(1, ctx.Stop) return nil, true, nil } 其中使用到了InformerFactory，包含了Pods、StatefulSets、PersistentVolumeClaims、ControllerRevisions的informer。\nstartStatefulSetController主要调用的函数为NewStatefulSetController和对应的Run函数。\n4.2. startDeploymentController func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"}] { return nil, false, nil } dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil } startDeploymentController主要调用的函数为NewDeploymentController和对应的Run函数。该部分逻辑在kubernetes/pkg/controller中。\n5. 总结 Kube-controller-manager的代码风格仍然是Cobra命令行框架。通过构造ControllerManagerCommand，然后执行command.Execute()函数。基本的流程就是构造option，添加Flags，执行Run函数。 cmd部分的调用流程如下：Main--\u003eNewControllerManagerCommand--\u003e Run(c.Complete(), wait.NeverStop)--\u003eStartControllers--\u003einitFn(ctx)--\u003estartDeploymentController/startStatefulSetController--\u003ests.NewStatefulSetController.Run/dc.NewDeploymentController.Run--\u003epkg/controller。 其中CreateControllerContext函数用来创建各类型controller所需要使用的context，NewControllerInitializers初始化了各种类型的controller，其中就包括DeploymentController和StatefulSetController等。 基本流程如下：\n构造controller manager option，并转化为Config对象，执行Run函数。 基于Config对象创建ControllerContext，其中包含InformerFactory。 基于ControllerContext运行各种controller，各种controller的定义在NewControllerInitializers中。 执行InformerFactory.Start。 每种controller都会构造自身的结构体并执行对应的Run函数。 参考：\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-controller-manager https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/controller-manager.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/controllermanager.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/apps.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分 …","ref":"/k8s-source-code-analysis/kube-controller-manager/newcontrollermanagercommand/","tags":["源码分析"],"title":"kube-controller-manager源码分析（一）之 NewControllerManagerCommand"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析 https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kubelet 部分的代码。\n本文主要分析 kubernetes/cmd/kubelet部分，该部分主要涉及kubelet的参数解析，及初始化和构造相关的依赖组件（主要在kubeDeps结构体中），并没有kubelet运行的详细逻辑，该部分位于kubernetes/pkg/kubelet模块，待后续文章分析。\nkubelet的cmd代码目录结构如下：\nkubelet ├── app │ ├── auth.go │ ├── init_others.go │ ├── init_windows.go │ ├── options # 包括kubelet使用到的option │ │ ├── container_runtime.go │ │ ├── globalflags.go │ │ ├── globalflags_linux.go │ │ ├── globalflags_other.go │ │ ├── options.go # 包括KubeletFlags、AddFlags、AddKubeletConfigFlags等 │ │ ├── osflags_others.go │ │ └── osflags_windows.go │ ├── plugins.go │ ├── server.go # 包括NewKubeletCommand、Run、RunKubelet、CreateAndInitKubelet、startKubelet等 │ ├── server_linux.go │ └── server_unsupported.go └── kubelet.go # kubelet的main入口函数 1. Main 函数 kubelet的入口函数 Main 函数，具体代码参考：https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kubelet/kubelet.go。\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewKubeletCommand(server.SetupSignalHandler()) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } kubelet代码主要采用了Cobra命令行框架，核心代码如下：\n// 初始化命令行 command := app.NewKubeletCommand(server.SetupSignalHandler()) // 执行Execute err := command.Execute() 2. NewKubeletCommand NewKubeletCommand基于参数创建了一个*cobra.Command对象。其中核心部分代码为参数解析部分和Run函数。\n// NewKubeletCommand creates a *cobra.Command object with default parameters func NewKubeletCommand(stopCh \u003c-chan struct{}) *cobra.Command { ... cmd := \u0026cobra.Command{ Use: componentKubelet, Long: `...`, // The Kubelet has special flag parsing requirements to enforce flag precedence rules, // so we do all our parsing manually in Run, below. // DisableFlagParsing=true provides the full set of flags passed to the kubelet in the // `args` arg to Run, without Cobra's interference. DisableFlagParsing: true, Run: func(cmd *cobra.Command, args []string) { ... // run the kubelet glog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil { glog.Fatal(err) } }, } ... return cmd } 2.1. 参数解析 kubelet开启了DisableFlagParsing参数，没有使用Cobra框架中的默认参数解析，而是自定义参数解析。\n2.1.1. 初始化参数和配置 初始化参数解析，初始化cleanFlagSet，kubeletFlags，kubeletConfig。\ncleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() 2.1.2. 打印帮助信息和版本信息 如果输入非法参数则打印使用帮助信息。\n// initial flag parse, since we disable cobra's flag parsing if err := cleanFlagSet.Parse(args); err != nil { cmd.Usage() glog.Fatal(err) } // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) \u003e 0 { cmd.Usage() glog.Fatalf(\"unknown command: %s\", cmds[0]) } 遇到help和version参数则打印相关内容并退出。\n// short-circuit on help help, err := cleanFlagSet.GetBool(\"help\") if err != nil { glog.Fatal(`\"help\" flag is non-bool, programmer error, please correct`) } if help { cmd.Help() return } // short-circuit on verflag verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cleanFlagSet) 2.1.3. kubelet config 加载并校验kubelet config。其中包括校验初始化的kubeletFlags，并从kubeletFlags的KubeletConfigFile参数获取kubelet config的内容。\n// set feature gates from initial flags-based config if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } // validate the initial KubeletFlags if err := options.ValidateKubeletFlags(kubeletFlags); err != nil { glog.Fatal(err) } if kubeletFlags.ContainerRuntime == \"remote\" \u0026\u0026 cleanFlagSet.Changed(\"pod-infra-container-image\") { glog.Warning(\"Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead\") } // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) \u003e 0 { kubeletConfig, err = loadConfigFile(configFile) if err != nil { glog.Fatal(err) } // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil { glog.Fatal(err) } // update feature gates based on new config if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } } // We always validate the local configuration (command line + config file). // This is the default \"last-known-good\" config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil { glog.Fatal(err) } 2.1.4. dynamic kubelet config 如果开启使用动态kubelet的配置，则由动态配置文件替换kubelet配置文件。\n// use dynamic kubelet config, if enabled var kubeletConfigController *dynamickubeletconfig.Controller if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) \u003e 0 { var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir, func(kc *kubeletconfiginternal.KubeletConfiguration) error { // Here, we enforce flag precedence inside the controller, prior to the controller's validation sequence, // so that we get a complete validation at the same point where we can decide to reject dynamic config. // This fixes the flag-precedence component of issue #63305. // See issue #56171 for general details on flag precedence. return kubeletConfigFlagPrecedence(kc, args) }) if err != nil { glog.Fatal(err) } // If we should just use our existing, local config, the controller will return a nil config if dynamicKubeletConfig != nil { kubeletConfig = dynamicKubeletConfig // Note: flag precedence was already enforced in the controller, prior to validation, // by our above transform function. Now we simply update feature gates from the new config. if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } } } 总结：以上通过对各种特定参数的解析，最终生成kubeletFlags和kubeletConfig两个重要的参数对象，用来构造kubeletServer和其他需求。\n2.2. 初始化kubeletServer和kubeletDeps 2.2.1. kubeletServer // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := \u0026options.KubeletServer{ KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, } 2.2.2. kubeletDeps // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer) if err != nil { glog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController 2.2.3. docker shim 如果开启了docker shim参数，则执行RunDockershim。\n// start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(\u0026kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { glog.Fatal(err) } return } 2.3. AddFlags // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) // ugly, but necessary, because Cobra's default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = \"Usage:\\n %s\\n\\nFlags:\\n%s\" cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) }) 其中：\nAddFlags代码可参考：kubernetes/cmd/kubelet/app/options/options.go#L323 AddKubeletConfigFlags代码可参考：kubernetes/cmd/kubelet/app/options/options.go#L424 2.4. 运行kubelet 运行kubelet并且不退出。由Run函数进入后续的操作。\n// run the kubelet glog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil { glog.Fatal(err) } 3. Run // Run runs the specified KubeletServer with the given Dependencies. This should never exit. // The kubeDeps argument may be nil - if so, it is initialized from the settings on KubeletServer. // Otherwise, the caller is assumed to have set up the Dependencies object and a default one will // not be generated. func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh \u003c-chan struct{}) error { // To help debugging, immediately log version glog.Infof(\"Version: %+v\", version.Get()) if err := initForOS(s.KubeletFlags.WindowsService); err != nil { return fmt.Errorf(\"failed OS init: %v\", err) } if err := run(s, kubeDeps, stopCh); err != nil { return fmt.Errorf(\"failed to run Kubelet: %v\", err) } return nil } 当运行环境是Windows的时候，初始化操作，但是该操作为空，只是预留。具体执行run(s, kubeDeps, stopCh)函数。\n3.1. 构造kubeDeps 3.1.1. clientConfig 创建clientConfig，该对象用来创建各种的kubeDeps属性中包含的client。\nclientConfig, err := createAPIServerClientConfig(s) if err != nil { return fmt.Errorf(\"invalid kubeconfig: %v\", err) } 3.1.2. kubeClient kubeClient, err = clientset.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"New kubeClient from clientConfig error: %v\", err) } else if kubeClient.CertificatesV1beta1() != nil \u0026\u0026 clientCertificateManager != nil { glog.V(2).Info(\"Starting client certificate rotation.\") clientCertificateManager.SetCertificateSigningRequestClient(kubeClient.CertificatesV1beta1().CertificateSigningRequests()) clientCertificateManager.Start() } 3.1.3. dynamicKubeClient dynamicKubeClient, err = dynamic.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"Failed to initialize dynamic KubeClient: %v\", err) } 3.1.4. eventClient // make a separate client for events eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) eventClient, err = v1core.NewForConfig(\u0026eventClientConfig) if err != nil { glog.Warningf(\"Failed to create API Server client for Events: %v\", err) } 3.1.5. heartbeatClient // make a separate client for heartbeat with throttling disabled and a timeout attached heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration // if the NodeLease feature is enabled, the timeout is the minimum of the lease duration and status update frequency if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout \u003e leaseTimeout { heartbeatClientConfig.Timeout = leaseTimeout } } heartbeatClientConfig.QPS = float32(-1) heartbeatClient, err = clientset.NewForConfig(\u0026heartbeatClientConfig) if err != nil { glog.Warningf(\"Failed to create API Server client for heartbeat: %v\", err) } 3.1.6. csiClient // csiClient works with CRDs that support json only clientConfig.ContentType = \"application/json\" csiClient, err := csiclientset.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"Failed to create CSI API client: %v\", err) } client赋值\nkubeDeps.KubeClient = kubeClient kubeDeps.DynamicKubeClient = dynamicKubeClient if heartbeatClient != nil { kubeDeps.HeartbeatClient = heartbeatClient kubeDeps.OnHeartbeatFailure = closeAllConns } if eventClient != nil { kubeDeps.EventClient = eventClient } kubeDeps.CSIClient = csiClient 3.1.7. CAdvisorInterface if kubeDeps.CAdvisorInterface == nil { imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil { return err } } 3.1.8. ContainerManager if kubeDeps.ContainerManager == nil { if s.CgroupsPerQOS \u0026\u0026 s.CgroupRoot == \"\" { glog.Infof(\"--cgroups-per-qos enabled, but --cgroup-root was not specified. defaulting to /\") s.CgroupRoot = \"/\" } kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil { return err } systemReserved, err := parseResourceList(s.SystemReserved) if err != nil { return err } var hardEvictionThresholds []evictionapi.Threshold // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here. if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold { hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil) if err != nil { return err } } experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil { return err } devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig{ RuntimeCgroupsName: s.RuntimeCgroups, SystemCgroupsName: s.SystemCgroups, KubeletCgroupsName: s.KubeletCgroups, ContainerRuntime: s.ContainerRuntime, CgroupsPerQOS: s.CgroupsPerQOS, CgroupRoot: s.CgroupRoot, CgroupDriver: s.CgroupDriver, KubeletRootDir: s.RootDirectory, ProtectKernelDefaults: s.ProtectKernelDefaults, NodeAllocatableConfig: cm.NodeAllocatableConfig{ KubeReservedCgroupName: s.KubeReservedCgroup, SystemReservedCgroupName: s.SystemReservedCgroup, EnforceNodeAllocatable: sets.NewString(s.EnforceNodeAllocatable...), KubeReserved: kubeReserved, SystemReserved: systemReserved, HardEvictionThresholds: hardEvictionThresholds, }, QOSReserved: *experimentalQOSReserved, ExperimentalCPUManagerPolicy: s.CPUManagerPolicy, ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration, ExperimentalPodPidsLimit: s.PodPidsLimit, EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, }, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) if err != nil { return err } } 3.1.9. oomAdjuster // TODO(vmarmol): Do this through container config. oomAdjuster := kubeDeps.OOMAdjuster if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil { glog.Warning(err) } 3.2. Health check if s.HealthzPort \u003e 0 { healthz.DefaultHealthz() go wait.Until(func() { err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil) if err != nil { glog.Errorf(\"Starting health server failed: %v\", err) } }, 5*time.Second, wait.NeverStop) } 3.3. RunKubelet 通过各种赋值构造了完整的kubeDeps结构体，最后再执行RunKubelet转入后续的kubelet执行流程。\nif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil { return err } 4. RunKubelet // RunKubelet is responsible for setting up and running a kubelet. It is used in three different applications: // 1 Integration tests // 2 Kubelet binary // 3 Standalone 'kubernetes' binary // Eventually, #2 will be replaced with instances of #3 func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(\u0026kubeServer.KubeletConfiguration, ... kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } // NewMainKubelet should have set up a pod source config if one didn't exist // when the builder was run. This is just a precaution. if kubeDeps.PodConfig == nil { return fmt.Errorf(\"failed to create kubelet, pod source config was nil\") } podCfg := kubeDeps.PodConfig rlimit.RlimitNumFiles(uint64(kubeServer.MaxOpenFiles)) // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } glog.Infof(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, \u0026kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(\"Started kubelet\") } return nil } RunKubelet函数核心代码为执行了CreateAndInitKubelet和startKubelet两个函数的操作，以下对这两个函数进行分析。\n4.1. CreateAndInitKubelet 通过传入kubeDeps调用CreateAndInitKubelet初始化Kubelet。\nk, err := CreateAndInitKubelet(\u0026kubeServer.KubeletConfiguration, kubeDeps, \u0026kubeServer.ContainerRuntimeOptions, kubeServer.ContainerRuntime, kubeServer.RuntimeCgroups, kubeServer.HostnameOverride, kubeServer.NodeIP, kubeServer.ProviderID, kubeServer.CloudProvider, kubeServer.CertDirectory, kubeServer.RootDirectory, kubeServer.RegisterNode, kubeServer.RegisterWithTaints, kubeServer.AllowedUnsafeSysctls, kubeServer.RemoteRuntimeEndpoint, kubeServer.RemoteImageEndpoint, kubeServer.ExperimentalMounterPath, kubeServer.ExperimentalKernelMemcgNotification, kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount, kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold, kubeServer.MinimumGCAge, kubeServer.MaxPerPodContainerCount, kubeServer.MaxContainerCount, kubeServer.MasterServiceNamespace, kubeServer.RegisterSchedulable, kubeServer.NonMasqueradeCIDR, kubeServer.KeepTerminatedPodVolumes, kubeServer.NodeLabels, kubeServer.SeccompProfileRoot, kubeServer.BootstrapCheckpointPath, kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } 4.1.1. NewMainKubelet CreateAndInitKubelet方法中执行的核心函数是NewMainKubelet，NewMainKubelet实例化一个kubelet对象，该部分的具体代码在kubernetes/pkg/kubelet中，具体参考：kubernetes/pkg/kubelet/kubelet.go#L325。\nfunc CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ... nodeStatusMaxImages int32) (k kubelet.Bootstrap, err error) { // TODO: block until all sources have delivered at least one update to the channel, or break the sync loop // up into \"per source\" synchronizations k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions, containerRuntime, runtimeCgroups, hostnameOverride, nodeIP, providerID, cloudProvider, certDirectory, rootDirectory, registerNode, registerWithTaints, allowedUnsafeSysctls, remoteRuntimeEndpoint, remoteImageEndpoint, experimentalMounterPath, experimentalKernelMemcgNotification, experimentalCheckNodeCapabilitiesBeforeMount, experimentalNodeAllocatableIgnoreEvictionThreshold, minimumGCAge, maxPerPodContainerCount, maxContainerCount, masterServiceNamespace, registerSchedulable, nonMasqueradeCIDR, keepTerminatedPodVolumes, nodeLabels, seccompProfileRoot, bootstrapCheckpointPath, nodeStatusMaxImages) if err != nil { return nil, err } k.BirthCry() k.StartGarbageCollection() return k, nil } 4.1.2. PodConfig if kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil { return nil, err } } NewMainKubelet--\u003ePodConfig--\u003eNewPodConfig--\u003ekubetypes.PodUpdate。会生成一个podUpdate的channel来监听pod的变化，该channel会在k.Run(podCfg.Updates())中作为关键入参。\n4.2. startKubelet // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } glog.Infof(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, \u0026kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(\"Started kubelet\") } 如果设置了只运行一次的参数，则执行k.RunOnce，否则执行核心函数startKubelet。具体实现如下：\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) // start the kubelet server if enableServer { go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) } if kubeCfg.ReadOnlyPort \u003e 0 { go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } } 4.2.1. k.Run // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) 通过长驻进程的方式运行k.Run，不退出，将kubelet的运行逻辑引入kubernetes/pkg/kubelet/kubelet.go部分，kubernetes/pkg/kubelet部分的运行逻辑待后续文章分析。\n5. 总结 kubelet采用Cobra命令行框架和pflag参数解析框架，和apiserver、scheduler、controller-manager形成统一的代码风格。\nkubernetes/cmd/kubelet部分主要对运行参数进行定义和解析，初始化和构造相关的依赖组件（主要在kubeDeps结构体中），并没有kubelet运行的详细逻辑，该部分位于kubernetes/pkg/kubelet模块。\ncmd部分调用流程如下：Main--\u003eNewKubeletCommand--\u003eRun(kubeletServer, kubeletDeps, stopCh)--\u003erun(s *options.KubeletServer, kubeDeps ..., stopCh ...)--\u003e RunKubelet(s, kubeDeps, s.RunOnce)--\u003estartKubelet--\u003ek.Run(podCfg.Updates())--\u003epkg/kubelet。\n同时RunKubelet(s, kubeDeps, s.RunOnce)--\u003eCreateAndInitKubelet--\u003ekubelet.NewMainKubelet--\u003epkg/kubelet。\n参考文章：\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0 ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。 …","ref":"/k8s-source-code-analysis/kubelet/newkubeletcommand/","tags":["源码分析"],"title":"kubelet源码分析（一）之 NewKubeletCommand"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\nscheduler的cmd代码目录结构如下：\nkube-scheduler ├── BUILD ├── OWNERS ├── app # app的目录下主要为运行scheduler相关的对象 │ ├── BUILD │ ├── config │ │ ├── BUILD │ │ └── config.go # Scheduler的配置对象config │ ├── options # options主要记录 Scheduler 使用到的参数 │ │ ├── BUILD │ │ ├── configfile.go │ │ ├── deprecated.go │ │ ├── deprecated_test.go │ │ ├── insecure_serving.go │ │ ├── insecure_serving_test.go │ │ ├── options.go # 主要包括Options、NewOptions、AddFlags、Config等函数 │ │ └── options_test.go │ └── server.go # 主要包括 NewSchedulerCommand、NewSchedulerConfig、Run等函数 └── scheduler.go # main入口函数 1. Main函数 此部分的代码为/cmd/kube-scheduler/scheduler.go\nkube-scheduler的入口函数Main函数，仍然是采用统一的代码风格，使用Cobra命令行框架。\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewSchedulerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 核心代码：\n// 初始化scheduler命令结构体 command := app.NewSchedulerCommand() // 执行Execute err := command.Execute() 2. NewSchedulerCommand 此部分的代码为/cmd/kube-scheduler/app/server.go\nNewSchedulerCommand主要用来构造和初始化SchedulerCommand结构体，\n// NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { opts, err := options.NewOptions() if err != nil { glog.Fatalf(\"unable to initialize command options: %v\", err) } cmd := \u0026cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) if len(args) != 0 { fmt.Fprint(os.Stderr, \"arguments are not supported\\n\") } if errs := opts.Validate(); len(errs) \u003e 0 { fmt.Fprintf(os.Stderr, \"%v\\n\", utilerrors.NewAggregate(errs)) os.Exit(1) } if len(opts.WriteConfigTo) \u003e 0 { if err := options.WriteConfigFile(opts.WriteConfigTo, \u0026opts.ComponentConfig); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } glog.Infof(\"Wrote configuration to: %s\\n\", opts.WriteConfigTo) return } c, err := opts.Config() if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } stopCh := make(chan struct{}) if err := Run(c.Complete(), stopCh); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } opts.AddFlags(cmd.Flags()) cmd.MarkFlagFilename(\"config\", \"yaml\", \"yml\", \"json\") return cmd } 核心代码：\n// 构造option opts, err := options.NewOptions() // 初始化config对象 c, err := opts.Config() // 执行run函数 err := Run(c.Complete(), stopCh) // 添加参数 opts.AddFlags(cmd.Flags()) 2.1. NewOptions NewOptions主要用来构造SchedulerServer使用的参数和上下文，其中核心参数是KubeSchedulerConfiguration。\nopts, err := options.NewOptions() NewOptions:\n// NewOptions returns default scheduler app options. func NewOptions() (*Options, error) { cfg, err := newDefaultComponentConfig() if err != nil { return nil, err } hhost, hport, err := splitHostIntPort(cfg.HealthzBindAddress) if err != nil { return nil, err } o := \u0026Options{ ComponentConfig: *cfg, SecureServing: nil, // TODO: enable with apiserveroptions.NewSecureServingOptions() CombinedInsecureServing: \u0026CombinedInsecureServingOptions{ Healthz: \u0026apiserveroptions.DeprecatedInsecureServingOptions{ BindNetwork: \"tcp\", }, Metrics: \u0026apiserveroptions.DeprecatedInsecureServingOptions{ BindNetwork: \"tcp\", }, BindPort: hport, BindAddress: hhost, }, Authentication: nil, // TODO: enable with apiserveroptions.NewDelegatingAuthenticationOptions() Authorization: nil, // TODO: enable with apiserveroptions.NewDelegatingAuthorizationOptions() Deprecated: \u0026DeprecatedOptions{ UseLegacyPolicyConfig: false, PolicyConfigMapNamespace: metav1.NamespaceSystem, }, } return o, nil } 2.2. Options.Config Config初始化调度器的配置对象。\nc, err := opts.Config() Config函数主要执行以下操作：\n构建scheduler client、leaderElectionClient、eventClient。 创建event recorder 设置leader选举 创建informer对象，主要函数有NewSharedInformerFactory和NewPodInformer。 Config具体代码如下：\n// Config return a scheduler config object func (o *Options) Config() (*schedulerappconfig.Config, error) { c := \u0026schedulerappconfig.Config{} if err := o.ApplyTo(c); err != nil { return nil, err } // prepare kube clients. client, leaderElectionClient, eventClient, err := createClients(c.ComponentConfig.ClientConnection, o.Master, c.ComponentConfig.LeaderElection.RenewDeadline.Duration) if err != nil { return nil, err } // Prepare event clients. eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, corev1.EventSource{Component: c.ComponentConfig.SchedulerName}) // Set up leader election if enabled. var leaderElectionConfig *leaderelection.LeaderElectionConfig if c.ComponentConfig.LeaderElection.LeaderElect { leaderElectionConfig, err = makeLeaderElectionConfig(c.ComponentConfig.LeaderElection, leaderElectionClient, recorder) if err != nil { return nil, err } } c.Client = client c.InformerFactory = informers.NewSharedInformerFactory(client, 0) c.PodInformer = factory.NewPodInformer(client, 0) c.EventClient = eventClient c.Recorder = recorder c.Broadcaster = eventBroadcaster c.LeaderElection = leaderElectionConfig return c, nil } 2.3. AddFlags AddFlags为SchedulerServer添加指定的参数。\nopts.AddFlags(cmd.Flags()) AddFlags函数的具体代码如下：\n// AddFlags adds flags for the scheduler options. func (o *Options) AddFlags(fs *pflag.FlagSet) { fs.StringVar(\u0026o.ConfigFile, \"config\", o.ConfigFile, \"The path to the configuration file. Flags override values in this file.\") fs.StringVar(\u0026o.WriteConfigTo, \"write-config-to\", o.WriteConfigTo, \"If set, write the configuration values to this file and exit.\") fs.StringVar(\u0026o.Master, \"master\", o.Master, \"The address of the Kubernetes API server (overrides any value in kubeconfig)\") o.SecureServing.AddFlags(fs) o.CombinedInsecureServing.AddFlags(fs) o.Authentication.AddFlags(fs) o.Authorization.AddFlags(fs) o.Deprecated.AddFlags(fs, \u0026o.ComponentConfig) leaderelectionconfig.BindFlags(\u0026o.ComponentConfig.LeaderElection.LeaderElectionConfiguration, fs) utilfeature.DefaultFeatureGate.AddFlag(fs) } 3. Run 此部分的代码为/cmd/kube-scheduler/app/server.go\nerr := Run(c.Complete(), stopCh) Run运行一个不退出的常驻进程，来执行scheduler的相关操作。\nRun函数的主要内容如下：\n通过scheduler config来创建scheduler的结构体。 运行event broadcaster、healthz server、metrics server。 运行所有的informer并在调度前等待cache的同步（重点）。 执行sched.Run()来运行scheduler的调度逻辑。 如果多个scheduler并开启了LeaderElect，则执行leader选举。 以下对重点代码分开分析：\n3.1. NewSchedulerConfig NewSchedulerConfig初始化SchedulerConfig（此部分具体逻辑待后续专门分析），最后初始化生成scheduler结构体。\n// Build a scheduler config from the provided algorithm source. schedulerConfig, err := NewSchedulerConfig(c) if err != nil { return err } // Create the scheduler. sched := scheduler.NewFromConfig(schedulerConfig) 3.2. InformerFactory.Start 运行PodInformer，并运行InformerFactory。此部分的逻辑为client-go的informer机制，在Informer机制中有详细分析。\n// Start all informers. go c.PodInformer.Informer().Run(stopCh) c.InformerFactory.Start(stopCh) 3.3. WaitForCacheSync 在调度前等待cache同步。\n// Wait for all caches to sync before scheduling. c.InformerFactory.WaitForCacheSync(stopCh) controller.WaitForCacheSync(\"scheduler\", stopCh, c.PodInformer.Informer().HasSynced) 3.3.1. InformerFactory.WaitForCacheSync InformerFactory.WaitForCacheSync等待所有启动的informer的cache进行同步，保持本地的store信息与etcd的信息是最新一致的。\n// WaitForCacheSync waits for all started informers' cache were synced. func (f *sharedInformerFactory) WaitForCacheSync(stopCh \u003c-chan struct{}) map[reflect.Type]bool { informers := func() map[reflect.Type]cache.SharedIndexInformer { f.lock.Lock() defer f.lock.Unlock() informers := map[reflect.Type]cache.SharedIndexInformer{} for informerType, informer := range f.informers { if f.startedInformers[informerType] { informers[informerType] = informer } } return informers }() res := map[reflect.Type]bool{} for informType, informer := range informers { res[informType] = cache.WaitForCacheSync(stopCh, informer.HasSynced) } return res } 接着调用 cache.WaitForCacheSync。\n// WaitForCacheSync waits for caches to populate. It returns true if it was successful, false // if the controller should shutdown func WaitForCacheSync(stopCh \u003c-chan struct{}, cacheSyncs ...InformerSynced) bool { err := wait.PollUntil(syncedPollPeriod, func() (bool, error) { for _, syncFunc := range cacheSyncs { if !syncFunc() { return false, nil } } return true, nil }, stopCh) if err != nil { glog.V(2).Infof(\"stop requested\") return false } glog.V(4).Infof(\"caches populated\") return true } 3.3.2. controller.WaitForCacheSync controller.WaitForCacheSync是对cache.WaitForCacheSync的一层封装，通过不同的controller的名字来记录不同controller等待cache同步。\ncontroller.WaitForCacheSync(\"scheduler\", stop, s.PodInformer.Informer().HasSynced) controller.WaitForCacheSync具体代码如下：\n// WaitForCacheSync is a wrapper around cache.WaitForCacheSync that generates log messages // indicating that the controller identified by controllerName is waiting for syncs, followed by // either a successful or failed sync. func WaitForCacheSync(controllerName string, stopCh \u003c-chan struct{}, cacheSyncs ...cache.InformerSynced) bool { glog.Infof(\"Waiting for caches to sync for %s controller\", controllerName) if !cache.WaitForCacheSync(stopCh, cacheSyncs...) { utilruntime.HandleError(fmt.Errorf(\"Unable to sync caches for %s controller\", controllerName)) return false } glog.Infof(\"Caches are synced for %s controller\", controllerName) return true } 3.4. LeaderElection 如果有多个scheduler，并开启leader选举，则运行LeaderElector直到选举结束或退出。\n// If leader election is enabled, run via LeaderElector until done and exit. if c.LeaderElection != nil { c.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: run, OnStoppedLeading: func() { utilruntime.HandleError(fmt.Errorf(\"lost master\")) }, } leaderElector, err := leaderelection.NewLeaderElector(*c.LeaderElection) if err != nil { return fmt.Errorf(\"couldn't create leader elector: %v\", err) } leaderElector.Run(ctx) return fmt.Errorf(\"lost lease\") } 3.5. Scheduler.Run // Prepare a reusable run function. run := func(ctx context.Context) { sched.Run() \u003c-ctx.Done() } ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here defer cancel() go func() { select { case \u003c-stopCh: cancel() case \u003c-ctx.Done(): } }() ... run(ctx) Scheduler.Run先等待cache同步，然后开启调度逻辑的goroutine。\nScheduler.Run的具体代码如下：\n// Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 以上是对/cmd/kube-scheduler/scheduler.go部分代码的分析，Scheduler.Run后续的具体代码位于pkg/scheduler/scheduler.go待后续文章分析。\n参考：\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-scheduler https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-scheduler/app/server.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\nscheduler的cmd代码目录结构如下：\nkube-scheduler …","ref":"/k8s-source-code-analysis/kube-scheduler/newschedulercommand/","tags":["源码分析"],"title":"kube-scheduler源码分析（一）之 NewSchedulerCommand"},{"body":" 如果要开发一个Dynamic Provisioner，需要使用到the helper library。\n1. Dynamic Provisioner 1.1. Provisioner Interface 开发Dynamic Provisioner需要实现Provisioner接口，该接口有两个方法，分别是：\nProvision：创建存储资源，并且返回一个PV对象。 Delete：移除对应的存储资源，但并没有删除PV对象。 Provisioner 接口源码如下：\n// Provisioner is an interface that creates templates for PersistentVolumes // and can create the volume as a new resource in the infrastructure provider. // It can also remove the volume it created from the underlying storage // provider. type Provisioner interface { // Provision creates a volume i.e. the storage asset and returns a PV object // for the volume Provision(VolumeOptions) (*v1.PersistentVolume, error) // Delete removes the storage asset that was created by Provision backing the // given PV. Does not delete the PV object itself. // // May return IgnoredError to indicate that the call has been ignored and no // action taken. Delete(*v1.PersistentVolume) error } 1.2. VolumeOptions Provisioner接口的Provision方法的入参是一个VolumeOptions对象。VolumeOptions对象包含了创建PV对象所需要的信息，例如：PV的回收策略，PV的名字，PV所对应的PVC对象以及PVC的StorageClass对象使用的参数等。\nVolumeOptions 源码如下：\n// VolumeOptions contains option information about a volume // https://github.com/kubernetes/kubernetes/blob/release-1.4/pkg/volume/plugins.go type VolumeOptions struct { // Reclamation policy for a persistent volume PersistentVolumeReclaimPolicy v1.PersistentVolumeReclaimPolicy // PV.Name of the appropriate PersistentVolume. Used to generate cloud // volume name. PVName string // PV mount options. Not validated - mount of the PVs will simply fail if one is invalid. MountOptions []string // PVC is reference to the claim that lead to provisioning of a new PV. // Provisioners *must* create a PV that would be matched by this PVC, // i.e. with required capacity, accessMode, labels matching PVC.Selector and // so on. PVC *v1.PersistentVolumeClaim // Volume provisioning parameters from StorageClass Parameters map[string]string // Node selected by the scheduler for the volume. SelectedNode *v1.Node // Topology constraint parameter from StorageClass AllowedTopologies []v1.TopologySelectorTerm } 1.3. ProvisionController ProvisionController是一个给PVC提供PV的控制器，具体执行Provisioner接口的Provision和Delete的方法的所有逻辑。\n1.4. 开发provisioner的步骤 写一个provisioner实现Provisioner接口（包含Provision和Delete的方法）。 通过该provisioner构建ProvisionController。 执行ProvisionController的Run方法。 2. NFS Client Provisioner nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。\nPV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上） PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上） 以下通过nfs-client-provisioner的源码分析来说明开发自定义provisioner整个过程。nfs-client-provisioner的主要代码都在provisioner.go的文件中。\nnfs-client-provisioner源码地址：https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client\n2.1. Main函数 2.1.1. 读取环境变量 源码如下：\nfunc main() { flag.Parse() flag.Set(\"logtostderr\", \"true\") server := os.Getenv(\"NFS_SERVER\") if server == \"\" { glog.Fatal(\"NFS_SERVER not set\") } path := os.Getenv(\"NFS_PATH\") if path == \"\" { glog.Fatal(\"NFS_PATH not set\") } provisionerName := os.Getenv(provisionerNameKey) if provisionerName == \"\" { glog.Fatalf(\"environment variable %s is not set! Please set it.\", provisionerNameKey) } ... } main函数先获取NFS_SERVER、NFS_PATH、PROVISIONER_NAME三个环境变量的值，因此在部署nfs-client-provisioner的时候，需要将这三个环境变量的值传入。\nNFS_SERVER：NFS服务端的IP地址。 NFS_PATH：NFS服务端设置的共享目录 PROVISIONER_NAME：provisioner的名字，需要和StorageClass对象中的provisioner字段一致。 例如StorageClass对象的yaml文件如下：\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. 2.1.2. 获取clientset对象 源码如下：\n// Create an InClusterConfig and use it to create a client for the controller // to use to communicate with Kubernetes config, err := rest.InClusterConfig() if err != nil { glog.Fatalf(\"Failed to create config: %v\", err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create client: %v\", err) } 通过读取对应的k8s的配置，创建clientset对象，用来执行k8s对应的API，其中主要包括对PV和PVC等对象的创建删除等操作。\n2.1.3. 构造nfsProvisioner对象 源码如下：\n// The controller needs to know what the server version is because out-of-tree // provisioners aren't officially supported until 1.5 serverVersion, err := clientset.Discovery().ServerVersion() if err != nil { glog.Fatalf(\"Error getting server version: %v\", err) } clientNFSProvisioner := \u0026nfsProvisioner{ client: clientset, server: server, path: path, } 通过clientset、server、path等值构造nfsProvisioner对象，同时还获取了k8s的版本信息，因为provisioners的功能在k8s 1.5及以上版本才支持。\nnfsProvisioner类型定义如下：\ntype nfsProvisioner struct { client kubernetes.Interface server string path string } var _ controller.Provisioner = \u0026nfsProvisioner{} nfsProvisioner是一个自定义的provisioner，用来实现Provisioner的接口，其中的属性除了server、path这两个关于NFS相关的参数，还包含了client，主要用来调用k8s的API。\nvar _ controller.Provisioner = \u0026nfsProvisioner{} 以上用法用来检测nfsProvisioner是否实现了Provisioner的接口。\n2.1.4. 构建并运行ProvisionController 源码如下：\n// Start the provision controller which will dynamically provision efs NFS // PVs pc := controller.NewProvisionController(clientset, provisionerName, clientNFSProvisioner, serverVersion.GitVersion) pc.Run(wait.NeverStop) 通过nfsProvisioner构造ProvisionController对象并执行Run方法，ProvisionController实现了具体的PV和PVC的相关逻辑，Run方法以常驻进程的方式运行。\n2.2. Provision和Delete方法 2.2.1. Provision方法 nfsProvisioner的Provision方法具体源码参考：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/cmd/nfs-client-provisioner/provisioner.go#L56\nProvision方法用来创建存储资源，并且返回一个PV对象。其中入参是VolumeOptions，用来指定PV对象的相关属性。\n1、构建PV和PVC的名称\nfunc (p *nfsProvisioner) Provision(options controller.VolumeOptions) (*v1.PersistentVolume, error) { if options.PVC.Spec.Selector != nil { return nil, fmt.Errorf(\"claim Selector is not supported\") } glog.V(4).Infof(\"nfs provisioner: VolumeOptions %v\", options) pvcNamespace := options.PVC.Namespace pvcName := options.PVC.Name pvName := strings.Join([]string{pvcNamespace, pvcName, options.PVName}, \"-\") fullPath := filepath.Join(mountPath, pvName) glog.V(4).Infof(\"creating path %s\", fullPath) if err := os.MkdirAll(fullPath, 0777); err != nil { return nil, errors.New(\"unable to create directory to provision new pv: \" + err.Error()) } os.Chmod(fullPath, 0777) path := filepath.Join(p.path, pvName) ... } 通过VolumeOptions的入参，构建PV和PVC的名称，以及创建路径path。\n2、构造PV对象\npv := \u0026v1.PersistentVolume{ ObjectMeta: metav1.ObjectMeta{ Name: options.PVName, }, Spec: v1.PersistentVolumeSpec{ PersistentVolumeReclaimPolicy: options.PersistentVolumeReclaimPolicy, AccessModes: options.PVC.Spec.AccessModes, MountOptions: options.MountOptions, Capacity: v1.ResourceList{ v1.ResourceName(v1.ResourceStorage): options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)], }, PersistentVolumeSource: v1.PersistentVolumeSource{ NFS: \u0026v1.NFSVolumeSource{ Server: p.server, Path: path, ReadOnly: false, }, }, }, } return pv, nil 综上可以看出，Provision方法只是通过VolumeOptions参数来构建PV对象，并没有执行具体PV的创建或删除的操作。\n不同类型的Provisioner的，一般是PersistentVolumeSource类型和参数不同，例如nfs-provisioner对应的PersistentVolumeSource为NFS，并且需要传入NFS相关的参数：Server，Path等。\n2.2.2. Delete方法 nfsProvisioner的delete方法具体源码参考：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/cmd/nfs-client-provisioner/provisioner.go#L99\n1、获取pvName和path等相关参数\nfunc (p *nfsProvisioner) Delete(volume *v1.PersistentVolume) error { path := volume.Spec.PersistentVolumeSource.NFS.Path pvName := filepath.Base(path) oldPath := filepath.Join(mountPath, pvName) if _, err := os.Stat(oldPath); os.IsNotExist(err) { glog.Warningf(\"path %s does not exist, deletion skipped\", oldPath) return nil } ... } 通过path和pvName生成oldPath，其中oldPath是原先NFS服务器上pod对应的数据持久化存储路径。\n2、获取archiveOnDelete参数并删除数据\n// Get the storage class for this volume. storageClass, err := p.getClassForVolume(volume) if err != nil { return err } // Determine if the \"archiveOnDelete\" parameter exists. // If it exists and has a falsey value, delete the directory. // Otherwise, archive it. archiveOnDelete, exists := storageClass.Parameters[\"archiveOnDelete\"] if exists { archiveBool, err := strconv.ParseBool(archiveOnDelete) if err != nil { return err } if !archiveBool { return os.RemoveAll(oldPath) } } 如果storageClass对象中指定archiveOnDelete参数并且值为false，则会自动删除oldPath下的所有数据，即pod对应的数据持久化存储数据。\narchiveOnDelete字面意思为删除时是否存档，false表示不存档，即删除数据，true表示存档，即重命名路径。\n3、重命名旧数据路径\narchivePath := filepath.Join(mountPath, \"archived-\"+pvName) glog.V(4).Infof(\"archiving path %s to %s\", oldPath, archivePath) return os.Rename(oldPath, archivePath) 如果storageClass对象中没有指定archiveOnDelete参数或者值为true，表明需要删除时存档，即将oldPath重命名，命名格式为oldPath前面增加archived-的前缀。\n3. ProvisionController 3.1. ProvisionController结构体 源码具体参考：https://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/controller.go#L82\nProvisionController是一个给PVC提供PV的控制器，具体执行Provisioner接口的Provision和Delete的方法的所有逻辑。\n3.1.1. 入参 // ProvisionController is a controller that provisions PersistentVolumes for // PersistentVolumeClaims. type ProvisionController struct { client kubernetes.Interface // The name of the provisioner for which this controller dynamically // provisions volumes. The value of annDynamicallyProvisioned and // annStorageProvisioner to set \u0026 watch for, respectively provisionerName string // The provisioner the controller will use to provision and delete volumes. // Presumably this implementer of Provisioner carries its own // volume-specific options and such that it needs in order to provision // volumes. provisioner Provisioner // Kubernetes cluster server version: // * 1.4: storage classes introduced as beta. Technically out-of-tree dynamic // provisioning is not officially supported, though it works // * 1.5: storage classes stay in beta. Out-of-tree dynamic provisioning is // officially supported // * 1.6: storage classes enter GA kubeVersion *utilversion.Version ... } client、provisionerName、provisioner、kubeVersion等属性作为NewProvisionController的入参。\nclient：clientset客户端，用来调用k8s的API。 provisionerName：provisioner的名字，需要和StorageClass对象中的provisioner字段一致。 provisioner：具体的provisioner的实现者，本文为nfsProvisioner。 kubeVersion：k8s的版本信息。 3.1.2. Controller和Informer type ProvisionController struct { ... claimInformer cache.SharedInformer claims cache.Store claimController cache.Controller volumeInformer cache.SharedInformer volumes cache.Store volumeController cache.Controller classInformer cache.SharedInformer classes cache.Store classController cache.Controller ... } ProvisionController结构体中包含了PV、PVC、StorageClass三个对象的Controller、Informer和Store，主要用来执行这三个对象的相关操作。\nController：通用的控制框架 Informer：消息通知器 Store：通用的对象存储接口 3.1.3. workqueue type ProvisionController struct { ... claimQueue workqueue.RateLimitingInterface volumeQueue workqueue.RateLimitingInterface ... } claimQueue和volumeQueue分别是PV和PVC的任务队列。\n3.1.4. 其他 // Identity of this controller, generated at creation time and not persisted // across restarts. Useful only for debugging, for seeing the source of // events. controller.provisioner may have its own, different notion of // identity which may/may not persist across restarts id string component string eventRecorder record.EventRecorder resyncPeriod time.Duration exponentialBackOffOnError bool threadiness int createProvisionedPVRetryCount int createProvisionedPVInterval time.Duration failedProvisionThreshold, failedDeleteThreshold int // The port for metrics server to serve on. metricsPort int32 // The IP address for metrics server to serve on. metricsAddress string // The path of metrics endpoint path. metricsPath string // Parameters of leaderelection.LeaderElectionConfig. leaseDuration, renewDeadline, retryPeriod time.Duration hasRun bool hasRunLock *sync.Mutex 3.2. NewProvisionController方法 源码地址：https://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/controller.go#L418\nNewProvisionController方法主要用来构造ProvisionController。\n3.2.1. 初始化默认值 // NewProvisionController creates a new provision controller using // the given configuration parameters and with private (non-shared) informers. func NewProvisionController( client kubernetes.Interface, provisionerName string, provisioner Provisioner, kubeVersion string, options ...func(*ProvisionController) error, ) *ProvisionController { ... controller := \u0026ProvisionController{ client: client, provisionerName: provisionerName, provisioner: provisioner, kubeVersion: utilversion.MustParseSemantic(kubeVersion), id: id, component: component, eventRecorder: eventRecorder, resyncPeriod: DefaultResyncPeriod, exponentialBackOffOnError: DefaultExponentialBackOffOnError, threadiness: DefaultThreadiness, createProvisionedPVRetryCount: DefaultCreateProvisionedPVRetryCount, createProvisionedPVInterval: DefaultCreateProvisionedPVInterval, failedProvisionThreshold: DefaultFailedProvisionThreshold, failedDeleteThreshold: DefaultFailedDeleteThreshold, leaseDuration: DefaultLeaseDuration, renewDeadline: DefaultRenewDeadline, retryPeriod: DefaultRetryPeriod, metricsPort: DefaultMetricsPort, metricsAddress: DefaultMetricsAddress, metricsPath: DefaultMetricsPath, hasRun: false, hasRunLock: \u0026sync.Mutex{}, } ... } 3.2.2. 初始化任务队列 ratelimiter := workqueue.NewMaxOfRateLimiter( workqueue.NewItemExponentialFailureRateLimiter(15*time.Second, 1000*time.Second), \u0026workqueue.BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) if !controller.exponentialBackOffOnError { ratelimiter = workqueue.NewMaxOfRateLimiter( workqueue.NewItemExponentialFailureRateLimiter(15*time.Second, 15*time.Second), \u0026workqueue.BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) } controller.claimQueue = workqueue.NewNamedRateLimitingQueue(ratelimiter, \"claims\") controller.volumeQueue = workqueue.NewNamedRateLimitingQueue(ratelimiter, \"volumes\") 3.2.3. ListWatch // PVC claimSource := \u0026cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { return client.CoreV1().PersistentVolumeClaims(v1.NamespaceAll).List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { return client.CoreV1().PersistentVolumeClaims(v1.NamespaceAll).Watch(options) }, } // PV volumeSource := \u0026cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { return client.CoreV1().PersistentVolumes().List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { return client.CoreV1().PersistentVolumes().Watch(options) }, } // StorageClass classSource = \u0026cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { return client.StorageV1().StorageClasses().List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { return client.StorageV1().StorageClasses().Watch(options) }, } list-watch机制是k8s中用来监听对象变化的核心机制，ListWatch包含ListFunc和WatchFunc两个函数，且不能为空，以上代码分别构造了PV、PVC、StorageClass三个对象的ListWatch结构体。该机制的实现在client-go的cache包中，具体参考：https://godoc.org/k8s.io/client-go/tools/cache。\n更多ListWatch代码如下:\n具体参考：https://github.com/kubernetes-incubator/external-storage/blob/89b0aaf6413b249b37834b124fc314ef7b8ee949/vendor/k8s.io/client-go/tools/cache/listwatch.go#L34\n// ListerWatcher is any object that knows how to perform an initial list and start a watch on a resource. type ListerWatcher interface { // List should return a list type object; the Items field will be extracted, and the // ResourceVersion field will be used to start the watch in the right place. List(options metav1.ListOptions) (runtime.Object, error) // Watch should begin a watch at the specified version. Watch(options metav1.ListOptions) (watch.Interface, error) } // ListFunc knows how to list resources type ListFunc func(options metav1.ListOptions) (runtime.Object, error) // WatchFunc knows how to watch resources type WatchFunc func(options metav1.ListOptions) (watch.Interface, error) // ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface. // It is a convenience function for users of NewReflector, etc. // ListFunc and WatchFunc must not be nil type ListWatch struct { ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool } 3.2.4. ResourceEventHandlerFuncs // PVC claimHandler := cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { controller.enqueueWork(controller.claimQueue, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.claimQueue, newObj) }, DeleteFunc: func(obj interface{}) { controller.forgetWork(controller.claimQueue, obj) }, } // PV volumeHandler := cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { controller.enqueueWork(controller.volumeQueue, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.volumeQueue, newObj) }, DeleteFunc: func(obj interface{}) { controller.forgetWork(controller.volumeQueue, obj) }, } // StorageClass classHandler := cache.ResourceEventHandlerFuncs{ // We don't need an actual event handler for StorageClasses, // but we must pass a non-nil one to cache.NewInformer() AddFunc: nil, UpdateFunc: nil, DeleteFunc: nil, } ResourceEventHandlerFuncs是资源事件处理函数，主要用来对k8s资源对象增删改变化的事件进行消息通知，该函数实现了ResourceEventHandler的接口。具体代码逻辑在client-go的cache包中。\n更多ResourceEventHandlerFuncs代码可参考：\n// ResourceEventHandler can handle notifications for events that happen to a // resource. The events are informational only, so you can't return an // error. // * OnAdd is called when an object is added. // * OnUpdate is called when an object is modified. Note that oldObj is the // last known state of the object-- it is possible that several changes // were combined together, so you can't use this to see every single // change. OnUpdate is also called when a re-list happens, and it will // get called even if nothing changed. This is useful for periodically // evaluating or syncing something. // * OnDelete will get the final state of the item if it is known, otherwise // it will get an object of type DeletedFinalStateUnknown. This can // happen if the watch is closed and misses the delete event and we don't // notice the deletion until the subsequent re-list. type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } // ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or // as few of the notification functions as you want while still implementing // ResourceEventHandler. type ResourceEventHandlerFuncs struct { AddFunc func(obj interface{}) UpdateFunc func(oldObj, newObj interface{}) DeleteFunc func(obj interface{}) } 3.2.5. 构造Store和Controller 1、PVC\nif controller.claimInformer != nil { controller.claimInformer.AddEventHandlerWithResyncPeriod(claimHandler, controller.resyncPeriod) controller.claims, controller.claimController = controller.claimInformer.GetStore(), controller.claimInformer.GetController() } else { controller.claims, controller.claimController = cache.NewInformer( claimSource, \u0026v1.PersistentVolumeClaim{}, controller.resyncPeriod, claimHandler, ) } 2、PV\nif controller.volumeInformer != nil { controller.volumeInformer.AddEventHandlerWithResyncPeriod(volumeHandler, controller.resyncPeriod) controller.volumes, controller.volumeController = controller.volumeInformer.GetStore(), controller.volumeInformer.GetController() } else { controller.volumes, controller.volumeController = cache.NewInformer( volumeSource, \u0026v1.PersistentVolume{}, controller.resyncPeriod, volumeHandler, ) } 3、StorageClass\nif controller.classInformer != nil { // no resource event handler needed for StorageClasses controller.classes, controller.classController = controller.classInformer.GetStore(), controller.classInformer.GetController() } else { controller.classes, controller.classController = cache.NewInformer( classSource, versionedClassType, controller.resyncPeriod, classHandler, ) } 通过cache.NewInformer的方法构造，入参是ListWatch结构体和ResourceEventHandlerFuncs函数等，返回值是Store和Controller。\n通过以上各个部分的构造，最后返回一个具体的ProvisionController对象。\n3.3. ProvisionController.Run方法 ProvisionController的Run方法是以常驻进程的方式运行，函数内部再运行其他的controller。\n3.3.1. prometheus数据收集 // Run starts all of this controller's control loops func (ctrl *ProvisionController) Run(stopCh \u003c-chan struct{}) { run := func(stopCh \u003c-chan struct{}) { ... if ctrl.metricsPort \u003e 0 { prometheus.MustRegister([]prometheus.Collector{ metrics.PersistentVolumeClaimProvisionTotal, metrics.PersistentVolumeClaimProvisionFailedTotal, metrics.PersistentVolumeClaimProvisionDurationSeconds, metrics.PersistentVolumeDeleteTotal, metrics.PersistentVolumeDeleteFailedTotal, metrics.PersistentVolumeDeleteDurationSeconds, }...) http.Handle(ctrl.metricsPath, promhttp.Handler()) address := net.JoinHostPort(ctrl.metricsAddress, strconv.FormatInt(int64(ctrl.metricsPort), 10)) glog.Infof(\"Starting metrics server at %s\\n\", address) go wait.Forever(func() { err := http.ListenAndServe(address, nil) if err != nil { glog.Errorf(\"Failed to listen on %s: %v\", address, err) } }, 5*time.Second) } ... } 3.3.2. Controller.Run // If a SharedInformer has been passed in, this controller should not // call Run again if ctrl.claimInformer == nil { go ctrl.claimController.Run(stopCh) } if ctrl.volumeInformer == nil { go ctrl.volumeController.Run(stopCh) } if ctrl.classInformer == nil { go ctrl.classController.Run(stopCh) } 运行消息通知器Informer。\n3.3.3. Worker for i := 0; i \u003c ctrl.threadiness; i++ { go wait.Until(ctrl.runClaimWorker, time.Second, stopCh) go wait.Until(ctrl.runVolumeWorker, time.Second, stopCh) } runClaimWorker和runVolumeWorker分别为PVC和PV的worker，这两个的具体执行体分别是processNextClaimWorkItem和processNextVolumeWorkItem。\n执行流程如下：\nPVC的函数调用流程\nrunClaimWorker→processNextClaimWorkItem→syncClaimHandler→syncClaim→provisionClaimOperation PV的函数调用流程\nrunVolumeWorker→processNextVolumeWorkItem→syncVolumeHandler→syncVolume→deleteVolumeOperation 可见最后执行的函数分别是provisionClaimOperation和deleteVolumeOperation。\n3.4. Operation 3.4.1. provisionClaimOperation 1、provisionClaimOperation入参是PVC，通过PVC获得PV对象，并判断PV对象是否存在，如果存在则退出后续操作。\n// provisionClaimOperation attempts to provision a volume for the given claim. // Returns error, which indicates whether provisioning should be retried // (requeue the claim) or not func (ctrl *ProvisionController) provisionClaimOperation(claim *v1.PersistentVolumeClaim) error { // Most code here is identical to that found in controller.go of kube's PV controller... claimClass := helper.GetPersistentVolumeClaimClass(claim) operation := fmt.Sprintf(\"provision %q class %q\", claimToClaimKey(claim), claimClass) glog.Infof(logOperation(operation, \"started\")) // A previous doProvisionClaim may just have finished while we were waiting for // the locks. Check that PV (with deterministic name) hasn't been provisioned // yet. pvName := ctrl.getProvisionedVolumeNameForClaim(claim) volume, err := ctrl.client.CoreV1().PersistentVolumes().Get(pvName, metav1.GetOptions{}) if err == nil \u0026\u0026 volume != nil { // Volume has been already provisioned, nothing to do. glog.Infof(logOperation(operation, \"persistentvolume %q already exists, skipping\", pvName)) return nil } ... } 2、获取StorageClass对象中的Provisioner和ReclaimPolicy参数，如果provisionerName和StorageClass对象中的provisioner字段不一致则报错并退出执行。\nprovisioner, parameters, err := ctrl.getStorageClassFields(claimClass) if err != nil { glog.Errorf(logOperation(operation, \"error getting claim's StorageClass's fields: %v\", err)) return nil } if provisioner != ctrl.provisionerName { // class.Provisioner has either changed since shouldProvision() or // annDynamicallyProvisioned contains different provisioner than // class.Provisioner. glog.Errorf(logOperation(operation, \"unknown provisioner %q requested in claim's StorageClass\", provisioner)) return nil } // Check if this provisioner can provision this claim. if err = ctrl.canProvision(claim); err != nil { ctrl.eventRecorder.Event(claim, v1.EventTypeWarning, \"ProvisioningFailed\", err.Error()) glog.Errorf(logOperation(operation, \"failed to provision volume: %v\", err)) return nil } reclaimPolicy := v1.PersistentVolumeReclaimDelete if ctrl.kubeVersion.AtLeast(utilversion.MustParseSemantic(\"v1.8.0\")) { reclaimPolicy, err = ctrl.fetchReclaimPolicy(claimClass) if err != nil { return err } } 3、执行具体的provisioner.Provision方法，构建PV对象，例如本文中的provisioner是nfs-provisioner。\noptions := VolumeOptions{ PersistentVolumeReclaimPolicy: reclaimPolicy, PVName: pvName, PVC: claim, MountOptions: mountOptions, Parameters: parameters, SelectedNode: selectedNode, AllowedTopologies: allowedTopologies, } ctrl.eventRecorder.Event(claim, v1.EventTypeNormal, \"Provisioning\", fmt.Sprintf(\"External provisioner is provisioning volume for claim %q\", claimToClaimKey(claim))) volume, err = ctrl.provisioner.Provision(options) if err != nil { if ierr, ok := err.(*IgnoredError); ok { // Provision ignored, do nothing and hope another provisioner will provision it. glog.Infof(logOperation(operation, \"volume provision ignored: %v\", ierr)) return nil } err = fmt.Errorf(\"failed to provision volume with StorageClass %q: %v\", claimClass, err) ctrl.eventRecorder.Event(claim, v1.EventTypeWarning, \"ProvisioningFailed\", err.Error()) return err } 4、创建k8s的PV对象。\n// Try to create the PV object several times for i := 0; i \u003c ctrl.createProvisionedPVRetryCount; i++ { glog.Infof(logOperation(operation, \"trying to save persistentvvolume %q\", volume.Name)) if _, err = ctrl.client.CoreV1().PersistentVolumes().Create(volume); err == nil || apierrs.IsAlreadyExists(err) { // Save succeeded. if err != nil { glog.Infof(logOperation(operation, \"persistentvolume %q already exists, reusing\", volume.Name)) err = nil } else { glog.Infof(logOperation(operation, \"persistentvolume %q saved\", volume.Name)) } break } // Save failed, try again after a while. glog.Infof(logOperation(operation, \"failed to save persistentvolume %q: %v\", volume.Name, err)) time.Sleep(ctrl.createProvisionedPVInterval) } 5、创建PV失败，清理存储资源。\nif err != nil { // Save failed. Now we have a storage asset outside of Kubernetes, // but we don't have appropriate PV object for it. // Emit some event here and try to delete the storage asset several // times. ... for i := 0; i \u003c ctrl.createProvisionedPVRetryCount; i++ { if err = ctrl.provisioner.Delete(volume); err == nil { // Delete succeeded glog.Infof(logOperation(operation, \"cleaning volume %q succeeded\", volume.Name)) break } // Delete failed, try again after a while. glog.Infof(logOperation(operation, \"failed to clean volume %q: %v\", volume.Name, err)) time.Sleep(ctrl.createProvisionedPVInterval) } if err != nil { // Delete failed several times. There is an orphaned volume and there // is nothing we can do about it. strerr := fmt.Sprintf(\"Error cleaning provisioned volume for claim %s: %v. Please delete manually.\", claimToClaimKey(claim), err) glog.Error(logOperation(operation, strerr)) ctrl.eventRecorder.Event(claim, v1.EventTypeWarning, \"ProvisioningCleanupFailed\", strerr) } } 如果创建成功，则打印成功的日志，并返回nil。\n3.4.2. deleteVolumeOperation 1、deleteVolumeOperation入参是PV，先获得PV对象，并判断是否需要删除。\n// deleteVolumeOperation attempts to delete the volume backing the given // volume. Returns error, which indicates whether deletion should be retried // (requeue the volume) or not func (ctrl *ProvisionController) deleteVolumeOperation(volume *v1.PersistentVolume) error { ... // This method may have been waiting for a volume lock for some time. // Our check does not have to be as sophisticated as PV controller's, we can // trust that the PV controller has set the PV to Released/Failed and it's // ours to delete newVolume, err := ctrl.client.CoreV1().PersistentVolumes().Get(volume.Name, metav1.GetOptions{}) if err != nil { return nil } if !ctrl.shouldDelete(newVolume) { glog.Infof(logOperation(operation, \"persistentvolume no longer needs deletion, skipping\")) return nil } ... } 2、调用具体的provisioner的Delete方法，例如，如果是nfs-provisioner，则是调用nfs-provisioner的Delete方法。\nerr = ctrl.provisioner.Delete(volume) if err != nil { if ierr, ok := err.(*IgnoredError); ok { // Delete ignored, do nothing and hope another provisioner will delete it. glog.Infof(logOperation(operation, \"volume deletion ignored: %v\", ierr)) return nil } // Delete failed, emit an event. glog.Errorf(logOperation(operation, \"volume deletion failed: %v\", err)) ctrl.eventRecorder.Event(volume, v1.EventTypeWarning, \"VolumeFailedDelete\", err.Error()) return err } 3、删除k8s中的PV对象。\n// Delete the volume if err = ctrl.client.CoreV1().PersistentVolumes().Delete(volume.Name, nil); err != nil { // Oops, could not delete the volume and therefore the controller will // try to delete the volume again on next update. glog.Infof(logOperation(operation, \"failed to delete persistentvolume: %v\", err)) return err } 4. 总结 Provisioner接口包含Provision和Delete两个方法，自定义的provisioner需要实现这两个方法，这两个方法只是处理了跟存储类型相关的事项，并没有针对PV、PVC对象的增删等操作。 Provision方法主要用来构造PV对象，不同类型的Provisioner的，一般是PersistentVolumeSource类型和参数不同，例如nfs-provisioner对应的PersistentVolumeSource为NFS，并且需要传入NFS相关的参数：Server，Path等。 Delete方法主要针对对应的存储类型，做数据存档（备份）或删除的处理。 StorageClass对象需要单独创建，用来指定具体的provisioner来执行相关逻辑。 provisionClaimOperation和deleteVolumeOperation具体执行了k8s中PV对象的创建和删除操作，同时调用了具体provisioner的Provision和Delete两个方法来对存储数据做处理。 参考文章\nhttps://github.com/kubernetes-incubator/external-storage/tree/master/docs/demo/hostpath-provisioner https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client https://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/controller.go https://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/volume.go ","categories":"","description":"","excerpt":" 如果要开发一个Dynamic Provisioner，需要使用到the helper library。\n1. Dynamic …","ref":"/kubernetes-notes/develop/csi/nfs-client-provisioner/","tags":["源码分析"],"title":"nfs-client-provisioner源码分析"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/openyurt/","tags":"","title":"OpenYurt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/develop/operator/","tags":"","title":"operator开发"},{"body":"1. PV概述 PersistentVolume（简称PV） 是 Volume 之类的卷插件，也是集群中的资源，但独立于Pod的生命周期（即不会因Pod删除而被删除），不归属于某个Namespace。\n2. PV和PVC的生命周期 2.1. 配置（Provision） 有两种方式来配置 PV：静态或动态。\n1、静态\n手动创建PV，可供k8s集群中的对象消费。\n2、动态\n可以通过StorageClass和具体的Provisioner（例如nfs-client-provisioner）来动态地创建和删除PV。\n2.2. 绑定 在动态配置的情况下，用户创建了特定的PVC，k8s会监听新的PVC，并寻找匹配的PV绑定。一旦绑定后，这种绑定是排他性的，PVC和PV的绑定是一对一的映射。\n2.3. 使用 Pod 使用PVC作为卷。集群检查PVC以查找绑定的卷并为集群挂载该卷。用户通过在 Pod 的 volume 配置中包含 persistentVolumeClaim 来调度 Pod 并访问用户声明的 PV。\n2.4. 回收 PV的回收策略可以设定PVC在释放后如何处理对应的Volume，目前有 Retained， Recycled 和 Deleted三种策略。\n1、保留（Retain）\n保留策略允许手动回收资源，当删除PVC的时候，PV仍然存在，可以通过以下步骤回收卷：\n删除PV 手动清理外部存储的数据资源 手动删除或重新使用关联的存储资产 2、回收（Resycle）\n该策略已废弃，推荐使用dynamic provisioning\n回收策略会在 volume上执行基本擦除（rm -rf / thevolume / *），可被再次声明使用。\n3、删除（Delete）\n删除策略，当发生删除操作的时候，会从k8s集群中删除PV对象，并执行外部存储资源的删除操作（根据不同的provisioner定义的删除逻辑不同，有的是重命名）。\n动态配置的卷继承其StorageClass的回收策略，默认为Delete，即当用户删除PVC的时候，会自动执行PV的删除策略。\n如果要修改PV的回收策略，可执行以下命令：\n# Get pv kubectl get pv # Change policy to Retaion kubectl patch pv \u003cpv_name\u003e -p ‘{“spec”:{“persistentVolumeReclaimPolicy”:“Retain”}}’ 3. PV的类型 PersistentVolume 类型以插件形式实现。以下仅列部分常用类型：\nGCEPersistentDisk AWSElasticBlockStore NFS RBD (Ceph Block Device) CephFS Glusterfs 4. PV的属性 每个 PV 配置中都包含一个 sepc 规格字段和一个 status 卷状态字段。\napiVersion: v1 kind: PersistentVolume metadata: annotations: pv.kubernetes.io/provisioned-by: fuseim.pri/ifs creationTimestamp: 2018-07-12T06:46:48Z name: default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985 resourceVersion: \"100163256\" selfLink: /api/v1/persistentvolumes/default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985 uid: 59796ba3-859f-11e8-9c50-c81f66bcff65 spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi volumeMode: Filesystem claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: test-web-0 namespace: default resourceVersion: \"100163248\" uid: 58cf5ec1-859f-11e8-bb61-005056b83985 nfs: path: /data/nfs-storage/default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985 server: 172.16.201.54 persistentVolumeReclaimPolicy: Delete storageClassName: managed-nfs-storage mountOptions: - hard - nfsvers=4.1 status: phase: Bound 4.1. Capacity 给PV设置特定的存储容量，更多 capacity 可参考Kubernetes 资源模型 。\n4.2. Volume Mode volumeMode 的有效值可以是Filesystem或Block。如果未指定，volumeMode 将默认为Filesystem。\n4.3. Access Modes 访问模式包括：\nReadWriteOnce——该卷可以被单个节点以读/写模式挂载 ReadOnlyMany——该卷可以被多个节点以只读模式挂载 ReadWriteMany——该卷可以被多个节点以读/写模式挂载 在命令行中，访问模式缩写为：\nRWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany 一个卷一次只能使用一种访问模式挂载，即使它支持很多访问模式。\n以下只列举部分常用插件：\nVolume 插件 ReadWriteOnce ReadOnlyMany ReadWriteMany AWSElasticBlockStore ✓ - - CephFS ✓ ✓ ✓ GCEPersistentDisk ✓ ✓ - Glusterfs ✓ ✓ ✓ HostPath ✓ - - NFS ✓ ✓ ✓ RBD ✓ ✓ - ... - 4.4. Class PV可以指定一个StorageClass来动态绑定PV和PVC，其中通过 storageClassName 属性来指定具体的StorageClass，如果没有指定该属性的PV，它只能绑定到不需要特定类的 PVC。\n4.5. Reclaim Policy 回收策略包括：\nRetain（保留）——手动回收 Recycle（回收）——基本擦除（rm -rf /thevolume/*） Delete（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和 OpenStack Cinder 卷）将被删除 当前，只有 NFS 和 HostPath 支持回收策略。AWS EBS、GCE PD、Azure Disk 和 Cinder 卷支持删除策略。\n4.6. Mount Options Kubernetes 管理员可以指定在节点上为挂载持久卷指定挂载选项。\n注意：不是所有的持久化卷类型都支持挂载选项。\n支持挂载选项常用的类型有：\nGCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk NFS RBD （Ceph Block Device） CephFS Cinder （OpenStack 卷存储） Glusterfs 4.7. Phase PV可以处于以下的某种状态：\nAvailable（可用）——一块空闲资源还没有被任何声明绑定 Bound（已绑定）——卷已经被声明绑定 Released（已释放）——声明被删除，但是资源还未被集群重新声明 Failed（失败）——该卷的自动回收失败 命令行会显示绑定到 PV 的 PVC 的名称。\n参考文章：\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/ ","categories":"","description":"","excerpt":"1. PV概述 PersistentVolume（简称PV） 是 Volume 之类的卷插件，也是集群中的资源，但独立于Pod的生命周期（即 …","ref":"/kubernetes-notes/storage/volume/persistent-volume/","tags":["Kubernetes"],"title":"PersistentVolume 介绍"},{"body":"问题描述 节点Pod被驱逐\n原因 1. 查看节点和该节点pod状态 查看节点状态为Ready，查看该节点的所有pod，发现存在被驱逐的pod和nvidia-device-plugin为pending\nroot@host:~$ kgpoallowide |grep 192.168.1.1 department-56 173e397c-ea35-4aac-85d8-07106e55d7b7 0/1 Evicted 0 52d \u003cnone\u003e 192.168.1.1 \u003cnone\u003e kube-system nvidia-device-plugin-daemonset-d58d2 0/1 Pending 0 1s \u003cnone\u003e 192.168.1.1 \u003cnone\u003e 2. 查看对应节点kubelet的日志 0905 15:42:13.182280 23506 eviction_manager.go:142] Failed to admit pod rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432) - node has conditions: [DiskPressure] I0905 15:42:14.827343 23506 kubelet.go:1836] SyncLoop (ADD, \"api\"): \"nvidia-device-plugin-daemonset-88sm6_kube-system(adbd9227-cfb0-11e9-9729-6c92bf5e2432)\" W0905 15:42:14.827372 23506 eviction_manager.go:142] Failed to admit pod nvidia-device-plugin-daemonset-88sm6_kube-system(adbd9227-cfb0-11e9-9729-6c92bf5e2432) - node has conditions: [DiskPressure] I0905 15:42:15.722378 23506 kubelet_node_status.go:607] Update capacity for nvidia.com/gpu-share to 0 I0905 15:42:16.692488 23506 kubelet.go:1852] SyncLoop (DELETE, \"api\"): \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\" W0905 15:42:16.698445 23506 status_manager.go:489] Failed to delete status for pod \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\": pod \"rdma-device-plugin-daemonset-8nwb8\" not found I0905 15:42:16.698490 23506 kubelet.go:1846] SyncLoop (REMOVE, \"api\"): \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\" I0905 15:42:16.699267 23506 kubelet.go:2040] Failed to delete pod \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\", err: pod not found W0905 15:42:16.777355 23506 eviction_manager.go:332] eviction manager: attempting to reclaim nodefs I0905 15:42:16.777384 23506 eviction_manager.go:346] eviction manager: must evict pod(s) to reclaim nodefs E0905 15:42:16.777390 23506 eviction_manager.go:357] eviction manager: eviction thresholds have been met, but no pods are active to evict 存在关于pod驱逐相关的日志，驱逐的原因为node has conditions: [DiskPressure]。\n3. 查看磁盘相关信息 [root@host /]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 126G 0 126G 0% /dev tmpfs 126G 0 126G 0% /dev/shm tmpfs 126G 27M 126G 1% /run tmpfs 126G 0 126G 0% /sys/fs/cgroup /dev/sda1 20G 19G 0 100% / # 根目录磁盘满 /dev/nvme1n1 3.0T 191G 2.8T 7% /data2 /dev/nvme0n1 3.0T 1.3T 1.7T 44% /data1 /dev/sda4 182G 95G 87G 53% /data /dev/sda3 20G 3.8G 15G 20% /usr/local tmpfs 26G 0 26G 0% /run/user/0 发现根目录的磁盘盘，接着查看哪些文件占用磁盘。\n[root@host ~/kata]# du -sh ./* 1.0M\t./log 944K\t./netlink 6.6G\t./kernel3 /var/log/下存在7G 的日志。清理相关日志和无用文件后，根目录恢复空间。\n[root@host /data]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 126G 0 126G 0% /dev tmpfs 126G 0 126G 0% /dev/shm tmpfs 126G 27M 126G 1% /run tmpfs 126G 0 126G 0% /sys/fs/cgroup /dev/sda1 20G 5.8G 13G 32% / # 根目录正常 /dev/nvme1n1 3.0T 191G 2.8T 7% /data2 查看节点pod状态，相关plugin的pod恢复正常。\nroot@host:~$ kgpoallowide |grep 192.168.1.1 kube-system nvidia-device-plugin-daemonset-h4pjc 1/1 Running 0 16m 192.168.1.1 192.168.1.1 \u003cnone\u003e kube-system rdma-device-plugin-daemonset-xlkbv 1/1 Running 0 16m 192.168.1.1 192.168.1.1 \u003cnone\u003e 4. 查看kubelet配置 查看kubelet关于pod驱逐相关的参数配置，可见节点kubelet开启了驱逐机制，正常情况下该配置应该是关闭的。\nExecStart=/usr/local/bin/kubelet \\ ... --eviction-hard=nodefs.available\u003c1% \\ 解决方案 总结以上原因为，kubelet开启了pod驱逐的机制，根目录的磁盘达到100%，pod被驱逐，且无法再正常创建在该节点。\n解决方案如下：\n1、关闭kubelet的驱逐机制。\n2、清除根目录的文件，恢复根目录空间，并后续增加根目录的磁盘监控。\n","categories":"","description":"","excerpt":"问题描述 节点Pod被驱逐\n原因 1. 查看节点和该节点pod状态 查看节点状态为Ready，查看该节点的所有pod，发现存在被驱逐的pod …","ref":"/kubernetes-notes/trouble-shooting/pod-evicted/","tags":["问题排查"],"title":"Pod驱逐"},{"body":"Pod限额（LimitRange） ResourceQuota对象是限制某个namespace下所有Pod(容器)的资源限额\nLimitRange对象是限制某个namespace单个Pod(容器)的资源限额\nLimitRange对象用来定义某个命名空间下某种资源对象的使用限额，其中资源对象包括：Pod、Container、PersistentVolumeClaim。\n1. 为namespace配置CPU和内存的默认值 如果在一个拥有默认内存或CPU限额的命名空间中创建一个容器，并且这个容器未指定它自己的内存或CPU的limit， 它会被分配这个默认的内存或CPU的limit。既没有设置pod的limit和request才会分配默认的内存或CPU的request。\n1.1. namespace的内存默认值 # 创建namespace $ kubectl create namespace default-mem-example # 创建LimitRange $ cat memory-defaults.yaml apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults.yaml --namespace=default-mem-example # 创建Pod,未指定内存的limit和request $ cat memory-defaults-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-mem-demo spec: containers: - name: default-mem-demo-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults-pod.yaml --namespace=default-mem-example # 查看Pod $ kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example containers: - image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi 1.2. namespace的CPU默认值 # 创建namespace $ kubectl create namespace default-cpu-example # 创建LimitRange $ cat cpu-defaults.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-limit-range spec: limits: - default: cpu: 1 defaultRequest: cpu: 0.5 type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-defaults.yaml --namespace=default-cpu-example # 创建Pod，未指定CPU的limit和request $ cat cpu-defaults-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-cpu-demo spec: containers: - name: default-cpu-demo-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-defaults-pod.yaml --namespace=default-cpu-example # 查看Pod $ kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example containers: - image: nginx imagePullPolicy: Always name: default-cpu-demo-ctr resources: limits: cpu: \"1\" requests: cpu: 500m 1.3 说明 如果没有指定pod的request和limit，则创建的pod会使用LimitRange对象定义的默认值（request和limit） 如果指定pod的limit但未指定request，则创建的pod的request值会取limit的值，而不会取LimitRange对象定义的request默认值。 如果指定pod的request但未指定limit，则创建的pod的limit值会取LimitRange对象定义的limit默认值。 默认Limit和request的动机\n如果命名空间具有资源配额（ResourceQuota）, 它为内存限额（CPU限额）设置默认值是有意义的。 以下是资源配额对命名空间施加的两个限制：\n在命名空间运行的每一个容器必须有它自己的内存限额（CPU限额）。 在命名空间中所有的容器使用的内存总量（CPU总量）不能超出指定的限额。 如果一个容器没有指定它自己的内存限额（CPU限额），它将被赋予默认的限额值，然后它才可以在被配额限制的命名空间中运行。\n2. 为namespace配置CPU和内存的最大最小值 2.1. 内存的最大最小值 创建LimitRange\n# 创建namespace $ kubectl create namespace constraints-mem-example # 创建LimitRange $ cat memory-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: mem-min-max-demo-lr spec: limits: - max: memory: 1Gi min: memory: 500Mi type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints.yaml --namespace=constraints-mem-example # 查看LimitRange $ kubectl get limitrange cpu-min-max-demo --namespace=constraints-mem-example --output=yaml ... limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container ... # LimitRange设置了最大最小值，但没有设置默认值，也会被自动设置默认值。 创建符合要求的Pod\n# 创建符合要求的Pod $ cat memory-constraints-pod.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo spec: containers: - name: constraints-mem-demo-ctr image: nginx resources: limits: memory: \"800Mi\" requests: memory: \"600Mi\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod.yaml --namespace=constraints-mem-example # 查看Pod $ kubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example ... resources: limits: memory: 800Mi requests: memory: 600Mi ... 创建超过最大内存limit的pod\n$ cat memory-constraints-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-2 spec: containers: - name: constraints-mem-demo-2-ctr image: nginx resources: limits: memory: \"1.5Gi\" # 超过最大值 1Gi requests: memory: \"800Mi\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-2.yaml --namespace=constraints-mem-example # Pod创建失败，因为容器指定的limit过大 Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/memory-constraints-pod-2.yaml\": pods \"constraints-mem-demo-2\" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi. 创建小于最小内存request的Pod\n$ cat memory-constraints-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-3 spec: containers: - name: constraints-mem-demo-3-ctr image: nginx resources: limits: memory: \"800Mi\" requests: memory: \"100Mi\" # 小于最小值500Mi $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-3.yaml --namespace=constraints-mem-example # Pod创建失败，因为容器指定的内存request过小 Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/memory-constraints-pod-3.yaml\": pods \"constraints-mem-demo-3\" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi. 创建没有指定任何内存limit和request的pod\n$ cat memory-constraints-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-4 spec: containers: - name: constraints-mem-demo-4-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-4.yaml --namespace=constraints-mem-example # 查看Pod $ kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml ... resources: limits: memory: 1Gi requests: memory: 1Gi ... 容器没有指定自己的 CPU 请求和限制，所以它将从 LimitRange 获取默认的 CPU 请求和限制值。\n2.2. CPU的最大最小值 创建LimitRange\n# 创建namespace $ kubectl create namespace constraints-cpu-example # 创建LimitRange $ cat cpu-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits: - max: cpu: \"800m\" min: cpu: \"200m\" type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints.yaml --namespace=constraints-cpu-example # 查看LimitRange $ kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example ... limits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container ... 创建符合要求的Pod\n$ cat cpu-constraints-pod.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo spec: containers: - name: constraints-cpu-demo-ctr image: nginx resources: limits: cpu: \"800m\" requests: cpu: \"500m\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod.yaml --namespace=constraints-cpu-example # 查看Pod $ kubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example ... resources: limits: cpu: 800m requests: cpu: 500m ... 创建超过最大CPU limit的Pod\n$ cat cpu-constraints-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-2 spec: containers: - name: constraints-cpu-demo-2-ctr image: nginx resources: limits: cpu: \"1.5\" requests: cpu: \"500m\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example # Pod创建失败，因为容器指定的CPU limit过大 Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/cpu-constraints-pod-2.yaml\": pods \"constraints-cpu-demo-2\" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m. 创建小于最小CPU request的Pod\n$ cat cpu-constraints-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-4 spec: containers: - name: constraints-cpu-demo-4-ctr image: nginx resources: limits: cpu: \"800m\" requests: cpu: \"100m\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example # Pod创建失败，因为容器指定的CPU request过小 Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/cpu-constraints-pod-3.yaml\": pods \"constraints-cpu-demo-4\" is forbidden: minimum cpu usage per Container is 200m, but request is 100m. 创建没有指定任何CPU limit和request的pod\n$ cat cpu-constraints-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-4 spec: containers: - name: constraints-cpu-demo-4-ctr image: vish/stress $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example # 查看Pod kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml ... resources: limits: cpu: 800m requests: cpu: 800m ... 容器没有指定自己的 CPU 请求和限制，所以它将从 LimitRange 获取默认的 CPU 请求和限制值。\n2.3. 说明 LimitRange 在 namespace 中施加的最小和最大内存（CPU）限制只有在创建和更新 Pod 时才会被应用。改变 LimitRange 不会对之前创建的 Pod 造成影响。\nKubernetes 都会执行下列步骤：\n如果容器没有指定自己的内存（CPU）请求（request）和限制（limit），系统将会为其分配默认值。 验证容器的内存（CPU）请求大于等于最小值。 验证容器的内存（CPU）限制小于等于最大值。 参考文章：\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/\n","categories":"","description":"","excerpt":"Pod限额（LimitRange） ResourceQuota对象是限制某个namespace下所有Pod(容器) …","ref":"/kubernetes-notes/resource/limit-range/","tags":["Kubernetes"],"title":"资源配额"},{"body":"pvc流程 流程如下：\n用户创建了一个包含 PVC 的 Pod，该 PVC 要求使用动态存储卷； Scheduler 根据 Pod 配置、节点状态、PV 配置等信息，把 Pod 调度到一个合适的 Worker 节点上； PV 控制器 watch 到该 Pod 使用的 PVC 处于 Pending 状态，于是调用 Volume Plugin（in-tree）创建存储卷，并创建 PV 对象（out-of-tree 由 External Provisioner 来处理）； AD 控制器发现 Pod 和 PVC 处于待挂接状态，于是调用 Volume Plugin 挂接存储设备到目标 Worker 节点上 在 Worker 节点上，Kubelet 中的 Volume Manager 等待存储设备挂接完成，并通过 Volume Plugin 将设备挂载到全局目录：/var/lib/kubelet/pods/[pod uid]/volumes/kubernetes.io~iscsi/[PVname]（以 iscsi 为例）； Kubelet 通过 Docker 启动 Pod 的 Containers，用 bind mount 方式将已挂载到本地全局目录的卷映射到容器中。 详细流程图 ","categories":"","description":"","excerpt":"pvc流程 流程如下：\n用户创建了一个包含 PVC 的 Pod，该 PVC 要求使用动态存储卷； Scheduler 根据 Pod 配置、节 …","ref":"/kubernetes-notes/principle/flow/pvc-flow/","tags":["Kubernetes"],"title":"PVC创建流程"},{"body":" 本文主要介绍redfish api的调用路径及格式。\nRedfish是一种基于HTTPs服务的管理标准，利用RESTful接口实现设备管理。可以理解为redfish api就是通过http的调用方式来操作服务器的bmc设备，从而实现对设备的远程控制。\n1. BMC常用功能操作 BIOS 管理\n启动设置（boot order）\n虚拟媒体管理\n电源管理（开机、关机、重启）\n固件升级\n远程控制（VNC/SOL）\n监控和日志\n2. 通用 Redfish API 接口格式 2.1. 通用 Redfish API 接口格式 2.1.1. 基础路径结构 /redfish/v1/ 2.1.2. 主要资源类型 系统资源 (Systems) /redfish/v1/Systems/{systemId}/ ├── Bios/ # BIOS设置 ├── Boot/ # 启动设置 ├── Memory/ # 内存信息 ├── Processors/ # 处理器信息 ├── Storage/ # 存储信息 ├── EthernetInterfaces/ # 网络接口 └── Actions/ # 系统操作 机箱资源 (Chassis) /redfish/v1/Chassis/{chassisId}/ ├── Power/ # 电源管理 ├── Thermal/ # 温度管理 ├── NetworkAdapters/ # 网络适配器 └── Actions/ # 机箱操作 管理控制器 (Managers) /redfish/v1/Managers/{managerId}/ ├── NetworkProtocol/ # 网络协议 ├── VirtualMedia/ # 虚拟媒体 ├── LogServices/ # 日志服务 └── Actions/ # 管理操作 更新服务 (UpdateService) /redfish/v1/UpdateService/ ├── FirmwareInventory/ # 固件清单 └── Actions/ # 更新操作 3. BIOS 管理 1. 通用 BIOS 接口路径 /redfish/v1/Systems/{systemId}/Bios 获取 BIOS 属性\nGET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性\nPATCH /redfish/v1/Systems/{systemId}/Bios/Settings 2. 各厂商路径 厂商 操作 方法 路径 request Dell 获取 BIOS 属性 GET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性 PATCH /redfish/v1/Systems/{systemId}/Bios/Settings {\"Attributes\": attrs,\n\"@Redfish.SettingsApplyTime\": \"OnReset\"} HPE 获取 BIOS 属性 GET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性 PATCH /redfish/v1/Systems/{systemId}/Bios/Settings {\"Attributes\": attrs,\n\"@Redfish.SettingsApplyTime\": \"OnReset\"} HUAWEI 获取 BIOS 属性 GET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性 PATCH /redfish/v1/Systems/{systemId}/Bios/Settings { \"Attributes\": attrs} Inspur 获取 BIOS 属性 GET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性 PATCH Lenovo 获取 BIOS 属性 GET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性 PATCH /redfish/v1/Systems/{systemId}/Bios/Settings { \"Attributes\": attrs} xFusion 获取 BIOS 属性 GET /redfish/v1/Systems/{systemId}/Bios 设置 BIOS 属性 PATCH /redfish/v1/Systems/{systemId}/Bios/Settings { \"Attributes\": attrs} 4. 启动设置（boot order） 5. VirtualMedia 的通用接口路径 1. 通用 VirtualMedia 接口路径 /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/ 获取虚拟媒体信息\nGET /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/ 插入虚拟媒体\nPOST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.InsertMedia 弹出虚拟媒体\nPOST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.EjectMedia 2. 各厂商路径 厂商 操作 方法 路径 request Dell 获取VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/CD 插入VirtualMedia 弹出VirtualMedia HPE 获取VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId} 插入VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.InsertMedia { \"Image\": \"string\", \"Inserted\": boolean, \"WriteProtected\": boolean} 弹出VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.EjectMedia HUAWEI 获取VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/CD 插入VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/Huawei/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Connect\", \"Image\": imageURL} 弹出VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/Huawei/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Disconnect\"} Inspur(M6) 获取VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId} mediaId=CD/CD1 插入VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.InsertMedia {\"Image\": image,\"TransferProtocolType\": \"NFS\"} 弹出VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.EjectMedia Lenovo 获取VirtualMedia GET /redfish/v1/Systems/{systemId}/VirtualMedia/EXT1 插入VirtualMedia 弹出VirtualMedia PATCH /redfish/v1/Systems/{systemId}/VirtualMedia/EXT1 {\"Inserted\": false} xFusion(华为子品牌) 获取VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/CD 插入VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/xFusion/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Connect\", \"Image\": imageURL} 弹出VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/xFusion/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Disconnect\"} 6. 电源管理（开机、关机、重启） 7. 固件升级 ","categories":"","description":"","excerpt":" 本文主要介绍redfish api的调用路径及格式。\nRedfish是一种基于HTTPs服务的管理标准，利用RESTful接口实现设备管 …","ref":"/linux-notes/baremetal/redfish-api/","tags":["裸金属"],"title":"Redfish API"},{"body":"1. Redis部署 以下以Linux系统为例\n1.1 下载和编译 $ wget http://download.redis.io/releases/redis-4.0.7.tar.gz $ tar xzf redis-4.0.7.tar.gz $ cd redis-4.0.7 $ make 编译完成后会在src目录下生成Redis服务端程序redis-server和客户端程序redis-cli。\n1.2 启动服务 1、前台运行\nsrc/redis-server 该方式启动默认为前台方式运行，使用默认配置。\n2、后台运行\n可以修改redis.conf文件的daemonize参数为yes，指定配置文件启动，例如：\nvi redis.conf # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize yes 指定配置文件启动。\nsrc/redis-server redis.conf 例如：\n#指定配置文件后台启动 [root@kube-node-1 redis-4.0.7]# src/redis-server redis.conf 95778:C 30 Jan 00:44:37.633 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 95778:C 30 Jan 00:44:37.634 # Redis version=4.0.7, bits=64, commit=00000000, modified=0, pid=95778, just started 95778:C 30 Jan 00:44:37.634 # Configuration loaded #查看Redis进程 [root@kube-node-1 redis-4.0.7]# ps aux|grep redis root 95779 0.0 0.0 145268 468 ? Ssl 00:44 0:00 src/redis-server 127.0.0.1:6379 更多启动参数如下：\n[root@kube-node-1 src]# ./redis-server --help Usage: ./redis-server [/path/to/redis.conf] [options] ./redis-server - (read config from stdin) ./redis-server -v or --version ./redis-server -h or --help ./redis-server --test-memory \u003cmegabytes\u003e Examples: ./redis-server (run the server with default conf) ./redis-server /etc/redis/6379.conf ./redis-server --port 7777 ./redis-server --port 7777 --slaveof 127.0.0.1 8888 ./redis-server /etc/myredis.conf --loglevel verbose Sentinel mode: ./redis-server /etc/sentinel.conf --sentinel 1.3 客户端测试 $ src/redis-cli redis\u003e set foo bar OK redis\u003e get foo \"bar\" 2. Redis集群部署 Redis的集群部署需要在每台集群部署的机器上安装Redis（可参考上述的[Redis安装] ），然后修改配置以集群的方式启动。\n2.1 手动部署集群 2.1.1 设置配置文件及启动实例 修改配置文件redis.conf，集群模式的最小化配置文件如下：\n#可选操作，该项设置后台方式运行， daemonize yes port 7000 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 appendonly yes 更多集群配置参数可参考默认配置文件redis.conf中Cluster模块的说明\n最小集群模式需要三个master实例，一般建议起六个实例，即三主三从。因此我们创建6个以端口号命名的目录存放实例的配置文件和其他信息。\nmkdir cluster-test cd cluster-test mkdir 7000 7001 7002 7003 7004 7005 在对应端口号的目录中创建redis.conf的文件，配置文件的内容可参考上述的集群模式配置。每个配置文件中的端口号port参数改为对应目录的端口号。\n复制redis-server的二进制文件到cluster-test目录中，通过指定配置文件的方式启动redis服务，例如：\ncd 7000 ../redis-server ./redis.conf 如果是以前台方式运行，则会在控制台输出以下信息：\n[82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1 每个实例都会生成一个Node ID，类似97a3a64667477371c4479320d683e4c8db5858b1，用来作为Redis实例在集群中的唯一标识，而不是通过IP和Port，IP和Port可能会改变，该Node ID不会改变。\n目录结构可参考：\ncluster-test/ ├── 7000 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7001 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7002 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7003 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7004 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7005 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── redis-cli └── redis-server 2.1.2 redis-trib创建集群 Redis的实例全部运行之后，还需要redis-trib.rb工具来完成集群的创建，redis-trib.rb二进制文件在Redis包主目录下的src目录中，运行该工具依赖Ruby环境和gem，因此需要提前安装。\n1、安装Ruby\nyum -y install ruby rubygems 查看Ruby版本信息。\n[root@kube-node-1 src]# ruby --version ruby 2.0.0p648 (2015-12-16) [x86_64-linux] 由于centos系统默认支持Ruby版本为2.0.0，因此执行gem install redis命令时会报以下错误。\n[root@kube-node-1 src]# gem install redis Fetching: redis-4.0.1.gem (100%) ERROR: Error installing redis: redis requires Ruby version \u003e= 2.2.2. 解决方法是先安装rvm，再升级ruby版本。\n2、安装rvm\ncurl -L get.rvm.io | bash -s stable 如果遇到以下报错，则执行报错中的gpg2 --recv-keys 的命令。\n[root@kube-node-1 ~]# curl -L get.rvm.io | bash -s stable % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 194 100 194 0 0 335 0 --:--:-- --:--:-- --:--:-- 335 100 24090 100 24090 0 0 17421 0 0:00:01 0:00:01 --:--:-- 44446 Downloading https://github.com/rvm/rvm/archive/1.29.3.tar.gz Downloading https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc gpg: 于 2017年09月11日 星期一 04时59分21秒 CST 创建的签名，使用 RSA，钥匙号 BF04FF17 gpg: 无法检查签名：没有公钥 Warning, RVM 1.26.0 introduces signed releases and automated check of signatures when GPG software found. Assuming you trust Michal Papis import the mpapis public key (downloading the signatures). GPG signature verification failed for '/usr/local/rvm/archives/rvm-1.29.3.tgz' - 'https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc'! Try to install GPG v2 and then fetch the public key: gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 or if it fails: command curl -sSL https://rvm.io/mpapis.asc | gpg2 --import - the key can be compared with: https://rvm.io/mpapis.asc https://keybase.io/mpapis NOTE: GPG version 2.1.17 have a bug which cause failures during fetching keys from remote server. Please downgrade or upgrade to newer version (if available) or use the second method described above. 执行报错中的gpg2 --recv-keys 的命令。\n例如：\n[root@kube-node-1 ~]# gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 gpg: 钥匙环‘/root/.gnupg/secring.gpg’已建立 gpg: 下载密钥‘D39DC0E3’，从 hkp 服务器 keys.gnupg.net gpg: /root/.gnupg/trustdb.gpg：建立了信任度数据库 gpg: 密钥 D39DC0E3：公钥“Michal Papis (RVM signing) \u003cmpapis@gmail.com\u003e”已导入 gpg: 没有找到任何绝对信任的密钥 gpg: 合计被处理的数量：1 gpg: 已导入：1 (RSA: 1) 再次执行命令curl -L get.rvm.io | bash -s stable。例如：\n[root@kube-node-1 ~]# curl -L get.rvm.io | bash -s stable % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 194 100 194 0 0 310 0 --:--:-- --:--:-- --:--:-- 309 100 24090 100 24090 0 0 18230 0 0:00:01 0:00:01 --:--:-- 103k Downloading https://github.com/rvm/rvm/archive/1.29.3.tar.gz Downloading https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc gpg: 于 2017年09月11日 星期一 04时59分21秒 CST 创建的签名，使用 RSA，钥匙号 BF04FF17 gpg: 完好的签名，来自于“Michal Papis (RVM signing) \u003cmpapis@gmail.com\u003e” gpg: 亦即“Michal Papis \u003cmichal.papis@toptal.com\u003e” gpg: 亦即“[jpeg image of size 5015]” gpg: 警告：这把密钥未经受信任的签名认证！ gpg: 没有证据表明这个签名属于它所声称的持有者。 主钥指纹： 409B 6B17 96C2 7546 2A17 0311 3804 BB82 D39D C0E3 子钥指纹： 62C9 E5F4 DA30 0D94 AC36 166B E206 C29F BF04 FF17 GPG verified '/usr/local/rvm/archives/rvm-1.29.3.tgz' Creating group 'rvm' Installing RVM to /usr/local/rvm/ Installation of RVM in /usr/local/rvm/ is almost complete: * First you need to add all users that will be using rvm to 'rvm' group, and logout - login again, anyone using rvm will be operating with `umask u=rwx,g=rwx,o=rx`. * To start using RVM you need to run `source /etc/profile.d/rvm.sh` in all your open bash windows, in rare cases you need to reopen all bash windows. 以上表示执行成功，\nsource /usr/local/rvm/scripts/rvm 查看rvm库中已知的ruby版本\nrvm list known 例如：\n[root@kube-node-1 ~]# rvm list known # MRI Rubies [ruby-]1.8.6[-p420] [ruby-]1.8.7[-head] # security released on head [ruby-]1.9.1[-p431] [ruby-]1.9.2[-p330] [ruby-]1.9.3[-p551] [ruby-]2.0.0[-p648] [ruby-]2.1[.10] [ruby-]2.2[.7] [ruby-]2.3[.4] [ruby-]2.4[.1] ruby-head ... 3、升级Ruby\n#安装ruby rvm install 2.4.0 #使用新版本 rvm use 2.4.0 #移除旧版本 rvm remove 2.0.0 #查看当前版本 ruby --version 例如：\n[root@kube-node-1 ~]# rvm install 2.4.0 Searching for binary rubies, this might take some time. Found remote file https://rvm_io.global.ssl.fastly.net/binaries/centos/7/x86_64/ruby-2.4.0.tar.bz2 Checking requirements for centos. Installing requirements for centos. Installing required packages: autoconf, automake, bison, bzip2, gcc-c++, libffi-devel, libtool, readline-devel, sqlite-devel, zlib-devel, libyaml-devel, openssl-devel................................ Requirements installation successful. ruby-2.4.0 - #configure ruby-2.4.0 - #download % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 14.0M 100 14.0M 0 0 852k 0 0:00:16 0:00:16 --:--:-- 980k No checksum for downloaded archive, recording checksum in user configuration. ruby-2.4.0 - #validate archive ruby-2.4.0 - #extract ruby-2.4.0 - #validate binary ruby-2.4.0 - #setup ruby-2.4.0 - #gemset created /usr/local/rvm/gems/ruby-2.4.0@global ruby-2.4.0 - #importing gemset /usr/local/rvm/gemsets/global.gems.............................. ruby-2.4.0 - #generating global wrappers........ ruby-2.4.0 - #gemset created /usr/local/rvm/gems/ruby-2.4.0 ruby-2.4.0 - #importing gemsetfile /usr/local/rvm/gemsets/default.gems evaluated to empty gem list ruby-2.4.0 - #generating default wrappers........ [root@kube-node-1 ~]# rvm use 2.4.0 Using /usr/local/rvm/gems/ruby-2.4.0 [root@kube-node-1 ~]# rvm remove 2.0.0 ruby-2.0.0-p648 - #already gone Using /usr/local/rvm/gems/ruby-2.4.0 [root@kube-node-1 ~]# ruby --version ruby 2.4.0p0 (2016-12-24 revision 57164) [x86_64-linux] 4、安装gem\ngem install redis 例如：\n[root@kube-node-1 ~]# gem install redis Fetching: redis-4.0.1.gem (100%) Successfully installed redis-4.0.1 Parsing documentation for redis-4.0.1 Installing ri documentation for redis-4.0.1 Done installing documentation for redis after 2 seconds 1 gem installed 5、执行redis-trib.rb命令\n以上表示安装成功，可以执行redis-trib.rb命令。\ncd src #执行redis-trib.rb命令 ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ \u003e 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 参数create表示创建一个新的集群，--replicas 1表示为每个master创建一个slave。\n如果创建成功会显示以下信息\n[OK] All 16384 slots covered 例如：\n[root@kube-node-1 src]# ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ \u003e 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \u003e\u003e\u003e Creating cluster \u003e\u003e\u003e Performing hash slots allocation on 6 nodes... Using 3 masters: 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 Adding replica 127.0.0.1:7004 to 127.0.0.1:7000 Adding replica 127.0.0.1:7005 to 127.0.0.1:7001 Adding replica 127.0.0.1:7003 to 127.0.0.1:7002 \u003e\u003e\u003e Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000 slots:0-5460 (5461 slots) master M: 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001 slots:5461-10922 (5462 slots) master M: be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002 slots:10923-16383 (5461 slots) master S: 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003 replicates 13d0c397604a0b2644244c37b666fce83f29faa8 S: dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004 replicates be2718476eba4e56f696e56b75e67df720b7fc24 S: 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005 replicates d5a834d075fd93eefab877c6ebb86efff680650f Can I set the above configuration? (type 'yes' to accept): yes \u003e\u003e\u003e Nodes configuration updated \u003e\u003e\u003e Assign a different config epoch to each node \u003e\u003e\u003e Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join.... \u003e\u003e\u003e Performing Cluster Check (using node 127.0.0.1:7000) M: d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s) M: be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003 slots: (0 slots) slave replicates 13d0c397604a0b2644244c37b666fce83f29faa8 S: 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005 slots: (0 slots) slave replicates d5a834d075fd93eefab877c6ebb86efff680650f S: dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004 slots: (0 slots) slave replicates be2718476eba4e56f696e56b75e67df720b7fc24 [OK] All nodes agree about slots configuration. \u003e\u003e\u003e Check for open slots... \u003e\u003e\u003e Check slots coverage... [OK] All 16384 slots covered. 2.1.3 部署结果验证 1、客户端访问\n使用客户端redis-cli 二进制访问某个实例，执行set和get的测试。\n$ redis-cli -c -p 7000 redis 127.0.0.1:7000\u003e set foo bar -\u003e Redirected to slot [12182] located at 127.0.0.1:7002 OK redis 127.0.0.1:7002\u003e set hello world -\u003e Redirected to slot [866] located at 127.0.0.1:7000 OK redis 127.0.0.1:7000\u003e get foo -\u003e Redirected to slot [12182] located at 127.0.0.1:7002 \"bar\" redis 127.0.0.1:7000\u003e get hello -\u003e Redirected to slot [866] located at 127.0.0.1:7000 \"world\" 2、查看集群状态\n使用cluster info命令查看集群状态。\n127.0.0.1:7000\u003e cluster info cluster_state:ok #集群状态 cluster_slots_assigned:16384 #被分配的槽位数 cluster_slots_ok:16384 #正确分配的槽位 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 #当前节点 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:48273 cluster_stats_messages_pong_sent:49884 cluster_stats_messages_sent:98157 cluster_stats_messages_ping_received:49879 cluster_stats_messages_pong_received:48273 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:98157 3、查看节点状态\n使用cluster nodes命令查看节点状态。\n127.0.0.1:7000\u003e cluster nodes be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002@17002 master - 0 1517303607000 3 connected 10923-16383 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001@17001 master - 0 1517303606000 2 connected 5461-10922 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003@17003 slave 13d0c397604a0b2644244c37b666fce83f29faa8 0 1517303606030 4 connected d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000@17000 myself,master - 0 1517303604000 1 connected 0-5460 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005@17005 slave d5a834d075fd93eefab877c6ebb86efff680650f 0 1517303607060 6 connected dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004@17004 slave be2718476eba4e56f696e56b75e67df720b7fc24 0 1517303608082 5 connected 参考文章：\nhttps://redis.io/download\nhttps://redis.io/topics/cluster-tutorial\n","categories":"","description":"","excerpt":"1. Redis部署 以下以Linux系统为例\n1.1 下载和编译 $ wget …","ref":"/linux-notes/redis/redis-cluster/","tags":["Redis"],"title":"Redis集群模式部署"},{"body":"1. shell变量 Shell支持自定义变量。\n1.1. 定义变量 定义变量时，变量名不加美元符号（$），如：\nvariableName=\"value\" 注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则：\n首个字符必须为字母（a-z，A-Z）。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 1.2. 使用变量 使用一个定义过的变量，只要在变量名前面加美元符号（$）即可，如：\nyour_name=\"mozhiyan\" echo $your_name echo ${your_name} 变量名外面的花括号是可选的，加不加都行，，比如下面这种情况：\nfor skill in Ada Coffe Action Java do echo \"I am good at ${skill}Script\" done 如果不给skill变量加花括号，写成echo \"I am good at $skillScript\"，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。推荐给所有变量加上花括号，这是个好的编程习惯。\n1.3. 重新定义变量 已定义的变量，可以被重新定义，如：\nmyUrl=\"http://see.xidian.edu.cn/cpp/linux/\" echo ${myUrl} myUrl=\"http://see.xidian.edu.cn/cpp/shell/\" echo ${myUrl} 这样写是合法的，但注意，第二次赋值的时候不能写 $myUrl=\"http://see.xidian.edu.cn/cpp/shell/\"， 使用变量的时候才加美元符（$）。\n1.4. 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。\n下面的例子尝试更改只读变量，结果报错：\n#!/bin/bash myUrl=\"http://see.xidian.edu.cn/cpp/shell/\" readonly myUrl myUrl=\"http://see.xidian.edu.cn/cpp/danpianji/\" 运行脚本，结果如下：\n/bin/sh: NAME: This variable is read only. 1.5. 删除变量 使用 unset 命令可以删除变量。语法：\nunset variable_name 变量被删除后不能再次使用；unset 命令不能删除只读变量。\n举个例子：\n#!/bin/sh myUrl=\"http://see.xidian.edu.cn/cpp/u/xitong/\" unset myUrl echo $myUrl 上面的脚本没有任何输出。\n1.6. 变量类型 运行shell时，会同时存在三种变量：\n1) 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。\n2) 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。\n3) shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行。\n2. shell的特殊变量 特殊变量列表\n变量 含义 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2。 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 $* 稍有不同，下面将会讲到。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。 2.1. 命令行参数 运行脚本时传递给脚本的参数称为命令行参数。命令行参数用\\ $n 表示，例如，$1 表示第一个参数，$2 表示第二个参数，依次类推。\n2.2. $* 和$@ 的区别 $* 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。 但是当它们被双引号(\" \")包含时，\n\"$*\" 会将所有的参数作为一个整体，以\"$1 $2 … $n\"的形式输出所有参数；\n\"$@\" 会将各个参数分开，以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。\n2.3. 退出状态 $? 可以获取上一个命令的退出状态。所谓退出状态，就是上一个命令执行后的返回结果。 退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。 不过，也有一些命令返回其他值，表示不同类型的错误。$? 也可以表示函数的返回值。\n3. 转义字符 如果表达式中包含特殊字符，Shell 将会进行替换。例如，在双引号中使用变量就是一种替换，转义字符也是一种替换。\necho -e \"Value of a is $a \\n\" -e 表示对转义字符进行替换\n下面的转义字符都可以用在 echo 中：\n转义字符 含义 \\ 反斜杠 \\a 警报，响铃 \\b 退格（删除键） \\f 换页(FF)，将当前位置移到下页开头 \\n 换行 \\r 回车 \\t 水平制表符（tab键） \\v 垂直制表符 可以使用 echo 命令的 -E 选项禁止转义，默认也是不转义的；使用 -n 选项可以禁止插入换行符。\n4. 变量替换 4.1. 命令替换 命令替换是指Shell可以先执行命令，将输出结果暂时保存，在适当的地方输出。 命令替换的语法：\n`command` 注意是反引号，不是单引号，这个键位于 Esc 键下方。\nDATE=`date` echo \"Date is $DATE\" 4.2. 变量替换 变量替换可以根据变量的状态（是否为空、是否定义等）来改变它的值 可以使用的变量替换形式：\n形式 说明 ${var} 变量本来的值 ${var:-word} 如果变量 var 为空或已被删除(unset)，那么返回 word，但不改变 var 的值。 ${var:=word} 如果变量 var 为空或已被删除(unset)，那么返回 word，并将 var 的值设置为 word。 ${var:?message} 如果变量 var 为空或已被删除(unset)，那么将消息 message 送到标准错误输出，可以用来检测变量 var 是否可以被正常赋值。 若此替换出现在Shell脚本中，那么脚本将停止运行。 ${var:+word} 如果变量 var 被定义，那么返回 word，但不改变 var 的值。 作用：用来检测变量是否为空，并提示相关信息。\n参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. shell变量 Shell支持自定义变量。\n1.1. 定义变量 定义变量时，变量名不加美元符号（$），如： …","ref":"/linux-notes/shell/shell-var/","tags":["Shell"],"title":"Shell变量"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/multi-cluster/virtual-kubelet/","tags":"","title":"Virtual Kubelet"},{"body":"1. 迁移Pod 1.1. 设置节点是否可调度 确定需要迁移和被迁移的节点，将不允许被迁移的节点设置为不可调度。\n# 查看节点 kubectl get nodes # 设置节点为不可调度 kubectl cordon \u003cNodeName\u003e # 设置节点为可调度 kubectl uncordon \u003cNodeName\u003e 1.2. 执行kubectl drain命令 # 驱逐节点的所有pod kubectl drain \u003cNodeName\u003e --force --ignore-daemonsets # 驱逐指定节点的pod kubectl drain \u003cNodeName\u003e --ignore-daemonsets\t--pod-selector=pod-template-hash=88964949c 示例：\n$ kubectl drain bjzw-prek8sredis-99-40 --force --ignore-daemonsets node \"bjzw-prek8sredis-99-40\" already cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: kube-proxy-bjzw-prek8sredis-99-40; Ignoring DaemonSet-managed pods: calicoopsmonitor-mfpqs, arachnia-agent-j56n8 pod \"pre-test-pro2-r-0-redis-2-8-19-1\" evicted pod \"pre-test-hwh1-r-8-redis-2-8-19-2\" evicted pod \"pre-eos-hdfs-vector-eos-hdfs-redis-2-8-19-0\" evicted 1.3. 特别说明 对于statefulset创建的Pod，kubectl drain的说明如下：\nkubectl drain操作会将相应节点上的旧Pod删除，并在可调度节点上面起一个对应的Pod。当旧Pod没有被正常删除的情况下，新Pod不会起来。例如：旧Pod一直处于Terminating状态。\n对应的解决方式是通过重启相应节点的kubelet，或者强制删除该Pod。\n示例：\n# 重启发生`Terminating`节点的kubelet systemctl restart kubelet # 强制删除`Terminating`状态的Pod kubectl delete pod \u003cPodName\u003e --namespace=\u003cNamespace\u003e --force --grace-period=0 2. kubectl drain 流程图 3. TroubleShooting 1、存在不是通过ReplicationController, ReplicaSet, Job, DaemonSet 或者 StatefulSet创建的Pod（即静态pod，通过文件方式创建的），所以需要设置强制执行的参数--force。\n$ kubectl drain bjzw-prek8sredis-99-40 node \"bjzw-prek8sredis-99-40\" already cordoned error: unable to drain node \"bjzw-prek8sredis-99-40\", aborting command... There are pending nodes to be drained: bjzw-prek8sredis-99-40 error: DaemonSet-managed pods (use --ignore-daemonsets to ignore): calicoopsmonitor-mfpqs, arachnia-agent-j56n8; pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): kube-proxy-bjzw-prek8sredis-99-40 2、存在DaemonSet方式管理的Pod，需要设置--ignore-daemonsets参数忽略报错。\n$ kubectl drain bjzw-prek8sredis-99-40 --force node \"bjzw-prek8sredis-99-40\" already cordoned error: unable to drain node \"bjzw-prek8sredis-99-40\", aborting command... There are pending nodes to be drained: bjzw-prek8sredis-99-40 error: DaemonSet-managed pods (use --ignore-daemonsets to ignore): calicoopsmonitor-mfpqs, arachnia-agent-j56n8 4. kubectl drain $ kubectl drain --help Drain node in preparation for maintenance. The given node will be marked unschedulable to prevent new pods from arriving. 'drain' evicts the pods if the API server supports https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ . Otherwise, it will use normal DELETE to delete the pods. The 'drain' evicts or deletes all pods except mirror pods (which cannot be deleted through the API server). If there are daemon set-managed pods, drain will not proceed without --ignore-daemonsets, and regardless it will not delete any daemon set-managed pods, because those pods would be immediately replaced by the daemon set controller, which ignores unschedulable markings. If there are any pods that are neither mirror pods nor managed by a replication controller, replica set, daemon set, stateful set, or job, then drain will not delete any pods unless you use --force. --force will also allow deletion to proceed if the managing resource of one or more pods is missing. 'drain' waits for graceful termination. You should not operate on the machine until the command completes. When you are ready to put the node back into service, use kubectl uncordon, which will make the node schedulable again. https://kubernetes.io/images/docs/kubectl_drain.svg Examples: # Drain node \"foo\", even if there are pods not managed by a replication controller, replica set, job, daemon set or stateful set on it kubectl drain foo --force # As above, but abort if there are pods not managed by a replication controller, replica set, job, daemon set or stateful set, and use a grace period of 15 minutes kubectl drain foo --grace-period=900 Options: --chunk-size=500: Return large lists in chunks rather than all at once. Pass 0 to disable. This flag is beta and may change in the future. --delete-emptydir-data=false: Continue even if there are pods using emptyDir (local data that will be deleted when the node is drained). --disable-eviction=false: Force drain to use delete, even if eviction is supported. This will bypass checking PodDisruptionBudgets, use with caution. --dry-run='none': Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. --force=false: Continue even if there are pods not managed by a ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet. --grace-period=-1: Period of time in seconds given to each pod to terminate gracefully. If negative, the default value specified in the pod will be used. --ignore-daemonsets=false: Ignore DaemonSet-managed pods. --ignore-errors=false: Ignore errors occurred between drain nodes in group. --pod-selector='': Label selector to filter pods on the node -l, --selector='': Selector (label query) to filter on --skip-wait-for-delete-timeout=0: If pod DeletionTimestamp older than N seconds, skip waiting for the pod. Seconds must be greater than 0 to skip. --timeout=0s: The length of time to wait before giving up, zero means infinite Usage: kubectl drain NODE [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). 参考文档：\nhttps://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/ https://kubernetes.io/docs/tasks/run-application/configure-pdb/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain ","categories":"","description":"","excerpt":"1. 迁移Pod 1.1. 设置节点是否可调度 确定需要迁移和被迁移的节点，将不允许被迁移的节点设置为不可调度。\n# …","ref":"/kubernetes-notes/operation/node/safely-drain-node/","tags":["Kubernetes"],"title":"安全迁移节点"},{"body":"1. CentOS 安装Docker 建议使用centos7\n1.1. 安装Docker 1.1.1. 卸载旧版本 旧版本的Docker命名为docker或docker-engine，如果有安装旧版本，先卸载旧版本\n$ sudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 1.1.2. 使用仓库安装 1、安装yum-utils、device-mapper-persistent-data、lvm2\n$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 2、添加软件源\n$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1.1.3. 安装Docker 安装最新版本的Docker CE。\n$ sudo yum install -y docker-ce 1.1.4. 启动Docker # 启动Docker $ sudo systemctl start docker # 运行容器 $ sudo docker run hello-world 1.2. 安装指定版本Docker 1、列出可安装版本\n$ yum list docker-ce --showduplicates | sort -r docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable 2、安装指定版本\n例如：docker-ce-18.03.0.ce\n$ sudo yum install docker-ce-\u003cVERSION STRING\u003e 1.3. 升级Docker 依据1.2的方法选择指定版本安装。\n1.4. 卸载Docker # 卸载Docker $ sudo yum remove docker-ce # 清理镜像、容器、存储卷等 $ sudo rm -rf /var/lib/docker 2. Ubuntu 安装Docker 2.1. 安装Docker 2.1.1. 卸载旧版本 旧版本的Docker命名为docker或docker-engine，如果有安装旧版本，先卸载旧版本\nsudo apt-get remove docker docker-engine docker.io 2.1.2. 使用仓库安装 1、升级apt\nsudo apt-get update 2、允许apt使用https\nsudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common 3、添加Docker 官方的GPG密钥\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 4、添加Docker软件源\nsudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 2.1.3. 安装Docker # update sudo apt-get update # install docker sudo apt-get install docker-ce 2.1.4. 启动Docker # 设置为开机启动 sudo systemctl enable docker # 启动docker sudo systemctl start docker 2.2. 安装指定版本Docker 1、列出仓库的可安装版本，apt-cache madison docker-ce。\n# apt-cache madison docker-ce docker-ce | 18.06.0~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.03.1~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages 2、指定版本安装\n例如：docker-ce=18.03.0~ce-0~ubuntu\nsudo apt-get install docker-ce=\u003cVERSION\u003e 2.3. 升级Docker # 更新源 sudo apt-get update # 依据上述方法，指定版本安装 2.4. 卸载Docker # 卸载 docker ce sudo apt-get purge docker-ce # 清理镜像、容器、存储卷等 sudo rm -rf /var/lib/docker 3. 离线rpm包安装Docker 3.1. 下载docker rpm包 rpm包地址：https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/\n下载指定版本的containerd.io、docker-ce、docker-ce-cli\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm wget https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-18.09.9-3.el7.x86_64.rpm wget https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-cli-18.09.9-3.el7.x86_64.rpm 下载container-selinux\n地址：http://mirror.centos.org/centos/7/extras/x86_64/Packages/\nwget http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm 3.2. 安装rpm包 # container-selinux rpm -ivh container-selinux*.rpm # containerd.io rpm -ivh containerd.io*.rpm # docker-ce rpm -ivh docker-ce*.rpm # docker-ce-cli rpm -ivh docker-ce-cli*.rpm 3.3. 启动docker服务 # 启动 systemctl start docker # 查看状态 systemctl status docker 文章参考：\nhttps://docs.docker.com/install/linux/docker-ce/centos/\nhttps://docs.docker.com/install/linux/docker-ce/ubuntu/\n","categories":"","description":"","excerpt":"1. CentOS 安装Docker 建议使用centos7\n1.1. 安装Docker 1.1.1. 卸载旧版本 旧版本的Docker命名 …","ref":"/kubernetes-notes/runtime/docker/install-docker/","tags":["Docker"],"title":"安装Docker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/introduction/","tags":"","title":"安装与配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/setup/","tags":"","title":"安装与配置"},{"body":"0. 说明 要求Kubernetes的版本在1.11及以上，k8s集群必须允许特权Pod（privileged pods），即apiserver和kubelet需要设置--allow-privileged为true。节点的Docker daemon需要允许挂载共享卷。\n涉及镜像 quay.io/k8scsi/csi-provisioner:v0.3.0 quay.io/k8scsi/csi-attacher:v0.3.0 quay.io/k8scsi/driver-registrar:v0.3.0 quay.io/cephcsi/cephfsplugin:v0.3.0 1. 部署RBAC 部署service accounts, cluster roles 和 cluster role bindings，这些可供RBD和CephFS CSI plugins共同使用，他们拥有相同的权限。\n$ kubectl create -f csi-attacher-rbac.yaml $ kubectl create -f csi-provisioner-rbac.yaml $ kubectl create -f csi-nodeplugin-rbac.yaml 1.1. csi-attacher-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: csi-attacher --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: external-attacher-runner rules: - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-attacher-role subjects: - kind: ServiceAccount name: csi-attacher namespace: default roleRef: kind: ClusterRole name: external-attacher-runner apiGroup: rbac.authorization.k8s.io 1.2. csi-provisioner-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: csi-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: external-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"list\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-provisioner-role subjects: - kind: ServiceAccount name: csi-provisioner namespace: default roleRef: kind: ClusterRole name: external-provisioner-runner apiGroup: rbac.authorization.k8s.io 1.3. csi-nodeplugin-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: csi-nodeplugin --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-nodeplugin rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"update\"] - apiGroups: [\"\"] resources: [\"namespaces\"] verbs: [\"get\", \"list\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-nodeplugin subjects: - kind: ServiceAccount name: csi-nodeplugin namespace: default roleRef: kind: ClusterRole name: csi-nodeplugin apiGroup: rbac.authorization.k8s.io 2. 部署CSI sidecar containers 通过StatefulSet的方式部署external-attacher和external-provisioner供CSI CephFS使用。\n$ kubectl create -f csi-cephfsplugin-attacher.yaml $ kubectl create -f csi-cephfsplugin-provisioner.yaml 2.1. csi-cephfsplugin-provisioner.yaml kind: Service apiVersion: v1 metadata: name: csi-cephfsplugin-provisioner labels: app: csi-cephfsplugin-provisioner spec: selector: app: csi-cephfsplugin-provisioner ports: - name: dummy port: 12345 --- kind: StatefulSet apiVersion: apps/v1beta1 metadata: name: csi-cephfsplugin-provisioner spec: serviceName: \"csi-cephfsplugin-provisioner\" replicas: 1 template: metadata: labels: app: csi-cephfsplugin-provisioner spec: serviceAccount: csi-provisioner containers: - name: csi-provisioner image: quay.io/k8scsi/csi-provisioner:v0.3.0 args: - \"--provisioner=csi-cephfsplugin\" - \"--csi-address=$(ADDRESS)\" - \"--v=5\" env: - name: ADDRESS value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin volumes: - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate 2.2. csi-cephfsplugin-attacher.yaml kind: Service apiVersion: v1 metadata: name: csi-cephfsplugin-attacher labels: app: csi-cephfsplugin-attacher spec: selector: app: csi-cephfsplugin-attacher ports: - name: dummy port: 12345 --- kind: StatefulSet apiVersion: apps/v1beta1 metadata: name: csi-cephfsplugin-attacher spec: serviceName: \"csi-cephfsplugin-attacher\" replicas: 1 template: metadata: labels: app: csi-cephfsplugin-attacher spec: serviceAccount: csi-attacher containers: - name: csi-cephfsplugin-attacher image: quay.io/k8scsi/csi-attacher:v0.3.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" env: - name: ADDRESS value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin volumes: - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate 3. 部署CSI-CephFS-driver(plugin) csi-cephfs-plugin 的作用类似nfs-client，部署在所有node节点上，执行ceph的挂载等相关任务。\n通过DaemonSet的方式部署，其中包括两个容器：CSI driver-registrar 和 CSI CephFS driver。\n$ kubectl create -f csi-cephfsplugin.yaml 3.1. csi-cephfsplugin.yaml kind: DaemonSet apiVersion: apps/v1beta2 metadata: name: csi-cephfsplugin spec: selector: matchLabels: app: csi-cephfsplugin template: metadata: labels: app: csi-cephfsplugin spec: serviceAccount: csi-nodeplugin hostNetwork: true # to use e.g. Rook orchestrated cluster, and mons' FQDN is # resolved through k8s service, set dns policy to cluster first dnsPolicy: ClusterFirstWithHostNet containers: - name: driver-registrar image: quay.io/k8scsi/driver-registrar:v0.3.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" - \"--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\" env: - name: ADDRESS value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock - name: DRIVER_REG_SOCK_PATH value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock - name: KUBE_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumeMounts: - name: socket-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin - name: registration-dir mountPath: /registration - name: csi-cephfsplugin securityContext: privileged: true capabilities: add: [\"SYS_ADMIN\"] allowPrivilegeEscalation: true image: quay.io/cephcsi/cephfsplugin:v0.3.0 args : - \"--nodeid=$(NODE_ID)\" - \"--endpoint=$(CSI_ENDPOINT)\" - \"--v=5\" - \"--drivername=csi-cephfsplugin\" env: - name: NODE_ID valueFrom: fieldRef: fieldPath: spec.nodeName - name: CSI_ENDPOINT value: unix://var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: plugin-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin - name: pods-mount-dir mountPath: /var/lib/kubelet/pods mountPropagation: \"Bidirectional\" - mountPath: /sys name: host-sys - name: lib-modules mountPath: /lib/modules readOnly: true - name: host-dev mountPath: /dev volumes: - name: plugin-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate - name: registration-dir hostPath: path: /var/lib/kubelet/plugins/ type: Directory - name: pods-mount-dir hostPath: path: /var/lib/kubelet/pods type: Directory - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate - name: host-sys hostPath: path: /sys - name: lib-modules hostPath: path: /lib/modules - name: host-dev hostPath: path: /dev 4. 确认部署结果 $ kubectl get all NAME READY STATUS RESTARTS AGE pod/csi-cephfsplugin-attacher-0 1/1 Running 0 26s pod/csi-cephfsplugin-provisioner-0 1/1 Running 0 25s pod/csi-cephfsplugin-rljcv 2/2 Running 0 24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/csi-cephfsplugin-attacher ClusterIP 10.104.116.218 \u003cnone\u003e 12345/TCP 27s service/csi-cephfsplugin-provisioner ClusterIP 10.101.78.75 \u003cnone\u003e 12345/TCP 26s ... 参考文档：\nhttps://github.com/ceph/ceph-csi https://github.com/ceph/ceph-csi/blob/master/docs/deploy-cephfs.md https://github.com/ceph/ceph-csi/tree/master/deploy/cephfs/kubernetes ","categories":"","description":"","excerpt":"0. 说明 要求Kubernetes的版本在1.11及以上，k8s集群必须允许特权Pod（privileged pods）， …","ref":"/kubernetes-notes/storage/csi/ceph/deploy-csi-cephfs/","tags":["CSI"],"title":"部署csi-cephfs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/disk/","tags":"","title":"磁盘"},{"body":"1. 计算机语言概述 学习一门计算机语言，将计算机语言分为以下几大部分：\n语言特点 环境准备 基本语法 数据类型 变量 常量 引用类型 流程语句 判断语句 循环语句 选择语句 函数 面向对象编程 封装（类与方法） 继承 多态（接口） 并发编程 特殊属性 包管理 标准库 2. 思维导图 ","categories":"","description":"","excerpt":"1. 计算机语言概述 学习一门计算机语言，将计算机语言分为以下几大部分：\n语言特点 环境准备 基本语法 数据类型 变量 常量 引用类型 流程 …","ref":"/golang-notes/summary/language/","tags":["Golang"],"title":"计算机语言概述"},{"body":" 本文介绍通过pod指定 ImagePullSecrets来拉取私有镜像仓库的镜像\n1. 创建secret secret是namespace级别的，创建时候需要指定namespace。\nkubectl create secret docker-registry \u003cname\u003e --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD -n \u003cNAMESPACE\u003e 2. 添加ImagePullSecrets到serviceAccount 可以通过将ImagePullSecrets到serviceAccount的方式来自动给pod添加imagePullSecrets参数值。\nserviceAccount同样是namespace级别，只对该namespace生效。\n#kubectl get secrets -n dev NAME TYPE DATA AGE docker.xxxx.com kubernetes.io/dockerconfigjson 1 6h23m 将ImagePullSecrets添加到serviceAccount对象中。\n默认serviceAccount对象如下\n#kubectl get serviceaccount default -n dev -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-02-27T03:30:38Z\" name: default namespace: dev resourceVersion: \"11651567\" selfLink: /api/v1/namespaces/dev/serviceaccounts/default uid: 85bcdd31-5911-11ea-9429-6c92bf3b7c33 secrets: - name: default-token-s7wfn 编辑或修改serviceAccount内容，增加imagePullSecrets字段。\nimagePullSecrets: - name: docker.xxxx.com kubectl edit serviceaccount default -n dev\n修改后内容为：\napiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-02-27T03:30:38Z\" name: default namespace: dev resourceVersion: \"11651567\" selfLink: /api/v1/namespaces/dev/serviceaccounts/default uid: 85bcdd31-5911-11ea-9429-6c92bf3b7c33 secrets: - name: default-token-s7wfn imagePullSecrets: - name: docker.xxxx.com 其中imagePullSecrets字段是一个数组，可以配置多个镜像仓库的账号密码。\n例如：\napiVersion: v1 kind: ServiceAccount ... imagePullSecrets: - name: docker.xxxx.com - name: docker.test.xxxx.com 3. 创建带有imagePullSecrets的pod 如果已经执行了第二步操作，添加ImagePullSecrets到serviceAccount，则无需在pod中指定imagePullSecrets参数，默认会自动添加。\n如果没有添加ImagePullSecrets到serviceAccount，则在pod中指定imagePullSecrets参数引用创建的镜像仓库的secret。\nspec: imagePullSecrets: - name: docker.xxxx.com 4. 说明 由于secret和serviceaccount对象是对namespace级别生效，因此不同的namespace需要再次创建和更新这两个对象。该场景适合不同用户具有独立的镜像仓库的密码，可以通过该方式创建不同的镜像密码使用的secret来拉取不同的镜像部署。\n参考：\nhttps://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account ","categories":"","description":"","excerpt":" 本文介绍通过pod指定 ImagePullSecrets来拉取私有镜像仓库的镜像\n1. 创建secret secret是namespace …","ref":"/kubernetes-notes/operation/registry/imagepullsecrets/","tags":["Kubernetes"],"title":"拉取私有镜像"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/principle/flow/","tags":"","title":"流程图"},{"body":"1. 内存分配器的问题 当给不同大小的变量分配连续地址的内存的时候，可能因为部分变量内存的回收导致在分配新的内存需求时无法利用被回收的内存地址，因此内存管理不当，容易导致内存的碎片。\n2.基本策略： 每次从操作系统申请⼀⼤块内存（⽐如 1MB），以减少系统调⽤。 将申请到的⼤块内存按照特定⼤⼩预先切分成⼩块，构成链表。 为对象分配内存时，只需从⼤⼩合适的链表提取⼀个⼩块即可。 回收对象内存时，将该⼩块内存重新归还到原链表，以便复⽤。 如闲置内存过多，则尝试归还部分内存给操作系统，降低整体开销 3.内存分配的本质 针对不同大小的对象，在不同的 cache 层中，使用不同的内存结构；将从系统中获得的一块连续内存分割成多层次的 cache，以减少锁的使用以提高内存分配效率；申请不同类大小的内存块来减少内存碎片，同时加速内存释放后的垃圾回收。\ngo的内存分配器将内存页分成67个不同大小规格（size class）的块，最小为8KB，最大为32768KB。\n内存块的分类：\nspan:由多个地址连续的页（page）组成的大块内存。面向内部管理。 object：将span按照特定的大小切分成多个小块，每个小块可以存储一个对象。面向对象分配。 内存分配器的三个数据结构（申请逐级向上）：\nmcache：goroutine cache，可以认为是 本地 cache。不涉及锁竞争。 mcentral：全局cache，mcache 不够用的时候向 mcentral 申请。涉及锁竞争。 mheap：当mcentral 也不够用的时候，通过 mheap 向操作系统申请。 4.内存分配的流程 object size \u003c 16K，使用 mcache 的小对象分配器 tiny 直接分配。 object size \u003e 32K，则使用 mheap 直接分配。 object size \u003e 16K \u0026\u0026 object size \u003c 32K，先使用 mcache 中对应的 size class 分配。 如果 mcache 对应的 size class 的 span 已经没有可用的块，则向 mcentral 请求。 如果 mcentral 也没有可用的块，则向 mheap申请，并切分。 如果 mheap 也没有合适的 span，则想操作系统申请。 5.内存回收的流程 mcache 归还内存分两部分：归还mcentral内存，可能涉及锁竞争；除此之外，归还到mheap，直接插入链表头。 mcentral 归还mheap。 mheap 定时归还系统内存。 6.tcmalloc(thread-caching mallo) 是google推出的一种内存分配器。\n具体策略：全局缓存堆和进程的私有缓存。\n1.对于一些小容量的内存申请试用进程的私有缓存，私有缓存不足的时候可以再从全局缓存申请一部分作为私有缓存。\n2.对于大容量的内存申请则需要从全局缓存中进行申请。而大小容量的边界就是32k。缓存的组织方式是一个单链表数组，数组的每个元素是一个单链表，链表中的每个元素具有相同的大小。\ngolang语言中MHeap就是全局缓存堆，MCache作为线程私有缓存。\n内存池就是利用MHeap实现，大小切分则是在申请内存的时候就做了，同时MCache分配内存时，可以用MCentral去取对应的sizeClass，多线程管理方面则是通过MCache去实现。\n总结 1.MHeap是一个全局变量，负责向系统申请内存，mallocinit()函数进行初始化。如果分配内存对象大于32K直接向MHeap申请。\n2.MCache线程级别管理内存池，关联结构体P，主要是负责线程内部内存申请。\n3.MCentral连接MHeap与MCache的，MCache内存不够则向MCentral申请，MCentral不够时向MHeap申请内存。\n","categories":"","description":"","excerpt":"1. 内存分配器的问题 当给不同大小的变量分配连续地址的内存的时候，可能因为部分变量内存的回收导致在分配新的内存需求时无法利用被回收的内存地 …","ref":"/golang-notes/principle/memory-allocation/","tags":["Golang"],"title":"内存分配"},{"body":"1. 环境准备 1.1. 部署机器 以下机器为虚拟机\n机器IP 主机名 角色 系统版本 备注 172.16.94.140 kube-master-0 k8s master Centos 4.17.14 内存：3G 172.16.94.141 kube-node-41 k8s node Centos 4.17.14 内存：3G 172.16.94.142 kube-node-42 k8s node Centos 4.17.14 内存：3G 172.16.94.135 部署管理机 - 1.2. 配置管理机 管理机主要用来部署k8s集群，需要安装以下版本的软件，具体可参考：\nhttps://github.com/kubernetes-incubator/kubespray#requirements\nhttps://github.com/kubernetes-incubator/kubespray/blob/master/requirements.txt\nansible\u003e=2.4.0 jinja2\u003e=2.9.6 netaddr pbr\u003e=1.6 ansible-modules-hashivault\u003e=3.9.4 hvac 1、安装及配置ansible\n参考ansible的使用。 给部署机器配置SSH的免密登录权限，具体参考ssh免密登录。 2、安装python-netaddr\n# 安装pip yum -y install epel-release yum -y install python-pip # 安装python-netaddr pip install netaddr 3、升级Jinja\n# Jinja 2.9 (or newer) pip install --upgrade jinja2 1.3. 配置部署机器 部署机器即用来运行k8s集群的机器，包括Master和Node。\n1、确认系统版本\n本文采用centos7的系统，建议将系统内核升级到4.x.x以上。\n2、关闭防火墙\nsystemctl stop firewalld systemctl disable firewalld iptables -F 3、关闭swap\nKubespary v2.5.0的版本需要关闭swap，具体参考\nhttps://github.com/kubernetes-incubator/kubespray/blob/02cd5418c22d51e40261775908d55bc562206023/roles/kubernetes/preinstall/tasks/verify-settings.yml#L75 - name: Stop if swap enabled assert: that: ansible_swaptotal_mb == 0 when: kubelet_fail_swap_on|default(true) ignore_errors: \"{{ ignore_assert_errors }}\" V2.6.0 版本去除了swap的检查，具体参考：\nhttps://github.com/kubernetes-incubator/kubespray/commit/b902602d161f8c147f3d155d2ac5360244577127#diff-b92ae64dd18d34a96fbeb7f7e48a6a9b 执行关闭swap命令swapoff -a。\n[root@master ~]#swapoff -a [root@master ~]# [root@master ~]# free -m total used free shared buff/cache available Mem: 976 366 135 6 474 393 Swap: 0 0 0 # swap 一栏为0，表示已经关闭了swap 4、确认部署机器内存\n由于本文采用虚拟机部署，内存可能存在不足的问题，因此将虚拟机内存调整为3G或以上；如果是物理机一般不会有内存不足的问题。具体参考：\nhttps://github.com/kubernetes-incubator/kubespray/blob/95f1e4634a1c50fa77312d058a2b713353f4307e/roles/kubernetes/preinstall/tasks/verify-settings.yml#L52 - name: Stop if memory is too small for masters assert: that: ansible_memtotal_mb \u003e= 1500 ignore_errors: \"{{ ignore_assert_errors }}\" when: inventory_hostname in groups['kube-master'] - name: Stop if memory is too small for nodes assert: that: ansible_memtotal_mb \u003e= 1024 ignore_errors: \"{{ ignore_assert_errors }}\" when: inventory_hostname in groups['kube-node'] 1.4. 涉及镜像 Docker版本为17.03.2-ce。\n1、Master节点\n镜像 版本 大小 镜像ID 备注 gcr.io/google-containers/hyperkube v1.9.5 620 MB a7e7fdbc5fee k8s quay.io/coreos/etcd v3.2.4 35.7 MB 498ffffcfd05 gcr.io/google_containers/pause-amd64 3.0 747 kB 99e59f495ffa quay.io/calico/node v2.6.8 282 MB e96a297310fd calico quay.io/calico/cni v1.11.4 70.8 MB 4c4cb67d7a88 calico quay.io/calico/ctl v1.6.3 44.4 MB 46d3aace8bc6 calico 2、Node节点\n镜像 版本 大小 镜像ID 备注 gcr.io/google-containers/hyperkube v1.9.5 620 MB a7e7fdbc5fee k8s gcr.io/google_containers/pause-amd64 3.0 747 kB 99e59f495ffa quay.io/calico/node v2.6.8 282 MB e96a297310fd calico quay.io/calico/cni v1.11.4 70.8 MB 4c4cb67d7a88 calico quay.io/calico/ctl v1.6.3 44.4 MB 46d3aace8bc6 calico gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64 1.14.8 40.9 MB c2ce1ffb51ed dns gcr.io/google_containers/k8s-dns-sidecar-amd64 1.14.8 42.2 MB 6f7f2dc7fab5 dns gcr.io/google_containers/k8s-dns-kube-dns-amd64 1.14.8 50.5 MB 80cc5ea4b547 dns gcr.io/google_containers/cluster-proportional-autoscaler-amd64 1.1.2 50.5 MB 78cf3f492e6b gcr.io/google_containers/kubernetes-dashboard-amd64 v1.8.3 102 MB 0c60bcf89900 dashboard nginx 1.13 109 MB ae513a47849c - 3、说明\n镜像被墙并且全部镜像下载需要较多时间，建议提前下载到部署机器上。 hyperkube镜像主要用来运行k8s核心组件（例如kube-apiserver等）。 此处使用的网络组件为calico。 2. 部署集群 2.1. 下载kubespary的源码 git clone https://github.com/kubernetes-incubator/kubespray.git 2.2. 编辑配置文件 2.2.1. hosts.ini hosts.ini主要为部署节点机器信息的文件，路径为：kubespray/inventory/sample/hosts.ini。\ncd kubespray # 复制一份配置进行修改 cp -rfp inventory/sample inventory/k8s vi inventory/k8s/hosts.ini 例如：\nhosts.ini文件可以填写部署机器的登录密码，也可以不填密码而设置ssh的免密登录。\n# Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # 主机名 ssh登陆IP ssh用户名 ssh登陆密码 机器IP 子网掩码 kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 [etcd] kube-master-0 [kube-node] kube-node-41 kube-node-42 [k8s-cluster:children] kube-node kube-master [calico-rr] 2.2.2. k8s-cluster.yml k8s-cluster.yml主要为k8s集群的配置文件，路径为：kubespray/inventory/k8s/group_vars/k8s-cluster.yml。该文件可以修改安装的k8s集群的版本，参数为：kube_version: v1.9.5。具体可参考：\nhttps://github.com/kubernetes-incubator/kubespray/blob/master/inventory/sample/group_vars/k8s-cluster.yml#L22 2.3. 执行部署操作 涉及文件为cluster.yml。\n# 进入主目录 cd kubespray # 执行部署命令 ansible-playbook -i inventory/k8s/hosts.ini cluster.yml -b -vvv -vvv 参数表示输出运行日志\n如果需要重置可以执行以下命令：\n涉及文件为reset.yml。\nansible-playbook -i inventory/k8s/hosts.ini reset.yml -b -vvv 3. 确认部署结果 3.1. ansible的部署结果 ansible命令执行完，出现以下日志，则说明部署成功，否则根据报错内容进行修改。\nPLAY RECAP ***************************************************************************** kube-master-0 : ok=309 changed=30 unreachable=0 failed=0 kube-node-41 : ok=203 changed=8 unreachable=0 failed=0 kube-node-42 : ok=203 changed=8 unreachable=0 failed=0 localhost : ok=2 changed=0 unreachable=0 failed=0 以下为部分部署执行日志：\nkubernetes/preinstall : Update package management cache (YUM) --------------------23.96s /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/main.yml:121 kubernetes/master : Master | wait for the apiserver to be running ----------------23.44s /root/gopath/src/kubespray/roles/kubernetes/master/handlers/main.yml:79 kubernetes/preinstall : Install packages requirements ----------------------------20.20s /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/main.yml:203 kubernetes/secrets : Check certs | check if a cert already exists on node --------13.94s /root/gopath/src/kubespray/roles/kubernetes/secrets/tasks/check-certs.yml:17 gather facts from all instances --------------------------------------------------9.98s /root/gopath/src/kubespray/cluster.yml:25 kubernetes/node : install | Compare host kubelet with hyperkube container --------9.66s /root/gopath/src/kubespray/roles/kubernetes/node/tasks/install_host.yml:2 kubernetes-apps/ansible : Kubernetes Apps | Start Resources -----------------------9.27s /root/gopath/src/kubespray/roles/kubernetes-apps/ansible/tasks/main.yml:37 kubernetes-apps/ansible : Kubernetes Apps | Lay Down KubeDNS Template ------------8.47s /root/gopath/src/kubespray/roles/kubernetes-apps/ansible/tasks/kubedns.yml:3 download : Sync container ---------------------------------------------------------8.23s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 kubernetes-apps/network_plugin/calico : Start Calico resources --------------------7.82s /root/gopath/src/kubespray/roles/kubernetes-apps/network_plugin/calico/tasks/main.yml:2 download : Download items ---------------------------------------------------------7.67s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------7.48s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Sync container ---------------------------------------------------------7.35s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 download : Download items ---------------------------------------------------------7.16s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 network_plugin/calico : Calico | Copy cni plugins from calico/cni container -------7.10s /root/gopath/src/kubespray/roles/network_plugin/calico/tasks/main.yml:62 download : Download items ---------------------------------------------------------7.04s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------7.01s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Sync container ---------------------------------------------------------7.00s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 download : Download items ---------------------------------------------------------6.98s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------6.79s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 3.2. k8s集群运行结果 1、k8s组件信息\n# kubectl get all --namespace=kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds/calico-node 3 3 3 3 3 \u003cnone\u003e 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 2 2 2 2 2h deploy/kubedns-autoscaler 1 1 1 1 2h deploy/kubernetes-dashboard 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/kube-dns-79d99cdcd5 2 2 2 2h rs/kubedns-autoscaler-5564b5585f 1 1 1 2h rs/kubernetes-dashboard-69cb58d748 1 1 1 2h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds/calico-node 3 3 3 3 3 \u003cnone\u003e 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 2 2 2 2 2h deploy/kubedns-autoscaler 1 1 1 1 2h deploy/kubernetes-dashboard 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/kube-dns-79d99cdcd5 2 2 2 2h rs/kubedns-autoscaler-5564b5585f 1 1 1 2h rs/kubernetes-dashboard-69cb58d748 1 1 1 2h NAME READY STATUS RESTARTS AGE po/calico-node-22vsg 1/1 Running 0 2h po/calico-node-t7zgw 1/1 Running 0 2h po/calico-node-zqnx8 1/1 Running 0 2h po/kube-apiserver-kube-master-0 1/1 Running 0 22h po/kube-controller-manager-kube-master-0 1/1 Running 0 2h po/kube-dns-79d99cdcd5-f2t6t 3/3 Running 0 2h po/kube-dns-79d99cdcd5-gw944 3/3 Running 0 2h po/kube-proxy-kube-master-0 1/1 Running 2 22h po/kube-proxy-kube-node-41 1/1 Running 3 22h po/kube-proxy-kube-node-42 1/1 Running 3 22h po/kube-scheduler-kube-master-0 1/1 Running 0 2h po/kubedns-autoscaler-5564b5585f-lt9bb 1/1 Running 0 2h po/kubernetes-dashboard-69cb58d748-wmb9x 1/1 Running 0 2h po/nginx-proxy-kube-node-41 1/1 Running 3 22h po/nginx-proxy-kube-node-42 1/1 Running 3 22h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.233.0.3 \u003cnone\u003e 53/UDP,53/TCP 2h svc/kubernetes-dashboard ClusterIP 10.233.27.24 \u003cnone\u003e 443/TCP 2h 2、k8s节点信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kube-master-0 Ready master 22h v1.9.5 kube-node-41 Ready node 22h v1.9.5 kube-node-42 Ready node 22h v1.9.5 3、组件健康信息\n# kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} 4. k8s集群扩容节点 4.1. 修改hosts.ini文件 如果需要扩容Node节点，则修改hosts.ini文件，增加新增的机器信息。例如，要增加节点机器kube-node-43（IP为172.16.94.143），修改后的文件内容如下：\n# Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # 主机名 ssh登陆IP ssh用户名 ssh登陆密码 机器IP 子网掩码 kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 kube-node-43 ansible_ssh_host=172.16.94.143 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.143 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 [etcd] kube-master-0 [kube-node] kube-node-41 kube-node-42 kube-node-43 [k8s-cluster:children] kube-node kube-master [calico-rr] 4.2. 执行扩容命令 涉及文件为scale.yml。\n# 进入主目录 cd kubespray # 执行部署命令 ansible-playbook -i inventory/k8s/hosts.ini scale.yml -b -vvv 4.3. 检查扩容结果 1、ansible的执行结果\nPLAY RECAP *************************************** kube-node-41 : ok=228 changed=11 unreachable=0 failed=0 kube-node-42 : ok=197 changed=6 unreachable=0 failed=0 kube-node-43 : ok=227 changed=69 unreachable=0 failed=0 # 新增Node节点 localhost : ok=2 changed=0 unreachable=0 failed=0 2、k8s的节点信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kube-master-0 Ready master 1d v1.9.5 kube-node-41 Ready node 1d v1.9.5 kube-node-42 Ready node 1d v1.9.5 kube-node-43 Ready node 1m v1.9.5 #该节点为新增Node节点 可以看到新增的kube-node-43节点已经扩容完成。\n3、k8s组件信息\n# kubectl get po --namespace=kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE calico-node-22vsg 1/1 Running 0 10h 172.16.94.140 kube-master-0 calico-node-8fz9x 1/1 Running 2 27m 172.16.94.143 kube-node-43 calico-node-t7zgw 1/1 Running 0 10h 172.16.94.142 kube-node-42 calico-node-zqnx8 1/1 Running 0 10h 172.16.94.141 kube-node-41 kube-apiserver-kube-master-0 1/1 Running 0 1d 172.16.94.140 kube-master-0 kube-controller-manager-kube-master-0 1/1 Running 0 10h 172.16.94.140 kube-master-0 kube-dns-79d99cdcd5-f2t6t 3/3 Running 0 10h 10.233.100.194 kube-node-41 kube-dns-79d99cdcd5-gw944 3/3 Running 0 10h 10.233.107.1 kube-node-42 kube-proxy-kube-master-0 1/1 Running 2 1d 172.16.94.140 kube-master-0 kube-proxy-kube-node-41 1/1 Running 3 1d 172.16.94.141 kube-node-41 kube-proxy-kube-node-42 1/1 Running 3 1d 172.16.94.142 kube-node-42 kube-proxy-kube-node-43 1/1 Running 0 26m 172.16.94.143 kube-node-43 kube-scheduler-kube-master-0 1/1 Running 0 10h 172.16.94.140 kube-master-0 kubedns-autoscaler-5564b5585f-lt9bb 1/1 Running 0 10h 10.233.100.193 kube-node-41 kubernetes-dashboard-69cb58d748-wmb9x 1/1 Running 0 10h 10.233.107.2 kube-node-42 nginx-proxy-kube-node-41 1/1 Running 3 1d 172.16.94.141 kube-node-41 nginx-proxy-kube-node-42 1/1 Running 3 1d 172.16.94.142 kube-node-42 nginx-proxy-kube-node-43 1/1 Running 0 26m 172.16.94.143 kube-node-43 5. 部署高可用集群 将hosts.ini文件中的master和etcd的机器增加到多台，执行部署命令。\nansible-playbook -i inventory/k8s/hosts.ini cluster.yml -b -vvv 例如：\n# Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # 主机名 ssh登陆IP ssh用户名 ssh登陆密码 机器IP 子网掩码 kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-master-1 ansible_ssh_host=172.16.94.144 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.144 mask=/24 kube-master-2 ansible_ssh_host=172.16.94.145 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.145 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 kube-node-43 ansible_ssh_host=172.16.94.143 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.143 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 kube-master-1 kube-master-2 [etcd] kube-master-0 kube-master-1 kube-master-2 [kube-node] kube-node-41 kube-node-42 kube-node-43 [k8s-cluster:children] kube-node kube-master [calico-rr] 6. 升级k8s集群 选择对应的k8s版本信息，执行升级命令。涉及文件为upgrade-cluster.yml。\nansible-playbook upgrade-cluster.yml -b -i inventory/k8s/hosts.ini -e kube_version=v1.10.4 -vvv 7. troubles shooting 在使用kubespary部署k8s集群时，主要遇到以下报错。\n7.1. python-netaddr未安装 报错内容： fatal: [node1]: FAILED! =\u003e {\"failed\": true, \"msg\": \"The ipaddr filter requires python-netaddr be installed on the ansible controller\"} 解决方法： 需要安装 python-netaddr，具体参考上述[环境准备]内容。\n7.2. swap未关闭 报错内容： fatal: [kube-master-0]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-41]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-42]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } 解决方法： 所有部署机器执行swapoff -a关闭swap，具体参考上述[环境准备]内容。\n7.3. 部署机器内存过小 报错内容： TASK [kubernetes/preinstall : Stop if memory is too small for masters] ********************************************************************************************************************************************************************************************************* task path: /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml:52 Friday 10 August 2018 21:50:26 +0800 (0:00:00.940) 0:01:14.088 ********* fatal: [kube-master-0]: FAILED! =\u003e { \"assertion\": \"ansible_memtotal_mb \u003e= 1500\", \"changed\": false, \"evaluated_to\": false } TASK [kubernetes/preinstall : Stop if memory is too small for nodes] *********************************************************************************************************************************************************************************************************** task path: /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml:58 Friday 10 August 2018 21:50:27 +0800 (0:00:00.570) 0:01:14.659 ********* fatal: [kube-node-41]: FAILED! =\u003e { \"assertion\": \"ansible_memtotal_mb \u003e= 1024\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-42]: FAILED! =\u003e { \"assertion\": \"ansible_memtotal_mb \u003e= 1024\", \"changed\": false, \"evaluated_to\": false } to retry, use: --limit @/root/gopath/src/kubespray/cluster.retry 解决方法： 调大所有部署机器的内存，本示例中调整为3G或以上。\n7.4. kube-scheduler组件运行失败 kube-scheduler组件运行失败，导致http://localhost:10251/healthz调用失败。\n报错内容： FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left). FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left). fatal: [node1]: FAILED! =\u003e {\"attempts\": 60, \"changed\": false, \"content\": \"\", \"failed\": true, \"msg\": \"Status code was not [200]: Request failed: \u003curlopen error [Errno 111] Connection refused\u003e\", \"redirected\": false, \"status\": -1, \"url\": \"http://localhost:10251/healthz\"} 解决方法： 可能是内存不足导致，本示例中调大了部署机器的内存。\n7.5. docker安装包冲突 报错内容： failed: [k8s-node-1] (item={u'name': u'docker-engine-1.13.1-1.el7.centos'}) =\u003e { \"attempts\": 4, \"changed\": false, ... \"item\": { \"name\": \"docker-engine-1.13.1-1.el7.centos\" }, \"msg\": \"Error: docker-ce-selinux conflicts with 2:container-selinux-2.66-1.el7.noarch\\n\", \"rc\": 1, \"results\": [ \"Loaded plugins: fastestmirror\\nLoading mirror speeds from cached hostfile\\n * elrepo: mirrors.tuna.tsinghua.edu.cn\\n * epel: mirrors.tongji.edu.cn\\nPackage docker-engine is obsoleted by docker-ce, trying to install docker-ce-17.03.2.ce-1.el7.centos.x86_64 instead\\nResolving Dependencies\\n--\u003e Running transaction check\\n---\u003e Package docker-ce.x86_64 0:17.03.2.ce-1.el7.centos will be installed\\n--\u003e Processing Dependency: docker-ce-selinux \u003e= 17.03.2.ce-1.el7.centos for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64\\n--\u003e Processing Dependency: libltdl.so.7()(64bit) for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64\\n--\u003e Running transaction check\\n---\u003e Package docker-ce-selinux.noarch 0:17.03.2.ce-1.el7.centos will be installed\\n---\u003e Package libtool-ltdl.x86_64 0:2.4.2-22.el7_3 will be installed\\n--\u003e Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux\\n--\u003e Restarting Dependency Resolution with new changes.\\n--\u003e Running transaction check\\n---\u003e Package container-selinux.noarch 2:2.55-1.el7 will be updated\\n---\u003e Package container-selinux.noarch 2:2.66-1.el7 will be an update\\n--\u003e Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux\\n--\u003e Finished Dependency Resolution\\n You could try using --skip-broken to work around the problem\\n You could try running: rpm -Va --nofiles --nodigest\\n\" ] } 解决方法： 卸载旧的docker版本，由kubespary自动安装。\nsudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 参考文章：\nhttps://github.com/kubernetes-incubator/kubespray https://github.com/kubernetes-incubator/kubespray/blob/master/docs/upgrades.md ","categories":"","description":"","excerpt":"1. 环境准备 1.1. 部署机器 以下机器为虚拟机\n机器IP 主机名 角色 系统版本 备注 172.16.94.140 …","ref":"/kubernetes-notes/setup/installer/install-k8s-by-kubespray/","tags":["Kubernetes"],"title":"使用kubespray安装kubernetes"},{"body":"文件操作 更多文件操作见Go的os包。\n1. 目录操作 func Mkdir(name string, perm FileMode) error\n创建名称为 name 的目录，权限设置是 perm，例如 0777\nfunc MkdirAll(path string, perm FileMode) error 根据 path 创建多级子目录，例如 astaxie/test1/test2。\nfunc Remove(name string) error 删除名称为 name 的目录，当目录下有文件或者其他目录是会出错\nfunc RemoveAll(path string) error 根据 path 删除多级子目录，如果 path 是单个名称，那么该目录不删除\n2. 文件操作 2.1. 建立与打开文件 新建文件：\nfunc Create(name string) (file *File, err Error) 根据提供的文件名创建新的文件，返回一个文件对象，默认权限是 0666 的文件，返回的文件对象是可读写的。 ** func NewFile(fd uintptr, name string) *File** 根据文件描述符创建相应的文件，返回一个文件对象 打开文件：\nfunc Open(name string) (file *File, err Error) 该方法打开一个名称为 name 的文件，但是是只读方式，内部实现其实调用了 OpenFile。 func OpenFile(name string, flag int, perm uint32) (file *File, err Error) 打开名称为 name 的文件，flag 是打开的方式，只读、读写等，perm 是权限 2.2. 写文件 写文件函数：\nfunc (file *File) Write(b []byte) (n int, err Error) 写入 byte 类型的信息到文件 func (file *File) WriteAt(b []byte, off int64) (n int, err Error) 在指定位置开始写入 byte 类型的信息 func (file *File) WriteString(s string) (ret int, err Error) 写入 string 信息到文件 package main import ( \"fmt\" \"os\" ) func main() { userFile := \"test.txt\" fout, err := os.Create(userFile) defer fout.Close() if err != nil { fmt.Println(userFile, err) return } for i := 0; i \u003c 10; i++ { fout.WriteString(\"Just a test!\\r\\n\") fout.Write([]byte(\"Just a test!\\r\\n\")) } } 2.3. 读文件 读文件函数：\nfunc (file *File) Read(b []byte) (n int, err Error) 读取数据到 b 中\nfunc (file *File) ReadAt(b []byte, off int64) (n int, err Error) 从 off 开始读取数据到 b 中\npackage main import ( \"fmt\" \"os\" ) func main() { userFile := \"text.txt\" fl, err := os.Open(userFile) defer fl.Close() if err != nil { fmt.Println(userFile, err) return } buf := make([]byte, 1024) for { n, _ := fl.Read(buf) if 0 == n { break } os.Stdout.Write(buf[:n]) } } 2.4. 删除文件 Go 语言里面删除文件和删除文件夹是同一个函数\nfunc Remove(name string) Error 调用该函数就可以删除文件名为 name 的文件 ","categories":"","description":"","excerpt":"文件操作 更多文件操作见Go的os包。\n1. 目录操作 func Mkdir(name string, perm FileMode) …","ref":"/golang-notes/text/file/","tags":["Golang"],"title":"文件操作"},{"body":"1. go pprof工具简介 在 Go 语言中，PProf 是用于可视化和分析性能分析数据的工具，PProf 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）。\nruntime/pprof：采集程序（非 Server）的指定区块的运行数据进行分析。 net/http/pprof：基于 HTTP Server 运行，并且可以采集运行时数据进行分析。 1.1. 分析内容 cpu（CPU Profiling）: $HOST/debug/pprof/profile，默认进行 30s 的 CPU Profiling，得到一个分析用的 profile 文件 block（Block Profiling）：$HOST/debug/pprof/block，查看导致阻塞同步的堆栈跟踪 goroutine：$HOST/debug/pprof/goroutine，查看当前所有运行的 goroutines 堆栈跟踪 heap（Memory Profiling）: $HOST/debug/pprof/heap，查看活动对象的内存分配情况 mutex（Mutex Profiling）：$HOST/debug/pprof/mutex，查看导致互斥锁的竞争持有者的堆栈跟踪 threadcreate：$HOST/debug/pprof/threadcreate，查看创建新OS线程的堆栈跟踪 1.2. 分析方式 go tool pprof命令交互方式\nweb网页方式查看\nhttp://ip:port/debug/pprof/ 2. 代码配置pprof 2.1. Gin框架集成pprof 调用github.com/gin-contrib/pprof，执行pprof.Register(*gin.Engine)。\n示例代码：\nimport ( \"github.com/gin-contrib/pprof\" \"github.com/gin-gonic/gin\" ) type server struct { conf *config.Config gin *gin.Engine } func (s *server) setupServer() *gin.Engine { // 注册pprof pprof.Register(s.gin) // register routers s.setupRoutes() return s.gin } 2.2. 非Gin框架集成pprof import ( \"net/http\" \"net/http/pprof\" \"github.com/gorilla/mux\" ) // Install adds the Profiling webservice to the given mux. func Install(c *mux.Router) { c.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) c.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) c.HandleFunc(\"/debug/pprof/trace\", pprof.Trace) c.HandleFunc(\"/debug/pprof\", redirectTo(\"/debug/pprof/\")) c.PathPrefix(\"/debug/pprof/\").HandlerFunc(pprof.Index) } func RegisterPprof(){ // NewRouter muxHandler := mux.NewRouter() // register handler for pprof Install(muxHandler) } 3. go tool pprof 3.1. 内存 go tool pprof http://ip:port/debug/pprof/heap 示例：\n# go tool pprof http://ip:port/debug/pprof/heap Fetching profile over HTTP from http://ip:port/debug/pprof/heap Saved profile in /root/pprof/pprof.yurt-tunnel-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz File: yurt-tunnel-server Type: inuse_space Time: May 10, 2023 at 11:03am (+08) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 46.08GB, 94.72% of 48.65GB total Dropped 319 nodes (cum \u003c= 0.24GB) Showing top 10 nodes out of 31 flat flat% sum% cum cum% 25.91GB 53.27% 53.27% 25.91GB 53.27% bufio.NewWriterSize (inline) 12.91GB 26.54% 79.80% 12.91GB 26.54% bufio.NewReaderSize 1.67GB 3.43% 83.23% 1.74GB 3.59% net/textproto.(*Reader).ReadMIMEHeader 1.26GB 2.59% 85.83% 1.26GB 2.59% runtime.malg 1.22GB 2.51% 88.34% 5.72GB 11.75% net/http.(*conn).readRequest 0.86GB 1.77% 90.11% 3.39GB 6.96% net/http.readRequest 0.68GB 1.39% 91.50% 13.72GB 28.20% sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP 0.58GB 1.20% 92.70% 0.86GB 1.78% context.propagateCancel 0.51GB 1.06% 93.76% 0.51GB 1.06% net/http.(*Server).newConn 0.47GB 0.96% 94.72% 1.33GB 2.74% context.WithCancel 3.2. CPU go tool pprof http://ip:port/debug/pprof/profile 示例：\n# go tool pprof http://ip:port/debug/pprof/profile Fetching profile over HTTP from http://ip:port/debug/pprof/profile Saved profile in /root/pprof/pprof.yurt-tunnel-server.samples.cpu.001.pb.gz File: yurt-tunnel-server Type: cpu Time: May 10, 2023 at 10:58am (+08) Duration: 30.14s, Total samples = 35.72s (118.52%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 25120ms, 70.32% of 35720ms total Dropped 240 nodes (cum \u003c= 178.60ms) Showing top 10 nodes out of 58 flat flat% sum% cum cum% 12100ms 33.87% 33.87% 12120ms 33.93% runtime.(*lfstack).pop (inline) 3360ms 9.41% 43.28% 3360ms 9.41% runtime.(*lfstack).push 2200ms 6.16% 49.44% 2200ms 6.16% runtime.pageIndexOf (inline) 1350ms 3.78% 53.22% 1520ms 4.26% runtime.findObject 1350ms 3.78% 57.00% 3490ms 9.77% runtime.scanobject 1120ms 3.14% 60.13% 3550ms 9.94% runtime.sweepone 1000ms 2.80% 62.93% 10850ms 30.38% runtime.scanblock 910ms 2.55% 65.48% 1310ms 3.67% runtime.step 890ms 2.49% 67.97% 890ms 2.49% runtime.markBits.isMarked (inline) 840ms 2.35% 70.32% 20300ms 56.83% runtime.gentraceback 4. 生成火焰图 安装graphviz，用于生成火焰图。\n# ubuntu apt-get install -y graphviz # centos yum install -y graphviz # mac brew install graphviz 当执行go pprof的命令时，会自动生成.pb.gz文件，例如：\n# 内存 Saved profile in /root/pprof/pprof.yurt-tunnel-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz # CPU Saved profile in /root/pprof/pprof.yurt-tunnel-server.samples.cpu.001.pb.gz 将.pb.gz文件拷贝到mac本地，执行以下命令，在浏览器查看相关视图。\n$ go tool pprof -http=:8081 pprof.yurt-tunnel-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz Serving web UI on http://localhost:8081 4.1. 内存分布图 4.2. 火焰图 5. 查看goroutine泄露 可以通过访问web地址的/debug/pprof/goroutine路径，查看goroutine的详细分布情况。goroutine数量分布过多的地方可能会存在内存泄露的情况。\n示例：\ngoroutine profile: total 3337450 3336630 @ 0x437fb6 0x40640c 0x405e38 0x1406b18 0x7c19bb 0x7bd528 0x4684c1 # 0x1406b17 sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP+0xbb7 /go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:105 # 0x7c19ba net/http.serverHandler.ServeHTTP+0x43a /usr/local/go/src/net/http/server.go:2878 # 0x7bd527 net/http.(*conn).serve+0xb07 /usr/local/go/src/net/http/server.go:1929 195 @ 0x437fb6 0x43095e 0x462d69 0x4c90b2 0x4ca41a 0x4ca408 0x525649 0x536665 0x7b7b8d 0x4fd5e6 0x140709a 0x7c19bb 0x7bd528 0x4684c1 # 0x462d68 internal/poll.runtime_pollWait+0x88 /usr/local/go/src/runtime/netpoll.go:229 # 0x4c90b1 internal/poll.(*pollDesc).wait+0x31 /usr/local/go/src/internal/poll/fd_poll_runtime.go:84 # 0x4ca419 internal/poll.(*pollDesc).waitRead+0x259 /usr/local/go/src/internal/poll/fd_poll_runtime.go:89 # 0x4ca407 internal/poll.(*FD).Read+0x247 /usr/local/go/src/internal/poll/fd_unix.go:167 # 0x525648 net.(*netFD).Read+0x28 /usr/local/go/src/net/fd_posix.go:56 # 0x536664 net.(*conn).Read+0x44 /usr/local/go/src/net/net.go:183 # 0x7b7b8c net/http.(*connReader).Read+0x16c /usr/local/go/src/net/http/server.go:780 # 0x4fd5e5 bufio.(*Reader).Read+0x105 /usr/local/go/src/bufio/bufio.go:213 # 0x1407099 sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP+0x1139 /go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:138 # 0x7c19ba net/http.serverHandler.ServeHTTP+0x43a /usr/local/go/src/net/http/server.go:2878 # 0x7bd527 net/http.(*conn).serve+0xb07 /usr/local/go/src/net/http/server.go:1929 上述goroutine的分布超过300万个，主要都分布在第一个部分，因此可以得出以下可能的结论：\n/go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:105 代码可能存在内存泄露的情况。\n# 0x1406b17 sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP+0xbb7 /go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:105 因此我们追踪tunnel.go:105的源码:\nselect { #105 case \u003c-connection.connected: // Waiting for response before we begin full communication. } select 没有返回，导致goroutine不断累计。\n5.1. 解决goroutine泄露 以上分析的代码来自apiserver-network-proxy。\n我们可以查看该内存泄露的issue:\nhttps://github.com/kubernetes-sigs/apiserver-network-proxy/pull/270 修复内存泄露的commit：\nFix obscure HTTP CONNECT goroutine leak by jveski · Pull Request #270 · kubernetes-sigs/apiserver-network-proxy · GitHub 涉及代码修改如下：\nselect { case \u003c-connection.connected: // Waiting for response before we begin full communication. case \u003c-closed: // Connection was closed before being established } 增加closed类型，退出goroutine。\n参考：\nGolang 大杀器之性能剖析 PProf - SegmentFault 思否\nhttps://geektutu.com/post/hpg-pprof.html\nhttps://coder.today/tech/2018-11-10_profiling-your-golang-app-in-3-steps/\nProfiling Go Programs - The Go Programming Language\nGo 大杀器之性能剖析 PProf（上） | Go 语言编程之旅\nDockerd资源泄露系列\n","categories":"","description":"","excerpt":"1. go pprof工具简介 在 Go 语言中，PProf 是用于可视化和分析性能分析数据的工具，PProf …","ref":"/golang-notes/framework/go-pprof-usage/","tags":"","title":"性能分析之go pprof工具使用"},{"body":"在 Go 中实现一个简单的 Rate Limiter（限流器） 可以有多种方式，常见的有基于 令牌桶（Token Bucket） 和 漏桶（Leaky Bucket） 的实现。下面我们先介绍一种经典方式 —— 令牌桶算法 实现。\n1. 令牌桶算法 “令牌桶算法（Token Bucket）”是一种 限流算法，用于控制请求的速率，是常见于 API 网关、负载均衡器、流控中间件等的核心策略之一。\n1.1. 核心思想 系统以固定速率往“桶”里放令牌，每次请求必须拿到一个令牌才能被处理，否则被拒绝（限流）。\n名称 含义 桶（Bucket） 保存令牌的容器，最多可以装 burst 个令牌（突发容量） 令牌（Token） 每个令牌代表一次允许的操作（如一次 HTTP 请求） 速率（Rate） 向桶中添加令牌的速度，比如每秒放 5 个 请求到来时 若桶中有令牌，请求就“取出一个令牌”被允许；否则就被拒绝或等待 1.2. 动态行为图示 假设：每秒生成1个令牌，桶最多装3个。\n时间：0s 桶：[●] 请求：允许（消耗1个令牌） 时间：1s 桶：[●] 请求：允许（消耗1个令牌） 时间：2s 桶：[●●] 请求：允许 时间：3s 桶：[●●●] 请求：允许 时间：4s 桶：[●●●] 请求：没有消费，桶满了（不再增加） 时间：5s 桶：[●●●] 来5个请求，只允许3个，其余被限流 1.3. 和漏桶算法的对比 比较项 令牌桶（Token Bucket） 漏桶（Leaky Bucket） 控制方式 控制请求进入速率 控制请求处理速率 支持突发请求 ✅ 支持 ❌ 严格匀速处理 应用场景 限流（API、接口） 排队（网络、处理任务） 1.4. 应用场景举例 API 请求速率控制（每个用户最多1秒10次）\n登录/注册防暴力攻击（每 IP 限1分钟5次）\nCDN 边缘限流（防止源站被打爆）\n后端任务投递（防止 RabbitMQ 拥堵）\n2. 实现简单 Token Bucket 限流器 2.1. 使用 time.Ticker 实现 package main import ( \"fmt\" \"time\" ) type RateLimiter struct { tokens chan struct{} ticker *time.Ticker maxTokens int refillPeriod time.Duration } func NewRateLimiter(rps int, burst int) *RateLimiter { rl := \u0026RateLimiter{ tokens: make(chan struct{}, burst), ticker: time.NewTicker(time.Second / time.Duration(rps)), maxTokens: burst, refillPeriod: time.Second / time.Duration(rps), } // 初始化 token 桶 for i := 0; i \u003c burst; i++ { rl.tokens \u003c- struct{}{} } // 后台协程定时放 token go func() { for range rl.ticker.C { select { case rl.tokens \u003c- struct{}{}: default: // 桶满时丢弃 token } } }() return rl } // Allow 会阻塞直到有 token 可用（可改成 TryAllow 非阻塞版本） func (rl *RateLimiter) Allow() bool { select { case \u003c-rl.tokens: return true default: return false } } func main() { limiter := NewRateLimiter(5, 10) // 每秒 5 个请求，最多缓冲 10 个 for i := 0; i \u003c 20; i++ { if limiter.Allow() { fmt.Println(\"Request\", i, \"allowed at\", time.Now()) } else { fmt.Println(\"Request\", i, \"denied at\", time.Now()) } time.Sleep(100 * time.Millisecond) } } 2.2. 使用Go rate包实现 Go 的 golang.org/x/time/rate 包就实现了 令牌桶算法，例如：\n// 每秒5个令牌，最多能装10个（允许突发10次） limiter := rate.NewLimiter(5, 10) 你可以使用 .Allow() 来判断是否获得令牌：\nif limiter.Allow() { // 有令牌，请求通过 } else { // 没令牌，请求被限流 } 3. 实现Gin按用户或 IP 限流中间件 3.1. 限流器结构（使用 rate.Limiter） package ratelimiter import ( \"sync\" \"golang.org/x/time/rate\" ) type RateLimiter struct { rate rate.Limit burst int buckets map[string]*rate.Limiter mutex sync.Mutex } func NewRateLimiter(r rate.Limit, b int) *RateLimiter { return \u0026RateLimiter{ rate: r, burst: b, buckets: make(map[string]*rate.Limiter), } } func (rl *RateLimiter) getLimiter(key string) *rate.Limiter { rl.mutex.Lock() defer rl.mutex.Unlock() if limiter, exists := rl.buckets[key]; exists { return limiter } limiter := rate.NewLimiter(rl.rate, rl.burst) rl.buckets[key] = limiter return limiter } func (rl *RateLimiter) Allow(key string) bool { return rl.getLimiter(key).Allow() } 3.2. Gin 中间件 package ratelimiter import ( \"net/http\" \"github.com/gin-gonic/gin\" ) type KeyFunc func(c *gin.Context) string func Middleware(rl *RateLimiter, keyFn KeyFunc) gin.HandlerFunc { return func(c *gin.Context) { key := keyFn(c) if !rl.Allow(key) { c.AbortWithStatusJSON(http.StatusTooManyRequests, gin.H{ \"error\": \"rate limit exceeded\", }) return } c.Next() } } 3.3. 在 main.go 中使用（按 IP 限流） package main import ( \"github.com/gin-gonic/gin\" \"golang.org/x/time/rate\" \"your_project/ratelimiter\" ) func main() { // 每秒2个请求，最多突发5个 rl := ratelimiter.NewRateLimiter(2, 5) r := gin.Default() r.Use(ratelimiter.Middleware(rl, func(c *gin.Context) string { return c.ClientIP() // 或使用 c.GetString(\"userID\") 实现按用户限流 })) r.GET(\"/hello\", func(c *gin.Context) { c.JSON(200, gin.H{\"msg\": \"hello\"}) }) r.Run(\":8080\") } ","categories":"","description":"","excerpt":"在 Go 中实现一个简单的 Rate Limiter（限流器） 可以有多种方式，常见的有基于 令牌桶（Token Bucket） 和 漏 …","ref":"/golang-notes/web/go-ratelimiter/","tags":["Golang"],"title":"Go实现限流器"},{"body":"1. 部署qemu-system-x86_64 # 更新包 sudo apt-get update # 安装QEMU和KVM相关的包。KVM（Kernel-based Virtual Machine）可以显著提高QEMU的性能。 sudo apt-get install -y qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils # 安装qemu-system-x86 sudo apt-get install -y qemu qemu-system-x86 # 为了在非root用户下使用QEMU/KVM，需要将当前用户添加到 libvirt 和 kvm 组。 sudo usermod -aG libvirt $(whoami) sudo usermod -aG kvm $(whoami) # 查看版本 qemu-system-x86_64 --version # 验证命令 virsh list --all ","categories":"","description":"","excerpt":"1. 部署qemu-system-x86_64 # 更新包 sudo apt-get update # 安装QEMU和KVM相关的 …","ref":"/kubernetes-notes/kvm/qemu-system/","tags":["KubeVirt"],"title":"qemu创建虚拟机"},{"body":"1. Raid是什么 RAID英文全称Redundant Array of Independent Disk，翻译过来就是“独立磁盘冗余系统”。RAID是一种可提高性能或提供容错功能的磁盘子系统。\nRAID的基本思想是将多个容量较小、速度较慢、可靠性较差的廉价磁盘，组合成一个磁盘组，从而以较低的成本获得与昂贵的大容量、高速磁盘相似的性能。\n优势：\n高性能\n可靠性可用性\n大容量\n可管理性\n2. Raid的基本原理 架构上，通过RAID控制器就将多块磁盘组合，在操作系统层我们看到的是一个或多个逻辑磁盘。\n技术上，RAID主要采用磁盘镜像技术、条带化技术和奇偶校验技术实现高性能、可靠性、容错能力和扩展性。\n磁盘镜像技术：将数据复制到多个磁盘，一方面可以提高可靠性，另一方面可以提高读性能。因为需要写入多个磁盘会导致写性能降低。\n数据条带技术：将数据分片保存到多个不同的磁盘，多个数据分片共同组成完整的数据。主要用于提示磁盘IO性能，当访问数据时可以同时对多个磁盘进行读取。\n数据校验技术：利益冗余数据对数据进行校验和修复。冗余数据通常采用海明码、异或操作等算法来计算获得。可以提高磁盘阵列的可靠性和容错能力。不过，数据校验需要从多处读取数据并进行计算和对比，会影响系统性能。\n3. Raid的级别 RAID级别命名方式如，RAID 0、RAID 1/RAID 5、RAID 10等。RAID每一个级别代表一种RAID组合实现方法和技术，级别之间并无高低之分。不同等级的 RAID 采用其中一个或多个技术，得到的是不同的容量、I/O性能、可靠性、可用性。在决定选择哪种RAID级别之前，需要深入理解系统需求，根据自身情况进行合理选择，综合评估可靠性、性能和成本来进行折中的选择。\nRaid等级 Raid0 Raid1 Raid3 Raid5 Raid6 Raid10 别名 条带 镜像 专用奇偶校验条带 分布奇偶校验条带 双重奇偶校验条带 镜像+条带 容错性 无 有 有 有 有 有 冗余类型 无 有 有 有 有 有 热备份选择 无 有 有 有 有 有 读性能 高 低 高 高 高 高 随机写性能 高 低 低 一般 低 一般 连续写性能 高 低 低 低 低 一般 需要磁盘数 n\u003e=1 2n(n\u003e=1) n\u003e=3 n\u003e=3 n\u003e=4 2n\u003e=4 可用容量 全部 50% （n-1）/n （n-1）/n （n-2）/n 50% 4. Raid级别介绍 4.1. Raid0（使用较多） Raid0是将多块物理磁盘组合成一个大的逻辑磁盘，在读写的时候会以独立的方式对N块磁盘进行读写，因此执行效率非常高。理论上读写的性能是单块磁盘的N倍。但是单块磁盘的损坏会导致数据丢失，无法恢复。因此适用于对可靠性不高，读写性能要求高的场景中。\n特点：RAID 0通过条带化技术，将数据分散到多个磁盘上，从而提高存储性能。它不提供数据冗余，因此没有容错能力。\n适用场景：适用于对性能要求极高，但对数据安全性要求不高的场景，如视频编辑、高性能计算等。\n4.2. Raid1（镜像） Raid1 是磁盘阵列中单位成本最高的一种方式。因为它的原理是在往磁盘写数据的时候，将同一份数据无差别的写两份到磁盘，分别写到工作磁盘和镜像磁盘，那么它的实际空间使用率只有50%了，两块磁盘当做一块用，这是一种比较昂贵的方案。可靠性高，但是性能低。\n特点：RAID 1通过镜像技术，将数据同时写入两个或多个相同的磁盘.中，提供数据冗余和容错能力。如果一个磁盘发生故障，另一个磁盘上的数据仍然可用。\n适用场景：适用于对数据安全性要求较高的场景，如金融机构、医疗机构等。\n4.3. Raid5（使用最多） Raid5是兼具Raid0和Raid1的优点，在性能和可靠性上兼具的一种方式，也是使用最多的一种方案。\nRaid5方案是总共有N块磁盘，会将要写入的数据分成N份，并发的写入到N块磁盘中，同时还将数据的校验码信息也写入到这N块磁盘中（数据与对应的校验码信息必须得分开存储在不同的磁盘上）。一旦某一块磁盘损坏了，就可以用剩下的数据和对应的奇偶校验码信息去恢复损坏的数据。\nRAID5的方式，最少需要三块磁盘来组建磁盘阵列，允许最多同时坏一块磁盘。如果有两块磁盘同时损坏了，那数据就无法恢复了。\n特点：RAID 5结合了条带化和数据冗余技术。它将数据分割成块并分布在多个磁盘上，同时使用一个或多个磁盘存储冗余信息(校验块)。如果一个磁盘发生故障，系统可以使用冗余信息来重建故障磁盘上的数据。\n适用场景：适用于对数据安全性和性能都有一定要求的场景，如企业数据库、文件服务器等。\n4.4. Raid6 RAID6在RAID5的基础上再次改进，引入了双重校验的概念。RAID6除了每块磁盘上都有同级数据XOR校验区以外，还有针对每个数据块的XOR校验区，这样的话，相当于每个数据块有两个校验保护措施，因此数据的冗余性更高了。但是RAID6的这种设计也带来了很高的复杂度，虽然数据冗余性好，读取的效率也比较高，但是写数据的性能就很差。因此RAID6在实际环境中应用的比较少。\n特点：RAID 6与RAID 5类似，但使用了两个冗余信息来提供更高的容错能力。这意味着即使两个磁盘同时发生故障，系统仍然可以使用冗余信息来重建数据。\n适用场景：适用于对数据安全性要求非常高的场景，如大型数据库、关键业务系统等。\n4.5. Raid10 RAID10其实就是RAID1与RAID0的一个合体。RAID10兼备了RAID1和RAID0的有优点。\n首先基于RAID1模式将磁盘分为2份，当要写入数据的时候，将所有的数据在两份磁盘上同时写入，相当于写了双份数据，起到了数据保障的作用。且在每一份磁盘上又会基于RAID0技术讲数据分为N份并发的读写，这样也保障了数据的效率。RAID10模式是有一半的磁盘空间用于存储冗余数据的，浪费的很严重，因此用的也不是很多。\n5. 硬Raid和软Raid区别 硬raid主要是通过厂商的硬件raid卡控制器来实现，而软raid主要是操作系统的raid命令来实现（例如 mdadm 命令）。\n以下是2者的区别：\n分类 硬raid 软raid 实现方式 硬件 RAID 使用专用的 RAID 控制器卡，这是一块独立的硬件，通常插入到服务器或工作站的 PCIe 插槽中。该控制器处理所有 RAID 计算和磁盘管理任务，减轻了主机 CPU 的负担。 软件 RAID 通过操作系统提供的 RAID 功能来实现（如 Linux 的 mdadm 或 Windows 的 Storage Spaces）。RAID 操作由主机的 CPU 处理。 性能 硬件 RAID 控制器有专用的处理器和缓存，用于处理 RAID 操作，因此通常提供更高的性能，特别是在高负载或复杂 RAID 配置（如 RAID 5 或 RAID 6）下。 由于 RAID 操作由主机 CPU 处理，软件 RAID 在高负载或复杂 RAID 配置下的性能可能不如硬件 RAID。 系统依赖性 RAID 配置存储在控制器上，因此更换主机系统或操作系统不会影响 RAID 阵列的数据完整性。 RAID 配置依赖于操作系统，因此更换操作系统或重新安装系统可能需要重新配置 RAID 阵列。 成本 硬件 RAID 控制器通常价格较高，增加了系统的总体成本。 不需要额外的硬件，降低了成本。对于预算有限的小型企业或个人用户来说，这是一个经济的选择。 数据安全与可靠性 硬件 RAID 提供更高的可靠性和数据保护功能，如热备用盘和电池备份缓存。 软件 RAID 在数据安全和可靠性上依赖于操作系统的实现和配置。 灵活性 依赖不同厂商不同的raid卡命令，配置比较复杂 软件 RAID 配置和管理更灵活，可以通过操作系统的命令行工具或图形界面轻松实现。 高级功能 硬件 RAID 控制器通常提供高级功能，如热插拔、在线扩展、热备用盘、硬件级别的加密和电池备份缓存。 软件 RAID 通常缺乏一些高级功能，如硬件 RAID 提供的电池备份缓存和专用处理器。 6. 如何配置硬Raid 配置硬件raid可以通过raid配置界面操作，也可以通过厂商提供的cli工具配置硬Raid。具体可参考创建硬件Raid\n参考：\nhttps://www.chinastor.com/baike/raid/ https://zhuanlan.zhihu.com/p/51170719 https://mp.weixin.qq.com/s/xWD5p7pmzdl_YpMXY5YO4g ","categories":"","description":"","excerpt":"1. Raid是什么 RAID英文全称Redundant Array of Independent Disk，翻译过来就是“独立磁盘冗余系 …","ref":"/linux-notes/disk/disk-raid/","tags":["disk"],"title":"Raid介绍"},{"body":"1. Tunnel-Agent简介 tunnel-agent是通过daemonset部署在每个worker节点，通过grpc协议与云端的tunnel-server建立连接。以下分析tunnel-agent的源码逻辑。\n常用的启动参数：\n- args: - --node-name=$(NODE_NAME) - --node-ip=$(POD_IP) - --tunnelserver-addr=tunnel-server-address[ip:port] - --v=2 command: - yurt-tunnel-agent 2. NewYurttunnelAgentCommand NewYurttunnelAgentCommand还是常用命令代码三板斧，此处不做展开，直接分析Run函数。\n// NewYurttunnelAgentCommand creates a new yurttunnel-agent command func NewYurttunnelAgentCommand(stopCh \u003c-chan struct{}) *cobra.Command { agentOptions := options.NewAgentOptions() // 已经删除非重要的代码 cmd := \u0026cobra.Command{ RunE: func(c *cobra.Command, args []string) error { cfg, err := agentOptions.Config() if err := Run(cfg.Complete(), stopCh); err != nil { return err } }, } agentOptions.AddFlags(cmd.Flags()) return cmd } 3. Run Run函数即启动一个agent服务，主要包含以下几个步骤：\n先获取配置项tunnelserver-addr中的地址，如果地址不存在，则获取x-tunnel-server-svc的service 地址。（说明：一般情况下，tunnel-server跟agent不在同一个网络域，因此网络会不通，所以一般需要配置独立且可连通的地址，可以通过Nginx转发）\nagent通过host的网络模式运行在宿主机上，启动证书manager。并等待证书生成。\n生成连接tunnel-server的证书。\n启动 yurttunnel-agent。\n启动meta server。\n// Run starts the yurttunel-agent func Run(cfg *config.CompletedConfig, stopCh \u003c-chan struct{}) error { // 1. get the address of the yurttunnel-server tunnelServerAddr = cfg.TunnelServerAddr if tunnelServerAddr == \"\" { if tunnelServerAddr, err = serveraddr.GetTunnelServerAddr(cfg.Client); err != nil { return err } } // 2. create a certificate manager // As yurttunnel-agent will run on the edge node with Host network mode, // we can use the status.podIP as the node IP nodeIP := os.Getenv(constants.YurttunnelAgentPodIPEnv) agentCertMgr, err = certfactory.NewCertManagerFactory(cfg.Client).New(\u0026certfactory.CertManagerConfig{ ComponentName: projectinfo.GetAgentName(), CertDir: cfg.CertDir, SignerName: certificatesv1.KubeAPIServerClientSignerName, CommonName: constants.YurtTunnelAgentCSRCN, Organizations: []string{constants.YurtTunnelCSROrg}, DNSNames: []string{os.Getenv(\"NODE_NAME\")}, IPs: []net.IP{net.ParseIP(nodeIP)}, }) agentCertMgr.Start() // 2.1. waiting for the certificate is generated _ = wait.PollUntil(5*time.Second, func() (bool, error) { if agentCertMgr.Current() != nil { return true, nil } klog.Infof(\"certificate %s not signed, waiting...\", projectinfo.GetAgentName()) return false, nil }, stopCh) // 3. generate a TLS configuration for securing the connection to server tlsCfg, err := certmanager.GenTLSConfigUseCertMgrAndCA(agentCertMgr, tunnelServerAddr, constants.YurttunnelCAFile) // 4. start the yurttunnel-agent ta := agent.NewTunnelAgent(tlsCfg, tunnelServerAddr, cfg.NodeName, cfg.AgentIdentifiers) ta.Run(stopCh) // 5. start meta server util.RunMetaServer(cfg.AgentMetaAddr) \u003c-stopCh return nil } 4. TunnelAgent TunnelAgent与tunnel-server建立tunnel，接收server的请求，并转发给kubelet。\n// TunnelAgent sets up tunnel to TunnelServer, receive requests // from tunnel, and forwards requests to kubelet type TunnelAgent interface { Run(\u003c-chan struct{}) } // NewTunnelAgent generates a new TunnelAgent func NewTunnelAgent(tlsCfg *tls.Config, tunnelServerAddr, nodeName, agentIdentifiers string) TunnelAgent { ata := anpTunnelAgent{ tlsCfg: tlsCfg, tunnelServerAddr: tunnelServerAddr, nodeName: nodeName, agentIdentifiers: agentIdentifiers, } return \u0026ata } 5. anpTunnelAgent.Run anpTunnelAgent使用apiserver-network-proxy包来实现tunnel逻辑。项目具体参考：https://github.com/kubernetes-sigs/apiserver-network-proxy)\n代码：/pkg/yurttunnel/agent/anpagent.go\n// RunAgent runs the yurttunnel-agent which will try to connect yurttunnel-server func (ata *anpTunnelAgent) Run(stopChan \u003c-chan struct{}) { dialOption := grpc.WithTransportCredentials(credentials.NewTLS(ata.tlsCfg)) cc := \u0026anpagent.ClientSetConfig{ Address: ata.tunnelServerAddr, // 指定反向连接的目标地址 AgentID: ata.nodeName, AgentIdentifiers: ata.agentIdentifiers, SyncInterval: 5 * time.Second, ProbeInterval: 5 * time.Second, DialOptions: []grpc.DialOption{dialOption}, ServiceAccountTokenPath: \"\", } // 调用apiserver-network-proxy的包来创建双向的grpc连接。 cs := cc.NewAgentClientSet(stopChan) cs.Serve() klog.Infof(\"start serving grpc request redirected from %s: %s\", projectinfo.GetServerName(), ata.tunnelServerAddr) } 以下是apiserver-network-proxy的源码分析。\n6. apiserver-network-proxy.Client分析 具体代码参考：\nhttps://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/clientset.go 通过NewAgentClientSet创建一个client结构体。\nfunc (cc *ClientSetConfig) NewAgentClientSet(stopCh \u003c-chan struct{}) *ClientSet { return \u0026ClientSet{ clients: make(map[string]*Client), agentID: cc.AgentID, agentIdentifiers: cc.AgentIdentifiers, address: cc.Address, syncInterval: cc.SyncInterval, probeInterval: cc.ProbeInterval, dialOptions: cc.DialOptions, serviceAccountTokenPath: cc.ServiceAccountTokenPath, stopCh: stopCh, } } 6.1. client.Serve client.Serve运行一个sync的goroutine的常驻进程，再调用syncOnce函数。\n// 运行一个sync的goroutine func (cs *ClientSet) Serve() { go cs.sync() } // sync makes sure that #clients \u003e= #proxy servers func (cs *ClientSet) sync() { defer cs.shutdown() backoff := cs.resetBackoff() var duration time.Duration for { if err := cs.syncOnce(); err != nil { klog.ErrorS(err, \"cannot sync once\") duration = backoff.Step() } else { backoff = cs.resetBackoff() duration = wait.Jitter(backoff.Duration, backoff.Jitter) } time.Sleep(duration) select { case \u003c-cs.stopCh: return default: } } } syncOnce运行了真正执行grpc通信的client。\nfunc (cs *ClientSet) syncOnce() error { if cs.serverCount != 0 \u0026\u0026 cs.ClientsCount() \u003e= cs.serverCount { return nil } // 创建封装的grpc client c, serverCount, err := cs.newAgentClient() if err != nil { return err } if cs.serverCount != 0 \u0026\u0026 cs.serverCount != serverCount { klog.V(2).InfoS(\"Server count change suggestion by server\", \"current\", cs.serverCount, \"serverID\", c.serverID, \"actual\", serverCount) } cs.serverCount = serverCount if err := cs.AddClient(c.serverID, c); err != nil { klog.ErrorS(err, \"closing connection failure when adding a client\") c.Close() return nil } klog.V(2).InfoS(\"sync added client connecting to proxy server\", \"serverID\", c.serverID) // 运行封装后的grpc 连接 go c.Serve() return nil } 7. Grpc client 代码参考：\nhttps://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/client.go 7.1. newAgentClient newAgentClient初始化一个grpc client，并启动连接。\nfunc newAgentClient(address, agentID, agentIdentifiers string, cs *ClientSet, opts ...grpc.DialOption) (*Client, int, error) { a := \u0026Client{ cs: cs, address: address, agentID: agentID, agentIdentifiers: agentIdentifiers, opts: opts, probeInterval: cs.probeInterval, stopCh: make(chan struct{}), serviceAccountTokenPath: cs.serviceAccountTokenPath, connManager: newConnectionManager(), } // 启动client的连接 serverCount, err := a.Connect() if err != nil { return nil, 0, err } return a, serverCount, nil } 7.2. connect Connect使grpc连接代理服务器。它返回服务器ID\n// Connect makes the grpc dial to the proxy server. It returns the serverID // it connects to. func (a *Client) Connect() (int, error) { // 运行grpc dial连接 conn, err := grpc.Dial(a.address, a.opts...) if err != nil { return 0, err } // 已删除非必要的代码 // 创建stream stream, err := agent.NewAgentServiceClient(conn).Connect(ctx) if err != nil { conn.Close() /* #nosec G104 */ return 0, err } serverID, err := serverID(stream) if err != nil { conn.Close() /* #nosec G104 */ return 0, err } serverCount, err := serverCount(stream) if err != nil { conn.Close() /* #nosec G104 */ return 0, err } a.conn = conn a.stream = stream a.serverID = serverID klog.V(2).InfoS(\"Connect to\", \"server\", serverID) return serverCount, nil } 7.3. Serve() Serve主要启动grpc双向传输通道的goroutine, 主要包括 send（发）和recv（收）2个操作。\nfunc (a *Client) Serve() { // 已经删除次要代码 for { // 收包 pkt, err := a.Recv() klog.V(5).InfoS(\"[tracing] recv packet\", \"type\", pkt.Type) // 根据不同包类型进行不同的处理 switch pkt.Type { case client.PacketType_DIAL_REQ: resp := \u0026client.Packet{ Type: client.PacketType_DIAL_RSP, Payload: \u0026client.Packet_DialResponse{DialResponse: \u0026client.DialResponse{}}, } if err := a.Send(resp); err != nil { } // 运行proxy go a.remoteToProxy(connID, ctx) go a.proxyToRemote(connID, ctx) // 接收到数据 case client.PacketType_DATA: data := pkt.GetData() ctx, ok := a.connManager.Get(data.ConnectID) if ok { ctx.dataCh \u003c- data.Data } case client.PacketType_CLOSE_REQ: // 已删除 } } } 8. remoteToProxy和proxyToRemote remoteToProxy\nfunc (a *Client) remoteToProxy(connID int64, ctx *connContext) { defer ctx.cleanup() var buf [1 \u003c\u003c 12]byte resp := \u0026client.Packet{ Type: client.PacketType_DATA, } for { n, err := ctx.conn.Read(buf[:]) klog.V(4).InfoS(\"received data from remote\", \"bytes\", n, \"connID\", connID) // 删除次要代码 resp.Payload = \u0026client.Packet_Data{Data: \u0026client.Data{ Data: buf[:n], ConnectID: connID, }} if err := a.Send(resp); err != nil { klog.ErrorS(err, \"stream send failure\", \"connID\", connID) } } } } proxyToRemote\nfunc (a *Client) proxyToRemote(connID int64, ctx *connContext) { defer ctx.cleanup() for d := range ctx.dataCh { pos := 0 for { n, err := ctx.conn.Write(d[pos:]) if err == nil { klog.V(4).InfoS(\"write to remote\", \"connID\", connID, \"lastData\", n) break } else if n \u003e 0 { klog.ErrorS(err, \"write to remote with failure\", \"connID\", connID, \"lastData\", n) pos += n } else { if !strings.Contains(err.Error(), \"use of closed network connection\") { klog.ErrorS(err, \"conn write failure\", \"connID\", connID) } return } } } } 9. Recv() 和Send() func (a *Client) Send(pkt *client.Packet) error { a.sendLock.Lock() defer a.sendLock.Unlock() err := a.stream.Send(pkt) if err != nil \u0026\u0026 err != io.EOF { metrics.Metrics.ObserveFailure(metrics.DirectionToServer) a.cs.RemoveClient(a.serverID) } return err } func (a *Client) Recv() (*client.Packet, error) { a.recvLock.Lock() defer a.recvLock.Unlock() pkt, err := a.stream.Recv() if err != nil \u0026\u0026 err != io.EOF { metrics.Metrics.ObserveFailure(metrics.DirectionFromServer) a.cs.RemoveClient(a.serverID) } return pkt, err } 10. 总结 Tunnel-agent本质是封装了apiserver-network-proxy库，最终运行一个grpc的双向收发数据包的通道，所以**本质上tunnel是通过grpc反向建立连接，并实现双向通信的能力。**因此该反向隧道能力的也可以通过其他双向通信的协议来实现，例如websocket（类似kubeedge通过websocket来实现反向隧道）。\n参考：\n/pkg/yurttunnel/agent/anpagent.go https://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/client.go https://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/clientset.go ","categories":"","description":"","excerpt":"1. Tunnel-Agent简介 tunnel-agent是通过daemonset部署在每个worker节点，通过grpc协议与云端 …","ref":"/kubernetes-notes/edge/openyurt/code-analysis/tennel-agent-code-analysis/","tags":["OpenYurt"],"title":"OpenYurt之Tunnel-Agent源码分析"},{"body":"1. Pod的DNS策略 可以在pod中定义dnsPolicy字段来设置dns的策略。\n\"Default\": Pod 从运行所在的节点继承名称解析配置。就是该Pod的DNS配置会跟宿主机完全一致。\n\"ClusterFirst\": 如果没有配置，即为默认的DNS策略。预先把kube-dns（或CoreDNS）的信息当作预设参数写入到该Pod内的DNS配置。与配置的集群域后缀不匹配的任何 DNS 查询（例如 \"www.kubernetes.io\"） 都会由 DNS 服务器转发到上游名称服务器。\n\"ClusterFirstWithHostNet\": 对于以 hostNetwork 方式运行的 Pod，应将其 DNS 策略显式设置为 \"ClusterFirstWithHostNet\"。否则，以 hostNetwork 方式和 \"ClusterFirst\" 策略运行的 Pod 将会做出回退至 \"Default\" 策略的行为。\n\"None\": 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 dnsConfig 字段所提供的 DNS 设置。\n2. Pod DNS的配置 当 Pod 的 dnsPolicy 设置为 \"None\" 时，必须指定 dnsConfig 字段。\ndnsConfig 字段中属性：\nnameservers：将用作于 Pod 的 DNS 服务器的 IP 地址列表。 最多可以指定 3 个 IP 地址。例如 coredns的Cluster IP。\nsearches：用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的。\noptions：可选的对象列表，其中每个对象可能具有 name 属性（必需）和 value 属性（可选）。\n示例：\napiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \"None\" dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster-domain.example - my.dns.search.suffix options: - name: ndots value: \"2\" - name: edns0 通过以上配置，容器内的/etc/resolv.conf文件内容为：\nkubectl exec -it dns-example -- cat /etc/resolv.conf nameserver 1.2.3.4 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0 3. 自定义DNS服务 默认一般使用coredns来作为k8s的dns服务器。默认使用deployment的方式来运行coredns，会创建一个名为kube-dns的service，并用ClusterIP（默认为10.96.0.10）来作为集群内的pod的nameserver。\nkubelet 使用 --cluster-dns=\u003cDNS 服务 IP\u003e 标志将 DNS 解析器的信息传递给每个容器。使用 --cluster-domain=\u003c默认本地域名\u003e 标志配置本地域名。\n可查看默认配置：\n# cat /var/lib/kubelet/config.yaml ... clusterDNS: - 10.96.0.10 clusterDomain: cluster.local 总结：\n当没有给pod设置任何dns策略时，则默认使用ClusterFirst的策略，即nameserver的IP为coredns的ClusterIP。通过coredns来解析服务。\n3.1. 配置继承节点的DNS解析 如果 Pod 的 dnsPolicy 设置为 default，则它将从 Pod 运行所在节点继承名称解析配置。\n使用 kubelet 的 --resolv-conf 标志设置为宿主机的/etc/resolv.conf文件。\n3.2. 配置CoreDNS 配置\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefi: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } 配置说明：\nCorefile 配置包括以下 CoreDNS 插件：\nerrors：错误记录到标准输出。\nhealth：在 http://localhost:8080/health 处提供 CoreDNS 的健康报告。 在这个扩展语法中，lameduck 会使此进程不健康，等待 5 秒后进程被关闭。\nready：在端口 8181 上提供的一个 HTTP 端点， 当所有能够表达自身就绪的插件都已就绪时，在此端点返回 200 OK。\nkubernetes：CoreDNS 将基于服务和 Pod 的 IP 来应答 DNS 查询。 你可以在 CoreDNS 网站找到有关此插件的更多细节。\n你可以使用 ttl 来定制响应的 TTL。默认值是 5 秒钟。TTL 的最小值可以是 0 秒钟， 最大值为 3600 秒。将 TTL 设置为 0 可以禁止对 DNS 记录进行缓存。\npods insecure 选项是为了与 kube-dns 向后兼容。\n你可以使用 pods verified 选项，该选项使得仅在相同名字空间中存在具有匹配 IP 的 Pod 时才返回 A 记录。\n如果你不使用 Pod 记录，则可以使用 pods disabled 选项。\nprometheus：CoreDNS 的度量指标值以 Prometheus 格式（也称为 OpenMetrics）在 http://localhost:9153/metrics 上提供。\nforward: 不在 Kubernetes 集群域内的任何查询都将转发到预定义的解析器 (/etc/resolv.conf)。\ncache：启用前端缓存。\nloop：检测简单的转发环，如果发现死循环，则中止 CoreDNS 进程。\nreload：允许自动重新加载已更改的 Corefile。 编辑 ConfigMap 配置后，请等待两分钟，以使更改生效。\nloadbalance：这是一个轮转式 DNS 负载均衡器， 它在应答中随机分配 A、AAAA 和 MX 记录的顺序。\n3.3. 配置存根域和上游域名服务器 CoreDNS 能够使用 forward 插件配置存根域和上游域名服务器。\n示例：\n在 \"10.150.0.1\" 处运行了 Consul 域服务器， 且所有 Consul 名称都带有后缀 .consul.local。\nconsul.local:53 { errors cache 30 forward . 10.150.0.1 } 要显式强制所有非集群 DNS 查找通过特定的域名服务器（位于 172.16.0.1），可将 forward 指向该域名服务器，而不是 /etc/resolv.conf。\nforward . 172.16.0.1 完整示例；\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 172.16.0.1 cache 30 loop reload loadbalance } consul.local:53 { errors cache 30 forward . 10.150.0.1 } 4. 调试DNS问题 创建一个调试的pod\napiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: - name: dnsutils image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3 command: - sleep - \"infinity\" imagePullPolicy: IfNotPresent restartPolicy: Always 部署调试pod\n4.1. 查看Coredns服务是否正常 kubectl get svc --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... kube-dns ClusterIP 10.0.0.10 \u003cnone\u003e 53/UDP,53/TCP 1h ... 4.2. 查看/etc/resolv.conf 查看容器内dns配置是否符合预期。\nkubectl exec -ti dnsutils -- cat /etc/resolv.conf 4.3. nslookup查看解析报错 kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 Name: kubernetes.default Address 1: 10.0.0.1 参考：\nService 与 Pod 的 DNS | Kubernetes\n自定义 DNS 服务 | Kubernetes\n调试 DNS 问题 | Kubernetes\n使用 CoreDNS 进行服务发现 | Kubernetes\n自动扩缩集群 DNS 服务 | Kubernetes\nresolv.conf(5) - Linux manual page\ndns/specification.md at master · kubernetes/dns · GitHub\ndeployment/CoreDNS-k8s_version.md at master · coredns/deployment · GitHub\nforward\n","categories":"","description":"","excerpt":"1. Pod的DNS策略 可以在pod中定义dnsPolicy字段来设置dns的策略。\n\"Default\": Pod 从运行所在的节点继承名 …","ref":"/kubernetes-notes/network/pod-dns/","tags":["Network"],"title":"Pod的DNS策略"},{"body":"ingress-controller架构图 ingress-controller流程图 ApisixRoute同步逻辑 数据结构转换 参考：\nhttps://apisix.apache.org/zh/docs/ingress-controller/getting-started/\nhttps://apisix.apache.org/zh/docs/ingress-controller/design/\n","categories":"","description":"","excerpt":"ingress-controller架构图 ingress-controller流程图 ApisixRoute同步逻辑 数据结构转换 参考： …","ref":"/kubernetes-notes/network/gateway/ingress-controller-design/","tags":["ApiSix"],"title":"ingress-controller原理"},{"body":"1. 简介 macvlan可以看做是物理接口eth（父接口）的子接口，每个macvlan都拥有独立的mac地址，可以被绑定IP作为正常的网卡接口使用。通过这个特性，可以实现在一个物理网络设备绑定多个IP，每个IP拥有独立的mac地址。该特性经常被应用在容器虚拟化中（容器可以配置macvlan的网络，将macvlan interface移动到容器的namespace中）。\n示意图：\n2. 四种工作模式 2.1. VEPA (Virtual Ethernet Port Aggregator) VEPA为默认的工作模式，该模式下，所有macvlan发出的流量都会经过父接口，不管目的地是否与该macvlan共用一个父接口。\n2.2. Bridge mode 该bridge模式类似于传统的网桥模式，拥有相同父接口的macvlan可以直接进行通信，不需要将数据发给父接口转发。该模式下不需要交换机支持hairpin模式，性能比VEPA模式好。另外相对于传统的网桥模式，该模式不需要学习mac地址，不需要STP，使得其性能比传统的网桥性能好得多。但是如果父接口down掉，则所有子接口也会down，同时无法通信。\n2.3. Private mode 该模式是VEPA模式的增强版，\n2.4. Passthru mode . 待完善\n参考：\nhttps://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/ https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan https://github.com/containernetworking/plugins/blob/master/plugins/main/macvlan/macvlan.go http://wikibon.org/wiki/v/Edge_Virtual_Bridging Linux 虚拟网卡技术：Macvlan http://hicu.be/bridge-vs-macvlan http://hicu.be/docker-networking-macvlan-bridge-mode-configuration ","categories":"","description":"","excerpt":"1. 简介 macvlan可以看做是物理接口eth（父接口）的子接口，每个macvlan都拥有独立的mac地址，可以被绑定IP作为正常的网卡 …","ref":"/kubernetes-notes/network/cni/macvlan/","tags":["CNI"],"title":"Macvlan介绍"},{"body":"文件存储结构 大部分的Linux文件系统（如ext2、ext3）规定，一个文件由目录项、inode和数据块组成\n目录项：包括文件名和inode节点号。 Inode：又称文件索引节点，包含文件的基础信息以及数据块的指针。 数据块：包含文件的具体内容。 1. inode 理解inode，要从文件储存说起。文件储存在硬盘上，硬盘的最小存储单位叫做\"扇区\"（Sector），每个扇区储存512字节（相当于0.5KB）。\n操作系统读取硬盘的时候，不会一个扇区一个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个\"块\"（block）。这种由多个扇区组成的\"块\"，是文件存取的最小单位。，即连续八个 sector组成一个 block。\n文件数据都储存在\"块\"中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为\"索引节点\"。\ninode包含文件的元信息，具体来说有以下内容：\n文件的字节数。 文件拥有者的User ID。 文件的Group ID。 文件的读、写、执行权限。 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。 链接数，即有多少文件名指向这个inode。 文件数据block的位置。 可以用stat命令，查看某个文件的inode信息：\nstat demo.txt 总之，除了文件名以外的所有文件信息，都存在inode之中。\n当查看某个文件时，会先从inode表中查出文件属性及数据存放点，再从数据块中读取数据。\n请看文件存储结构示意图：\n1.1. inode的大小 inode也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table），存放inode所包含的信息。\n每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。\n查看每个硬盘分区的inode总数和已经使用的数量，可以使用df -i 命令。\n查看每个inode节点的大小，可以用如下命令：\nsudo dumpe2fs -h /dev/hda | grep \"Inode size\" 由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。\n1.2. inode号码 每个inode都有一个号码，操作系统用inode号码来识别不同的文件。\nLinux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号。表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。\n使用ls -i命令，可以看到文件名对应的inode号码，例如：\nls -i demo.txt 3. 目录项 Linux系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。\n目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码。\nls命令只列出目录文件中的所有文件名：\nls /etc ls -i命令列出整个目录文件，即文件名和inode号码：\nls -i /etc 如果要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。ls -l命令列出文件的详细信息。\nls -l /etc 4. 硬链接和软链接 4.1. 硬链接 一般情况下，文件名和inode号码是\"一一对应\"关系，每个inode号码对应一个文件名。但是，Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为\"硬链接\"（hard link）。\nln命令可以创建硬链接\nln source_file target_file 运行上面这条命令以后，源文件与目标文件的inode号码相同，都指向同一个inode。inode信息中有一项叫做\"链接数\"，记录指向该inode的文件名总数，这时就会增加1。反过来，删除一个文件名，就会使得inode节点中的\"链接数\"减1。当这个值减到0，表明没有文件名指向这个inode，系统就会回收这个inode号码，以及其所对应block区域。\n这里顺便说一下目录文件的\"链接数\"。创建目录时，默认会生成两个目录项：\".\"和\"..\"。前者的inode号码就是当前目录的inode号码，等同于当前目录的\"硬链接\"；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的\"硬链接\"。所以，任何一个目录的\"硬链接\"总数，总是等于2加上它的子目录总数（含隐藏目录）,这里的2是父目录对其的“硬链接”和当前目录下的\".硬链接“。\n4.2. 软链接 除了硬链接以外，还有一种特殊情况。文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的\"软链接\"（soft link）或者\"符号链接（symbolic link）。\n这意味着，文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错：\"No such file or directory\"。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode\"链接数\"不会因此发生变化。\nln -s命令可以创建软链接\nln -s source_file target_file 参考：\nhttp://c.biancheng.net/cpp/linux/ ","categories":"","description":"","excerpt":"文件存储结构 大部分的Linux文件系统（如ext2、ext3）规定，一个文件由目录项、inode和数据块组成\n目录项：包括文件名 …","ref":"/linux-notes/file/linux-file-storage/","tags":["Linux"],"title":"Linux文件存储"},{"body":"流程语句 1. 条件语句 //在if之后条件语句之前可以添加变量初始化语句，用;号隔离 if \u003c条件语句\u003e { //条件语句不需要用括号括起来，花括号必须存在 //语句体 }else{ //语句体 } //在有返回值的函数中，不允许将最后的return语句放在if...else...的结构中，否则会编译失败 //例如以下为错误范例 func example(x int) int{ if x==0{ return 5 }else{ return x //最后的return语句放在if-else结构中，所以编译失败 } } 2. 选择语句 //1、根据条件不同，对应不同的执行体 switch i{ case 0: fmt.Printf(\"0\") case 1: //满足条件就会退出，只有添加fallthrough才会继续执行下一个case语句 fmt.Prinntf(\"1\") case 2,3,1: //单个case可以出现多个选项 fmt.Printf(\"2,3,1\") default: //当都不满足以上条件时，执行default语句 fmt.Printf(\"Default\") } //2、该模式等价于多个if-else的功能 switch { case \u003c条件表达式1\u003e: 语句体1 case \u003c条件表达式2\u003e: 语句体2 } 3. 循环语句 //1、Go只支持for关键字，不支持while，do-while结构 for i,j:=0,1;i\u003c10;i++{ //支持多个赋值 //语句体 } //2、无限循环 sum:=1 for{ //不接条件表达式表示无限循环 sum++ if sum \u003e 100{ break //满足条件跳出循环 } } //3、支持continue和break，break可以指定中断哪个循环，break JLoop(标签) for j:=0;j\u003c5;j++{ for i:=0;i\u003c10;i++{ if i\u003e5{ break JLoop //终止JLoop标签处的外层循环 } fmt.Println(i) } JLoop: //标签处 ... 4. 跳转语句 //关键字goto支持跳转 func myfunc(){ i:=0 HERE: //定义标签处 fmt.Println(i) i++ if i\u003c10{ goto HERE //跳转到标签处 } } ","categories":"","description":"","excerpt":"流程语句 1. 条件语句 //在if之后条件语句之前可以添加变量初始化语句，用;号隔离 if \u003c条件语句\u003e { //条件语句不需要用括号括起 …","ref":"/golang-notes/basis/control-structures/","tags":["Golang"],"title":"流程语句"},{"body":"添加Flags 1. Persistent Flags Persistent Flags表示该类参数可以被用于当前命令及其子命令。\n例如，以下表示verbose参数可以被用于rootCmd及其子命令。\nrootCmd.PersistentFlags().BoolVarP(\u0026Verbose, \"verbose\", \"v\", false, \"verbose output\") 2. Local Flags Local Flags表示该类参数只能用于当前命令。\n例如，以下表示source只能用于localCmd这个命令，不能用于其子命令。\nlocalCmd.Flags().StringVarP(\u0026Source, \"source\", \"s\", \"\", \"Source directory to read from\") 3. Local Flag on Parent Commands cobra默认只解析当前命令的local flags，通过开启Command.TraverseChildren参数，可以解析每个命令的local flags。\ncommand := cobra.Command{ Use: \"print [OPTIONS] [COMMANDS]\", TraverseChildren: true, } 4. Bind Flags with Config 可以通过 viper来绑定flags。\nvar author string func init() { rootCmd.PersistentFlags().StringVar(\u0026author, \"author\", \"YOUR NAME\", \"Author name for copyright attribution\") viper.BindPFlag(\"author\", rootCmd.PersistentFlags().Lookup(\"author\")) } 更多参考： viper documentation。\n5. Required flags 默认添加的flags的可选参数，如果需要在二进制运行时添加必要参数，即当该参数没指定时会报错。可使用以下设置。\nrootCmd.Flags().StringVarP(\u0026Region, \"region\", \"r\", \"\", \"AWS region (required)\") rootCmd.MarkFlagRequired(\"region\") 参考：\nhttps://github.com/spf13/cobra https://github.com/spf13/cobra/blob/master/cobra/README.md ","categories":"","description":"","excerpt":"添加Flags 1. Persistent Flags Persistent Flags表示该类参数可以被用于当前命令及其子命令。\n例如，以 …","ref":"/golang-notes/framework/cobra/cobra-flags/","tags":["Golang"],"title":"cobra flags"},{"body":"添加iptables规则 # 单个端口 iptables -A INPUT -p tcp --dport 22 -j ACCEPT # 多个端口 iptables -A INPUT -p tcp -m multiport --dports 6443,8443,2379,2380,10250 -j ACCEPT 删除iptables规则 # 显示iptables规则行号 iptables -nL --line-numbers # 删除某行规则 iptables -D INPUT 11 持久化iptables（重启仍生效） 持久化iptables规则，添加规则到文件中/etc/sysconfig/iptables\n# vi /etc/sysconfig/iptables -A INPUT -p vrrp -j ACCEPT -A OUTPUT -p vrrp -j ACCEPT 或者\napt-get install iptables-persistent netfilter-persistent save netfilter-persistent reload # 生成的规则存储在以下文件 /etc/iptables/rules.v4 /etc/iptables/rules.v6 ","categories":"","description":"","excerpt":"添加iptables规则 # 单个端口 iptables -A INPUT -p tcp --dport 22 -j ACCEPT # 多个 …","ref":"/linux-notes/network/iptables-command/","tags":["iptables"],"title":"iptables命令"},{"body":"chrome快捷键 1. 标签页和窗口快捷键 操作 快捷键 打开新窗口 ⌘ + n 在无痕模式下打开新窗口 ⌘ + Shift + n 打开新的标签页，并跳转到该标签页 ⌘ + t 重新打开最后关闭的标签页，并跳转到该标签页 ⌘ + Shift + t 跳转到下一个打开的标签页 ⌘ + Option + 向右箭头键 跳转到上一个打开的标签页 ⌘ + Option + 向左箭头键 跳转到特定标签页 ⌘ + 1 到 ⌘ + 8 顺序切换标签页 Ctrl + Tab 跳转到最后一个标签页 ⌘ + 9 打开当前标签页浏览记录中记录的上一个页面 ⌘ + [ 或 ⌘ + 向左箭头键 打开当前标签页浏览记录中记录的下一个页面 ⌘ + ] 或 ⌘ + 向右箭头键 关闭当前标签页或弹出式窗口 ⌘ + w 关闭当前窗口 ⌘ + Shift + w 最小化窗口 ⌘ + m 隐藏 Google Chrome ⌘ + h 退出 Google Chrome ⌘ + q 2. Google Chrome 功能快捷键 操作 快捷键 显示或隐藏书签栏 ⌘ + Shift + b 打开书签管理器 ⌘ + Option + b 在新标签页中打开“设置”页 ⌘ + , 在新标签页中打开“历史记录”页 ⌘ + y 在新标签页中打开“下载内容”页 ⌘ + Shift + j 打开查找栏搜索当前网页 ⌘ + f 跳转到与查找栏中搜索字词相匹配的下一条内容 ⌘ + g 跳转到与查找栏中搜索字词相匹配的上一条内容 ⌘ + Shift + g 打开查找栏后，搜索选定文本 ⌘ + e 打开“开发者工具” ⌘ + Option + i 打开“清除浏览数据”选项 ⌘ + Shift + Delete 使用其他帐号登录或以访客身份浏览 ⌘ + Shift + m 3. 地址栏快捷键 在地址栏中可使用以下快捷键：\n操作 快捷键 使用默认搜索引擎进行搜索 输入搜索字词并按 Enter 键 使用其他搜索引擎进行搜索 输入搜索引擎名称并按 Tab 键 为网站名称添加 www. 和 .com，并在当前标签页中打开该网站 输入网站名称并按 Control + Enter 键 为网站名称添加 www. 和 .com，并在新标签页中打开该网站 输入网站名称并按 Control + Shift + Enter 键 在新的后台标签页中打开网站 输入网址并按 ⌘ + Enter 键 跳转到地址栏 ⌘ + l 从地址栏中移除联想查询内容 按向下箭头键以突出显示相应内容，然后按 Shift + fn + Delete 4. 网页快捷键 操作 快捷键 打开选项以打印当前网页 ⌘ + p 打开选项以保存当前网页 ⌘ + s 打开“页面设置”对话框 ⌘ + Option + p 通过电子邮件发送当前网页 ⌘ + Shift + i 重新加载当前网页 ⌘ + r 重新加载当前网页（忽略缓存的内容） ⌘ + Shift + r 停止加载网页 Esc 浏览下一个可点击项 Tab 浏览上一个可点击项 Shift + Tab 使用 Google Chrome 打开计算机中的文件 按住 ⌘ + o 键并选择文件 显示当前网页的 HTML 源代码（不可修改） ⌘ + Option + u 打开 JavaScript 控制台 ⌘ + Option + j 将当前网页保存为书签 ⌘ + d 将所有打开的标签页以书签的形式保存在新文件夹中 ⌘ + Shift + d 开启或关闭全屏模式 ⌘ + Ctrl + f 放大网页上的所有内容 ⌘ 和 + 缩小网页上的所有内容 ⌘ 和 - 将网页上的所有内容恢复到默认大小 ⌘ + 0 向下滚动网页，一次一个屏幕 空格键 向上滚动网页，一次一个屏幕 Shift + 空格键 搜索网络 ⌘ + Option + f 将光标移到文本字段中的上一个字词前面 Option + 向左箭头键 将光标移到文本字段中的上一个字词后面 Option + 向右箭头键 删除文本字段中的上一个字词 Option + Delete 在当前标签页中打开主页 ⌘ + Shift + h 5. 鼠标快捷键 以下快捷键要求您使用鼠标：\n操作 快捷键 在当前标签页中打开链接（仅限鼠标） 将链接拖到标签页中 在新的后台标签页中打开链接 按住 ⌘ 键的同时点击链接 打开链接，并跳转到该链接 按住 ⌘ + Shift 键的同时点击链接 打开链接，并跳转到该链接（仅使用鼠标） 将链接拖到标签栏的空白区域 在新窗口中打开链接 按住 Shift 键的同时点击链接 在新窗口中打开标签页（仅使用鼠标） 将标签页拖出标签栏 将标签页移至当前窗口（仅限鼠标） 将标签页拖到现有窗口中 将标签页移回其原始位置 拖动标签页的同时按 Esc 将当前网页保存为书签 将相应网址拖动到书签栏中 下载链接目标 按住 Option 键的同时点击链接 显示浏览记录 右键点击“后退”箭头 或“前进”箭头 ，或者左键点击（并按住鼠标左键不放）“后退”箭头或“前进”箭头 将窗口高度最大化 双击标签栏的空白区域 来自：https://support.google.com/chrome/answer/157179?hl=zh-Hans\n","categories":"","description":"","excerpt":"chrome快捷键 1. 标签页和窗口快捷键 操作 快捷键 打开新窗口 ⌘ + n 在无痕模式下打开新窗口 ⌘ + Shift + n 打开 …","ref":"/linux-notes/keymap/chrome-keymap/","tags":["快捷键"],"title":"chrome快捷键"},{"body":"Git 命令详解 1. 示意图 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 2. Git 命令分类 2.1. 新建代码库 # 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] 2.2. 配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。\n# 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name \"[name]\" $ git config [--global] user.email \"[email address]\" 2.3. 增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 2.4. 代码提交 # 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... 2.5. 分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 2.6. 标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 2.7. 查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其\"提交说明\"必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat \"@{0 day ago}\" # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog 2.8. 远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all 2.9. 撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 2.10. 其他 # 生成一个可供发布的压缩包 $ git archive # 设置换行符为LF git config --global core.autocrlf false #拒绝提交包含混合换行符的文件 git config --global core.safecrlf true 参考文章：\nhttp://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html ","categories":"","description":"","excerpt":"Git 命令详解 1. 示意图 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） …","ref":"/linux-notes/git/git-commands/","tags":["Git"],"title":"Git命令分类"},{"body":"1. 常用命令 1.1. 查看当前VIP在哪个节点上 # 查看VIP是否在筛选结果中 ip addr show|grep \"scope global\" # 或者 ip addr show|grep {vip} 1.2. 查看keepalived的日志 tail /var/log/messages 1.3. 抓包命令 # 抓包 tcpdump -nn vrrp # 可以用这条命令来查看该网络中所存在的vrid tcpdump -nn -i any net 224.0.0.0/8 # tcpdump -nn -i any net 224.0.0.0/8 # tcpdump -nn vrrp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 14:40:00.576387 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:01.577605 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:02.578429 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:03.579605 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:04.580443 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 1.4. VIP操作 # 解绑VIP ip addr del dev # 绑定VIP ip addr add dev 1.5. keepalived 切 VIP 例如将 A 机器上的 VIP 迁移到B 机器上。\n1.5.1. 停止keepalived服务 停止被迁移的机器（A机器）的keepalived服务。\nsystemctl stop keepalived 1.5.2. 查看日志 解绑 A机器 VIP的日志\nSep 19 14:28:09 localhost systemd: Stopping LVS and VRRP High Availability Monitor... Sep 19 14:28:09 localhost Keepalived[45705]: Stopping Sep 19 14:28:09 localhost Keepalived_vrrp[45707]: VRRP_Instance(twemproxy) sent 0 priority Sep 19 14:28:09 localhost Keepalived_vrrp[45707]: VRRP_Instance(twemproxy) removing protocol VIPs. Sep 19 14:28:09 localhost Keepalived_healthcheckers[45706]: Stopped Sep 19 14:28:10 localhost Keepalived_vrrp[45707]: Stopped Sep 19 14:28:10 localhost Keepalived[45705]: Stopped Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 Sep 19 14:28:10 localhost systemd: Stopped LVS and VRRP High Availability Monitor. Sep 19 14:28:10 localhost ntpd[1186]: Deleting interface #10 bond0, 192.168.99.9#123, interface stats: received=0, sent=0, dropped=0, active_time=6755768 secs 绑定 B 机器 VIP的日志\nSep 17 17:20:25 localhost systemd: Starting LVS and VRRP High Availability Monitor... Sep 17 17:20:26 localhost Keepalived[34566]: Starting Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 Sep 17 17:20:26 localhost Keepalived[34566]: Opening file '/etc/keepalived/keepalived.conf'. Sep 17 17:20:26 localhost Keepalived[34568]: Starting Healthcheck child process, pid=34569 Sep 17 17:20:26 localhost Keepalived[34568]: Starting VRRP child process, pid=34570 Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering Kernel netlink reflector Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering Kernel netlink command channel Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering gratuitous ARP shared channel Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Opening file '/etc/keepalived/keepalived.conf'. Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Truncating auth_pass to 8 characters Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP_Instance(twemproxy) removing protocol VIPs. Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Using LinkWatch kernel netlink reflector... Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP_Instance(twemproxy) Entering BACKUP STATE Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP sockpool: [ifindex(4), proto(112), unicast(0), fd(10,11)] Sep 17 17:20:26 localhost systemd: Started LVS and VRRP High Availability Monitor. Sep 17 17:20:26 localhost kernel: IPVS: Registered protocols (TCP, UDP, SCTP, AH, ESP) Sep 17 17:20:26 localhost kernel: IPVS: Connection hash table configured (size=4096, memory=64Kbytes) Sep 17 17:20:26 localhost kernel: IPVS: Creating netns size=2192 id=0 Sep 17 17:20:26 localhost kernel: IPVS: Creating netns size=2192 id=1 Sep 17 17:20:26 localhost kernel: IPVS: ipvs loaded. Sep 17 17:20:26 localhost Keepalived_healthcheckers[34569]: Opening file '/etc/keepalived/keepalived.conf'. 2. 指定keepalived的输出日志文件 2.1. 修改 /etc/sysconfig/keepalived 将KEEPALIVED_OPTIONS=\"-D\"改为KEEPALIVED_OPTIONS=\"-D -d -S 0\"。\n# Options for keepalived. See `keepalived --help' output and keepalived(8) and # keepalived.conf(5) man pages for a list of all options. Here are the most # common ones : # # --vrrp -P Only run with VRRP subsystem. # --check -C Only run with Health-checker subsystem. # --dont-release-vrrp -V Dont remove VRRP VIPs \u0026 VROUTEs on daemon stop. # --dont-release-ipvs -I Dont remove IPVS topology on daemon stop. # --dump-conf -d Dump the configuration data. # --log-detail -D Detailed log messages. # --log-facility -S 0-7 Set local syslog facility (default=LOG_DAEMON) # KEEPALIVED_OPTIONS=\"-D -d -S 0\" 2.2. 修改rsyslog的配置 /etc/rsyslog.conf 在/etc/rsyslog.conf 添加 keepalived的日志路径\nvi /etc/rsyslog.conf ... # keepalived log local0.* /etc/keepalived/keepalived.log 2.3. 重启rsyslog和keepalived # 重启rsyslog systemctl restart rsyslog # 重启keepalived systemctl restart keepalived 3. Troubleshooting 3.1. virtual_router_id 同网段重复 日志报错如下：\nMar 09 21:28:28 k8s4 Keepalived_vrrp[8548]: bogus VRRP packet received on eth0 !!! Mar 09 21:28:28 k8s4 Keepalived_vrrp[8548]: VRRP_Instance(VI-kube-master) ignoring received advertisment... Mar 09 21:28:43 k8s4 Keepalived_vrrp[8548]: ip address associated with VRID not present in received packet : 192.168.1.10 Mar 09 21:28:43 k8s4 Keepalived_vrrp[8548]: one or more VIP associated with VRID mismatch actual MASTER advert 解决方法:\n同一网段内LB节点配置的 virtual_router_id 值有重复了，选择一个不重复的0~255之间的值，可以用以下命令查看已存在的vrid。\ntcpdump -nn -i any net 224.0.0.0/8 3.2. Operation not permitted 问题：\n两台主备机器都绑定了VIP，查看日志如下：\nSep 28 14:28:37 node Keepalived_vrrp[1686]: (VI_1): send advert error 1 (Operation not permitted) Sep 28 14:28:39 node Keepalived_vrrp[1686]: (VI_1): send advert error 1 (Operation not permitted) 原因：\n由于iptables vrrp协议没有放通，导致keepalived直接无法互相探测选主。\n解决方法：\n添加iptabels vrrp协议规则\niptables -A INPUT -p vrrp -j ACCEPT iptables -A OUTPUT -p vrrp -j ACCEPT 持久化iptables规则，添加规则到文件中/etc/sysconfig/iptables\n# vi /etc/sysconfig/iptables -A INPUT -p vrrp -j ACCEPT -A OUTPUT -p vrrp -j ACCEPT ","categories":"","description":"","excerpt":"1. 常用命令 1.1. 查看当前VIP在哪个节点上 # 查看VIP是否在筛选结果中 ip addr show|grep \"scope …","ref":"/linux-notes/keepalived/keepalived-operation/","tags":["Keepalived"],"title":"Keepalived相关操作"},{"body":"1. kubectl-aliases kubectl-aliases开源工具是由脚本通过拼接各种kubectl相关元素组成的alias命令别名列表，其中命令别名拼接元素如下：\nbase [system?] [operation] [resource] [flags] kubectl -n=kube-system get\ndescribe rm:delete\nlogs\nexec\napply pods\ndeployment\nsecret\ningress\nnode svc\nns cm oyaml ojson\nowide all\nwatch\nfile\nl k=kubectl sys=--namespace kube-system commands: g=get d=describe rm=delete a:apply -f ex: exec -i -t lo: logs -f resources: po=pod dep=deployment ing=ingress svc=service cm=configmap sec=secret ns=namespace no=node flags: output format: oyaml, ojson, owide all: --all or --all-namespaces depending on the command sl: --show-labels w=-w/--watch value flags (should be at the end): f=-f/--filename l=-l/--selector 2. 示例 # 示例1 kd → kubectl describe # 示例2 kgdepallw → kubectl get deployment —all-namespaces —watch alias get示例：\nalias k='kubectl' alias kg='kubectl get' alias kgpo='kubectl get pods' alias kgpoojson='kubectl get pods -o=json' alias kgpon='kubectl get pods --namespace' alias ksysgpooyamll='kubectl --namespace=kube-system get pods -o=yaml -l' 3. 安装 # 将 .kubectl_aliases下载到 home 目录 cd ~ \u0026\u0026 wget https://raw.githubusercontent.com/ahmetb/kubectl-aliases/master/.kubectl_aliases # 将以下内容添加到 .bashrc中，并执行 source .bashrc [ -f ~/.kubectl_aliases ] \u0026\u0026 source ~/.kubectl_aliases function kubectl() { command kubectl $@; } # 如果需要提示别名的完整命令，则将以下内容添加到 .bashrc中，并执行 source .bashrc [ -f ~/.kubectl_aliases ] \u0026\u0026 source ~/.kubectl_aliases function kubectl() { echo \"+ kubectl $@\"; command kubectl $@; } 参考：\nhttps://ahmet.im/blog/kubectl-aliases/ https://github.com/ahmetb/kubectl-aliases ","categories":"","description":"","excerpt":"1. kubectl-aliases kubectl-aliases开源工具是由脚本通过拼接各种kubectl相关元素组成的alias命令别 …","ref":"/kubernetes-notes/operation/kubectl/kubectl-alias/","tags":["Kubernetes"],"title":"kubectl命令别名"},{"body":"1. 传输层的作用 1.1. 传输层的定义 IP首部有个协议字段，用来标识传输层协议，识别数据是TCP的内容还是UDP的内容。同样，传输层，为了识别数据应该发给哪个应用也设定了这样的编号，即端口。\n1.2. 通信处理 应用协议大多以C/S形式运行，即服务端需提前启动服务，监听某个端口，当客户端往该端口发送数据时，可以及时处理请求。\n服务端程序在UNIX系统中称为守护进程，例如HTTP的服务端程序为httpd；ssh的服务端程序为sshd。UNIX中不必要逐个启动这些守护进程，而是由超级守护进程inetd(互联网守护进程)启动，当收到客户端请求时会创建（fork）新的进程并转换（exec）为httpd等各个守护进程。根据请求端口分配到对应的服务端守护进程上处理。\n1.3. TCP和UDP 1.3.1. TCP TCP是面向连接、可靠的数据流。流就是不间断的数据结构，可理解为水管中的水流。虽然可以保证发送顺序，但犹如没有间隔的发送数据流给接收端。例如：发送10次100字节的消息，接收端可能会收到一个1000字节连续不断的数据。TCP为实现可靠传输，实行“顺序控制”和“重发控制”；还具备“流量控制”、“拥塞控制”、提高网络利用率等。TCP可以类比为“打电话”，有去有回。\n1.3.2. UDP UDP是不具备可靠性的数据报协议，可以确保发送消息的大小，但不能保证消息一定到达，应用有时会根据自己的需要进行重发处理。UDP可以类比“发短信”，有去无回。\n1.3.3. 套接字 应用在使用TCP或UDP时会用到系统提供的类库，即API（应用编程接口），通信时会用到套接字（socket）的API。应用程序利用套接字，可以设置对端的IP地址、端口号，并实现数据的发送与接收。\n2. 端口号 2.1. 端口号的定义 类别 地址 层 说明 端口号 程序地址 传输层 同一个计算机中不同的应用程序 IP地址 主机地址 网络层 识别TCP/IP网络中不同的主机或路由器 MAC地址 物理地址 数据链路层 在同一个数据链路中识别不同的计算机 把数据传输比作快递传递；IP地址就像你的家庭地址；那么端口号相当于你家具体的收件人；知道了家庭地址和收件人才能将快递准确送达。\n2.2. 根据端口号识别应用 2.3. 通过IP地址、端口号、协议号进行通信 5个信息唯一标识一个通信：源地址IP、目标地址IP、协议号、源端口号、目标端口号。\n2.4. 端口号如何确定 2.4.1. 标准既定的端口号 该方法也叫静态方法，是指每个应用程序都有其指定的端口号。例如HTTP、FTP等应用协议使用的端口号，这类端口号称为知名端口号，一般由0-1023的数字分配而成。除知名端口号外，还有一些端口号也被正式注册，分布在1024-49151的数字之间。这些端口可用于任何通信用途。\n2.4.2. 时序分配法 该方法也叫动态分配法，服务端有必要确定监听端口号，但接受服务的客户端没必要确定端口号。客户端可以不用自己设置端口号，由操作系统进行分配。操作系统为每个应用程序分配互不冲突的端口号。例如，新增一个端口号则在之前的端口号上加1，动态分配的端口号取值范围：49152-65535。\n3. TCP协议概述 TCP:Transmission Control Protocol (传输控制协议)，TCP实现了数据传输时的各种控制功能，可以进行丢包重发，乱序纠正，控制通信流量的浪费。\n参考：\n《图解TCP/IP》 ","categories":"","description":"","excerpt":"1. 传输层的作用 1.1. 传输层的定义 IP首部有个协议字段，用来标识传输层协议，识别数据是TCP的内容还是UDP的内容。同样，传输层， …","ref":"/linux-notes/tcpip/tcp/","tags":["TCPIP"],"title":"TCP协议"},{"body":"1. Nginx的系统架构 Nginx包含一个单一的master进程和多个worker进程，每个进程都是单进程，并且设计为同时处理成千上万个连接。 worker进程是处理连接的地方，Nginx使用了操作系统事件机制来快速响应这些请求。 master进程负责读取配置文件、处理套接字、派生worker进程、打开日志文件和编译嵌入式的perl脚本。master进程是一个可以通过处理信号量来管理请求的进程。 worker进程运行在一个忙碌的事件循环处理中，用于处理进入的连接。每一个nginx模块被构筑在worker中。任何请求处理、过滤、处理代理的连接和更多操作都在worker中完成。 如果没有阻塞worker进程的进程（例如磁盘I/O），那么需要配置的worker进程要多于CPU内核数，以便处理负载。 2. Http核心模块 2.1.1. server 指令server开始一个新的上下文（context）。\nhttp server指令\n指令 说明 port_in_redirect 确认nginx是否对端口指定重定向 server 创建一个新的配置区域，定义一个虚拟主机。listen指令指定IP和端口；server_name列举用于匹配的Host头值 server_name 配置用于响应请求的虚拟主机名称 server_name_in_redirect server_tokens 在错误信息中禁止发送nginx的版本号和server响应头 2.1.2. 日志格式 参数 说明 示例 $remote_addr 客户端地址 211.28.65.253 $remote_user 客户端用户名称 -- $time_local 访问时间和时区 18/Jul/2012:17:00:01 +0800 $request 请求的URI和HTTP协议 \"GET /article-10000.html HTTP/1.1\" $http_host 请求地址，即浏览器中你输入的地址（IP或域名） www.it300.com192.168.100.100 $status HTTP请求状态 200 $upstream_status upstream状态 200 $body_bytes_sent 发送给客户端文件内容大小 1547 $http_referer url跳转来源 https://www.baidu.com/ $http_user_agent 用户终端浏览器等信息 \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; GTB7.0; .NET4.0C; $ssl_protocol SSL协议版本 TLSv1 $ssl_cipher 交换数据中的算法 RC4-SHA $upstream_addr 后台upstream的地址，即真正提供服务的主机地址 10.10.10.100:80 $request_time 整个请求的总时间 0.205 $upstream_response_time 请求过程中，upstream响应时间 0.002 日志切割\n# vim /etc/logrotate.d/nginx /usr/local/nginx/logs/*.log{ #指定转储周期为每天 daily #保留30个备份 rotate 30 #需要压缩 delaycompress #YYYYMMDD日期格式 dateext #忽略错误 missingok #如果日志为空则不做轮询 notifempty #只为整个日志组运行一次的脚本 sharedscripts #日志轮询后执行的脚本 postrotate service nginx reload endscript } ","categories":"","description":"","excerpt":"1. Nginx的系统架构 Nginx包含一个单一的master进程和多个worker进程，每个进程都是单进程，并且设计为同时处理成千上万个 …","ref":"/linux-notes/nginx/nginx-http/","tags":["Nginx"],"title":"Nginx http服务器"},{"body":"beego项目逻辑 1. 路由设置 1.1. beego.Router 入口文件main.go\npackage main import ( _ \"quickstart/routers\" \"github.com/astaxie/beego\" ) func main() { beego.Run() } go中导入包中init函数的执行逻辑\n_ \"quickstart/routers\",包只引入执行了里面的init函数\npackage routers import ( \"quickstart/controllers\" \"github.com/astaxie/beego\" ) func init() { beego.Router(\"/\", \u0026controllers.MainController{}) } 路由包里执行了路由注册beego.Router, 这个函数的功能是映射URL到controller，第一个参数是URL(用户请求的地址)，这里是 /，也就是访问的不带任何参数的URL，第二个参数是对应的 Controller，就是把请求分发到那个控制器来执行相应的逻辑。\n1.2. beego.Run 解析配置文件\nbeego 会自动解析在 conf 目录下面的配置文件 app.conf，通过修改配置文件相关的属性，我们可以定义：开启的端口，是否开启 session，应用名称等信息。\n执行用户的hookfunc\nbeego会执行用户注册的hookfunc，默认的已经存在了注册mime，用户可以通过函数AddAPPStartHook注册自己的启动函数。\n是否开启 session\n会根据上面配置文件的分析之后判断是否开启 session，如果开启的话就初始化全局的 session。\n是否编译模板\nbeego 会在启动的时候根据配置把 views 目录下的所有模板进行预编译，然后存在 map 里面，这样可以有效的提高模板运行的效率，无需进行多次编译。\n是否开启文档功能\n根据EnableDocs配置判断是否开启内置的文档路由功能\n是否启动管理模块\nbeego 目前做了一个很酷的模块，应用内监控模块，会在 8088 端口做一个内部监听，我们可以通过这个端口查询到 QPS、CPU、内存、GC、goroutine、thread 等统计信息。\n监听服务端口\n这是最后一步也就是我们看到的访问 8080 看到的网页端口，内部其实调用了 ListenAndServe，充分利用了 goroutine 的优势，一旦 run 起来之后，我们的服务就监听在两个端口了，一个服务端口 8080 作为对外服务，另一个 8088 端口实行对内监控。\n2. controller 逻辑 package controllers import ( \"github.com/astaxie/beego\" ) type MainController struct { beego.Controller } func (this *MainController) Get() { this.Data[\"Website\"] = \"beego.me\" this.Data[\"Email\"] = \"astaxie@gmail.com\" this.TplName = \"index.tpl\" } 1、声明了一个控制器 MainController，这个控制器里面内嵌了 beego.Controller，即Go 的嵌入方式，也就是 MainController 自动拥有了所有 beego.Controller 的方法。而 beego.Controller 拥有很多方法，其中包括 Init、Prepare、Post、Get、Delete、Head等方法。可以通过重写的方式来实现这些方法，以上例子重写了 Get 方法。\n2、beego 是一个 RESTful 的框架，请求默认是执行对应 req.Method 的方法。例如浏览器的是 GET 请求，那么默认就会执行 MainController 下的 Get 方法。（用户可以改变这个行为，通过注册自定义的函数名）。\n3、获取数据，赋值到 this.Data 中，这是一个用来存储输出数据的 map。\n4、渲染模板，this.TplName 就是需要渲染的模板，这里指定了 index.tpl，如果用户不设置该参数，那么默认会去到模板目录的 Controller/\u003c方法名\u003e.tpl 查找，例如上面的方法会去 maincontroller/get.tpl(文件、文件夹必须小写)。用户设置了模板之后系统会自动的调用 Render 函数（这个函数是在 beego.Controller 中实现的），所以无需用户自己来调用渲染。\n5、如果不使用模板可以直接输出：\nfunc (this *MainController) Get() { this.Ctx.WriteString(\"hello\") } 3. model逻辑 model一般用来处理数据库操作，如果逻辑中存在可以复用的部分就可以抽象成一个model。\npackage models import ( \"loggo/utils\" \"path/filepath\" \"strconv\" \"strings\" ) var ( NotPV []string = []string{\"css\", \"js\", \"class\", \"gif\", \"jpg\", \"jpeg\", \"png\", \"bmp\", \"ico\", \"rss\", \"xml\", \"swf\"} ) const big = 0xFFFFFF func LogPV(urls string) bool { ext := filepath.Ext(urls) if ext == \"\" { return true } for _, v := range NotPV { if v == strings.ToLower(ext) { return false } } return true } 4. view逻辑 Controller中的this.TplName = \"index.tpl\"，设置显示的模板文件，默认支持 tpl 和 html 的后缀名，如果想设置其他后缀你可以调用 beego.AddTemplateExt 接口设置。beego 采用了 Go 语言默认的模板引擎，和 Go 的模板语法一样。\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eBeego\u003c/title\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c/head\u003e \u003cbody\u003e \u003cheader class=\"hero-unit\" style=\"background-color:#A9F16C\"\u003e \u003cdiv class=\"container\"\u003e \u003cdiv class=\"row\"\u003e \u003cdiv class=\"hero-text\"\u003e \u003ch1\u003eWelcome to Beego!\u003c/h1\u003e \u003cp class=\"description\"\u003e Beego is a simple \u0026 powerful Go web framework which is inspired by tornado and sinatra. \u003cbr /\u003e Official website: \u003ca href=\"http://{{.Website}}\"\u003e{{.Website}}\u003c/a\u003e \u003cbr /\u003e Contact me: {{.Email}} \u003c/p\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/header\u003e \u003c/body\u003e \u003c/html\u003e Controller 里面把数据赋值给了 data（map 类型），然后在模板中就直接通过 key 访问 .Website 和 .Email 。这样就做到了数据的输出。\n5. 静态文件 网页往往包含了很多的静态文件，包括图片、JS、CSS 等\n├── static │ ├── css │ ├── img │ └── js beego 默认注册了 static 目录为静态处理的目录，注册样式：URL 前缀和映射的目录（在/main.go文件中beego.Run()之前加入）：\nStaticDir[\"/static\"] = \"static\" 用户可以设置多个静态文件处理目录，例如你有多个文件下载目录 download1、download2，你可以这样映射（在/main.go文件中beego.Run()之前加入）：\nbeego.SetStaticPath(\"/down1\", \"download1\") beego.SetStaticPath(\"/down2\", \"download2\") 这样用户访问 URL http://localhost:8080/down1/123.txt 则会请求 download1 目录下的 123.txt 文件。\n参考：\nhttps://beego.me/docs/quickstart/router.md https://beego.me/docs/quickstart/controller.md https://beego.me/docs/quickstart/model.md https://beego.me/docs/quickstart/view.md https://beego.me/docs/quickstart/static.md ","categories":"","description":"","excerpt":"beego项目逻辑 1. 路由设置 1.1. beego.Router 入口文件main.go\npackage main import ( …","ref":"/golang-notes/web/beego/beego-project/","tags":["Golang"],"title":"Beego 项目逻辑"},{"body":"1. Scheduler简介 Scheduler负责Pod调度。在整个系统中起\"承上启下\"作用，承上：负责接收Controller Manager创建的新的Pod，为其选择一个合适的Node；启下：Node上的kubelet接管Pod的生命周期。\nScheduler：\n1）通过调度算法为待调度Pod列表的每个Pod从Node列表中选择一个最适合的Node，并将信息写入etcd中\n2）kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载Image，并启动容器。\n2. 调度流程 1、预选调度过程，即遍历所有目标Node,筛选出符合要求的候选节点，kubernetes内置了多种预选策略（xxx Predicates）供用户选择\n2、确定最优节点，在第一步的基础上采用优选策略（xxx Priority）计算出每个候选节点的积分，取最高积分。\n调度流程通过插件式加载的“调度算法提供者”（AlgorithmProvider）具体实现，一个调度算法提供者就是包括一组预选策略与一组优选策略的结构体。\n3. 预选策略 说明：返回true表示该节点满足该Pod的调度条件；返回false表示该节点不满足该Pod的调度条件。\n3.1. NoDiskConflict 判断备选Pod的数据卷是否与该Node上已存在Pod挂载的数据卷冲突，如果是则返回false，否则返回true。\n3.2. PodFitsResources 判断备选节点的资源是否满足备选Pod的需求，即节点的剩余资源满不满足该Pod的资源使用。\n计算备选Pod和节点中已用资源（该节点所有Pod的使用资源）的总和。 获取备选节点的状态信息，包括节点资源信息。 如果（备选Pod+节点已用资源\u003e该节点总资源）则返回false，即剩余资源不满足该Pod使用；否则返回true。 3.3. PodSelectorMatches 判断节点是否包含备选Pod的标签选择器指定的标签，即通过标签来选择Node。\n如果Pod中没有指定spec.nodeSelector，则返回true。 否则获得备选节点的标签信息，判断该节点的标签信息中是否包含该Pod的spec.nodeSelector中指定的标签，如果包含返回true，否则返回false。 3.4. PodFitsHost 判断备选Pod的spec.nodeName所指定的节点名称与备选节点名称是否一致，如果一致返回true，否则返回false。\n3.5. CheckNodeLabelPresence 检查备选节点中是否有Scheduler配置的标签，如果有返回true，否则返回false。\n3.6. CheckServiceAffinity 判断备选节点是否包含Scheduler配置的标签，如果有返回true，否则返回false。\n3.7. PodFitsPorts 判断备选Pod所用的端口列表中的端口是否在备选节点中已被占用，如果被占用返回false，否则返回true。\n4. 优选策略 4.1. LeastRequestedPriority 优先从备选节点列表中选择资源消耗最小的节点（CPU+内存）。\n4.2. CalculateNodeLabelPriority 优先选择含有指定Label的节点。\n4.3. BalancedResourceAllocation 优先从备选节点列表中选择各项资源使用率最均衡的节点。\n参考《Kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. Scheduler简介 Scheduler负责Pod调度。在整个系统中起\"承上启下\"作用，承上：负责接收Controller …","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-scheduler/","tags":["Kubernetes"],"title":"Kubernetes核心原理（三）之Scheduler"},{"body":"1. cAdvisor简介 ​ cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，cAdvisor集成在Kubelet中，当kubelet启动时会自动启动cAdvisor，即一个cAdvisor仅对一台Node机器进行监控。kubelet的启动参数--cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器\u003cNode_IP:port\u003e访问。项目主页：http://github.com/google/cadvisor。\n2. cAdvisor结构图 3. Metrics 分类 字段 描述 cpu cpu_usage_total cpu_usage_system cpu_usage_user cpu_usage_per_cpu load_average Smoothed average of number of runnable threads x 1000 memory memory_usage Memory Usage memory_working_set Working set size network rx_bytes Cumulative count of bytes received rx_errors Cumulative count of receive errors encountered tx_bytes Cumulative count of bytes transmitted tx_errors Cumulative count of transmit errors encountered filesystem fs_device Filesystem device fs_limit Filesystem limit fs_usage Filesystem usage 4. cAdvisor源码 4.1. cAdvisor入口函数 cadvisor.go\nfunc main() { defer glog.Flush() flag.Parse() if *versionFlag { fmt.Printf(\"cAdvisor version %s (%s)/n\", version.Info[\"version\"], version.Info[\"revision\"]) os.Exit(0) } setMaxProcs() memoryStorage, err := NewMemoryStorage() if err != nil { glog.Fatalf(\"Failed to initialize storage driver: %s\", err) } sysFs, err := sysfs.NewRealSysFs() if err != nil { glog.Fatalf(\"Failed to create a system interface: %s\", err) } collectorHttpClient := createCollectorHttpClient(*collectorCert, *collectorKey) containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, \u0026collectorHttpClient) if err != nil { glog.Fatalf(\"Failed to create a Container Manager: %s\", err) } mux := http.NewServeMux() if *enableProfiling { mux.HandleFunc(\"/debug/pprof/\", pprof.Index) mux.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline) mux.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) mux.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) } // Register all HTTP handlers. err = cadvisorhttp.RegisterHandlers(mux, containerManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm) if err != nil { glog.Fatalf(\"Failed to register HTTP handlers: %v\", err) } cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, nil) // Start the manager. if err := containerManager.Start(); err != nil { glog.Fatalf(\"Failed to start container manager: %v\", err) } // Install signal handler. installSignalHandler(containerManager) glog.Infof(\"Starting cAdvisor version: %s-%s on port %d\", version.Info[\"version\"], version.Info[\"revision\"], *argPort) addr := fmt.Sprintf(\"%s:%d\", *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux)) } 核心代码：\nmemoryStorage, err := NewMemoryStorage() sysFs, err := sysfs.NewRealSysFs() #创建containerManager containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, \u0026collectorHttpClient) #启动containerManager err := containerManager.Start() 4.2. cAdvisor Client的使用 import \"github.com/google/cadvisor/client\" func main(){ client, err := client.NewClient(\"http://192.168.19.30:4194/\") //http://\u003chost-ip\u003e:\u003cport\u003e/ } 4.2.1 client定义 cadvisor/client/client.go\n// Client represents the base URL for a cAdvisor client. type Client struct { baseUrl string } // NewClient returns a new v1.3 client with the specified base URL. func NewClient(url string) (*Client, error) { if !strings.HasSuffix(url, \"/\") { url += \"/\" } return \u0026Client{ baseUrl: fmt.Sprintf(\"%sapi/v1.3/\", url), }, nil } 4.2.2. client方法 1）MachineInfo\n// MachineInfo returns the JSON machine information for this client. // A non-nil error result indicates a problem with obtaining // the JSON machine information data. func (self *Client) MachineInfo() (minfo *v1.MachineInfo, err error) { u := self.machineInfoUrl() ret := new(v1.MachineInfo) if err = self.httpGetJsonData(ret, nil, u, \"machine info\"); err != nil { return } minfo = ret return } 2）ContainerInfo\n// ContainerInfo returns the JSON container information for the specified // container and request. func (self *Client) ContainerInfo(name string, query *v1.ContainerInfoRequest) (cinfo *v1.ContainerInfo, err error) { u := self.containerInfoUrl(name) ret := new(v1.ContainerInfo) if err = self.httpGetJsonData(ret, query, u, fmt.Sprintf(\"container info for %q\", name)); err != nil { return } cinfo = ret return } 3）DockerContainer\n// Returns the JSON container information for the specified // Docker container and request. func (self *Client) DockerContainer(name string, query *v1.ContainerInfoRequest) (cinfo v1.ContainerInfo, err error) { u := self.dockerInfoUrl(name) ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(\u0026ret, query, u, fmt.Sprintf(\"Docker container info for %q\", name)); err != nil { return } if len(ret) != 1 { err = fmt.Errorf(\"expected to only receive 1 Docker container: %+v\", ret) return } for _, cont := range ret { cinfo = cont } return } 4）AllDockerContainers\n// Returns the JSON container information for all Docker containers. func (self *Client) AllDockerContainers(query *v1.ContainerInfoRequest) (cinfo []v1.ContainerInfo, err error) { u := self.dockerInfoUrl(\"/\") ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(\u0026ret, query, u, \"all Docker containers info\"); err != nil { return } cinfo = make([]v1.ContainerInfo, 0, len(ret)) for _, cont := range ret { cinfo = append(cinfo, cont) } return } ","categories":"","description":"","excerpt":"1. cAdvisor简介 ​ cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网 …","ref":"/kubernetes-notes/monitor/cadvisor-introduction/","tags":["Monitor"],"title":"cAdvisor介绍"},{"body":"2. 数据库操作 #创建数据库 create database \u003c数据库名\u003e #显示数据库 show databases #删除数据 drop database \u003c数据库名\u003e 3. 数据表操作 3.1. 创建表 create table 表名( 列名 类型 是否可以为空， 列名 类型 是否可以为空 )ENGINE=InnoDB DEFAULT CHARSET=utf8 默认值，创建列时可以指定默认值，当插入数据时如果未主动设置，则自动添加默认值 自增，如果为某列设置自增列，插入数据时无需设置此列，默认将自增（表中只能有一个自增列）注意：1、对于自增列，必须是索引（含主键）2、对于自增可以设置步长和起始值 主键，一种特殊的唯一索引，不允许有空值，如果主键使用单个列，则它的值必须唯一，如果是多列，则其组合必须唯一。 3.2. 查看表 show tables; # 查看数据库全部表 select * from 表名; # 查看表所有内容 3.3. 删除表 drop table 表名 3.4. 清空表内容 delete from 表名 truncate table 表名 3.5. 查看表结构 desc 表名 3.6. 修改表 列操作\n#添加列 alter table 表名 add 列名 类型 alter table 表名 add 列名 类型 after `列名` #删除列 alter table 表名 drop column 列名 #修改列 alter table 表名 modify column 列名 类型; -- 类型 alter table 表名 change 原列名 新列名 类型; -- 列名，类型 主键操作\n#添加主键 alter table 表名 add primary key(列名); #删除主键 alter table 表名 drop primary key; alter table 表名 modify 列名 int, drop primary key; #修改主键：先删除后添加 alter table 表名 drop primary key; alter table 表名 add primary key(列名); #添加外键 alter table 从表 add constraint 外键名称（形如：FK从表主表） foreign key 从表(外键字段) references 主表(主键字段); #删除外键 alter table 表名 drop foreign key 外键名称 默认值操作\n#修改默认值： ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000; #删除默认值： ALTER TABLE testalter_tbl ALTER i DROP DEFAULT; 调整表结构字段顺序\nalter table \u003ctable_name\u003e modify \u003c字段1\u003e varchar(10) after \u003c字段2\u003e; alter table \u003ctable_name\u003e modify id int(10) unsigned auto_increment first; ","categories":"","description":"","excerpt":"2. 数据库操作 #创建数据库 create database \u003c数据库名\u003e #显示数据库 show databases # …","ref":"/linux-notes/mysql/table-operation/","tags":["Mysql"],"title":"Mysql常用命令之数据表操作"},{"body":"1. Pod phase Pod的phase是Pod生命周期中的简单宏观描述，定义在Pod的PodStatus对象的phase 字段中。\nphase有以下几种值：\n状态值 说明 挂起（Pending） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间。 运行中（Running） 该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded） Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed） Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 未知（Unknown） 因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。 2. Pod 状态 Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。 PodCondition包含以下以下字段：\nlastProbeTime：Pod condition最后一次被探测到的时间戳。 lastTransitionTime：Pod最后一次状态转变的时间戳。 message：状态转化的信息，一般为报错信息，例如：containers with unready status: [c-1]。 reason：最后一次状态形成的原因，一般为报错原因，例如：ContainersNotReady。 status：包含的值有 True、False 和 Unknown。 type：Pod状态的几种类型。 其中type字段包含以下几个值：\nPodScheduled：Pod已经被调度到运行节点。 Ready：Pod已经可以接收请求提供服务。 Initialized：所有的init container已经成功启动。 Unschedulable：无法调度该Pod，例如节点资源不够。 ContainersReady：Pod中的所有容器已准备就绪。 3. 重启策略 Pod通过restartPolicy字段指定重启策略，重启策略类型为：Always、OnFailure 和 Never，默认为 Always。\nrestartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。\n重启策略 说明 Always 当容器失效时，由kubelet自动重启该容器 OnFailure 当容器终止运行且退出码不为0时，由kubelet自动重启该容器 Never 不论容器运行状态如何，kubelet都不会重启该容器 说明：\n可以管理Pod的控制器有Replication Controller，Job，DaemonSet，及kubelet（静态Pod）。\nRC和DaemonSet：必须设置为Always，需要保证该容器持续运行。 Job：OnFailure或Never，确保容器执行完后不再重启。 kubelet：在Pod失效的时候重启它，不论RestartPolicy设置为什么值，并且不会对Pod进行健康检查。 4. Pod的生命 Pod的生命周期一般通过Controler\t的方式管理，每种Controller都会包含PodTemplate来指明Pod的相关属性，Controller可以自动对pod的异常状态进行重新调度和恢复，除非通过Controller的方式删除其管理的Pod，不然kubernetes始终运行用户预期状态的Pod。\n控制器的分类\n使用 Job运行预期会终止的 Pod，例如批量计算。Job 仅适用于重启策略为 OnFailure 或 Never 的 Pod。 对预期不会终止的 Pod 使用 ReplicationController、ReplicaSet和 Deployment，例如 Web 服务器。 ReplicationController 仅适用于具有 restartPolicy 为 Always 的 Pod。 提供特定于机器的系统服务，使用 DaemonSet为每台机器运行一个 Pod 。 如果节点死亡或与集群的其余部分断开连接，则 Kubernetes 将应用一个策略将丢失节点上的所有 Pod 的 phase 设置为 Failed。\n5. Pod状态转换 常见的状态转换\nPod的容器数 Pod当前状态 发生的事件 Pod结果状态 RestartPolicy=Always RestartPolicy=OnFailure RestartPolicy=Never 包含一个容器 Running 容器成功退出 Running Succeeded Succeeded 包含一个容器 Running 容器失败退出 Running Running Failure 包含两个容器 Running 1个容器失败退出 Running Running Running 包含两个容器 Running 容器被OOM杀掉 Running Running Failure 5.1. 容器运行时内存超出限制 容器以失败状态终止。 记录 OOM 事件。 如果restartPolicy为： Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never: 记录失败事件；Pod phase 仍为 Failed。 5.2. 磁盘故障 杀掉所有容器。 记录适当事件。 Pod phase 变成 Failed。 如果使用控制器来运行，Pod 将在别处重建。 5.3. 运行节点挂掉 节点控制器等待直到超时。 节点控制器将 Pod phase 设置为 Failed。 如果是用控制器来运行，Pod 将在别处重建。 参考文章：\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ ","categories":"","description":"","excerpt":"1. Pod phase Pod的phase是Pod生命周期中的简单宏观描述，定义在Pod的PodStatus对象的phase 字段中。 …","ref":"/kubernetes-notes/concepts/pod/pod-lifecycle/","tags":["Kubernetes"],"title":"Pod生命周期"},{"body":"1. Docker的总架构图 docker是一个C/S模式的架构，后端是一个松耦合架构，模块各司其职。\n用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。 Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求； Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。 Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储； 当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境； 当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。 libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。 2. Docker各模块组件分析 2.1. Docker Client[发起请求] Docker Client是和Docker Daemon建立通信的客户端。用户使用的可执行文件为docker（类似可执行脚本的命令），docker命令后接参数的形式来实现一个完整的请求命令（例如docker images，docker为命令不可变，images为参数可变）。 Docker Client可以通过以下三种方式和Docker Daemon建立通信：tcp://host:port，unix://path_to_socket和fd://socketfd。 Docker Client发送容器管理请求后，由Docker Daemon接受并处理请求，当Docker Client接收到返回的请求相应并简单处理后，Docker Client一次完整的生命周期就结束了。[一次完整的请求：发送请求→处理请求→返回结果]，与传统的C/S架构请求流程并无不同。 2.2. Docker Daemon[后台守护进程] Docker Daemon的架构图\n2.2.1. Docker Server[调度分发请求] Docker Server的架构图\nDocker Server相当于C/S架构的服务端。功能为接受并调度分发Docker Client发送的请求。接受请求后，Server通过路由与分发调度，找到相应的Handler来执行请求。 在Docker的启动过程中，通过包gorilla/mux，创建了一个mux.Router，提供请求的路由功能。在Golang中，gorilla/mux是一个强大的URL路由器以及调度分发器。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。 创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。 在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。 2.2.2. Engine Engine是Docker架构中的运行引擎，同时也Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。 在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。举例说明，Engine的handler对象中有一项为：{\"create\": daemon.ContainerCreate,}，则说明当名为\"create\"的job在运行时，执行的是daemon.ContainerCreate的handler。 2.2.3. Job 一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。例如：在容器内部运行一个进程，这是一个job；创建一个新的容器，这是一个job。Docker Server的运行过程也是一个job，名为serveapi。 Job的设计者，把Job设计得与Unix进程相仿。比如说：Job有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。 2.3. Docker Registry[镜像注册中心] Docker Registry是一个存储容器镜像的仓库（注册中心），可理解为云端镜像仓库，按repository来分类，docker pull 按照[repository]:[tag]来精确定义一个image。 在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为\"search\"，\"pull\" 与 \"push\"。 可分为公有仓库（docker hub）和私有仓库。 2.4. Graph[docker内部数据库] Graph的架构图\n2.4.1. Repository 已下载镜像的保管者（包括下载镜像和dockerfile构建的镜像）。 一个repository表示某类镜像的仓库（例如Ubuntu），同一个repository内的镜像用tag来区分（表示同一类镜像的不同标签或版本）。一个registry包含多个repository，一个repository包含同类型的多个image。 镜像的存储类型有aufs，devicemapper,Btrfs，Vfs等。其中centos系统使用devicemapper的存储类型。 同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。 2.4.2. GraphDB 已下载容器镜像之间关系的记录者。 GraphDB是一个构建在SQLite之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录 2.5. Driver[执行部分] Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。即Graph负责镜像的存储，Driver负责容器的执行。\n2.5.1. graphdriver graphdriver架构图\ngraphdriver主要用于完成容器镜像的管理，包括存储与获取。 存储：docker pull下载的镜像由graphdriver存储到本地的指定目录（Graph中）。 获取：docker run（create）用镜像来创建容器的时候由graphdriver到本地Graph中获取镜像。 2.5.2. networkdriver networkdriver的架构图\nnetworkdriver的用途是完成Docker容器网络环境的配置，其中包括 Docker启动时为Docker环境创建网桥； Docker容器创建时为其创建专属虚拟网卡设备； Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。 2.5.3. execdriver execdriver的架构图\nexecdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。 现在execdriver默认使用native驱动，不依赖于LXC。 2.6. libcontainer[函数库] libcontainer的架构图\nlibcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。 Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。 libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。 2.7. docker container[服务交付的最终形式] container架构\nDocker container（Docker容器）是Docker架构中服务交付的最终体现形式。\nDocker按照用户的需求与指令，订制相应的Docker容器：\n用户通过指定容器镜像，使得Docker容器可以自定义rootfs等文件系统； 用户通过指定计算资源的配额，使得Docker容器使用指定的计算资源； 用户通过配置网络及其安全策略，使得Docker容器拥有独立且安全的网络环境； 用户通过指定运行的命令，使得Docker容器执行指定的工作。 参考文章：\n《Docker源码分析》 ","categories":"","description":"","excerpt":"1. Docker的总架构图 docker是一个C/S模式的架构，后端是一个松耦合架构，模块各司其职。\n用户是使用Docker Client …","ref":"/kubernetes-notes/runtime/docker/docker-architecture/","tags":["Docker"],"title":"Docker整体架构图"},{"body":"问题描述 内核版本： 5.4.56-200.el7.x86_64\ndocker报错\nMay 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.565235530+08:00\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.devmapper\" error=\"devmapper not configured\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.565525512+08:00\" level=warning msg=\"could not use snapshotter devmapper in metadata plugin\" error=\"devmapper not configured\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.574734345+08:00\" level=warning msg=\"Your kernel does not support CPU realtime scheduler\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.574792864+08:00\" level=warning msg=\"Your kernel does not support cgroup blkio weight\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.574800326+08:00\" level=warning msg=\"Your kernel does not support cgroup blkio weight_device\" kubelet报错\n解决 cgroup问题解决：\n1、curl https://pi-ops.oss-cn-hangzhou.aliyuncs.com/scripts/cgroupfs-mount.sh | bash\n2、重启设备即可解决\n","categories":"","description":"","excerpt":"问题描述 内核版本： 5.4.56-200.el7.x86_64\ndocker报错\nMay 13 16:54:26 8b26d7a8 …","ref":"/kubernetes-notes/trouble-shooting/node/cgroup-subsystem-not-mount/","tags":["问题排查"],"title":"Cgroup子系统无法挂载"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/cni/","tags":"","title":"CNI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/develop/csi/","tags":"","title":"CSI插件开发"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要以deployment controller为例，分析该类controller的运行逻辑。此部分代码主要为位于pkg/controller/deployment。pkg/controller部分的代码包括了各种类型的controller的具体实现。\ncontroller manager的pkg部分代码目录结构如下：\ncontroller # 主要包含各种controller的具体实现 ├── apis ├── bootstrap ├── certificates ├── client_builder.go ├── cloud ├── clusterroleaggregation ├── controller_ref_manager.go ├── controller_utils.go # WaitForCacheSync ├── cronjob ├── daemon ├── deployment # deployment controller │ ├── deployment_controller.go # NewDeploymentController、Run、syncDeployment │ ├── progress.go # syncRolloutStatus │ ├── recreate.go # rolloutRecreate │ ├── rollback.go # rollback │ ├── rolling.go # rolloutRolling │ ├── sync.go ├── disruption # disruption controller ├── endpoint ├── garbagecollector ├── history ├── job ├── lookup_cache.go ├── namespace # namespace controller ├── nodeipam ├── nodelifecycle ├── podautoscaler ├── podgc ├── replicaset # replicaset controller ├── replication # replication controller ├── resourcequota ├── route ├── service # service controller ├── serviceaccount ├── statefulset # statefulset controller └── volume # PersistentVolumeController、AttachDetachController、PVCProtectionController 1. startDeploymentController func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"}] { return nil, false, nil } dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil } startDeploymentController主要调用的函数为NewDeploymentController和对应的Run函数。该部分逻辑在kubernetes/pkg/controller中。\n2. NewDeploymentController NewDeploymentController主要构建DeploymentController结构体。\n该部分主要处理了以下逻辑：\n构建并运行事件处理器eventBroadcaster。 初始化赋值rsControl、clientset、workqueue。 添加dInformer、rsInformer、podInformer的ResourceEventHandlerFuncs，其中主要为AddFunc、UpdateFunc、DeleteFunc三类方法。 构造deployment、rs、pod的Informer的Lister函数和HasSynced函数。 调用syncHandler，来实现syncDeployment。 2.1. eventBroadcaster 调用事件处理器来记录deployment相关的事件。\neventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) // TODO: remove the wrapper when every clients have moved to use the clientset. eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{Interface: v1core.New(client.CoreV1().RESTClient()).Events(\"\")}) 2.2. rsControl 构造DeploymentController，包括clientset、workqueue和rsControl。其中rsControl是具体实现rs逻辑的controller。\ndc := \u0026DeploymentController{ client: client, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } 2.3. Informer().AddEventHandler 添加dInformer、rsInformer、podInformer的ResourceEventHandlerFuncs，其中主要为AddFunc、UpdateFunc、DeleteFunc三类方法。\ndInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) 2.4. Informer.Lister() 调用dInformer、rsInformer和podInformer的Lister()方法。\ndc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() 2.5. Informer().HasSynced 调用Informer().HasSynced，判断是否缓存完成；\ndc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced 2.6. syncHandler syncHandler具体为syncDeployment，syncHandler负责deployment的同步实现。\ndc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue 完整代码如下：\n// NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer extensionsinformers.DeploymentInformer, rsInformer extensionsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) // TODO: remove the wrapper when every clients have moved to use the clientset. eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{Interface: v1core.New(client.CoreV1().RESTClient()).Events(\"\")}) if client != nil \u0026\u0026 client.CoreV1().RESTClient().GetRateLimiter() != nil { if err := metrics.RegisterMetricAndTrackRateLimiterUsage(\"deployment_controller\", client.CoreV1().RESTClient().GetRateLimiter()); err != nil { return nil, err } } dc := \u0026DeploymentController{ client: client, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil } 3. DeploymentController.Run Run执行watch和sync的操作。\n// Run begins watching and syncing. func (dc *DeploymentController) Run(workers int, stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() defer dc.queue.ShutDown() glog.Infof(\"Starting deployment controller\") defer glog.Infof(\"Shutting down deployment controller\") if !controller.WaitForCacheSync(\"deployment\", stopCh, dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) { return } for i := 0; i \u003c workers; i++ { go wait.Until(dc.worker, time.Second, stopCh) } \u003c-stopCh } 3.1. WaitForCacheSync WaitForCacheSync主要是用来在List-Watch机制中可以保持当前cache的数据与etcd的数据一致。\n// WaitForCacheSync is a wrapper around cache.WaitForCacheSync that generates log messages // indicating that the controller identified by controllerName is waiting for syncs, followed by // either a successful or failed sync. func WaitForCacheSync(controllerName string, stopCh \u003c-chan struct{}, cacheSyncs ...cache.InformerSynced) bool { glog.Infof(\"Waiting for caches to sync for %s controller\", controllerName) if !cache.WaitForCacheSync(stopCh, cacheSyncs...) { utilruntime.HandleError(fmt.Errorf(\"Unable to sync caches for %s controller\", controllerName)) return false } glog.Infof(\"Caches are synced for %s controller\", controllerName) return true } 3.2. dc.worker worker调用了processNextWorkItem，processNextWorkItem最终调用了syncHandler，而syncHandler在NewDeploymentController中赋值的具体函数为syncDeployment。\n// worker runs a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the syncHandler is never invoked concurrently with the same key. func (dc *DeploymentController) worker() { for dc.processNextWorkItem() { } } func (dc *DeploymentController) processNextWorkItem() bool { key, quit := dc.queue.Get() if quit { return false } defer dc.queue.Done(key) err := dc.syncHandler(key.(string)) dc.handleErr(err, key) return true } NewDeploymentController中的syncHandler赋值：\nfunc NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { ... dc.syncHandler = dc.syncDeployment ... } 4. syncDeployment syncDeployment基于给定的key执行sync deployment的操作。\n主要流程如下：\n通过SplitMetaNamespaceKey获取namespace和deployment对象的name。 调用Lister的接口获取的deployment的对象。 getReplicaSetsForDeployment获取deployment管理的ReplicaSet对象。 getPodMapForDeployment获取deployment管理的pod，基于ReplicaSet来分组。 checkPausedConditions检查deployment是否是pause状态并添加合适的condition。 isScalingEvent检查deployment的更新是否来自于一个scale的事件，如果是则执行scale的操作。 根据DeploymentStrategyType类型执行rolloutRecreate或rolloutRolling。 完整代码如下：\n// syncDeployment will sync the deployment with the given key. // This function is not meant to be invoked concurrently with the same key. func (dc *DeploymentController) syncDeployment(key string) error { startTime := time.Now() glog.V(4).Infof(\"Started syncing deployment %q (%v)\", key, startTime) defer func() { glog.V(4).Infof(\"Finished syncing deployment %q (%v)\", key, time.Since(startTime)) }() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return err } deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { glog.V(2).Infof(\"Deployment %v has been deleted\", key) return nil } if err != nil { return err } // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector{} if reflect.DeepEqual(d.Spec.Selector, \u0026everything) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, \"SelectingAll\", \"This deployment is selecting all pods. A non-empty selector is required.\") if d.Status.ObservedGeneration \u003c d.Generation { d.Status.ObservedGeneration = d.Generation dc.client.ExtensionsV1beta1().Deployments(d.Namespace).UpdateStatus(d) } return nil } // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(d) if err != nil { return err } // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil { return err } if d.DeletionTimestamp != nil { return dc.syncStatusOnly(d, rsList, podMap) } // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(d); err != nil { return err } if d.Spec.Paused { return dc.sync(d, rsList, podMap) } // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won't proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if d.Spec.RollbackTo != nil { return dc.rollback(d, rsList, podMap) } scalingEvent, err := dc.isScalingEvent(d, rsList, podMap) if err != nil { return err } if scalingEvent { return dc.sync(d, rsList, podMap) } switch d.Spec.Strategy.Type { case extensions.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case extensions.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList, podMap) } return fmt.Errorf(\"unexpected deployment strategy type: %s\", d.Spec.Strategy.Type) } 4.1. Get deployment // get namespace and deployment name namespace, name, err := cache.SplitMetaNamespaceKey(key) // get deployment by name deployment, err := dc.dLister.Deployments(namespace).Get(name) 4.2. getReplicaSetsForDeployment // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(d) getReplicaSetsForDeployment具体代码:\n// getReplicaSetsForDeployment uses ControllerRefManager to reconcile // ControllerRef by adopting and orphaning. // It returns the list of ReplicaSets that this Deployment should manage. func (dc *DeploymentController) getReplicaSetsForDeployment(d *apps.Deployment) ([]*apps.ReplicaSet, error) { // List all ReplicaSets to find those we own but that no longer match our // selector. They will be orphaned by ClaimReplicaSets(). rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) if err != nil { return nil, err } deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, fmt.Errorf(\"deployment %s/%s has invalid label selector: %v\", d.Namespace, d.Name, err) } // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing ReplicaSets (see #42639). canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) { fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(d.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != d.UID { return nil, fmt.Errorf(\"original Deployment %v/%v is gone: got uid %v, wanted %v\", d.Namespace, d.Name, fresh.UID, d.UID) } return fresh, nil }) cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) return cm.ClaimReplicaSets(rsList) } 4.3. getPodMapForDeployment // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) getPodMapForDeployment具体代码：\n// getPodMapForDeployment returns the Pods managed by a Deployment. // // It returns a map from ReplicaSet UID to a list of Pods controlled by that RS, // according to the Pod's ControllerRef. func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID]*v1.PodList, error) { // Get all Pods that potentially belong to this Deployment. selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, err } pods, err := dc.podLister.Pods(d.Namespace).List(selector) if err != nil { return nil, err } // Group Pods by their controller (if it's in rsList). podMap := make(map[types.UID]*v1.PodList, len(rsList)) for _, rs := range rsList { podMap[rs.UID] = \u0026v1.PodList{} } for _, pod := range pods { // Do not ignore inactive Pods because Recreate Deployments need to verify that no // Pods from older versions are running before spinning up new Pods. controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { continue } // Only append if we care about this UID. if podList, ok := podMap[controllerRef.UID]; ok { podList.Items = append(podList.Items, *pod) } } return podMap, nil } 4.4. checkPausedConditions // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(d); err != nil { return err } if d.Spec.Paused { return dc.sync(d, rsList) } checkPausedConditions具体代码:\n// checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition. // These conditions are needed so that we won't accidentally report lack of progress for resumed deployments // that were paused for longer than progressDeadlineSeconds. func (dc *DeploymentController) checkPausedConditions(d *apps.Deployment) error { if !deploymentutil.HasProgressDeadline(d) { return nil } cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if cond != nil \u0026\u0026 cond.Reason == deploymentutil.TimedOutReason { // If we have reported lack of progress, do not overwrite it with a paused condition. return nil } pausedCondExists := cond != nil \u0026\u0026 cond.Reason == deploymentutil.PausedDeployReason needsUpdate := false if d.Spec.Paused \u0026\u0026 !pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, \"Deployment is paused\") deploymentutil.SetDeploymentCondition(\u0026d.Status, *condition) needsUpdate = true } else if !d.Spec.Paused \u0026\u0026 pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, \"Deployment is resumed\") deploymentutil.SetDeploymentCondition(\u0026d.Status, *condition) needsUpdate = true } if !needsUpdate { return nil } var err error d, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(d) return err } 4.5. isScalingEvent scalingEvent, err := dc.isScalingEvent(d, rsList) if err != nil { return err } if scalingEvent { return dc.sync(d, rsList) } isScalingEvent具体代码:\n// isScalingEvent checks whether the provided deployment has been updated with a scaling event // by looking at the desired-replicas annotation in the active replica sets of the deployment. // // rsList should come from getReplicaSetsForDeployment(d). // podMap should come from getPodMapForDeployment(d, rsList). func (dc *DeploymentController) isScalingEvent(d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil { return false, err } allRSs := append(oldRSs, newRS) for _, rs := range controller.FilterActiveReplicaSets(allRSs) { desired, ok := deploymentutil.GetDesiredReplicasAnnotation(rs) if !ok { continue } if desired != *(d.Spec.Replicas) { return true, nil } } return false, nil } 4.6. rolloutRecreate switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) rolloutRecreate具体代码:\n// rolloutRecreate implements the logic for recreating a replica set. func (dc *DeploymentController) rolloutRecreate(d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) error { // Don't create a new RS if not already existed, so that we avoid scaling up before scaling down. newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) // scale down old replica sets. scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus. return dc.syncRolloutStatus(allRSs, newRS, d) } // Do not process a deployment when it has old pods running. if oldPodsRunning(newRS, oldRSs, podMap) { return dc.syncRolloutStatus(allRSs, newRS, d) } // If we need to create a new RS, create it now. if newRS == nil { newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs = append(oldRSs, newRS) } // scale up new replica set. if _, err := dc.scaleUpNewReplicaSetForRecreate(newRS, d); err != nil { return err } if util.DeploymentComplete(d, \u0026d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // Sync deployment status. return dc.syncRolloutStatus(allRSs, newRS, d) } 4.7. rolloutRolling switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) } rolloutRolling具体代码:\n// rolloutRolling implements the logic for rolling a new replica set. func (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // Scale up, if we can. scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d) if err != nil { return err } if scaledUp { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } // Scale down, if we can. scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } if deploymentutil.DeploymentComplete(d, \u0026d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // Sync deployment status return dc.syncRolloutStatus(allRSs, newRS, d) } 5. 总结 startDeploymentController主要包括NewDeploymentController和DeploymentController.Run两部分。\nNewDeploymentController主要构建DeploymentController结构体。\n该部分主要处理了以下逻辑：\n构建并运行事件处理器eventBroadcaster。 初始化赋值rsControl、clientset、workqueue。 添加dInformer、rsInformer、podInformer的ResourceEventHandlerFuncs，其中主要为AddFunc、UpdateFunc、DeleteFunc三类方法。 构造deployment、rs、pod的Informer的Lister函数和HasSynced函数。 赋值syncHandler，来实现syncDeployment。 DeploymentController.Run主要包含WaitForCacheSync和syncDeployment两部分。\nsyncDeployment基于给定的key执行sync deployment的操作。\n主要流程如下：\n通过SplitMetaNamespaceKey获取namespace和deployment对象的name。 调用Lister的接口获取的deployment的对象。 getReplicaSetsForDeployment获取deployment管理的ReplicaSet对象。 getPodMapForDeployment获取deployment管理的pod，基于ReplicaSet来分组。 checkPausedConditions检查deployment是否是pause状态并添加合适的condition。 isScalingEvent检查deployment的更新是否来自于一个scale的事件，如果是则执行scale的操作。 根据DeploymentStrategyType类型执行rolloutRecreate或rolloutRolling。 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/controller/deployment/deployment_controller.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/controller/deployment/rolling.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/apps.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要以deployment controller为例，分析该 …","ref":"/k8s-source-code-analysis/kube-controller-manager/deployment-controller/","tags":["源码分析"],"title":"kube-controller-manager源码分析（二）之 DeploymentController"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/docker/","tags":"","title":"Docker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/etcd/etcdctl/","tags":"","title":"etcdctl命令工具"},{"body":"1. glide简介 glide是一个golang项目的包管理工具，非常方便快捷，一般只需要2-3个命令就可以将go依赖包自动下载并归档到vendor的目录中。\n2. glide安装 go get github.com/Masterminds/glide 3. glide使用 #进入到项目目录 cd /home/gopath/src/demo #glide初始化，初始化配置文件glide.yaml glide init #glide加载依赖包，自动归档到vendor目录 glide up -v 4. glide的配置文件 glide.yaml记录依赖包列表。\npackage: demo import: - package: github.com/astaxie/beego version: v1.9.2 testImport: - package: github.com/smartystreets/goconvey version: 1.6.3 subpackages: - convey 5. glide-help 更多glide的命令帮助参考glide —help。\n➜ demo glide --help NAME: glide - Vendor Package Management for your Go projects. Each project should have a 'glide.yaml' file in the project directory. Files look something like this: package: github.com/Masterminds/glide imports: - package: github.com/Masterminds/cookoo version: 1.1.0 - package: github.com/kylelemons/go-gypsy subpackages: - yaml For more details on the 'glide.yaml' files see the documentation at https://glide.sh/docs/glide.yaml USAGE: glide [global options] command [command options] [arguments...] VERSION: 0.13.2-dev COMMANDS: create, init Initialize a new project, creating a glide.yaml file config-wizard, cw Wizard that makes optional suggestions to improve config in a glide.yaml file. get Install one or more packages into `vendor/` and add dependency to glide.yaml. remove, rm Remove a package from the glide.yaml file, and regenerate the lock file. import Import files from other dependency management systems. name Print the name of this project. novendor, nv List all non-vendor paths in a directory. rebuild Rebuild ('go build') the dependencies install, i Install a project's dependencies update, up Update a project's dependencies tree (Deprecated) Tree prints the dependencies of this project as a tree. list List prints all dependencies that the present code references. info Info prints information about this project cache-clear, cc Clears the Glide cache. about Learn about Glide mirror Manage mirrors help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --yaml value, -y value Set a YAML configuration file. (default: \"glide.yaml\") --quiet, -q Quiet (no info or debug messages) --debug Print debug verbose informational messages --home value The location of Glide files (default: \"/Users/meitu/.glide\") [$GLIDE_HOME] --tmp value The temp directory to use. Defaults to systems temp [$GLIDE_TMP] --no-color Turn off colored output for log messages --help, -h show help --version, -v print the version ","categories":"","description":"","excerpt":"1. glide简介 glide是一个golang项目的包管理工具，非常方便快捷，一般只需要2-3个命令就可以将go依赖包自动下载并归档 …","ref":"/golang-notes/introduction/package/glide-usage/","tags":["Golang"],"title":"glide的使用"},{"body":"在golangci-lint的使用介绍一文中我们提到了linter常见的几种配置，例如，gocyclo（圈复杂度），dupl（重复代码）等。本文主要介绍这几种常见linter的问题如何优化。\n1. gocyclo（圈复杂度） 在 Go 中，圈复杂度（Cyclomatic Complexity） 是一种衡量代码复杂度的指标，表示代码中可能的独立执行路径的数量。它反映了程序的逻辑复杂度，也就是代码有多少个分支或决策点（如 if、for、switch 等）。圈复杂度越高，代码的理解和维护成本就越大。\n.golangci.yml中圈复杂度的配置如下：\nlinters: enabel: - gocycle linters-settings: gocyclo: # Minimal code complexity to report. # Default: 30 (but we recommend 10-20) min-complexity: 10 1.1. 计算规则 圈复杂度的计算公式为：\nM=E−N+2P\n其中：\nE：代码中的边数（控制流图中的边，表示程序中的控制流路径，例如从一条语句跳转到下一条语句）。 N：代码中的节点数（控制流图中的节点，表示程序中的基本块或语句）。 P：控制流图中连通部分的数量（通常为 1）。 简单来说，圈复杂度可以通过统计代码中的以下决策点来估算，其中初始值为1，即默认返回路径，再根据以下规则累加：\n每个 if 或 else if 增加 1。 每个 for 或 while 循环增加 1。 每个 case（不包括 default）增加 1。 每个 \u0026\u0026 或 || 表达式增加 1。 举例说明：\nfunc Example(a, b int) int { if a \u003e 0 { // if语句 +1 if b \u003e 0 { // if语句 +1 return a + b } else { return a - b } } return 0 初始值为1 } 控制流路径为：\n初始值为1 2个if语句 + 2 因此该函数的圈复杂度为3。\n1.2. 常见标准 通常，圈复杂度以合理的范围为佳（例如：10-15），过低的圈复杂度会导致拆分函数过多，代码可读性变差。\n以下是圈复杂度的参考标准：\n1-10：代码逻辑简单，可维护性高。 11-20：代码逻辑较复杂，可能需要重构。 21+：代码逻辑非常复杂，维护成本高，建议拆分函数。 1.3. 优化圈复杂度 拆分函数：（最常见的方式）将复杂的函数拆分成多个小函数，每个函数只处理一种逻辑。 减少嵌套：使用 early return 减少嵌套层级。 简化逻辑：合并条件表达式，避免重复的分支逻辑。 优化圈复杂度的关键在于：\n减少嵌套，增加代码的扁平化。 提高代码的模块化和复用性。 2. dupl（重复代码） Golang 的静态分析工具（如 golangci-lint）能够检测代码中的重复部分。这类工具通常使用重复代码检测算法，通过对代码块进行特定分析，找到相似或完全相同的代码片段。\ngolangci-lint中检查重复代码的配置如下：\nlinters: enabel: - dupl linters-settings: dupl: # Tokens count to trigger issue. # Default: 150 threshold: 100 2.1. 计算规则 Tokenization（代码标记化）\n工具会将源代码分解为标记（Token），比如关键字、标识符、操作符等。通过这种方式，它可以忽略注释和格式差异，只关注代码的语义。\nToken 是代码的最小组成单元，包括：\n关键字：如 if、for、switch。 标识符：变量名、函数名、类型名等。 操作符：如 +、-、*、/ 等。 常量值：如 123、\"text\"。 界符：如 {、}、(、)。 threshold 参数的作用\n定义： threshold 是重复代码块的最小 Token 数。 如果两个代码块中有 相同 Token 的数量 \u003e= threshold，则会被认为是重复代码，并报告。 示例：threshold: 50\n如果两个代码块中有 50 个或更多的 Token 是相同的，dupl 会报告它们为重复代码。 小于 50 个 Token 的重复代码不会被报告。 2.2. 配置方式 通过配置threshold的大小可以控制重复代码的粒度。threshold默认值是150，可根据需要调整大小。\n阈值越小：越容易检测到小的重复代码，但可能导致误报增多。\n阈值越大：检测更宽松，只报告较大的重复代码块。\n当发现重复代码时，golangci-lint 会输出类似的报告：\nmain.go:10-20: duplicated code found in main.go:30-40 (dupl) 如果要忽略特定代码，在代码中添加注释：\n//nolint:dupl func SomeFunction() { // 重复代码块 } 2.3. 优化重复代码 提取函数：将重复逻辑抽象为通用函数。 映射表代替分支： 用键值对简化条件逻辑。 利用泛型： 合并不同类型的重复逻辑。 示例1：\n如果重复代码包含多个 if-else 或 switch，尝试用映射表或配置文件替代。\n// Before: 重复代码块 func GetDiscount(category string) float64 { switch category { case \"student\": return 0.15 case \"veteran\": return 0.2 case \"senior\": return 0.25 default: return 0.0 } } // After: 使用映射表 func GetDiscount(category string) float64 { discounts := map[string]float64{ \"student\": 0.15, \"veteran\": 0.2, \"senior\": 0.25, } return discounts[category] } 示例2：\n使用 泛型\n// Before: 重复代码块 func FindMaxInt(a, b int) int { if a \u003e b { return a } return b } func FindMaxFloat(a, b float64) float64 { if a \u003e b { return a } return b } // After: 使用泛型 func FindMax[T constraints.Ordered](a, b T) T { if a \u003e b { return a } return b } 3. 总结 本文主要分析了gocyclo（圈复杂度），dupl（重复代码）这2种linter配置的检测、配置和优化方式。因为在多人团队的开发中经常会因为开发者的水平不一，标准不一导致无法开发出统一且较高质量的代码。虽然代码的质量不会严格影响功能的运行，但可以为未来的开发者提供可读性更强，更方便接手开发的代码。同样通过这种方式也能初步看出一个开发者代码质量水平的高低。\n参考：\nhttps://golangci-lint.run/usage/linters/#gocyclo https://golangci-lint.run/usage/linters/#dupl ","categories":"","description":"","excerpt":"在golangci-lint的使用介绍一文中我们提到了linter常见的几种配置，例如，gocyclo（圈复杂度），dupl（重复代码）等。 …","ref":"/golang-notes/standard/fix-golangci-lint/","tags":"","title":"golangci-lint问题优化"},{"body":"1. HPA简介 HPA全称HorizontalPodAutoscaler，即pod水平扩容，增加或减少pod的数量。相对于VPA而言，VPA是增加或减少单个pod的CPU或内存。HPA主要作用于Deployment或Statefulset的工作负载，无法作用于Daemonset的工作负载。\n示例图：\nKubernetes 将水平 Pod 自动扩缩实现为一个间歇运行的控制回路（它不是一个连续的过程）。间隔由 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 参数设置（默认间隔为 15 秒）。\n在每个时间段内，控制器管理器都会根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 控制器管理器找到由 scaleTargetRef 定义的目标资源，然后根据目标资源的 .spec.selector 标签选择 Pod， 并从资源指标 API（针对每个 Pod 的资源指标）或自定义指标获取指标 API（适用于所有其他指标）。\nHPA依赖metrics-server来获取CPU和内存数据，以下说明metrics-server的部署流程。\n2. 部署metrics-server 下载metrics-server文件\nwget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 修改启动参数，增加--kubelet-insecure-tls，否则会报获取接口证书失败。\nspec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # 增加该参数 创建yaml服务\nkubectl apply -f components.yaml 通过kubectl top查看资源信息\n# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node1 144m 0% 5762Mi 2% node2 337m 0% 5475Mi 2% node3 100m 0% 5326Mi 2% node4 302m 0% 5649Mi 2% # kubectl top pod -n prometheus NAME CPU(cores) MEMORY(bytes) alertmanager-kube-prometheus-stack-alertmanager-0 1m 30Mi kube-prometheus-stack-grafana-7688b45b4c-mvwd6 1m 225Mi kube-prometheus-stack-kube-state-metrics-5d6578867c-25xbq 1m 21Mi kube-prometheus-stack-operator-9c5fbdc68-nrn7h 1m 33Mi kube-prometheus-stack-prometheus-node-exporter-8ghd8 1m 4Mi kube-prometheus-stack-prometheus-node-exporter-brtp9 1m 4Mi kube-prometheus-stack-prometheus-node-exporter-n4kdp 1m 4Mi kube-prometheus-stack-prometheus-node-exporter-ttksv 1m 4Mi prometheus-kube-prometheus-stack-prometheus-0 8m 622Mi 同时在k8s dashboard上也可以查看到实时的CPU和内存信息。\n3. HPA配置 todo\n参考：\nhttps://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/ https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server https://github.com/kubernetes-sigs/metrics-server https://www.qikqiak.com/post/k8s-hpa-usage/ ","categories":"","description":"","excerpt":"1. HPA简介 HPA全称HorizontalPodAutoscaler，即pod水平扩容，增加或减少pod的数量。相对于VPA而 …","ref":"/kubernetes-notes/operation/deployment/k8s-hpa-usage/","tags":["Kubernetes"],"title":"HPA[自动扩缩容]配置"},{"body":"1. 证书分类 服务器证书：server cert，用于客户端验证服务端的身份。\n客户端证书：client cert，用于服务端验证客户端的身份。\n对等证书：peer cert（既是server cert又是client cert），用户成员之间的身份验证，例如 etcd。\n1.1. k8s集群的证书分类 etcd节点：需要标识自己服务的server cert，也需要client cert与etcd集群其他节点交互，因此需要一个对等证书。 master节点：需要标识 apiserver服务的server cert，也需要client cert连接etcd集群，也需要一个对等证书。 kubelet：需要标识自己服务的server cert，也需要client cert请求apiserver，也使用一个对等证书。 kubectl、kube-proxy、calico：需要client证书。 2. CA证书及秘钥 目录：/etc/kubernetes/ssl\n分类 证书/秘钥 说明 组件 ca ca-key.pem ca.pem ca.csr Kubernetes kubernetes-key.pem kubernetes.pem kubernetes.csr Admin admin-key.pem admin.pem admin.csr Kubelet kubelet.crt kubelet.key 配置文件\n分类 证书/秘钥 说明 ca ca-config.json ca-csr.json Kubernetes kubernetes-csr.json Admin admin-csr.json Kube-proxy kube-proxy-csr.json 3. cfssl工具 安装cfssl：\n# 下载cfssl $ curl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl $ curl https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson $ curl https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfo # 添加可执行权限 $ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson /usr/local/bin/cfssl-certinfo 4. 创建 CA (Certificate Authority) 4.1. 配置源文件 创建 CA 配置文件\nca-config.json\ncat \u003c\u003c EOF \u003e ca-config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"876000h\" } } } } EOF 参数说明\nca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示client可以用该 CA 对server提供的证书进行验证； client auth：表示server可以用该CA对client提供的证书进行验证； 创建 CA 证书签名请求\nca-csr.json\ncat \u003c\u003c EOF \u003e ca-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 参数说明\nca-csr.json的参数\nCN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； names中的字段：\nC : country，国家 ST: state，州或省份 L：location，城市 O：organization，组织，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) OU：organization unit 4.2. 执行命令 cfssl gencert -initca ca-csr.json | cfssljson -bare ca 输出如下：\n# cfssl gencert -initca ca-csr.json | cfssljson -bare ca 2019/12/13 14:35:52 [INFO] generating a new CA key and certificate from CSR 2019/12/13 14:35:52 [INFO] generate received request 2019/12/13 14:35:52 [INFO] received CSR 2019/12/13 14:35:52 [INFO] generating key: rsa-2048 2019/12/13 14:35:52 [INFO] encoded CSR 2019/12/13 14:35:52 [INFO] signed certificate with serial number 248379771349454958117219047414671162179070747780 生成以下文件：\n# 生成文件 -rw-r--r-- 1 root root 1005 12月 13 11:32 ca.csr -rw------- 1 root root 1675 12月 13 11:32 ca-key.pem -rw-r--r-- 1 root root 1363 12月 13 11:32 ca.pem # 配置源文件 -rw-r--r-- 1 root root 293 12月 13 11:31 ca-config.json -rw-r--r-- 1 root root 210 12月 13 11:31 ca-csr.json 5. 创建 kubernetes 证书 5.1. 配置源文件 创建 kubernetes 证书签名请求文件kubernetes-csr.json。\ncat \u003c\u003c EOF \u003e kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"\u003cMASTER_IP\u003e\", \"\u003cMASTER_CLUSTER_IP\u003e\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [{ \"C\": \"\u003ccountry\u003e\", \"ST\": \"\u003cstate\u003e\", \"L\": \"\u003ccity\u003e\", \"O\": \"\u003corganization\u003e\", \"OU\": \"\u003corganization unit\u003e\" }] } EOF 参数说明：\nMASTER_IP：master节点的IP或域名 MASTER_CLUSTER_IP：kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，例如（10.254.0.1）。 5.2. 执行命令 $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 输出如下：\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 2019/12/13 14:40:28 [INFO] generate received request 2019/12/13 14:40:28 [INFO] received CSR 2019/12/13 14:40:28 [INFO] generating key: rsa-2048 2019/12/13 14:40:28 [INFO] encoded CSR 2019/12/13 14:40:28 [INFO] signed certificate with serial number 392795299385191732458211386861696542628305189374 2019/12/13 14:40:28 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). 生成以下文件：\n# 生成文件 -rw-r--r-- 1 root root 1269 12月 13 14:40 kubernetes.csr -rw------- 1 root root 1679 12月 13 14:40 kubernetes-key.pem -rw-r--r-- 1 root root 1643 12月 13 14:40 kubernetes.pem # 配置源文件 -rw-r--r-- 1 root root 580 12月 13 14:40 kubernetes-csr.json 6. 创建 admin 证书 6.1. 配置源文件 创建 admin 证书签名请求文件 admin-csr.json：\ncat \u003c\u003c EOF \u003e admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } EOF 6.2. 执行命令 $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 输出如下：\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 2019/12/13 14:52:37 [INFO] generate received request 2019/12/13 14:52:37 [INFO] received CSR 2019/12/13 14:52:37 [INFO] generating key: rsa-2048 2019/12/13 14:52:37 [INFO] encoded CSR 2019/12/13 14:52:37 [INFO] signed certificate with serial number 465422983473444224050765004141217688748259757371 2019/12/13 14:52:37 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). 生成文件\n# 生成文件 -rw-r--r-- 1 root root 1013 12月 13 14:52 admin.csr -rw------- 1 root root 1675 12月 13 14:52 admin-key.pem -rw-r--r-- 1 root root 1407 12月 13 14:52 admin.pem # 配置源文件 -rw-r--r-- 1 root root 231 12月 13 14:49 admin-csr.json 7. 创建 kube-proxy 证书 7.1. 配置源文件 创建 kube-proxy 证书签名请求文件 kube-proxy-csr.json：\ncat \u003c\u003c EOF \u003e kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 7.2. 执行命令 $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 输出如下：\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 2019/12/13 19:37:48 [INFO] generate received request 2019/12/13 19:37:48 [INFO] received CSR 2019/12/13 19:37:48 [INFO] generating key: rsa-2048 2019/12/13 19:37:48 [INFO] encoded CSR 2019/12/13 19:37:48 [INFO] signed certificate with serial number 526712749765692443642491255093816136154324531741 2019/12/13 19:37:48 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). 生成文件：\n# 生成文件 -rw-r--r-- 1 root root 1009 12月 13 19:37 kube-proxy.csr -rw------- 1 root root 1675 12月 13 19:37 kube-proxy-key.pem -rw-r--r-- 1 root root 1407 12月 13 19:37 kube-proxy.pem # 配置源文件 -rw-r--r-- 1 root root 230 12月 13 19:37 kube-proxy-csr.json 8. 校验证书 openssl x509 -noout -text -in kubernetes.pem 输出如下：\n# openssl x509 -noout -text -in kubernetes.pem Certificate: Data: Version: 3 (0x2) Serial Number: 44:cd:8c:e6:a4:60:ff:3f:09:af:02:e7:68:5e:f2:0f:e6:a0:39:fe Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=ShenZhen, L=ShenZhen, O=k8s, OU=System, CN=kubernetes Validity Not Before: Dec 13 06:35:00 2019 GMT Not After : Nov 19 06:35:00 2119 GMT Subject: C=CN, ST=ShenZhen, L=ShenZhen, O=k8s, OU=System, CN=kubernetes Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:d7:91:4f:90:56:fb:ab:a9:de:c4:98:9e:d7:e6: 45:db:5a:14:9a:76:78:6a:4c:db:3c:47:3c:e7:1c: 3c:37:4e:8a:cf:9c:a1:8a:4c:51:4c:cd:45:b0:03: 87:06:b9:20:2c:3a:51:f9:21:55:1c:90:7c:f8:93: bc:6a:48:05:3d:8b:74:fd:f2:f1:e6:5e:ad:b4:a8: f6:6d:f9:63:9e:e4:b4:cc:68:9e:90:d7:ef:de:ce: c1:1d:1b:68:59:68:5e:5f:7d:5c:f3:49:4f:18:72: be:b5:c8:af:e2:8d:34:9c:d2:68:b7:8c:26:69:cc: a5:f4:ca:69:2d:d7:21:f5:19:2e:b2:b5:97:16:87: 9f:9c:fd:01:97:c2:0e:20:b4:88:27:9a:37:9a:af: 0a:cf:82:4f:26:24:cb:07:ac:8c:b1:34:20:42:22: 00:b2:b0:98:c5:53:01:fb:32:aa:15:1b:7e:39:44: ae:af:6e:c3:65:96:f6:38:7a:87:37:d0:31:63:d8: a4:15:13:f2:56:da:e6:09:45:2b:46:2c:cb:63:db: f7:ba:7f:44:0a:36:39:7c:cc:5b:42:e5:56:c7:7f: dd:64:5c:f2:4a:af:d3:a9:d1:6e:06:27:57:09:4d: db:08:62:87:66:c8:2c:36:00:41:f1:90:f6:5f:68: 20:3d Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 3D:3F:FA:B8:36:D7:FE:B1:59:BE:B1:F5:C1:5D:88:3D:BC:78:9F:87 X509v3 Authority Key Identifier: keyid:40:A2:D4:30:22:12:2E:C2:FB:A2:55:2C:CB:F0:F6:3E:4D:B8:02:03 X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.20.0.112, IP Address:172.20.0.113, IP Address:172.20.0.114, IP Address:172.20.0.115, IP Address:10.254.0.1 Signature Algorithm: sha256WithRSAEncryption 63:50:f6:2a:03:c7:35:dd:e9:10:8d:2f:b3:27:9a:64:f3:e1: 11:8a:18:1e:fa:6d:85:30:11:b4:59:a3:6c:86:cd:2b:5c:67: 17:4f:aa:0d:bb:4c:ee:c8:af:e7:3d:61:6d:03:9d:14:6f:00: 48:56:59:b5:76:13:a9:30:23:e0:b2:d2:12:64:0c:60:0d:76: ec:c6:4f:b1:bc:24:01:7a:48:c6:fd:9e:5d:68:da:b9:a1:ad: 30:7a:ba:90:e2:e3:4e:b4:92:1b:c5:f2:8c:c1:b0:3d:fc:14: d2:46:e8:f8:22:8f:d9:4d:85:4f:58:6b:0f:84:78:06:b4:b9: 92:b9:0d:bd:1d:95:e9:0d:42:d3:fd:dd:2a:59:60:3f:63:35: eb:07:25:d2:ea:0d:19:a6:f3:dc:92:8e:ee:73:04:15:5e:97: e8:da:51:c3:69:49:96:36:c7:cc:5b:e5:e5:cb:e5:ce:9f:21: 6f:6b:56:16:bf:85:ad:1c:8c:91:c1:91:0a:90:18:e2:4a:b0: 32:58:33:ef:55:8e:8f:4a:e3:0f:b8:f7:41:04:65:89:e1:1b: d8:41:28:6e:84:c3:1c:8e:a9:a0:8a:42:e4:fe:d7:fe:0e:24: dc:74:37:fa:5e:be:20:69:c5:9a:5a:e6:83:1c:0b:9e:e1:43: ef:4f:7a:37 字段说明：\n确认 Issuer 字段的内容和 ca-csr.json 一致； 确认 Subject 字段的内容和 kubernetes-csr.json 一致； 确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致； 确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致； 9. 分发证书 将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下。\nmkdir -p /etc/kubernetes/ssl cp *.pem /etc/kubernetes/ssl 参考文章：\nhttps://kubernetes.io/docs/concepts/cluster-administration/certificates/ https://coreos.com/os/docs/latest/generate-self-signed-certificates.html https://jimmysong.io/kubernetes-handbook/practice/create-tls-and-secret-key.html ","categories":"","description":"","excerpt":"1. 证书分类 服务器证书：server cert，用于客户端验证服务端的身份。\n客户端证书：client cert，用于服务端验证客户端的 …","ref":"/kubernetes-notes/setup/k8s-cert/","tags":["Kubernetes"],"title":"k8s证书及秘钥"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/multi-cluster/karmada/","tags":"","title":"Karmada"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kube-controller-manager/","tags":"","title":"kube-controller-manager"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析 https://github.com/kubernetes/kubernetes/tree/v1.12.0/pkg/kubelet 部分的代码。\n本文主要分析kubelet中的NewMainKubelet部分。\n1. NewMainKubelet NewMainKubelet主要用来初始化和构造一个kubelet结构体，kubelet结构体定义参考:https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go#L888\n// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, containerRuntime string, runtimeCgroups string, hostnameOverride string, nodeIP string, providerID string, cloudProvider string, certDirectory string, rootDirectory string, registerNode bool, registerWithTaints []api.Taint, allowedUnsafeSysctls []string, remoteRuntimeEndpoint string, remoteImageEndpoint string, experimentalMounterPath string, experimentalKernelMemcgNotification bool, experimentalCheckNodeCapabilitiesBeforeMount bool, experimentalNodeAllocatableIgnoreEvictionThreshold bool, minimumGCAge metav1.Duration, maxPerPodContainerCount int32, maxContainerCount int32, masterServiceNamespace string, registerSchedulable bool, nonMasqueradeCIDR string, keepTerminatedPodVolumes bool, nodeLabels map[string]string, seccompProfileRoot string, bootstrapCheckpointPath string, nodeStatusMaxImages int32) (*Kubelet, error) { ... } 1.1. PodConfig 通过makePodSourceConfig生成Pod config。\nif kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil { return nil, err } } 1.1.1. makePodSourceConfig // makePodSourceConfig creates a config.PodConfig from the given // KubeletConfiguration or returns an error. func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, bootstrapCheckpointPath string) (*config.PodConfig, error) { ... // source of all configuration cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder) // define file config source if kubeCfg.StaticPodPath != \"\" { glog.Infof(\"Adding pod path: %v\", kubeCfg.StaticPodPath) config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource)) } // define url config source if kubeCfg.StaticPodURL != \"\" { glog.Infof(\"Adding pod url %q with HTTP header %v\", kubeCfg.StaticPodURL, manifestURLHeader) config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(kubetypes.HTTPSource)) } // Restore from the checkpoint path // NOTE: This MUST happen before creating the apiserver source // below, or the checkpoint would override the source of truth. ... if kubeDeps.KubeClient != nil { glog.Infof(\"Watching apiserver\") if updatechannel == nil { updatechannel = cfg.Channel(kubetypes.ApiserverSource) } config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, updatechannel) } return cfg, nil } 1.1.2. NewPodConfig // NewPodConfig creates an object that can merge many configuration sources into a stream // of normalized updates to a pod configuration. func NewPodConfig(mode PodConfigNotificationMode, recorder record.EventRecorder) *PodConfig { updates := make(chan kubetypes.PodUpdate, 50) storage := newPodStorage(updates, mode, recorder) podConfig := \u0026PodConfig{ pods: storage, mux: config.NewMux(storage), updates: updates, sources: sets.String{}, } return podConfig } 1.1.3. NewSourceApiserver // NewSourceApiserver creates a config source that watches and pulls from the apiserver. func NewSourceApiserver(c clientset.Interface, nodeName types.NodeName, updates chan\u003c- interface{}) { lw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), \"pods\", metav1.NamespaceAll, fields.OneTermEqualSelector(api.PodHostField, string(nodeName))) newSourceApiserverFromLW(lw, updates) } 1.2. Lister serviceLister和nodeLister分别通过List-Watch机制监听service和node的列表变化。\n1.2.1. serviceLister serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}) if kubeDeps.KubeClient != nil { serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything()) r := cache.NewReflector(serviceLW, \u0026v1.Service{}, serviceIndexer, 0) go r.Run(wait.NeverStop) } serviceLister := corelisters.NewServiceLister(serviceIndexer) 1.2.2. nodeLister nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{}) if kubeDeps.KubeClient != nil { fieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector() nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector) r := cache.NewReflector(nodeLW, \u0026v1.Node{}, nodeIndexer, 0) go r.Run(wait.NeverStop) } nodeInfo := \u0026predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)} 1.3. 各种Manager 1.3.1. containerRefManager containerRefManager := kubecontainer.NewRefManager() 1.3.2. oomWatcher oomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder) 1.3.3. dnsConfigurer clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS)) for _, ipEntry := range kubeCfg.ClusterDNS { ip := net.ParseIP(ipEntry) if ip == nil { glog.Warningf(\"Invalid clusterDNS ip '%q'\", ipEntry) } else { clusterDNS = append(clusterDNS, ip) } } ... dns.NewConfigurer(kubeDeps.Recorder, nodeRef, parsedNodeIP, clusterDNS, kubeCfg.ClusterDomain, kubeCfg.ResolverConfig), 1.3.4. secretManager \u0026 configMapManager var secretManager secret.Manager var configMapManager configmap.Manager switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy { case kubeletconfiginternal.WatchChangeDetectionStrategy: secretManager = secret.NewWatchingSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewWatchingConfigMapManager(kubeDeps.KubeClient) case kubeletconfiginternal.TTLCacheChangeDetectionStrategy: secretManager = secret.NewCachingSecretManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) configMapManager = configmap.NewCachingConfigMapManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) case kubeletconfiginternal.GetChangeDetectionStrategy: secretManager = secret.NewSimpleSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewSimpleConfigMapManager(kubeDeps.KubeClient) default: return nil, fmt.Errorf(\"unknown configmap and secret manager mode: %v\", kubeCfg.ConfigMapAndSecretChangeDetectionStrategy) } klet.secretManager = secretManager klet.configMapManager = configMapManager 1.3.5. livenessManager klet.livenessManager = proberesults.NewManager() 1.3.6. podManager // podManager is also responsible for keeping secretManager and configMapManager contents up-to-date. klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager) 1.3.7. resourceAnalyzer klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration) 1.3.8. containerGC // setup containerGC containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) if err != nil { return nil, err } klet.containerGC = containerGC klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, integer.IntMax(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod)) 1.3.9. imageManager // setup imageManager imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage) if err != nil { return nil, fmt.Errorf(\"failed to initialize image manager: %v\", err) } klet.imageManager = imageManager 1.3.10. statusManager klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) 1.3.11. probeManager klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.runner, containerRefManager, kubeDeps.Recorder) 1.3.12. tokenManager tokenManager := token.NewManager(kubeDeps.KubeClient) 1.3.13. volumePluginMgr klet.volumePluginMgr, err = NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber) if err != nil { return nil, err } if klet.enablePluginsWatcher { klet.pluginWatcher = pluginwatcher.NewWatcher(klet.getPluginsDir()) } 1.3.14. volumeManager // setup volumeManager klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.statusManager, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, klet.getPodsDir(), kubeDeps.Recorder, experimentalCheckNodeCapabilitiesBeforeMount, keepTerminatedPodVolumes) 1.3.15. evictionManager // setup eviction manager evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock) klet.evictionManager = evictionManager klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler) 1.4. containerRuntime 目前pod所使用的runtime只有docker和remote两种，rkt已经废弃。\nif containerRuntime == \"rkt\" { glog.Fatalln(\"rktnetes has been deprecated in favor of rktlet. Please see https://github.com/kubernetes-incubator/rktlet for more information.\") } 当runtime是docker的时候，会执行docker相关操作。\nswitch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. ... // The unix socket for kubelet \u003c-\u003e dockershim communication. ... // Create dockerLegacyService when the logging driver is not supported. ... case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\"unsupported CRI runtime: %q\", containerRuntime) } 1.4.1. NewDockerService // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil { return nil, err } if crOptions.RedirectContainerStreaming { klet.criHandler = ds } 1.4.2. NewDockerServer // The unix socket for kubelet \u003c-\u003e dockershim communication. glog.V(5).Infof(\"RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q\", remoteRuntimeEndpoint, remoteImageEndpoint) glog.V(2).Infof(\"Starting the GRPC server for the docker CRI shim.\") server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } 1.4.3. DockerServer.Start // Start starts the dockershim grpc server. func (s *DockerServer) Start() error { // Start the internal service. if err := s.service.Start(); err != nil { glog.Errorf(\"Unable to start docker service\") return err } glog.V(2).Infof(\"Start dockershim grpc server\") l, err := util.CreateListener(s.endpoint) if err != nil { return fmt.Errorf(\"failed to listen on %q: %v\", s.endpoint, err) } // Create the grpc server and register runtime and image services. s.server = grpc.NewServer( grpc.MaxRecvMsgSize(maxMsgSize), grpc.MaxSendMsgSize(maxMsgSize), ) runtimeapi.RegisterRuntimeServiceServer(s.server, s.service) runtimeapi.RegisterImageServiceServer(s.server, s.service) go func() { if err := s.server.Serve(l); err != nil { glog.Fatalf(\"Failed to serve connections: %v\", err) } }() return nil } 1.5. podWorker 构造podWorkers和workQueue。\nklet.workQueue = queue.NewBasicWorkQueue(klet.clock) klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) 1.5.1. PodWorkers接口 // PodWorkers is an abstract interface for testability. type PodWorkers interface { UpdatePod(options *UpdatePodOptions) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) ForgetWorker(uid types.UID) } podWorker主要用来对pod相应事件进行处理和同步，包含以下三个方法：UpdatePod、ForgetNonExistingPodWorkers、ForgetWorker。\n2. 总结 NewMainKubelet主要用来构造kubelet结构体，其中kubelet除了包含必要的配置和client（例如：kubeClient、csiClient等）外，最主要的包含各种manager来管理不同的任务。\n核心的manager有以下几种：\noomWatcher：监控pod内存是否发生OOM。 podManager：管理pod的生命周期，包括对pod的增删改查操作等。 containerGC：对死亡容器进行垃圾回收。 imageManager：对容器镜像进行垃圾回收。 statusManager：与apiserver同步pod状态，同时也作状态缓存。 volumeManager：对pod的volume进行attached/detached/mounted/unmounted操作。 evictionManager：保证节点稳定，必要时对pod进行驱逐（例如资源不足的情况下）。 NewMainKubelet还包含了serviceLister和nodeLister来监听service和node的列表变化。\nkubelet使用到的containerRuntime目前主要是docker，其中rkt已废弃。NewMainKubelet启动了dockershim grpc server来执行docker相关操作。\n构建了podWorker来对pod相关的更新逻辑进行处理。\n参考文章：\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0 https://github.com/kubernetes/kubernetes/tree/v1.12.0/pkg/kubelet ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。 …","ref":"/k8s-source-code-analysis/kubelet/newmainkubelet/","tags":["源码分析"],"title":"kubelet源码分析（二）之 NewMainKubelet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/openyurt/code-analysis/","tags":"","title":"OpenYurt源码分析"},{"body":"1. PVC概述 PersistentVolumeClaim（简称PVC）是用户存储的请求，PVC消耗PV的资源，可以请求特定的大小和访问模式，需要指定归属于某个Namespace，在同一个Namespace的Pod才可以指定对应的PVC。\n当需要不同性质的PV来满足存储需求时，可以使用StorageClass来实现。\n每个 PVC 中都包含一个 spec 规格字段和一个 status 声明状态字段。\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} 2. PVC的属性 2.1. accessModes 对应存储的访问模式，例如：ReadWriteOnce。\n2.2. volumeMode 对应存储的数据卷模式，例如：Filesystem。\n2.3. resources 声明可以请求特定数量的资源。相同的资源模型适用于Volume和PVC。\n2.4. selector 声明label selector，只有标签与选择器匹配的卷可以绑定到声明。\nmatchLabels：volume 必须有具有该值的标签 matchExpressions：条件列表，通过条件表达式筛选匹配的卷。有效的运算符包括 In、NotIn、Exists 和 DoesNotExist。 2.5. storageClassName 通过storageClassName参数来指定使用对应名字的StorageClass，只有所请求的类与 PVC 具有相同 storageClassName 的 PV 才能绑定到 PVC。\nPVC可以不指定storageClassName，或者将该值设置为空，如果打开了准入控制插件，并且指定一个默认的 StorageClass，则PVC会使用默认的StorageClass，否则就绑定到没有StorageClass的 PV上。\n之前使用注解 volume.beta.kubernetes.io/storage-class 而不是 storageClassName 属性。这个注解仍然有效，但是在未来的 Kubernetes 版本中不会支持。\n3. 将PVC作为Volume 将PVC作为Pod的Volume，PVC与Pod需要在同一个命名空间下，其实Pod的声明如下：\nkind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \"/var/www/html\" name: mypd volumes: - name: mypd persistentVolumeClaim: # 使用PVC claimName: myclaim PersistentVolumes 绑定是唯一的，并且由于 PersistentVolumeClaims 是命名空间对象，因此只能在一个命名空间内挂载具有“多个”模式（ROX、RWX）的PVC。\n参考文章：\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/ ","categories":"","description":"","excerpt":"1. PVC概述 PersistentVolumeClaim（简称PVC）是用户存储的请求，PVC消耗PV的资源，可以请求特定的大小和访问模 …","ref":"/kubernetes-notes/storage/volume/persistent-volume-claim/","tags":["Kubernetes"],"title":"PersistentVolumeClaim 介绍"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/pod/","tags":"","title":"Pod对象"},{"body":"1. 部署Redis集群 redis的安装及配置参考[redis部署]\n本文以创建一主二从的集群为例。\n1.1 部署与配置 先创建sentinel目录，在该目录下创建8000，8001，8002三个以端口号命名的目录。\nmkdir sentinel cd sentinel mkdir 8000 8001 8002 在对应端口号目录中创建redis.conf的文件，配置文件中的端口号port参数改为对应目录的端口号。配置如下：\n# 守护进程模式 daemonize yes # pid file pidfile /var/run/redis.pid # 监听端口 port 8000 # TCP接收队列长度，受/proc/sys/net/core/somaxconn和tcp_max_syn_backlog这两个内核参数的影响 tcp-backlog 511 # 一个客户端空闲多少秒后关闭连接(0代表禁用，永不关闭) timeout 0 # 如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACK tcp-keepalive 60 # 指定服务器调试等级 # 可能值： # debug （大量信息，对开发/测试有用） # verbose （很多精简的有用信息，但是不像debug等级那么多） # notice （适量的信息，基本上是你生产环境中需要的） # warning （只有很重要/严重的信息会记录下来） loglevel notice # 指明日志文件名 logfile \"./redis8000.log\" # 设置数据库个数 databases 16 # 会在指定秒数和数据变化次数之后把数据库写到磁盘上 # 900秒（15分钟）之后，且至少1次变更 # 300秒（5分钟）之后，且至少10次变更 # 60秒之后，且至少10000次变更 save 900 1 save 300 10 save 60 10000 # 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作 # 这将使用户知道数据没有正确的持久化到硬盘，否则可能没人注意到并且造成一些灾难 stop-writes-on-bgsave-error yes # 当导出到 .rdb 数据库时是否用LZF压缩字符串对象 rdbcompression yes # 版本5的RDB有一个CRC64算法的校验和放在了文件的最后。这将使文件格式更加可靠。 rdbchecksum yes # 持久化数据库的文件名 dbfilename dump.rdb # 工作目录 dir ./ # 当master服务设置了密码保护时，slave服务连接master的密码 masterauth 0234kz9*l # 当一个slave失去和master的连接，或者同步正在进行中，slave的行为可以有两种： # # 1) 如果 slave-serve-stale-data 设置为 \"yes\" (默认值)，slave会继续响应客户端请求， # 可能是正常数据，或者是过时了的数据，也可能是还没获得值的空数据。 # 2) 如果 slave-serve-stale-data 设置为 \"no\"，slave会回复\"正在从master同步 # （SYNC with master in progress）\"来处理各种请求，除了 INFO 和 SLAVEOF 命令。 slave-serve-stale-data yes # 你可以配置salve实例是否接受写操作。可写的slave实例可能对存储临时数据比较有用(因为写入salve # 的数据在同master同步之后将很容易被删除 slave-read-only yes # 是否在slave套接字发送SYNC之后禁用 TCP_NODELAY？ # 如果你选择“yes”Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave # 上有延迟，Linux内核的默认配置会达到40毫秒 # 如果你选择了 \"no\" 数据传输到salve的延迟将会减少但要使用更多的带宽 repl-disable-tcp-nodelay no # slave的优先级是一个整数展示在Redis的Info输出中。如果master不再正常工作了，哨兵将用它来 # 选择一个slave提升=升为master。 # 优先级数字小的salve会优先考虑提升为master，所以例如有三个slave优先级分别为10，100，25， # 哨兵将挑选优先级最小数字为10的slave。 # 0作为一个特殊的优先级，标识这个slave不能作为master，所以一个优先级为0的slave永远不会被 # 哨兵挑选提升为master slave-priority 100 # 密码验证 # 警告：因为Redis太快了，所以外面的人可以尝试每秒150k的密码来试图破解密码。这意味着你需要 # 一个高强度的密码，否则破解太容易了 requirepass 0234kz9*l # redis实例最大占用内存，不要用比设置的上限更多的内存。一旦内存使用达到上限，Redis会根据选定的回收策略（参见： # maxmemmory-policy）删除key maxmemory 3gb # 最大内存策略：如果达到内存限制了，Redis如何选择删除key。你可以在下面五个行为里选： # volatile-lru -\u003e 根据LRU算法删除带有过期时间的key。 # allkeys-lru -\u003e 根据LRU算法删除任何key。 # volatile-random -\u003e 根据过期设置来随机删除key, 具备过期时间的key。 # allkeys-\u003erandom -\u003e 无差别随机删, 任何一个key。 # volatile-ttl -\u003e 根据最近过期时间来删除（辅以TTL）, 这是对于有过期时间的key # noeviction -\u003e 谁也不删，直接在写操作时返回错误。 maxmemory-policy volatile-lru # 默认情况下，Redis是异步的把数据导出到磁盘上。这种模式在很多应用里已经足够好，但Redis进程 # 出问题或断电时可能造成一段时间的写操作丢失(这取决于配置的save指令)。 # # AOF是一种提供了更可靠的替代持久化模式，例如使用默认的数据写入文件策略（参见后面的配置） # 在遇到像服务器断电或单写情况下Redis自身进程出问题但操作系统仍正常运行等突发事件时，Redis # 能只丢失1秒的写操作。 # # AOF和RDB持久化能同时启动并且不会有问题。 # 如果AOF开启，那么在启动时Redis将加载AOF文件，它更能保证数据的可靠性。 appendonly no # aof文件名 appendfilename \"appendonly.aof\" # fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。 # 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。 # # Redis支持三种不同的模式： # # no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。 # always：每次写操作都立刻写入到aof文件。慢，但是最安全。 # everysec：每秒写一次。折中方案。 appendfsync everysec # 如果AOF的同步策略设置成 \"always\" 或者 \"everysec\"，并且后台的存储进程（后台存储或写入AOF # 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 fsync()系统调用而阻塞很久。 # 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。 # # 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止主进程进行fsync()。 # # 这就意味着如果有子进程在进行保存操作，那么Redis就处于\"不可同步\"的状态。 # 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定） # # 如果你有延时问题把这个设置成\"yes\"，否则就保持\"no\"，这是保存持久数据的最安全的方式。 no-appendfsync-on-rewrite yes # 自动重写AOF文件 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # AOF文件可能在尾部是不完整的（这跟system关闭有问题，尤其是mount ext4文件系统时 # 没有加上data=ordered选项。只会发生在os死时，redis自己死不会不完整）。 # 那redis重启时load进内存的时候就有问题了。 # 发生的时候，可以选择redis启动报错，并且通知用户和写日志，或者load尽量多正常的数据。 # 如果aof-load-truncated是yes，会自动发布一个log给客户端然后load（默认）。 # 如果是no，用户必须手动redis-check-aof修复AOF文件才可以。 # 注意，如果在读取的过程中，发现这个aof是损坏的，服务器也是会退出的， # 这个选项仅仅用于当服务器尝试读取更多的数据但又找不到相应的数据时。 aof-load-truncated yes # Lua 脚本的最大执行时间，毫秒为单位 lua-time-limit 5000 # Redis慢查询日志可以记录超过指定时间的查询 slowlog-log-slower-than 10000 # 这个长度没有限制。只是要主要会消耗内存。你可以通过 SLOWLOG RESET 来回收内存。 slowlog-max-len 128 # redis延时监控系统在运行时会采样一些操作，以便收集可能导致延时的数据根源。 # 通过 LATENCY命令 可以打印一些图样和获取一些报告，方便监控 # 这个系统仅仅记录那个执行时间大于或等于预定时间（毫秒）的操作, # 这个预定时间是通过latency-monitor-threshold配置来指定的， # 当设置为0时，这个监控系统处于停止状态 latency-monitor-threshold 0 # Redis能通知 Pub/Sub 客户端关于键空间发生的事件，默认关闭 notify-keyspace-events \"\" # 当hash只有少量的entry时，并且最大的entry所占空间没有超过指定的限制时，会用一种节省内存的 # 数据结构来编码。可以通过下面的指令来设定限制 hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # 与hash似，数据元素较少的list，可以用另一种方式来编码从而节省大量空间。 # 这种特殊的方式只有在符合下面限制时才可以用 list-max-ziplist-entries 512 list-max-ziplist-value 64 # set有一种特殊编码的情况：当set数据全是十进制64位有符号整型数字构成的字符串时。 # 下面这个配置项就是用来设置set使用这种编码来节省内存的最大长度。 set-max-intset-entries 512 # 与hash和list相似，有序集合也可以用一种特别的编码方式来节省大量空间。 # 这种编码只适合长度和元素都小于下面限制的有序集合 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog稀疏结构表示字节的限制。该限制包括 # 16个字节的头。当HyperLogLog使用稀疏结构表示 # 这些限制，它会被转换成密度表示。 # 值大于16000是完全没用的，因为在该点 # 密集的表示是更多的内存效率。 # 建议值是3000左右，以便具有的内存好处, 减少内存的消耗 hll-sparse-max-bytes 3000 # 启用哈希刷新，每100个CPU毫秒会拿出1个毫秒来刷新Redis的主哈希表（顶级键值映射表） activerehashing yes # 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器读取数据的速度不够快的客户端 client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # 默认情况下，“hz”的被设定为10。提高该值将在Redis空闲时使用更多的CPU时，但同时当有多个key # 同时到期会使Redis的反应更灵敏，以及超时可以更精确地处理 hz 10 # 当一个子进程重写AOF文件时，如果启用下面的选项，则文件每生成32M数据会被同步 aof-rewrite-incremental-fsync yes 1.2 配置主从关系 1、启动实例\n三个Redis实例配置相同，分别启动三个Redis实例。建议将redis-server、redis-cli、redis-sentinel的二进制复制到/usr/local/bin的目录下。\ncd 8000 redis-server redis.conf 2、配置主从关系\n例如，将8000端口实例设为主，8001和8002端口的实例设为从。\n则分别登录8001和8002的实例，执行slaveof \u003cMASTER_IP\u003e \u003cMASTER_PORT\u003e命令。\n例如：\n[root@kube-node-1 8000]# redis-cli -c -p 8001 -a 0234kz9*l 127.0.0.1:8001\u003e slaveof 127.0.0.1 8000 OK 3、检查集群状态\n登录master和slave实例，执行info replication查看集群状态。\nMaster\n[root@kube-node-1 8000]# redis-cli -c -p 8000 -a 0234kz9*l 127.0.0.1:8000\u003e info replication # Replication role:master connected_slaves:2 slave0:ip=127.0.0.1,port=8001,state=online,offset=2853,lag=0 slave1:ip=127.0.0.1,port=8002,state=online,offset=2853,lag=0 master_replid:4f8331d5f180a4669241ab0dd97e43508abd6d8f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:2853 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:2853 Slave\n[root@kube-node-1 8000]# redis-cli -c -p 8001 -a 0234kz9*l 127.0.0.1:8001\u003e info replication # Replication role:slave master_host:127.0.0.1 master_port:8000 master_link_status:up master_last_io_seconds_ago:3 master_sync_in_progress:0 slave_repl_offset:2909 slave_priority:100 slave_read_only:1 connected_slaves:0 master_replid:4f8331d5f180a4669241ab0dd97e43508abd6d8f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:2909 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:2909 也可以往master写数据，从slave读取数据来验证。\n2. 部署sentinel集群 2.1 部署与配置 在之前创建的sentinel目录中场景sentinel端口号命名的目录28000，28001，28002。\ncd sentinel mkdir 28000 28001 28002 在对应端口号目录中创建redis.conf的文件，配置文件中的端口号port参数改为对应目录的端口号。配置如下：\nport 28000 sentinel monitor mymaster 127.0.0.1 8000 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 2.2 启动sentinel实例 #\u0026 表示后台运行的方式 redis-sentinel sentinel.conf \u0026 2.3 查看状态 使用sentinel masters命令查看监控的master节点。\n[root@kube-node-1 28000]# redis-cli -c -p 28000 -a 0234kz9*l 127.0.0.1:28000\u003e 127.0.0.1:28000\u003e ping PONG 127.0.0.1:28000\u003e 127.0.0.1:28000\u003e sentinel masters 1) 1) \"name\" 1) \"mymaster\" 2) \"ip\" 3) \"127.0.0.1\" 4) \"port\" 5) \"8000\" 6) \"runid\" 7) \"\" 8) \"flags\" 1) \"s_down,master,disconnected\" 2) \"link-pending-commands\" 3) \"0\" 4) \"link-refcount\" 5) \"1\" 6) \"last-ping-sent\" 7) \"187539\" 8) \"last-ok-ping-reply\" 9) \"187539\" 10) \"last-ping-reply\" 11) \"3943\" 12) \"s-down-time\" 13) \"127491\" 14) \"down-after-milliseconds\" 15) \"60000\" 16) \"info-refresh\" 17) \"1517346914642\" 18) \"role-reported\" 19) \"master\" 20) \"role-reported-time\" 21) \"187539\" 22) \"config-epoch\" 23) \"0\" 24) \"num-slaves\" 25) \"0\" 26) \"num-other-sentinels\" 27) \"0\" 28) \"quorum\" 29) \"2\" 30) \"failover-timeout\" 31) \"180000\" 32) \"parallel-syncs\" 33) \"1\" 参考文章：\nhttps://redis.io/topics/sentinel\n","categories":"","description":"","excerpt":"1. 部署Redis集群 redis的安装及配置参考[redis部署]\n本文以创建一主二从的集群为例。\n1.1 部署与配置 先创 …","ref":"/linux-notes/redis/redis-sentinel/","tags":["Redis"],"title":"Redis哨兵模式部署"},{"body":"1. shell运算符 Bash 支持很多运算符，包括算数运算符、关系运算符、布尔运算符、字符串运算符和文件测试运算符。 awk 和 expr，expr 最常用\n例如，两个数相加：\n#!/bin/bash val=`expr 2 + 2` echo \"Total value : $val\" 运行脚本输出：\nTotal value : 4 两点注意：\n表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。 完整的表达式要被``包含，注意这个字符不是常用的单引号，在 Esc 键下边。 1.1. 算术运算符 算术运算符列表\n运算符 说明 举例 + 加法 `expr $a + $b` 结果为 30。 - 减法 `expr $a - $b` 结果为 10。 * 乘法 `expr $a * $b` 结果为 200。 / 除法 `expr $b / $a` 结果为 2。 % 取余 `expr $b % $a` 结果为 0。 = 赋值 a=$b 将把变量 b 的值赋给 a。 == 相等。用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false。 != 不相等。用于比较两个数字，不相同则返回 true。 [ $a != $b ] 返回 true。 注意：\n乘号(*)前边必须加反斜杠()才能实现乘法运算\n条件表达式要放在方括号之间，并且要有空格，例如 [$a==$b] 是错误的，必须写成 [$a == $b ]。\n1.2. 关系运算符 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n关系运算符列表\n运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ] 返回 true。 -ne 检测两个数是否相等，不相等返回 true。 [ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 [ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 [ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大等于右边的，如果是，则返回 true。 [ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 [ $a -le $b ] 返回 true。 eq:equal\nne:not equal\ngt:greater than\nlt:less than\nge:greater or equal\nle:less or equal\n1.3. 布尔运算符 布尔运算符列表\n运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 -o:or 或 -a: and 与 1.4. 字符串运算符 字符串运算符列表\n运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 != 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。 -z 检测字符串长度是否为0，为0返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为0，不为0返回 true。 [ -z $a ] 返回 true。 str 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 字符串长度：\n-z:zero ---\u003etrue -n: not zero---\u003etrue 1.5. 文件测试运算符 文件测试运算符用于检测 Unix 文件的各种属性。\n文件测试运算符列表\n操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ] 返回 false。 -p file 检测文件是否是具名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 [ -e $file ] 返回 true。 2. shell注释 以“#”开头的行就是注释，会被解释器忽略。 sh里没有多行注释，只能每一行加一个#号。\n如果在开发过程中，遇到大段的代码需要临时注释起来，过一会儿又取消注释，怎么办呢？每一行加个#符号太费力了，可以把这一段要注释的代码用一对花括号括起来，定义成一个函数，没有地方调用这个函数，这块代码就不会执行，达到了和注释一样的效果。\n参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. shell运算符 Bash 支持很多运算符，包括算数运算符、关系运算符、布尔运算符、字符串运算符和文件测试运算符。 awk …","ref":"/linux-notes/shell/shell-char/","tags":["Shell"],"title":"Shell运算符"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/tcpip/","tags":"","title":"TCP/IP"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/ide/vim/","tags":"","title":"VIM配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/introduction/package/","tags":"","title":"包管理工具"},{"body":"1. 安装cephfs客户端 所有node节点安装cephfs客户端，主要用来和ceph集群挂载使用。\nyum install -y ceph-common 2. 部署RBAC 2.1. ClusterRole kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] 2.2. ClusterRoleBinding kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 2.3. Role apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] 2.4. RoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner 2.5. ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs 3. 部署 cephfs-provisioner apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" resources: limits: cpu: 500m memory: 512Mi requests: cpu: 100m memory: 64Mi env: - name: PROVISIONER_NAME # 与storageclass的provisioner参数相同 value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE # 与rbac的namespace相同 value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 4. 部署storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cephfs-provisioner-sc provisioner: ceph.com/cephfs volumeBindingMode: WaitForFirstConsumer parameters: monitors: 192.168.27.43:6789,192.168.27.44:6789,192.168.27.45:6789 adminId: admin adminSecretName: csi-cephfs-secret adminSecretNamespace: \"kube-csi\" claimRoot: /pvc-volumes 5. 部署statefulset apiVersion: apps/v1 kind: StatefulSet metadata: name: cephfs-provisioner-nginx spec: serviceName: \"nginx\" replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest #nginx的镜像 imagePullPolicy: IfNotPresent volumeMounts: - mountPath: \"/mnt\" #容器里面的挂载目录，该目录挂载到NFS的共享目录上 name: test volumeClaimTemplates: - metadata: name: test spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 2Gi storageClassName: cephfs-provisioner-sc 6. 日志 6.1. cephfs-provisoner 执行日志 I0327 07:18:19.742239 1 controller.go:987] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": started I0327 07:18:19.745239 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-22-0\", UID:\"7f6b60d5-5060-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347214256\", FieldPath:\"\"}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim \"default/test-cephfs-ngx-wait-22-0\" I0327 07:18:23.281277 1 cephfs-provisioner.go:222] successfully created CephFS share \u0026CephFSPersistentVolumeSource{Monitors:[192.168.27.43:6789 192.168.27.44:6789 192.168.27.45:6789],Path:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-7f7cb62f-5060-11e9-85c0-0adb8ef08100,User:kubernetes-dynamic-user-7f7cb69f-5060-11e9-85c0-0adb8ef08100,SecretFile:,SecretRef:\u0026SecretReference{Name:ceph-kubernetes-dynamic-user-7f7cb69f-5060-11e9-85c0-0adb8ef08100-secret,Namespace:default,},ReadOnly:false,} I0327 07:18:23.281371 1 controller.go:1087] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": volume \"pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65\" provisioned I0327 07:18:23.281415 1 controller.go:1101] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": trying to save persistentvvolume \"pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65\" I0327 07:18:23.284621 1 controller.go:1108] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": persistentvolume \"pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65\" saved I0327 07:18:23.284723 1 controller.go:1149] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": succeeded I0327 07:18:23.284810 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-22-0\", UID:\"7f6b60d5-5060-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347214256\", FieldPath:\"\"}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65 6.2. debug 日志 I0327 08:08:11.789608 1 controller.go:987] provision \"default/test-cephfs-ngx-wait-44-0\" class \"cephfs-sc-wait\": started I0327 08:08:11.793258 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-44-0\", UID:\"81846859-5067-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347237916\", FieldPath:\"\"}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim \"default/test-cephfs-ngx-wait-44-0\" E0327 08:08:12.164705 1 cephfs-provisioner.go:158] failed to provision share \"kubernetes-dynamic-pvc-76ecdc5a-5067-11e9-9421-2a1b1be1aeef\" for \"kubernetes-dynamic-user-76ecdcee-5067-11e9-9421-2a1b1be1aeef\", err: exit status 1, output: Traceback (most recent call last): File \"/usr/local/bin/cephfs_provisioner\", line 364, in \u003cmodule\u003e main() File \"/usr/local/bin/cephfs_provisioner\", line 358, in main print cephfs.create_share(share, user, size=size) File \"/usr/local/bin/cephfs_provisioner\", line 228, in create_share volume = self.volume_client.create_volume(volume_path, size=size, namespace_isolated=not self.ceph_namespace_isolation_disabled) File \"/usr/local/bin/cephfs_provisioner\", line 112, in volume_client self._volume_client.connect(None) File \"/lib/python2.7/site-packages/ceph_volume_client.py\", line 458, in connect self.rados.connect() File \"rados.pyx\", line 895, in rados.Rados.connect (/home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos7/DIST/centos7/MACHINE_SIZE/huge/release/13.2.1/rpm/el7/BUILD/ceph-13.2.1/build/src/pybind/rados/pyrex/rados.c:9815) rados.IOError: [errno 5] error connecting to the cluster W0327 08:08:12.164908 1 controller.go:746] Retrying syncing claim \"default/test-cephfs-ngx-wait-44-0\" because failures 2 \u003c threshold 15 E0327 08:08:12.164977 1 controller.go:761] error syncing claim \"default/test-cephfs-ngx-wait-44-0\": failed to provision volume with StorageClass \"cephfs-sc-wait\": exit status 1 I0327 08:08:12.165974 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-44-0\", UID:\"81846859-5067-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347237916\", FieldPath:\"\"}): type: 'Warning' reason: 'ProvisioningFailed' failed to provision volume with StorageClass \"cephfs-sc-wait\": exit status 1 参考\nhttps://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs/deploy ","categories":"","description":"","excerpt":"1. 安装cephfs客户端 所有node节点安装cephfs客户端，主要用来和ceph集群挂载使用。\nyum install -y …","ref":"/kubernetes-notes/storage/csi/provisioner/cephfs-provisioner/","tags":["CSI"],"title":"部署cephfs-provisioner"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n此部分主要介绍调度中使用的各种调度算法，包括调度算法的注册部分。注册部分的代码主要在/pkg/scheduler/algorithmprovider中，具体的预选策略和优选策略的算法实现在/pkg/scheduler/algorithm中。\n1. ApplyFeatureGates 注册调度算法的调用入口在SchedulerCommand的Run函数中。\n此部分代码位于/cmd/kube-scheduler/app/server.go\n// Run runs the Scheduler. func Run(c schedulerserverconfig.CompletedConfig, stopCh \u003c-chan struct{}) error { ... // Apply algorithms based on feature gates. // TODO: make configurable? algorithmprovider.ApplyFeatureGates() ... } ApplyFeatureGates的具体实现在pkg/scheduler/algorithmprovider的包中。\n此部分代码位于/pkg/scheduler/algorithmprovider/plugins.go\n// ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { defaults.ApplyFeatureGates() } ApplyFeatureGates具体实现如下：\n此部分代码位于/pkg/scheduler/algorithmprovider/defaults/defaults.go\n根据feature移除部分调度策略。\n// ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { if utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition) { // Remove \"CheckNodeCondition\", \"CheckNodeMemoryPressure\", \"CheckNodePIDPressurePred\" // and \"CheckNodeDiskPressure\" predicates factory.RemoveFitPredicate(predicates.CheckNodeConditionPred) factory.RemoveFitPredicate(predicates.CheckNodeMemoryPressurePred) factory.RemoveFitPredicate(predicates.CheckNodeDiskPressurePred) factory.RemoveFitPredicate(predicates.CheckNodePIDPressurePred) // Remove key \"CheckNodeCondition\", \"CheckNodeMemoryPressure\" and \"CheckNodeDiskPressure\" // from ALL algorithm provider // The key will be removed from all providers which in algorithmProviderMap[] // if you just want remove specific provider, call func RemovePredicateKeyFromAlgoProvider() factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeConditionPred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeMemoryPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeDiskPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodePIDPressurePred) // Fit is determined based on whether a pod can tolerate all of the node's taints factory.RegisterMandatoryFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints) // Fit is determined based on whether a pod can tolerate unschedulable of node factory.RegisterMandatoryFitPredicate(predicates.CheckNodeUnschedulablePred, predicates.CheckNodeUnschedulablePredicate) // Insert Key \"PodToleratesNodeTaints\" and \"CheckNodeUnschedulable\" To All Algorithm Provider // The key will insert to all providers which in algorithmProviderMap[] // if you just want insert to specific provider, call func InsertPredicateKeyToAlgoProvider() factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.PodToleratesNodeTaintsPred) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.CheckNodeUnschedulablePred) glog.Warningf(\"TaintNodesByCondition is enabled, PodToleratesNodeTaints predicate is mandatory\") } // Prioritizes nodes that satisfy pod's resource limits if utilfeature.DefaultFeatureGate.Enabled(features.ResourceLimitsPriorityFunction) { factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1) } } 2. init 当函数逻辑调用到algorithmprovider包时，就会自动调用init的初始化函数，此部分主要包括对预选算法和优选算法的注册。\n此部分代码位于/pkg/scheduler/algorithmprovider/defaults/defaults.go\nfunc init() { // Register functions that extract metadata used by predicates and priorities computations. factory.RegisterPredicateMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PredicateMetadataProducer { return predicates.NewPredicateMetadataFactory(args.PodLister) }) factory.RegisterPriorityMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PriorityMetadataProducer { return priorities.NewPriorityMetadataFactory(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }) registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) // IMPORTANT NOTES for predicate developers: // We are using cached predicate result for pods belonging to the same equivalence class. // So when implementing a new predicate, you are expected to check whether the result // of your predicate function can be affected by related API object change (ADD/DELETE/UPDATE). // If yes, you are expected to invalidate the cached predicate result for related API object change. // For example: // https://github.com/kubernetes/kubernetes/blob/36a218e/plugin/pkg/scheduler/factory/factory.go#L422 // Registers predicates and priorities that are not enabled by default, but user can pick when creating their // own set of priorities/predicates. // PodFitsPorts has been replaced by PodFitsHostPorts for better user understanding. // For backwards compatibility with 1.0, PodFitsPorts is registered as well. factory.RegisterFitPredicate(\"PodFitsPorts\", predicates.PodFitsHostPorts) // Fit is defined based on the absence of port conflicts. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts) // Fit is determined by resource availability. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) // Fit is determined by the presence of the Host parameter and a string match // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost) // Fit is determined by node selector query. factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) // ServiceSpreadingPriority is a priority config factory that spreads pods by minimizing // the number of pods (belonging to the same service) on the same node. // Register the factory so that it's available, but do not include it as part of the default priorities // Largely replaced by \"SelectorSpreadPriority\", but registered for backward compatibility with 1.0 factory.RegisterPriorityConfigFactory( \"ServiceSpreadingPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, algorithm.EmptyControllerLister{}, algorithm.EmptyReplicaSetLister{}, algorithm.EmptyStatefulSetLister{}) }, Weight: 1, }, ) // EqualPriority is a prioritizer function that gives an equal weight of one to all nodes // Register the priority function so that its available // but do not include it as part of the default priorities factory.RegisterPriorityFunction2(\"EqualPriority\", core.EqualPriorityMap, nil, 1) // Optional, cluster-autoscaler friendly priority function - give used nodes higher priority. factory.RegisterPriorityFunction2(\"MostRequestedPriority\", priorities.MostRequestedPriorityMap, nil, 1) factory.RegisterPriorityFunction2( \"RequestedToCapacityRatioPriority\", priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) } 以下对init中的注册进行拆分介绍。\n2.1. registerAlgorithmProvider 此部分主要注册默认的预选和优选策略。\n// Register functions that extract metadata used by predicates and priorities computations. factory.RegisterPredicateMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PredicateMetadataProducer { return predicates.NewPredicateMetadataFactory(args.PodLister) }) factory.RegisterPriorityMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PriorityMetadataProducer { return priorities.NewPriorityMetadataFactory(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }) registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) registerAlgorithmProvider\n注册AlgorithmProvider，其中包括DefaultProvider和ClusterAutoscalerProvider。\nfunc registerAlgorithmProvider(predSet, priSet sets.String) { // Registers algorithm providers. By default we use 'DefaultProvider', but user can specify one to be used // by specifying flag. factory.RegisterAlgorithmProvider(factory.DefaultProvider, predSet, priSet) // Cluster autoscaler friendly scheduling algorithm. factory.RegisterAlgorithmProvider(ClusterAutoscalerProvider, predSet, copyAndReplace(priSet, \"LeastRequestedPriority\", \"MostRequestedPriority\")) } 2.2. RegisterFitPredicate 在init部分注册预选策略函数。\n预选策略如下：\n调度策略 函数 描述 PodFitsPorts PodFitsHostPorts PodFitsPorts已经被PodFitsHostPorts代替，此处主要是为了兼容性。 PodFitsHostPortsPred PodFitsHostPorts 判断是否与宿主机的端口冲突。 PodFitsResourcesPred PodFitsResources 判断node资源是否充足。 HostNamePred PodFitsHost 判断pod所指定调度的节点是否是当前的节点。 MatchNodeSelectorPred PodMatchNodeSelector 判断pod指定的node selector是否匹配当前的node。 具体代码如下：\n// PodFitsPorts has been replaced by PodFitsHostPorts for better user understanding. // For backwards compatibility with 1.0, PodFitsPorts is registered as well. factory.RegisterFitPredicate(\"PodFitsPorts\", predicates.PodFitsHostPorts) // Fit is defined based on the absence of port conflicts. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts) // Fit is determined by resource availability. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) // Fit is determined by the presence of the Host parameter and a string match // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost) // Fit is determined by node selector query. factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) 2.3. RegisterPriorityFunction2 在init部分注册优选策略函数。\n// EqualPriority is a prioritizer function that gives an equal weight of one to all nodes // Register the priority function so that its available // but do not include it as part of the default priorities factory.RegisterPriorityFunction2(\"EqualPriority\", core.EqualPriorityMap, nil, 1) // Optional, cluster-autoscaler friendly priority function - give used nodes higher priority. factory.RegisterPriorityFunction2(\"MostRequestedPriority\", priorities.MostRequestedPriorityMap, nil, 1) factory.RegisterPriorityFunction2( \"RequestedToCapacityRatioPriority\", priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) 3. defaultPredicates 此部分为默认预选策略的注册函数。\n默认的预选策略如下：\n预选策略 函数 描述 NoVolumeZoneConflictPred NewVolumeZonePredicate 判断pod使用到的volume是否有节点的要求。目前只支持pvc。 MaxEBSVolumeCountPred NewMaxPDVolumeCountPredicate 判断pod使用EBSVolume在该节点上是否已经达到上限了。 MaxGCEPDVolumeCountPred NewMaxPDVolumeCountPredicate 判断pod使用GCEPDVolume在该节点上是否已经达到上限了。 MaxAzureDiskVolumeCountPred NewMaxPDVolumeCountPredicate 判断pod使用AzureDiskVolume在该节点上是否已经达到上限了。 MaxCSIVolumeCountPred NewCSIMaxVolumeLimitPredicate 判断CSIVolume是否达到上限了。 MatchInterPodAffinityPred NewPodAffinityPredicate 匹配pod的亲缘性。 NoDiskConflictPred NoDiskConflict 判断是否有disk volumes的冲突。 GeneralPred GeneralPredicates 通用的预选策略 CheckNodeMemoryPressurePred CheckNodeMemoryPressurePredicate 判断节点内存是否充足。 CheckNodeDiskPressurePred CheckNodeDiskPressurePredicate 判断节点是否有磁盘压力。 CheckNodePIDPressurePred CheckNodePIDPressurePredicate 判断节点上的PID CheckNodeConditionPred CheckNodeConditionPredicate 判断node是否ready。 PodToleratesNodeTaintsPred PodToleratesNodeTaints 判断pod是否可以容忍节点的taints。 CheckVolumeBindingPred NewVolumeBindingPredicate 判断是否有volume拓扑的要求。 具体代码如下：\nfunc defaultPredicates() sets.String { return sets.NewString( // Fit is determined by volume zone requirements. factory.RegisterFitPredicateFactory( predicates.NoVolumeZoneConflictPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeZonePredicate(args.PVInfo, args.PVCInfo, args.StorageClassInfo) }, ), // Fit is determined by whether or not there would be too many AWS EBS volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxEBSVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.EBSVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), // Fit is determined by whether or not there would be too many GCE PD volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxGCEPDVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.GCEPDVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), // Fit is determined by whether or not there would be too many Azure Disk volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxAzureDiskVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.AzureDiskVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), factory.RegisterFitPredicateFactory( predicates.MaxCSIVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewCSIMaxVolumeLimitPredicate(args.PVInfo, args.PVCInfo) }, ), // Fit is determined by inter-pod affinity. factory.RegisterFitPredicateFactory( predicates.MatchInterPodAffinityPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewPodAffinityPredicate(args.NodeInfo, args.PodLister) }, ), // Fit is determined by non-conflicting disk volumes. factory.RegisterFitPredicate(predicates.NoDiskConflictPred, predicates.NoDiskConflict), // GeneralPredicates are the predicates that are enforced by all Kubernetes components // (e.g. kubelet and all schedulers) factory.RegisterFitPredicate(predicates.GeneralPred, predicates.GeneralPredicates), // Fit is determined by node memory pressure condition. factory.RegisterFitPredicate(predicates.CheckNodeMemoryPressurePred, predicates.CheckNodeMemoryPressurePredicate), // Fit is determined by node disk pressure condition. factory.RegisterFitPredicate(predicates.CheckNodeDiskPressurePred, predicates.CheckNodeDiskPressurePredicate), // Fit is determined by node pid pressure condition. factory.RegisterFitPredicate(predicates.CheckNodePIDPressurePred, predicates.CheckNodePIDPressurePredicate), // Fit is determined by node conditions: not ready, network unavailable or out of disk. factory.RegisterMandatoryFitPredicate(predicates.CheckNodeConditionPred, predicates.CheckNodeConditionPredicate), // Fit is determined based on whether a pod can tolerate all of the node's taints factory.RegisterFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints), // Fit is determined by volume topology requirements. factory.RegisterFitPredicateFactory( predicates.CheckVolumeBindingPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeBindingPredicate(args.VolumeBinder) }, ), ) } 4. defaultPriorities 此部分主要为默认优选策略的注册函数。\n默认优选策略如下：\n优选策略 函数 描述 SelectorSpreadPriority NewSelectorSpreadPriority 属于相同service和rs下的pod尽量分布在不同的node上。 InterPodAffinityPriority NewInterPodAffinityPriority 根据pod的亲缘性，将相同拓扑域中的pod放在同一个节点 LeastRequestedPriority LeastRequestedPriorityMap 按最少请求的利用率对节点进行优先级排序。 BalancedResourceAllocation BalancedResourceAllocationMap 实现资源的平衡使用。 NodePreferAvoidPodsPriority CalculateNodePreferAvoidPodsPriorityMap 将此权重设置为足以覆盖所有其他优先级函数。 NodeAffinityPriority CalculateNodeAffinityPriorityMap pod指定label节点调度，来匹配node亲缘性。 TaintTolerationPriority ComputeTaintTolerationPriorityMap pod有设置tolerate属性来容忍node的taint。 ImageLocalityPriority ImageLocalityPriorityMap 根据节点上是否有该pod使用到的镜像打分。 具体代码实现如下：\nfunc defaultPriorities() sets.String { return sets.NewString( // spreads pods by minimizing the number of pods (belonging to the same service or replication controller) on the same node. factory.RegisterPriorityConfigFactory( \"SelectorSpreadPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }, Weight: 1, }, ), // pods should be placed in the same topological domain (e.g. same node, same rack, same zone, same power domain, etc.) // as some other pods, or, conversely, should not be placed in the same topological domain as some other pods. factory.RegisterPriorityConfigFactory( \"InterPodAffinityPriority\", factory.PriorityConfigFactory{ Function: func(args factory.PluginFactoryArgs) algorithm.PriorityFunction { return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) }, Weight: 1, }, ), // Prioritize nodes by least requested utilization. factory.RegisterPriorityFunction2(\"LeastRequestedPriority\", priorities.LeastRequestedPriorityMap, nil, 1), // Prioritizes nodes to help achieve balanced resource usage factory.RegisterPriorityFunction2(\"BalancedResourceAllocation\", priorities.BalancedResourceAllocationMap, nil, 1), // Set this weight large enough to override all other priority functions. // TODO: Figure out a better way to do this, maybe at same time as fixing #24720. factory.RegisterPriorityFunction2(\"NodePreferAvoidPodsPriority\", priorities.CalculateNodePreferAvoidPodsPriorityMap, nil, 10000), // Prioritizes nodes that have labels matching NodeAffinity factory.RegisterPriorityFunction2(\"NodeAffinityPriority\", priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1), // Prioritizes nodes that marked with taint which pod can tolerate. factory.RegisterPriorityFunction2(\"TaintTolerationPriority\", priorities.ComputeTaintTolerationPriorityMap, priorities.ComputeTaintTolerationPriorityReduce, 1), // ImageLocalityPriority prioritizes nodes that have images requested by the pod present. factory.RegisterPriorityFunction2(\"ImageLocalityPriority\", priorities.ImageLocalityPriorityMap, nil, 1), ) } 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithmprovider/defaults/defaults.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n此部分主要介绍调度中使用的各种调度算法，包括调度算法的注册部分。注册部分的 …","ref":"/k8s-source-code-analysis/kube-scheduler/registeralgorithmprovider/","tags":["源码分析"],"title":"kube-scheduler源码分析（二）之 调度算法"},{"body":"本文主要描述重装操作系统如何格式化跟分区和数据盘以及设置磁盘挂载配置。\n1.将操作系统写入根分区设备 1、解绑根分区磁盘目录挂载\n# 例如跟分区下的挂载目录如下： cat /proc/mounts | grep ^/dev/sda /dev/sda4 / ext4 rw,relatime,errors=remount-ro 0 0 /dev/sda3 /boot ext4 rw,relatime 0 0 /dev/sda2 /boot/efi vfat rw,relatime,fmask=0077,dmask=0077,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro 0 0 # 例如根分区的块设备是/dev/sda，则解绑该块设备下所有挂载目录 cat /proc/mounts | grep ^/dev/sda | awk '{print $2}' | xargs -n1 -i umount {} 2、删除块设备分区\nsfdisk --delete /dev/sda 3、将操作系统镜像写入根分区设备\nqemu-img dd -f qcow2 -O raw bs=16M if=osi.qcow2 of=/dev/sda 命令参数说明：\n-f qcow2\n指定输入文件的格式为 qcow2（QEMU Copy-On-Write v2）。 osi.qcow2 是一个虚拟磁盘文件，包含系统或数据。 -O raw\n指定输出格式为 raw，即不包含额外元数据的裸数据格式。 裸数据格式适用于直接写入物理磁盘设备。 bs=16M\n设置块大小为 16 MB，在数据复制过程中以 16 MB 为单位进行读写。 较大的块大小通常能提高写入效率，特别是对大容量文件。 if=osi.qcow2\n输入文件路径，osi.qcow2 是源虚拟磁盘镜像文件。 of=/dev/sda\n输出文件路径，/dev/sda 是目标物理磁盘设备。 数据将直接写入 /dev/sda，覆盖其内容。 4、检查并修复根分区\n如果分区表无问题，parted 会直接显示分区信息。 如果检测到分区表错误，parted 会自动应用修复并输出结果。 echo Fix | parted ---pretend-input-tty /dev/sda print 5、 通知内核重新读取分区表\n# 通知内核重新加载指定设备的分区表，无需重启 partprobe /dev/sda 2. 根分区设备重新分区 跟设备的分区主要包括三个分区\nswap分区：是否需要做swap分区\n根分区：在swap分区做完后再做根分区\n跟设备的data分区：在根分区做完后再做data分区。\n2.1. 配置swap分区 swap分区一般从根分区设备大小中切分出来128G作为swap的分区，剩余的做根分区和data分区。\n# 假设跟设备为sda root_device=\"sda\" swap_device_num=1 # 假设swap分区的大小是128G，换算为131072MiB swap_size=131072 # 移除根分区 echo Ignore | parted ---pretend-input-tty /dev/${root_device} rm ${swap_device_num} # 获取指定磁盘设备上最后一段空闲空间的起始位置（单位为 MiB）以便新建分区。 free_end=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') # 创建指定大小的swap分区 swap_start=${free_end} swap_end=$((swap_start + ${swap_size})) echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${swap_start}MiB ${swap_end}MiB # 格式化分区为 Swap 类型，例如：mkswap /dev/sda1 mkswap /dev/${root_device}${swap_device_num} 2.2. 格式化根分区 2.2.1. 未指定根分区大小 如果没有指定根分区大小，一般不需要再做一个data分区，而是把根分区扩展为剩余的所有空间。\n1、如果根分区目录没有指定分区大小，且没有做swap分区。则重新调整大小，扩展到设备的所有剩余空间。\nroot_device=\"sda\" root_device_num=1 echo Yes | parted ---pretend-input-tty /dev/${root_device} -- resizepart ${root_device_num} 100% 2、如果根分区没有指定分区大小，但是有做swap分区。\nroot_device=\"sda\" # 获取空闲空间的起始位置 root_start=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') # 将剩余空间做一个root分区 echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${root_start}MiB 100% 2.2.2. 指定根分区大小 如果指定了根分区大小，一般需要再创建一个data分区，将data分区扩展为剩余的所有的空间。\n1、如果根分区目录有指定大小，且没有做swap分区。则按指定大小分区，例如将跟分区大小设置300G\nroot_device=\"sda\" root_device_num=1 root_size=307200 # 300G换算成单位MiB # 获取根分区的起始位置和终止位置 root_start=$(parted /dev/${root_device} unit MiB print | awk '/./{end=$2} END{print end}' | sed 's/MiB//') root_end=$((1+ root_start + ${root_size})) # 调整跟分区大小 echo Yes | parted ---pretend-input-tty /dev/${root_device} -- resizepart ${root_device_num} ${root_end} 2、如果根分区有指定大小，但是有做swap分区。\nroot_device=\"sda\" root_device_num=1 root_size=307200 # 300G换算成单位MiB # 获取根分区的起始位置和终止位置 root_start=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') root_end=$((1+ root_start + ${root_size})) # 创建指定大小的根分区 echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${root_start}MiB ${root_end}MiB 2.3. 创建跟设备的data分区 如果有指定需要创建跟设备的data分区，则在创建完swap分区和根分区后，继续创建data分区。\n1、创建跟设备的data分区\nroot_device=\"sda\" # 获取空闲空间的起始位置 data_start=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') # 将剩余空间做一个data分区 echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${data_start}MiB 100% 2、格式化根data分区的文件系统\n# data分区的序号一般是在root分区的序号+1 device=\"/dev/sda2\" # 格式化为xfs文件系统 mkfs.xfs -f -n ftype=1 ${device} # 格式化为ext4文件系统 mkfs.ext4 -F ${device} 2. 格式化数据盘 1、找出数据盘所对应的块设备，例如：/dev/sdb\nlsblk -J -d { \"blockdevices\": [ {\"name\":\"sda\", \"maj:min\":\"8:0\", \"rm\":false, \"size\":\"1.1T\", \"ro\":false, \"type\":\"disk\", \"mountpoint\":null}, {\"name\":\"sdb\", \"maj:min\":\"8:16\", \"rm\":false, \"size\":\"6.1T\", \"ro\":false, \"type\":\"disk\", \"mountpoint\":\"/data\"} ] } 2、删除块设备分区并重建分区\n# 删除块设备所有的分区 sfdisk --delete /dev/sdb # 将分区重建为GPT格式 echo label:gpt | sfdisk /dev/sdb 3、 通知内核重新读取分区表\n# 通知内核重新加载指定设备的分区表，无需重启 partprobe /dev/sdb 4、格式化数据盘的文件系统\n# 格式化为xfs文件系统 mkfs.xfs -f -n ftype=1 /dev/sdb # 格式化为ext4文件系统 mkfs.ext4 -F /dev/sdb 3. 设置fstab磁盘挂载 设置swap分区磁盘挂载\nswap_uuid=$(lsblk -f |grep swap|awk '{print $3}') echo \"UUID=${swap_uuid} swap swap defaults 0 0\" \u003e\u003e /etc/fstab 设置根分区磁盘挂载\ntodo\n","categories":"","description":"","excerpt":"本文主要描述重装操作系统如何格式化跟分区和数据盘以及设置磁盘挂载配置。\n1.将操作系统写入根分区设备 1、解绑根分区磁盘目录挂载\n# 例如跟 …","ref":"/linux-notes/baremetal/format-disk/","tags":["裸金属"],"title":"格式化磁盘分区"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/","tags":"","title":"基本概念"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/node/","tags":"","title":"节点迁移"},{"body":"常见镜像拉取问题排查\n1. Pod状态为ErrImagePull或ImagePullBackOff docker-hub-75d4dfb984-5hggg 0/1 ImagePullBackOff 0 14m 192.168.1.30 \u003cnode ip\u003e docker-hub-75d4dfb984-9r57b 0/1 ErrImagePull 0 53s 192.168.0.42 \u003cnode ip\u003e ErrImagePull：表示pod已经调度到node节点，kubelet调用docker去拉取镜像失败。 ImagePullBackOff：表示kubelet拉取镜像失败后，不断重试去拉取仍然失败。 2. 查看pod的事件 通过kubectl describe pod 命令查看pod事件，该事件的报错信息在kubelet或docker的日志中也会查看到。\n2.1. http: server gave HTTP response to HTTPS client 如果遇到以下报错，尝试将该镜像仓库添加到docker可信任的镜像仓库配置中。\nError getting v2 registry: Get https://docker.com:8080/v2/: http: server gave HTTP response to HTTPS client\" 具体操作是修改/etc/docker/daemon.json的insecure-registries参数\n#cat /etc/docker/daemon.json { ... \"insecure-registries\": [ ... \"docker.com:8080\" ], ... } 2.2. no basic auth credentials 如果遇到no basic auth credentials报错，说明kubelet调用docker接口去拉取镜像时，镜像仓库的认证信息失败。\nNormal BackOff 18s kubelet, 192.168.1.1 Back-off pulling image \"docker.com:8080/public/2048:latest\" Warning Failed 18s kubelet, 192.168.1.1 Error: ImagePullBackOff Normal Pulling 5s (x2 over 18s) kubelet, 192.168.1.1 Pulling image \"docker.com:8080/public/2048:latest\" Warning Failed 5s (x2 over 18s) kubelet, 192.168.1.1 Failed to pull image \"docker.com:8080/public/2048:latest\": rpc error: code = Unknown desc = Error response from daemon: Get http://docker.com:8080/v2/public/2048/manifests/latest: no basic auth credentials Warning Failed 5s (x2 over 18s) kubelet, 192.168.1.1 Error: ErrImagePull 具体操作，在拉取镜像失败的节点上登录该镜像仓库，认证信息会更新到 $HOME/.docker/config.json文件中。将该文件拷贝到/var/lib/kubelet/config.json中。\n","categories":"","description":"","excerpt":"常见镜像拉取问题排查\n1. Pod状态为ErrImagePull或ImagePullBackOff …","ref":"/kubernetes-notes/trouble-shooting/pod-image-error/","tags":["问题排查"],"title":"镜像拉取失败问题"},{"body":"1. 三色标记法 go的垃圾回收机制是通过三色标记法来实现的，其中\n黑色：没有指向（引用）白色集合中的任何对象 灰色：可能指向（引用）白色集合中的某些对象 白色：剩下的需要被回收的候选对象，当灰色集合为空时，表示白色集合中的对象都没有被引用，那么这些对象就可以被回收。 一个垃圾回收循环的步骤：\n将所有的对象都放入白色集合中 扫描所有roots对象，放入灰色集合中，roots对象表示在应用中可以被直接访问，一般是全局变量和其他在栈中的对象。 将灰色集合中的某个对象放入黑色集合，然后扫描这个对象有引到到的白色集合中的对象，将那些白色集合中引用到的所有对象放入灰色集合，以此类推，将灰色集合中的对象不断放入黑色集合中，然后白色集合中的对象不断放入灰色集合中。 当灰色集合中的对象为0，即都被放入到黑色集合中了，表示没有任何对象会引用到白色集合中的对象了，因为黑色集合存放不会引用白色集合对象的元素，而灰色集合为0，也不存在引用白色集合对象的元素。所以白色集合中的对象即是没有被引用的对象，可以回收的对象。 2. 三色标记和写屏障 这是让标记和⽤户代码并发的基本保障，基本原理：\n起初所有对象都是⽩⾊。 扫描找出所有可达对象，标记为灰⾊，放⼊待处理队列。 从队列提取灰⾊对象，将其引⽤对象标记为灰⾊放⼊队列，⾃⾝标记为⿊⾊。 写屏障监视对象内存修改，重新标⾊或放回队列。 当完成全部扫描和标记⼯作后，剩余不是⽩⾊就是⿊⾊，分别代表要待回收和活跃对象，清理操作只需将⽩⾊对象内存收回即可。\n3. 标记清除算法 (Mark-Sweep) 标记-清除算法分为两个阶段：标记阶段和清除阶段。\n标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。\n优点是简单，容易实现。 缺点是容易产生内存碎片，碎片太多可能会导致后续过程中需要为大对象分配空间时无法找到足够的空间而提前触发新的一次垃圾收集动作。（因为没有对不同生命周期的对象采用不同算法，所以碎片多，内存容易满，gc频率高，耗时）\n待完善\n","categories":"","description":"","excerpt":"1. 三色标记法 go的垃圾回收机制是通过三色标记法来实现的，其中\n黑色：没有指向（引用）白色集合中的任何对象 灰色：可能指向（引用）白色集 …","ref":"/golang-notes/principle/garbage-collection/","tags":["Golang"],"title":"垃圾回收"},{"body":" 以下内容基于Linux系统，特别为Ubuntu系统\n1. 安装kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ 下载指定版本，例如下载v1.9.0版本\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ 2. 安装minikube minikube的源码地址：https://github.com/kubernetes/minikube\n2.1 安装minikube 以下命令为安装latest版本的minikube。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026\u0026 chmod +x minikube \u0026\u0026 sudo mv minikube /usr/local/bin/ 安装指定版本可到https://github.com/kubernetes/minikube/releases下载对应版本。\n例如：以下为安装v0.28.2版本\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-linux-amd64 \u0026\u0026 chmod +x minikube \u0026\u0026 sudo mv minikube /usr/local/bin/ 2.2 minikube命令帮助 Minikube is a CLI tool that provisions and manages single-node Kubernetes clusters optimized for development workflows. Usage: minikube [command] Available Commands: addons Modify minikube's kubernetes addons cache Add or delete an image from the local cache. completion Outputs minikube shell completion for the given shell (bash or zsh) config Modify minikube config dashboard Opens/displays the kubernetes dashboard URL for your local cluster delete Deletes a local kubernetes cluster docker-env Sets up docker env variables; similar to '$(docker-machine env)' get-k8s-versions Gets the list of Kubernetes versions available for minikube when using the localkube bootstrapper ip Retrieves the IP address of the running cluster logs Gets the logs of the running localkube instance, used for debugging minikube, not user code mount Mounts the specified directory into minikube profile Profile sets the current minikube profile service Gets the kubernetes URL(s) for the specified service in your local cluster ssh Log into or run a command on a machine with SSH; similar to 'docker-machine ssh' ssh-key Retrieve the ssh identity key path of the specified cluster start Starts a local kubernetes cluster status Gets the status of a local kubernetes cluster stop Stops a running local kubernetes cluster update-check Print current and latest version number update-context Verify the IP address of the running cluster in kubeconfig. version Print the version of minikube Flags: --alsologtostderr log to standard error as well as files -b, --bootstrapper string The name of the cluster bootstrapper that will set up the kubernetes cluster. (default \"localkube\") --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --loglevel int Log level (0 = DEBUG, 5 = FATAL) (default 1) --logtostderr log to standard error instead of files -p, --profile string The name of the minikube VM being used. This can be modified to allow for multiple minikube instances to be run independently (default \"minikube\") --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Use \"minikube [command] --help\" for more information about a command. 3. 使用minikube安装k8s集群 3.1. minikube start 可以以Docker的方式运行k8s的组件，但需要先安装Docker(可参考Docker安装)，启动参数使用--vm-driver=none。\nminikube start --vm-driver=none 例如：\nroot@ubuntu:~# minikube start --vm-driver=none Starting local Kubernetes v1.10.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Downloading kubeadm v1.10.0 Downloading kubelet v1.10.0 ^[[DFinished Downloading kubelet v1.10.0 Finished Downloading kubeadm v1.10.0 Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. =================== WARNING: IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacks When using the none driver, the kubectl config and credentials generated will be root owned and will appear in the root home directory. You will need to move the files to the appropriate location and then set the correct permissions. An example of this is below: sudo mv /root/.kube $HOME/.kube # this will write over any previous configuration sudo chown -R $USER $HOME/.kube sudo chgrp -R $USER $HOME/.kube sudo mv /root/.minikube $HOME/.minikube # this will write over any previous configuration sudo chown -R $USER $HOME/.minikube sudo chgrp -R $USER $HOME/.minikube This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true Loading cached images from config file. 安装指定版本的kubernetes集群\n# 查阅版本 minikube get-k8s-versions # 选择版本启动 minikube start --kubernetes-version v1.7.3 --vm-driver=none 3.2. minikube status $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 172.16.94.139 3.3. minikube stop minikube stop 命令可以用来停止集群。 该命令会关闭 minikube 虚拟机，但将保留所有集群状态和数据。 再次启动集群将恢复到之前的状态。\n3.4. minikube delete minikube delete 命令可以用来删除集群。 该命令将关闭并删除 minikube 虚拟机。没有数据或状态会被保存下来。\n4. 查看部署结果 4.1. 部署组件 root@ubuntu:~# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE pod/etcd-minikube 1/1 Running 0 38m pod/kube-addon-manager-minikube 1/1 Running 0 38m pod/kube-apiserver-minikube 1/1 Running 1 39m pod/kube-controller-manager-minikube 1/1 Running 0 38m pod/kube-dns-86f4d74b45-bdfnx 3/3 Running 0 38m pod/kube-proxy-dqdvg 1/1 Running 0 38m pod/kube-scheduler-minikube 1/1 Running 0 38m pod/kubernetes-dashboard-5498ccf677-c2gnh 1/1 Running 0 38m pod/storage-provisioner 1/1 Running 0 38m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP 38m service/kubernetes-dashboard NodePort 10.104.48.227 \u003cnone\u003e 80:30000/TCP 38m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/kube-proxy 1 1 1 1 1 \u003cnone\u003e 38m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/kube-dns 1 1 1 1 38m deployment.apps/kubernetes-dashboard 1 1 1 1 38m NAME DESIRED CURRENT READY AGE replicaset.apps/kube-dns-86f4d74b45 1 1 1 38m replicaset.apps/kubernetes-dashboard-5498ccf677 1 1 1 38m 4.2. dashboard 通过访问ip:port，例如：http://172.16.94.139:30000/，可以访问k8s的dashboard控制台。\n\u003cimg src=\"http://res.cloudinary.com/dqxtn0ick/image/upload/v1533695750/article/kubernetes/arch/dashboard.png\" width = \"100%\"/\u003e\n5. troubleshooting 5.1. 没有安装VirtualBox [root@minikube ~]# minikube start Starting local Kubernetes v1.10.0 cluster... Starting VM... Downloading Minikube ISO 160.27 MB / 160.27 MB [============================================] 100.00% 0s E0727 15:47:08.655647 9407 start.go:174] Error starting host: Error creating host: Error executing step: Running precreate checks. : VBoxManage not found. Make sure VirtualBox is installed and VBoxManage is in the path. Retrying. E0727 15:47:08.656994 9407 start.go:180] Error starting host: Error creating host: Error executing step: Running precreate checks. : VBoxManage not found. Make sure VirtualBox is installed and VBoxManage is in the path ================================================================================ An error has occurred. Would you like to opt in to sending anonymized crash information to minikube to help prevent future errors? To opt out of these messages, run the command: minikube config set WantReportErrorPrompt false ================================================================================ Please enter your response [Y/n]: 解决方法，先安装VirtualBox。\n5.2. 没有安装Docker [root@minikube ~]# minikube start --vm-driver=none Starting local Kubernetes v1.10.0 cluster... Starting VM... E0727 15:56:54.936706 9441 start.go:174] Error starting host: Error creating host: Error executing step: Running precreate checks. : docker cannot be found on the path for this machine. A docker installation is a requirement for using the none driver: exec: \"docker\": executable file not found in $PATH. Retrying. E0727 15:56:54.938930 9441 start.go:180] Error starting host: Error creating host: Error executing step: Running precreate checks. : docker cannot be found on the path for this machine. A docker installation is a requirement for using the none driver: exec: \"docker\": executable file not found in $PATH 解决方法，先安装Docker。\n文章参考：\nhttps://github.com/kubernetes/minikube\nhttps://kubernetes.io/docs/setup/minikube/\nhttps://kubernetes.io/docs/tasks/tools/install-minikube/\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl/\n","categories":"","description":"","excerpt":" 以下内容基于Linux系统，特别为Ubuntu系统\n1. 安装kubectl curl -LO …","ref":"/kubernetes-notes/setup/installer/install-k8s-by-minikube/","tags":["Kubernetes"],"title":"使用minikube安装kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/basis/","tags":"","title":"顺序编程"},{"body":" TODO\n参考：\n更新：移除 Dockershim 的常见问题 | Kubernetes\n别慌: Kubernetes 和 Docker | Kubernetes\n关于 dockershim 移除和使用兼容 CRI 运行时的文章 | Kubernetes\nKEP-2221: Removing dockershim from kubelete-dockershim\nDockershim removal feedback \u0026 issues 将 Docker Engine 节点从 dockershim 迁移到 cri-dockerd | Kubernetes\n查明节点上所使用的容器运行时 | Kubernetes\n","categories":"","description":"","excerpt":" TODO\n参考：\n更新：移除 Dockershim 的常见问题 | Kubernetes\n别慌: Kubernetes 和 Docker …","ref":"/kubernetes-notes/runtime/containerd/remove-dockershim/","tags":"","title":"移除Dockershim"},{"body":"Resource Quality of Service 1. 资源QoS简介 request值表示容器保证可被分配到资源。limit表示容器可允许使用的最大资源。Pod级别的request和limit是其所有容器的request和limit之和。\n2. Requests and Limits Pod可以指定request和limit资源。其中0 \u003c= request \u003c=Node Allocatable \u0026 request \u003c= limit \u003c= Infinity。调度是基于request而不是limit，即如果Pod被成功调度，那么可以保证Pod分配到指定的 request的资源。Pod使用的资源能否超过指定的limit值取决于该资源是否可被压缩。\n2.1. 可压缩的资源 目前只支持CPU pod可以保证获得它们请求的CPU数量，它们可能会也可能不会获得额外的CPU时间(取决于正在运行的其他作业)。因为目前CPU隔离是在容器级别而不是pod级别。 2.2. 不可压缩的资源 目前只支持内存 pod将获得它们请求的内存数量，如果超过了它们的内存请求，它们可能会被杀死(如果其他一些pod需要内存)，但如果pod消耗的内存小于请求的内存，那么它们将不会被杀死(除非在系统任务或守护进程需要更多内存的情况下)。 3. QoS 级别 在机器资源超卖的情况下（limit的总量大于机器的资源容量），即CPU或内存耗尽，将不得不杀死部分不重要的容器。因此对容器分成了3个QoS的级别：Guaranteed, Burstable, Best-Effort，三个级别的优先级依次递减。\n当CPU资源无法满足，pod不会被杀死可能被短暂控制。\n内存是不可压缩的资源，当内存耗尽的情况下，会依次杀死优先级低的容器。Guaranteed的级别最高，不会被杀死，除非容器使用量超过limit限值或者资源耗尽，已经没有更低级别的容器可驱逐。\n3.1. Guaranteed 所有的容器的limit值和request值被配置且两者相等（如果只配置limit没有request，则request取值于limit）。\n例如：\n# 示例1 containers: name: foo resources: limits: cpu: 10m memory: 1Gi name: bar resources: limits: cpu: 100m memory: 100Mi # 示例2 containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar resources: limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi 3.2. Burstable 如果一个或多个容器的limit和request值被配置且两者不相等。\n例如：\n# 示例1 containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar # 示例2 containers: name: foo resources: limits: memory: 1Gi name: bar resources: limits: cpu: 100m # 示例3 containers: name: foo resources: requests: cpu: 10m memory: 1Gi name: bar 3.3. Best-Effort 所有的容器的limit和request值都没有配置。\n例如：\ncontainers: name: foo resources: name: bar resources: 参考文章：\nhttps://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md ","categories":"","description":"","excerpt":"Resource Quality of Service 1. 资源QoS简介 request值表示容器保证可被分配到资源。limit表示容器 …","ref":"/kubernetes-notes/resource/quality-of-service/","tags":["Kubernetes"],"title":"资源服务质量"},{"body":"字符串处理 字符串操作涉及的标准库有strings和strconv两个包\n1. 字符串操作 函数 说明 func Contains(s, substr string) bool 字符串 s 中是否包含 substr，返回 bool 值 func Join(a []string, sep string) string 字符串链接，把 slice a 通过 sep 链接起来 func Index(s, sep string) int 在字符串 s 中查找 sep 所在的位置，返回位置值，找不到返回-1 func Repeat(s string, count int) string 重复 s 字符串 count 次，最后返回重复的字符串 func Replace(s, old, new string, n int) string 在 s 字符串中，把 old 字符串替换为 new 字符串，n 表示替换的次数，小于 0 表示全部替换 func Split(s, sep string) []string 把 s 字符串按照 sep 分割，返回 slice func Trim(s string, cutset string) string 在 s 字符串中去除 cutset 指定的字符串 func Fields(s string) []string 去除 s 字符串的空格符，并且按照空格分割返回 slice 2. 字符串转换 1、Append 系列函数将整数等转换为字符串后，添加到现有的字节数组中\npackage main import ( \"fmt\" \"strconv\" ) func main() { str := make([]byte, 0, 100) str = strconv.AppendInt(str, 4567, 10) str = strconv.AppendBool(str, false) str = strconv.AppendQuote(str, \"abcdefg\") str = strconv.AppendQuoteRune(str, '单') fmt.Println(string(str)) } 2、Format 系列函数把其他类型的转换为字符串\npackage main import ( \"fmt\" \"strconv\" ) func main() { a := strconv.FormatBool(false) b := strconv.FormatFloat(123.23, 'g', 12, 64) c := strconv.FormatInt(1234, 10) d := strconv.FormatUint(12345, 10) e := strconv.Itoa(1023) fmt.Println(a, b, c, d, e) } 3、Parse 系列函数把字符串转换为其他类型\npackage main import ( \"fmt\" \"strconv\" ) func main() { a, err := strconv.ParseBool(\"false\") if err != nil { fmt.Println(err) } b, err := strconv.ParseFloat(\"123.23\", 64) if err != nil { fmt.Println(err) } c, err := strconv.ParseInt(\"1234\", 10, 64) if err != nil { fmt.Println(err) } d, err := strconv.ParseUint(\"12345\", 10, 64) if err != nil { fmt.Println(err) } e, err := strconv.Itoa(\"1023\") if err != nil { fmt.Println(err) } fmt.Println(a, b, c, d, e) } ","categories":"","description":"","excerpt":"字符串处理 字符串操作涉及的标准库有strings和strconv两个包\n1. 字符串操作 函数 说明 func Contains(s, …","ref":"/golang-notes/text/string/","tags":["Golang"],"title":"字符串处理"},{"body":"本文主要说明如何用 Go 实现简单的本地缓存系统 SDK。\n1. 目标特性 ✅ 通用缓存接口（Set/Get/Delete）\n✅ 支持 TTL（过期自动清除）\n✅ 并发安全（支持高并发读写）\n✅ 易于集成到任意平台（封装为模块）\n2. 简易版实现 localcache/ ├── cache.go # 外部可用接口 └── entry.go # 缓存条目定义 ✅ 1. 缓存条目定义 entry.go package localcache import \"time\" type entry struct { value interface{} expireAt time.Time hasExpired bool } func (e *entry) isExpired() bool { if !e.hasExpired { return false } return time.Now().After(e.expireAt) } ✅ 2. 缓存实现 cache.go package localcache import ( \"sync\" \"time\" ) type Cache struct { items map[string]*entry mu sync.RWMutex ttl time.Duration quit chan struct{} } func NewCache(defaultTTL time.Duration) *Cache { c := \u0026Cache{ items: make(map[string]*entry), ttl: defaultTTL, quit: make(chan struct{}), } go c.cleanupLoop() return c } func (c *Cache) Set(key string, value interface{}, ttl ...time.Duration) { exp := c.ttl if len(ttl) \u003e 0 { exp = ttl[0] } c.mu.Lock() defer c.mu.Unlock() c.items[key] = \u0026entry{ value: value, expireAt: time.Now().Add(exp), hasExpired: exp \u003e 0, } } func (c *Cache) Get(key string) (interface{}, bool) { c.mu.RLock() defer c.mu.RUnlock() item, ok := c.items[key] if !ok || item.isExpired() { return nil, false } return item.value, true } func (c *Cache) Delete(key string) { c.mu.Lock() defer c.mu.Unlock() delete(c.items, key) } func (c *Cache) cleanupLoop() { ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for { select { case \u003c-ticker.C: c.cleanExpired() case \u003c-c.quit: return } } } func (c *Cache) cleanExpired() { c.mu.Lock() defer c.mu.Unlock() now := time.Now() for k, v := range c.items { if v.hasExpired \u0026\u0026 now.After(v.expireAt) { delete(c.items, k) } } } func (c *Cache) Close() { close(c.quit) } ✅ 3. 如何使用 cache := localcache.NewCache(10 * time.Minute) defer cache.Close() cache.Set(\"username\", \"huweihuang\") value, found := cache.Get(\"username\") if found { fmt.Println(\"Got:\", value) } 3. 增强版实现 基于最大容量的缓存，并使用 LRU（Least Recently Used，最近最少使用）策略进行淘汰。\n我们在前面提供的 Cache 实现基础上，引入 双向链表 + map 来实现高效的 O(1) 时间复杂度的 LRU 缓存淘汰机制。\n✅ 增强目标 ✅ 支持最大容量限制 maxEntries\n✅ 最近使用的缓存项保留，最久未使用的优先被淘汰\n✅ 并发安全、支持 TTL 清理\n✅ Get / Set 都更新访问时间\n✅ 实现结构 LRUCache ├── map[string]*list.Element // 快速查找 ├── list.List // 保存访问顺序，头是最近使用，尾是最久未使用 └── entry { key, value, expireAt } ✅ LRU 缓存结构代码 package lru import ( \"container/list\" \"sync\" \"time\" ) type entry struct { key string value interface{} expireAt time.Time ttlSet bool } type LRUCache struct { capacity int mu sync.Mutex ll *list.List cache map[string]*list.Element ttl time.Duration } func NewLRUCache(capacity int, defaultTTL time.Duration) *LRUCache { return \u0026LRUCache{ capacity: capacity, ll: list.New(), cache: make(map[string]*list.Element), ttl: defaultTTL, } } ✅ 核心操作逻辑 Set\nfunc (c *LRUCache) Set(key string, value interface{}, ttl ...time.Duration) { c.mu.Lock() defer c.mu.Unlock() if ele, ok := c.cache[key]; ok { // update existing en := ele.Value.(*entry) en.value = value en.expireAt = c.getExpireAt(ttl) en.ttlSet = true c.ll.MoveToFront(ele) return } // new entry en := \u0026entry{ key: key, value: value, expireAt: c.getExpireAt(ttl), ttlSet: true, } ele := c.ll.PushFront(en) c.cache[key] = ele if c.ll.Len() \u003e c.capacity { c.removeOldest() } } func (c *LRUCache) getExpireAt(ttl []time.Duration) time.Time { d := c.ttl if len(ttl) \u003e 0 { d = ttl[0] } if d == 0 { return time.Time{} } return time.Now().Add(d) } Get\nfunc (c *LRUCache) Get(key string) (interface{}, bool) { c.mu.Lock() defer c.mu.Unlock() ele, ok := c.cache[key] if !ok { return nil, false } en := ele.Value.(*entry) if en.ttlSet \u0026\u0026 !en.expireAt.IsZero() \u0026\u0026 time.Now().After(en.expireAt) { c.removeElement(ele) return nil, false } c.ll.MoveToFront(ele) return en.value, true } removeOldest \u0026 removeElement\nfunc (c *LRUCache) removeOldest() { ele := c.ll.Back() if ele != nil { c.removeElement(ele) } } func (c *LRUCache) removeElement(e *list.Element) { en := e.Value.(*entry) delete(c.cache, en.key) c.ll.Remove(e) } ✅ 使用方式示例 cache := lru.NewLRUCache(100, 5*time.Minute) cache.Set(\"a\", \"1\") cache.Set(\"b\", \"2\") val, ok := cache.Get(\"a\") // 会变为“最近使用” 4. 原理 要理解这个 Go 实现的 本地 LRU 缓存系统 的原理，我们可以从两部分来讲：基本思想 和 核心数据结构设计。\n🔧 4.1. 基本原理（逻辑流程） 📌 缓存的本质： 把热点数据保存在内存中，提高读取速度。\n每个缓存项可能有「生命周期」（TTL）。\n当缓存空间满了，需要淘汰旧的数据。\n📌 LRU（Least Recently Used）策略： 含义：优先淘汰最久没有被访问过的缓存项。\n适用场景：访问数据具有局部性原则（近期使用的数据可能会再次被访问）。\n📌 怎么实现 LRU + TTL： 功能 实现方法 O(1) 访问 用 map[string]*list.Element 快速查找 key O(1) 更新访问顺序 用 container/list 双向链表（LRU 队列） O(1) 淘汰最旧项 从链表尾部直接删除 TTL 过期 每个 entry 存 expireAt 时间戳，访问时检查 🧠 4.2. 核心数据结构设计 1. Map + 双向链表 LRU 队列（最近访问的放头部）： [Most Recently Used] ---\u003e [Middle] ---\u003e [Least Recently Used] ↑ ↑ list.Front() list.Back() type LRUCache struct { ll *list.List // 双向链表：记录访问顺序 cache map[string]*list.Element // 快速定位 key -\u003e 链表节点 } 2. 每个缓存 entry 包含信息 type entry struct { key string value interface{} expireAt time.Time // TTL 过期时间 ttlSet bool // 是否设置了 TTL } ⚙️ 4.3. 工作流程举例 1. 添加元素 Set(\"a\", 1)： 如果 a 已存在：更新值，移动到链表头部。\n如果不存在：\n创建新 entry，放到链表头部。\n如果超过最大容量：删除链表尾部元素。\n2. 访问元素 Get(\"a\")： 找到 a 对应的链表节点。\n如果 TTL 未过期，移动它到链表头部（表示最近使用）。\n3. 淘汰策略： 添加新元素时，如果超过容量，就执行： removeElement(list.Back()) // 淘汰最久未用的缓存项 🛡️ 4.4. 并发安全 所有操作都使用 sync.Mutex 加锁。\n确保在多 goroutine 下不会出现竞争状态。\n✅ 总结一句话： 这个缓存系统本质上是一个「哈希表 + 双向链表」的组合，用于快速定位 + 快速维护访问顺序，并结合 TTL 保证缓存不过期且高效清理。\n","categories":"","description":"","excerpt":"本文主要说明如何用 Go 实现简单的本地缓存系统 SDK。\n1. 目标特性 ✅ 通用缓存接口（Set/Get/Delete）\n✅ …","ref":"/golang-notes/web/go-cache/","tags":["Golang"],"title":"Go实现本地缓存系统"},{"body":"本文主要描述如何针对物理机做硬件的raid配置，硬件raid不依赖于操作系统，具有更高的性能，经常在装机系统中使用到。\n1. 查询raid控制器信息 可以通过lspci -mm命令的输出查找raid控制器信息。实际情况可以执行以下命令。\nlspci -mm |egrep \"Broadcom / LSI|Adaptec\" 示例：\n# lspci -mm |egrep \"Broadcom / LSI|Adaptec\" 3b:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID SAS-3 3108 [Invader]\" -r02 -p00 \"Dell\" \"PERC H730P Mini\" lspci -mm 输出中的每一列含义如下：\nPCI 地址：设备的总线号、设备号和功能号，用于唯一标识设备位置。例如：3b:00.0 设备类型：设备的功能类别，例如 RAID 控制器、网络控制器等。例如： \"RAID bus controller\" ,\"Serial Attached SCSI controller\" 制造商：设备的制造商名称。例如：\"Broadcom / LSI\", \"Adaptec\"，最关键的信息，决定raid命令。 设备型号：设备的具体产品型号和名称。例如：\"MegaRAID SAS-3 3108 [Invader]\",\"Smart Storage PQI SAS\" 子系统制造商：子系统的生产厂商（可能为空）。例如：\"Dell\",\"Huawei\",\"Lenovo\" 子系统设备名称：子系统的设备名称（可能为空）。例如：\"PERC H730P Mini\" 以下是常见的几个物理机厂商的设备的raid信息:\n通过执行lspci -mm |egrep \"Broadcom / LSI|Adaptec\"命令可以查看不同厂商设备的raid信息。\n# DELL 18:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID SAS-3 3108 [Invader]\" -r02 -p00 \"Dell\" \"PERC H730P Adapter\" # HUAWEI 1c:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3508\" -r01 -p00 \"Huawei Technologies Co., Ltd.\" \"MegaRAID Tri-Mode SAS3508\" # XFUSION 2a:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID 12GSAS/PCIe Secure SAS39xx\" \"Broadcom / LSI\" \"MegaRAID 12GSAS/PCIe Secure SAS39xx\" # INSPUR 18:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3516\" -r01 \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3516\" # LENOVO 4b:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3516\" -r01 \"Lenovo\" \"ThinkSystem RAID 930-16i 4GB Flash PCIe 12Gb Adapter\" # HPE 5c:00.0 \"Serial Attached SCSI controller\" \"Adaptec\" \"Smart Storage PQI SAS\" -r01 -p00 \"Hewlett-Packard Company\" \"Smart Array P408i-a SR Gen10\" # KAYTUS 31:00.0 \"Serial Attached SCSI controller\" \"Adaptec\" \"Smart Storage PQI 12G SAS/PCIe 3\" -r01 \"Inspur Electronic Information Industry Co., Ltd.\" \"PM8222-SHBA\" 常见的raid制造商：\nBroadcom / LSI：LSI 是知名的存储控制器制造商，现已被 Broadcom 收购。MegaRAID 系列是其广泛使用的 RAID 控制器。 Adaptec: Adaptec 提供一系列 RAID 控制器，主要用于高性能存储解决方案。 2. 常见的 RAID 厂商及其命令行工具 设备类型 raid控制器制造商 设备型号 Raid命令 使用该raid的服务器厂商 RAID bus controller Broadcom / LSI（MegaRAID 系列） MegaRAID Tri-Mode SAS3508 storcli DELL，HUAWEI，XFUSION，INSPUR，LENOVO Serial Attached SCSI controller Adaptec Smart Storage PQI ssacli HPE，KAYTUS Non-Volatile memory controller SATA controller 3. 使用storcli命令创建raid 3.1. 安装storcli 登录https://www.broadcom.com/products/storage/raid-controllers/megaraid-9560-8i下载最新的storcli的解压包，解压后可以执行rpm -ivh /tmp/StorCLIxxx.aarch64.rpm命令安装。\n参数说明\n/cx = Controller ID /vx = Virtual Drive Number. /ex = Enclosure ID /sx = Slot ID 3.2. 创建raid 在创建之前需要查看当前有几个controller，有几块物理磁盘以及物理磁盘的EID:Slt信息，用于选定哪几快物理磁盘来做指定级别的raid。\n# 由第3、4、5块物理磁盘来构建RAID5，分配所有空间的逻辑磁盘命名tmp1。 storcli /c0 add vd type=raid5 size=all names=tmp1 drives=32:2-4 WB 参数说明：\nstorcli 是 Broadcom / LSI 的命令行工具，用于管理 MegaRAID 控制器。\n/c0 表示第一个 RAID 控制器（控制器 0）。如果有多个 RAID 控制器，可以用 /c1、/c2 等来指定其他控制器。\nadd vd 表示添加一个虚拟驱动器（Virtual Drive，简称 VD）。VD 是 RAID 阵列的逻辑表示，用户通过它来访问 RAID 上的数据。\ntype=raid5 表示要创建的 RAID 类型是 RAID 5。RAID 5 使用分布式奇偶校验，提供了较好的读取性能和数据冗余，至少需要 3 个物理磁盘。\nsize=all 表示使用所有可用的磁盘空间来创建这个虚拟驱动器。可以指定特定大小，但此处使用 all 表示使用磁盘的全部空间。\nnames=tmp1 为虚拟驱动器命名为 tmp1。这有助于在系统中区分不同的虚拟驱动器。名称是用户定义的，便于识别。\ndrives=32:2-4 或**drives=32:2,3,4**\n指定用于创建 RAID 的物理磁盘。\n32 是扩展器（enclosure）的编号即EID。如果磁盘是直接连接到控制器，通常编号为 0，如果是通过扩展器连接，扩展器的编号为 32。 2-4 指定物理磁盘编号即Slt，表示使用该扩展器上的 2、3、4 号磁盘。此处的 RAID 5 至少需要 3 个磁盘。 WB: 表示启用 Write Back 缓存。Write Back 意味着在写操作时，数据首先写入缓存，然后返回成功响应，实际数据写入磁盘可能稍后进行。\nWrite Back 缓存可以提高写入性能，但需要有备用电源（如电池或超级电容）来保证缓存数据在断电时不会丢失。 另一种模式是 Write Through (WT)，数据直接写入磁盘，只有数据成功写入磁盘时才返回成功响应，相对更安全但性能较低。 3.3. 查看raid 3.3.1. 查看有哪些controller # storcli show System Overview : -------------------------------------------------------------------- Ctl Model Ports PDs DGs DNOpt VDs VNOpt BBU sPR DS EHS ASOs Hlth -------------------------------------------------------------------- 0 SAS3508 8 2 1 0 1 0 Opt On 1\u00262 Y 3 Opt -------------------------------------------------------------------- Ctl=Controller Index|DGs=Drive groups|VDs=Virtual drives|Fld=Failed PDs=Physical drives|DNOpt=Array NotOptimal|VNOpt=VD NotOptimal|Opt=Optimal Msng=Missing|Dgd=Degraded|NdAtn=Need Attention|Unkwn=Unknown sPR=Scheduled Patrol Read|DS=DimmerSwitch|EHS=Emergency Spare Drive Y=Yes|N=No|ASOs=Advanced Software Options|BBU=Battery backup unit/CV Hlth=Health|Safe=Safe-mode boot|CertProv-Certificate Provision mode Chrg=Charging | MsngCbl=Cable Failure 上述信息可以看出当前有1个controller， 第一个controller有2个物理磁盘。\n如果要用json格式的方式查看可以增加J参数。\n# storcli show J { \"Controllers\": [ { \"Command Status\": { \"CLI Version\": \"007.2203.0000.0000 May 11, 2022\", \"Status Code\": 0, \"Status\": \"Success\", \"Description\": \"None\" }, \"Response Data\": { \"Number of Controllers\": 1, \"System Overview\": [ { \"Ctl\": 0, \"Model\": \"SAS3508\", \"Ports\": 8, \"PDs\": 2, \"DGs\": 1, \"DNOpt\": 0, \"VDs\": 1, \"VNOpt\": 0, \"BBU\": \"Opt\", \"sPR\": \"On\", \"DS\": \"1\u00262\", \"EHS\": \"Y\", \"ASOs\": 3, \"Hlth\": \"Opt\" } ] } } ] } 3.3.2. 查看controller信息 # storcli /c0 show TOPOLOGY : --------------------------------------------------------------------------- DG Arr Row EID:Slot DID Type State BT Size PDC PI SED DS3 FSpace TR --------------------------------------------------------------------------- 0 - - - - RAID1 Optl N 1.089 TB dflt N N dflt N N 0 0 - - - RAID1 Optl N 1.089 TB dflt N N dflt N N 0 0 0 134:0 1 DRIVE Onln N 1.089 TB dflt N N dflt - N 0 0 1 134:1 0 DRIVE Onln N 1.089 TB dflt N N dflt - N --------------------------------------------------------------------------- DG=Disk Group Index|Arr=Array Index|Row=Row Index|EID=Enclosure Device ID DID=Device ID|Type=Drive Type|Onln=Online|Rbld=Rebuild|Optl=Optimal|Dgrd=Degraded Pdgd=Partially degraded|Offln=Offline|BT=Background Task Active PDC=PD Cache|PI=Protection Info|SED=Self Encrypting Drive|Frgn=Foreign DS3=Dimmer Switch 3|dflt=Default|Msng=Missing|FSpace=Free Space Present TR=Transport Ready Virtual Drives = 1 VD LIST : ------------------------------------------------------------- DG/VD TYPE State Access Consist Cache Cac sCC Size Name ------------------------------------------------------------- 0/0 RAID1 Optl RW No RWBD - ON 1.089 TB ------------------------------------------------------------- VD=Virtual Drive| DG=Drive Group|Rec=Recovery Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled Check Consistency Physical Drives = 2 PD LIST : ---------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------- 134:0 1 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - 134:1 0 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - ---------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 134 OK 8 2 0 0 0 0 1 - SGPIO -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID Cachevault_Info : ------------------------------------ Model State Temp Mode MfgDate ------------------------------------ CVPM02 Optimal 26C - 2018/06/05 ------------------------------------ 3.3.2. 查看物理机磁盘 # storcli /c0 /eall /sall show Drive Information : ---------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------- 134:0 1 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - 134:1 0 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - ---------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild 3.3.2. 查看虚拟磁盘 # storcli /c0 /v0 show Virtual Drives : ------------------------------------------------------------- DG/VD TYPE State Access Consist Cache Cac sCC Size Name ------------------------------------------------------------- 0/0 RAID1 Optl RW No RWBD - ON 1.089 TB ------------------------------------------------------------- VD=Virtual Drive| DG=Drive Group|Rec=Recovery Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled Check Consistency 可以看到虚拟磁盘做的是RAID1的配置。\n3.4. 删除raid 删除 RAID 阵列\n/c0 表示控制器 0。\n/v0 表示虚拟驱动器 0。\nstorcli /c0 /v0 delete force 删除所有虚拟驱动器\nstorcli /c0/vall delete preservedCache 删除 外部阵列（foreign configurations）\nstorcli /c0/fall delete /fall：fall 表示 所有的外部阵列（foreign configurations）。外部阵列通常是指在不同的 RAID 控制器或硬盘被移动到当前系统中后，RAID 控制器检测到的现有 RAID 配置，但这些配置尚未被本地控制器认可为有效的 RAID 配置。\n4. 使用ssacli命令创建raid Todo\n3.2.1. 创建raid 3.2.2. 查看raid 3.2.3. 删除raid 5. 总结 本文主要描述如何创建硬件raid，主要包含以下几个步骤\n通过lspci -mm |egrep \"Broadcom / LSI|Adaptec\"命令查询当前机器使用的是哪个raid厂商，对应使用哪种raid命令，例如storcli或ssacli等。 根据选中的命令清除当前的raid配置。 根据选中的命令查询当前有哪些物理磁盘，大小，磁盘编号，控制器有哪些并记录下来。 根据用户配置选择做哪种raid（raid级别），以及选择哪些磁盘用来做raid（通过磁盘编号即EID:Slt） 参考：\nstorcli ssacli ","categories":"","description":"","excerpt":"本文主要描述如何针对物理机做硬件的raid配置，硬件raid不依赖于操作系统，具有更高的性能，经常在装机系统中使用到。\n1. 查询raid控 …","ref":"/linux-notes/disk/make-hardware-raid/","tags":["disk"],"title":"创建硬件Raid"},{"body":"1. 介绍 网站的 SSL/TLS 加密会为您的用户带来更靠前的搜索排名和更出色的安全性。但是最大障碍是证书获取成本高昂和所涉人工流程繁琐。\nLet’s Encrypt 是一家免费、开放、自动化的证书颁发机构 (CA)。本文介绍了如何使用 Let’s Encrypt 客户端生成证书，以及如何自动配置 NGINX 开源版和 NGINX Plus 以使用这些证书。\n2. 安装certbot apt-get update sudo apt-get install certbot apt-get install python3-certbot-nginx 3. 为域名生成证书 执行以下命令会生成一个90天到期的证书文件。\nsudo certbot --nginx -d www.example.com 以上命令会在/etc/letsencrypt/live/生成证书文件。\ncd /etc/letsencrypt/live/www.example.com ls README cert.pem chain.pem fullchain.pem privkey.pem 如果配置成功会生成以下信息：\nCongratulations! You have successfully enabled https://example.com and https://www.example.com ------------------------------------------------------------------------------------- IMPORTANT NOTES: Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/example.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/example.com//privkey.pem Your cert will expire on 2017-12-12. 并且certbot会自动为domain‑name.conf文件自行修改证书路径。\nserver { listen 80 default_server; listen [::]:80 default_server; root /var/www/html; server_name example.com www.example.com; listen 443 ssl; # managed by Certbot # RSA certificate ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot # Redirect non-https traffic to https if ($scheme != \"https\") { return 301 https://$host$request_uri; } # managed by Certbot } 3. 重启Nginx nginx -t \u0026\u0026 nginx -s reload 4. 自动更新证书 Let’s Encrypt 证书将在 90 天后到期， 因此设置定时任务自动更新证书。\ncrontab -e # 将以下信息写入到crontab文件中 0 12 * * * /usr/bin/certbot renew --quiet 参考：\n更新：为 NGINX 配置免费的 Let’s Encrypt SSL/TLS 证书 ","categories":"","description":"","excerpt":"1. 介绍 网站的 SSL/TLS 加密会为您的用户带来更靠前的搜索排名和更出色的安全性。但是最大障碍是证书获取成本高昂和所涉人工流程繁琐。 …","ref":"/linux-notes/nginx/config-ssl-for-nginx/","tags":["Nginx"],"title":"配置Nginx免费证书"},{"body":"1. Installation # mac brew install wrk 2. Usage $ wrk --help Usage: wrk \u003coptions\u003e \u003curl\u003e Options: -c, --connections \u003cN\u003e Connections to keep open # 跟服务器建立并保持的TCP连接数量 -d, --duration \u003cT\u003e Duration of test # 压测时间 -t, --threads \u003cN\u003e Number of threads to use # 使用多少个线程进行压测 -s, --script \u003cS\u003e Load Lua script file # 指定Lua脚本路径 -H, --header \u003cH\u003e Add header to request # 为每一个HTTP请求添加HTTP头 --latency Print latency statistics # 在压测结束后，打印延迟统计信息 --timeout \u003cT\u003e Socket/request timeout # 超时时间 -v, --version Print version details # 打印正在使用的wrk的详细版本信息 Numeric arguments may include a SI unit (1k, 1M, 1G) # 代表数字参数，支持国际单位 (1k, 1M, 1G) Time arguments may include a time unit (2s, 2m, 2h) # 代表时间参数，支持时间单位 (2s, 2m, 2h) 参数设置说明：\n一般设置线程数t，并发数c，压测时间d，--latency四个通用的参数。\n线程数：一般设置为压测机器CPU核数的2-4倍，过大会导致线程切换频繁，效果下降。\n并发数：根据压测结果，动态调整并发数使得压测达到瓶颈。\n示例：\nwrk -t12 -c400 -d30s --latency http://www.baidu.com 3. 压测结果 # wrk -t12 -c400 -d30s --latency http://www.baidu.com Running 30s test @ http://www.baidu.com 12 threads and 400 connections （平均值） （标准差） （最大值）（正负一个标准差所占比例） Thread Stats Avg Stdev Max +/- Stdev （延迟） Latency 568.16ms 250.70ms 2.00s 83.49% (每秒请求数) Req/Sec 28.26 14.99 90.00 65.71% Latency Distribution （延迟分布） 50% 530.91ms 75% 585.73ms 90% 691.64ms 99% 1.78s 9842 requests in 30.10s, 99.03MB read (30.10s内处理了9842个请求，耗费流量99.03MB) Socket errors: connect 158, read 0, write 0, timeout 580 (发生错误数) Requests/sec: 327.00 (QPS ,即平均每秒处理请求数) Transfer/sec: 3.29MB (平均每秒流量) 4. Lua脚本定制压测参数 例如：压测post请求，需要设置指定参数。\n写入以下lua脚本，login.lua\nwrk.method = \"POST\" wrk.body = '{\"username\":\"xxx\",\"password\":\"xxx\"}' wrk.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\" 发起压测请求：\nwrk -t12 -c1000 -d30s --latency http://127.0.0.1:8081/login -s login.lua 参考：\nGitHub - wg/wrk: Modern HTTP benchmarking tool\nhttps://www.cnblogs.com/quanxiaoha/p/10661650.html\n","categories":"","description":"","excerpt":"1. Installation # mac brew install wrk 2. Usage $ wrk --help Usage: …","ref":"/linux-notes/network/wrk-usage/","tags":"","title":"wrk的使用"},{"body":"1. 社区说明 1.1. Community membership Role Responsibilities Requirements Defined by Member Active contributor in the community Sponsored by 2 reviewers and multiple contributions to the project Kubernetes GitHub org member Reviewer Review contributions from other members History of review and authorship in a subproject OWNERS file reviewer entry Approver Contributions acceptance approval Highly experienced active reviewer and contributor to a subproject OWNERS file approver entry Subproject owner Set direction and priorities for a subproject Demonstrated responsibility and excellent technical judgement for the subproject sigs.yaml subproject OWNERS file owners entry 1.2. 社区活动日历 Community Calendar | Kubernetes Contributors\n1.3. 加入k8s slack 点击 https://communityinviter.com/apps/kubernetes/community\n1.4. 特别兴趣小组（SIG） 列表： https://github.com/kubernetes/community/blob/master/sig-list.md\n2. 编译k8s仓库 参考：\nBuilding Kubernetes https://github.com/kubernetes/community/blob/master/contributors/devel/development.md#building-kubernetes 2.1. 编译二进制 2.1.1. 基于docker构建容器编译。 该方式为官方镜像及二进制文件的构建方式。\n构建镜像(大小：5.97GB)为： kube-build:build-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0\ngit clone https://github.com/kubernetes/kubernetes.git cd kubernetes build/run.sh make # 构建全部 #指定模块构建 build/run.sh make kubeadm 输出如下：\n# build/run.sh make +++ [0804 18:39:11] Verifying Prerequisites.... +++ [0804 18:39:16] Building Docker image kube-build:build-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0 +++ [0804 18:40:49] Creating data container kube-build-data-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0 +++ [0804 18:40:50] Syncing sources to container +++ [0804 18:40:58] Output from this container will be rsynced out upon completion. Set KUBE_RUN_COPY_OUTPUT=n to disable. +++ [0804 18:40:58] Running build command... go: downloading go.uber.org/automaxprocs v1.5.2 +++ [0804 18:41:04] Setting GOMAXPROCS: 8 Go version: go version go1.20.6 linux/amd64 +++ [0804 18:41:04] Building go targets for linux/amd64 k8s.io/kubernetes/cmd/kube-proxy (static) k8s.io/kubernetes/cmd/kube-apiserver (static) k8s.io/kubernetes/cmd/kube-controller-manager (static) k8s.io/kubernetes/cmd/kubelet (non-static) k8s.io/kubernetes/cmd/kubeadm (static) k8s.io/kubernetes/cmd/kube-scheduler (static) k8s.io/component-base/logs/kube-log-runner (static) k8s.io/kube-aggregator (static) k8s.io/apiextensions-apiserver (static) k8s.io/kubernetes/cluster/gce/gci/mounter (non-static) k8s.io/kubernetes/cmd/kubectl (static) k8s.io/kubernetes/cmd/kubectl-convert (static) github.com/onsi/ginkgo/v2/ginkgo (non-static) k8s.io/kubernetes/test/e2e/e2e.test (test) k8s.io/kubernetes/test/conformance/image/go-runner (non-static) k8s.io/kubernetes/cmd/kubemark (static) github.com/onsi/ginkgo/v2/ginkgo (non-static) k8s.io/kubernetes/test/e2e_node/e2e_node.test (test) Env for linux/amd64: GOOS=linux GOARCH=amd64 GOROOT=/usr/local/go CGO_ENABLED= CC= Coverage is disabled. Coverage is disabled. +++ [0804 18:48:17] Placing binaries +++ [0804 18:48:25] Syncing out of container 产物文件在_output目录上。\nkubernetes/_output# tree . |-- dockerized | |-- bin | | `-- linux | | `-- amd64 | | |-- apiextensions-apiserver | | |-- e2e_node.test | | |-- e2e.test | | |-- ginkgo | | |-- go-runner | | |-- kubeadm | | |-- kube-aggregator | | |-- kube-apiserver | | |-- kube-controller-manager | | |-- kubectl | | |-- kubectl-convert | | |-- kubelet | | |-- kube-log-runner | | |-- kubemark | | |-- kube-proxy | | |-- kube-scheduler | | |-- mounter | | `-- ncpu | `-- go `-- images `-- kube-build:build-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0 |-- Dockerfile |-- localtime |-- rsyncd.password `-- rsyncd.sh 2.1.2. 基于构建机环境编译 git clone https://github.com/kubernetes/kubernetes.git cd kubernetes # 构建全部二进制 make # 构建指定二进制 make WHAT=cmd/kubeadm 输出如下：\n# make WHAT=cmd/kubeadm go version go1.20.6 linux/amd64 +++ [0804 19:30:55] Setting GOMAXPROCS: 8 +++ [0804 19:30:56] Building go targets for linux/amd64 k8s.io/kubernetes/cmd/kubeadm (static) 2.2. 编译镜像 git clone https://github.com/kubernetes/kubernetes cd kubernetes make quick-release 3. 如何给k8s提交PR 参考：\nhttps://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md\nhttps://github.com/kubernetes/community/blob/master/contributors/guide/first-contribution.md\nHere is the bot commands documentation.\ntesting guide\n参考：\nhttps://github.com/kubernetes/community/\nhttps://github.com/kubernetes/community/tree/master/contributors/guide\nhttps://github.com/kubernetes/community/blob/master/contributors/guide/first-contribution.md\nissues labeled as a good first issue\n","categories":"","description":"","excerpt":"1. 社区说明 1.1. Community membership Role Responsibilities Requirements …","ref":"/kubernetes-notes/develop/develop-k8s/","tags":["Kubernetes"],"title":"k8s社区开发指南"},{"body":"本文介绍如何通过kubectl进入节点的shell环境。\n1. 安装krew node-shell 1.1. 安装krew ( set -x; cd \"$(mktemp -d)\" \u0026\u0026 OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" \u0026\u0026 ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" \u0026\u0026 KREW=\"krew-${OS}_${ARCH}\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" \u0026\u0026 tar zxvf \"${KREW}.tar.gz\" \u0026\u0026 ./\"${KREW}\" install krew ) 在~/.bashrc或~/.zshrc添加以下命令\nexport PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" 1.2. 安装node-shell node-shell的代码参考：kubectl-node-shell/kubectl-node_shell at master · kvaps/kubectl-node-shell · GitHub\nkubectl krew install node-shell 示例：\n# kubectl krew install node-shell Updated the local copy of plugin index. Installing plugin: node-shell Installed plugin: node-shell \\ | Use this plugin: | kubectl node-shell | Documentation: | https://github.com/kvaps/kubectl-node-shell | Caveats: | \\ | | You need to be allowed to start privileged pods in the cluster | / / WARNING: You installed plugin \"node-shell\" from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. 2. 进入节点的shell 2.1. 登录node 创建一个临时的特权容器，登录容器即登录node shell。\nkubectl node-shell \u003cnode-name\u003e 示例：\n# kubectl node-shell node1 spawning \"nsenter-9yqytp\" on \"node1\" If you don't see a command prompt, try pressing enter. groups: cannot find name for group ID 11 To run a command as administrator (user \"root\"), use \"sudo \u003ccommand\u003e\". See \"man sudo_root\" for details. root@node1:/# 2.2. 退出node 退出容器，容器会被自动删除。\n# exit logout pod default/nsenter-9yqytp terminated (Error) pod \"nsenter-9yqytp\" deleted 3. 原理 容器是弱隔离，共享节点的内核，通过cgroup和namespace来实现进程级别的隔离。那么通过在特权容器里执行nsenter的命令，则可以通过登录特权容器来实现登录node的shell环境。\n创建一个特权容器，进入node shell的命令为：\nnsenter --target 1 --mount --uts --ipc --net --pid -- bash -l 进入 node shell 的权限：\nhostPID: true 共享 host 的 pid\nhostNetwork: true 共享 host 的网络\nprivileged: true: PSP 权限策略是 privileged, 即完全无限制。\n3.1. Pod.yaml apiVersion: v1 kind: Pod metadata: labels: run: nsenter-9yqytp name: nsenter-9yqytp namespace: default spec: containers: - command: - nsenter - --target - \"1\" - --mount - --uts - --ipc - --net - --pid - -- - bash - -l image: docker.io/library/alpine imagePullPolicy: Always name: nsenter resources: limits: cpu: 100m memory: 256Mi requests: cpu: 100m memory: 256Mi securityContext: privileged: true stdin: true stdinOnce: true tty: true volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-4ktlf readOnly: true enableServiceLinks: true hostNetwork: true hostPID: true nodeName: node1 preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Never schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoExecute operator: Exists volumes: - name: kube-api-access-4ktlf projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace 创建完容器后，直接登录容器即可登录节点的shell\nkubectl exec -it nsenter-9yqytp bash 参考：\n如何通过 kubectl 进入 node shell - 东风微鸣技术博客\nGitHub - kvaps/kubectl-node-shell: Exec into node via kubectl\nhttps://krew.sigs.k8s.io/docs/user-guide/setup/install/\n","categories":"","description":"","excerpt":"本文介绍如何通过kubectl进入节点的shell环境。\n1. 安装krew node-shell 1.1. 安装krew ( set …","ref":"/kubernetes-notes/operation/kubectl/kubectl-node-shell/","tags":["Kubernetes"],"title":"kubectl进入node shell"},{"body":" 本文主要记录apisix相关组件默认配置，便于查阅。\n1. apisix配置 参考：apisix/config-default.yaml at master · apache/apisix · GitHub\n# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # PLEASE DO NOT UPDATE THIS FILE! # If you want to set the specified configuration value, you can set the new # value in the conf/config.yaml file. # apisix: # node_listen: 9080 # APISIX listening port node_listen: # This style support multiple ports - 9080 # - port: 9081 # enable_http2: true # If not set, the default value is `false`. # - ip: 127.0.0.2 # Specific IP, If not set, the default value is `0.0.0.0`. # port: 9082 # enable_http2: true enable_admin: true enable_admin_cors: true # Admin API support CORS response headers. enable_dev_mode: false # Sets nginx worker_processes to 1 if set to true enable_reuseport: true # Enable nginx SO_REUSEPORT switch if set to true. show_upstream_status_in_response_header: false # when true all upstream status write to `X-APISIX-Upstream-Status` otherwise only 5xx code enable_ipv6: true config_center: etcd # etcd: use etcd to store the config value # yaml: fetch the config value from local yaml file `/your_path/conf/apisix.yaml` #proxy_protocol: # Proxy Protocol configuration #listen_http_port: 9181 # The port with proxy protocol for http, it differs from node_listen and admin_listen. # This port can only receive http request with proxy protocol, but node_listen \u0026 admin_listen # can only receive http request. If you enable proxy protocol, you must use this port to # receive http request with proxy protocol #listen_https_port: 9182 # The port with proxy protocol for https #enable_tcp_pp: true # Enable the proxy protocol for tcp proxy, it works for stream_proxy.tcp option #enable_tcp_pp_to_upstream: true # Enables the proxy protocol to the upstream server enable_server_tokens: true # Whether the APISIX version number should be shown in Server header. # It's enabled by default. # configurations to load third party code and/or override the builtin one. extra_lua_path: \"\" # extend lua_package_path to load third party code extra_lua_cpath: \"\" # extend lua_package_cpath to load third party code #lua_module_hook: \"my_project.my_hook\" # the hook module which will be used to inject third party code into APISIX proxy_cache: # Proxy Caching configuration cache_ttl: 10s # The default caching time in disk if the upstream does not specify the cache time zones: # The parameters of a cache - name: disk_cache_one # The name of the cache, administrator can specify # which cache to use by name in the admin api (disk|memory) memory_size: 50m # The size of shared memory, it's used to store the cache index for # disk strategy, store cache content for memory strategy (disk|memory) disk_size: 1G # The size of disk, it's used to store the cache data (disk) disk_path: /tmp/disk_cache_one # The path to store the cache data (disk) cache_levels: 1:2 # The hierarchy levels of a cache (disk) #- name: disk_cache_two # memory_size: 50m # disk_size: 1G # disk_path: \"/tmp/disk_cache_two\" # cache_levels: \"1:2\" - name: memory_cache memory_size: 50m allow_admin: # http://nginx.org/en/docs/http/ngx_http_access_module.html#allow - 127.0.0.0/24 # If we don't set any IP list, then any IP access is allowed by default. #- \"::/64\" #admin_listen: # use a separate port # ip: 127.0.0.1 # Specific IP, if not set, the default value is `0.0.0.0`. # port: 9180 #https_admin: true # enable HTTPS when use a separate port for Admin API. # Admin API will use conf/apisix_admin_api.crt and conf/apisix_admin_api.key as certificate. admin_api_mtls: # Depends on `admin_listen` and `https_admin`. admin_ssl_cert: \"\" # Path of your self-signed server side cert. admin_ssl_cert_key: \"\" # Path of your self-signed server side key. admin_ssl_ca_cert: \"\" # Path of your self-signed ca cert.The CA is used to sign all admin api callers' certificates. admin_api_version: v3 # The version of admin api, latest version is v3. # Default token when use API to call for Admin API. # *NOTE*: Highly recommended to modify this value to protect APISIX's Admin API. # Disabling this configuration item means that the Admin API does not # require any authentication. admin_key: - name: admin key: edd1c9f034335f136f87ad84b625c8f1 role: admin # admin: manage all configuration data # viewer: only can view configuration data - name: viewer key: 4054f7cf07e344346cd3f287985e76a2 role: viewer delete_uri_tail_slash: false # delete the '/' at the end of the URI # The URI normalization in servlet is a little different from the RFC's. # See https://github.com/jakartaee/servlet/blob/master/spec/src/main/asciidoc/servlet-spec-body.adoc#352-uri-path-canonicalization, # which is used under Tomcat. # Turn this option on if you want to be compatible with servlet when matching URI path. normalize_uri_like_servlet: false router: http: radixtree_uri # radixtree_uri: match route by uri(base on radixtree) # radixtree_host_uri: match route by host + uri(base on radixtree) # radixtree_uri_with_parameter: like radixtree_uri but match uri with parameters, # see https://github.com/api7/lua-resty-radixtree/#parameters-in-path for # more details. ssl: radixtree_sni # radixtree_sni: match route by SNI(base on radixtree) #stream_proxy: # TCP/UDP proxy # only: true # use stream proxy only, don't enable HTTP stuff # tcp: # TCP proxy port list # - addr: 9100 # tls: true # - addr: \"127.0.0.1:9101\" # udp: # UDP proxy port list # - 9200 # - \"127.0.0.1:9201\" #dns_resolver: # If not set, read from `/etc/resolv.conf` # - 1.1.1.1 # - 8.8.8.8 #dns_resolver_valid: 30 # if given, override the TTL of the valid records. The unit is second. resolver_timeout: 5 # resolver timeout enable_resolv_search_opt: true # enable search option in resolv.conf ssl: enable: true listen: # APISIX listening port in https. - port: 9443 enable_http2: true # - ip: 127.0.0.3 # Specific IP, If not set, the default value is `0.0.0.0`. # port: 9445 # enable_http2: true #ssl_trusted_certificate: /path/to/ca-cert # Specifies a file path with trusted CA certificates in the PEM format # used to verify the certificate when APISIX needs to do SSL/TLS handshaking # with external services (e.g. etcd) ssl_protocols: TLSv1.2 TLSv1.3 ssl_ciphers: ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl_session_tickets: false # disable ssl_session_tickets by default for 'ssl_session_tickets' would make Perfect Forward Secrecy useless. # ref: https://github.com/mozilla/server-side-tls/issues/135 key_encrypt_salt: edd1c9f0985e76a2 # If not set, will save origin ssl key into etcd. # If set this, must be a string of length 16. And it will encrypt ssl key with AES-128-CBC # !!! So do not change it after saving your ssl, it can't decrypt the ssl keys have be saved if you change !! #fallback_sni: \"my.default.domain\" # If set this, when the client doesn't send SNI during handshake, the fallback SNI will be used instead enable_control: true #control: # ip: 127.0.0.1 # port: 9090 disable_sync_configuration_during_start: false # safe exit. Remove this once the feature is stable nginx_config: # config for render the template to generate nginx.conf #user: root # specifies the execution user of the worker process. # the \"user\" directive makes sense only if the master process runs with super-user privileges. # if you're not root user,the default is current user. error_log: logs/error.log error_log_level: warn # warn,error worker_processes: auto # if you want use multiple cores in container, you can inject the number of cpu as environment variable \"APISIX_WORKER_PROCESSES\" enable_cpu_affinity: true # enable cpu affinity, this is just work well only on physical machine worker_rlimit_nofile: 20480 # the number of files a worker process can open, should be larger than worker_connections worker_shutdown_timeout: 240s # timeout for a graceful shutdown of worker processes max_pending_timers: 16384 # increase it if you see \"too many pending timers\" error max_running_timers: 4096 # increase it if you see \"lua_max_running_timers are not enough\" error event: worker_connections: 10620 #envs: # allow to get a list of environment variables # - TEST_ENV meta: lua_shared_dict: prometheus-metrics: 15m stream: enable_access_log: false # enable access log or not, default false access_log: logs/access_stream.log access_log_format: \"$remote_addr [$time_local] $protocol $status $bytes_sent $bytes_received $session_time\" # create your custom log format by visiting http://nginx.org/en/docs/varindex.html access_log_format_escape: default # allows setting json or default characters escaping in variables lua_shared_dict: etcd-cluster-health-check-stream: 10m lrucache-lock-stream: 10m plugin-limit-conn-stream: 10m # As user can add arbitrary configurations in the snippet, # it is user's responsibility to check the configurations # don't conflict with APISIX. main_configuration_snippet: | # Add custom Nginx main configuration to nginx.conf. # The configuration should be well indented! http_configuration_snippet: | # Add custom Nginx http configuration to nginx.conf. # The configuration should be well indented! http_server_configuration_snippet: | # Add custom Nginx http server configuration to nginx.conf. # The configuration should be well indented! http_server_location_configuration_snippet: | # Add custom Nginx http server location configuration to nginx.conf. # The configuration should be well indented! http_admin_configuration_snippet: | # Add custom Nginx admin server configuration to nginx.conf. # The configuration should be well indented! http_end_configuration_snippet: | # Add custom Nginx http end configuration to nginx.conf. # The configuration should be well indented! stream_configuration_snippet: | # Add custom Nginx stream configuration to nginx.conf. # The configuration should be well indented! http: enable_access_log: true # enable access log or not, default true access_log: logs/access.log access_log_format: \"$remote_addr - $remote_user [$time_local] $http_host \\\"$request\\\" $status $body_bytes_sent $request_time \\\"$http_referer\\\" \\\"$http_user_agent\\\" $upstream_addr $upstream_status $upstream_response_time \\\"$upstream_scheme://$upstream_host$upstream_uri\\\"\" access_log_format_escape: default # allows setting json or default characters escaping in variables keepalive_timeout: 60s # timeout during which a keep-alive client connection will stay open on the server side. client_header_timeout: 60s # timeout for reading client request header, then 408 (Request Time-out) error is returned to the client client_body_timeout: 60s # timeout for reading client request body, then 408 (Request Time-out) error is returned to the client client_max_body_size: 0 # The maximum allowed size of the client request body. # If exceeded, the 413 (Request Entity Too Large) error is returned to the client. # Note that unlike Nginx, we don't limit the body size by default. send_timeout: 10s # timeout for transmitting a response to the client.then the connection is closed underscores_in_headers: \"on\" # default enables the use of underscores in client request header fields real_ip_header: X-Real-IP # http://nginx.org/en/docs/http/ngx_http_realip_module.html#real_ip_header real_ip_recursive: \"off\" # http://nginx.org/en/docs/http/ngx_http_realip_module.html#real_ip_recursive real_ip_from: # http://nginx.org/en/docs/http/ngx_http_realip_module.html#set_real_ip_from - 127.0.0.1 - \"unix:\" #custom_lua_shared_dict: # add custom shared cache to nginx.conf # ipc_shared_dict: 100m # custom shared cache, format: `cache-key: cache-size` # Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) # when establishing a connection with the proxied HTTPS server. proxy_ssl_server_name: true upstream: keepalive: 320 # Sets the maximum number of idle keepalive connections to upstream servers that are preserved in the cache of each worker process. # When this number is exceeded, the least recently used connections are closed. keepalive_requests: 1000 # Sets the maximum number of requests that can be served through one keepalive connection. # After the maximum number of requests is made, the connection is closed. keepalive_timeout: 60s # Sets a timeout during which an idle keepalive connection to an upstream server will stay open. charset: utf-8 # Adds the specified charset to the \"Content-Type\" response header field, see # http://nginx.org/en/docs/http/ngx_http_charset_module.html#charset variables_hash_max_size: 2048 # Sets the maximum size of the variables hash table. lua_shared_dict: internal-status: 10m plugin-limit-req: 10m plugin-limit-count: 10m prometheus-metrics: 10m plugin-limit-conn: 10m upstream-healthcheck: 10m worker-events: 10m lrucache-lock: 10m balancer-ewma: 10m balancer-ewma-locks: 10m balancer-ewma-last-touched-at: 10m plugin-limit-count-redis-cluster-slot-lock: 1m tracing_buffer: 10m plugin-api-breaker: 10m etcd-cluster-health-check: 10m discovery: 1m jwks: 1m introspection: 10m access-tokens: 1m ext-plugin: 1m kubernetes: 1m tars: 1m etcd: host: # it's possible to define multiple etcd hosts addresses of the same etcd cluster. - \"http://127.0.0.1:2379\" # multiple etcd address, if your etcd cluster enables TLS, please use https scheme, # e.g. https://127.0.0.1:2379. prefix: /apisix # apisix configurations prefix #timeout: 30 # 30 seconds #resync_delay: 5 # when sync failed and a rest is needed, resync after the configured seconds plus 50% random jitter #health_check_timeout: 10 # etcd retry the unhealthy nodes after the configured seconds startup_retry: 2 # the number of retry to etcd during the startup, default to 2 #user: root # root username for etcd #password: 5tHkHhYkjr6cQY # root password for etcd tls: # To enable etcd client certificate you need to build APISIX-Base, see # https://apisix.apache.org/docs/apisix/FAQ#how-do-i-build-the-apisix-base-environment #cert: /path/to/cert # path of certificate used by the etcd client #key: /path/to/key # path of key used by the etcd client verify: true # whether to verify the etcd endpoint certificate when setup a TLS connection to etcd, # the default value is true, e.g. the certificate will be verified strictly. #sni: # the SNI for etcd TLS requests. If missed, the host part of the URL will be used. # HashiCorp Vault storage backend for sensitive data retrieval. The config shows an example of what APISIX expects if you # wish to integrate Vault for secret (sensetive string, public private keys etc.) retrieval. APISIX communicates with Vault # server HTTP APIs. By default, APISIX doesn't need this configuration. # vault: # host: \"http://0.0.0.0:8200\" # The host address where the vault server is running. # timeout: 10 # request timeout 30 seconds # token: root # Authentication token to access Vault HTTP APIs # prefix: kv/apisix # APISIX supports vault kv engine v1, where sensitive data are being stored # and retrieved through vault HTTP APIs. enabling a prefix allows you to better enforcement of # policies, generate limited scoped tokens and tightly control the data that can be accessed # from APISIX. #discovery: # service discovery center # dns: # servers: # - \"127.0.0.1:8600\" # use the real address of your dns server # eureka: # host: # it's possible to define multiple eureka hosts addresses of the same eureka cluster. # - \"http://127.0.0.1:8761\" # prefix: /eureka/ # fetch_interval: 30 # default 30s # weight: 100 # default weight for node # timeout: # connect: 2000 # default 2000ms # send: 2000 # default 2000ms # read: 5000 # default 5000ms # nacos: # host: # - \"http://${username}:${password}@${host1}:${port1}\" # prefix: \"/nacos/v1/\" # fetch_interval: 30 # default 30 sec # weight: 100 # default 100 # timeout: # connect: 2000 # default 2000 ms # send: 2000 # default 2000 ms # read: 5000 # default 5000 ms # consul_kv: # servers: # - \"http://127.0.0.1:8500\" # - \"http://127.0.0.1:8600\" # prefix: \"upstreams\" # skip_keys: # if you need to skip special keys # - \"upstreams/unused_api/\" # timeout: # connect: 2000 # default 2000 ms # read: 2000 # default 2000 ms # wait: 60 # default 60 sec # weight: 1 # default 1 # fetch_interval: 3 # default 3 sec, only take effect for keepalive: false way # keepalive: true # default true, use the long pull way to query consul servers # default_server: # you can define default server when missing hit # host: \"127.0.0.1\" # port: 20999 # metadata: # fail_timeout: 1 # default 1 ms # weight: 1 # default 1 # max_fails: 1 # default 1 # dump: # if you need, when registered nodes updated can dump into file # path: \"logs/consul_kv.dump\" # expire: 2592000 # unit sec, here is 30 day # kubernetes: # service: # schema: https #apiserver schema, options [http, https], default https # host: ${KUBERNETES_SERVICE_HOST} #apiserver host, options [ipv4, ipv6, domain, environment variable], default ${KUBERNETES_SERVICE_HOST} # port: ${KUBERNETES_SERVICE_PORT} #apiserver port, options [port number, environment variable], default ${KUBERNETES_SERVICE_PORT} # client: # # serviceaccount token or path of serviceaccount token_file # token_file: ${KUBERNETES_CLIENT_TOKEN_FILE} # # token: |- # # eyJhbGciOiJSUzI1NiIsImtpZCI6Ikx5ME1DNWdnbmhQNkZCNlZYMXBsT3pYU3BBS2swYzBPSkN3ZnBESGpkUEEif # # 6Ikx5ME1DNWdnbmhQNkZCNlZYMXBsT3pYU3BBS2swYzBPSkN3ZnBESGpkUEEifeyJhbGciOiJSUzI1NiIsImtpZCI # # kubernetes discovery plugin support use namespace_selector # # you can use one of [equal, not_equal, match, not_match] filter namespace # namespace_selector: # # only save endpoints with namespace equal default # equal: default # # only save endpoints with namespace not equal default # #not_equal: default # # only save endpoints with namespace match one of [default, ^my-[a-z]+$] # #match: # #- default # #- ^my-[a-z]+$ # # only save endpoints with namespace not match one of [default, ^my-[a-z]+$ ] # #not_match: # #- default # #- ^my-[a-z]+$ # # kubernetes discovery plugin support use label_selector # # for the expression of label_selector, please refer to https://kubernetes.io/docs/concepts/overview/working-with-objects/labels # label_selector: |- # first=\"a\",second=\"b\" graphql: max_size: 1048576 # the maximum size limitation of graphql in bytes, default 1MiB #ext-plugin: #cmd: [\"ls\", \"-l\"] plugins: # plugin list (sorted by priority) - real-ip # priority: 23000 - client-control # priority: 22000 - proxy-control # priority: 21990 - request-id # priority: 12015 - zipkin # priority: 12011 #- skywalking # priority: 12010 #- opentelemetry # priority: 12009 - ext-plugin-pre-req # priority: 12000 - fault-injection # priority: 11000 - mocking # priority: 10900 - serverless-pre-function # priority: 10000 #- batch-requests # priority: 4010 - cors # priority: 4000 - ip-restriction # priority: 3000 - ua-restriction # priority: 2999 - referer-restriction # priority: 2990 - csrf # priority: 2980 - uri-blocker # priority: 2900 - request-validation # priority: 2800 - openid-connect # priority: 2599 - authz-casbin # priority: 2560 - authz-casdoor # priority: 2559 - wolf-rbac # priority: 2555 - ldap-auth # priority: 2540 - hmac-auth # priority: 2530 - basic-auth # priority: 2520 - jwt-auth # priority: 2510 - key-auth # priority: 2500 - consumer-restriction # priority: 2400 - forward-auth # priority: 2002 - opa # priority: 2001 - authz-keycloak # priority: 2000 #- error-log-logger # priority: 1091 - proxy-mirror # priority: 1010 - proxy-cache # priority: 1009 - proxy-rewrite # priority: 1008 - workflow # priority: 1006 - api-breaker # priority: 1005 - limit-conn # priority: 1003 - limit-count # priority: 1002 - limit-req # priority: 1001 #- node-status # priority: 1000 - gzip # priority: 995 - server-info # priority: 990 - traffic-split # priority: 966 - redirect # priority: 900 - response-rewrite # priority: 899 - kafka-proxy # priority: 508 #- dubbo-proxy # priority: 507 - grpc-transcode # priority: 506 - grpc-web # priority: 505 - public-api # priority: 501 - prometheus # priority: 500 - datadog # priority: 495 - echo # priority: 412 - loggly # priority: 411 - http-logger # priority: 410 - splunk-hec-logging # priority: 409 - skywalking-logger # priority: 408 - google-cloud-logging # priority: 407 - sls-logger # priority: 406 - tcp-logger # priority: 405 - kafka-logger # priority: 403 - rocketmq-logger # priority: 402 - syslog # priority: 401 - udp-logger # priority: 400 - file-logger # priority: 399 - clickhouse-logger # priority: 398 - tencent-cloud-cls # priority: 397 #- log-rotate # priority: 100 # \u003c- recommend to use priority (0, 100) for your custom plugins - example-plugin # priority: 0 - aws-lambda # priority: -1899 - azure-functions # priority: -1900 - openwhisk # priority: -1901 - serverless-post-function # priority: -2000 - ext-plugin-post-req # priority: -3000 - ext-plugin-post-resp # priority: -4000 stream_plugins: # sorted by priority - ip-restriction # priority: 3000 - limit-conn # priority: 1003 - mqtt-proxy # priority: 1000 #- prometheus # priority: 500 - syslog # priority: 401 # \u003c- recommend to use priority (0, 100) for your custom plugins #wasm: #plugins: #- name: wasm_log #priority: 7999 #file: t/wasm/log/main.go.wasm #xrpc: #protocols: #- name: pingpong plugin_attr: log-rotate: interval: 3600 # rotate interval (unit: second) max_kept: 168 # max number of log files will be kept max_size: -1 # max size bytes of log files to be rotated, size check would be skipped with a value less than 0 enable_compression: false # enable log file compression(gzip) or not, default false skywalking: service_name: APISIX service_instance_name: APISIX Instance Name endpoint_addr: http://127.0.0.1:12800 opentelemetry: trace_id_source: x-request-id resource: service.name: APISIX collector: address: 127.0.0.1:4318 request_timeout: 3 request_headers: Authorization: token batch_span_processor: drop_on_queue_full: false max_queue_size: 1024 batch_timeout: 2 inactive_timeout: 1 max_export_batch_size: 16 prometheus: export_uri: /apisix/prometheus/metrics metric_prefix: apisix_ enable_export_server: true export_addr: ip: 127.0.0.1 port: 9091 #metrics: # http_status: # # extra labels from nginx variables # extra_labels: # # the label name doesn't need to be the same as variable name # # below labels are only examples, you could add any valid variables as you need # - upstream_addr: $upstream_addr # - upstream_status: $upstream_status # http_latency: # extra_labels: # - upstream_addr: $upstream_addr # bandwidth: # extra_labels: # - upstream_addr: $upstream_addr server-info: report_ttl: 60 # live time for server info in etcd (unit: second) dubbo-proxy: upstream_multiplex_count: 32 request-id: snowflake: enable: false snowflake_epoc: 1609459200000 # the starting timestamp is expressed in milliseconds data_machine_bits: 12 # data machine bit, maximum 31, because Lua cannot do bit operations greater than 31 sequence_bits: 10 # each machine generates a maximum of (1 \u003c\u003c sequence_bits) serial numbers per millisecond data_machine_ttl: 30 # live time for data_machine in etcd (unit: second) data_machine_interval: 10 # lease renewal interval in etcd (unit: second) proxy-mirror: timeout: # proxy timeout in mirrored sub-request connect: 60s read: 60s send: 60s # redirect: # https_port: 8443 # the default port for use by HTTP redirects to HTTPS #deployment: # role: traditional # role_traditional: # config_provider: etcd # etcd: # host: # it's possible to define multiple etcd hosts addresses of the same etcd cluster. # - \"http://127.0.0.1:2379\" # multiple etcd address, if your etcd cluster enables TLS, please use https scheme, # # e.g. https://127.0.0.1:2379. # prefix: /apisix # configuration prefix in etcd # timeout: 30 # 30 seconds 2. apisix-ingress-controller 参考：apisix-ingress-controller/config-default.yaml at master · apache/apisix-ingress-controller · GitHub\n# Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # log options log_level: \"info\" # the error log level, default is info, optional values are: # debug # info # warn # error # panic # fatal log_output: \"stderr\" # the output file path of error log, default is stderr, when # the file path is \"stderr\" or \"stdout\", logs are marshalled # plainly, which is more readable for human; otherwise logs # are marshalled in JSON format, which can be parsed by # programs easily. log_rotate_output_path: \"\" # rotate output path, the logs will be written in this file log_rotation_max_size: 100 # rotate max size, max size in megabytes of log file before it get rotated. It defaults to 100 log_rotation_max_age: 0 # rotate max age, max age of old log files to retain log_rotation_max_backups: 0 # rotate max backups, max numbers of old log files to retain cert_file: \"/etc/webhook/certs/cert.pem\" # the TLS certificate file path. key_file: \"/etc/webhook/certs/key.pem\" # the TLS key file path. http_listen: \":8080\" # the HTTP Server listen address, default is \":8080\" https_listen: \":8443\" # the HTTPS Server listen address, default is \":8443\" ingress_publish_service: \"\" # the controller will use the Endpoint of this Service to # update the status information of the Ingress resource. # The format is \"namespace/svc-name\" to solve the situation that # the data plane and the controller are not deployed in the same namespace. ingress_status_address: [] # when there is no available information on the Service # used for publishing on the data plane, # the static address provided here will be # used to update the status information of Ingress. # When ingress-publish-service is specified at the same time, ingress-status-address is preferred. # For example, no available LB exists in the bare metal environment. enable_profiling: true # enable profiling via web interfaces # host:port/debug/pprof, default is true. apisix-resource-sync-interval: \"300s\" # Default interval for synchronizing Kubernetes resources to APISIX # Kubernetes related configurations. kubernetes: kubeconfig: \"\" # the Kubernetes configuration file path, default is # \"\", so the in-cluster configuration will be used. resync_interval: \"6h\" # how long should apisix-ingress-controller # re-synchronizes with Kubernetes, default is 6h, # and the minimal resync interval is 30s. app_namespaces: [\"*\"] # namespace list that controller will watch for resources, # by default all namespaces (represented by \"*\") are watched. # The `app_namespace` is deprecated, using `namespace_selector` instead since version 1.4.0 namespace_selector: [\"\"] # namespace_selector represent basis for selecting managed namespaces. # the field is support since version 1.4.0 # For example, \"apisix.ingress=watching\", so ingress will watching the namespaces which labels \"apisix.ingress=watching\" election_id: \"ingress-apisix-leader\" # the election id for the controller leader campaign, # only the leader will watch and delivery resource changes, # other instances (as candidates) stand by. ingress_class: \"apisix\" # the class of an Ingress object is set using the field # IngressClassName in Kubernetes clusters version v1.18.0 # or higher or the annotation \"kubernetes.io/ingress.class\" # (deprecated). ingress_version: \"networking/v1\" # the supported ingress api group version, can be \"networking/v1beta1\" # , \"networking/v1\" (for Kubernetes version v1.19.0 or higher), and # \"extensions/v1beta1\", default is \"networking/v1\". watch_endpointslices: false # whether to watch EndpointSlices rather than Endpoints. apisix_route_version: \"apisix.apache.org/v2\" # the supported apisixroute api group version. # the latest version is \"apisix.apache.org/v2\". enable_gateway_api: false # whether to enable support for Gateway API. # Note: This feature is currently under development and may not work as expected. # It is not recommended to use it in a production environment. # Before we announce support for it to reach Beta level or GA. api_version: apisix.apache.org/v2 # the default value of API version is \"apisix.apache.org/v2\", support \"apisix.apache.org/v2beta3\" and \"apisix.apache.org/v2\". # APISIX related configurations. apisix: default_cluster_base_url: \"http://127.0.0.1:9080/apisix/admin\" # The base url of admin api / manager api # of the default APISIX cluster default_cluster_admin_key: \"\" # the admin key used for the authentication of admin api / manager api in the # default APISIX cluster, by default this field is unset. default_cluster_name: \"default\" # name of the default APISIX cluster. 3. apisix-dashboard 参考：apisix-dashboard/conf.yaml at master · apache/apisix-dashboard · GitHub\n# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # yamllint disable rule:comments-indentation conf: listen: # host: 127.0.0.1 # the address on which the `Manager API` should listen. # The default value is 0.0.0.0, if want to specify, please enable it. # This value accepts IPv4, IPv6, and hostname. port: 9000 # The port on which the `Manager API` should listen. # ssl: # host: 127.0.0.1 # the address on which the `Manager API` should listen for HTTPS. # The default value is 0.0.0.0, if want to specify, please enable it. # port: 9001 # The port on which the `Manager API` should listen for HTTPS. # cert: \"/tmp/cert/example.crt\" # Path of your SSL cert. # key: \"/tmp/cert/example.key\" # Path of your SSL key. allow_list: # If we don't set any IP list, then any IP access is allowed by default. - 127.0.0.1 # The rules are checked in sequence until the first match is found. - ::1 # In this example, access is allowed only for IPv4 network 127.0.0.1, and for IPv6 network ::1. # It also support CIDR like 192.168.1.0/24 and 2001:0db8::/32 etcd: endpoints: # supports defining multiple etcd host addresses for an etcd cluster - 127.0.0.1:2379 # yamllint disable rule:comments-indentation # etcd basic auth info # username: \"root\" # ignore etcd username if not enable etcd auth # password: \"123456\" # ignore etcd password if not enable etcd auth mtls: key_file: \"\" # Path of your self-signed client side key cert_file: \"\" # Path of your self-signed client side cert ca_file: \"\" # Path of your self-signed ca cert, the CA is used to sign callers' certificates # prefix: /apisix # apisix config's prefix in etcd, /apisix by default log: error_log: level: warn # supports levels, lower to higher: debug, info, warn, error, panic, fatal file_path: logs/error.log # supports relative path, absolute path, standard output # such as: logs/error.log, /tmp/logs/error.log, /dev/stdout, /dev/stderr # such as absolute path on Windows: winfile:///C:\\error.log access_log: file_path: logs/access.log # supports relative path, absolute path, standard output # such as: logs/access.log, /tmp/logs/access.log, /dev/stdout, /dev/stderr # such as absolute path on Windows: winfile:///C:\\access.log # log example: 2020-12-09T16:38:09.039+0800 INFO filter/logging.go:46 /apisix/admin/routes/r1 {\"status\": 401, \"host\": \"127.0.0.1:9000\", \"query\": \"asdfsafd=adf\u0026a=a\", \"requestId\": \"3d50ecb8-758c-46d1-af5b-cd9d1c820156\", \"latency\": 0, \"remoteIP\": \"127.0.0.1\", \"method\": \"PUT\", \"errs\": []} max_cpu: 0 # supports tweaking with the number of OS threads are going to be used for parallelism. Default value: 0 [will use max number of available cpu cores considering hyperthreading (if any)]. If the value is negative, is will not touch the existing parallelism profile. # security: # access_control_allow_origin: \"http://httpbin.org\" # access_control_allow_credentials: true # support using custom cors configration # access_control_allow_headers: \"Authorization\" # access_control-allow_methods: \"*\" # x_frame_options: \"deny\" # content_security_policy: \"default-src 'self'; script-src 'self' 'unsafe-eval' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; frame-src xx.xx.xx.xx:3000\" # You can set frame-src to provide content for your grafana panel. authentication: secret: secret # secret for jwt token generation. # NOTE: Highly recommended to modify this value to protect `manager api`. # if it's default value, when `manager api` start, it will generate a random string to replace it. expire_time: 3600 # jwt token expire time, in second users: # yamllint enable rule:comments-indentation - username: admin # username and password for login `manager api` password: admin - username: user password: user plugins: # plugin list (sorted in alphabetical order) - api-breaker - authz-keycloak - basic-auth - batch-requests - consumer-restriction - cors # - dubbo-proxy - echo # - error-log-logger # - example-plugin - fault-injection - grpc-transcode - hmac-auth - http-logger - ip-restriction - jwt-auth - kafka-logger - key-auth - limit-conn - limit-count - limit-req # - log-rotate # - node-status - openid-connect - prometheus - proxy-cache - proxy-mirror - proxy-rewrite - redirect - referer-restriction - request-id - request-validation - response-rewrite - serverless-post-function - serverless-pre-function # - skywalking - sls-logger - syslog - tcp-logger - udp-logger - uri-blocker - wolf-rbac - zipkin - server-info - traffic-split ","categories":"","description":"","excerpt":" 本文主要记录apisix相关组件默认配置，便于查阅。\n1. apisix配置 参考：apisix/config-default.yaml …","ref":"/kubernetes-notes/network/gateway/apisix-config/","tags":["ApiSix"],"title":"APISIX配置"},{"body":"函数 1. 函数定义与调用 //1、函数组成：关键字func ,函数名，参数列表，返回值，函数体，返回语句 //先名称后类型 func 函数名(参数列表)(返回值列表){ //参数列表和返回值列表以变量声明的形式，如果单返回值可以直接加类型 函数体 return //返回语句 } //例子 func Add(a,b int)(ret int,err error){ //函数体 return //return语句 } //2、函数调用 //先导入函数所在的包，直接调用函数 import \"mymath\" sum,err:=mymath.Add(1,2) //多返回值和错误处理机制 //可见性，包括函数、类型、变量 //本包内可见(private)：小写字母开头 //包外可见(public)：大写字母开头 2. 不定参数 //1、不定参数的类型 func myfunc(args ...int){ //...type不定参数的类型，必须是最后一个参数，本质是切片 for _,arg:=range args{ fmt.Println(arg) } } //函数调用,传参可以选择多个，个数不定 myfunc(1,2,3) myfunc(1,2,3,4,5) //2、不定参数的传递，假如有个变参函数myfunc2(args ...int) func myfunc1(args ...int){ //按原样传递 myfunc2(args...) //传递切片 myfunc2(args[1:]...) } //3、任意类型的不定参数，使用interface{}作为指定类型 func Printf(format string,args ...interface{}){ //此为标准库中fmt.Printf()函数的原型 //函数体 } 3. 多返回值 //多返回值 func (file *File) Read(b []byte) (n int,err error) //使用下划线\"_\"来丢弃返回值 n,_:=f.Read(buf) 4. 匿名函数 匿名函数：不带函数名的函数，可以像变量一样被传递。\nfunc(a,b int,z float32) bool{ //没有函数名 return a*b\u003cint(z) } f:=func(x,y int) int{ return x+y } 5. 闭包 5.1. 闭包的概念 闭包是可以包含自由变量（未绑定到特定的对象）的代码块，这些变量不在代码块内或全局上下文中定义，而在定义代码块的环境中定义。要执行的代码块为自由变量提供绑定的计算环境（作用域）。\n5.2. 闭包的价值 闭包的价值在于可以作为一个变量对象来进行传递和返回。即可以把函数本身看作是一个变量。\n5.3. Go中的闭包 Go闭包是指引用了函数外的变量的一种函数，这样该函数就被绑定在某个变量上，只要闭包还被使用则引用的变量会一直存在。\nGo的匿名函数是一个闭包，Go闭包常用在go和defer关键字中。\n5.4. 闭包的坑 在for range中goroutine的方式使用闭包，如果没有给匿名函数传入一个变量，或新建一个变量存储迭代的变量，那么goroutine执行的结果会是最后一个迭代变量的结果，而不是每个迭代变量的结果。这是因为如果没有通过一个变量来拷贝迭代变量，那么闭包因为绑定了变量，当每个groutine运行时，迭代变量可能被更改。\n示例如下：\n// false, print 3 3 3 values := []int{1,2,3} for _, val := range values { go func() { fmt.Println(val) }() } // true, print 1 2 3 for _, val := range values { go func(val interface{}) { fmt.Println(val) }(val) } 5.5 闭包的示例 closure.go\npackage main import ( \"fmt\" ) func main() { var j int = 5 a := func() func() { var i int = 10 return func() { fmt.Printf(\"i, j: %d, %d\\n\", i, j) } }() a() j *= 2 a() } 5.6 闭包的参考链接 https://tour.golang.org/moretypes/25 https://golang.org/doc/faq#closures_and_goroutines https://github.com/golang/go/wiki/CommonMistakes 参考：\n《Go语言编程》 ","categories":"","description":"","excerpt":"函数 1. 函数定义与调用 //1、函数组成：关键字func ,函数名，参数列表，返回值，函数体，返回语句 //先名称后类型 func 函数 …","ref":"/golang-notes/basis/functions/","tags":["Golang"],"title":"函数与闭包"},{"body":"1. 资源视图隔离 容器中的执行top、free等命令展示出来的CPU，内存等信息是从/proc目录中的相关文件里读取出来的。而容器并没有对/proc，/sys等文件系统做隔离，因此容器中读取出来的CPU和内存的信息是宿主机的信息，与容器实际分配和限制的资源量不同。\n/proc/cpuinfo /proc/diskstats /proc/meminfo /proc/stat /proc/swaps /proc/uptime 为了实现让容器内部的资源视图更像虚拟机，使得应用程序可以拿到真实的CPU和内存信息，就需要通过文件挂载的方式将cgroup的真实的容器资源信息挂载到容器内/proc下的文件，使得容器内执行top、free等命令时可以拿到真实的CPU和内存信息。\n2. Lxcfs简介 lxcfs是一个FUSE文件系统，使得Linux容器的文件系统更像虚拟机。lxcfs是一个常驻进程运行在宿主机上，从而来自动维护宿主机cgroup中容器的真实资源信息与容器内/proc下文件的映射关系。\nlxcfs的命令信息如下：\n#/usr/local/bin/lxcfs -h Usage: lxcfs [-f|-d] -u -l -n [-p pidfile] mountpoint -f running foreground by default; -d enable debug output -l use loadavg -u no swap Default pidfile is /run/lxcfs.pid lxcfs -h lxcfs的源码：https://github.com/lxc/lxcfs\n3. Lxcfs原理 lxcfs实现的基本原理是通过文件挂载的方式，把cgroup中容器相关的信息读取出来，存储到lxcfs相关的目录下，并将相关目录映射到容器内的/proc目录下，从而使得容器内执行top,free等命令时拿到的/proc下的数据是真实的cgroup分配给容器的CPU和内存数据。\n原理图\n映射目录\n类别 容器内目录 宿主机lxcfs目录 cpu /proc/cpuinfo /var/lib/lxcfs/{container_id}/proc/cpuinfo 内存 /proc/meminfo /var/lib/lxcfs/{container_id}/proc/meminfo /proc/diskstats /var/lib/lxcfs/{container_id}/proc/diskstats /proc/stat /var/lib/lxcfs/{container_id}/proc/stat /proc/swaps /var/lib/lxcfs/{container_id}/proc/swaps /proc/uptime /var/lib/lxcfs/{container_id}/proc/uptime /proc/loadavg /var/lib/lxcfs/{container_id}/proc/loadavg /sys/devices/system/cpu/online /var/lib/lxcfs/{container_id}/sys/devices/system/cpu/online 4. 使用方式 4.1. 安装lxcfs 环境准备\nyum install -y fuse fuse-lib fuse-devel 源码编译安装\ngit clone git://github.com/lxc/lxcfs cd lxcfs ./bootstrap.sh ./configure make make install 或者通过rpm包安装\nwget https://copr-be.cloud.fedoraproject.org/results/ganto/lxc3/epel-7-x86_64/01041891-lxcfs/lxcfs-3.1.2-0.2.el7.x86_64.rpm; rpm -ivh lxcfs-3.1.2-0.2.el7.x86_64.rpm --force --nodeps 查看是否安装成功\nlxcfs -h 4.2. 运行lxcfs 运行lxcfs主要执行两条命令。\nsudo mkdir -p /var/lib/lxcfs sudo lxcfs /var/lib/lxcfs 可以通过systemd运行。\nlxcfs.service文件：\ncat \u003e /usr/lib/systemd/system/lxcfs.service \u003c\u003cEOF [Unit] Description=lxcfs [Service] ExecStart=/usr/bin/lxcfs -f /var/lib/lxcfs Restart=on-failure #ExecReload=/bin/kill -s SIGHUP $MAINPID [Install] WantedBy=multi-user.target EOF 运行命令\nsystemctl daemon-reload \u0026\u0026 systemctl enable lxcfs \u0026\u0026 systemctl start lxcfs \u0026\u0026 systemctl status lxcfs 4.3. 挂载容器内/proc下的文件目录 docker run -it --rm -m 256m --cpus 2 \\ -v /var/lib/lxcfs/proc/cpuinfo:/proc/cpuinfo:rw \\ -v /var/lib/lxcfs/proc/diskstats:/proc/diskstats:rw \\ -v /var/lib/lxcfs/proc/meminfo:/proc/meminfo:rw \\ -v /var/lib/lxcfs/proc/stat:/proc/stat:rw \\ -v /var/lib/lxcfs/proc/swaps:/proc/swaps:rw \\ -v /var/lib/lxcfs/proc/uptime:/proc/uptime:rw \\ nginx:latest /bin/sh 4.4. 验证容器内CPU和内存 # cpu grep -c processor /proc/cpuinfo cat /proc/cpuinfo # memory free -g cat /proc/meminfo 5. 使用k8s集群部署 使用k8s集群部署与systemd部署方式同理，需要解决2个问题：\n在每个node节点上部署lxcfs常驻进程，lxcfs需要通过镜像来运行，可以通过daemonset来部署。 实现将lxcfs维护的目录自动挂载到pod内的/proc目录。 具体可参考：https://github.com/denverdino/lxcfs-admission-webhook\n5.1. lxcfs-image Dockerfile\nFROM centos:7 as build RUN yum -y update RUN yum -y install fuse-devel pam-devel wget install gcc automake autoconf libtool make ENV LXCFS_VERSION 3.1.2 RUN wget https://linuxcontainers.org/downloads/lxcfs/lxcfs-$LXCFS_VERSION.tar.gz \u0026\u0026 \\ mkdir /lxcfs \u0026\u0026 tar xzvf lxcfs-$LXCFS_VERSION.tar.gz -C /lxcfs --strip-components=1 \u0026\u0026 \\ cd /lxcfs \u0026\u0026 ./configure \u0026\u0026 make FROM centos:7 STOPSIGNAL SIGINT COPY --from=build /lxcfs/lxcfs /usr/local/bin/lxcfs COPY --from=build /lxcfs/.libs/liblxcfs.so /usr/local/lib/lxcfs/liblxcfs.so COPY --from=build /lxcfs/lxcfs /lxcfs/lxcfs COPY --from=build /lxcfs/.libs/liblxcfs.so /lxcfs/liblxcfs.so COPY --from=build /usr/lib64/libfuse.so.2.9.2 /usr/lib64/libfuse.so.2.9.2 COPY --from=build /usr/lib64/libulockmgr.so.1.0.1 /usr/lib64/libulockmgr.so.1.0.1 RUN ln -s /usr/lib64/libfuse.so.2.9.2 /usr/lib64/libfuse.so.2 \u0026\u0026 \\ ln -s /usr/lib64/libulockmgr.so.1.0.1 /usr/lib64/libulockmgr.so.1 COPY start.sh / CMD [\"/start.sh\"] star.sh\n#!/bin/bash # Cleanup nsenter -m/proc/1/ns/mnt fusermount -u /var/lib/lxcfs 2\u003e /dev/null || true nsenter -m/proc/1/ns/mnt [ -L /etc/mtab ] || \\ sed -i \"/^lxcfs \\/var\\/lib\\/lxcfs fuse.lxcfs/d\" /etc/mtab # Prepare mkdir -p /usr/local/lib/lxcfs /var/lib/lxcfs # Update lxcfs cp -f /lxcfs/lxcfs /usr/local/bin/lxcfs cp -f /lxcfs/liblxcfs.so /usr/local/lib/lxcfs/liblxcfs.so # Mount exec nsenter -m/proc/1/ns/mnt /usr/local/bin/lxcfs /var/lib/lxcfs/ 5.2. daemonset lxcfs-daemonset.yaml\napiVersion: apps/v1 kind: DaemonSet metadata: name: lxcfs labels: app: lxcfs spec: selector: matchLabels: app: lxcfs template: metadata: labels: app: lxcfs spec: hostPID: true tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: lxcfs image: registry.cn-hangzhou.aliyuncs.com/denverdino/lxcfs:3.1.2 imagePullPolicy: Always securityContext: privileged: true volumeMounts: - name: cgroup mountPath: /sys/fs/cgroup - name: lxcfs mountPath: /var/lib/lxcfs mountPropagation: Bidirectional - name: usr-local mountPath: /usr/local volumes: - name: cgroup hostPath: path: /sys/fs/cgroup - name: usr-local hostPath: path: /usr/local - name: lxcfs hostPath: path: /var/lib/lxcfs type: DirectoryOrCreate 5.3. lxcfs-admission-webhook lxcfs-admission-webhook实现了一个动态的准入webhook，更准确的讲是实现了一个修改性质的webhook，即监听pod的创建，然后对pod执行patch的操作，从而将lxcfs与容器内的目录映射关系植入到pod创建的yaml中从而实现自动挂载。\ndeployment\napiVersion: apps/v1 kind: Deployment metadata: name: lxcfs-admission-webhook-deployment labels: app: lxcfs-admission-webhook spec: replicas: 1 selector: matchLabels: app: lxcfs-admission-webhook template: metadata: labels: app: lxcfs-admission-webhook spec: containers: - name: lxcfs-admission-webhook image: registry.cn-hangzhou.aliyuncs.com/denverdino/lxcfs-admission-webhook:v1 imagePullPolicy: IfNotPresent args: - -tlsCertFile=/etc/webhook/certs/cert.pem - -tlsKeyFile=/etc/webhook/certs/key.pem - -alsologtostderr - -v=4 - 2\u003e\u00261 volumeMounts: - name: webhook-certs mountPath: /etc/webhook/certs readOnly: true volumes: - name: webhook-certs secret: secretName: lxcfs-admission-webhook-certs 具体部署参考:install.sh\n#!/bin/bash ./deployment/webhook-create-signed-cert.sh kubectl get secret lxcfs-admission-webhook-certs kubectl create -f deployment/deployment.yaml kubectl create -f deployment/service.yaml cat ./deployment/mutatingwebhook.yaml | ./deployment/webhook-patch-ca-bundle.sh \u003e ./deployment/mutatingwebhook-ca-bundle.yaml kubectl create -f deployment/mutatingwebhook-ca-bundle.yaml 执行命令\n/deployment/install.sh 参考：\nhttps://github.com/lxc/lxcfs https://linuxcontainers.org/lxcfs/ https://github.com/denverdino/lxcfs-admission-webhook https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/ ","categories":"","description":"","excerpt":"1. 资源视图隔离 容器中的执行top、free等命令展示出来的CPU，内存等信息是从/proc目录中的相关文件里读取出来的。而容器并没有 …","ref":"/kubernetes-notes/resource/lxcfs/lxcfs/","tags":["Kubernetes"],"title":"Lxcfs资源视图隔离"},{"body":"1. Linux文件管理 Linux中的所有数据都被保存在文件中，所有的文件被分配到不同的目录。目录是一种类似于树的结构，称为文件系统。\n1.1. 文件类型 1、普通文件\n普通文件是以字节为单位的数据流，包括文本文件、源码文件、可执行文件等。文本和二进制对Linux来说并无区别，对普通文件的解释由处理该文件的应用程序进行。\n2、目录\n目录可以包含普通文件和特殊文件，目录相当于Windows和Mac OS中的文件夹。\n3、设备文件\nLinux 与外部设备（例如光驱，打印机，终端，modern等）是通过一种被称为设备文件的文件来进行通信。Linux 输入输出到外部设备的方式和输入输出到一个文件的方式是相同的。Linux 和一个外部设备通讯之前，这个设备必须首先要有一个设备文件存在。\n设备文件和普通文件不一样，设备文件中并不包含任何数据。\n设备文件有两种类型：字符设备文件和块设备文件。\n字符设备文件以字母\"c\"开头。字符设备文件向设备传送数据时，一次传送一个字符。典型的通过字符传送数据的设备有终端、打印机、绘图仪、modern等。字符设备文件有时也被称为\"raw\"设备文件。 块设备文件以字母\"b\"开头。块设备文件向设备传送数据时，先从内存中的buffer中读或写数据，而不是直接传送数据到物理磁盘。磁盘和CD-ROMS既可以使用字符设备文件也可以使用块设备文件。 1.2. 文件属性 可以使用ls -al来查看当前目录下的所有文件列表。\n[root@www ~]# ls -al total 156 drwxr-x--- 4 root root 4096 Sep 8 14:06 . # 当前目录 drwxr-xr-x 23 root root 4096 Sep 8 14:21 .. # 父目录 -rw------- 1 root root 1474 Sep 4 18:27 anaconda-ks.cfg -rw------- 1 root root 199 Sep 8 17:14 .bash_history -rw-r--r-- 1 root root 24 Jan 6 2007 .bash_logout -rw-r--r-- 1 root root 191 Jan 6 2007 .bash_profile -rw-r--r-- 1 root root 176 Jan 6 2007 .bashrc -rw-r--r-- 1 root root 100 Jan 6 2007 .cshrc drwx------ 3 root root 4096 Sep 5 10:37 .gconf drwx------ 2 root root 4096 Sep 5 14:09 .gconfd -rw-r--r-- 1 root root 42304 Sep 4 18:26 install.log -rw-r--r-- 1 root root 5661 Sep 4 18:25 install.log.syslog [ 1 ] [ 2 ][ 3 ][ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 权限 ][文件数][所有者] [用户组][文件容量][ 修改日期 ] [ 文件名 ] 每列含义说明：\n第一列：文件类型。 第二列：表示文件个数。如果是文件，那么就是1；如果是目录，那么就是该目录中文件的数目。 第三列：文件的所有者，即文件的创建者。 第四列：文件所有者所在的用户组。在Linux中，每个用户都隶属于一个用户组。 第五列：文件大小（以字节计）。 第六列：文件被创建或上次被修改的时间。 第七列：文件名或目录名。 文件类型字符\n前缀 描述 - 普通文件。如文本文件、二进制可执行文件、源代码等。 b 块设备文件。硬盘可以使用块设备文件。 c 字符设备文件。硬盘也可以使用字符设备文件。 d 目录文件。目录可以包含文件和其他目录。 l 符号链接（软链接）。可以链接任何普通文件，类似于 Windows 中的快捷方式。 p 具名管道。管道是进程间的一种通信机制。 s 用于进程间通信的套接字。 隐藏文件\n隐藏文件的第一个字符为英文句号或点号(.)，Linux程序（包括Shell）通常使用隐藏文件来保存配置信息。可以通过ls -a来查看所有文件，即包含隐藏文件。\n常见的隐藏文件： .profile：Bourne shell (sh) 初始化脚本 .kshrc：Korn shell (ksh) 初始化脚本 .cshrc：C shell (csh) 初始化脚本 .rhosts：Remote shell (rsh) 配置文件\n1.3. 文件的操作 操作 命令 创建 touch filename 编辑 vi filename 查看 cat filename 复制 cp filename copyfile 重命名 mv filename newfile 删除 rm filename filename2 统计词数 wc filename 1.4. 标准的Linux流 一般情况下，每个Linux程序运行时都会创建三个文件流（三个文件）：\n标准输入流(stdin)：stdin的文件描述符为0，Linux程序默认从stdin读取数据。 标准输出流(stdout)：stdout 的文件描述符为1，Linux程序默认向stdout输出数据。 标准错误流(stderr)：stderr的文件描述符为2，Linux程序会向stderr流中写入错误信息。 2. 文件权限和访问模式 2.1. 查看文件权限 Linux每个文件都有三类权限：\n所有者权限(user)：文件所有者能够进行的操作 组权限(group)：文件所属用户组能够进行的操作 外部权限（other）：其他用户可以进行的操作。 通过ls -l的命令可以查看文件权限信息。\n$ls -l /home/amrood -rwxr-xr-- 1 amrood users 1024 Nov 2 00:10 myfile drwxr-xr--- 1 amrood users 1024 Nov 2 00:10 mydir 第一列-rwxr-xr-- 包含了文件或目录的权限。\n除了第一个字符-或d分别用来表示文件或目录外，其他的九个字符可以分为三组，分别对应所有者权限，用户组权限，其他用户权限，即-|user|group|other。\n每组的权限又可分为三类：\n读取（r），对应权限数字4\n写入（w），对应权限数字2\n执行（x），对应权限数字1\n使用数字表示权限：\n数字 说明 权限 0 没有任何权限 --- 1 执行权限 --x 2 写入权限 -w- 3 执行权限和写入权限：1 (执行) + 2 (写入) = 3 -wx 4 读取权限 r-- 5 读取和执行权限：4 (读取) + 1 (执行) = 5 r-x 6 读取和写入权限：4 (读取) + 2 (写入) = 6 rw- 7 所有权限: 4 (读取) + 2 (写入) + 1 (执行) = 7 rwx 2.2. 访问模式 2.2.1. 文件访问模式 基本的权限有读取(r)、写入(w)和执行(x)：\n读取：用户能够读取文件信息，查看文件内容。 写入：用户可以编辑文件，可以向文件写入内容，也可以删除文件内容。 执行：用户可以将文件作为程序来运行。 2.2.2. 目录访问模式 目录的访问模式和文件类似，但是稍有不同：\n读取：用户可以查看目录中的文件 写入：用户可以在当前目录中删除文件或创建文件 执行：执行权限赋予用户遍历目录的权利，例如执行 cd 和 ls 命令。 2.3. 权限的操作 2.3.1. chmod chmod (change mode) 命令来改变文件或目录的访问权限，权限可以使用符号或数字来表示。\n1、通过符号方式\n可以使用符号来改变文件或目录的权限，你可以增加(+)和删除(-)权限，也可以指定特定权限(=)。\n指定权限范围\nu (user)：所有者权限 g(group)：所属用户组权限 o(other)：其他用户权限 符号 说明 + 为文件或目录增加权限 - 删除文件或目录的权限 = 设置指定的权限 示例\n# 查看权限 $ls -l testfile -rwxrwxr-- 1 amrood users 1024 Nov 2 00:10 testfile # 增加权限 $chmod o+wx testfile $ls -l testfile -rwxrwxrwx 1 amrood users 1024 Nov 2 00:10 testfile # 删除权限 $chmod u-x testfile $ls -l testfile -rw-rwxrwx 1 amrood users 1024 Nov 2 00:10 testfile # 指定权限 $chmod g=rx testfile $ls -l testfile -rw-r-xrwx 1 amrood users 1024 Nov 2 00:10 testfile # 同时使用多个符号 $chmod o+wx,u-x,g=rx testfile $ls -l testfile -rw-r-xrwx 1 amrood users 1024 Nov 2 00:10 testfile 2、通过数字权限方式\n数字权限依照2.1的权限说明。\n示例\n$ls -l testfile -rwxrwxr-- 1 amrood users 1024 Nov 2 00:10 testfile $ chmod 755 testfile $ls -l testfile -rwxr-xr-x 1 amrood users 1024 Nov 2 00:10 testfile 2.3.2. chown chown 命令是\"change owner\"的缩写，用来改变文件的所有者。\n# user可以是用户名或用户ID $ chown user filelist # 例如： $ chown amrood testfile 超级用户 root 可以不受限制的更改文件的所有者和用户组，但是普通用户只能更改所有者是自己的文件或目录。\n2.3.3. chgrp chgrp 命令是\"change group\"的缩写，用来改变文件所在的群组。\n# group可以是用户组名或用户组ID $ chgrp group filelist # 例如： $ chgrp special testfile 2.4. SUID和SGID位 在Linux中，一些程序需要特殊权限才能完成用户指定的操作。例如密码文件/etc/shadow。\nLinux 通过给程序设置SUID(Set User ID)和SGID(Set Group ID)位来赋予普通用户特殊权限。当我们运行一个带有SUID位的程序时，就会继承该程序所有者的权限；如果程序不带SUID位，则会根据程序使用者的权限来运行。\n例如：\n$ ls -l /usr/bin/passwd -r-sr-xr-x 1 root bin 19031 Feb 7 13:47 /usr/bin/passwd* 上面第一列第四个字符不是'x'或'-'，而是's'，说明 /usr/bin/passwd 文件设置了SUID位，这时普通用户会以root用户的权限来执行passwd程序。\n小写字母's'说明文件所有者有执行权限(x)，大写字母'S'说明程序所有者没有执行权限(x)。\n为一个目录设置SUID和SGID位可以使用下面的命令：\n$ chmod ug+s dirname $ ls -l drwsr-sr-x 2 root root 4096 Jun 19 06:45 dirname ","categories":"","description":"","excerpt":"1. Linux文件管理 Linux中的所有数据都被保存在文件中，所有的文件被分配到不同的目录。目录是一种类似于树的结构，称为文件系统。 …","ref":"/linux-notes/file/linux-file-permission/","tags":["Linux"],"title":"Linux文件权限"},{"body":"1. Git commit规范 1.1. 格式 \u003ctype\u003e(\u003cscope\u003e): \u003csubject\u003e 示例：\nfix(ngRepeat): fix trackBy function being invoked with incorrect scope 1.2. type 主要的提交类型如下：\nType 说明 备注 feat 提交新功能 常用 fix 修复bug 常用 docs 修改文档 style 修改格式，例如格式化代码，空格，拼写错误等 refactor 重构代码，没有添加新功能也没有修复bug test 添加或修改测试用例 perf 代码性能调优 chore 修改构建工具、构建流程、更新依赖库、文档生成逻辑 例如vendor包 1.3. scope 表示此次commit涉及的文件范围，可以使用*来表示涉及多个范围。\n1.4. subject 描述此次commit涉及的修改内容。\n使用祈使句（动词开头）、动宾短语。 第一个字母不要大写。 不要以.句号结尾。 2. Git commit工具 安装commitizen和cz-conventional-changelog。\nnpm install -g commitizen cz-conventional-changelog echo '{ \"path\": \"cz-conventional-changelog\" }' \u003e ~/.czrc 使用cz-cli\n$ git cz cz-cli@4.0.3, cz-conventional-changelog@3.0.1 ? Select the type of change that you're committing: (Use arrow keys) ❯ feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests (Move up and down to reveal more choices) 参考：\nhttps://github.com/angular/angular.js/blob/master/DEVELOPERS.md#-git-commit-guidelines https://juejin.im/post/5afc5242f265da0b7f44bee4 commitizen/cz-cli commitizen/cz-conventional-changelog ","categories":"","description":"","excerpt":"1. Git commit规范 1.1. 格式 \u003ctype\u003e(\u003cscope\u003e): \u003csubject\u003e 示例：\nfix(ngRepeat): …","ref":"/linux-notes/git/git-commit-msg/","tags":["Git"],"title":"Git commit规范"},{"body":"详细配置说明 keepalived只有一个配置文件/etc/keepalived/keepalived.conf。\n里面主要包括以下几个配置区域，分别是:\nglobal_defs static_ipaddress static_routes vrrp_script vrrp_instance virtual_server 1. global_defs区域 主要是配置故障发生时的通知对象以及机器标识。\nglobal_defs { notification_email { # notification_email 故障发生时给谁发邮件通知 a@abc.com b@abc.com ... } notification_email_from alert@abc.com # notification_email_from 通知邮件从哪个地址发出 smtp_server smtp.abc.com # smpt_server 通知邮件的smtp地址 smtp_connect_timeout 30 # smtp_connect_timeout 连接smtp服务器的超时时间 enable_traps # enable_traps 开启SNMP陷阱（Simple Network Management Protocol） router_id host163 # router_id 标识本节点的字条串，通常为hostname，但不一定非得是hostname。故障发生时，邮件通知会用到。 } 2. static_ipaddress和static_routes区域[可忽略] static_ipaddress和static_routes区域配置的是是本节点的IP和路由信息。如果你的机器上已经配置了IP和路由，那么这两个区域可以不用配置。其实，一般情况下你的机器都会有IP地址和路由信息的，因此没必要再在这两个区域配置。\nstatic_ipaddress { 10.210.214.163/24 brd 10.210.214.255 dev eth0 ... } static_routes { 10.0.0.0/8 via 10.210.214.1 dev eth0 ... } 3. vrrp_script区域 用来做健康检查的，当时检查失败时会将vrrp_instance的priority减少相应的值。\nvrrp_script chk_http_port { script \"\u003c/dev/tcp/127.0.0.1/80\" #一句指令或者一个脚本文件，需返回0(成功)或非0(失败)，keepalived以此为依据判断其监控的服务状态。 interval 1 #健康检查周期 weight -10 # 优先级变化幅度，如果script中的指令执行失败，那么相应的vrrp_instance的优先级会减少10个点。 } 4. vrrp_instance和vrrp_sync_group区域 vrrp_instance用来定义对外提供服务的VIP区域及其相关属性。\nvrrp_rsync_group用来定义vrrp_intance组，使得这个组内成员动作一致。\nvrrp_sync_group VG_1 { #监控多个网段的实例 group { inside_network # name of vrrp_instance (below) outside_network # One for each moveable IP. ... } notify_master /path/to_master.sh # notify_master表示切换为主机执行的脚本 notify_backup /path/to_backup.sh # notify_backup表示切换为备机师的脚本 notify_fault \"/path/fault.sh VG_1\" # notify_fault表示出错时执行的脚本 notify /path/notify.sh # notify表示任何一状态切换时都会调用该脚本，且在以上三个脚本执行完成之后进行调用 smtp_alert # smtp_alert 表示是否开启邮件通知（用全局区域的邮件设置来发通知） } vrrp_instance VI_1 { state MASTER # state MASTER或BACKUP，当其他节点keepalived启动时会将priority比较大的节点选举为MASTER，因此该项其实没有实质用途。 interface eth0 # interface 节点固有IP（非VIP）的网卡，用来发VRRP包 use_vmac dont_track_primary # use_vmac 是否使用VRRP的虚拟MAC地址，dont_track_primary 忽略VRRP网卡错误（默认未设置） track_interface {# track_interface 监控以下网卡，如果任何一个不通就会切换到FALT状态。（可选项） eth0 eth1 } #mcast_src_ip 修改vrrp组播包的源地址，默认源地址为master的IP mcast_src_ip lvs_sync_daemon_interface eth1 #lvs_sync_daemon_interface 绑定lvs syncd的网卡 garp_master_delay 10 # garp_master_delay 当切为主状态后多久更新ARP缓存，默认5秒 virtual_router_id 1 # virtual_router_id 取值在0-255之间，用来区分多个instance的VRRP组播， 同一网段中virtual_router_id的值不能重复，否则会出错 priority 100 #priority用来选举master的，根据服务是否可用，以weight的幅度来调整节点的priority，从而选取priority高的为master，该项取值范围是1-255（在此范围之外会被识别成默认值100） advert_int 1 # advert_int 发VRRP包的时间间隔，即多久进行一次master选举（可以认为是健康查检时间间隔） authentication { # authentication 认证区域，认证类型有PASS和HA（IPSEC），推荐使用PASS（密码只识别前8位） auth_type PASS #认证方式 auth_pass 12345678 #认证密码 } virtual_ipaddress { # 设置vip 10.210.214.253/24 brd 10.210.214.255 dev eth0 192.168.1.11/24 brd 192.168.1.255 dev eth1 } virtual_routes { # virtual_routes 虚拟路由，当IP漂过来之后需要添加的路由信息 172.16.0.0/12 via 10.210.214.1 192.168.1.0/24 via 192.168.1.1 dev eth1 default via 202.102.152.1 } track_script { chk_http_port } nopreempt # nopreempt 允许一个priority比较低的节点作为master，即使有priority更高的节点启动 preempt_delay 300 # preempt_delay master启动多久之后进行接管资源（VIP/Route信息等），并提是没有nopreempt选项 debug notify_master| notify_backup| notify_fault| notify| smtp_alert } 5. virtual_server_group和virtual_server区域 virtual_server_group一般在超大型的LVS中用到，一般LVS用不到这东西。\nvirtual_server IP Port { delay_loop # delay_loop 延迟轮询时间（单位秒） lb_algo rr|wrr|lc|wlc|lblc|sh|dh # lb_algo 后端调试算法（load balancing algorithm） lb_kind NAT|DR|TUN # lb_kind LVS调度类型NAT/DR/TUN persistence_timeout #会话保持时间 persistence_granularity #lvs会话保持粒度 protocol TCP #使用的协议 ha_suspend virtualhost # virtualhost 用来给HTTP_GET和SSL_GET配置请求header的 alpha omega quorum hysteresis quorum_up| quorum_down| sorry_server #备用机，所有realserver失效后启用 real_server{ # real_server 真正提供服务的服务器 weight 1 # 默认为1,0为失效 inhibit_on_failure #在服务器健康检查失效时，将其设为0，而不是直接从ipvs中删除 notify_up| # real server宕掉时执行的脚本 notify_down| # real server启动时执行的脚本 # HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|MISC_CHECK TCP_CHECK { connect_timeout 3 #连接超时时间 nb_get_retry 3 #重连次数 delay_before_retry 3 #重连间隔时间 connect_port 23 #健康检查的端口的端口 bindto } HTTP_GET|SSL_GET { url {# 检查url，可以指定多个 path # path 请求real serserver上的路径 digest # 用genhash算出的摘要信息 status_code # 检查的http状态码 } connect_port # connect_port 健康检查，如果端口通则认为服务器正常 connect_timeout # 超时时长 nb_get_retry # 重试次数 delay_before_retry # 下次重试的时间延迟 } SMTP_CHECK { host { connect_ip connect_port #默认检查25端口 bindto } connect_timeout 5 retry 3 delay_before_retry 2 helo_name | #smtp helo请求命令参数，可选 } MISC_CHECK { misc_path | #外部脚本路径 misc_timeout #脚本执行超时时间 misc_dynamic #如设置该项，则退出状态码会用来动态调整服务器的权重，返回0 正常，不修改；返回1， #检查失败，权重改为0；返回2-255，正常，权重设置为：返回状态码-2 } } } ","categories":"","description":"","excerpt":"详细配置说明 keepalived只有一个配置文件/etc/keepalived/keepalived.conf。\n里面主要包括以下几个配置 …","ref":"/linux-notes/keepalived/keepalived-conf/","tags":["Keepalived"],"title":"Keepalived配置详解"},{"body":"1. 安装tmux # linux yum install -y tmux # mac brew install tmux 2. tmux常用命令 2.1. 进入tmux tmux 2.2. 退出tmux，程序后台运行 按ctrl + b 进入控制台，再按 d 2.3. 重回上次tmux窗口 ctrl + a 2.4. 结束tmux窗口运行的进程 ctrl + d 3. 更多快捷键说明 类别 快捷键 说明 进入控制台 Ctrl+b 激活控制台；此时以下按键生效 系统操作 ? 列出所有快捷键；按q返回 d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话 D 选择要脱离的会话；在同时开启了多个会话时使用 Ctrl+z 挂起当前会话 r 强制重绘未脱离的会话 s 选择并切换会话；在同时开启了多个会话时使用 : 进入命令行模式；此时可以输入支持的命令，例如kill-server可以关闭服务器 [ 进入复制模式；此时的操作与vi/emacs相同，按q/Esc退出 ~ 列出提示信息缓存；其中包含了之前tmux返回的各种提示信息 窗口操作 c 创建新窗口 \u0026 关闭当前窗口 数字键 切换至指定窗口 p 切换至上一窗口 n 切换至下一窗口 l 在前后两个窗口间互相切换 w 通过窗口列表切换窗口 , 重命名当前窗口；这样便于识别 . 修改当前窗口编号；相当于窗口重新排序 f 在所有窗口中查找指定文本 面板操作 ” 将当前面板平分为上下两块 % 将当前面板平分为左右两块 x 关闭当前面板 ! 将当前面板置于新窗口；即新建一个窗口，其中仅包含当前面板 Ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小 Alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小 Space 在预置的面板布局中循环切换；依次包括even-horizontal、even-vertical、main-horizontal、main-vertical、tiled q 显示面板编号 o 在当前窗口中选择下一面板 方向键 移动光标以选择面板 { 向前置换当前面板 } 向后置换当前面板 Alt+o 逆时针旋转当前窗口的面板 Ctrl+o 顺时针旋转当前窗口的面板 快捷键列表原文：https://blog.csdn.net/hcx25909/article/details/7602935\n","categories":"","description":"","excerpt":"1. 安装tmux # linux yum install -y tmux # mac brew install tmux 2. tmux常 …","ref":"/linux-notes/keymap/tmux-keymap/","tags":["快捷键"],"title":"tmux快捷键"},{"body":"1. UDP协议概述 UDP:User Datagram Protocol的缩写，提供面向无连接的通信服务，在应用程序发来数据收到那一刻则立即原样发送到网络上。即使出现丢包也不负责重发，包出现乱序也不能纠正。\nUDP可以随时发送数据，本身处理简单高效，但不具备可靠性，适合以下场景：\n包总量较少的通信（DNS、SNMP等） 视频、音频等多媒体通信（即使通信） 限定于LAN等特定网络中的应用通信 广播通信（广播、多播） ","categories":"","description":"","excerpt":"1. UDP协议概述 UDP:User Datagram Protocol的缩写，提供面向无连接的通信服务，在应用程序发来数据收到那一刻则立 …","ref":"/linux-notes/tcpip/udp/","tags":["TCPIP"],"title":"UDP协议"},{"body":"1. 使用入门 beego 的日志处理是基于 logs 模块搭建的，内置了一个变量 BeeLogger，默认已经是 logs.BeeLogger 类型，初始化了 console，也就是默认输出到 console。\nbeego.Emergency(\"this is emergency\") beego.Alert(\"this is alert\") beego.Critical(\"this is critical\") beego.Error(\"this is error\") beego.Warning(\"this is warning\") beego.Notice(\"this is notice\") beego.Informational(\"this is informational\") beego.Debug(\"this is debug\") 2. 设置输出 我们的程序往往期望把信息输出到 log 中，现在设置输出到文件很方便，如下所示：\nbeego.SetLogger(\"file\", `{\"filename\":\"logs/test.log\"}`) 这个默认情况就会同时输出到两个地方，一个 console，一个 file，如果只想输出到文件，就需要调用删除操作：\nbeego.BeeLogger.DelLogger(\"console\") 3. 设置级别 LevelEmergency LevelAlert LevelCritical LevelError LevelWarning LevelNotice LevelInformational LevelDebug 级别依次降低，默认全部打印，但是一般我们在部署环境，可以通过设置级别设置日志级别：\nbeego.SetLevel(beego.LevelInformational) 4. 输出文件名和行号 日志默认不输出调用的文件名和文件行号,如果你期望输出调用的文件名和文件行号,可以如下设置\nbeego.SetLogFuncCall(true) 开启传入参数 true, 关闭传入参数 false, 默认是关闭的。\n5. beego/logs模块的使用 是一个用来处理日志的库，目前支持的引擎有 file、console、net、smtp，可以通过如下方式进行安装：\ngo get github.com/astaxie/beego/logs 5.1. 通用方式 首先引入包：\nimport ( \"github.com/astaxie/beego/logs\" ) 然后添加输出引擎（log 支持同时输出到多个引擎），这里我们以 console 为例，第一个参数是引擎名（包括：console、file、conn、smtp、es、multifile）\nlogs.SetLogger(\"console\") 添加输出引擎也支持第二个参数,用来表示配置信息，详细的配置请看下面介绍：\nlogs.SetLogger(logs.AdapterFile,`{\"filename\":\"project.log\",\"level\":7,\"maxlines\":0,\"maxsize\":0,\"daily\":true,\"maxdays\":10}`) 示例：\npackage main import ( \"github.com/astaxie/beego/logs\" ) func main() { //an official log.Logger l := logs.GetLogger() l.Println(\"this is a message of http\") //an official log.Logger with prefix ORM logs.GetLogger(\"ORM\").Println(\"this is a message of orm\") logs.Debug(\"my book is bought in the year of \", 2016) logs.Info(\"this %s cat is %v years old\", \"yellow\", 3) logs.Warn(\"json is a type of kv like\", map[string]int{\"key\": 2016}) logs.Error(1024, \"is a very\", \"good game\") logs.Critical(\"oh,crash\") } 5.2. 输出文件名和行号 日志默认不输出调用的文件名和文件行号,如果你期望输出调用的文件名和文件行号,可以如下设置\nlogs.EnableFuncCallDepth(true) 开启传入参数 true,关闭传入参数 false,默认是关闭的.\n如果你的应用自己封装了调用 log 包,那么需要设置 SetLogFuncCallDepth,默认是 2,也就是直接调用的层级,如果你封装了多层,那么需要根据自己的需求进行调整.\nlogs.SetLogFuncCallDepth(3) 5.3. 异步输出日志 为了提升性能, 可以设置异步输出:\nlogs.Async() 异步输出允许设置缓冲 chan 的大小\nlogs.Async(1e3) 5.4. 引擎配置 console\n可以设置输出的级别，或者不设置保持默认，默认输出到 os.Stdout：\nlogs.SetLogger(logs.AdapterConsole, `{\"level\":1}`) file\n设置的例子如下所示：\nlogs.SetLogger(logs.AdapterFile, `{\"filename\":\"test.log\"}`) 主要的参数如下说明：\nfilename 保存的文件名 maxlines 每个文件保存的最大行数，默认值 1000000 maxsize 每个文件保存的最大尺寸，默认值是 1 \u003c\u003c 28, //256 MB daily 是否按照每天 logrotate，默认是 true maxdays 文件最多保存多少天，默认保存 7 天 rotate 是否开启 logrotate，默认是 true level 日志保存的时候的级别，默认是 Trace 级别 perm 日志文件权限 multifile\n设置的例子如下所示：\nlogs.SetLogger(logs.AdapterMultiFile, ``{\"filename\":\"test.log\",\"separate\":[\"emergency\", \"alert\", \"critical\", \"error\", \"warning\", \"notice\", \"info\", \"debug\"]}``) 主要的参数如下说明(除 separate 外,均与file相同)：\nfilename 保存的文件名 maxlines 每个文件保存的最大行数，默认值 1000000 maxsize 每个文件保存的最大尺寸，默认值是 1 \u003c\u003c 28, //256 MB daily 是否按照每天 logrotate，默认是 true maxdays 文件最多保存多少天，默认保存 7 天 rotate 是否开启 logrotate，默认是 true level 日志保存的时候的级别，默认是 Trace 级别 perm 日志文件权限 separate 需要单独写入文件的日志级别,设置后命名类似 test.error.log conn\n网络输出，设置的例子如下所示：\nlogs.SetLogger(logs.AdapterConn, `{\"net\":\"tcp\",\"addr\":\":7020\"}`) 主要的参数说明如下：\nreconnectOnMsg 是否每次链接都重新打开链接，默认是 false reconnect 是否自动重新链接地址，默认是 false net 发开网络链接的方式，可以使用 tcp、unix、udp 等 addr 网络链接的地址 level 日志保存的时候的级别，默认是 Trace 级别 smtp\n邮件发送，设置的例子如下所示：\nlogs.SetLogger(logs.AdapterMail, `{\"username\":\"beegotest@gmail.com\",\"password\":\"xxxxxxxx\",\"host\":\"smtp.gmail.com:587\",\"sendTos\":[\"xiemengjun@gmail.com\"]}`) 主要的参数说明如下：\nusername smtp 验证的用户名 password smtp 验证密码 host 发送的邮箱地址 sendTos 邮件需要发送的人，支持多个 subject 发送邮件的标题，默认是 Diagnostic message from server level 日志发送的级别，默认是 Trace 级别 ElasticSearch\n输出到 ElasticSearch:\nlogs.SetLogger(logs.AdapterEs, `{\"dsn\":\"http://localhost:9200/\",\"level\":1}` 参考文章：\nhttps://beego.me/docs/mvc/controller/logs.md https://beego.me/docs/module/logs.md ","categories":"","description":"","excerpt":"1. 使用入门 beego 的日志处理是基于 logs 模块搭建的，内置了一个变量 BeeLogger， …","ref":"/golang-notes/web/beego/beego-log/","tags":["Golang"],"title":"Beego 日志处理"},{"body":"1. govendor简介 golang工程的依赖包经常使用go get 的方式来获取，例如：go get github.com/kardianos/govendor ，会将依赖包下载到GOPATH的路径下。\n常用的依赖包管理工具有godep，govendor等，在Golang1.5之后，Go提供了 GO15VENDOREXPERIMENT 环境变量，用于将go build时的应用路径搜索调整成为 当前项目目录/vendor 目录方式。通过这种形式，我们可以实现类似于 godep 方式的项目依赖管理。\n2. 安装与使用 2.1. 安装 go get -u -v github.com/kardianos/govendor 2.2. 使用 #进入到项目目录 cd /home/gopath/src/mytool #初始化vendor目录 govendor init #查看vendor目录 [root@CC54425A mytool]# ls commands main.go vendor mytool_test.sh #进入vendor目录 cd vendor #将GOPATH中本工程使用到的依赖包自动移动到vendor目录中 #说明：如果本地GOPATH没有依赖包，先go get相应的依赖包 govendor add +external #通过设置环境变量 GO15VENDOREXPERIMENT=1 使用vendor文件夹构建文件。 #可以选择 export GO15VENDOREXPERIMENT=1 或 GO15VENDOREXPERIMENT=1 go build 执行编译 export GO15VENDOREXPERIMENT=1 2.3. 说明 govendor只是用来管理项目的依赖包，如果GOPATH中本身没有项目的依赖包，则需要通过go get先下载到GOPATH中，再通过govendor add +external 拷贝到vendor目录中。\n3. govendor的配置文件 vendor.json记录依赖包列表。\n{ \"comment\": \"\", \"ignore\": \"test\", \"package\": [ { \"checksumSHA1\": \"uGalSICR4r7354vvKuNnh7Y/R/4=\", \"path\": \"github.com/urfave/cli\", \"revision\": \"b99aa811b4c1dd84cc6bccb8499c82c72098085a\", \"revisionTime\": \"2017-08-04T09:34:15Z\" } ], \"rootPath\": \"mytool\" } 4. govendor使用命令 [root@CC54425A mytool]# govendor govendor (v1.0.8): record dependencies and copy into vendor folder -govendor-licenses Show govendor's licenses. -version Show govendor version -cpuprofile 'file' Writes a CPU profile to 'file' for debugging. -memprofile 'file' Writes a heap profile to 'file' for debugging. Sub-Commands init Create the \"vendor\" folder and the \"vendor.json\" file. list List and filter existing dependencies and packages. add Add packages from $GOPATH. update Update packages from $GOPATH. remove Remove packages from the vendor folder. status Lists any packages missing, out-of-date, or modified locally. fetch Add new or update vendor folder packages from remote repository. sync Pull packages into vendor folder from remote repository with revisions from vendor.json file. migrate Move packages from a legacy tool to the vendor folder with metadata. get Like \"go get\" but copies dependencies into a \"vendor\" folder. license List discovered licenses for the given status or import paths. shell Run a \"shell\" to make multiple sub-commands more efficient for large projects. go tool commands that are wrapped: \"+status\" package selection may be used with them fmt, build, install, clean, test, vet, generate, tool Status Types +local (l) packages in your project +external (e) referenced packages in GOPATH but not in current project +vendor (v) packages in the vendor folder +std (s) packages in the standard library +excluded (x) external packages explicitly excluded from vendoring +unused (u) packages in the vendor folder, but unused +missing (m) referenced packages but not found +program (p) package is a main package +outside +external +missing +all +all packages Status can be referenced by their initial letters. Package specifier \u003cpath\u003e[::\u003corigin\u003e][{/...|/^}][@[\u003cversion-spec\u003e]] Ignoring files with build tags, or excluding packages from being vendored: The \"vendor.json\" file contains a string field named \"ignore\". It may contain a space separated list of build tags to ignore when listing and copying files. This list may also contain package prefixes (containing a \"/\", possibly as last character) to exclude when copying files in the vendor folder. If \"foo/\" appears in this field, then package \"foo\" and all its sub-packages (\"foo/bar\", …) will be excluded (but package \"bar/foo\" will not). By default the init command adds the \"test\" tag to the ignore list. If using go1.5, ensure GO15VENDOREXPERIMENT=1 is set. ","categories":"","description":"","excerpt":"1. govendor简介 golang工程的依赖包经常使用go get 的方式来获取，例如：go get …","ref":"/golang-notes/introduction/package/govendor-usage/","tags":["Golang"],"title":"govendor的使用"},{"body":"1. heapster简介 Heapster是容器集群监控和性能分析工具，天然的支持Kubernetes和CoreOS。 Kubernetes有个出名的监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor，它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。\n2. heapster部署与配置 2.1. 注意事项 需同步部署机器和被采集机器的时间：ntpdate time.windows.com\n加入定时任务，定期同步时间\ncrontab –e\n30 5 * * * /usr/sbin/ntpdate time.windows.com //每天早晨5点半执行\n2.2. 容器式部署 #拉取镜像 docker pull heapster:latest #运行容器 docker run -d -p 8082:8082 --net=host heapster:latest --source=kubernetes:http://\u003ck8s-server-ip\u003e:8080?inClusterConfig=false\\\u0026useServiceAccount=false --sink=influxdb:http://\u003cinfluxdb-ip\u003e:8086?db=\u003ck8s_env_zone\u003e 2.3. 配置说明 可以参考官方文档\n2.3.1. –source –source: 指定数据获取源。这里指定kube-apiserver即可。 后缀参数： inClusterConfig: kubeletPort: 指定kubelet的使用端口，默认10255 kubeletHttps: 是否使用https去连接kubelets(默认：false) apiVersion: 指定K8S的apiversion insecure: 是否使用安全证书(默认：false) auth: 安全认证 useServiceAccount: 是否使用K8S的安全令牌\n2.3.2. –sink –sink: 指定后端数据存储。这里指定influxdb数据库。 后缀参数： user: InfluxDB用户 pw: InfluxDB密码 db: 数据库名 secure: 安全连接到InfluxDB(默认：false) withfields： 使用InfluxDB fields(默认：false)。\n3. Metrics 分类 Metric Name Description 备注 cpu cpu/limit CPU hard limit in millicores. CPU上限 cpu/node_capacity Cpu capacity of a node. Node节点的CPU容量 cpu/node_allocatable Cpu allocatable of a node. Node节点可分配的CPU cpu/node_reservation Share of cpu that is reserved on the node allocatable. cpu/node_utilization CPU utilization as a share of node allocatable. cpu/request CPU request (the guaranteed amount of resources) in millicores. cpu/usage Cumulative CPU usage on all cores. CPU总使用量 cpu/usage_rate CPU usage on all cores in millicores. filesystem filesystem/usage Total number of bytes consumed on a filesystem. 文件系统的使用量 filesystem/limit The total size of filesystem in bytes. 文件系统的使用上限 filesystem/available The number of available bytes remaining in a the filesystem 可用的文件系统容量 filesystem/inodes The number of available inodes in a the filesystem filesystem/inodes_free The number of free inodes remaining in a the filesystem memory memory/limit Memory hard limit in bytes. 内存上限 memory/major_page_faults Number of major page faults. memory/major_page_faults_rate Number of major page faults per second. memory/node_capacity Memory capacity of a node. memory/node_allocatable Memory allocatable of a node. memory/node_reservation Share of memory that is reserved on the node allocatable. memory/node_utilization Memory utilization as a share of memory allocatable. memory/page_faults Number of page faults. memory/page_faults_rate Number of page faults per second. memory/request Memory request (the guaranteed amount of resources) in bytes. memory/usage Total memory usage. memory/cache Cache memory usage. memory/rss RSS memory usage. memory/working_set Total working set usage. Working set is the memory being used and not easily dropped by the kernel. network network/rx Cumulative number of bytes received over the network. network/rx_errors Cumulative number of errors while receiving over the network. network/rx_errors_rate Number of errors while receiving over the network per second. network/rx_rate Number of bytes received over the network per second. network/tx Cumulative number of bytes sent over the network network/tx_errors Cumulative number of errors while sending over the network network/tx_errors_rate Number of errors while sending over the network network/tx_rate Number of bytes sent over the network per second. uptime Number of milliseconds since the container was started. - 4. Labels Label Name Description pod_id Unique ID of a Pod pod_name User-provided name of a Pod pod_namespace The namespace of a Pod container_base_image Base image for the container container_name User-provided name of the container or full cgroup name for system containers host_id Cloud-provider specified or user specified Identifier of a node hostname Hostname where the container ran labels Comma-separated(Default) list of user-provided labels. Format is 'key:value' namespace_id UID of the namespace of a Pod resource_id A unique identifier used to differentiate multiple metrics of the same type. e.x. Fs partitions under filesystem/usage 5. heapster API 见官方文档：https://github.com/kubernetes/heapster/blob/master/docs/model.md\n","categories":"","description":"","excerpt":"1. heapster简介 Heapster是容器集群监控和性能分析工具，天然的支持Kubernetes和CoreOS。 …","ref":"/kubernetes-notes/monitor/heapster-introduction/","tags":["Monitor"],"title":"Heapster介绍"},{"body":"4. 表内容操作 4.1. 增 insert into 表 (列名,列名...) values (值,值,...) insert into 表 (列名,列名...) values (值,值,...),(值,值,值...) insert into 表 (列名,列名...) select (列名,列名...) from 表 例： insert into tab1(name,email) values('zhangyanlin','zhangyanlin8851@163.com') 4.2. 删 delete from 表 # 删除表里全部数据 delete from 表 where id＝1 and name＝'zhangyanlin' # 删除ID =1 和name='zhangyanlin' 那一行数据 4.3. 改 update 表 set name ＝ 'zhangyanlin' where id\u003e1 4.4. 查 select * from 表 select * from 表 where id \u003e 1 select nid,name,gender as gg from 表 where id \u003e 1 4.5. 条件判断 4.5.1. where select * from \u003ctable\u003e where id \u003e1 and name!='huwh' and num =12; select * from \u003ctable\u003e where id between 5 and 6; select * from \u003ctable\u003e where id in (11,22,33); select * from \u003ctable\u003e where id not in (11,22,33); select * from \u003ctable\u003e where id in (select nid from \u003ctable\u003e) 4.5.2. 通配符like select * from \u003ctable\u003e where name like 'hu%'; #hu开头 select * from \u003ctable\u003e where name like 'hu_' #hu开头后接一个字符 4.5.3. 限制limit select * from \u003ctable\u003e limit 5; #前5行 select * from \u003ctable\u003e limit 4,5 #从第四行开始的5行 select * from \u003ctable\u003e limit 5 offset 4;#从第四行开始的5行 4.5.4. 排序asc，desc select * from \u003ctable\u003e order by 列 asc; #跟据“列”从小到大排序（不指定默认为从小到大排序） select * from \u003ctable\u003e order by 列 desc; #根据“列”从大到小排序 select * from \u003ctable\u003e order by 列1 desc,列2 asc; #根据“列1”从大到小排序，如果相同则按“列2”从小到大排序 4.5.5. 分组group by group by 必须在where之后，order by之前。\nselect num,from \u003ctable\u003e group by num; select num,nid from \u003ctable\u003e group by num,nid; select num from \u003ctable\u003e where nid \u003e 10 group by num,nid order nid desc; select num,nid,count(*),sum(score),max(score) from \u003ctable\u003e group by num; select num from \u003ctable\u003e group by num having max(id) \u003e 10; select num from \u003ctable\u003e group by num; ","categories":"","description":"","excerpt":"4. 表内容操作 4.1. 增 insert into 表 (列名,列名...) values (值,值,...) insert into  …","ref":"/linux-notes/mysql/curd-commands/","tags":["Mysql"],"title":"Mysql常用命令之表内容操作"},{"body":"Pod健康检查 Pod的健康状态由两类探针来检查：LivenessProbe和ReadinessProbe。\n1. 探针类型 1. livenessProbe(存活探针)\n表明容器是否正在运行。 如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略的影响。 如果容器不提供存活探针，则默认状态为 Success。 2. readinessProbe(就绪探针)\n表明容器是否可以正常接受请求。 如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。 初始延迟之前的就绪状态默认为 Failure。 如果容器不提供就绪探针，则默认状态为 Success。 2. 探针使用场景 如果容器异常可以自动崩溃，则不一定要使用探针，可以由Pod的restartPolicy执行重启操作。 存活探针适用于希望容器探测失败后被杀死并重新启动，需要指定restartPolicy 为 Always 或 OnFailure。 就绪探针适用于希望Pod在不能正常接收流量的时候被剔除，并且在就绪探针探测成功后才接收流量。 探针是kubelet对容器执行定期的诊断，主要通过调用容器配置的四类Handler实现：\nHandler的类型：\nExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。 GRPCAction：调用GRPC接口来判断服务是否健康。 如果响应的状态是 \"SERVING\"，则认为诊断成功。 探测结果为以下三种之一：\n成功：容器通过了诊断。 失败：容器未通过诊断。 未知：诊断失败，因此不会采取任何行动。 3. 探针的配置 探针配置在pod的container结构体下，livenessProbe和readinessProbe参数基本一致。\n3.1. 探针通用参数 常用的参数为timeoutSeconds、periodSeconds、periodSeconds，即接口超时时间，重试频率，重试次数三个值。\ninitialDelaySeconds：启动容器后首次进行健康检查的等待时间，单位为秒，默认值为0。 timeoutSeconds:健康检查接口超时响应的时间，默认为1秒，最小值为1秒。 periodSeconds：重试的频率，默认值为10秒，即10秒重试一次，最小值是1秒，建议可以设置为3-5秒。 failureThreshold：失败重试的次数，默认值为3，最小值为1。 successThreshold：最小探测成功次数，默认值为1，一般不设置。 除了以上的通用参数外，livenessProbe和readinessProbe参数基本一致。以下以readinessProbe为例说明探针的使用方式。\n3.2. ReadinessProbe三种实现方式 3.2.1. HTTPGetAction 通过容器的IP地址、端口号及路径调用HTTP Get方法，如果响应的状态码大于等于200且小于等于400，则认为容器健康。\napiVersion: v1 kind: Pod metadata: name: pod-with-healthcheck spec: containers: - name: nginx image: nginx ports: - containnerPort: 80 readinessProbe: httpGet: path: /_status/healthz port: 80 scheme: HTTP initialDelaySeconds: 1 periodSeconds: 5 timeoutSeconds: 5 3.2.2. TCPSocketAction 通过容器IP地址和端口号执行TCP检查，如果能够建立TCP连接，则表明容器健康。\napiVersion: v1 kind: Pod metadata: name: pod-with-healthcheck spec: containers: - name: nginx image: nginx ports: - containnerPort: 80 readinessProbe: tcpSocket: port: 80 initialDelaySeconds: 1 timeoutSeconds: 5 3.2.3. ExecAction 在一个容器内部执行一个命令，如果该命令状态返回值为0，则表明容器健康。\napiVersion: v1 kind: Pod metadata: name: readiness-exec spec: containers: - name: readiness image: tomcagcr.io/google_containers/busybox args: - /bin/sh - -c - echo ok \u003e /tmp/health;sleep 10;rm -fr /tmp/health;sleep 600 readinessreadinessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 1 timeoutSeconds: 5 4. 探针相关源码 探针配置在pod的container结构体下：\n// 存活探针 LivenessProbe *Probe `json:\"livenessProbe,omitempty\" protobuf:\"bytes,10,opt,name=livenessProbe\"` // 就绪探针 ReadinessProbe *Probe `json:\"readinessProbe,omitempty\" protobuf:\"bytes,11,opt,name=readinessProbe\"` 4.1. Probe源码 type Probe struct { // The action taken to determine the health of a container ProbeHandler `json:\",inline\" protobuf:\"bytes,1,opt,name=handler\"` // Number of seconds after the container has started before liveness probes are initiated. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes // +optional InitialDelaySeconds int32 `json:\"initialDelaySeconds,omitempty\" protobuf:\"varint,2,opt,name=initialDelaySeconds\"` // Number of seconds after which the probe times out. // Defaults to 1 second. Minimum value is 1. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes // +optional TimeoutSeconds int32 `json:\"timeoutSeconds,omitempty\" protobuf:\"varint,3,opt,name=timeoutSeconds\"` // How often (in seconds) to perform the probe. // Default to 10 seconds. Minimum value is 1. // +optional PeriodSeconds int32 `json:\"periodSeconds,omitempty\" protobuf:\"varint,4,opt,name=periodSeconds\"` // Minimum consecutive successes for the probe to be considered successful after having failed. // Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1. // +optional SuccessThreshold int32 `json:\"successThreshold,omitempty\" protobuf:\"varint,5,opt,name=successThreshold\"` // Minimum consecutive failures for the probe to be considered failed after having succeeded. // Defaults to 3. Minimum value is 1. // +optional FailureThreshold int32 `json:\"failureThreshold,omitempty\" protobuf:\"varint,6,opt,name=failureThreshold\"` // Optional duration in seconds the pod needs to terminate gracefully upon probe failure. // The grace period is the duration in seconds after the processes running in the pod are sent // a termination signal and the time when the processes are forcibly halted with a kill signal. // Set this value longer than the expected cleanup time for your process. // If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this // value overrides the value provided by the pod spec. // Value must be non-negative integer. The value zero indicates stop immediately via // the kill signal (no opportunity to shut down). // This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate. // Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset. // +optional TerminationGracePeriodSeconds *int64 `json:\"terminationGracePeriodSeconds,omitempty\" protobuf:\"varint,7,opt,name=terminationGracePeriodSeconds\"` } 4.2. ProbeHandler源码 // ProbeHandler defines a specific action that should be taken in a probe. // One and only one of the fields must be specified. type ProbeHandler struct { // Exec specifies the action to take. // +optional Exec *ExecAction `json:\"exec,omitempty\" protobuf:\"bytes,1,opt,name=exec\"` // HTTPGet specifies the http request to perform. // +optional HTTPGet *HTTPGetAction `json:\"httpGet,omitempty\" protobuf:\"bytes,2,opt,name=httpGet\"` // TCPSocket specifies an action involving a TCP port. // +optional TCPSocket *TCPSocketAction `json:\"tcpSocket,omitempty\" protobuf:\"bytes,3,opt,name=tcpSocket\"` // GRPC specifies an action involving a GRPC port. // This is a beta field and requires enabling GRPCContainerProbe feature gate. // +featureGate=GRPCContainerProbe // +optional GRPC *GRPCAction `json:\"grpc,omitempty\" protobuf:\"bytes,4,opt,name=grpc\"` } 4.3. ProbeAction 4.3.1. HTTPGetAction // HTTPHeader describes a custom header to be used in HTTP probes type HTTPHeader struct { // The header field name Name string `json:\"name\" protobuf:\"bytes,1,opt,name=name\"` // The header field value Value string `json:\"value\" protobuf:\"bytes,2,opt,name=value\"` } // HTTPGetAction describes an action based on HTTP Get requests. type HTTPGetAction struct { // Path to access on the HTTP server. // +optional Path string `json:\"path,omitempty\" protobuf:\"bytes,1,opt,name=path\"` // Name or number of the port to access on the container. // Number must be in the range 1 to 65534. // Name must be an IANA_SVC_NAME. Port intstr.IntOrString `json:\"port\" protobuf:\"bytes,2,opt,name=port\"` // Host name to connect to, defaults to the pod IP. You probably want to set // \"Host\" in httpHeaders instead. // +optional Host string `json:\"host,omitempty\" protobuf:\"bytes,3,opt,name=host\"` // Scheme to use for connecting to the host. // Defaults to HTTP. // +optional Scheme URIScheme `json:\"scheme,omitempty\" protobuf:\"bytes,4,opt,name=scheme,casttype=URIScheme\"` // Custom headers to set in the request. HTTP allows repeated headers. // +optional HTTPHeaders []HTTPHeader `json:\"httpHeaders,omitempty\" protobuf:\"bytes,5,rep,name=httpHeaders\"` } // URIScheme identifies the scheme used for connection to a host for Get actions // +enum type URIScheme string const ( // URISchemeHTTP means that the scheme used will be http:// URISchemeHTTP URIScheme = \"HTTP\" // URISchemeHTTPS means that the scheme used will be https:// URISchemeHTTPS URIScheme = \"HTTPS\" ) 4.3.2. TCPSocketAction // TCPSocketAction describes an action based on opening a socket type TCPSocketAction struct { // Number or name of the port to access on the container. // Number must be in the range 1 to 65534. // Name must be an IANA_SVC_NAME. Port intstr.IntOrString `json:\"port\" protobuf:\"bytes,1,opt,name=port\"` // Optional: Host name to connect to, defaults to the pod IP. // +optional Host string `json:\"host,omitempty\" protobuf:\"bytes,2,opt,name=host\"` } 4.3.3. ExecAction // ExecAction describes a \"run in container\" action. type ExecAction struct { // Command is the command line to execute inside the container, the working directory for the // command is root ('/') in the container's filesystem. The command is simply exec'd, it is // not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use // a shell, you need to explicitly call out to that shell. // Exit status of 0 is treated as live/healthy and non-zero is unhealthy. // +optional Command []string `json:\"command,omitempty\" protobuf:\"bytes,1,rep,name=command\"` } 4.3.4. GRPCAction type GRPCAction struct { // Port number of the gRPC service. Number must be in the range 1 to 65534. Port int32 `json:\"port\" protobuf:\"bytes,1,opt,name=port\"` // Service is the name of the service to place in the gRPC HealthCheckRequest // (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md). // // If this is not specified, the default behavior is defined by gRPC. // +optional // +default=\"\" Service *string `json:\"service\" protobuf:\"bytes,2,opt,name=service\"` } 参考文章：\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ 配置存活、就绪和启动探针 | Kubernetes https://mp.weixin.qq.com/s/ApD8D0_UAPftUjw-0Txcyw 《Kubernetes权威指南》 ","categories":"","description":"","excerpt":"Pod健康检查 Pod的健康状态由两类探针来检查：LivenessProbe和ReadinessProbe。\n1. 探针类型 1. …","ref":"/kubernetes-notes/concepts/pod/pod-probe/","tags":["Pod"],"title":"Pod健康检查"},{"body":"1. ETCD资源类型 There are three types of resources in etcd\npermission resources: users and roles in the user store key-value resources: key-value pairs in the key-value store settings resources: security settings, auth settings, and dynamic etcd cluster settings (election/heartbeat) 2. 权限资源 Users：user用来设置身份认证（user：passwd），一个用户可以拥有多个角色，每个角色被分配一定的权限（只读、只写、可读写），用户分为root用户和非root用户。\nRoles：角色用来关联权限，角色主要三类：root角色。默认创建root用户时即创建了root角色，该角色拥有所有权限；guest角色，默认自动创建，主要用于非认证使用。普通角色，由root用户创建角色，并分配指定权限。\n注意：如果没有指定任何验证方式，即没显示指定以什么用户进行访问，那么默认会设定为 guest 角色。默认情况下 guest 也是具有全局访问权限的。如果不希望未授权就获取或修改etcd的数据，则可收回guest角色的权限或删除该角色，etcdctl role revoke 。\nPermissions:权限分为只读、只写、可读写三种权限，权限即对指定目录或key的读写权限。\n3. ETCD访问控制 3.1. 访问控制相关命令 NAME: etcdctl - A simple command line client for etcd. USAGE: etcdctl [global options] command [command options] [arguments...] VERSION: 2.2.0 COMMANDS: user user add, grant and revoke subcommands role role add, grant and revoke subcommands auth overall auth controls GLOBAL OPTIONS: --peers, -C a comma-delimited list of machine addresses in the cluster (default: \"http://127.0.0.1:4001,http://127.0.0.1:2379\") --endpoint a comma-delimited list of machine addresses in the cluster (default: \"http://127.0.0.1:4001,http://127.0.0.1:2379\") --cert-file identify HTTPS client using this SSL certificate file --key-file identify HTTPS client using this SSL key file --ca-file verify certificates of HTTPS-enabled servers using this CA bundle --username, -u provide username[:password] and prompt if password is not supplied. --timeout '1s' connection timeout per request 3.2. user相关命令 [root@localhost etcd]# etcdctl user --help NAME: etcdctl user - user add, grant and revoke subcommands USAGE: etcdctl user command [command options] [arguments...] COMMANDS: add add a new user for the etcd cluster get get details for a user list list all current users remove remove a user for the etcd cluster grant grant roles to an etcd user revoke revoke roles for an etcd user passwd change password for a user help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.2.1. 添加root用户并设置密码 etcdctl --endpoints http://172.16.22.36:2379 user add root\n3.2.2. 添加非root用户并设置密码 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user add huwh\n3.2.3. 查看当前所有用户 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user list\n3.2.4. 将用户添加到对应角色 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user grant --roles test1 phpor\n3.2.5. 查看用户拥有哪些角色 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user get phpor\n3.3. role相关命令 [root@localhost etcd]# etcdctl role --help NAME: etcdctl role - role add, grant and revoke subcommands USAGE: etcdctl role command [command options] [arguments...] COMMANDS: add add a new role for the etcd cluster get get details for a role list list all roles remove remove a role from the etcd cluster grant grant path matches to an etcd role revoke revoke path matches for an etcd role help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.3.1. 添加角色 etcdctl --endpoints http://172.16.22.36:2379 --username root:2379 role add test1\n3.3.2. 查看所有角色 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 role list\n3.3.3. 给角色分配权限 [root@localhost etcd]# etcdctl role grant --help NAME: grant - grant path matches to an etcd role USAGE: command grant [command options] [arguments...] OPTIONS: --path Path granted for the role to access --read Grant read-only access --write Grant write-only access --readwrite Grant read-write access 1、只包含目录 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 role grant --readwrite --path /test1 test1\n2、包括目录和子目录或文件 etcdctl --endpoints http://172.16.22.36:2379 --username root:123 role grant --readwrite --path /test1/* test1\n3.3.4. 查看角色所拥有的权限 etcdctl --endpoints http://172.16.22.36:2379 --username root:2379 role get test1\n3.4. auth相关操作 [root@localhost etcd]# etcdctl auth --help NAME: etcdctl auth - overall auth controls USAGE: etcdctl auth command [command options] [arguments...] COMMANDS: enable enable auth access controls disable disable auth access controls help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.4.1. 开启认证 etcdctl --endpoints http://172.16.22.36:2379 auth enable\n4. 访问控制设置步骤 顺序 步骤 命令 1 添加root用户 etcdctl --endpoints http://: user add root 2 开启认证 etcdctl --endpoints http://: auth enable 3 添加非root用户 etcdctl --endpoints http://: –username root: user add 4 添加角色 etcdctl --endpoints http://: –username root: role add 5 给角色授权（只读、只写、可读写） etcdctl --endpoints http://: –username root: role grant --readwrite --path 6 给用户分配角色（即分配了角色对应的权限） etcdctl --endpoints http://: –username root: user grant --roles 5. 访问认证的API调用 更多参考\nhttps://coreos.com/etcd/docs/latest/v2/auth_api.html\nhttps://coreos.com/etcd/docs/latest/v2/authentication.html\n","categories":"","description":"","excerpt":"1. ETCD资源类型 There are three types of resources in etcd\npermission …","ref":"/kubernetes-notes/etcd/etcd-auth-and-security/","tags":["Etcd"],"title":"Etcd访问控制"},{"body":"1. 基本概念 1.1. image layer（镜像层） 镜像可以看成是由多个镜像层叠加起来的一个文件系统，镜像层也可以简单理解为一个基本的镜像，而每个镜像层之间通过指针的形式进行叠加。\n根据上图，镜像层的主要组成部分包括镜像层id，镜像层指针【指向父层】，元数据【layer metadata】包含了docker构建和运行的信息还有父层的层次信息。\n只读层和读写层【top layer】的组成部分基本一致。同时读写层可以转换成只读层【docker commit操作实现】\n1.2. image（镜像）---【只读层的集合】 1、镜像是一堆只读层的统一视角，除了最底层没有指向外，每一层都指向它的父层，统一文件系统（union file system）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。而每一层都是不可写的，就是只读层。\n1.3. container（容器）---【一层读写层+多层只读层】 1、容器和镜像的区别在于容器的最上面一层是读写层【top layer】，而这边并没有区分容器是否在运行。运行状态的容器【running container】即一个可读写的文件系统【静态容器】+隔离的进程空间和其中的进程。\n隔离的进程空间中的进程可以对该读写层进行增删改，其运行状态容器的进程操作都作用在该读写层上。每个容器只能有一个进程隔离空间。\n2. Docker常用命令原理图概览： 3. Docker常用命令说明 3.1. 标识说明 3.1.1. image---（统一只读文件系统） 3.1.2. 静态容器【未运行的容器】---（统一可读写文件系统） 3.1.3. 动态容器【running container】---（进程空间（包括进程）+统一可读写文件系统） 3.2. 命令说明 3.2.1. docker生命周期相关命令: 3.2.1.1. docker create {image-id} 即为只读文件系统添加一层可读写层【top layer】，生成可读写文件系统，该命令状态下容器为静态容器，并没有运行。\n3.2.1.2. docker start（restart） {container-id} docker stop即为docker start的逆过程\n即为可读写文件系统添加一个进程空间【包括进程】，生成动态容器【running container】\n3.2.1.3. docker run {image-id} docker run=docker create+docker start\n类似流程如下 ：\n3.2.1.4. docker stop {container-id} 向运行的容器中发一个SIGTERM的信号，然后停止所有的进程。即为docker start的逆过程。\n3.2.1.5. docker kill {container-id} docker kill向容器发送不友好的SIGKILL的信号，相当于快速强制关闭容器，与docker stop的区别在于docker stop是正常关闭，先发SIGTERM信号，清理进程，再发SIGKILL信号退出。\n3.2.1.6. docker pause {container-id} docker unpause为逆过程---比较少使用\n暂停容器中的所有进程，使用cgroup的freezer顺序暂停容器里的所有进程，docker unpause为逆过程即恢复所有进程。比较少使用。\n3.2.1.7. docker commit {container-id} 把容器的可读写层转化成只读层，即从容器状态【可读写文件系统】变为镜像状态【只读文件系统】，可理解为【固化】。\n3.2.1.8. docker build docker build=docker run【运行容器】+【进程修改数据】+docker commit【固化数据】，不断循环直至生成所需镜像。\n循环一次便会形成新的层（镜像）【原镜像层+已固化的可读写层】\ndocker build 一般作用在dockerfile文件上。\n3.2.2. docker查询类命令 查询对象：①image，②container，③image/container中的数据，④系统信息[容器数，镜像数及其他]\n3.2.2.1. Image 1、docker images docker images 列出当前镜像【以顶层镜像id来表示整个完整镜像】，每个顶层镜像下面隐藏多个镜像层。\n2、docker images -a docker images -a列出所有镜像层【排序以每个顶层镜像id为首后接该镜像下的所有镜像层】，依次列出每个镜像的所有镜像层。\n3、docker history {image-id} docker history 列出该镜像id下的所有历史镜像。\n3.2.2.2. Container 1、docker ps 列出所有运行的容器【running container】\n2、docker ps -a 列出所有容器，包括静态容器【未运行的容器】和动态容器【running container】\n3.2.2.3. Info 1、docker inspect {container-id} or {image-id} 提取出容器或镜像最顶层的元数据。\n2、docker info 显示 Docker 系统信息，包括镜像和容器数。\n3.2.3. docker操作类命令： 3.2.3.1. docker rm {container-id} docker rm会移除镜像，该命令只能对静态容器【非运行状态】进行操作。\n通过docker rm -f {container-id}的-f （force）参数可以强制删除运行状态的容器【running container】。\n3.2.3.2. docker rmi {image-id} 3.2.3.3. docker exec {running-container-id} docker exec会在运行状态的容器中执行一个新的进程。\n3.2.3.4. docker export {container-id} docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容。\n参考文章：\nhttp://merrigrove.blogspot.com/2015/10/visualizing-docker-containers-and-images.html ","categories":"","description":"","excerpt":"1. 基本概念 1.1. image layer（镜像层） 镜像可以看成是由多个镜像层叠加起来的一个文件系统，镜像层也可以简单理解为一个基本 …","ref":"/kubernetes-notes/runtime/docker/docker-commands-principle/","tags":["Docker"],"title":"Docker常用命令原理图"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/flannel/","tags":"","title":"Flannel"},{"body":"1. FlexVolume介绍 Flexvolume提供了一种扩展k8s存储插件的方式，用户可以自定义自己的存储插件。类似的功能的实现还有CSI的方式。Flexvolume在k8s 1.8+以上版本提供GA功能版本。\n2. 使用方式 在每个node节点安装存储插件二进制，该二进制实现flexvolume的相关接口，默认存储插件的存放路径为/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\u003cvendor~driver\u003e/\u003cdriver\u003e。\n其中vendor~driver的名字需要和pod中flexVolume.driver的字段名字匹配，该字段名字通过/替换~。\n例如：\npath:/usr/libexec/kubernetes/kubelet-plugins/volume/exec/foo~cifs/cifs\npod中flexVolume.driver:foo/cifs\n3. FlexVolume接口 节点上的存储插件需要实现以下的接口。\n3.1. init \u003cdriver executable\u003e init 3.2. attach \u003cdriver executable\u003e attach \u003cjson options\u003e \u003cnode name\u003e 3.3. detach \u003cdriver executable\u003e detach \u003cmount device\u003e \u003cnode name\u003e 3.4. waitforattach \u003cdriver executable\u003e waitforattach \u003cmount device\u003e \u003cjson options\u003e 3.5. isattached \u003cdriver executable\u003e isattached \u003cjson options\u003e \u003cnode name\u003e 3.6. mountdevice \u003cdriver executable\u003e mountdevice \u003cmount dir\u003e \u003cmount device\u003e \u003cjson options\u003e 3.7. unmountdevice \u003cdriver executable\u003e unmountdevice \u003cmount device\u003e 3.8. mount \u003cdriver executable\u003e mount \u003cmount dir\u003e \u003cjson options\u003e 3.9. unmount \u003cdriver executable\u003e unmount \u003cmount dir\u003e 3.10. 插件输出 { \"status\": \"\u003cSuccess/Failure/Not supported\u003e\", \"message\": \"\u003cReason for success/failure\u003e\", \"device\": \"\u003cPath to the device attached. This field is valid only for attach \u0026 waitforattach call-outs\u003e\" \"volumeName\": \"\u003cCluster wide unique name of the volume. Valid only for getvolumename call-out\u003e\" \"attached\": \u003cTrue/False (Return true if volume is attached on the node. Valid only for isattached call-out)\u003e \"capabilities\": \u003cOnly included as part of the Init response\u003e { \"attach\": \u003cTrue/False (Return true if the driver implements attach and detach)\u003e } } 4. 示例 4.1. pod的yaml文件内容 nginx-nfs.yaml\n相关参数为flexVolume.driver等。\napiVersion: v1 kind: Pod metadata: name: nginx-nfs namespace: default spec: containers: - name: nginx-nfs image: nginx volumeMounts: - name: test mountPath: /data ports: - containerPort: 80 volumes: - name: test flexVolume: driver: \"k8s/nfs\" fsType: \"nfs\" options: server: \"172.16.0.25\" share: \"dws_nas_scratch\" 4.2. 插件脚本 nfs脚本实现了flexvolume的接口。\n/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs。\n#!/bin/bash # Copyright 2015 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Notes: # - Please install \"jq\" package before using this driver. usage() { err \"Invalid usage. Usage: \" err \"\\t$0 init\" err \"\\t$0 mount \u003cmount dir\u003e \u003cjson params\u003e\" err \"\\t$0 unmount \u003cmount dir\u003e\" exit 1 } err() { echo -ne $* 1\u003e\u00262 } log() { echo -ne $* \u003e\u00261 } ismounted() { MOUNT=`findmnt -n ${MNTPATH} 2\u003e/dev/null | cut -d' ' -f1` if [ \"${MOUNT}\" == \"${MNTPATH}\" ]; then echo \"1\" else echo \"0\" fi } domount() { MNTPATH=$1 NFS_SERVER=$(echo $2 | jq -r '.server') SHARE=$(echo $2 | jq -r '.share') if [ $(ismounted) -eq 1 ] ; then log '{\"status\": \"Success\"}' exit 0 fi mkdir -p ${MNTPATH} \u0026\u003e /dev/null mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} \u0026\u003e /dev/null if [ $? -ne 0 ]; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\\\"}\" exit 1 fi log '{\"status\": \"Success\"}' exit 0 } unmount() { MNTPATH=$1 if [ $(ismounted) -eq 0 ] ; then log '{\"status\": \"Success\"}' exit 0 fi umount ${MNTPATH} \u0026\u003e /dev/null if [ $? -ne 0 ]; then err \"{ \\\"status\\\": \\\"Failed\\\", \\\"message\\\": \\\"Failed to unmount volume at ${MNTPATH}\\\"}\" exit 1 fi log '{\"status\": \"Success\"}' exit 0 } op=$1 if ! command -v jq \u003e/dev/null 2\u003e\u00261; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"'jq' binary not found. Please install jq package before using this driver\\\"}\" exit 1 fi if [ \"$op\" = \"init\" ]; then log '{\"status\": \"Success\", \"capabilities\": {\"attach\": false}}' exit 0 fi if [ $# -lt 2 ]; then usage fi shift case \"$op\" in mount) domount $* ;; unmount) unmount $* ;; *) log '{\"status\": \"Not supported\"}' exit 0 esac exit 1 参考：\nhttps://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nginx-nfs.yaml https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nfs ","categories":"","description":"","excerpt":"1. FlexVolume介绍 Flexvolume提供了一种扩展k8s存储插件的方式，用户可以自定义自己的存储插件。类似的功能的实现还 …","ref":"/kubernetes-notes/storage/csi/flexvolume/","tags":["CSI"],"title":"FlexVolume介绍"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析k8s中各个核心组件经常使用到的Informer机制(即List-Watch)。该部分的代码主要位于client-go这个第三方包中。\n此部分的逻辑主要位于/vendor/k8s.io/client-go/tools/cache包中，代码目录结构如下：\ncache ├── controller.go # 包含：Config、Run、processLoop、NewInformer、NewIndexerInformer ├── delta_fifo.go # 包含：NewDeltaFIFO、DeltaFIFO、AddIfNotPresent ├── expiration_cache.go ├── expiration_cache_fakes.go ├── fake_custom_store.go ├── fifo.go # 包含：Queue、FIFO、NewFIFO ├── heap.go ├── index.go # 包含：Indexer、MetaNamespaceIndexFunc ├── listers.go ├── listwatch.go # 包含：ListerWatcher、ListWatch、List、Watch ├── mutation_cache.go ├── mutation_detector.go ├── reflector.go # 包含：Reflector、NewReflector、Run、ListAndWatch ├── reflector_metrics.go ├── shared_informer.go # 包含：NewSharedInformer、WaitForCacheSync、Run、HasSynced ├── store.go # 包含：Store、MetaNamespaceKeyFunc、SplitMetaNamespaceKey ├── testing │ ├── fake_controller_source.go ├── thread_safe_store.go # 包含：ThreadSafeStore、threadSafeMap ├── undelta_store.go 0. 原理示意图 示意图1：\n示意图2：\n0.1. client-go组件 Reflector：reflector用来watch特定的k8s API资源。具体的实现是通过ListAndWatch的方法，watch可以是k8s内建的资源或者是自定义的资源。当reflector通过watch API接收到有关新资源实例存在的通知时，它使用相应的列表API获取新创建的对象，并将其放入watchHandler函数内的Delta Fifo队列中。\nInformer：informer从Delta Fifo队列中弹出对象。执行此操作的功能是processLoop。base controller的作用是保存对象以供以后检索，并调用我们的控制器将对象传递给它。\nIndexer：索引器提供对象的索引功能。典型的索引用例是基于对象标签创建索引。 Indexer可以根据多个索引函数维护索引。Indexer使用线程安全的数据存储来存储对象及其键。 在Store中定义了一个名为MetaNamespaceKeyFunc的默认函数，该函数生成对象的键作为该对象的\u003cnamespace\u003e / \u003cname\u003e组合。\n0.2. 自定义controller组件 Informer reference：指的是Informer实例的引用，定义如何使用自定义资源对象。 自定义控制器代码需要创建对应的Informer。\nIndexer reference: 自定义控制器对Indexer实例的引用。自定义控制器需要创建对应的Indexser。\nclient-go中提供NewIndexerInformer函数可以创建Informer 和 Indexer。\nResource Event Handlers：资源事件回调函数，当它想要将对象传递给控制器时，它将被调用。 编写这些函数的典型模式是获取调度对象的key，并将该key排入工作队列以进行进一步处理。\nWork queue：任务队列。 编写资源事件处理程序函数以提取传递的对象的key并将其添加到任务队列。\nProcess Item：处理任务队列中对象的函数， 这些函数通常使用Indexer引用或Listing包装器来重试与该key对应的对象。\n1. sharedInformerFactory.Start 在controller-manager的Run函数部分调用了InformerFactory.Start的方法。\n此部分代码位于/cmd/kube-controller-manager/app/controllermanager.go\n// Run runs the KubeControllerManagerOptions. This should never exit. func Run(c *config.CompletedConfig, stopCh \u003c-chan struct{}) error { ... controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) ... } InformerFactory是一个SharedInformerFactory的接口，接口定义如下：\n此部分代码位于vendor/k8s.io/client-go/informers/internalinterfaces/factory_interfaces.go\n// SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh \u003c-chan struct{}) InformerFor(obj runtime.Object, newFunc NewInformerFunc) cache.SharedIndexInformer } Start方法初始化各种类型的informer，并且每个类型起了个informer.Run的goroutine。\n此部分代码位于vendor/k8s.io/client-go/informers/factory.go\n// Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh \u003c-chan struct{}) { f.lock.Lock() defer f.lock.Unlock() for informerType, informer := range f.informers { if !f.startedInformers[informerType] { go informer.Run(stopCh) f.startedInformers[informerType] = true } } } 2. sharedIndexInformer.Run 此部分的代码位于/vendor/k8s.io/client-go/tools/cache/shared_informer.go\nfunc (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) cfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) } 2.1. NewDeltaFIFO DeltaFIFO是一个对象变化的存储队列，依据先进先出的原则，process的函数接收该队列的Pop方法的输出对象来处理相关功能。\nfifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) 2.2. Config 构造controller的配置文件，构造process，即HandleDeltas，该函数为后面使用到的process函数。\ncfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } 2.3. controller 调用New(cfg)，构建sharedIndexInformer的controller。\nfunc() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() 2.4. cacheMutationDetector.Run 调用s.cacheMutationDetector.Run，检查缓存对象是否变化。\nwg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) defaultCacheMutationDetector.Run\nfunc (d *defaultCacheMutationDetector) Run(stopCh \u003c-chan struct{}) { // we DON'T want protection from panics. If we're running this code, we want to die for { d.CompareObjects() select { case \u003c-stopCh: return case \u003c-time.After(d.period): } } } CompareObjects\nfunc (d *defaultCacheMutationDetector) CompareObjects() { d.lock.Lock() defer d.lock.Unlock() altered := false for i, obj := range d.cachedObjs { if !reflect.DeepEqual(obj.cached, obj.copied) { fmt.Printf(\"CACHE %s[%d] ALTERED!\\n%v\\n\", d.name, i, diff.ObjectDiff(obj.cached, obj.copied)) altered = true } } if altered { msg := fmt.Sprintf(\"cache %s modified\", d.name) if d.failureFunc != nil { d.failureFunc(msg) return } panic(msg) } } 2.5. processor.run 调用s.processor.run，将调用sharedProcessor.run，会调用Listener.run和Listener.pop,执行处理queue的函数。\nwg.StartWithChannel(processorStopCh, s.processor.run) sharedProcessor.Run\nfunc (p *sharedProcessor) run(stopCh \u003c-chan struct{}) { func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } }() \u003c-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } 该部分逻辑待后面分析。\n2.6. controller.Run 调用s.controller.Run，构建Reflector，进行对etcd的缓存\ndefer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) controller.Run\n此部分代码位于/vendor/k8s.io/client-go/tools/cache/controller.go\n// Run begins processing items, and will continue until a value is sent down stopCh. // It's an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() go func() { \u003c-stopCh c.config.Queue.Close() }() r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.clock = c.clock c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group defer wg.Wait() wg.StartWithChannel(stopCh, r.Run) wait.Until(c.processLoop, time.Second, stopCh) } 核心代码：\n// 构建Reflector r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) // 运行Reflector wg.StartWithChannel(stopCh, r.Run) // 执行processLoop wait.Until(c.processLoop, time.Second, stopCh) 3. Reflector 3.1. Reflector Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector只会放置指定的expectedType类型的资源到store中，除非expectedType为nil。如果resyncPeriod不为零，那么Reflector为以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。\n常用属性说明：\nexpectedType：期望放入缓存store的资源类型。 store：watch的资源对应的本地缓存。 listerWatcher：list和watch的接口。 period：watch的周期，默认为1秒。 resyncPeriod：resync的周期，当非零的时候，会按该周期执行list。 lastSyncResourceVersion：最新一次看到的资源的版本号，主要在watch时候使用。 // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { // name identifies this reflector. By default it will be a file:line if possible. name string // metrics tracks basic metric information about the reflector metrics *reflectorMetrics // The type of object we expect to place in the store. expectedType reflect.Type // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // period controls timing between one watch ending and // the beginning of the next one. period time.Duration resyncPeriod time.Duration ShouldResync func() bool // clock allows tests to manipulate time clock clock.Clock // lastSyncResourceVersion is the resource version token last // observed when doing a sync with the underlying store // it is thread safe, but not synchronized with the underlying store lastSyncResourceVersion string // lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion lastSyncResourceVersionMutex sync.RWMutex } 3.2. NewReflector NewReflector主要用来构建Reflector的结构体。\n此部分的代码位于/vendor/k8s.io/client-go/tools/cache/reflector.go\n// NewReflector creates a new Reflector object which will keep the given store up to // date with the server's contents for the given resource. Reflector promises to // only put things in the store that have the type of expectedType, unless expectedType // is nil. If resyncPeriod is non-zero, then lists will be executed after every // resyncPeriod, so that you can use reflectors to periodically process everything as // well as incrementally processing the things that change. func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(getDefaultReflectorName(internalPackages...), lw, expectedType, store, resyncPeriod) } // reflectorDisambiguator is used to disambiguate started reflectors. // initialized to an unstable value to ensure meaning isn't attributed to the suffix. var reflectorDisambiguator = int64(time.Now().UnixNano() % 12345) // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(\u0026reflectorDisambiguator, 1) r := \u0026Reflector{ name: name, // we need this to be unique per process (some names are still the same)but obvious who it belongs to metrics: newReflectorMetrics(makeValidPromethusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: \u0026clock.RealClock{}, } return r } 3.3. Reflector.Run Reflector.Run主要执行了ListAndWatch的方法。\n// Run starts a watch and handles watch events. Will restart the watch if it is closed. // Run will exit when stopCh is closed. func (r *Reflector) Run(stopCh \u003c-chan struct{}) { glog.V(3).Infof(\"Starting reflector %v (%s) from %s\", r.expectedType, r.resyncPeriod, r.name) wait.Until(func() { if err := r.ListAndWatch(stopCh); err != nil { utilruntime.HandleError(err) } }, r.period, stopCh) } 3.4. ListAndWatch ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。\n3.4.1. List // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { glog.V(3).Infof(\"Listing and watching %v from %s\", r.expectedType, r.name) var resourceVersion string // Explicitly set \"0\" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1.ListOptions{ResourceVersion: \"0\"} r.metrics.numberOfLists.Inc() start := r.clock.Now() list, err := r.listerWatcher.List(options) if err != nil { return fmt.Errorf(\"%s: Failed to list %v: %v\", r.name, r.expectedType, err) } r.metrics.listDuration.Observe(time.Since(start).Seconds()) listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v: %v\", r.name, list, err) } resourceVersion = listMetaInterface.GetResourceVersion() items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v (%v)\", r.name, list, err) } r.metrics.numberOfItemsInList.Observe(float64(len(items))) if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err) } r.setLastSyncResourceVersion(resourceVersion) ... } 首先将资源的版本号设置为0，然后调用listerWatcher.List(options)，列出所有list的内容。\n// 版本号设置为0 options := metav1.ListOptions{ResourceVersion: \"0\"} // list接口 list, err := r.listerWatcher.List(options) 获取资源版本号，并将list的内容提取成对象列表。\n// 获取版本号 resourceVersion = listMetaInterface.GetResourceVersion() // 将list的内容提取成对象列表 items, err := meta.ExtractList(list) 将list中对象列表的内容和版本号存储到本地的缓存store中，并全量替换已有的store的内容。\nerr := r.syncWith(items, resourceVersion) syncWith调用了store的Replace的方法来替换原来store中的数据。\n// syncWith replaces the store's items with the given list. func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error { found := make([]interface{}, 0, len(items)) for _, item := range items { found = append(found, item) } return r.store.Replace(found, resourceVersion) } Store.Replace方法定义如下：\ntype Store interface { ... // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. Replace([]interface{}, string) error ... } 最后设置最新的资源版本号。\nr.setLastSyncResourceVersion(resourceVersion) setLastSyncResourceVersion:\nfunc (r *Reflector) setLastSyncResourceVersion(v string) { r.lastSyncResourceVersionMutex.Lock() defer r.lastSyncResourceVersionMutex.Unlock() r.lastSyncResourceVersion = v rv, err := strconv.Atoi(v) if err == nil { r.metrics.lastResourceVersion.Set(float64(rv)) } } 3.4.2. store.Resync resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) go func() { resyncCh, cleanup := r.resyncChan() defer func() { cleanup() // Call the last one written into cleanup }() for { select { case \u003c-resyncCh: case \u003c-stopCh: return case \u003c-cancelCh: return } if r.ShouldResync == nil || r.ShouldResync() { glog.V(4).Infof(\"%s: forcing resync\", r.name) if err := r.store.Resync(); err != nil { resyncerrc \u003c- err return } } cleanup() resyncCh, cleanup = r.resyncChan() } }() 核心代码：\nerr := r.store.Resync() store的具体对象为DeltaFIFO，即调用DeltaFIFO.Resync\n// Resync will send a sync event for each item func (f *DeltaFIFO) Resync() error { f.lock.Lock() defer f.lock.Unlock() if f.knownObjects == nil { return nil } keys := f.knownObjects.ListKeys() for _, k := range keys { if err := f.syncKeyLocked(k); err != nil { return err } } return nil } 3.4.3. Watch for { // give the stopCh a chance to stop the loop, even in case of continue statements further down on errors select { case \u003c-stopCh: return nil default: } timemoutseconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0)) options = metav1.ListOptions{ ResourceVersion: resourceVersion, // We want to avoid situations of hanging watchers. Stop any wachers that do not // receive any events within the timeout window. TimeoutSeconds: \u0026timemoutseconds, } r.metrics.numberOfWatches.Inc() w, err := r.listerWatcher.Watch(options) if err != nil { switch err { case io.EOF: // watch closed normally case io.ErrUnexpectedEOF: glog.V(1).Infof(\"%s: Watch for %v closed with unexpected EOF: %v\", r.name, r.expectedType, err) default: utilruntime.HandleError(fmt.Errorf(\"%s: Failed to watch %v: %v\", r.name, r.expectedType, err)) } // If this is \"connection refused\" error, it means that most likely apiserver is not responsive. // It doesn't make sense to re-list all objects because most likely we will be able to restart // watch where we ended. // If that's the case wait and resend watch request. if urlError, ok := err.(*url.Error); ok { if opError, ok := urlError.Err.(*net.OpError); ok { if errno, ok := opError.Err.(syscall.Errno); ok \u0026\u0026 errno == syscall.ECONNREFUSED { time.Sleep(time.Second) continue } } } return nil } if err := r.watchHandler(w, \u0026resourceVersion, resyncerrc, stopCh); err != nil { if err != errorStopRequested { glog.Warningf(\"%s: watch of %v ended with: %v\", r.name, r.expectedType, err) } return nil } } 设置watch的超时时间，默认为5分钟。\ntimemoutseconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0)) options = metav1.ListOptions{ ResourceVersion: resourceVersion, // We want to avoid situations of hanging watchers. Stop any wachers that do not // receive any events within the timeout window. TimeoutSeconds: \u0026timemoutseconds, } 执行listerWatcher.Watch(options)。\nw, err := r.listerWatcher.Watch(options) 执行watchHandler。\nerr := r.watchHandler(w, \u0026resourceVersion, resyncerrc, stopCh) 3.4.4. watchHandler watchHandler主要是通过watch的方式保证当前的资源版本是最新的。\n// watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh \u003c-chan struct{}) error { start := r.clock.Now() eventCount := 0 // Stopping the watcher should be idempotent and if we return from this function there's no way // we're coming back in with the same watch interface. defer w.Stop() // update metrics defer func() { r.metrics.numberOfItemsInWatch.Observe(float64(eventCount)) r.metrics.watchDuration.Observe(time.Since(start).Seconds()) }() loop: for { select { case \u003c-stopCh: return errorStopRequested case err := \u003c-errc: return err case event, ok := \u003c-w.ResultChan(): if !ok { break loop } if event.Type == watch.Error { return apierrs.FromObject(event.Object) } if e, a := r.expectedType, reflect.TypeOf(event.Object); e != nil \u0026\u0026 e != a { utilruntime.HandleError(fmt.Errorf(\"%s: expected type %v, but watch event object had type %v\", r.name, e, a)) continue } meta, err := meta.Accessor(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) continue } newResourceVersion := meta.GetResourceVersion() switch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) eventCount++ } } watchDuration := r.clock.Now().Sub(start) if watchDuration \u003c 1*time.Second \u0026\u0026 eventCount == 0 { r.metrics.numberOfShortWatches.Inc() return fmt.Errorf(\"very short watch: %s: Unexpected watch close - watch lasted less than a second and no items received\", r.name) } glog.V(4).Infof(\"%s: Watch close - %v total %v items received\", r.name, r.expectedType, eventCount) return nil } 获取watch接口中的事件的channel，来获取事件的内容。\nfor { select { ... case event, ok := \u003c-w.ResultChan(): ... } 当获得添加、更新、删除的事件时，将对应的对象更新到本地缓存store中。\nswitch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } 更新当前的最新版本号。\nnewResourceVersion := meta.GetResourceVersion() *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) 通过对Reflector模块的分析，可以看到多次使用到本地缓存store模块，而store的数据由DeltaFIFO赋值而来，以下针对DeltaFIFO和store做分析。\n4. DeltaFIFO DeltaFIFO由NewDeltaFIFO初始化，并赋值给config.Queue。\nfunc (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) cfg := \u0026Config{ Queue: fifo, ... } ... } 4.1. NewDeltaFIFO // NewDeltaFIFO returns a Store which can be used process changes to items. // // keyFunc is used to figure out what key an object should have. (It's // exposed in the returned DeltaFIFO's KeyOf() method, with bonus features.) // // 'compressor' may compress as many or as few items as it wants // (including returning an empty slice), but it should do what it // does quickly since it is called while the queue is locked. // 'compressor' may be nil if you don't want any delta compression. // // 'keyLister' is expected to return a list of keys that the consumer of // this queue \"knows about\". It is used to decide which items are missing // when Replace() is called; 'Deleted' deltas are produced for these items. // It may be nil if you don't need to detect all deletions. // TODO: consider merging keyLister with this object, tracking a list of // \"known\" keys when Pop() is called. Have to think about how that // affects error retrying. // TODO(lavalamp): I believe there is a possible race only when using an // external known object source that the above TODO would // fix. // // Also see the comment on DeltaFIFO. func NewDeltaFIFO(keyFunc KeyFunc, compressor DeltaCompressor, knownObjects KeyListerGetter) *DeltaFIFO { f := \u0026DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: keyFunc, deltaCompressor: compressor, knownObjects: knownObjects, } f.cond.L = \u0026f.lock return f } controller.Run的部分调用了NewReflector。\nfunc (c *controller) Run(stopCh \u003c-chan struct{}) { ... r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) ... } NewReflector构造函数，将c.config.Queue赋值给Reflector.store的属性。\nfunc NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(getDefaultReflectorName(internalPackages...), lw, expectedType, store, resyncPeriod) } // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(\u0026reflectorDisambiguator, 1) r := \u0026Reflector{ name: name, // we need this to be unique per process (some names are still the same)but obvious who it belongs to metrics: newReflectorMetrics(makeValidPromethusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: \u0026clock.RealClock{}, } return r } 4.2. DeltaFIFO DeltaFIFO是一个生产者与消费者的队列，其中Reflector是生产者，消费者调用Pop()的方法。\nDeltaFIFO主要用在以下场景：\n希望对象变更最多处理一次 处理对象时，希望查看自上次处理对象以来发生的所有事情 要处理对象的删除 希望定期重新处理对象 // DeltaFIFO is like FIFO, but allows you to process deletes. // // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // // DeltaFIFO solves this use case: // * You want to process every object change (delta) at most once. // * When you process an object, you want to see everything // that's happened to it since you last processed it. // * You want to process the deletion of objects. // * You might want to periodically reprocess objects. // // DeltaFIFO's Pop(), Get(), and GetByKey() methods return // interface{} to satisfy the Store/Queue interfaces, but it // will always return an object of type Deltas. // // A note on threading: If you call Pop() in parallel from multiple // threads, you could end up with multiple threads processing slightly // different versions of the same object. // // A note on the KeyLister used by the DeltaFIFO: It's main purpose is // to list keys that are \"known\", for the purpose of figuring out which // items have been deleted when Replace() or Delete() are called. The deleted // object will be included in the DeleteFinalStateUnknown markers. These objects // could be stale. // // You may provide a function to compress deltas (e.g., represent a // series of Updates as a single Update). type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. lock sync.RWMutex cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. items map[string]Deltas queue []string // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // deltaCompressor tells us how to combine two or more // deltas. It may be nil. deltaCompressor DeltaCompressor // knownObjects list keys that are \"known\", for the // purpose of figuring out which items have been deleted // when Replace() or Delete() is called. knownObjects KeyListerGetter // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRED operations. closed bool closedLock sync.Mutex } 4.3. Queue \u0026 Store DeltaFIFO的类型是Queue接口，Reflector.store是Store接口，Queue接口是一个存储队列，Process的方法执行Queue.Pop出来的数据对象，\n// Queue is exactly like a Store, but has a Pop() method too. type Queue interface { Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // Return true if the first batch of items has been popped HasSynced() bool // Close queue Close() } 5. store Store是一个通用的存储接口，Reflector通过watch server的方式更新数据到store中，store给Reflector提供本地的缓存，让Reflector可以像消息队列一样的工作。\nStore实现的是一种可以准确的写入对象和获取对象的机制。\n// Store is a generic object storage interface. Reflector knows how to watch a server // and update a store. A generic store is provided, which allows Reflector to be used // as a local caching system, and an LRU store, which allows Reflector to work like a // queue of items yet to be processed. // // Store makes no assumptions about stored object identity; it is the responsibility // of a Store implementation to provide a mechanism to correctly key objects and to // define the contract for obtaining objects by some arbitrary key type. type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. Replace([]interface{}, string) error Resync() error } 其中Replace方法会删除原来store中的内容，并将新增的list的内容存入store中，即完全替换数据。\n6.1. cache cache实现了store的接口，而cache的具体实现又是调用ThreadSafeStore接口来实现功能的。\ncache的功能主要有以下两点：\n通过keyFunc计算对象的key 调用ThreadSafeStorage接口的方法 // cache responsibilities are limited to: //\t1. Computing keys for objects via keyFunc // 2. Invoking methods of a ThreadSafeStorage interface type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } 其中ListAndWatch主要用到以下的方法：\ncache.Replace\n// Replace will delete the contents of 'c', using instead the given list. // 'c' takes ownership of the list, you should not reference the list again // after calling this function. func (c *cache) Replace(list []interface{}, resourceVersion string) error { items := map[string]interface{}{} for _, item := range list { key, err := c.keyFunc(item) if err != nil { return KeyError{item, err} } items[key] = item } c.cacheStorage.Replace(items, resourceVersion) return nil } cache.Add\n// Add inserts an item into the cache. func (c *cache) Add(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Add(key, obj) return nil } cache.Update\n// Update sets an item in the cache to its updated state. func (c *cache) Update(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Update(key, obj) return nil } cache.Delete\n// Delete removes an item from the cache. func (c *cache) Delete(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Delete(key) return nil } 6.2. ThreadSafeStore cache的具体是调用ThreadSafeStore来实现的。\n// ThreadSafeStore is an interface that allows concurrent access to a storage backend. // TL;DR caveats: you must not modify anything returned by Get or List as it will break // the indexing feature in addition to not being thread safe. // // The guarantees of thread safety provided by List/Get are only valid if the caller // treats returned items as read-only. For example, a pointer inserted in the store // through `Add` will be returned as is by `Get`. Multiple clients might invoke `Get` // on the same key and modify the pointer in a non-thread-safe way. Also note that // modifying objects stored by the indexers (if any) will *not* automatically lead // to a re-index. So it's not a good idea to directly modify the objects returned by // Get/List, in general. type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error Resync() error } threadSafeMap\n// threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} // indexers maps a name to an IndexFunc indexers Indexers // indices maps a name to an Index indices Indices } 6. processLoop func (c *controller) Run(stopCh \u003c-chan struct{}) { ... wait.Until(c.processLoop, time.Second, stopCh) } 在controller.Run方法中会调用processLoop，以下分析processLoop的处理逻辑。\n// processLoop drains the work queue. // TODO: Consider doing the processing in parallel. This will require a little thought // to make sure that we don't end up processing the same object multiple times // concurrently. // // TODO: Plumb through the stopCh here (and down to the queue) so that this can // actually exit when the controller is stopped. Or just give up on this stuff // ever being stoppable. Converting this whole package to use Context would // also be helpful. func (c *controller) processLoop() { for { obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == FIFOClosedError { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } processLoop主要处理任务队列中的任务，其中处理逻辑是调用具体的ProcessFunc函数来实现，核心代码为：\nobj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) 5.1. DeltaFIFO.Pop Pop会阻塞住直到队列里面添加了新的对象，如果有多个对象，按照先进先出的原则处理，如果某个对象没有处理成功会重新被加入该队列中。\nPop中会调用具体的process函数来处理对象。\n// Pop blocks until an item is added to the queue, and then returns it. If // multiple items are ready, they are returned in the order in which they were // added/updated. The item is removed from the queue (and the store) before it // is returned, so if you don't successfully process it, you need to add it back // with AddIfNotPresent(). // process function is called under lock, so it is safe update data structures // in it that need to be in sync with the queue (e.g. knownKeys). The PopProcessFunc // may return an instance of ErrRequeue with a nested error to indicate the current // item should be requeued (equivalent to calling AddIfNotPresent under the lock). // // Pop returns a 'Deltas', which has a complete list of all the things // that happened to the object (deltas) while it was sitting in the queue. func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.IsClosed() { return nil, FIFOClosedError } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] item, ok := f.items[id] if f.initialPopulationCount \u003e 0 { f.initialPopulationCount-- } if !ok { // Item may have been deleted subsequently. continue } delete(f.items, id) err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } } 核心代码：\nfor { ... item, ok := f.items[id] ... err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } 5.2. HandleDeltas cfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } 其中process函数就是在sharedIndexInformer.Run方法中，给config.Process赋值的HandleDeltas函数。\nfunc (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026\u0026 exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 核心代码：\nswitch d.Type { case Sync, Added, Updated: ... if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026\u0026 exists { ... s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { ... s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: ... s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } 根据不同的类型，调用processor.distribute方法，该方法将对象加入processorListener的channel中。\n5.3. sharedProcessor.distribute func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { listener.add(obj) } } else { for _, listener := range p.listeners { listener.add(obj) } } } processorListener.add:\nfunc (p *processorListener) add(notification interface{}) { p.addCh \u003c- notification } 综合以上的分析，可以看出processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入processorListener的channel中，供processorListener.Run使用。以下分析processorListener.Run的部分。\n7. processor processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。在sharedIndexInformer.Run部分会调用processor.run。\n流程：\nlistenser的add函数负责将notify装进pendingNotifications。 pop函数取出pendingNotifications的第一个nofify,输出到nextCh channel。 run函数则负责取出notify，然后根据notify的类型(增加、删除、更新)触发相应的处理函数，这些函数是在不同的NewXxxcontroller实现中注册的。 func (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { ... wg.StartWithChannel(processorStopCh, s.processor.run) ... } 7.1. sharedProcessor.Run func (p *sharedProcessor) run(stopCh \u003c-chan struct{}) { func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } }() \u003c-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } 7.1.1. listener.pop pop函数取出pendingNotifications的第一个nofify,输出到nextCh channel。\nfunc (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop var nextCh chan\u003c- interface{} var notification interface{} for { select { case nextCh \u003c- notification: // Notification dispatched var ok bool notification, ok = p.pendingNotifications.ReadOne() if !ok { // Nothing to pop nextCh = nil // Disable this select case } case notificationToAdd, ok := \u003c-p.addCh: if !ok { return } if notification == nil { // No notification to pop (and pendingNotifications is empty) // Optimize the case - skip adding to pendingNotifications notification = notificationToAdd nextCh = p.nextCh } else { // There is already a notification waiting to be dispatched p.pendingNotifications.WriteOne(notificationToAdd) } } } } 7.1.2. listener.run listener.run部分根据不同的更新类型调用不同的处理函数。\nfunc (p *processorListener) run() { defer utilruntime.HandleCrash() for next := range p.nextCh { switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\"unrecognized notification: %#v\", next)) } } } 其中具体的实现函数handler是在NewDeploymentController（其他不同类型的controller类似）中赋值的，而该handler是一个接口，具体如下：\n// ResourceEventHandler can handle notifications for events that happen to a // resource. The events are informational only, so you can't return an // error. // * OnAdd is called when an object is added. // * OnUpdate is called when an object is modified. Note that oldObj is the // last known state of the object-- it is possible that several changes // were combined together, so you can't use this to see every single // change. OnUpdate is also called when a re-list happens, and it will // get called even if nothing changed. This is useful for periodically // evaluating or syncing something. // * OnDelete will get the final state of the item if it is known, otherwise // it will get an object of type DeletedFinalStateUnknown. This can // happen if the watch is closed and misses the delete event and we don't // notice the deletion until the subsequent re-list. type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } 7.2. ResourceEventHandler 以下以DeploymentController的处理逻辑为例。\n在NewDeploymentController部分会注册deployment的事件函数，以下注册了三种类型的事件函数，其中包括：dInformer、rsInformer和podInformer。\n// NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer extensionsinformers.DeploymentInformer, rsInformer extensionsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { ... dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) ... } 7.2.1. addDeployment 以下以addDeployment为例，addDeployment主要是将对象加入到enqueueDeployment的队列中。\nfunc (dc *DeploymentController) addDeployment(obj interface{}) { d := obj.(*extensions.Deployment) glog.V(4).Infof(\"Adding deployment %s\", d.Name) dc.enqueueDeployment(d) } enqueueDeployment的定义\ntype DeploymentController struct { ... enqueueDeployment func(deployment *extensions.Deployment) ... } 将dc.enqueue赋值给dc.enqueueDeployment\ndc.enqueueDeployment = dc.enqueue dc.enqueue调用了dc.queue.Add(key)\nfunc (dc *DeploymentController) enqueue(deployment *extensions.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf(\"Couldn't get key for object %#v: %v\", deployment, err)) return } dc.queue.Add(key) } dc.queue主要记录了需要被同步的deployment的对象，供syncDeployment使用。\ndc := \u0026DeploymentController{ ... queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } NewNamedRateLimitingQueue\nfunc NewNamedRateLimitingQueue(rateLimiter RateLimiter, name string) RateLimitingInterface { return \u0026rateLimitingType{ DelayingInterface: NewNamedDelayingQueue(name), rateLimiter: rateLimiter, } } 通过以上分析，可以看出processor记录了不同类似的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。\n8. 总结 本文分析的部分主要是k8s的informer机制，即List-Watch机制。\n8.1. Reflector Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector只会放置指定的expectedType类型的资源到store中，除非expectedType为nil。如果resyncPeriod不为零，那么Reflector为以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。\n8.2. ListAndWatch ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。\n8.3. DeltaFIFO DeltaFIFO是一个生产者与消费者的队列，其中Reflector是生产者，消费者调用Pop()的方法。\nDeltaFIFO主要用在以下场景：\n希望对象变更最多处理一次 处理对象时，希望查看自上次处理对象以来发生的所有事情 要处理对象的删除 希望定期重新处理对象 8.4. store Store是一个通用的存储接口，Reflector通过watch server的方式更新数据到store中，store给Reflector提供本地的缓存，让Reflector可以像消息队列一样的工作。\nStore实现的是一种可以准确的写入对象和获取对象的机制。\n8.5. processor processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。在sharedIndexInformer.Run部分会调用processor.run。\n流程：\nlistenser的add函数负责将notify装进pendingNotifications。 pop函数取出pendingNotifications的第一个nofify,输出到nextCh channel。 run函数则负责取出notify，然后根据notify的类型(增加、删除、更新)触发相应的处理函数，这些函数是在不同的NewXxxcontroller实现中注册的。 processor记录了不同类似的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。\n8.6. 主要步骤 在controller-manager的Run函数部分调用了InformerFactory.Start的方法，Start方法初始化各种类型的informer，并且每个类型起了个informer.Run的goroutine。 informer.Run的部分先生成一个DeltaFIFO的队列来存储对象变化的数据。然后调用processor.Run和controller.Run函数。 controller.Run函数会生成一个Reflector，Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。 Reflector接着执行ListAndWatch函数，ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。 controller.Run函数还会调用processLoop函数，processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入processorListener的channel中，供processorListener.Run使用。 processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。processor记录了不同类型的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。 参考文章：\nhttps://github.com/kubernetes/client-go/tree/master/tools/cache\nhttps://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md\nhttps://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go\n","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析k8s中各个核心组件经常使用到的Informer机制( …","ref":"/k8s-source-code-analysis/kube-controller-manager/informer/","tags":["源码分析"],"title":"kube-controller-manager源码分析（三）之 Informer机制"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/kata/","tags":"","title":"Kata Container"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kube-scheduler/","tags":"","title":"kube-scheduler"},{"body":"通过kubeadm搭建的集群默认的证书时间是1年（由于官方期望每年更新一次k8s的版本，在更新的时候会默认更新证书），当你执行命令出现以下报错，说明你的证书已经到期了，则需要手动更新证书。\n# kubectl get node Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2023-08-03T18:06:23+08:00 is after 2023-07-04T06:30:54Z # 或者出现以下报错 You must be logged in to the server(unauthorized) 以下说明手动更新证书的流程。\n具体可以参考：使用 kubeadm 进行证书管理 | Kubernetes\n1. 检查证书是否过期 kubeadm certs check-expiration 会输出以下的内容：\nCERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Dec 30, 2020 23:36 UTC 364d no apiserver Dec 30, 2020 23:36 UTC 364d ca no apiserver-etcd-client Dec 30, 2020 23:36 UTC 364d etcd-ca no apiserver-kubelet-client Dec 30, 2020 23:36 UTC 364d ca no controller-manager.conf Dec 30, 2020 23:36 UTC 364d no etcd-healthcheck-client Dec 30, 2020 23:36 UTC 364d etcd-ca no etcd-peer Dec 30, 2020 23:36 UTC 364d etcd-ca no etcd-server Dec 30, 2020 23:36 UTC 364d etcd-ca no front-proxy-client Dec 30, 2020 23:36 UTC 364d front-proxy-ca no scheduler.conf Dec 30, 2020 23:36 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Dec 28, 2029 23:36 UTC 9y no etcd-ca Dec 28, 2029 23:36 UTC 9y no front-proxy-ca Dec 28, 2029 23:36 UTC 9y no 2. 手动更新过期的证书 分别在master节点执行以下命令。\n2.1. 备份/etc/kubernetes目录 cp -fr /etc/kubernetes /etc/kubernetes.bak cp -fr ~/.kube ~/.kube.bak 2.2. 执行更新证书命令 kubeadm certs renew all 输出如下：\n# kubeadm certs renew all [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [renew] Error reading configuration from the Cluster. Falling back to default configuration certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates. 2.3. 重启k8s组件 先重启etcd\n注意事项：需要将三个master节点的证书都重新更新后，然后三个master的etcd服务一起重启，使得etcd集群使用新的证书可以正常运行，否则会导致kube-apiserver也启动失败。\ncrictl ps |grep \"etcd\"|awk '{print $1}'|xargs crictl stop 再重启kube-apiserver、kube-controller、kube-scheduler容器。\ncrictl ps |egrep \"kube-apiserver|kube-scheduler|kube-controller\"|awk '{print $1}'|xargs crictl stop 2.4. 更新默认的kubeconfig文件 cp -fr /etc/kubernetes/admin.conf $HOME/.kube/config 2.5. 配置kubelet证书轮转 由于kubelet默认支持证书轮转，当证书过期时，可以自动生成新的密钥，并从 Kubernetes API 申请新的证书。可以查看kubelet的配置检查是否已经开启。\n# cat /var/lib/kubelet/config.yaml |grep rotate rotateCertificates: true 3. 修改kubeadm源码证书时间 由于社区不允许用户配置超过1年的证书，因此自定义证书时间的参数不被允许开发。\n相关issue如下：\nhttps://github.com/kubernetes/kubernetes/issues/119350 如果要实现自定义参数设置证书时间，可参考一下pr：\nhttps://github.com/kubernetes/kubernetes/pull/100907/files 如果需要修改kubeadm源码证书可以参考如下代码修改。\nkubeadm中跟证书相关的代码有：\n3.1. ca文件的有效期（默认为10年） 代码文件：./staging/src/k8s.io/client-go/util/cert/cert.go 中 NewSelfSignedCACert 函数的NotAfter字段\n代码如下：\n// NewSelfSignedCACert creates a CA certificate func NewSelfSignedCACert(cfg Config, key crypto.Signer) (*x509.Certificate, error) { now := time.Now() // returns a uniform random value in [0, max-1), then add 1 to serial to make it a uniform random value in [1, max). serial, err := cryptorand.Int(cryptorand.Reader, new(big.Int).SetInt64(math.MaxInt64-1)) if err != nil { return nil, err } serial = new(big.Int).Add(serial, big.NewInt(1)) notBefore := now.UTC() if !cfg.NotBefore.IsZero() { notBefore = cfg.NotBefore.UTC() } tmpl := x509.Certificate{ SerialNumber: serial, Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, DNSNames: []string{cfg.CommonName}, NotBefore: notBefore, NotAfter: now.Add(duration365d * 10).UTC(), # 默认为10年 KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, IsCA: true, } certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, \u0026tmpl, \u0026tmpl, key.Public(), key) if err != nil { return nil, err } return x509.ParseCertificate(certDERBytes) } 3.2. 证书文件的有效期（默认为1年） 代码文件：cmd/kubeadm/app/util/pkiutil/pki_helpers.go中 NewSignedCert 函数的 notAfter 字段\n常量参数kubeadmconstants.CertificateValidity ： /cmd/kubeadm/app/constants/constants.go 代码如下：\n// NewSignedCert creates a signed certificate using the given CA certificate and key func NewSignedCert(cfg *CertConfig, key crypto.Signer, caCert *x509.Certificate, caKey crypto.Signer, isCA bool) (*x509.Certificate, error) { // returns a uniform random value in [0, max-1), then add 1 to serial to make it a uniform random value in [1, max). serial, err := cryptorand.Int(cryptorand.Reader, new(big.Int).SetInt64(math.MaxInt64-1)) if err != nil { return nil, err } serial = new(big.Int).Add(serial, big.NewInt(1)) if len(cfg.CommonName) == 0 { return nil, errors.New(\"must specify a CommonName\") } keyUsage := x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature if isCA { keyUsage |= x509.KeyUsageCertSign } RemoveDuplicateAltNames(\u0026cfg.AltNames) # 此处引用了一个常量 notAfter := time.Now().Add(kubeadmconstants.CertificateValidity).UTC() if cfg.NotAfter != nil { notAfter = *cfg.NotAfter } certTmpl := x509.Certificate{ Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, DNSNames: cfg.AltNames.DNSNames, IPAddresses: cfg.AltNames.IPs, SerialNumber: serial, NotBefore: caCert.NotBefore, NotAfter: notAfter, KeyUsage: keyUsage, ExtKeyUsage: cfg.Usages, BasicConstraintsValid: true, IsCA: isCA, } certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, \u0026certTmpl, caCert, key.Public(), caKey) if err != nil { return nil, err } return x509.ParseCertificate(certDERBytes) } 其中常量文件为：\n/cmd/kubeadm/app/constants/constants.go 代码如下：\n# 常量默认证书为1年。 // CertificateValidity defines the validity for all the signed certificates generated by kubeadm CertificateValidity = time.Hour * 24 * 365 可以修改此处常量的值10年，例如：\n# 常量默认证书为1年。 // CertificateValidity defines the validity for all the signed certificates generated by kubeadm CertificateValidity = time.Hour * 24 * 365 * 10 修改源码后，就可以重新编译kubeadm二进制。生成10年的证书文件。\n参考：\n使用 kubeadm 进行证书管理 | Kubernetes\nKubernetes v1.25 编译 kubeadm 修改证书有效期到 100 年\n","categories":"","description":"","excerpt":"通过kubeadm搭建的集群默认的证书时间是1年（由于官方期望每年更新一次k8s的版本，在更新的时候会默认更新证书），当你执行命令出现以下报 …","ref":"/kubernetes-notes/setup/kubeadm-certs/","tags":["kubeadm"],"title":"kubeadm管理证书"},{"body":"问题描述 pvc terminating pvc在删除时，卡在terminating中。\n解决方法 kubectl patch pvc {PVC_NAME} -p '{\"metadata\":{\"finalizers\":null}}' ","categories":"","description":"","excerpt":"问题描述 pvc terminating pvc在删除时，卡在terminating中。\n解决方法 kubectl patch pvc …","ref":"/kubernetes-notes/trouble-shooting/pvc-terminating/","tags":["问题排查"],"title":"PVC Terminating"},{"body":" 以下为redis.conf的文件的中文描述，整理于网络\n# Redis 配置文件示例 # 注意单位: 当需要配置内存大小时, 可能需要指定像1k,5GB,4M等常见格式 # # 1k =\u003e 1000 bytes # 1kb =\u003e 1024 bytes # 1m =\u003e 1000000 bytes # 1mb =\u003e 1024*1024 bytes # 1g =\u003e 1000000000 bytes # 1gb =\u003e 1024*1024*1024 bytes # # 单位是对大小写不敏感的 1GB 1Gb 1gB 是相同的。 INCLUDES ################################## INCLUDES ################################### # 可以在这里包含一个或多个其他的配置文件。如果你有一个适用于所有Redis服务器的标准配置模板 # 但也需要一些每个服务器自定义的设置，这个功能将很有用。被包含的配置文件也可以包含其他配置文件， # 所以需要谨慎的使用这个功能。 # # 注意“inclue”选项不能被admin或Redis哨兵的\"CONFIG REWRITE\"命令重写。 # 因为Redis总是使用最后解析的配置行最为配置指令的值, 你最好在这个文件的开头配置includes来 # 避免它在运行时重写配置。 # 如果相反你想用includes的配置覆盖原来的配置，你最好在该文件的最后使用include # # include /path/to/local.conf # include /path/to/other.conf GENERAL ################################ GENERAL ##################################### # 默认Rdis不会作为守护进程运行。如果需要的话配置成'yes' # 注意配置成守护进程后Redis会将进程号写入文件/var/run/redis.pid daemonize no # 当以守护进程方式运行时，默认Redis会把进程ID写到 /var/run/redis.pid。你可以在这里修改路径。 pidfile /var/run/redis.pid # 接受连接的特定端口，默认是6379 # 如果端口设置为0，Redis就不会监听TCP套接字。 port 6379 # TCP listen() backlog. # # 在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。注意Linux内核默默地将这个值减小 # 到/proc/sys/net/core/somaxconn的值，所以需要确认增大somaxconn和tcp_max_syn_backlog # 两个值来达到想要的效果。 tcp-backlog 511 # 默认Redis监听服务器上所有可用网络接口的连接。可以用\"bind\"配置指令跟一个或多个ip地址来实现 # 监听一个或多个网络接口 # # 示例: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 # 指定用来监听Unix套套接字的路径。没有默认值， 所以在没有指定的情况下Redis不会监听Unix套接字 # # unixsocket /tmp/redis.sock # unixsocketperm 755 # 一个客户端空闲多少秒后关闭连接。(0代表禁用，永不关闭) timeout 0 # TCP keepalive. # # 如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACK，由于以下两个原因这是很有用的： # # 1）能够检测无响应的对端 # 2）让该连接中间的网络设备知道这个连接还存活 # # 在Linux上，这个指定的值(单位：秒)就是发送ACK的时间间隔。 # 注意：要关闭这个连接需要两倍的这个时间值。 # 在其他内核上这个时间间隔由内核配置决定 # # 这个选项的一个合理值是60秒 tcp-keepalive 0 # 指定服务器调试等级 # 可能值： # debug （大量信息，对开发/测试有用） # verbose （很多精简的有用信息，但是不像debug等级那么多） # notice （适量的信息，基本上是你生产环境中需要的） # warning （只有很重要/严重的信息会记录下来） loglevel notice # 指明日志文件名。也可以使用\"stdout\"来强制让Redis把日志信息写到标准输出上。 # 注意:如果Redis以守护进程方式运行，而设置日志显示到标准输出的话，日志会发送到/dev/null logfile \"\" # 要使用系统日志记录器，只要设置 \"syslog-enabled\" 为 \"yes\" 就可以了。 # 然后根据需要设置其他一些syslog参数就可以了。 # syslog-enabled no # 指明syslog身份 # syslog-ident redis # 指明syslog的设备。必须是user或LOCAL0 ~ LOCAL7之一。 # syslog-facility local0 # 设置数据库个数。默认数据库是 DB 0， # 可以通过select \u003cdbid\u003e (0 \u003c= dbid \u003c= 'databases' - 1 ）来为每个连接使用不同的数据库。 databases 16 SNAPSHOTTING ################################ SNAPSHOTTING ################################ # # 把数据库存到磁盘上: # # save \u003cseconds\u003e \u003cchanges\u003e # # 会在指定秒数和数据变化次数之后把数据库写到磁盘上。 # # 下面的例子将会进行把数据写入磁盘的操作: # 900秒（15分钟）之后，且至少1次变更 # 300秒（5分钟）之后，且至少10次变更 # 60秒之后，且至少10000次变更 # # 注意：你要想不写磁盘的话就把所有 \"save\" 设置注释掉就行了。 # # 通过添加一条带空字符串参数的save指令也能移除之前所有配置的save指令 # 像下面的例子： # save \"\" save 900 1 save 300 10 save 60 10000 # 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作 # 这将使用户知道数据没有正确的持久化到硬盘，否则可能没人注意到并且造成一些灾难。 # # 如果后台保存进程能重新开始工作，Redis将自动允许写操作 # # 然而如果你已经部署了适当的Redis服务器和持久化的监控，你可能想关掉这个功能以便于即使是 # 硬盘，权限等出问题了Redis也能够像平时一样正常工作， stop-writes-on-bgsave-error yes # 当导出到 .rdb 数据库时是否用LZF压缩字符串对象？ # 默认设置为 \"yes\"，因为几乎在任何情况下它都是不错的。 # 如果你想节省CPU的话你可以把这个设置为 \"no\"，但是如果你有可压缩的key和value的话， # 那数据文件就会更大了。 rdbcompression yes # 因为版本5的RDB有一个CRC64算法的校验和放在了文件的最后。这将使文件格式更加可靠但在 # 生产和加载RDB文件时，这有一个性能消耗(大约10%)，所以你可以关掉它来获取最好的性能。 # # 生成的关闭校验的RDB文件有一个0的校验和，它将告诉加载代码跳过检查 rdbchecksum yes # 持久化数据库的文件名 dbfilename dump.rdb # 工作目录 # # 数据库会写到这个目录下，文件名就是上面的 \"dbfilename\" 的值。 # # 累加文件也放这里。 # # 注意你这里指定的必须是目录，不是文件名。 dir ./ REPLICATION ################################# REPLICATION ################################# # 主从同步。通过 slaveof 指令来实现Redis实例的备份。 # 注意，这里是本地从远端复制数据。也就是说，本地可以有不同的数据库文件、绑定不同的IP、监听 # 不同的端口。 # # slaveof \u003cmasterip\u003e \u003cmasterport\u003e # 如果master设置了密码保护（通过 \"requirepass\" 选项来配置），那么slave在开始同步之前必须 # 进行身份验证，否则它的同步请求会被拒绝。 # # masterauth \u003cmaster-password\u003e # 当一个slave失去和master的连接，或者同步正在进行中，slave的行为有两种可能： # # 1) 如果 slave-serve-stale-data 设置为 \"yes\" (默认值)，slave会继续响应客户端请求， # 可能是正常数据，也可能是还没获得值的空数据。 # 2) 如果 slave-serve-stale-data 设置为 \"no\"，slave会回复\"正在从master同步 # （SYNC with master in progress）\"来处理各种请求，除了 INFO 和 SLAVEOF 命令。 # slave-serve-stale-data yes # 你可以配置salve实例是否接受写操作。可写的slave实例可能对存储临时数据比较有用(因为写入salve # 的数据在同master同步之后将很容被删除)，但是如果客户端由于配置错误在写入时也可能产生一些问题。 # # 从Redis2.6默认所有的slave为只读 # # 注意:只读的slave不是为了暴露给互联网上不可信的客户端而设计的。它只是一个防止实例误用的保护层。 # 一个只读的slave支持所有的管理命令比如config,debug等。为了限制你可以用'rename-command'来 # 隐藏所有的管理和危险命令来增强只读slave的安全性 slave-read-only yes # slave根据指定的时间间隔向master发送ping请求。 # 时间间隔可以通过 repl_ping_slave_period 来设置。 # 默认10秒。 # # repl-ping-slave-period 10 # 以下选项设置同步的超时时间 # # 1）slave在与master SYNC期间有大量数据传输，造成超时 # 2）在slave角度，master超时，包括数据、ping等 # 3）在master角度，slave超时，当master发送REPLCONF ACK pings # # 确保这个值大于指定的repl-ping-slave-period，否则在主从间流量不高时每次都会检测到超时 # # repl-timeout 60 # 是否在slave套接字发送SYNC之后禁用 TCP_NODELAY ？ # # 如果你选择“yes”Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave # 上有延迟，Linux内核的默认配置会达到40毫秒 # # 如果你选择了 \"no\" 数据传输到salve的延迟将会减少但要使用更多的带宽 # # 默认我们会为低延迟做优化，但高流量情况或主从之间的跳数过多时，把这个选项设置为“yes” # 是个不错的选择。 repl-disable-tcp-nodelay no # 设置数据备份的backlog大小。backlog是一个slave在一段时间内断开连接时记录salve数据的缓冲， # 所以一个slave在重新连接时，不必要全量的同步，而是一个增量同步就足够了，将在断开连接的这段 # 时间内slave丢失的部分数据传送给它。 # # 同步的backlog越大，slave能够进行增量同步并且允许断开连接的时间就越长。 # # backlog只分配一次并且至少需要一个slave连接 # # repl-backlog-size 1mb # 当master在一段时间内不再与任何slave连接，backlog将会释放。以下选项配置了从最后一个 # slave断开开始计时多少秒后，backlog缓冲将会释放。 # # 0表示永不释放backlog # # repl-backlog-ttl 3600 # slave的优先级是一个整数展示在Redis的Info输出中。如果master不再正常工作了，哨兵将用它来 # 选择一个slave提升=升为master。 # # 优先级数字小的salve会优先考虑提升为master，所以例如有三个slave优先级分别为10，100，25， # 哨兵将挑选优先级最小数字为10的slave。 # # 0作为一个特殊的优先级，标识这个slave不能作为master，所以一个优先级为0的slave永远不会被 # 哨兵挑选提升为master # # 默认优先级为100 slave-priority 100 # 如果master少于N个延时小于等于M秒的已连接slave，就可以停止接收写操作。 # # N个slave需要是“oneline”状态 # # 延时是以秒为单位，并且必须小于等于指定值，是从最后一个从slave接收到的ping（通常每秒发送） # 开始计数。 # # This option does not GUARANTEES that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough slaves # are available, to the specified number of seconds. # # 例如至少需要3个延时小于等于10秒的slave用下面的指令： # # min-slaves-to-write 3 # min-slaves-max-lag 10 # # 两者之一设置为0将禁用这个功能。 # # 默认 min-slaves-to-write 值是0（该功能禁用）并且 min-slaves-max-lag 值是10。 SECURITY ################################## SECURITY ################################### # 要求客户端在处理任何命令时都要验证身份和密码。 # 这个功能在有你不信任的其它客户端能够访问redis服务器的环境里非常有用。 # # 为了向后兼容的话这段应该注释掉。而且大多数人不需要身份验证(例如:它们运行在自己的服务器上) # # 警告：因为Redis太快了，所以外面的人可以尝试每秒150k的密码来试图破解密码。这意味着你需要 # 一个高强度的密码，否则破解太容易了。 # # requirepass foobared # 命令重命名 # # 在共享环境下，可以为危险命令改变名字。比如，你可以为 CONFIG 改个其他不太容易猜到的名字， # 这样内部的工具仍然可以使用，而普通的客户端将不行。 # # 例如： # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # 也可以通过改名为空字符串来完全禁用一个命令 # # rename-command CONFIG \"\" # # 请注意：改变命令名字被记录到AOF文件或被传送到从服务器可能产生问题。 LIMITS ################################### LIMITS #################################### # 设置最多同时连接的客户端数量。默认这个限制是10000个客户端，然而如果Redis服务器不能配置 # 处理文件的限制数来满足指定的值，那么最大的客户端连接数就被设置成当前文件限制数减32（因 # 为Redis服务器保留了一些文件描述符作为内部使用） # # 一旦达到这个限制，Redis会关闭所有新连接并发送错误'max number of clients reached' # # maxclients 10000 # 不要用比设置的上限更多的内存。一旦内存使用达到上限，Redis会根据选定的回收策略（参见： # maxmemmory-policy）删除key # # 如果因为删除策略Redis无法删除key，或者策略设置为 \"noeviction\"，Redis会回复需要更 # 多内存的错误信息给命令。例如，SET,LPUSH等等，但是会继续响应像Get这样的只读命令。 # # 在使用Redis作为LRU缓存，或者为实例设置了硬性内存限制的时候（使用 \"noeviction\" 策略） # 的时候，这个选项通常事很有用的。 # # 警告：当有多个slave连上达到内存上限的实例时，master为同步slave的输出缓冲区所需 # 内存不计算在使用内存中。这样当驱逐key时，就不会因网络问题 / 重新同步事件触发驱逐key # 的循环，反过来slaves的输出缓冲区充满了key被驱逐的DEL命令，这将触发删除更多的key， # 直到这个数据库完全被清空为止 # # 总之...如果你需要附加多个slave，建议你设置一个稍小maxmemory限制，这样系统就会有空闲 # 的内存作为slave的输出缓存区(但是如果最大内存策略设置为\"noeviction\"的话就没必要了) # # maxmemory \u003cbytes\u003e # 最大内存策略：如果达到内存限制了，Redis如何选择删除key。你可以在下面五个行为里选： # # volatile-lru -\u003e 根据LRU算法生成的过期时间来删除。 # allkeys-lru -\u003e 根据LRU算法删除任何key。 # volatile-random -\u003e 根据过期设置来随机删除key。 # allkeys-\u003erandom -\u003e 无差别随机删。 # volatile-ttl -\u003e 根据最近过期时间来删除（辅以TTL） # noeviction -\u003e 谁也不删，直接在写操作时返回错误。 # # 注意：对所有策略来说，如果Redis找不到合适的可以删除的key都会在写操作时返回一个错误。 # # 目前为止涉及的命令：set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # 默认值如下： # # maxmemory-policy volatile-lru # LRU和最小TTL算法的实现都不是很精确，但是很接近（为了省内存），所以你可以用样本量做检测。 # 例如：默认Redis会检查3个key然后取最旧的那个，你可以通过下面的配置指令来设置样本的个数。 # # maxmemory-samples 3 APPEND ONLY MODE ############################## APPEND ONLY MODE ############################### # 默认情况下，Redis是异步的把数据导出到磁盘上。这种模式在很多应用里已经足够好，但Redis进程 # 出问题或断电时可能造成一段时间的写操作丢失(这取决于配置的save指令)。 # # AOF是一种提供了更可靠的替代持久化模式，例如使用默认的数据写入文件策略（参见后面的配置） # 在遇到像服务器断电或单写情况下Redis自身进程出问题但操作系统仍正常运行等突发事件时，Redis # 能只丢失1秒的写操作。 # # AOF和RDB持久化能同时启动并且不会有问题。 # 如果AOF开启，那么在启动时Redis将加载AOF文件，它更能保证数据的可靠性。 # # 请查看 http://redis.io/topics/persistence 来获取更多信息. appendonly no # 纯累加文件名字（默认：\"appendonly.aof\"） appendfilename \"appendonly.aof\" # fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。 # 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。 # # Redis支持三种不同的模式： # # no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。 # always：每次写操作都立刻写入到aof文件。慢，但是最安全。 # everysec：每秒写一次。折中方案。 # # 默认的 \"everysec\" 通常来说能在速度和数据安全性之间取得比较好的平衡。根据你的理解来 # 决定，如果你能放宽该配置为\"no\" 来获取更好的性能(但如果你能忍受一些数据丢失，可以考虑使用 # 默认的快照持久化模式)，或者相反，用“always”会比较慢但比everysec要更安全。 # # 请查看下面的文章来获取更多的细节 # http://antirez.com/post/redis-persistence-demystified.html # # 如果不能确定，就用 \"everysec\" # appendfsync always appendfsync everysec # appendfsync no # 如果AOF的同步策略设置成 \"always\" 或者 \"everysec\"，并且后台的存储进程（后台存储或写入AOF # 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 fsync()系统调用而阻塞很久。 # 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。 # # 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止fsync()。 # # 这就意味着如果有子进程在进行保存操作，那么Redis就处于\"不可同步\"的状态。 # 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定） # # 如果把这个设置成\"yes\"带来了延迟问题，就保持\"no\"，这是保存持久数据的最安全的方式。 no-appendfsync-on-rewrite no # 自动重写AOF文件 # 如果AOF日志文件增大到指定百分比，Redis能够通过 BGREWRITEAOF 自动重写AOF日志文件。 # # 工作原理：Redis记住上次重写时AOF文件的大小（如果重启后还没有写操作，就直接用启动时的AOF大小） # # 这个基准大小和当前大小做比较。如果当前大小超过指定比例，就会触发重写操作。你还需要指定被重写 # 日志的最小尺寸，这样避免了达到指定百分比但尺寸仍然很小的情况还要重写。 # # 指定百分比为0会禁用AOF自动重写特性。 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb ################################ LUA SCRIPTING ############################### # Lua 脚本的最大执行时间，毫秒为单位 # # 如果达到了最大的执行时间，Redis将要记录在达到最大允许时间之后一个脚本仍然在执行，并且将 # 开始对查询进行错误响应。 # # 当一个长时间运行的脚本超过了最大执行时间，只有 SCRIPT KILL 和 SHUTDOWN NOSAVE 两个 # 命令可用。第一个可以用于停止一个还没有调用写命名的脚本。第二个是关闭服务器唯一方式，当 # 写命令已经通过脚本开始执行，并且用户不想等到脚本的自然终止。 # # 设置成0或者负值表示不限制执行时间并且没有任何警告 lua-time-limit 5000 SLOW LOG ################################## SLOW LOG ################################### # Redis慢查询日志可以记录超过指定时间的查询。运行时间不包括各种I/O时间，例如：连接客户端， # 发送响应数据等，而只计算命令执行的实际时间（这只是线程阻塞而无法同时为其他请求服务的命令执 # 行阶段） # # 你可以为慢查询日志配置两个参数:一个指明Redis的超时时间(单位为微秒)来记录超过这个时间的命令 # 另一个是慢查询日志长度。当一个新的命令被写进日志的时候，最老的那个记录从队列中移除。 # # 下面的时间单位是微秒，所以1000000就是1秒。注意，负数时间会禁用慢查询日志，而0则会强制记录 # 所有命令。 slowlog-log-slower-than 10000 # 这个长度没有限制。只是要主要会消耗内存。你可以通过 SLOWLOG RESET 来回收内存。 slowlog-max-len 128 Event notification ############################# Event notification ############################## # Redis 能通知 Pub/Sub 客户端关于键空间发生的事件 # 这个功能文档位于http://redis.io/topics/keyspace-events # # 例如：如果键空间事件通知被开启，并且客户端对 0 号数据库的键 foo 执行 DEL 命令时，将通过 # Pub/Sub发布两条消息： # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # 可以在下表中选择Redis要通知的事件类型。事件类型由单个字符来标识： # # K 键空间通知，以__keyspace@\u003cdb\u003e__为前缀 # E 键事件通知，以__keysevent@\u003cdb\u003e__为前缀 # g DEL , EXPIRE , RENAME 等类型无关的通用命令的通知, ... # $ String命令 # l List命令 # s Set命令 # h Hash命令 # z 有序集合命令 # x 过期事件（每次key过期时生成） # e 驱逐事件（当key在内存满了被清除时生成） # A g$lshzxe的别名，因此”AKE”意味着所有的事件 # # notify-keyspace-events 带一个由0到多个字符组成的字符串参数。空字符串意思是通知被禁用。 # # 例子：启用List和通用事件通知： # notify-keyspace-events Elg # # 例子2：为了获取过期key的通知订阅名字为 __keyevent@__:expired 的频道，用以下配置 # notify-keyspace-events Ex # # 默认所用的通知被禁用，因为用户通常不需要该特性，并且该特性会有性能损耗。 # 注意如果你不指定至少K或E之一，不会发送任何事件。 notify-keyspace-events \"\" ADVANCED CONFIG # 当hash只有少量的entry时，并且最大的entry所占空间没有超过指定的限制时，会用一种节省内存的 # 数据结构来编码。可以通过下面的指令来设定限制 hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # 与hash似，数据元素较少的list，可以用另一种方式来编码从而节省大量空间。 # 这种特殊的方式只有在符合下面限制时才可以用： list-max-ziplist-entries 512 list-max-ziplist-value 64 # set有一种特殊编码的情况：当set数据全是十进制64位有符号整型数字构成的字符串时。 # 下面这个配置项就是用来设置set使用这种编码来节省内存的最大长度。 set-max-intset-entries 512 # 与hash和list相似，有序集合也可以用一种特别的编码方式来节省大量空间。 # 这种编码只适合长度和元素都小于下面限制的有序集合： zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # 启用哈希刷新，每100个CPU毫秒会拿出1个毫秒来刷新Redis的主哈希表（顶级键值映射表）。 # redis所用的哈希表实现（见dict.c）采用延迟哈希刷新机制：你对一个哈希表操作越多，哈希刷新 # 操作就越频繁；反之，如果服务器是空闲的，那么哈希刷新就不会完成，哈希表就会占用更多的一些 # 内存而已。 # # 默认是每秒钟进行10次哈希表刷新，用来刷新字典，然后尽快释放内存。 # # 建议： # 如果你对延迟比较在意，不能够接受Redis时不时的对请求有2毫秒的延迟的话，就用 # \"activerehashing no\"，如果不太在意延迟而希望尽快释放内存就设置\"activerehashing yes\" activerehashing yes # 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器读取数据的速度不够快的客户端， # （一个常见的原因是一个发布/订阅客户端消费消息的速度无法赶上生产它们的速度） # # 可以对三种不同的客户端设置不同的限制： # normal -\u003e 正常客户端 # slave -\u003e slave和 MONITOR 客户端 # pubsub -\u003e 至少订阅了一个pubsub channel或pattern的客户端 # # 下面是每个client-output-buffer-limit语法: # client-output-buffer-limit \u003cclass\u003e\u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e # 一旦达到硬限制客户端会立即被断开，或者达到软限制并持续达到指定的秒数（连续的）。 # 例如，如果硬限制为32兆字节和软限制为16兆字节/10秒，客户端将会立即断开 # 如果输出缓冲区的大小达到32兆字节，或客户端达到16兆字节并连续超过了限制10秒，就将断开连接。 # # 默认normal客户端不做限制，因为他们在不主动请求时不接收数据（以推的方式），只有异步客户端 # 可能会出现请求数据的速度比它可以读取的速度快的场景。 # # pubsub和slave客户端会有一个默认值，因为订阅者和slaves以推的方式来接收数据 # # 把硬限制和软限制都设置为0来禁用该功能 client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Redis调用内部函数来执行许多后台任务，如关闭客户端超时的连接，清除未被请求过的过期Key等等。 # # 不是所有的任务都以相同的频率执行，但Redis依照指定的“hz”值来执行检查任务。 # # 默认情况下，“hz”的被设定为10。提高该值将在Redis空闲时使用更多的CPU时，但同时当有多个key # 同时到期会使Redis的反应更灵敏，以及超时可以更精确地处理。 # # 范围是1到500之间，但是值超过100通常不是一个好主意。 # 大多数用户应该使用10这个默认值，只有在非常低的延迟要求时有必要提高到100。 hz 10 # 当一个子进程重写AOF文件时，如果启用下面的选项，则文件每生成32M数据会被同步。为了增量式的 # 写入硬盘并且避免大的延迟高峰这个指令是非常有用的 aof-rewrite-incremental-fsync yes ","categories":"","description":"","excerpt":" 以下为redis.conf的文件的中文描述，整理于网络\n# Redis 配置文件示例 # 注意单位: 当需要配置内存大小时, 可能需要指定 …","ref":"/linux-notes/redis/redis-conf-cn/","tags":["Redis"],"title":"Redis配置详解（中文版）"},{"body":"问题描述 当使用runc 1.1.3的版本时，如果执行systemctl daemon-reload后，通过exec进入容器则会触发以下错误，无法进入容器。\nFATA[0000] execing command in container: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec \"33d05b4f71a2da69c8c77cc3f7e61451814eb150edd15d0a3153b57a862126d4\": OCI runtime exec failed: exec failed: unable to start container process: open /dev/pts/0: operation not permitted: unknown 原因 这个是runc 1.1.3版本起存在的一个bug，runc1.1.2的版本不存在，社区在runc 1.1.4的版本中修复了这个bug。由于 runc v1.1.3 中不再添加 DeviceAllow=char-pts rwm 规则了，当执行 systemctl daemon-reload 后， 会导致重新应用 systemd 的规则，进而导致这条规则的缺失。\n解决方案 升级runc到1.1.4的版本，并且所有通过runc1.1.3创建的pod都需要重建才能生效。\n参考：\nhttps://github.com/containerd/containerd/issues/7219#issuecomment-1225358826\nhttps://github.com/opencontainers/runc/issues/3551\nRelease runc 1.1.4 -- \"If you look for perfection, you'll never be content.\" · opencontainers/runc · GitHub\n","categories":"","description":"","excerpt":"问题描述 当使用runc 1.1.3的版本时，如果执行systemctl daemon-reload后，通过exec进入容器则会触发以下错 …","ref":"/kubernetes-notes/trouble-shooting/node/runc-1.1.3-exec-failed/","tags":["问题排查"],"title":"runc-v1.1.3-exec-failed"},{"body":"1. 字符串 字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。\n1.1. 单引号 str='this is a string' 单引号字符串的限制：\n单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单引号（对单引号使用转义符后也不行）。 1.2. 双引号 your_name='qinjx' str=\"Hello, I know your are \\\"$your_name\\\"! \\n\" 双引号的优点：\n双引号里可以有变量 双引号里可以出现转义字符 1.3. 拼接字符串 your_name=\"qinjx\" greeting=\"hello, \"$your_name\" !\" greeting_1=\"hello, ${your_name} !\" echo $greeting $greeting_1 1.4. 获取字符串长度 string=\"abcd\" echo ${#string} #输出 4 1.5. 提取子字符串 string=\"alibaba is a great company\" echo ${string:1:4} #输出liba 1.6. 查找子字符串 string=\"alibaba is a great company\" echo `expr index \"$string\" is` 2. 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。\n2.1. 定义数组 在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。\n定义数组的一般形式为：\narray_name=(value1 ... valuen) 例如：\narray_name=(value0 value1 value2 value3) 或者\narray_name=( value0 value1 value2 value3 ) 还可以单独定义数组的各个分量：\narray_name[0]=value0 array_name[1]=value1 array_name[2]=value2 可以不使用连续的下标，而且下标的范围没有限制。\n2.2. 读取数组 读取数组元素值的一般格式是：\n${array_name[index]} 例如：\nvaluen=${array_name[2]} 使用@ 或 * 可以获取数组中的所有元素，例如：\n${array_name[*]} ${array_name[@]} 2.3. 获取数组的长度 获取数组长度的方法与获取字符串长度的方法相同，例如：\n# 取得数组元素的个数 length=${#array_name[@]} # 或者 length=${#array_name[*]} # 取得数组单个元素的长度 lengthn=${#array_name[n]} 参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. 字符串 字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号， …","ref":"/linux-notes/shell/shell-array/","tags":["Shell"],"title":"Shell数组"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析startKubelet，其中主要是kubelet.Run部分，该部分的内容主要是初始化并运行一些manager。对于kubelet所包含的各种manager的执行逻辑和pod的生命周期管理逻辑待后续文章分析。\n后续的文章主要会分类分析pkg/kubelet部分的代码实现。\nkubelet的pkg代码目录结构：\nkubelet ├── apis # 定义一些相关接口 ├── cadvisor # cadvisor ├── cm # ContainerManager、cpu manger、cgroup manager ├── config ├── configmap # configmap manager ├── container # Runtime、ImageService ├── dockershim # docker的相关调用 ├── eviction # eviction manager ├── images # image manager ├── kubeletconfig ├── kuberuntime # 核心：kubeGenericRuntimeManager、runtime容器的相关操作 ├── lifecycle ├── mountpod ├── network # pod dns ├── nodelease ├── nodestatus # MachineInfo、节点相关信息 ├── pleg # PodLifecycleEventGenerator ├── pod # 核心：pod manager、mirror pod ├── preemption ├── qos # 资源服务质量，不过暂时内容很少 ├── remote # RemoteRuntimeService ├── server ├── stats # StatsProvider ├── status # status manager ├── types # PodUpdate、PodOperation ├── volumemanager # VolumeManager ├── kubelet.go # 核心: SyncHandler、kubelet的大部分操作 ├── kubelet_getters.go # 各种get操作，例如获取相关目录：getRootDir、getPodsDir、getPluginsDir ├── kubelet_network.go # ├── kubelet_network_linux.go ├── kubelet_node_status.go # registerWithAPIServer、initialNode、syncNodeStatus ├── kubelet_pods.go # 核心：pod的增删改查等相关操作、podKiller、 ├── kubelet_resources.go ├── kubelet_volumes.go # ListVolumesForPod、cleanupOrphanedPodDirs ├── oom_watcher.go # OOMWatcher ├── pod_container_deletor.go ├── pod_workers.go # 核心：PodWorkers、UpdatePodOptions、syncPodOptions、managePodLoop ├── runonce.go # RunOnce ├── runtime.go ... 1. startKubelet startKubelet的函数位于cmd/kubelet/app/server.go，启动并运行一个kubelet，运行kubelet的逻辑代码位于pkg/kubelet/kubelet.go。\n主要内容如下：\n运行一个kubelet，执行kubelet中各种manager的相关逻辑。 运行kubelet server启动监听服务。 此部分代码位于cmd/kubelet/app/server.go\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) // start the kubelet server if enableServer { go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) } if kubeCfg.ReadOnlyPort \u003e 0 { go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } } 2. Kubelet.Run Kubelet.Run方法主要将NewMainKubelet构造的各种manager运行起来，让各种manager执行相应的功能，大部分manager为常驻进程的方式运行。\nKubelet.Run完整代码如下：\n此部分代码位于pkg/kubelet/kubelet.go\n// Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u003c-chan kubetypes.PodUpdate) { if kl.logServer == nil { kl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\"))) } if kl.kubeClient == nil { glog.Warning(\"No api server defined - no node status update will be sent.\") } // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) glog.Fatal(err) } // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil { // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { go kl.nodeLeaseController.Run(wait.NeverStop) } } go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Start loop to sync iptables util rules if kl.makeIPTablesUtilChains { go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) } // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) // Start component sync loops. kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { go kl.runtimeClassManager.Run(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl) } 以下对Kubelet.Run分段进行分析。\n3. initializeModules initializeModules包含了imageManager、serverCertificateManager、oomWatcher和resourceAnalyzer。\n主要流程如下：\n创建文件系统目录，包括kubelet的root目录、pods的目录、plugins的目录和容器日志目录。 启动imageManager、serverCertificateManager、oomWatcher、resourceAnalyzer。 各种manager的说明如下：\nimageManager：负责镜像垃圾回收。 serverCertificateManager：负责处理证书。 oomWatcher：监控内存使用，是否发生内存耗尽。 resourceAnalyzer：监控资源使用情况。 完整代码如下：\n此部分代码位于pkg/kubelet/kubelet.go\n// initializeModules will initialize internal modules that do not require the container runtime to be up. // Note that the modules here must not depend on modules that are not initialized here. func (kl *Kubelet) initializeModules() error { // Prometheus metrics. metrics.Register(kl.runtimeCache, collectors.NewVolumeStatsCollector(kl)) // Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { glog.Errorf(\"Failed to create directory %q: %v\", ContainerLogsDir, err) } } // Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } // Start out of memory watcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\"Failed to start OOM watcher %v\", err) } // Start resource analyzer kl.resourceAnalyzer.Start() return nil } 3.1. setupDataDirs initializeModules先创建相关目录。\n具体目录如下：\nContainerLogsDir：目录为/var/log/containers。 rootDirectory：由参数传入，一般为/var/lib/kubelet。 PodsDir：目录为{rootDirectory}/pods。 PluginsDir：目录为{rootDirectory}/plugins。 initializeModules中setupDataDirs的相关代码如下：\n// Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { glog.Errorf(\"Failed to create directory %q: %v\", ContainerLogsDir, err) } } setupDataDirs代码如下\n// setupDataDirs creates: // 1. the root directory // 2. the pods directory // 3. the plugins directory func (kl *Kubelet) setupDataDirs() error { kl.rootDirectory = path.Clean(kl.rootDirectory) if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil { return fmt.Errorf(\"error creating root directory: %v\", err) } if err := kl.mounter.MakeRShared(kl.getRootDir()); err != nil { return fmt.Errorf(\"error configuring root directory: %v\", err) } if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil { return fmt.Errorf(\"error creating pods directory: %v\", err) } if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil { return fmt.Errorf(\"error creating plugins directory: %v\", err) } return nil } 3.2. manager initializeModules中的manager如下：\n// Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } // Start out of memory watcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\"Failed to start OOM watcher %v\", err) } // Start resource analyzer kl.resourceAnalyzer.Start() 4. 运行各种manager 4.1. volumeManager volumeManager主要运行一组异步循环，根据在此节点上安排的pod调整哪些volume需要attached/detached/mounted/unmounted。\n// Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) volumeManager.Run实现代码如下：\nfunc (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) { defer runtime.HandleCrash() go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh) glog.V(2).Infof(\"The desired_state_of_world populator starts\") glog.Infof(\"Starting Kubelet Volume Manager\") go vm.reconciler.Run(stopCh) metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr) \u003c-stopCh glog.Infof(\"Shutting down Kubelet Volume Manager\") } 4.2. syncNodeStatus syncNodeStatus通过goroutine的方式定期执行，它将节点的状态同步给master，必要的时候注册kubelet。\nif kl.kubeClient != nil { // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { go kl.nodeLeaseController.Run(wait.NeverStop) } } 4.3. updateRuntimeUp updateRuntimeUp调用容器运行时状态回调，在容器运行时首次启动时初始化运行时相关模块，如果状态检查失败则返回错误。 如果状态检查正常，在kubelet runtimeState中更新容器运行时的正常运行时间。\ngo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) 4.4. syncNetworkUtil 通过循环的方式同步iptables的规则，不过当前代码并没有执行任何操作。\n// Start loop to sync iptables util rules if kl.makeIPTablesUtilChains { go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) } 4.5. podKiller 但pod没有被podworker正确处理的时候，启动一个goroutine负责杀死pod。\n// Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) podKiller代码如下：\n此部分代码位于pkg/kubelet/kubelet_pods.go\n// podKiller launches a goroutine to kill a pod received from the channel if // another goroutine isn't already in action. func (kl *Kubelet) podKiller() { killing := sets.NewString() // guard for the killing set lock := sync.Mutex{} for podPair := range kl.podKillingCh { runningPod := podPair.RunningPod apiPod := podPair.APIPod lock.Lock() exists := killing.Has(string(runningPod.ID)) if !exists { killing.Insert(string(runningPod.ID)) } lock.Unlock() if !exists { go func(apiPod *v1.Pod, runningPod *kubecontainer.Pod) { glog.V(2).Infof(\"Killing unwanted pod %q\", runningPod.Name) err := kl.killPod(apiPod, runningPod, nil, nil) if err != nil { glog.Errorf(\"Failed killing the pod %q: %v\", runningPod.Name, err) } lock.Lock() killing.Delete(string(runningPod.ID)) lock.Unlock() }(apiPod, runningPod) } } } 4.6. statusManager 使用apiserver同步pods状态; 也用作状态缓存。\n// Start component sync loops. kl.statusManager.Start() statusManager.Start的实现代码如下：\nfunc (m *manager) Start() { // Don't start the status manager if we don't have a client. This will happen // on the master, where the kubelet is responsible for bootstrapping the pods // of the master components. if m.kubeClient == nil { glog.Infof(\"Kubernetes client is nil, not starting status manager.\") return } glog.Info(\"Starting to sync pod status with apiserver\") syncTicker := time.Tick(syncPeriod) // syncPod and syncBatch share the same go routine to avoid sync races. go wait.Forever(func() { select { case syncRequest := \u003c-m.podStatusChannel: glog.V(5).Infof(\"Status Manager: syncing pod: %q, with status: (%d, %v) from podStatusChannel\", syncRequest.podUID, syncRequest.status.version, syncRequest.status.status) m.syncPod(syncRequest.podUID, syncRequest.status) case \u003c-syncTicker: m.syncBatch() } }, 0) } 4.7. probeManager 处理容器探针\nkl.probeManager.Start() 4.8. runtimeClassManager // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { go kl.runtimeClassManager.Run(wait.NeverStop) } 4.9. PodLifecycleEventGenerator // Start the pod lifecycle event generator. kl.pleg.Start() PodLifecycleEventGenerator是一个pod生命周期时间生成器接口，具体如下：\n// PodLifecycleEventGenerator contains functions for generating pod life cycle events. type PodLifecycleEventGenerator interface { Start() Watch() chan *PodLifecycleEvent Healthy() (bool, error) } start方法具体实现如下：\n// Start spawns a goroutine to relist periodically. func (g *GenericPLEG) Start() { go wait.Until(g.relist, g.relistPeriod, wait.NeverStop) } 4.10. syncLoop 最后调用syncLoop来执行同步变化变更的循环。\nkl.syncLoop(updates, kl) 5. syncLoop syncLoop是处理变化的循环。 它监听来自三种channel（file，apiserver和http）的更改。 对于看到的任何新更改，将针对所需状态和运行状态运行同步。 如果没有看到配置的变化，将在每个同步频率秒同步最后已知的所需状态。\n// syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates \u003c-chan kubetypes.PodUpdate, handler SyncHandler) { glog.Info(\"Starting kubelet main sync loop.\") // The resyncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync'd. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 { glog.Infof(\"skipping pod synchronization - %v\", rs) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } 其中调用了syncLoopIteration的函数来执行更具体的监控pod变化的循环。syncLoopIteration代码逻辑待后续单独分析。\n6. 总结 6.1. 基本流程 Kubelet.Run主要流程如下：\n初始化模块，其实就是运行imageManager、serverCertificateManager、oomWatcher、resourceAnalyzer。 运行各种manager，大部分以常驻goroutine的方式运行，其中包括volumeManager、statusManager等。 执行处理变更的循环函数syncLoop，对pod的生命周期进行管理。 syncLoop：\nsyncLoop函数，对pod的生命周期进行管理，其中syncLoop调用了syncLoopIteration函数，该函数根据podUpdate的信息，针对不同的操作，由SyncHandler来执行pod的增删改查等生命周期的管理，其中的syncHandler包括HandlePodSyncs和HandlePodCleanups等。该部分逻辑待后续文章具体分析。\n6.2. Manager 以下介绍kubelet运行时涉及到的manager的内容。\nmanager 说明 imageManager 负责镜像垃圾回收 serverCertificateManager 负责处理证书 oomWatcher 监控内存使用，是否发生内存耗尽即OOM resourceAnalyzer 监控资源使用情况 volumeManager 对pod执行attached/detached/mounted/unmounted操作 statusManager 使用apiserver同步pods状态; 也用作状态缓存 probeManager 处理容器探针 runtimeClassManager 同步RuntimeClasses podKiller 负责杀死pod 参考文章：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kubelet/app/server.go\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go\n","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析startKubelet，其中主要是kubelet.Run部 …","ref":"/k8s-source-code-analysis/kubelet/startkubelet/","tags":["源码分析"],"title":"kubelet源码分析（三）之 RunKubelet"},{"body":"1. StorageClass概述 StorageClass提供了一种描述存储类（class）的方法，不同的class可能会映射到不同的服务质量等级和备份策略或其他策略等。\nStorageClass 对象中包含 provisioner、parameters 和 reclaimPolicy 字段，当需要动态分配 PersistentVolume 时会使用到。当创建 StorageClass 对象时，设置名称和其他参数，一旦创建了对象就不能再对其更新。也可以为没有申请绑定到特定 class 的 PVC 指定一个默认的 StorageClass 。\nStorageClass对象文件\nkind: StorageClass apiVersion: storage.k8s.io/v3 metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug 2. StorageClass的属性 2.1. Provisioner（存储分配器） Storage class 有一个分配器（provisioner），用来决定使用哪个卷插件分配 PV，该字段必须指定。可以指定内部分配器，也可以指定外部分配器。外部分配器的代码地址为： kubernetes-incubator/external-storage，其中包括NFS和Ceph等。\n2.2. Reclaim Policy（回收策略） 可以通过reclaimPolicy字段指定创建的Persistent Volume的回收策略，回收策略包括：Delete 或者 Retain，没有指定默认为Delete。\n2.3. Mount Options（挂载选项） 由 storage class 动态创建的 Persistent Volume 将使用 class 中 mountOptions 字段指定的挂载选项。\n2.4. 参数 Storage class 具有描述属于 storage class 卷的参数。取决于分配器，可以接受不同的参数。 当参数被省略时，会使用默认值。\n例如以下使用Ceph RBD\nkind: StorageClass apiVersion: storage.k8s.io/v3 metadata: name: fast provisioner: kubernetes.io/rbd parameters: monitors: 30.36.353.305:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" 对应的参数说明\nmonitors：Ceph monitor，逗号分隔。该参数是必需的。\nadminId：Ceph 客户端 ID，用于在池（ceph pool）中创建映像。 默认是 “admin”。\nadminSecretNamespace：adminSecret 的 namespace。默认是 “default”。\nadminSecret：adminId 的 Secret 名称。该参数是必需的。 提供的 secret 必须有值为 “kubernetes.io/rbd” 的 type 参数。\npool: Ceph RBD 池. 默认是 “rbd”。\nuserId：Ceph 客户端 ID，用于映射 RBD 镜像（RBD image）。默认与 adminId 相同。\nuserSecretName：用于映射 RBD 镜像的 userId 的 Ceph Secret 的名字。 它必须与 PVC 存在于相同的 namespace 中。该参数是必需的。 提供的 secret 必须具有值为 “kubernetes.io/rbd” 的 type 参数，例如以这样的方式创建：\nkubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" \\ --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \\ --namespace=kube-system fsType：Kubernetes 支持的 fsType。默认：\"ext4\"。\nimageFormat：Ceph RBD 镜像格式，”1” 或者 “2”。默认值是 “1”。\nimageFeatures：这个参数是可选的，只能在你将 imageFormat 设置为 “2” 才使用。 目前支持的功能只是 layering。 默认是 ““，没有功能打开。\n参考文章：\nhttps://kubernetes.io/docs/concepts/storage/storage-classes/ ","categories":"","description":"","excerpt":"1. StorageClass概述 StorageClass提供了一种描述存储类（class）的方法，不同的class可能会映射到不同的服务 …","ref":"/kubernetes-notes/storage/volume/storage-class/","tags":["Kubernetes"],"title":"StorageClass 介绍"},{"body":"scheduleOne 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析/pkg/scheduler/中调度的基本流程。具体的预选调度逻辑、优选调度逻辑、节点抢占逻辑待后续再独立分析。\nscheduler的pkg代码目录结构如下：\nscheduler ├── algorithm # 主要包含调度的算法 │ ├── predicates # 预选的策略 │ ├── priorities # 优选的策略 │ ├── scheduler_interface.go # ScheduleAlgorithm、SchedulerExtender接口定义 │ ├── types.go # 使用到的type的定义 ├── algorithmprovider │ ├── defaults │ │ ├── defaults.go # 默认算法的初始化操作，包括预选和优选策略 ├── cache # scheduler调度使用到的cache │ ├── cache.go # schedulerCache │ ├── interface.go │ ├── node_info.go │ ├── node_tree.go ├── core # 调度逻辑的核心代码 │ ├── equivalence │ │ ├── eqivalence.go # 存储相同pod的调度结果缓存，主要给预选策略使用 │ ├── extender.go │ ├── generic_scheduler.go # genericScheduler,主要包含默认调度器的调度逻辑 │ ├── scheduling_queue.go # 调度使用到的队列，主要用来存储需要被调度的pod ├── factory │ ├── factory.go # 主要包括NewConfigFactory、NewPodInformer，监听pod事件来更新调度队列 ├── metrics │ └── metrics.go # 主要给prometheus使用 ├── scheduler.go # pkg部分的Run入口(核心代码)，主要包含Run、scheduleOne、schedule、preempt等函数 └── volumebinder └── volume_binder.go # volume bind 1. Scheduler.Run 此部分代码位于pkg/scheduler/scheduler.go\n此处为具体调度逻辑的入口。\n// Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 2. Scheduler.scheduleOne 此部分代码位于pkg/scheduler/scheduler.go\nscheduleOne主要为单个pod选择一个适合的节点，为调度逻辑的核心函数。\n对单个pod进行调度的基本流程如下：\n通过podQueue的待调度队列中弹出需要调度的pod。 通过具体的调度算法为该pod选出合适的节点，其中调度算法就包括预选和优选两步策略。 如果上述调度失败，则会尝试抢占机制，将优先级低的pod剔除，让优先级高的pod调度成功。 将该pod和选定的节点进行假性绑定，存入scheduler cache中，方便具体绑定操作可以异步进行。 实际执行绑定操作，将node的名字添加到pod的节点相关属性中。 完整代码如下：\n// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. func (sched *Scheduler) scheduleOne() { pod := sched.config.NextPod() if pod.DeletionTimestamp != nil { sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) glog.V(3).Infof(\"Skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) return } glog.V(3).Infof(\"Attempting to schedule pod: %v/%v\", pod.Namespace, pod.Name) // Synchronously attempt to find a fit for the pod. start := time.Now() suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInMicroseconds(start)) // Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPod := pod.DeepCopy() // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then allBound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumedPod' if volume binding is required. allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) if err != nil { return } // assume modifies `assumedPod` by setting NodeName=suggestedHost err = sched.assume(assumedPod, suggestedHost) if err != nil { return } // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod if !allBound { err = sched.bindVolumes(assumedPod) if err != nil { return } } err := sched.bind(assumedPod, \u0026v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) metrics.E2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start)) if err != nil { glog.Errorf(\"Internal error binding pod: (%v)\", err) } }() } 以下对重要代码分别进行分析。\n3. config.NextPod 通过podQueue的方式存储待调度的pod队列，NextPod拿出下一个需要被调度的pod。\npod := sched.config.NextPod() if pod.DeletionTimestamp != nil { sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) glog.V(3).Infof(\"Skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) return } glog.V(3).Infof(\"Attempting to schedule pod: %v/%v\", pod.Namespace, pod.Name) NextPod的具体函数在factory.go的CreateFromKey函数中定义，如下：\nfunc (c *configFactory) CreateFromKeys(predicateKeys, priorityKeys sets.String, extenders []algorithm.SchedulerExtender) (*scheduler.Config, error) { ... return \u0026scheduler.Config{ ... NextPod: func() *v1.Pod { return c.getNextPod() } ... } 3.1. getNextPod 通过一个podQueue来存储需要调度的pod的队列，通过队列Pop的方式弹出需要被调度的pod。\nfunc (c *configFactory) getNextPod() *v1.Pod { pod, err := c.podQueue.Pop() if err == nil { glog.V(4).Infof(\"About to try and schedule pod %v/%v\", pod.Namespace, pod.Name) return pod } glog.Errorf(\"Error while retrieving next pod from scheduling queue: %v\", err) return nil } 4. Scheduler.schedule 此部分代码位于pkg/scheduler/scheduler.go\n此部分为调度逻辑的核心，通过不同的算法为具体的pod选择一个最合适的节点。\n// Synchronously attempt to find a fit for the pod. start := time.Now() suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } schedule通过调度算法返回一个最优的节点。\n// schedule implements the scheduling algorithm and returns the suggested host. func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) { host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil { pod = pod.DeepCopy() sched.config.Error(pod, err) sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"%v\", err) sched.config.PodConditionUpdater.Update(pod, \u0026v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: v1.PodReasonUnschedulable, Message: err.Error(), }) return \"\", err } return host, err } 4.1. ScheduleAlgorithm ScheduleAlgorithm是一个调度算法的接口，主要的实现体是genericScheduler，后续分析genericScheduler.Schedule。\nScheduleAlgorithm接口定义如下：\n// ScheduleAlgorithm is an interface implemented by things that know how to schedule pods // onto machines. type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt receives scheduling errors for a pod and tries to create room for // the pod by preempting lower priority pods if possible. // It returns the node where preemption happened, a list of preempted pods, a // list of pods whose nominated node name should be removed, and error if any. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) // Predicates() returns a pointer to a map of predicate functions. This is // exposed for testing. Predicates() map[string]FitPredicate // Prioritizers returns a slice of priority config. This is exposed for // testing. Prioritizers() []PriorityConfig } 5. genericScheduler.Schedule 此部分代码位于/pkg/scheduler/core/generic_scheduler.go\ngenericScheduler.Schedule实现了基本的调度逻辑，基于给定需要调度的pod和node列表，如果执行成功返回调度的节点的名字，如果执行失败，则返回错误和原因。主要通过预选和优选两步操作完成调度的逻辑。\n基本流程如下：\n对pod做基本性检查，目前主要是对pvc的检查。 通过findNodesThatFit预选策略选出满足调度条件的node列表。 通过PrioritizeNodes优选策略给预选的node列表中的node进行打分。 在打分的node列表中选择一个分数最高的node作为调度的节点。 完整代码如下：\n// Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { trace := utiltrace.New(fmt.Sprintf(\"Scheduling %s/%s\", pod.Namespace, pod.Name)) defer trace.LogIfLong(100 * time.Millisecond) if err := podPassesBasicChecks(pod, g.pvcLister); err != nil { return \"\", err } nodes, err := nodeLister.List() if err != nil { return \"\", err } if len(nodes) == 0 { return \"\", ErrNoNodesAvailable } // Used for all fit and priority funcs. err = g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return \"\", err } trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", \u0026FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) trace.Step(\"Selecting host\") return g.selectHost(priorityList) } 5.1. podPassesBasicChecks podPassesBasicChecks主要做一下基本性检查，目前主要是对pvc的检查。\nif err := podPassesBasicChecks(pod, g.pvcLister); err != nil { return \"\", err } podPassesBasicChecks具体实现如下：\n// podPassesBasicChecks makes sanity checks on the pod if it can be scheduled. func podPassesBasicChecks(pod *v1.Pod, pvcLister corelisters.PersistentVolumeClaimLister) error { // Check PVCs used by the pod namespace := pod.Namespace manifest := \u0026(pod.Spec) for i := range manifest.Volumes { volume := \u0026manifest.Volumes[i] if volume.PersistentVolumeClaim == nil { // Volume is not a PVC, ignore continue } pvcName := volume.PersistentVolumeClaim.ClaimName pvc, err := pvcLister.PersistentVolumeClaims(namespace).Get(pvcName) if err != nil { // The error has already enough context (\"persistentvolumeclaim \"myclaim\" not found\") return err } if pvc.DeletionTimestamp != nil { return fmt.Errorf(\"persistentvolumeclaim %q is being deleted\", pvc.Name) } } return nil } 5.2. findNodesThatFit 预选，通过预选函数来判断每个节点是否适合被该Pod调度。\n具体的findNodesThatFit代码实现细节待后续文章独立分析。\ngenericScheduler.Schedule中对findNodesThatFit的调用过程如下：\ntrace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", \u0026FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) 5.3. PrioritizeNodes 优选，从满足的节点中选择出最优的节点。\n具体操作如下：\nPrioritizeNodes通过并行运行各个优先级函数来对节点进行优先级排序。 每个优先级函数会给节点打分，打分范围为0-10分。 0 表示优先级最低的节点，10表示优先级最高的节点。 每个优先级函数也有各自的权重。 优先级函数返回的节点分数乘以权重以获得加权分数。 最后组合（添加）所有分数以获得所有节点的总加权分数。 具体PrioritizeNodes的实现逻辑待后续文章独立分析。\ngenericScheduler.Schedule中对PrioritizeNodes的调用过程如下：\ntrace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) 5.4. selectHost scheduler在最后会从priorityList中选择分数最高的一个节点。\ntrace.Step(\"Selecting host\") return g.selectHost(priorityList) selectHost获取优先级的节点列表，然后从分数最高的节点以循环方式选择一个节点。\n具体代码如下：\n// selectHost takes a prioritized list of nodes and then picks one // in a round-robin manner from the nodes that had the highest score. func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) { if len(priorityList) == 0 { return \"\", fmt.Errorf(\"empty priorityList\") } maxScores := findMaxScores(priorityList) ix := int(g.lastNodeIndex % uint64(len(maxScores))) g.lastNodeIndex++ return priorityList[maxScores[ix]].Host, nil } 5.4.1. findMaxScores findMaxScores返回priorityList中具有最高Score的节点的索引。\n// findMaxScores returns the indexes of nodes in the \"priorityList\" that has the highest \"Score\". func findMaxScores(priorityList schedulerapi.HostPriorityList) []int { maxScoreIndexes := make([]int, 0, len(priorityList)/2) maxScore := priorityList[0].Score for i, hp := range priorityList { if hp.Score \u003e maxScore { maxScore = hp.Score maxScoreIndexes = maxScoreIndexes[:0] maxScoreIndexes = append(maxScoreIndexes, i) } else if hp.Score == maxScore { maxScoreIndexes = append(maxScoreIndexes, i) } } return maxScoreIndexes } 6. Scheduler.preempt 如果pod在预选和优选调度中失败，则执行抢占操作。抢占主要是将低优先级的pod的资源空间腾出给待调度的高优先级的pod。\n具体Scheduler.preempt的实现逻辑待后续文章独立分析。\nsuggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } 7. Scheduler.assume 将该pod和选定的节点进行假性绑定，存入scheduler cache中，方便可以继续执行调度逻辑，而不需要等待绑定操作的发生，具体绑定操作可以异步进行。\n// Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPod := pod.DeepCopy() // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then allBound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumedPod' if volume binding is required. allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) if err != nil { return } // assume modifies `assumedPod` by setting NodeName=suggestedHost err = sched.assume(assumedPod, suggestedHost) if err != nil { return } 如果假性绑定成功则发送请求给apiserver，如果失败则scheduler会立即释放已分配给假性绑定的pod的资源。\nassume方法的具体实现：\n// assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous. // assume modifies `assumed`. func (sched *Scheduler) assume(assumed *v1.Pod, host string) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed.Spec.NodeName = host // NOTE: Because the scheduler uses snapshots of SchedulerCache and the live // version of Ecache, updates must be written to SchedulerCache before // invalidating Ecache. if err := sched.config.SchedulerCache.AssumePod(assumed); err != nil { glog.Errorf(\"scheduler cache AssumePod failed: %v\", err) // This is most probably result of a BUG in retrying logic. // We report an error here so that pod scheduling can be retried. // This relies on the fact that Error will check if the pod has been bound // to a node and if so will not add it back to the unscheduled pods queue // (otherwise this would cause an infinite loop). sched.config.Error(assumed, err) sched.config.Recorder.Eventf(assumed, v1.EventTypeWarning, \"FailedScheduling\", \"AssumePod failed: %v\", err) sched.config.PodConditionUpdater.Update(assumed, \u0026v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: \"SchedulerError\", Message: err.Error(), }) return err } // Optimistically assume that the binding will succeed, so we need to invalidate affected // predicates in equivalence cache. // If the binding fails, these invalidated item will not break anything. if sched.config.Ecache != nil { sched.config.Ecache.InvalidateCachedPredicateItemForPodAdd(assumed, host) } return nil } 8. Scheduler.bind 异步的方式给pod绑定到具体的调度节点上。\n// bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod if !allBound { err = sched.bindVolumes(assumedPod) if err != nil { return } } err := sched.bind(assumedPod, \u0026v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) metrics.E2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start)) if err != nil { glog.Errorf(\"Internal error binding pod: (%v)\", err) } }() bind具体实现如下：\n// bind binds a pod to a given node defined in a binding object. We expect this to run asynchronously, so we // handle binding metrics internally. func (sched *Scheduler) bind(assumed *v1.Pod, b *v1.Binding) error { bindingStart := time.Now() // If binding succeeded then PodScheduled condition will be updated in apiserver so that // it's atomic with setting host. err := sched.config.GetBinder(assumed).Bind(b) if err := sched.config.SchedulerCache.FinishBinding(assumed); err != nil { glog.Errorf(\"scheduler cache FinishBinding failed: %v\", err) } if err != nil { glog.V(1).Infof(\"Failed to bind pod: %v/%v\", assumed.Namespace, assumed.Name) if err := sched.config.SchedulerCache.ForgetPod(assumed); err != nil { glog.Errorf(\"scheduler cache ForgetPod failed: %v\", err) } sched.config.Error(assumed, err) sched.config.Recorder.Eventf(assumed, v1.EventTypeWarning, \"FailedScheduling\", \"Binding rejected: %v\", err) sched.config.PodConditionUpdater.Update(assumed, \u0026v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: \"BindingRejected\", }) return err } metrics.BindingLatency.Observe(metrics.SinceInMicroseconds(bindingStart)) metrics.SchedulingLatency.WithLabelValues(metrics.Binding).Observe(metrics.SinceInSeconds(bindingStart)) sched.config.Recorder.Eventf(assumed, v1.EventTypeNormal, \"Scheduled\", \"Successfully assigned %v/%v to %v\", assumed.Namespace, assumed.Name, b.Target.Name) return nil } 9. 总结 本文主要分析了单个pod的调度过程。具体流程如下：\n通过podQueue的待调度队列中弹出需要调度的pod。 通过具体的调度算法为该pod选出合适的节点，其中调度算法就包括预选和优选两步策略。 如果上述调度失败，则会尝试抢占机制，将优先级低的pod剔除，让优先级高的pod调度成功。 将该pod和选定的节点进行假性绑定，存入scheduler cache中，方便具体绑定操作可以异步进行。 实际执行绑定操作，将node的名字添加到pod的节点相关属性中。 其中核心的部分为通过具体的调度算法选出调度节点的过程，即genericScheduler.Schedule的实现部分。该部分包括预选和优选两个部分。\ngenericScheduler.Schedule调度的基本流程如下：\n对pod做基本性检查，目前主要是对pvc的检查。 通过findNodesThatFit预选策略选出满足调度条件的node列表。 通过PrioritizeNodes优选策略给预选的node列表中的node进行打分。 在打分的node列表中选择一个分数最高的node作为调度的节点。 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go ","categories":"","description":"","excerpt":"scheduleOne 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析/pkg/scheduler/中调度的基 …","ref":"/k8s-source-code-analysis/kube-scheduler/scheduleone/","tags":["源码分析"],"title":"kube-scheduler源码分析（三）之 调度流程"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/principle/","tags":"","title":"核心原理"},{"body":"后端开发技能树 图片来源于网络\n参考：\nhttps://github.com/xingshaocheng/architect-awesome ","categories":"","description":"","excerpt":"后端开发技能树 图片来源于网络\n参考：\nhttps://github.com/xingshaocheng/architect-awesome …","ref":"/golang-notes/summary/skill-tree/","tags":["Golang"],"title":"后端开发技能树"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/registry/","tags":"","title":"镜像仓库"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/oop/","tags":"","title":"面向对象编程"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/configmap/","tags":"","title":"配置"},{"body":"channel的原理 channel的作用是解决goroutine之间的通信问题。不要通过共享内存来通信，而应该通过通信来共享内存。\n1. channel的特性 goroutine安全 提供FIFO语义(buffered channel提供)，缓冲大小 在不同的goroutine之间存储和传输值 可以让goroutine block/unblock 2. channel的数据结构 type hchan struct { qcount uint // 当前队列中剩余元素个数 dataqsiz uint // 环形队列长度，即可以存放的元素个数 buf unsafe.Pointer // 环形队列指针 elemsize uint16 // 每个元素的大小 closed uint32 // 标识关闭状态 elemtype *_type // 元素类型 sendx uint // 队列下标，指示元素写入时存放到队列中的位置 recvx uint // 队列下标，指示元素从队列的该位置读出 recvq waitq // 等待读消息的goroutine队列 sendq waitq // 等待写消息的goroutine队列 lock mutex // 互斥锁，chan不允许并发读写 } hchan维护了两个链表，recvq是因读这个chan而阻塞的G，sendq则是因写这个chan而阻塞的G。waitq队列中每个元素的数据结构为sudog，其中elem用于保存数据。\n3. 创建channel make函数在创建channel的时候会在该进程的heap区申请一块内存，创建一个hchan结构体，返回执行该内存的指针，所以获取的的ch变量本身就是一个指针，在函数之间传递的时候是同一个channel。\nhchan结构体使用一个环形队列来保存groutine之间传递的数据(如果是缓存channel的话)，使用两个list保存像该chan发送和从改chan接收数据的goroutine，还有一个mutex来保证操作这些结构的安全。\n4. 写入channel recvq存放读取channel阻塞的G，此时说明channel里面没有数据。sendq存放写入channel阻塞的G，此时说明channel已经满了。\n如果等待接收队列recvq不为空，说明缓冲区中没有数据或者没有缓冲区，此时直接从recvq取出G,并把数据写入，最后把该recvq的G唤醒，结束发送过程； 如果缓冲区中有空余位置，将数据写入缓冲区，结束发送过程； 如果缓冲区满了，将待发送数据写入G，将当前G加入sendq，进入睡眠（阻塞状态），等待被读goroutine唤醒； 5. 读出channel 如果等待发送队列sendq不为空，且没有缓冲区，直接从sendq中取出G，把G中数据读出，最后把G唤醒，结束读取过程； 如果等待发送队列sendq不为空，此时说明缓冲区已满，从缓冲区中首部读出数据，把G中数据写入缓冲区尾部，把G唤醒，结束读取过程； 如果等待发送队列sendq为空，说明缓冲区中有数据，则从缓冲区取出数据，结束读取过程； 如果channel读取不到数据，将当前goroutine加入recvq，进入睡眠（阻塞状态），等待被写goroutine唤醒； 6. 关闭channel 将 c.closed 设置为 1\n唤醒 recvq 队列里面的阻塞 goroutine\n唤醒 sendq 队列里面的阻塞 goroutine\n7. 阻塞 当G1向buf已经满了的ch发送数据的时候，当runtine检测到对应的hchan的buf已经满了，会通知调度器，调度器会将G1的状态设置为waiting, 移除与线程M的联系，然后从P的runqueue中选择一个goroutine在线程M中执行，此时G1就是阻塞状态，但是不是操作系统的线程阻塞，所以这个时候只用消耗少量的资源。\n8. 唤醒 当G1变为waiting状态后，会创建一个代表自己的sudog的结构，然后放到sendq这个list中，sudog结构中保存了channel相关的变量的指针。当G2从ch中接收一个数据时，会通知调度器，设置G1的状态为runnable，然后将加入P的runqueue里，等待线程执行.\n","categories":"","description":"","excerpt":"channel的原理 channel的作用是解决goroutine之间的通信问题。不要通过共享内存来通信，而应该通过通信来共享内存。\n1. …","ref":"/golang-notes/principle/understand-channel/","tags":["Golang"],"title":"深入理解Channel"},{"body":"1. 安装kind On mac or linux\ncurl -Lo ./kind \"https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-$(uname)-amd64\" chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind 2. 创建k8s集群 $ kind create cluster Creating cluster \"kind\" ... ✓ Ensuring node image (kindest/node:v1.17.0) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Not sure what to do next? 😅 Check out https://kind.sigs.k8s.io/docs/user/quick-start/ 查看集群信息\n$ kubectl cluster-info --context kind-kind Kubernetes master is running at https://127.0.0.1:32768 KubeDNS is running at https://127.0.0.1:32768/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 查看node\n$ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-control-plane Ready master 35h v1.17.0 172.17.0.2 \u003cnone\u003e Ubuntu 19.10 3.10.107-1-tlinux2_kvm_guest-0049 containerd://1.3.2 查看pod\n$ kubectl get po --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-6955765f44-lqk9v 1/1 Running 0 35h 10.244.0.4 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system coredns-6955765f44-zpsmc 1/1 Running 0 35h 10.244.0.3 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system etcd-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kindnet-8mt7d 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-5w25s 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e local-path-storage local-path-provisioner-7745554f7f-dckzr 1/1 Running 0 35h 10.244.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e docker ps\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 93b291f99dd4 kindest/node:v1.17.0 \"/usr/local/bin/entr…\" 2 minutes ago Up 2 minutes 127.0.0.1:32768-\u003e6443/tcp kind-control-plane 3. kindest/node容器内进程 $ docker exec -it 93b291f99dd4 bash root@kind-control-plane:/# ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.0 19512 7480 ? Ss 03:18 0:00 /sbin/init root 105 0.0 0.0 26396 7344 ? S\u003cs 03:18 0:00 /lib/systemd/systemd-journald root 141 2.3 0.3 2374736 51564 ? Ssl 03:18 0:06 /usr/local/bin/containerd root 325 0.0 0.0 112540 5036 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 3f415d609e15ef12b9f53557891c311c156b912d5a326544a25c8b29cfa9d366 -address /run/containerd/containerd.sock root 346 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 370 0.0 0.0 112540 5108 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 1e1f3eed09f701fb621325e7b9e96d1c3de60ebd3bd64e0aec376e9490cf0e57 -address /run/containerd/containerd.sock root 397 0.0 0.0 112540 4684 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c5e451089a1a5b3dfb2cc68ee27ac7d414285be55ecfdc5bd59180fbfbc7df2e -address /run/containerd/containerd.sock root 424 0.0 0.0 112540 4924 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 81e35f29ac8c2dda344125a10e3791be7ccf788a88f1efbc3397fa319f02881f -address /run/containerd/containerd.sock root 443 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 458 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 465 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 548 0.7 0.1 145500 27724 ? Ssl 03:18 0:02 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true root 589 1.0 0.3 159536 54384 ? Ssl 03:18 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.cr root 613 3.8 1.6 445780 273484 ? Ssl 03:18 0:10 kube-apiserver --advertise-address=172.17.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/ root 660 1.4 0.2 10613604 37448 ? Ssl 03:18 0:04 etcd --advertise-client-urls=https://172.17.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://172.17.0.2:2380 --initial-cluster=kind-control-plane=https://172. root 718 1.3 0.3 2084848 52772 ? Ssl 03:18 0:03 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail root 876 0.0 0.0 112540 5084 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id adfbea8fec5ac6986407291f5bfc5aecead176954e5dabbe1517b98dd77bf78b -address /run/containerd/containerd.sock root 893 0.0 0.0 112540 4796 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 53bdce023626b60ffaa0548b5888e457dc9c3bc45c7808a385dd0f63dcc90327 -address /run/containerd/containerd.sock root 924 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 931 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 1000 0.0 0.0 127616 11100 ? Ssl 03:18 0:00 /bin/kindnetd root 1017 0.0 0.1 141060 19420 ? Ssl 03:18 0:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=kind-control-plane root 1066 0.0 0.0 0 0 ? Z 03:18 0:00 [iptables-nft-sa] \u003cdefunct\u003e root 1080 0.0 0.0 0 0 ? Z 03:18 0:00 [iptables-nft-sa] \u003cdefunct\u003e root 1241 0.0 0.0 112540 5156 ? Sl 03:19 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 5cbd7bbe186cf5847786c7a03aa4c6f82e6c805d0a189f0f3e8fb1750594260d -address /run/containerd/containerd.sock root 1262 0.0 0.0 1012 4 ? Ss 03:19 0:00 /pause root 1303 0.1 0.0 134372 14088 ? Ssl 03:19 0:00 local-path-provisioner --debug start --helper-image k8s.gcr.io/debian-base:v2.0.0 --config /etc/config/config.json root 1411 0.0 0.0 112540 4876 ? Sl 03:19 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 196b440345cb5ef47a6c31222323d35bbfef85d1d79c149ec0e3a6e22022a5f0 -address /run/containerd/containerd.sock root 1437 0.0 0.0 1012 4 ? Ss 03:19 0:00 /pause root 1450 0.0 0.0 112540 4380 ? Sl 03:19 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id de7bdf052083978c78708383f842567d4fb38adff22a56792437a4de82425afe -address /run/containerd/containerd.sock root 1480 0.0 0.0 1012 4 ? Ss 03:19 0:00 /pause root 1530 0.1 0.1 144324 19056 ? Ssl 03:19 0:00 /coredns -conf /etc/coredns/Corefile root 1531 0.1 0.1 144580 19204 ? Ssl 03:19 0:00 /coredns -conf /etc/coredns/Corefile 参考：\nhttps://github.com/kubernetes-sigs/kind https://kind.sigs.k8s.io/docs/user/quick-start/ ","categories":"","description":"","excerpt":"1. 安装kind On mac or linux\ncurl -Lo ./kind …","ref":"/kubernetes-notes/setup/installer/install-k8s-by-kind/","tags":["Kubernetes"],"title":"使用kind安装kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/network/","tags":"","title":"网络"},{"body":"1. vlan是什么 VLAN（Virtual Local Area Network）即虚拟局域网，是将一个物理的LAN在逻辑上划分成多个广播域的通信技术。每个VLAN是一个广播域，VLAN内的主机间可以直接通信，而VLAN间则不能直接互通。这样，广播报文就被限制在一个VLAN内。\n2. 为什么需要vlan 没有vlan前，广播报文会被发送到较大的广播域，主机数量较多时候会造成广播泛滥，性能低下，网络不可用等问题。如果把一个LAN网分成多个逻辑LAN网，每个逻辑LAN网通过id进行标识，相同逻辑LAN内的主机可以直接通信，不同逻辑LAN网内的主机不能直接通信，那么广播报文就限制在一个逻辑LAN网内，而这个逻辑LAN就是所谓的VLAN。\n由此可见，VLAN有以下的优点：\n限制广播域：节省了带宽，提高网络处理效率。 增加安全性：不同VLAN互相隔离，增加安全性。 灵活构建局域网：可以方便的构建安全的局域网。 3. vlan ID及vlan tag 为了让交换机识别不同vlan的报文，则需要通过某种标识来区分，因此在报文中加了vlan tag的字段，其中包括vlan id的信息，\nVlan id(简称VID)，取值范围为0-4095,0和4095为保留字段，因此VLAN ID的有效范围为1-4094。\n交换机内部处理的数据帧都带有VLAN标签。而交换机连接的部分设备（如用户主机、服务器）只会收发不带VLAN tag的传统以太网数据帧。因此，要与这些设备交互，就需要交换机的接口能够识别传统以太网数据帧，并在收发时给帧添加、剥除VLAN标签。添加什么VLAN标签，由接口上的缺省VLAN（Port Default VLAN ID，PVID）决定。\n4. vlan的接口类型 现网中属于同一个VLAN的用户可能会被连接在不同的交换机上，且跨越交换机的VLAN可能不止一个，如果需要用户间的互通，就需要交换机间的接口能够同时识别和发送多个VLAN的数据帧。根据接口连接对象以及对收发数据帧处理的不同，当前有VLAN的多种接口类型，以适应不同的连接和组网。\n常见的VLAN接口类型有三种，包括：Access、Trunk和Hybrid。\n4.1. Access接口（接入VLAN） Access接口一般用于和不能识别Tag的用户终端（如用户主机、服务器）相连，或者不需要区分不同VLAN成员时使用。\n定义：Access VLAN接口是一种用于连接终端设备（如计算机、打印机等）的接口，每个接口只属于一个VLAN。\n使用场景：适用于接入层交换机端口，每个端口连接一个终端设备，只能传输单个VLAN的流量。例如，一个办公区域内的计算机都连接到Access VLAN，以便这些计算机能够互相通信。\n4.2. Trunk接口（干线VLAN） Trunk接口一般用于连接交换机、路由器、AP以及可同时收发Tagged帧和Untagged帧的语音终端。它可以允许多个VLAN的帧带Tag通过，但只允许属于缺省VLAN的帧从该类接口上发出时不带Tag（即剥除Tag）。\n定义：Trunk VLAN接口用于在交换机之间传输多个VLAN的流量。Trunk端口能够标记（tagging）VLAN信息，以便区分不同的VLAN流量。 使用场景：适用于交换机之间或交换机与路由器之间的连接，用于传输多VLAN流量。例如，在数据中心内，需要通过Trunk端口将多个VLAN的数据传输到核心交换机。\n4.3. Hybrid接口（混合VLAN） Hybrid接口既可以用于连接不能识别Tag的用户终端（如用户主机、服务器）和网络设备（如Hub），也可以用于连接交换机、路由器以及可同时收发Tagged帧和Untagged帧的语音终端、AP。它可以允许多个VLAN的帧带Tag通过，且允许从该类接口发出的帧根据需要配置某些VLAN的帧带Tag（即不剥除Tag）、某些VLAN的帧不带Tag（即剥除Tag）。\n定义：Hybrid VLAN接口可以同时处理Access VLAN和Trunk VLAN的流量。它可以将未标记的流量作为Access VLAN流量处理，并将标记的流量作为Trunk VLAN流量处理。\n使用场景：适用于需要灵活配置的环境，既需要处理来自终端设备的单一VLAN流量，又需要处理来自交换机的多VLAN流量。例如，在一个网络中，需要同时连接计算机和其他交换机的端口可以配置为Hybrid VLAN。\n参考：\nhttps://mp.weixin.qq.com/s/5wH9QbBTGKpYaolRbMijMA https://www.wpgdadatong.com.cn/blog/detail/71784 ","categories":"","description":"","excerpt":"1. vlan是什么 VLAN（Virtual Local Area Network）即虚拟局域网，是将一个物理的LAN在逻辑上划分成多个广 …","ref":"/linux-notes/network/vlan/","tags":["network"],"title":"VLAN介绍"},{"body":"1. 部署dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml 镜像： kubernetesui/dashboard:v2.5.0\n默认端口：8443\n登录页面需要填入token或kubeconfig\n2. 登录dashboard 2.1. 创建超级管理员 参考：dashboard/creating-sample-user\n创建dashboard-adminuser.yaml文件如下：\nk8s 1.24+版本需要自行创建secret绑定serviceaccount\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: name: admin-user-secret namespace: kubernetes-dashboard annotations: kubernetes.io/service-account.name: \"admin-user\" type: kubernetes.io/service-account-token 创建serviceaccount和ClusterRoleBinding，绑定cluster-admin的超级管理员的权限。\nkubectl apply -f dashboard-adminuser.yaml 创建用户token\nkubectl -n kubernetes-dashboard create token admin-user --duration 8760h 或者通过secret查询token\nkubectl get secret admin-user-secret -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d 移除账号\nkubectl -n kubernetes-dashboard delete serviceaccount admin-user kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user 2.2. 创建Namespace管理员 1、创建角色权限（role）\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: \u003cnamespace\u003e name: \u003cnamespace\u003e-admin-role rules: - apiGroups: - '*' resources: - '*' verbs: - '*' 2、创建用户账号（ServiceAccount）\napiVersion: v1 kind: ServiceAccount metadata: name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e 创建secret 可自动生成token\napiVersion: v1 kind: Secret metadata: name: ${SecretName} namespace: ${ServiceAccountNS} annotations: kubernetes.io/service-account.name: \"${ServiceAccountName}\" type: kubernetes.io/service-account-token 3、创建角色绑定关系\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: \u003cnamespace\u003e-admin-role subjects: - kind: ServiceAccount name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e 4、生成token\nkubectl -n \u003cnamespace\u003e create token \u003cServiceAccount\u003e --duration 8760h 或者通过上述secret中的token获得\nkubectl get secret ${SecretName} -n ${ServiceAccountNS} -o jsonpath={\".data.token\"} | base64 -d 2.3. 创建只读账户 集群默认提供了几种命名空间级别的权限，分别设置ClusterRole: [admin, edit, view], 将授权设置为ClusterRole为view即可。\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: view subjects: - kind: ServiceAccount name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e 3. 集成SSO登录 社区提供了添加Authorization header的方式来集成自定义的SSO登录。即在HTTP请求中增加Header: Authorization: Bearer \u003ctoken\u003e。该操作可以通过apisix或Nginx等插件注入Header。\n参考：\n部署和访问 Kubernetes 仪表板（Dashboard） | Kubernetes\ndashboard/creating-sample-user.md at master · kubernetes/dashboard · GitHub\ndashboard/docs/user/access-control at master · kubernetes/dashboard · GitHub\n","categories":"","description":"","excerpt":"1. 部署dashboard kubectl apply -f …","ref":"/kubernetes-notes/setup/installer/install-dashboard/","tags":["Kubernetes"],"title":"安装k8s dashboard"},{"body":"错误处理 1. error接口 //定义error接口 type error interface{ Error() string } //调用error接口 func Foo(param int) (n int,err error){ //... } n,err:=Foo(0) if err!=nil{ //错误处理 }else{ //使用返回值 } 2. defer[延迟函数] 语法：\ndefer function_name() 1）defer在声明时不会执行，而是推迟执行，在return执行前，倒序执行defer[先进后出]，一般用于释放资源，清理数据，记录日志，异常处理等。\n2）defer有一个特性：即使函数抛出异常，defer仍会被执行，这样不会出现程序错误导致资源不被释放，或者因为第三方包的异常导致程序崩溃。\n3）一般用于打开文件后释放资源的操作，比如打开一个文件，最后总是要关闭的。而在打开和关闭之间，会有诸多的处理，可能会有诸多的if-else、根据不同的情况需要提前返回\nf, = os.open(filename) defer f.close() do_something() if (condition_a) {return} do_something_again() if (condition_b) {return} do_further_things() 4）defer示例\npackage main import \"fmt\" func deferTest(number int) int { defer func() { number++ fmt.Println(\"three:\", number) }() defer func() { number++ fmt.Println(\"two:\", number) }() defer func() { number++ fmt.Println(\"one:\", number) }() return number } func main() { fmt.Println(\"函数返回值：\", deferTest(0)) } /* one: 1 two: 2 three: 3 函数返回值： 0 */ 3. panic()和recover() Go中使用内置函数panic()和recover()来处理程序中的错误。\nfunc panic(interface{}) func recover() interface{} 3.1. panic() 当函数执行触发了panic()函数时，如果没有使用到defer关键字，函数执行流程会被立即终止；如果使用了defer关键字，则会逐层执行defer语句，直到所有的函数被终止。\n错误信息，包括panic函数传入的参数，将会被报告出来。\n示例如下：\npanic(404) panic(\"network broken\") panic(Error(\"file not exists\")) 3.2. recover() recover()函数用于终止错误处理流程。一般使用在defer关键字后以有效截取错误处理流程。如果没有在发生异常的goroutine中明确调用恢复过程（使用recover关键字） ，会导致该goroutine所属的进程打印异常信息后直接退出。\n示例如下：\ndefer func() { if r := recover(); r != nil { log.Printf(\"Runtime error caught: %v\", r) } }() foo() 无论foo()中是否触发了错误处理流程，该匿名defer函数都将在函数退出时得到执行。假如foo()中触发了错误处理流程， recover()函数执行将使得该错误处理过程终止。如果错误处理流程被触发时，程序传给panic函数的参数不为nil，则该函数还会打印详细的错误信息。\n参考：\n《Go语言编程》 ","categories":"","description":"","excerpt":"错误处理 1. error接口 //定义error接口 type error interface{ Error() string } //调 …","ref":"/golang-notes/basis/errors/","tags":["Golang"],"title":"错误处理"},{"body":" 以下由zsh-plugin-git内容转载过来，以备查询使用。\ngit plugin The git plugin provides many aliases and a few useful functions.\nTo use it, add git to the plugins array in your zshrc file:\nplugins=(... git) Aliases 常用 Alias Command g git ga git add gaa git add --all gcmsg git commit -m -------------------- ------------------------------------------------------------ ggp git push origin $(current_branch) ggf git push --force origin $(current_branch) gp git push gl git pull ggl git pull origin $(current_branch) -------------------- ------------------------------------------------------------ gco git checkout gcb git checkout -b gcm git checkout master gb git branch gba git branch -a gcf git config --list gd git diff 完整列表 Alias Command g git ga git add gaa git add --all gapa git add --patch gau git add --update gav git add --verbose gap git apply gb git branch gba git branch -a gbd git branch -d gbda - gbD git branch -D gbl git blame -b -w gbnm git branch --no-merged gbr git branch --remote gbs git bisect gbsb git bisect bad gbsg git bisect good gbsr git bisect reset gbss git bisect start gc git commit -v gc! git commit -v --amend gcn! git commit -v --no-edit --amend gca git commit -v -a gca! git commit -v -a --amend gcan! git commit -v -a --no-edit --amend gcans! git commit -v -a -s --no-edit --amend gcam git commit -a -m gcsm git commit -s -m gcb git checkout -b gcf git config --list gcl git clone --recurse-submodules gclean git clean -id gpristine git reset --hard \u0026\u0026 git clean -dfx gcm git checkout master gcd git checkout develop gcmsg git commit -m gco git checkout gcount git shortlog -sn gcp git cherry-pick gcpa git cherry-pick --abort gcpc git cherry-pick --continue gcs git commit -S gd git diff gdca git diff --cached gdcw git diff --cached --word-diff gdct git describe --tags $(git rev-list --tags --max-count=1) gds git diff --staged gdt git diff-tree --no-commit-id --name-only -r gdv - gdw git diff --word-diff gf git fetch gfa git fetch --all --prune gfg - gfo git fetch origin gg git gui citool gga git gui citool --amend ggf git push --force origin $(current_branch) ggfl git push --force-with-lease origin $(current_branch) ggl git pull origin $(current_branch) ggp git push origin $(current_branch) ggpnp ggl \u0026\u0026 ggp ggpull git pull origin \"$(git_current_branch)\" ggpur ggu ggpush git push origin \"$(git_current_branch)\" ggsup git branch --set-upstream-to=origin/$(git_current_branch) ggu git pull --rebase origin $(current_branch) gpsup git push --set-upstream origin $(git_current_branch) ghh git help gignore git update-index --assume-unchanged gignored - git-svn-dcommit-push git svn dcommit \u0026\u0026 git push github master:svntrunk gk gitk --all --branches gke gitk --all $(git log -g --pretty=%h) gl git pull glg git log --stat glgp git log --stat -p glgg git log --graph glgga git log --graph --decorate --all glgm git log --graph --max-count=10 glo git log --oneline --decorate glol git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' glols git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --stat glod git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%ad) %C(bold blue)\u003c%an\u003e%Creset' glods git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%ad) %C(bold blue)\u003c%an\u003e%Creset' --date=short glola git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --all glog git log --oneline --decorate --graph gloga git log --oneline --decorate --graph --all glp _git_log_prettily gm git merge gmom git merge origin/master gmt git mergetool --no-prompt gmtvim git mergetool --no-prompt --tool=vimdiff gmum git merge upstream/master gma git merge --abort gp git push gpd git push --dry-run gpf git push --force-with-lease gpf! git push --force gpoat git push origin --all \u0026\u0026 git push origin --tags gpu git push upstream gpv git push -v gr git remote gra git remote add grb git rebase grba git rebase --abort grbc git rebase --continue grbd git rebase develop grbi git rebase -i grbm git rebase master grbs git rebase --skip grh git reset grhh git reset --hard groh git reset origin/$(git_current_branch) --hard grm git rm grmc git rm --cached grmv git remote rename grrm git remote remove grset git remote set-url grt - gru git reset -- grup git remote update grv git remote -v gsb git status -sb gsd git svn dcommit gsh git show gsi git submodule init gsps git show --pretty=short --show-signature gsr git svn rebase gss git status -s gst git status gsta git stash push gsta git stash save gstaa git stash apply gstc git stash clear gstd git stash drop gstl git stash list gstp git stash pop gsts git stash show --text gstall git stash --all gsu git submodule update gts git tag -s gtv - gtl gtl(){ git tag --sort=-v:refname -n -l ${1}* }; noglob gtl gunignore git update-index --no-assume-unchanged gunwip - gup git pull --rebase gupv git pull --rebase -v gupa git pull --rebase --autostash gupav git pull --rebase --autostash -v glum git pull upstream master gwch git whatchanged -p --abbrev-commit --pretty=medium gwip git add -A; git rm $(git ls-files --deleted) 2\u003e /dev/null; git commit --no-verify --no-gpg-sign -m \"--wip-- [skip ci]\" Deprecated These are aliases that have been removed, renamed, or otherwise modified in a way that may, or may not, receive further support.\nAlias Command Modification gap git add --patch new alias gapa gcl git config --list new alias gcf gdc git diff --cached new alias gdca gdt git difftool no replacement ggpull git pull origin $(current_branch) new alias ggl (ggpull still exists for now though) ggpur git pull --rebase origin $(current_branch) new alias ggu (ggpur still exists for now though) ggpush git push origin $(current_branch) new alias ggp (ggpush still exists for now though) gk gitk --all --branches now aliased to gitk --all --branches glg git log --stat --max-count = 10 now aliased to git log --stat --color glgg git log --graph --max-count = 10 now aliased to git log --graph --color gwc git whatchanged -p --abbrev-commit --pretty = medium new alias gwch Functions Current Command Description current_branch Return the name of the current branch git_current_user_name Returns the user.name config value git_current_user_email Returns the user.email config value Work in Progress (WIP) These features allow to pause a branch development and switch to another one (\"Work in Progress\", or wip). When you want to go back to work, just unwip it.\nCommand Description work_in_progress Echoes a warning if the current branch is a wip gwip Commit wip branch gunwip Uncommit wip branch Deprecated Command Description Reason current_repository Return the names of the current remotes Didn't work properly. Use git remote -v instead (grv alias) 参考\nhttps://github.com/robbyrussell/oh-my-zsh/tree/master/plugins/git ","categories":"","description":"","excerpt":" 以下由zsh-plugin-git内容转载过来，以备查询使用。\ngit plugin The git plugin provides …","ref":"/linux-notes/git/git-alias-zsh/","tags":["Git"],"title":"Git命令别名"},{"body":" 本文由网络文章整理备份。\niterm2 rz与sz的功能 本文主要介绍mac环境下使用iterm2的rz sz功能的安装流程。\n1. 安装lrzsz brew install lrzsz 2. 安装执行脚本 将iterm2-send-zmodem.sh和iterm2-recv-zmodem.sh保存到/usr/local/bin目录下。\niterm2-send-zmodem.sh\n#!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' \u003e /dev/null 2\u003e\u00261 \u0026\u0026 NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=$(osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") else FILE=$(osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else /usr/local/bin/sz \"$FILE\" --escape --binary --bufsize 4096 sleep 1 echo echo \\# Received \"$FILE\" fi iterm2-recv-zmodem.sh\n#!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' \u003e /dev/null 2\u003e\u00261 \u0026\u0026 NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=$(osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") else FILE=$(osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else cd \"$FILE\" /usr/local/bin/rz --rename --escape --binary --bufsize 4096 sleep 1 echo echo echo \\# Sent \\-\\\u003e $FILE fi 3. 赋予这两个文件可执行权限 chmod 777 /usr/local/bin/iterm2-* 4. 设置Iterm2的Tirgger特性 设置Iterm2的Tirgger特性，profiles-\u003edefault-\u003eeditProfiles-\u003eAdvanced中的Tirgger\n添加两条trigger，分别设置 Regular expression，Action，Parameters，Instant如下：\nRegular expression: rz waiting to receive.\\*\\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checked Regular expression: \\*\\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked 示例图：\n5. 使用 上传文件：rz 下载文件：sz + file 参考：\nhttps://www.robberphex.com/use-zmodem-at-macos/ https://github.com/RobberPhex/iterm2-zmodem ","categories":"","description":"","excerpt":" 本文由网络文章整理备份。\niterm2 rz与sz的功能 本文主要介绍mac环境下使用iterm2的rz sz功能的安装流程。\n1. 安 …","ref":"/linux-notes/keymap/iterm2-rzsz/","tags":["快捷键"],"title":"iterm2 rz与sz的使用"},{"body":"1. web及网络基础 1.1. 通过HTTP访问web[C/S] 1.2. TCP/IP四层模型 1.2.1. 数据包的封装 1.3. TCP/IP协议族 1.3.1. 负责传输的IP协议 使用ARP协议凭借MAC地址通信\n1.3.2. 确保可靠的TCP协议 1.3.3. 负责域名解析的DNS服务 1.3.4. 各协议与HTTP的关系 1.4. URI与URL URI(Uniform Resource Identifier):统一资源标识符 URL(Uniform Resource Locator):统一资源定位符；URL是URI的子集 1.4.1. URI的格式 字段 说明 协议 http/https 登录信息（认证） user:pass@(一般没有) 服务器地址 域名或IP 服务器端口号 服务端口号，省略则取默认端口号 带层次的文件路径 指定服务器上的文件路径来定位特指的资源 查询字符串 使用查询字符串传入参数 片段标识符 标记以获取资源中的子资源（文档内的某个位置） 1.4.2. URI的示例 2. HTTP协议 2.1. 通过请求和响应的交换达成通信 2.1.1. 请求报文 2.1.2. 响应报文 2.2. HTTP请求方法 2.2.1. GET:获取资源 2.2.2. POST:传输实体主体 2.2.3. PUT:传输文件 PUT方法用来传输文件，像FTP协议一样，要求在请求报文的主体中包含文件内容，然后保存到请求URI指定的位置。\n因为自身不带验证机制，有安全问题，因此一般不采用。若配合验证机制或者REST标准则可使用。\n2.2.4. HEAD:获取报文头部 HEAD和GET一样但不返回报文主体部分，用于确认URI的有效性及资源的更新时间等。\n2.2.5. DELETE:删除文件 DELETE与PUT作用相反，但不带安全验证机制一般不采用。\n2.2.6. OPTIONS:询问支持的方法 OPTIONS用来查询针对请求URI指定的资源支持的方法\n2.2.7. TRACE:追踪路径 TRACE用来查询发送出去的请求是怎样被加工修改/篡改的，因为易引发XST（跨站追踪）攻击，一般不使用。\n2.2.8. CONNECT:要求用隧道协议连接代理 CONNECT要求在与代理服务器通信时建立隧道，实现用隧道协议进行TCP通信。主要使用SSL（Source Sockets Layer:安全套接字）和TLS（Transport Layer Security:传输层安全）协议把通信内容加密后经网络隧道传输。\n方法格式如下：\n2.3. 持久连接 2.3.1. keep-alive 为解决每进行一次HTTP通信就要断开一次TCP连接，增加了通信量的开销，HTTP/1.1通过keep-alive持久连接，只要任意一端没有明确提出断开连接，则保持TCP连接状态。\n持久连接减少了TCP连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。\n2.3.2. 管线化 持续连接使得多数请求以管线化（pipelining）方式发送成为可能。管线化即同时并行发送多个请求，而不需要一个接一个等待响应。管线化技术比持续连接速度快，请求数越多越明显。\n2.3.3. 使用cookie的状态管理 HTTP是无状态协议，不对之前发生过的请求和响应的状态进行管理，即无法根据之前的状态进行本次的请求处理。无状态协议的优点在于不必保存状态，减少服务器CPU及内存资源的消耗。\ncookie技术通过在请求和响应报文中写入cookie信息来控制客户端的状态。cookie会根据从服务端发送的响应报文内的一个叫做Set-Cookie的首部字段通知客户端保存Cookie；当客户端再往服务端发送请求时，客户端自动在请求报文中加入Cookie值后发送出去。服务器发现Cookie后会检查从哪个客户端发送来的连接请求，对比服务器上的记录，最后得到之前的状态信息。\n3. HTTP报文 3.1. HTTP报文 用于HTTP协议交互的信息被称为HTTP报文，客户端的HTTP报文叫做请求报文，服务端的叫做响应报文。报文大致分为报文首部和报文主体，但并不一定要有报文主体。\n3.2. 报文结构 字段 说明 请求行 请求方法，请求URI和HTTP版本 状态行 响应结果的状态码，原因短语和HTTP版本 首部字段 请求和响应的各种条件和属性的各类首部：通用首部、请求首部、响应首部、实体首部 其他 HTTP的RFC里未定义的首部（Cookie等） 3.3. 编码提升传输速率 HTTP在传输数据时可以按照数据原貌直接传输也可以在传输过程中编码提升传输速率；通过编码可以处理大量请求但会消耗更多的CPU等资源。\n3.3.1. 报文主体和实体主体的差异 报文：是HTTP通信中的基本单位，由8位组字节流组成，通过HTTP通信传输。 实体：作为请求或响应的有效载荷数据被传输，其内容由实体首部和实体主体组成。 通常报文主体等于实体主体，但当传输中进行编码时，实体主体的内容发生变化才会与报文主体产生差异。\n3.3.2. 压缩传输的内容编码 HTTP中的内容编码指明应用在实体内容上的编码格式，并保持实体信息原样压缩，内容编码后的实体由客户端接收并负责解码。\n常用的内容编码：\ngzip(GNU ZIP) compress(UNIX系统的标准压缩) deflate(zlib) identity(不进行编码) 3.3.3. 分块传输编码 分块传输编码会将实体主体分成多个块，每一块都会用十六进制来标记快的大小，而实体的最后一块会使用“0（CR+LF）”来标记。\n由接收的客户端负责解码，回复到编码前的实体主体。\n3.4. 发送多种数据的多部分对象集合 HTTP中的多部分对象集合即发送一份报文主体内可含有多类型实体，通常是图片或文本文件上传等。\n多部分对象集合包含的对象：\nmultipart/form-data:在web表单文件上传时使用 multipart/byteranges：状态码206响应报文包含了多个范围的内容时使用 3.5. 获取部分内容的范围请求 指定范围发送的请求叫做范围请求，对于一份10000字节大小的资源，如果使用范围请求，可以只请求5001-10000字节内的资源。\n执行范围请求时，会用到首部字段Range来指定资源的byte范围\n3.6. 内容协商返回最合适的内容 内容协商机制是指客户端和服务端就响应的资源内容进行交涉，然后提供给客户端最合适的资源。内容协商会以响应资源的语言、编码方式等作为判断的基准。\n内容协商类型：\n服务器驱动协商 客户端驱动协商 透明协商 4. HTTP状态码 状态码即服务器返回的请求结果。\n状态码 类型 说明 1xx Informational(信息性状态码) 接收的请求正在处理 2xx Success(成功) 请求正常处理完毕 3xx Redirection(重定向) 需要进行附加操作以完成请求 4xx Client Error(客户端错误) 服务器无法处理请求 5xx Server Error(服务端错误) 服务器处理请求出错 4.1. 2XX成功 4.1.1. 200 OK 4.1.2. 204 No Content 表示请求已成功处理，但在返回的响应报文中不含实体的主体部分。\n4.1.3. 206 Partial Content 该状态码表示客户端进行了范围请求，服务器成功执行了这部分的GET请求。响应报文中包含由Content-Range指定范围的实体内容。\n4.2. 3XX 重定向 4.2.1. 301 Moved Permanently 永久性重定向，表示资源已被分配了新的URI，以后应使用新的URI。\n4.2.2. 302 Found 临时性重定向，表示请求的资源已被分配了新的URI，但是临时性的。\n4.2.3. 303 See Other 表示由于请求的资源存在另一个URI，应使用GET方法重定向获取请求的资源。\n4.2.4. 304 Not Modified 表示客户端发送附带条件的请求时（GET中的If-Modified-Since等首部），服务器允许访问资源，但未满足附带条件因此直接返回304（服务器的资源未改变，可直接使用客户端未过期的缓存），不包含任何响应的主体部分。\n4.2.5. 307 Temporary Redirect 临时重定向，该状态与302有相同的含义。\n4.3. 4XX 客户端错误 4.3.1. 400 Bad Request 表示请求报文中存在语法错误，需修改内容重新发送请求。\n4.3.2. 401 Unauthorized 表示需要通过HTTP认证。\n4.3.3. 403 Forbidden 表示请求被服务器拒绝，未获得访问授权。\n4.3.4. 404 No Found 表明服务器上找不到请求的资源，也可以在服务器拒绝请求且不想说明理由时使用。\n4.4. 5XX 服务器错误 4.4.1. 500 Internal Server Error 表明服务器在执行请求时发生了错误，也可能是Web应用存在bug或临时故障等。\n4.4.2. 503 Service Unavailable 表明服务器暂时处于超负荷或正在进行停机维护，现在不能处理请求。\n参考：\n《图解HTTP》 ","categories":"","description":"","excerpt":"1. web及网络基础 1.1. 通过HTTP访问web[C/S] 1.2. TCP/IP四层模型 1.2.1. 数据包的封装 1.3. …","ref":"/linux-notes/tcpip/http/","tags":["TCPIP"],"title":"HTTP协议"},{"body":"1. InfluxDB简介 InfluxDB是一个当下比较流行的时序数据库，InfluxDB使用 Go 语言编写，无需外部依赖，安装配置非常方便，适合构建大型分布式系统的监控系统。\n主要特色功能：\n1）基于时间序列，支持与时间有关的相关函数（如最大，最小，求和等）\n2）可度量性：你可以实时对大量数据进行计算\n3）基于事件：它支持任意的事件数据\n2. InfluxDB安装 1）安装 wget https://dl.influxdata.com/influxdb/releases/influxdb-0.13.0.x86_64.rpm\nyum localinstall influxdb-0.13.0.armhf.rpm\n2）启动 service influxdb start\n3）访问 http://服务器IP:8083\n4）docker image方式安装 docker pull influxdb\ndocker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --volume=/opt/data/influxdb:/data --name influxsrv influxdb:latest\n3. InfluxDB的基本概念 3.1. 与传统数据库中的名词做比较 influxDB中的名词 传统数据库中的概念 database 数据库 measurement 数据库中的表 points 表里面的一行数据 3.2. InfluxDB中独有的概念 3.2.1. Point Point由时间戳（time）、数据（field）、标签（tags）组成。\nPoint相当于传统数据库里的一行数据，如下表所示：\nPoint属性 传统数据库中的概念 time 每个数据记录时间，是数据库中的主索引(会自动生成) fields 各种记录值（没有索引的属性）也就是记录的值：温度， 湿度 tags 各种有索引的属性：地区，海拔 3.2.2. series 所有在数据库中的数据，都需要通过图表来展示，而这个series表示这个表里面的数据，可以在图表上画成几条线：通过tags排列组合算出来\nshow series from cpu\n4. InfluxDB的基本操作 InfluxDB提供三种操作方式：\n1）客户端命令行方式\n2）HTTP API接口\n3）各语言API库\n4.1. InfluxDB数据库操作 操作 命令 显示数据库 show databases 创建数据库 create database db_name 删除数据库 drop database db_name 使用某个数据库 use db_name 4.2. InfluxDB数据表操作 操作 命令 说明 显示所有表 SHOW MEASUREMENTS 创建数据表 insert table_name,hostname=server01 value=442221834240i 1435362189575692182 其中 disk_free 就是表名，hostname是索引，value=xx是记录值，记录值可以有多个，最后是指定的时间 删除数据表 drop measurement table_name 查看表内容 select * from table_name 查看series show series from table_name series表示这个表里面的数据，可以在图表上画成几条线，series主要通过tags排列组合算出来 ","categories":"","description":"","excerpt":"1. InfluxDB简介 InfluxDB是一个当下比较流行的时序数据库，InfluxDB使用 Go 语言编写，无需外部依赖，安装配置非常 …","ref":"/kubernetes-notes/monitor/influxdb-introduction/","tags":["Monitor"],"title":"Influxdb介绍"},{"body":"Pod Volume 同一个Pod中的多个容器可以共享Pod级别的存储卷Volume,Volume可以定义为各种类型，多个容器各自进行挂载，将Pod的Volume挂载为容器内部需要的目录。\n例如：Pod级别的Volume:\"app-logs\",用于tomcat向其中写日志文件，busybox读日志文件。\npod-volumes-applogs.yaml\napiVersion: v1 kind: Pod metadata: name: volume-pod spec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080 volumeMounts: - name: app-logs mountPath: /usr/local/tomcat/logs - name: busybox image: busybox command: [\"sh\",\"-c\",\"tailf /logs/catalina*.log\"] volumeMounts: - name: app-logs mountPath: /logs volumes: - name: app-logs emptuDir: {} 查看日志\nkubectl logs \u003cpod_name\u003e -c \u003ccontainer_name\u003e kubectl exec -it \u003cpod_name\u003e -c \u003ccontainer_name\u003e – tail /usr/local/tomcat/logs/catalina.xx.log 参考文章\n《Kubernetes权威指南》 ","categories":"","description":"","excerpt":"Pod Volume 同一个Pod中的多个容器可以共享Pod级别的存储卷Volume,Volume可以定义为各种类型，多个容器各自进行挂载， …","ref":"/kubernetes-notes/concepts/pod/pod-volume/","tags":["Kubernetes"],"title":"Pod存储卷"},{"body":"1. Etcd配置参数 / # etcd --help usage: etcd [flags] start an etcd server etcd --version show the version of etcd etcd -h | --help show the help information about etcd etcd --config-file path to the server configuration file etcd gateway run the stateless pass-through etcd TCP connection forwarding proxy etcd grpc-proxy run the stateless etcd v3 gRPC L7 reverse proxy 1.1. member flags member flags: --name 'default' human-readable name for this member. --data-dir '${name}.etcd' path to the data directory. --wal-dir '' path to the dedicated wal directory. --snapshot-count '100000' number of committed transactions to trigger a snapshot to disk. --heartbeat-interval '100' time (in milliseconds) of a heartbeat interval. --election-timeout '1000' time (in milliseconds) for an election to timeout. See tuning documentation for details. --initial-election-tick-advance 'true' whether to fast-forward initial election ticks on boot for faster election. --listen-peer-urls 'http://localhost:2380' list of URLs to listen on for peer traffic. --listen-client-urls 'http://localhost:2379' list of URLs to listen on for client traffic. --max-snapshots '5' maximum number of snapshot files to retain (0 is unlimited). --max-wals '5' maximum number of wal files to retain (0 is unlimited). --cors '' comma-separated whitelist of origins for CORS (cross-origin resource sharing). --quota-backend-bytes '0' raise alarms when backend size exceeds the given quota (0 defaults to low space quota). --max-txn-ops '128' maximum number of operations permitted in a transaction. --max-request-bytes '1572864' maximum client request size in bytes the server will accept. --grpc-keepalive-min-time '5s' minimum duration interval that a client should wait before pinging server. --grpc-keepalive-interval '2h' frequency duration of server-to-client ping to check if a connection is alive (0 to disable). --grpc-keepalive-timeout '20s' additional duration of wait before closing a non-responsive connection (0 to disable). 1.2. clustering flags clustering flags: --initial-advertise-peer-urls 'http://localhost:2380' list of this member's peer URLs to advertise to the rest of the cluster. --initial-cluster 'default=http://localhost:2380' initial cluster configuration for bootstrapping. --initial-cluster-state 'new' initial cluster state ('new' or 'existing'). --initial-cluster-token 'etcd-cluster' initial cluster token for the etcd cluster during bootstrap. Specifying this can protect you from unintended cross-cluster interaction when running multiple clusters. --advertise-client-urls 'http://localhost:2379' list of this member's client URLs to advertise to the public. The client URLs advertised should be accessible to machines that talk to etcd cluster. etcd client libraries parse these URLs to connect to the cluster. --discovery '' discovery URL used to bootstrap the cluster. --discovery-fallback 'proxy' expected behavior ('exit' or 'proxy') when discovery services fails. \"proxy\" supports v2 API only. --discovery-proxy '' HTTP proxy to use for traffic to discovery service. --discovery-srv '' dns srv domain used to bootstrap the cluster. --strict-reconfig-check 'true' reject reconfiguration requests that would cause quorum loss. --auto-compaction-retention '0' auto compaction retention length. 0 means disable auto compaction. --auto-compaction-mode 'periodic' interpret 'auto-compaction-retention' one of: periodic|revision. 'periodic' for duration based retention, defaulting to hours if no time unit is provided (e.g. '5m'). 'revision' for revision number based retention. --enable-v2 'true' Accept etcd V2 client requests. 1.3. proxy flags proxy flags: \"proxy\" supports v2 API only. --proxy 'off' proxy mode setting ('off', 'readonly' or 'on'). --proxy-failure-wait 5000 time (in milliseconds) an endpoint will be held in a failed state. --proxy-refresh-interval 30000 time (in milliseconds) of the endpoints refresh interval. --proxy-dial-timeout 1000 time (in milliseconds) for a dial to timeout. --proxy-write-timeout 5000 time (in milliseconds) for a write to timeout. --proxy-read-timeout 0 time (in milliseconds) for a read to timeout. 1.4. security flags security flags: --ca-file '' [DEPRECATED] path to the client server TLS CA file. '-ca-file ca.crt' could be replaced by '-trusted-ca-file ca.crt -client-cert-auth' and etcd will perform the same. --cert-file '' path to the client server TLS cert file. --key-file '' path to the client server TLS key file. --client-cert-auth 'false' enable client cert authentication. --client-crl-file '' path to the client certificate revocation list file. --trusted-ca-file '' path to the client server TLS trusted CA cert file. --auto-tls 'false' client TLS using generated certificates. --peer-ca-file '' [DEPRECATED] path to the peer server TLS CA file. '-peer-ca-file ca.crt' could be replaced by '-peer-trusted-ca-file ca.crt -peer-client-cert-auth' and etcd will perform the same. --peer-cert-file '' path to the peer server TLS cert file. --peer-key-file '' path to the peer server TLS key file. --peer-client-cert-auth 'false' enable peer client cert authentication. --peer-trusted-ca-file '' path to the peer server TLS trusted CA file. --peer-auto-tls 'false' peer TLS using self-generated certificates if --peer-key-file and --peer-cert-file are not provided. --peer-crl-file '' path to the peer certificate revocation list file. 1.5. logging flags logging flags --debug 'false' enable debug-level logging for etcd. --log-package-levels '' specify a particular log level for each etcd package (eg: 'etcdmain=CRITICAL,etcdserver=DEBUG'). --log-output 'default' specify 'stdout' or 'stderr' to skip journald logging even when running under systemd. 1.6. unsafe flags unsafe flags: Please be CAUTIOUS when using unsafe flags because it will break the guarantees given by the consensus protocol. --force-new-cluster 'false' force to create a new one-member cluster. 1.7. profiling flags profiling flags: --enable-pprof 'false' Enable runtime profiling data via HTTP server. Address is at client URL + \"/debug/pprof/\" --metrics 'basic' Set level of detail for exported metrics, specify 'extensive' to include histogram metrics. --listen-metrics-urls '' List of URLs to listen on for metrics. 1.8. auth flags auth flags: --auth-token 'simple' Specify a v3 authentication token type and its options ('simple' or 'jwt'). 1.9. experimental flags experimental flags: --experimental-initial-corrupt-check 'false' enable to check data corruption before serving any client/peer traffic. --experimental-corrupt-check-time '0s' duration of time between cluster corruption check passes. --experimental-enable-v2v3 '' serve v2 requests through the v3 backend under a given prefix. ","categories":"","description":"","excerpt":"1. Etcd配置参数 / # etcd --help usage: etcd [flags] start an etcd server …","ref":"/kubernetes-notes/etcd/etcd-setup-flags/","tags":["Etcd"],"title":"Etcd启动配置参数"},{"body":"1. Dockerfile的说明 dockerfile指令忽略大小写，建议大写，#作为注释，每行只支持一条指令，指令可以带多个参数。\ndockerfile指令分为构建指令和设置指令。\n构建指令：用于构建image，其指定的操作不会在运行image的容器中执行。 设置指令：用于设置image的属性，其指定的操作会在运行image的容器中执行。 2. Dockerfile指令说明 2.1. FROM（指定基础镜像）[构建指令] 该命令用来指定基础镜像，在基础镜像的基础上修改数据从而构建新的镜像。基础镜像可以是本地仓库也可以是远程仓库。\n指令有两种格式：\nFROM image 【默认为latest版本】 FROM image:tag 【指定版本】 2.2. MAINTAINER（镜像创建者信息）[构建指令] 将镜像制作者（维护者）的信息写入image中，执行docker inspect时会输出该信息。\n格式：MAINTAINER name\nMAINTAINER命令已废弃，可使用maintainer label的方式。\nLABEL maintainer=\"SvenDowideit@home.org.au\" 2.3. RUN（安装软件用）[构建指令] RUN可以运行任何被基础镜像支持的命令（即在基础镜像上执行一个进程），可以使用多条RUN指令，指令较长可以使用\\来换行。\n指令有两种格式：\nRUN command (the command is run in a shell - /bin/sh -c) RUN [\"executable\", \"param1\", \"param2\" ... ] (exec form) 指定使用其他终端实现，使用exec执行。 例子：RUN[\"/bin/bash\",\"-c\",\"echo hello\"] 2.4. CMD（设置container启动时执行的操作）[设置指令] 用于容器启动时的指定操作，可以是自定义脚本或命令，只执行一次，多个默认执行最后一个。\n指令有三种格式：\nCMD [\"executable\",\"param1\",\"param2\"] (like an exec, this is the preferred form) 运行一个可执行文件并提供参数。 CMD command param1 param2 (as a shell) 直接执行shell命令，默认以/bin/sh -c执行。 CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT) 和ENTRYPOINT配合使用，只作为完整命令的参数部分。 2.5. ENTRYPOINT（设置container启动时执行的操作）[设置指令] 指定容器启动时执行的命令，若多次设置只执行最后一次。\nENTRYPOINT翻译为“进入点”，它的功能可以让容器表现得像一个可执行程序一样。\n例子：ENTRYPOINT [\"/bin/echo\"] ，那么docker build出来的镜像以后的容器功能就像一个/bin/echo程序，docker run -it imageecho “this is a test”，就会输出对应的字符串。这个imageecho镜像对应的容器表现出来的功能就像一个echo程序一样。\n指令有两种格式：\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] (like an exec, the preferred form)\n和CMD配合使用，CMD则作为完整命令的参数部分，ENTRYPOINT以JSON格式指定执行的命令部分。CMD可以为ENTRYPOINT提供可变参数，不需要变动的参数可以写在ENTRYPOINT里面。\n例子：\nENTRYPOINT [\"/usr/bin/ls\",\"-a\"]\nCMD [\"-l\"]\nENTRYPOINT command param1 param2 (as a shell)\n独自使用，即和CMD类似，如果CMD也是个完整命令[CMD command param1 param2 (as a shell) ]，那么会相互覆盖，只执行最后一个CMD或ENTRYPOINT。 例子：ENTRYPOINT ls -l 2.6. USER（设置container容器启动的登录用户）[设置指令] 设置启动容器的用户，默认为root用户。\n格式：USER daemon\n2.7. EXPOSE（指定容器需要映射到宿主机的端口）[设置指令] 该指令会将容器中的端口映射为宿主机中的端口[确保宿主机的端口号没有被使用]。通过宿主机IP和映射后的端口即可访问容器[避免每次运行容器时IP随机生成不固定的问题]。前提是EXPOSE设置映射端口，运行容器时加上-p参数指定EXPOSE设置的端口。EXPOSE可以设置多个端口号，相应地运行容器配套多次使用-p参数。可以通过docker port +容器需要映射的端口号和容器ID来参考宿主机的映射端口。\n格式：EXPOSE port [port...]\n2.8. ENV（用于设置环境变量）[构建指令] 在image中设置环境变量[以键值对的形式]，设置之后RUN命令可以使用该环境变量，在容器启动后也可以通过docker inspect查看环境变量或者通过 docker run --env key=value设置或修改环境变量。\n格式：ENV key value\n例子：ENV JAVA_HOME /path/to/java/dirent\n2.9. ARG（用于设置变量）[构建指令] ARG定义一个默认参数，可以在dockerfile中引用。构建阶段可以通过docker build --build-arg =参数向dockerfile文件中传入参数。\nARG \u003carg_name\u003e[=\u003cdefault value\u003e] # 可以搭配ENV使用 ENV env_name ${arg_name} 示例：\ndocker build --build-arg user=what_user . 2.10. ADD（从src复制文件到container的dest路径）[构建指令] 复制指定的src到容器中的dest，其中src是相对被构建的源目录的相对路径，可以是文件或目录的路径，也可以是一个远程的文件url。dest 是container中的绝对路径。所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0。\n如果src是一个目录，那么会将该目录下的所有文件添加到container中，不包括目录； 如果src文件是可识别的压缩格式，则docker会帮忙解压缩（注意压缩格式）； 如果src是文件且dest中不使用斜杠结束，则会将dest视为文件，src的内容会写入dest； 如果src是文件且dest中使用斜杠结束，则会src文件拷贝到dest目录下。 格式：ADD src dest\n为避免 ADD命令带来的未知风险和复杂性，可以使用COPY命令替代ADD命令\n2.11. COPY（复制文件） 复制本地主机的src为容器中的dest，目标路径不存在时会自动创建。\n格式：COPY src dest\n2.12. VOLUME（指定挂载点）[设置指令] 创建一个可以从本地主机或其他容器挂载的挂载点，使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用也可以被其他容器使用。\n格式：VOLUME [\"mountpoint\"]\n其他容器使用共享数据卷：docker run -t -i -rm -volumes-from container1 image2 bash [container1为第一个容器的ID，image2为第二个容器运行image的名字。]\n2.13. WORKDIR（切换目录）[设置指令] 相当于cd命令，可以多次切换目录，为RUN,CMD,ENTRYPOINT配置工作目录。可以使用多个WORKDIR的命令，后续命令如果是相对路径则是在上一级路径的基础上执行[类似cd的功能]。\n格式：WORKDIR /path/to/workdir\n2.14. ONBUILD（在子镜像中执行） 当所创建的镜像作为其他新创建镜像的基础镜像时执行的操作命令，即在创建本镜像时不运行，当作为别人的基础镜像时再在构建时运行（可认为基础镜像为父镜像，而该命令即在它的子镜像构建时运行，相当于在子镜像构建时多加了一些命令）。\n格式：ONBUILD Dockerfile关键字\n3. dockerfile示例 最佳实践\n镜像可以分为三层：系统基础镜像、业务基础镜像、业务镜像。 尽量将不变的镜像操作放dockerfile前面。 一类RUN命令操作可以通过\\和\u0026\u0026方式组合成一条RUN命令。 dockerfile尽量清晰简洁。 文件目录\n./ |-- Dockerfile |-- docker-entrypoint.sh |-- dumb-init |-- conf # 配置文件路径 | `-- app_conf.py |-- pkg # 安装包路径 | `-- install.tar.gz |-- run.sh # 启动脚本 dockerfile示例\nFROM centos:latest LABEL maintainer=\"xxx@xxx.com\" ARG APP=appname ENV APP ${APP} # copy and install app COPY conf/app_conf.py /usr/local/app/app_conf/app_conf.py COPY pkg/${APP}-*-install.tar.gz /data/${APP}-install.tar.gz RUN mkdir -p /data/${APP} \\ \u0026\u0026 tar -zxvf /data/${APP}-install.tar.gz -C /data/${APP} \\ \u0026\u0026 cd /data/${APP}/${APP}* \\ \u0026\u0026 ./install.sh WORKDIR /usr/local/app/ # init COPY dumb-init /usr/bin/dumb-init COPY docker-entrypoint.sh /docker-entrypoint.sh ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\",\"/docker-entrypoint.sh\"] COPY run.sh /run.sh RUN chmod +x /run.sh CMD [\"/run.sh\"] 4. docker build 指定dockerfile文件构建\n默认不指定dockerfile文件名，则读取指定路径的Dockerfile\ndocker build -t \u003cimage_name\u003e -f \u003cdockerfile_name\u003e \u003cdockerfile_path\u003e docker build --help\ndocker build --help Usage:\tdocker build [OPTIONS] PATH | URL | - Build an image from a Dockerfile Options: --add-host list Add a custom host-to-IP mapping (host:ip) --build-arg list Set build-time variables --cache-from strings Images to consider as cache sources --cgroup-parent string Optional parent cgroup for the container --compress Compress the build context using gzip --cpu-period int Limit the CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit the CPU CFS (Completely Fair Scheduler) quota -c, --cpu-shares int CPU shares (relative weight) --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) --disable-content-trust Skip image verification (default true) -f, --file string Name of the Dockerfile (Default is 'PATH/Dockerfile') --force-rm Always remove intermediate containers --iidfile string Write the image ID to the file --isolation string Container isolation technology --label list Set metadata for an image -m, --memory bytes Memory limit --memory-swap bytes Swap limit equal to memory plus swap: '-1' to enable unlimited swap --network string Set the networking mode for the RUN instructions during build (default \"default\") --no-cache Do not use cache when building the image --pull Always attempt to pull a newer version of the image -q, --quiet Suppress the build output and print image ID on success --rm Remove intermediate containers after a successful build (default true) --security-opt strings Security options --shm-size bytes Size of /dev/shm -t, --tag list Name and optionally a tag in the 'name:tag' format --target string Set the target build stage to build. --ulimit ulimit Ulimit options (default []) 参考：\nhttps://docs.docker.com/engine/reference/builder/ ","categories":"","description":"","excerpt":"1. Dockerfile的说明 dockerfile指令忽略大小写，建议大写，#作为注释，每行只支持一条指令，指令可以带多个参数。 …","ref":"/kubernetes-notes/runtime/docker/dockerfile-usage/","tags":["Docker"],"title":"Dockerfile使用说明"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/calico/","tags":"","title":"Calico"},{"body":"问题 configmap出现多行文本无法正常显示换行格式，而是以\\n连接文本，查看和编辑时可读性很差。\napiVersion: v1 data: config.yaml: \"# log options\\nlog_level: \\\"info\\\"\\nlog_output: \\\"stderr\\\"\\ncert_file: \\\"/etc/webhook/certs/cert.pem\\\"\\nkey_file: \\\"/etc/webhook/certs/key.pem\\\"\\nhttp_listen: \\\":8080\\\"\\nhttps_listen: \\\":8443\\\"\\ningress_publish_service: \\nenable_profiling: true\\nkubernetes:\\n kubeconfig: \\\"\\\"\\n resync_interval: \\\"6h\\\"\\n app_namespaces:\\n \\ - \\\"*\\\"\\n namespace_selector:\\n - \\\"\\\"\\n election_id: \\\"ingress-apisix-leader\\\"\\n \\ ingress_class: \\\"ph-apisix\\\"\\n ingress_version: \\\"networking/v1\\\"\\n watch_endpointslices: false\\n apisix_route_version: \\\"apisix.apache.org/v2beta3\\\"\\n enable_gateway_api: false\\napisix:\\n default_cluster_base_url: http://apisix-admin.apisix.svc.cluster.local:9180/apisix/admin\\n \\ default_cluster_admin_key: \\\"edd1c9f034335f136f87ad84b625c8f1\\\"\\n default_cluster_name: \\\"default\\\"\" kind: ConfigMap 解决方案 如果要保持多行输入和输出的格式，则需要符合以下情况：\n文本不要以空格结尾 不要换行前再带个空格 不要在文本中添加不可见特殊字符 将文本拷贝并格式化yaml文本。可使用在线格式化工具：YAML在线格式化。\n将格式化的文本拷贝到configmap文件，并检查上述三个问题。一般是因以空格结尾导致，搜索空格并去除行末的空格。\napiVersion: v1 data: config.yaml: |- # log options log_level: \"info\" log_output: \"stderr\" cert_file: \"/etc/webhook/certs/cert.pem\" key_file: \"/etc/webhook/certs/key.pem\" http_listen: \":8080\" https_listen: \":8443\" ingress_publish_service: enable_profiling: true 参考：\nhttps://kennylong.io/fix-yaml-multi-line-format/ ","categories":"","description":"","excerpt":"问题 configmap出现多行文本无法正常显示换行格式，而是以\\n连接文本，查看和编辑时可读性很差。\napiVersion: v1 …","ref":"/kubernetes-notes/trouble-shooting/configmap-yaml-format/","tags":["问题排查"],"title":"ConfigMap多行格式"},{"body":"Dynamic Volume Provisioning Dynamic volume provisioning允许用户按需自动创建存储卷，这种方式可以让用户不需要关心存储的复杂性和差别，又可以选择不同的存储类型。\n1. 开启Dynamic Provisioning 需要先提前创建StorageClass对象，StorageClass中定义了使用哪个provisioner，并且在provisioner被调用时传入哪些参数，具体可参考StorageClass介绍。\n例如：\n磁盘类存储 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/gce-pd parameters: type: pd-standard SSD类存储 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd 2. 使用Dynamic Provisioning 创建一个PVC对象，并且在其中storageClassName字段指明需要用到的StorageClass的名称，例如：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: fast resources: requests: storage: 30Gi 当使用到PVC的时候会自动创建对应的外部存储，当PVC被删除的时候，会自动销毁（或备份）外部存储。\n3. 默认的StorageClass 当没有对应的StorageClass配置时，可以设定默认的StorageClass，需要执行以下操作：\n在API Server开启DefaultStorageClass admission controller 。 设置默认的StorageClass对象。 可以通过添加storageclass.kubernetes.io/is-default-class注解的方式设置某个StorageClass为默认的StorageClass。当用户创建了一个PersistentVolumeClaim，但没有指定storageClassName的时候，会自动将该PVC的storageClassName指向默认的StorageClass。\n参考文章：\nhttps://kubernetes.io/docs/concepts/storage/dynamic-provisioning/ ","categories":"","description":"","excerpt":"Dynamic Volume Provisioning Dynamic volume provisioning允许用户按需自动创建存储卷，这 …","ref":"/kubernetes-notes/storage/volume/dynamic-provisioning/","tags":["Kubernetes"],"title":"Dynamic Volume Provisioning 介绍"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/gpu/","tags":"","title":"GPU"},{"body":"问题描述 容器启动报错：increase the mlock limit，原因是ulimit mlock值比较小，需要将ulimit值调大。\n报错如下：\nruntime: mlock of signal stack failed: 12 runtime: increase the mlock limit (ulimit -l) or runtime: update your kernel to 5.3.15+, 5.4.2+, or 5.5+ fatal error: mlock failed runtime stack: runtime.throw(0x1a7729f, 0xc) /usr/local/go/src/runtime/panic.go:1112 +0x72 runtime.mlockGsignal(0xc000702300) /usr/local/go/src/runtime/os_linux_x86.go:72 +0x107 runtime.mpreinit(0xc000588380) /usr/local/go/src/runtime/os_linux.go:341 +0x78 runtime.mcommoninit(0xc000588380) /usr/local/go/src/runtime/proc.go:630 +0x108 runtime.allocm(0xc000072000, 0x1adcb70, 0x0) /usr/local/go/src/runtime/proc.go:1390 +0x14e runtime.newm(0x1adcb70, 0xc000072000) /usr/local/go/src/runtime/proc.go:1704 +0x39 runtime.startm(0x0, 0xc000267e01) /usr/local/go/src/runtime/proc.go:1869 +0x12a runtime.wakep(...) /usr/local/go/src/runtime/proc.go:1953 runtime.resetspinning() /usr/local/go/src/runtime/proc.go:2415 +0x93 runtime.schedule() /usr/local/go/src/runtime/proc.go:2527 +0x2de runtime.mstart1() /usr/local/go/src/runtime/proc.go:1104 +0x8e runtime.mstart() /usr/local/go/src/runtime/proc.go:1062 +0x6e goroutine 1 [runnable, locked to thread]: github.com/xdg/stringprep.init() /root/go/pkg/mod/github.com/xdg/stringprep@v1.0.3/tables.go:443 +0x19087 goroutine 43 [select]: go.opencensus.io/stats/view.(*worker).start(0xc00067e800) /root/go/pkg/mod/go.opencensus.io@v0.23.0/stats/view/worker.go:276 +0x100 created by go.opencensus.io/stats/view.init.0 /root/go/pkg/mod/go.opencensus.io@v0.23.0/stats/view/worker.go:34 +0x68 原因 宿主机的ulimit值比较小，需要将内存的ulimit值调大。\n$ ulimit -l 64 解决方案 vi /lib/systemd/system/containerd.service。在containerd.service文件中增加LimitMEMLOCK=infinity 参数。\n[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitMEMLOCK=infinity LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 重启containerd\nsystemctl daemon-reload systemctl restart containerd systemctl status containerd ","categories":"","description":"","excerpt":"问题描述 容器启动报错：increase the mlock limit，原因是ulimit mlock值比较小，需要将ulimit值调大。 …","ref":"/kubernetes-notes/trouble-shooting/node/increase-the-mlock-limit/","tags":["问题排查"],"title":"increase the mlock limit"},{"body":"1. k8s版本号说明 k8s维护最新三个版本的发布分支（[2022.7.2]当前最新三个版本为1.24、1.23、1.22），Kubernetes 1.19 和更新的版本获得大约 1 年的补丁支持。\nKubernetes 版本表示为 x.y.z， 其中 x 是主要版本，y 是次要版本，z 是补丁版本。遵循语义化版本规范。\n2. 版本偏差策略 2.1. 支持的版本偏差 总结：\nkubelet 版本不能比 kube-apiserver 版本新，最多只可落后两个次要版本。\nkube-controller-manager、kube-scheduler 和 cloud-controller-manager 不能比 kube-apiserver 版本新。最多落后一个次要版本（允许实时升级）。\nkubectl 在 kube-apiserver 的一个次要版本（较旧或较新）中支持。\nkube-proxy 和节点上的 kubelet 必须是相同的次要版本。\n1）kube-apiserver 在高可用性（HA）集群中， 最新版和最老版的 kube-apiserver 实例版本偏差最多为一个次要版本。\n例如：\n最新的 kube-apiserver 实例处于 1.24 版本 其他 kube-apiserver 实例支持 1.24 和 1.23 版本 2）kubelet kubelet 版本不能比 kube-apiserver 版本新，并且最多只可落后两个次要版本。\n例如：\nkube-apiserver 处于 1.24 版本 kubelet 支持 1.24、1.23 和 1.22 版本 说明：\n如果 HA 集群中的 kube-apiserver 实例之间存在版本偏差，这会缩小允许的 kubelet 版本范围。\n例如：\nkube-apiserver 实例处于 1.24 和 1.23 版本 kubelet 支持 1.23 和 1.22 版本， （不支持 1.24 版本，因为这将比 kube-apiserver 1.23 版本的实例新） 3）kube-controller-manager、kube-scheduler 和 cloud-controller-manager kube-controller-manager、kube-scheduler 和 cloud-controller-manager 不能比与它们通信的 kube-apiserver 实例新。 它们应该与 kube-apiserver 次要版本相匹配，但可能最多旧一个次要版本（允许实时升级）。\n例如：\nkube-apiserver 处于 1.24 版本 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 支持 1.24 和 1.23 版本 说明：\n如果 HA 集群中的 kube-apiserver 实例之间存在版本偏差， 并且这些组件可以与集群中的任何 kube-apiserver 实例通信（例如，通过负载均衡器），这会缩小这些组件所允许的版本范围。\n例如：\nkube-apiserver 实例处于 1.24 和 1.23 版本 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 与可以路由到任何 kube-apiserver 实例的负载均衡器通信 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 支持 1.23 版本（不支持 1.24 版本，因为它比 1.23 版本的 kube-apiserver 实例新） 4）kubectl kubectl 在 kube-apiserver 的一个次要版本（较旧或较新）中支持。\n例如：\nkube-apiserver 处于 1.24 版本 kubectl 支持 1.25、1.24 和 1.23 版本 说明：\n如果 HA 集群中的 kube-apiserver 实例之间存在版本偏差，这会缩小支持的 kubectl 版本范围。\n例如：\nkube-apiserver 实例处于 1.24 和 1.23 版本 kubectl 支持 1.24 和 1.23 版本（其他版本将与 kube-apiserver 组件之一相差不止一个的次要版本） 5）kube-proxy kube-proxy 和节点上的 kubelet 必须是相同的次要版本。 kube-proxy 版本不能比 kube-apiserver 版本新。 kube-proxy 最多只能比 kube-apiserver 落后两个次要版本。 例如：\n如果 kube-proxy 版本处于 1.22 版本：\nkubelet 必须处于相同的次要版本 1.22。 kube-apiserver 版本必须介于 1.22 和 1.24 之间，包括两者。 2.2. 组件升级顺序 优先升级kube-apiserver，其他的组件按照上述的版本要求进行升级，最好保持一致的版本。\n3. k8s版本发布周期 k8s每年大概发布三次，即3-4个月发布一次大版本（发布版本为 vX.Y 里程碑创建的 Git 分支 release-X.Y）。\n发布过程可被认为具有三个主要阶段：\n特性增强定义 实现 稳定 3.1. 发布周期 1）正常开发（第 1-11 周）\n/sig {name}\n/sig {name}\n/kind {type}\n/lgtm\n/approved\n2）代码冻结（第 12-14 周）\n/milestone {v1.y} /sig {name} /kind {bug, failing-test} /lgtm /approved 3）发布后（第 14 周以上）\n回到“正常开发”阶段要求：\n/sig {name} /kind {type} /lgtm /approved 参考： 发行版本 | Kubernetes\nkubernetes/CHANGELOG at master · GitHub\nsig-release/releases at master · kubernetes/sig-release · GitHub\ndesign-proposals-archive/versioning.md at main · kubernetes/design-proposals-archive · GitHub\nKubernetes 发布周期 | Kubernetes\n","categories":"","description":"","excerpt":"1. k8s版本号说明 k8s维护最新三个版本的发布分支（[2022.7.2]当前最新三个版本 …","ref":"/kubernetes-notes/setup/k8s-version-release/","tags":["Kubernetes"],"title":"k8s版本说明"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kubelet/","tags":"","title":"kubelet"},{"body":"创建用户名和密码 # 通过写文件创建用户，/home/mybot是用户目录 echo \"mybot:x:1001:1001::/home/mybot:/bin/bash\" \u003e\u003e /etc/passwd # 给用户创建密码 echo \"mybot:{password}\" | chpasswd # 指定文件系统根目录创建密码，例如：/mnt/dev/sda，一般为外挂文件系统，并没有chroot echo \"mybot:{password}\" | chpasswd -R /mnt/dev/sda # 给用户分配sudo权限，NOPASSWD表示不需要root密码执行sudo echo \"mybot ALL=(root) NOPASSWD: ALL\" \u003e /etc/sudoers.d/mybot # 设置允许ssh用户密码登录 sudo sed -i 's/^#\\?PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config # 设置不允许ssh密码登录 sudo sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config # 不重启ssh服务使得配置修改生效 sudo systemctl reload sshd ","categories":"","description":"","excerpt":"创建用户名和密码 # 通过写文件创建用户，/home/mybot是用户目录 echo …","ref":"/linux-notes/file/linux-command/","tags":["Linux"],"title":"Linux常用命令"},{"body":" 本文来自redis 官方配置文件\n# Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k =\u003e 1000 bytes # 1kb =\u003e 1024 bytes # 1m =\u003e 1000000 bytes # 1mb =\u003e 1024*1024 bytes # 1g =\u003e 1000000000 bytes # 1gb =\u003e 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. INCLUDES ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\" # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf MODULES ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so NETWORK ################################## NETWORK ##################################### # By default, if no \"bind\" configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \"bind\" configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 lookback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \"bind\" directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \"bind\" directive. protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 GENERAL ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize yes # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \"process is ready.\" # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \"/var/run/redis.pid\". # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \"\" # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT \u003cdbid\u003e where # dbid is a number between 0 and 'databases'-1 databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes SNAPSHOTTING ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save \u003cseconds\u003e \u003cchanges\u003e # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \"save\" lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \"\" save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that's set to 'yes' as it's almost always a win. # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./ REPLICATION ################################# REPLICATION ################################# # Master-Slave replication. Use slaveof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of slaves. # 2) Redis slaves are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition slaves automatically try to reconnect to masters # and resynchronize with them. # # slaveof \u003cmasterip\u003e \u003cmasterport\u003e # If the master is password protected (using the \"requirepass\" configuration # directive below) it is possible to tell the slave to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the slave request. # # masterauth \u003cmaster-password\u003e # When a slave loses its connection with the master, or when the replication # is still in progress, the slave can act in two different ways: # # 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if slave-serve-stale-data is set to 'no' the slave will reply with # an error \"SYNC with master in progress\" to all the kind of commands # but to INFO and SLAVEOF. # slave-serve-stale-data yes # You can configure a slave instance to accept writes or not. Writing against # a slave instance may be useful to store some ephemeral data (because data # written on a slave will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default slaves are read-only. # # Note: read only slaves are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only slave exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only slaves using 'rename-command' to shadow all the # administrative / dangerous commands. slave-read-only yes # Replication SYNC strategy: disk or socket. # # ------------------------------------------------------- # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY # ------------------------------------------------------- # # New slaves and reconnecting slaves that are not able to continue the replication # process just receiving differences, need to do what is called a \"full # synchronization\". An RDB file is transmitted from the master to the slaves. # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the slaves incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to slave sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more slaves # can be queued and served with the RDB file as soon as the current child producing # the RDB file finishes its work. With diskless replication instead once # the transfer starts, new slaves arriving will be queued and a new transfer # will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple slaves # will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the slaves. # # This is important since once the transfer starts, it is not possible to serve # new slaves arriving, that will be queued for the next RDB transfer, so the server # waits a delay in order to let more slaves arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # Slaves send PINGs to server in a predefined interval. It's possible to change # this interval with the repl_ping_slave_period option. The default value is 10 # seconds. # # repl-ping-slave-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of slave. # 2) Master timeout from the point of view of slaves (data, pings). # 3) Slave timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-slave-period otherwise a timeout will be detected # every time there is low traffic between the master and the slave. # # repl-timeout 60 # Disable TCP_NODELAY on the slave socket after SYNC? # # If you select \"yes\" Redis will use a smaller number of TCP packets and # less bandwidth to send data to slaves. But this can add a delay for # the data to appear on the slave side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \"no\" the delay for data to appear on the slave side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and slaves are many hops away, turning this to \"yes\" may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # slave data when slaves are disconnected for some time, so that when a slave # wants to reconnect again, often a full resync is not needed, but a partial # resync is enough, just passing the portion of data the slave missed while # disconnected. # # The bigger the replication backlog, the longer the time the slave can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a slave connected. # # repl-backlog-size 1mb # After a master has no longer connected slaves for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last slave disconnected, for # the backlog buffer to be freed. # # Note that slaves never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \"partially # resynchronize\" with the slaves: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The slave priority is an integer number published by Redis in the INFO output. # It is used by Redis Sentinel in order to select a slave to promote into a # master if the master is no longer working correctly. # # A slave with a low priority number is considered better for promotion, so # for instance if there are three slaves with priority 10, 100, 25 Sentinel will # pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the slave as not able to perform the # role of master, so a slave with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. slave-priority 100 # It is possible for a master to stop accepting writes if there are less than # N slaves connected, having a lag less or equal than M seconds. # # The N slaves need to be in \"online\" state. # # The lag in seconds, that must be \u003c= the specified value, is calculated from # the last ping received from the slave, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough slaves # are available, to the specified number of seconds. # # For example to require at least 3 slaves with a lag \u003c= 10 seconds use: # # min-slaves-to-write 3 # min-slaves-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-slaves-to-write is set to 0 (feature disabled) and # min-slaves-max-lag is set to 10. # A Redis master is able to list the address and port of the attached # slaves in different ways. For example the \"INFO replication\" section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover slave instances. # Another place where this info is available is in the output of the # \"ROLE\" command of a master. # # The listed IP and address normally reported by a slave is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the slave to connect with the master. # # Port: The port is communicated by the slave during the replication # handshake, and is normally the port that the slave is using to # list for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the slave may be actually reachable via different IP and port # pairs. The following two options can be used by a slave in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # slave-announce-ip 5.5.5.5 # slave-announce-port 1234 SECURITY ################################## SECURITY ################################### # Require clients to issue AUTH \u003cPASSWORD\u003e before processing any other # commands. This might be useful in environments in which you do not trust # others with access to the host running redis-server. # # This should stay commented out for backward compatibility and because most # people do not need auth (e.g. they run their own servers). # # Warning: since Redis is pretty fast an outside user can try up to # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. # # requirepass foobared # Command renaming. # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \"\" # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to slaves may cause problems. CLIENTS ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # # maxclients 10000 MEMORY MANAGEMENT ############################## MEMORY MANAGEMENT ################################ # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have slaves attached to an instance with maxmemory on, # the size of the output buffers needed to feed the slaves are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of slaves is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have slaves attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for slave # output buffers (but this is not needed if the policy is 'noeviction'). # # maxmemory \u003cbytes\u003e # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select among five behaviors: # # volatile-lru -\u003e Evict using approximated LRU among the keys with an expire set. # allkeys-lru -\u003e Evict any key using approximated LRU. # volatile-lfu -\u003e Evict using approximated LFU among the keys with an expire set. # allkeys-lfu -\u003e Evict any key using approximated LFU. # volatile-random -\u003e Remove a random key among the ones with an expire set. # allkeys-random -\u003e Remove a random key, any key. # volatile-ttl -\u003e Remove the key with the nearest expire time (minor TTL) # noeviction -\u003e Don't evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5 LAZY FREEING ############################# LAZY FREEING #################################### # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 4) During replication, when a slave performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transfered. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives: lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no slave-lazy-flush no APPEND ONLY MODE ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \"appendonly.aof\") appendfilename \"appendonly.aof\" # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don't fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \"everysec\", as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # \"no\" that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use \"always\" that's very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \"everysec\". # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \"appendfsync none\". In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \"yes\". Otherwise leave it as # \"no\" that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \"redis-check-aof\" utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \"REDIS\" # string and loads the prefixed RDB file, and continues loading the AOF # tail. # # This is currently turned off by default in order to avoid the surprise # of a format change, but will at some point be used as the default. aof-use-rdb-preamble no LUA SCRIPTING ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 REDIS CLUSTER ################################ REDIS CLUSTER ############################### # # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however # in order to mark it as \"mature\" we need to wait for a non trivial percentage # of users to deploy it in production. # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A slave of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a slave to actually have an exact measure of # its \"data age\", so the following two checks are performed: # # 1) If there are multiple slaves able to failover, they exchange messages # in order to try to give an advantage to the slave with the best # replication offset (more data from the master processed). # Slaves will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single slave computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \"connected\" state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the slave will not try to failover # at all. # # The point \"2\" can be tuned by user. Specifically a slave will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * slave-validity-factor) + repl-ping-slave-period # # So for example if node-timeout is 30 seconds, and the slave-validity-factor # is 10, and assuming a default repl-ping-slave-period of 10 seconds, the # slave will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large slave-validity-factor may allow slaves with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a slave at all. # # For maximum availability, it is possible to set the slave-validity-factor # to a value of 0, which means, that slaves will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-slave-validity-factor 10 # Cluster slaves are able to migrate to orphaned masters, that are masters # that are left without working slaves. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working slaves. # # Slaves migrate to orphaned masters only if there are still at least a # given number of other working slaves for their old master. This number # is the \"migration barrier\". A migration barrier of 1 means that a slave # will migrate only if there is at least 1 other working slave for its master # and so forth. It usually reflects the number of slaves you want for every # master in your cluster. # # Default is 1 (slaves migrate only if their masters remain with at least # one slave). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. CLUSTER DOCKER/NAT support ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 SLOW LOG ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 LATENCY MONITOR ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \"CONFIG SET latency-monitor-threshold \u003cmilliseconds\u003e\" if needed. latency-monitor-threshold 0 EVENT NOTIFICATION ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \"foo\" stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@\u003cdb\u003e__ prefix. # E Keyevent events, published with __keyevent@\u003cdb\u003e__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # A Alias for g$lshzxe, so that the \"AKE\" string means all the events. # # The \"notify-keyspace-events\" takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don't need # this feature and the feature has some overhead. Note that if you don't # specify at least one of K or E, no events will be delivered. notify-keyspace-events \"\" ADVANCED CONFIG ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb \u003c-- not recommended for normal workloads # -4: max size: 32 Kb \u003c-- not recommended # -3: max size: 16 Kb \u003c-- probably not recommended # -2: max size: 8 Kb \u003c-- good # -1: max size: 4 Kb \u003c-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \"don't start compressing until after 1 node into the list, # going from either the head or tail\" # So: [head]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]-\u003e[next]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[prev]-\u003e[tail] # 2 here means: don't compress head or head-\u003enext or tail-\u003eprev or tail, # but compress all nodes between them. # 3: [head]-\u003e[next]-\u003e[next]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[prev]-\u003e[prev]-\u003e[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \"steps\" are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \"activerehashing no\" if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \"activerehashing yes\" if you don't have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -\u003e normal clients including MONITOR clients # slave -\u003e slave clients # pubsub -\u003e clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit \u003cclass\u003e \u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and slave clients, since # subscribers and slaves receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here. # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \"hz\" value. # # By default \"hz\" is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R \u003c P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits | # +--------+------------+------------+------------+------------+------------+ # | 0 | 104 | 255 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 1 | 18 | 49 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 10 | 10 | 18 | 142 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 100 | 8 | 11 | 49 | 143 | 255 | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # # redis-benchmark -n 1000000 incr foo # redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less \u003c= 10). # # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # # lfu-log-factor 10 # lfu-decay-time 1 ACTIVE DEFRAGMENTATION ########################### ACTIVE DEFRAGMENTATION ####################### # # WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested # even in production and manually tested by multiple engineers for some # time. # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \"hot\" way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \"CONFIG SET activedefrag yes\". # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag yes # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage # active-defrag-cycle-min 25 # Maximal effort for defrag in CPU percentage # active-defrag-cycle-max 75 ","categories":"","description":"","excerpt":" 本文来自redis 官方配置文件\n# Redis configuration file example. # # Note that in …","ref":"/linux-notes/redis/redis-conf-en/","tags":["Redis"],"title":"Redis配置详解（英文版）"},{"body":"本文主要分析replicaset-controller的源码逻辑，replicas对象创建主要是由deployment-controller中封装。而replicas是pod的维护控制器。可以把replicas理解为deployment中的版本控制器，该控制器封装每次版本的pod对象。\n1. startReplicaSetController startReplicaSetController函数是ReplicaSetController的入口函数。基本的操作即new controller对象，然后起一个goroutine运行run函数。\nfunc startReplicaSetController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) { go replicaset.NewReplicaSetController( klog.FromContext(ctx), controllerContext.InformerFactory.Apps().V1().ReplicaSets(), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.ClientBuilder.ClientOrDie(\"replicaset-controller\"), replicaset.BurstReplicas, ).Run(ctx, int(controllerContext.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs)) return nil, true, nil } 2. NewReplicaSetController NewReplicaSetController初始化controller对象，最终通过NewBaseController实现具体的初始化操作。\n// NewReplicaSetController configures a replica set controller with the specified event recorder func NewReplicaSetController(logger klog.Logger, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int) *ReplicaSetController { eventBroadcaster := record.NewBroadcaster() if err := metrics.Register(legacyregistry.Register); err != nil { logger.Error(err, \"unable to register metrics\") } return NewBaseController(logger, rsInformer, podInformer, kubeClient, burstReplicas, apps.SchemeGroupVersion.WithKind(\"ReplicaSet\"), \"replicaset_controller\", \"replicaset\", controller.RealPodControl{ KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"replicaset-controller\"}), }, eventBroadcaster, ) } 2.1. NewBaseController NewBaseController是一个常见的k8s controller构建函数，主要包括以下几个部分：\n初始化常用client，包括kube client 添加event handler对象。 添加informer索引。 添加informer syncCache的函数，在处理controller逻辑前先同步一下etcd的数据到本地cache。 赋值syncHandler函数，是具体实现controller逻辑的函数。 func NewBaseController(logger klog.Logger, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.PodControlInterface, eventBroadcaster record.EventBroadcaster) *ReplicaSetController { // 初始化常用配置 rsc := \u0026ReplicaSetController{ GroupVersionKind: gvk, kubeClient: kubeClient, podControl: podControl, eventBroadcaster: eventBroadcaster, burstReplicas: burstReplicas, expectations: controller.NewUIDTrackingControllerExpectations(controller.NewControllerExpectations()), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), queueName), } // 添加event handler对象 rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { rsc.addRS(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { rsc.updateRS(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { rsc.deleteRS(logger, obj) }, }) // 添加informer索引 rsInformer.Informer().AddIndexers(cache.Indexers{ controllerUIDIndex: func(obj interface{}) ([]string, error) { rs, ok := obj.(*apps.ReplicaSet) if !ok { return []string{}, nil } controllerRef := metav1.GetControllerOf(rs) if controllerRef == nil { return []string{}, nil } return []string{string(controllerRef.UID)}, nil }, }) rsc.rsIndexer = rsInformer.Informer().GetIndexer() rsc.rsLister = rsInformer.Lister() // 初始化informer的sync函数 rsc.rsListerSynced = rsInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { rsc.addPod(logger, obj) }, // This invokes the ReplicaSet for every pod change, eg: host assignment. Though this might seem like // overkill the most frequent pod update is status, and the associated ReplicaSet will only list from // local storage, so it should be ok. UpdateFunc: func(oldObj, newObj interface{}) { rsc.updatePod(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { rsc.deletePod(logger, obj) }, }) rsc.podLister = podInformer.Lister() rsc.podListerSynced = podInformer.Informer().HasSynced // 初始化syncHandler函数，该函数为具体实现controller业务逻辑的函数。 rsc.syncHandler = rsc.syncReplicaSet return rsc } 3. Run Run函数仍然是k8s controller的代码风格。主要包含了以下几个部分。\n同步本地cache内容 运行多个不退出的goroutine处理控制器逻辑。 func (rsc *ReplicaSetController) Run(ctx context.Context, workers int) { // 已删除非核心逻辑代码。 defer rsc.queue.ShutDown() // 处理前同步下cache的内容。 if !cache.WaitForNamedCacheSync(rsc.Kind, ctx.Done(), rsc.podListerSynced, rsc.rsListerSynced) { return } // 运行指定个数的goroutine来处理controller逻辑。 for i := 0; i \u003c workers; i++ { go wait.UntilWithContext(ctx, rsc.worker, time.Second) } \u003c-ctx.Done() } 3.1. processNextWorkItem processNextWorkItemz主要运行syncHandler函数，和对返回的错误进行处理。\n如果错误为空，则不再入队。 如果错误不为空，则入队重新处理。 func (rsc *ReplicaSetController) worker(ctx context.Context) { for rsc.processNextWorkItem(ctx) { } } func (rsc *ReplicaSetController) processNextWorkItem(ctx context.Context) bool { key, quit := rsc.queue.Get() if quit { return false } defer rsc.queue.Done(key) // 具体的逻辑代码由syncHandler实现。 err := rsc.syncHandler(ctx, key.(string)) if err == nil { // 如果错误为空，则不再入队 rsc.queue.Forget(key) return true } utilruntime.HandleError(fmt.Errorf(\"sync %q failed with %v\", key, err)) // 如果错误不为空，则重新入队 rsc.queue.AddRateLimited(key) return true } 4. syncReplicaSet syncReplicaSet是syncHandler的具体实现，常见的syncHandler的实现包含以下几个部分\n获取集群中的controller对象，例如 rs。 获取该controller对象及其子对象的当前状态。 对比当前状态与预期状态是否一致。 更新当前状态，以上循环直到当前状态达到期望状态。 func (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error { // 已删除非核心代码 namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return err } // 获取集群中的rs对象 rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) if apierrors.IsNotFound(err) { logger.V(4).Info(\"deleted\", \"kind\", rsc.Kind, \"key\", key) rsc.expectations.DeleteExpectations(logger, key) return nil } rsNeedsSync := rsc.expectations.SatisfiedExpectations(logger, key) // 获取指定selector下pod selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector) allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything()) filteredPods := controller.FilterActivePods(logger, allPods) filteredPods, err = rsc.claimPods(ctx, rs, selector, filteredPods) // 处理replica逻辑 var manageReplicasErr error if rsNeedsSync \u0026\u0026 rs.DeletionTimestamp == nil { manageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs) } rs = rs.DeepCopy() newStatus := calculateStatus(rs, filteredPods, manageReplicasErr) // 更新状态 updatedRS, err := updateReplicaSetStatus(logger, rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus) // Resync the ReplicaSet after MinReadySeconds as a last line of defense to guard against clock-skew. if manageReplicasErr == nil \u0026\u0026 updatedRS.Spec.MinReadySeconds \u003e 0 \u0026\u0026 updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) \u0026\u0026 updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) { rsc.queue.AddAfter(key, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second) } return manageReplicasErr } 5. manageReplicas manageReplicas主要实现pod的创建和删除，从而保证当前rs下的pod跟预期的一致。\nfunc (rsc *ReplicaSetController) manageReplicas(ctx context.Context, filteredPods []*v1.Pod, rs *apps.ReplicaSet) error { // 计算当前的pod数量和预期的pod是否一致。 diff := len(filteredPods) - int(*(rs.Spec.Replicas)) rsKey, err := controller.KeyFunc(rs) // 如果少于预期 if diff \u003c 0 { // 则批量创建pod successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error { err := rsc.podControl.CreatePods(ctx, rs.Namespace, \u0026rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind)) }) // 如果pod数量多于预期 } else if diff \u003e 0 { relatedPods, err := rsc.getIndirectlyRelatedPods(logger, rs) utilruntime.HandleError(err) // Choose which Pods to delete, preferring those in earlier phases of startup. podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff) errCh := make(chan error, diff) var wg sync.WaitGroup wg.Add(diff) for _, pod := range podsToDelete { go func(targetPod *v1.Pod) { defer wg.Done() // 批量删除pod if err := rsc.podControl.DeletePod(ctx, rs.Namespace, targetPod.Name, rs); err != nil { } }(pod) } wg.Wait() // 处理错误 select { case err := \u003c-errCh: // all errors have been reported before and they're likely to be the same, so we'll only return the first one we hit. if err != nil { return err } default: } } return nil } 总结 replicaset-controller的代码逻辑相对简单，基本的代码风格是k8s控制器通用的代码逻辑，由于k8s的代码风格高度一致，因此如果读清楚一类controller的控制逻辑。其他的控制器的代码逻辑大同小异。\n参考：\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replicaset/replica_set.go ","categories":"","description":"","excerpt":"本文主要分析replicaset-controller的源码逻辑，replicas对象创建主要是由deployment-controller …","ref":"/k8s-source-code-analysis/kube-controller-manager/replicaset-controller/","tags":["源码分析"],"title":"kube-controller-manager源码分析（四）之 ReplicaSetController"},{"body":"1. echo echo是Shell的一个内部指令，用于在屏幕上打印出指定的字符串。命令格式：\necho arg 您可以使用echo实现更复杂的输出格式控制。\n1.1. 显示转义字符 echo \"\\\"It is a test\\\"\" 结果将是：\n\"It is a test\" 双引号也可以省略。\n1.2. 显示变量 name=\"OK\" echo \"$name It is a test\" 结果将是：\nOK It is a test 同样双引号也可以省略。\n如果变量与其它字符相连的话，需要使用大括号（{ }）：\nmouth=8 echo \"${mouth}-1-2009\" 结果将是：\n8-1-2009 1.3. 显示换行 echo \"OK!\\n\" echo \"It is a test\" 输出：\nOK! It is a test 1.4. 显示不换行 echo \"OK!\\c\" echo \"It is a test\" 输出：\nOK!It si a test 1.5. 显示结果重定向至文件 echo \"It is a test\" \u003e myfile 1.6. 原样输出字符串 若需要原样输出字符串（不进行转义），请使用单引号。例如：\necho '$name\\\"' 1.7. 显示命令执行结果 echo `date` 结果将显示当前日期 从上面可看出，双引号可有可无，单引号主要用在原样输出中。\n2. printf printf 命令用于格式化输出， 是echo命令的增强版。它是C语言printf()库函数的一个有限的变形，并且在语法上有些不同。 printf 不像 echo 那样会自动换行，必须显式添加换行符(\\n)。 注意：printf 由 POSIX 标准所定义，移植性要比 echo 好。\nprintf 命令的语法：\nprintf format-string [arguments...] format-string 为格式控制字符串，arguments 为参数列表。\nprintf()功能和用法与 printf 命令类似\n这里仅说明与C语言printf()函数的不同：\nprintf 命令不用加括号 format-string 可以没有引号，但最好加上，单引号双引号均可。 参数多于格式控制符(%)时，format-string 可以重用，可以将所有参数都转换。 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用 arguments 使用空格分隔，不用逗号。 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替\n如果以 %d 的格式来显示字符串，那么会有警告，提示无效的数字，此时默认置为 0\n# format-string为双引号,单引号与双引号效果一样,没有引号也可以输出 $ printf \"%d %s\\n\" 1 \"abc\" 1 abc 注意，根据POSIX标准，浮点格式%e、%E、%f、%g与%G是“不需要被支持”。这是因为awk支持浮点预算，且有它自己的printf语句。这样Shell程序中需要将浮点数值进行格式化的打印时，可使用小型的awk程序实现。然而，内建于bash、ksh93和zsh中的printf命令都支持浮点格式。\n参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. echo echo是Shell的一个内部指令，用于在屏幕上打印出指定的字符串。命令格式：\necho arg 您可以使用echo实现更复 …","ref":"/linux-notes/shell/shell-echo/","tags":["Shell"],"title":"Shell echo命令"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/shell/","tags":"","title":"Shell脚本"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析kubelet中syncLoopIteration部分。syncLoopIteration通过几种channel来对不同类型的事件进行监听并做增删改查的处理。\n1. syncLoop syncLoop是处理变更的循环。 它监听来自三种channel（file，apiserver和http）的更改。 对于看到的任何新更改，将针对所需状态和运行状态运行同步。 如果没有看到配置的变化，将在每个同步频率秒同步最后已知的所需状态。\n此部分代码位于pkg/kubelet/kubelet.go\n// syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates \u003c-chan kubetypes.PodUpdate, handler SyncHandler) { glog.Info(\"Starting kubelet main sync loop.\") // The resyncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync'd. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 { glog.Infof(\"skipping pod synchronization - %v\", rs) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } 其中调用了syncLoopIteration的函数来执行更具体的监控pod变化的循环。\n2. syncLoopIteration syncLoopIteration主要通过几种channel来对不同类型的事件进行监听并处理。其中包括：configCh、plegCh、syncCh、houseKeepingCh、livenessManager.Updates()。\nsyncLoopIteration实际执行了pod的操作，此部分设置了几种不同的channel:\nconfigCh：将配置更改的pod分派给事件类型的相应处理程序回调。 plegCh：更新runtime缓存，同步pod。 syncCh：同步所有等待同步的pod。 houseKeepingCh：触发清理pod。 livenessManager.Updates()：对失败的pod或者liveness检查失败的pod进行sync操作。 syncLoopIteration部分代码位于pkg/kubelet/kubelet.go\n2.1. configCh configCh将配置更改的pod分派给事件类型的相应处理程序回调，该部分主要通过SyncHandler对pod的不同事件进行增删改查等操作。\nfunc (kl *Kubelet) syncLoopIteration(configCh \u003c-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u003c-chan time.Time, housekeepingCh \u003c-chan time.Time, plegCh \u003c-chan *pleg.PodLifecycleEvent) bool { select { case u, open := \u003c-configCh: // Update from a config source; dispatch it to the right handler // callback. if !open { glog.Errorf(\"Update channel is closed. Exiting the sync loop.\") return false } switch u.Op { case kubetypes.ADD: glog.V(2).Infof(\"SyncLoop (ADD, %q): %q\", u.Source, format.Pods(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: glog.V(2).Infof(\"SyncLoop (UPDATE, %q): %q\", u.Source, format.PodsWithDeletionTimestamps(u.Pods)) handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: glog.V(2).Infof(\"SyncLoop (REMOVE, %q): %q\", u.Source, format.Pods(u.Pods)) handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: glog.V(4).Infof(\"SyncLoop (RECONCILE, %q): %q\", u.Source, format.Pods(u.Pods)) handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: glog.V(2).Infof(\"SyncLoop (DELETE, %q): %q\", u.Source, format.Pods(u.Pods)) // DELETE is treated as a UPDATE because of graceful deletion. handler.HandlePodUpdates(u.Pods) case kubetypes.RESTORE: glog.V(2).Infof(\"SyncLoop (RESTORE, %q): %q\", u.Source, format.Pods(u.Pods)) // These are pods restored from the checkpoint. Treat them as new // pods. handler.HandlePodAdditions(u.Pods) case kubetypes.SET: // TODO: Do we want to support this? glog.Errorf(\"Kubelet does not support snapshot update\") } ... } 可以看出syncLoopIteration根据podUpdate的值来执行不同的pod操作，具体如下：\nADD：HandlePodAdditions UPDATE：HandlePodUpdates REMOVE：HandlePodRemoves RECONCILE：HandlePodReconcile DELETE：HandlePodUpdates RESTORE：HandlePodAdditions podsToSync：HandlePodSyncs 其中执行pod的handler操作的是SyncHandler，该类型是一个接口，实现体为kubelet本身，具体见后续分析。\n2.2. plegCh plegCh：更新runtime缓存，同步pod。此处调用了HandlePodSyncs的函数。\ncase e := \u003c-plegCh: if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { glog.V(2).Infof(\"SyncLoop (PLEG): %q, event: %#v\", format.Pod(pod), e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. glog.V(4).Infof(\"SyncLoop (PLEG): ignore irrelevant event: %#v\", e) } } if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } 2.3. syncCh syncCh：同步所有等待同步的pod。此处调用了HandlePodSyncs的函数。\ncase \u003c-syncCh: // Sync pods waiting for sync podsToSync := kl.getPodsToSync() if len(podsToSync) == 0 { break } glog.V(4).Infof(\"SyncLoop (SYNC): %d pods; %s\", len(podsToSync), format.Pods(podsToSync)) handler.HandlePodSyncs(podsToSync) 2.4. livenessManager.Update livenessManager.Updates()：对失败的pod或者liveness检查失败的pod进行sync操作。此处调用了HandlePodSyncs的函数。\ncase update := \u003c-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { // The liveness manager detected a failure; sync the pod. // We should not use the pod from livenessManager, because it is never updated after // initialization. pod, ok := kl.podManager.GetPodByUID(update.PodUID) if !ok { // If the pod no longer exists, ignore the update. glog.V(4).Infof(\"SyncLoop (container unhealthy): ignore irrelevant update: %#v\", update) break } glog.V(1).Infof(\"SyncLoop (container unhealthy): %q\", format.Pod(pod)) handler.HandlePodSyncs([]*v1.Pod{pod}) } 2.5. housekeepingCh houseKeepingCh：触发清理pod。此处调用了HandlePodCleanups的函数。\ncase \u003c-housekeepingCh: if !kl.sourcesReady.AllReady() { // If the sources aren't ready or volume manager has not yet synced the states, // skip housekeeping, as we may accidentally delete pods from unready sources. glog.V(4).Infof(\"SyncLoop (housekeeping, skipped): sources aren't ready yet.\") } else { glog.V(4).Infof(\"SyncLoop (housekeeping)\") if err := handler.HandlePodCleanups(); err != nil { glog.Errorf(\"Failed cleaning pods: %v\", err) } } 3. SyncHandler SyncHandler是一个定义Pod的不同Handler的接口，具体是实现者是kubelet，该接口的方法主要在syncLoopIteration中调用，接口定义如下：\n// SyncHandler is an interface implemented by Kubelet, for testability type SyncHandler interface { HandlePodAdditions(pods []*v1.Pod) HandlePodUpdates(pods []*v1.Pod) HandlePodRemoves(pods []*v1.Pod) HandlePodReconcile(pods []*v1.Pod) HandlePodSyncs(pods []*v1.Pod) HandlePodCleanups() error } SyncHandler部分代码位于pkg/kubelet/kubelet.go\n3.1. HandlePodAdditions HandlePodAdditions先根据pod创建时间对pod进行排序，然后遍历pod列表，来执行pod的相关操作。\n// HandlePodAdditions is the callback in SyncHandler for pods being added from // a config source. func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods { ... } } 将pod添加到pod manager中。\nfor _, pod := range pods { // Responsible for checking limits in resolv.conf if kl.dnsConfigurer != nil \u0026\u0026 kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } existingPods := kl.podManager.GetPods() // Always add the pod to the pod manager. Kubelet relies on the pod // manager as the source of truth for the desired state. If a pod does // not exist in the pod manager, it means that it has been deleted in // the apiserver and no action (other than cleanup) is required. kl.podManager.AddPod(pod) ... } 如果是mirror pod，则对mirror pod进行处理。\nif kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } 如果当前pod的状态不是Terminated状态，则判断是否接受该pod，如果不接受则将pod状态改为Failed。\nif !kl.podIsTerminated(pod) { // Only go through the admission process if the pod is not // terminated. // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutTerminatedPods(existingPods) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue } } 执行dispatchWork函数，该函数是syncHandler中调用到的核心函数，该函数在pod worker中启动一个异步循环，来分派pod的相关操作。该函数的具体操作待后续分析。\nmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) 最后加pod添加到probe manager中。\nkl.probeManager.AddPod(pod) 3.2. HandlePodUpdates HandlePodUpdates同样遍历pod列表，执行相应的操作。\n// HandlePodUpdates is the callback in the SyncHandler interface for pods // being updated from a config source. func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } 将pod更新到pod manager中。\nfor _, pod := range pods { // Responsible for checking limits in resolv.conf if kl.dnsConfigurer != nil \u0026\u0026 kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } kl.podManager.UpdatePod(pod) ... } 如果是mirror pod，则对mirror pod进行处理。\nif kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } 执行dispatchWork函数。\n// TODO: Evaluate if we need to validate and reject updates. mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodUpdate, mirrorPod, start) 3.3. HandlePodRemoves HandlePodRemoves遍历pod列表。\n// HandlePodRemoves is the callback in the SyncHandler interface for pods // being removed from a config source. func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } 从pod manager中删除pod。\nfor _, pod := range pods { kl.podManager.DeletePod(pod) ... } 如果是mirror pod，则对mirror pod进行处理。\nif kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } 调用kubelet的deletePod函数来删除pod。\n// Deletion is allowed to fail because the periodic cleanup routine // will trigger deletion again. if err := kl.deletePod(pod); err != nil { glog.V(2).Infof(\"Failed to delete pod %q, err: %v\", format.Pod(pod), err) } deletePod 函数将需要删除的pod加入podKillingCh的channel中，有podKiller监听这个channel去执行删除任务，实现如下：\n// deletePod deletes the pod from the internal state of the kubelet by: // 1. stopping the associated pod worker asynchronously // 2. signaling to kill the pod by sending on the podKillingCh channel // // deletePod returns an error if not all sources are ready or the pod is not // found in the runtime cache. func (kl *Kubelet) deletePod(pod *v1.Pod) error { if pod == nil { return fmt.Errorf(\"deletePod does not allow nil pod\") } if !kl.sourcesReady.AllReady() { // If the sources aren't ready, skip deletion, as we may accidentally delete pods // for sources that haven't reported yet. return fmt.Errorf(\"skipping delete because sources aren't ready yet\") } kl.podWorkers.ForgetWorker(pod.UID) // Runtime cache may not have been updated to with the pod, but it's okay // because the periodic cleanup routine will attempt to delete again later. runningPods, err := kl.runtimeCache.GetPods() if err != nil { return fmt.Errorf(\"error listing containers: %v\", err) } runningPod := kubecontainer.Pods(runningPods).FindPod(\"\", pod.UID) if runningPod.IsEmpty() { return fmt.Errorf(\"pod not found\") } podPair := kubecontainer.PodPair{APIPod: pod, RunningPod: \u0026runningPod} kl.podKillingCh \u003c- \u0026podPair // TODO: delete the mirror pod here? // We leave the volume/directory cleanup to the periodic cleanup routine. return nil } 从probe manager中移除pod。\nkl.probeManager.RemovePod(pod) 3.4. HandlePodReconcile 遍历pod列表。\n// HandlePodReconcile is the callback in the SyncHandler interface for pods // that should be reconciled. func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } 将pod更新到pod manager中。\nfor _, pod := range pods { // Update the pod in pod manager, status manager will do periodically reconcile according // to the pod manager. kl.podManager.UpdatePod(pod) ... } 必要时调整pod的Ready状态，执行dispatchWork函数。\n// Reconcile Pod \"Ready\" condition if necessary. Trigger sync pod for reconciliation. if status.NeedToReconcilePodReadiness(pod) { mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start) } 如果pod被设定为需要被驱逐的，则删除pod中的容器。\n// After an evicted pod is synced, all dead containers in the pod can be removed. if eviction.PodIsEvicted(pod.Status) { if podStatus, err := kl.podCache.Get(pod.UID); err == nil { kl.containerDeletor.deleteContainersInPod(\"\", podStatus, true) } } 3.5. HandlePodSyncs HandlePodSyncs是syncHandler接口回调函数，调用dispatchWork，通过pod worker来执行任务。\n// HandlePodSyncs is the callback in the syncHandler interface for pods // that should be dispatched to pod workers for sync. func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start) } } 3.6. HandlePodCleanups HandlePodCleanups主要用来执行pod的清理任务，其中包括terminating的pod，orphaned的pod等。\n首先查看pod使用到的cgroup。\n// HandlePodCleanups performs a series of cleanup work, including terminating // pod workers, killing unwanted pods, and removing orphaned volumes/pod // directories. // NOTE: This function is executed by the main sync loop, so it // should not contain any blocking calls. func (kl *Kubelet) HandlePodCleanups() error { // The kubelet lacks checkpointing, so we need to introspect the set of pods // in the cgroup tree prior to inspecting the set of pods in our pod manager. // this ensures our view of the cgroup tree does not mistakenly observe pods // that are added after the fact... var ( cgroupPods map[types.UID]cm.CgroupName err error ) if kl.cgroupsPerQOS { pcm := kl.containerManager.NewPodContainerManager() cgroupPods, err = pcm.GetAllPodsFromCgroups() if err != nil { return fmt.Errorf(\"failed to get list of pods that still exist on cgroup mounts: %v\", err) } } ... } 列出所有pod包括mirror pod。\nallPods, mirrorPods := kl.podManager.GetPodsAndMirrorPods() // Pod phase progresses monotonically. Once a pod has reached a final state, // it should never leave regardless of the restart policy. The statuses // of such pods should not be changed, and there is no need to sync them. // TODO: the logic here does not handle two cases: // 1. If the containers were removed immediately after they died, kubelet // may fail to generate correct statuses, let alone filtering correctly. // 2. If kubelet restarted before writing the terminated status for a pod // to the apiserver, it could still restart the terminated pod (even // though the pod was not considered terminated by the apiserver). // These two conditions could be alleviated by checkpointing kubelet. activePods := kl.filterOutTerminatedPods(allPods) desiredPods := make(map[types.UID]empty) for _, pod := range activePods { desiredPods[pod.UID] = empty{} } pod worker停止不再存在的pod的任务，并从probe manager中清除pod。\n// Stop the workers for no-longer existing pods. // TODO: is here the best place to forget pod workers? kl.podWorkers.ForgetNonExistingPodWorkers(desiredPods) kl.probeManager.CleanupPods(activePods) 将需要杀死的pod加入到podKillingCh的channel中，podKiller的任务会监听该channel并获取需要杀死的pod列表来执行杀死pod的操作。\nrunningPods, err := kl.runtimeCache.GetPods() if err != nil { glog.Errorf(\"Error listing containers: %#v\", err) return err } for _, pod := range runningPods { if _, found := desiredPods[pod.ID]; !found { kl.podKillingCh \u003c- \u0026kubecontainer.PodPair{APIPod: nil, RunningPod: pod} } } 当pod不再被绑定到该节点，移除podStatus，其中removeOrphanedPodStatuses最后调用的函数是statusManager的RemoveOrphanedStatuses方法。\nkl.removeOrphanedPodStatuses(allPods, mirrorPods) 移除所有的orphaned volume。\n// Remove any orphaned volumes. // Note that we pass all pods (including terminated pods) to the function, // so that we don't remove volumes associated with terminated but not yet // deleted pods. err = kl.cleanupOrphanedPodDirs(allPods, runningPods) if err != nil { // We want all cleanup tasks to be run even if one of them failed. So // we just log an error here and continue other cleanup tasks. // This also applies to the other clean up tasks. glog.Errorf(\"Failed cleaning up orphaned pod directories: %v\", err) } 移除mirror pod。\n// Remove any orphaned mirror pods. kl.podManager.DeleteOrphanedMirrorPods() 删除不再运行的pod的cgroup。\n// Remove any cgroups in the hierarchy for pods that are no longer running. if kl.cgroupsPerQOS { kl.cleanupOrphanedPodCgroups(cgroupPods, activePods) } 执行垃圾回收（GC）操作。\nkl.backOff.GC() 4. dispatchWork dispatchWork通过pod worker启动一个异步的循环。\n完整代码如下：\n// dispatchWork starts the asynchronous sync of the pod in a pod worker. // If the pod is terminated, dispatchWork func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { // If the pod is in a terminated state, there is no pod worker to // handle the work item. Check if the DeletionTimestamp has been // set, and force a status update to trigger a pod deletion request // to the apiserver. kl.statusManager.TerminatePod(pod) } return } // Run the sync in an async worker. kl.podWorkers.UpdatePod(\u0026UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) // Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } } 以下分段进行分析：\n如果pod的状态是处于Terminated状态，则执行statusManager的TerminatePod操作。\n// dispatchWork starts the asynchronous sync of the pod in a pod worker. // If the pod is terminated, dispatchWork func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { // If the pod is in a terminated state, there is no pod worker to // handle the work item. Check if the DeletionTimestamp has been // set, and force a status update to trigger a pod deletion request // to the apiserver. kl.statusManager.TerminatePod(pod) } return } ... } 执行pod worker的UpdatePod函数，该函数是pod worker的核心函数，来执行pod相关操作。具体逻辑待下文分析。\n// Run the sync in an async worker. kl.podWorkers.UpdatePod(\u0026UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) 当创建类型是SyncPodCreate（即创建pod的时候），统计新pod中容器的数目。\n// Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } 5. PodWorkers.UpdatePod PodWorkers是一个接口类型：\n// PodWorkers is an abstract interface for testability. type PodWorkers interface { UpdatePod(options *UpdatePodOptions) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) ForgetWorker(uid types.UID) } 其中UpdatePod是一个核心方法，通过podUpdates的channel来传递需要处理的pod信息，对于新创建的pod每个pod都会由一个goroutine来执行managePodLoop。\n此部分代码位于pkg/kubelet/pod_workers.go\n// Apply the new setting to the specified pod. // If the options provide an OnCompleteFunc, the function is invoked if the update is accepted. // Update requests are ignored if a kill pod request is pending. func (p *podWorkers) UpdatePod(options *UpdatePodOptions) { pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() if podUpdates, exists = p.podUpdates[uid]; !exists { // We need to have a buffer here, because checkForUpdates() method that // puts an update into channel is called from the same goroutine where // the channel is consumed. However, it is guaranteed that in such case // the channel is empty, so buffer of size 1 is enough. podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // Creating a new pod worker either means this is a new pod, or that the // kubelet just restarted. In either case the kubelet is willing to believe // the status of the pod for the first pod worker sync. See corresponding // comment in syncPod. go func() { defer runtime.HandleCrash() p.managePodLoop(podUpdates) }() } if !p.isWorking[pod.UID] { p.isWorking[pod.UID] = true podUpdates \u003c- *options } else { // if a request to kill a pod is pending, we do not let anything overwrite that request. update, found := p.lastUndeliveredWorkUpdate[pod.UID] if !found || update.UpdateType != kubetypes.SyncPodKill { p.lastUndeliveredWorkUpdate[pod.UID] = *options } } } 6. managePodLoop managePodLoop通过读取podUpdateschannel的信息，执行syncPodFn函数，而syncPodFn函数在newPodWorkers的时候赋值了，即kubelet.syncPod。kubelet.syncPod具体代码逻辑待后续文章单独分析。\n// newPodWorkers传入syncPod函数 klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) newPodWorkers函数参考：\nfunc newPodWorkers(syncPodFn syncPodFnType, recorder record.EventRecorder, workQueue queue.WorkQueue, resyncInterval, backOffPeriod time.Duration, podCache kubecontainer.Cache) *podWorkers { return \u0026podWorkers{ podUpdates: map[types.UID]chan UpdatePodOptions{}, isWorking: map[types.UID]bool{}, lastUndeliveredWorkUpdate: map[types.UID]UpdatePodOptions{}, syncPodFn: syncPodFn, // 构造传入klet.syncPod函数 recorder: recorder, workQueue: workQueue, resyncInterval: resyncInterval, backOffPeriod: backOffPeriod, podCache: podCache, } } managePodLoop函数参考：\n此部分代码位于pkg/kubelet/pod_workers.go\nfunc (p *podWorkers) managePodLoop(podUpdates \u003c-chan UpdatePodOptions) { var lastSyncTime time.Time for update := range podUpdates { err := func() error { podUID := update.Pod.UID // This is a blocking call that would return only if the cache // has an entry for the pod that is newer than minRuntimeCache // Time. This ensures the worker doesn't start syncing until // after the cache is at least newer than the finished time of // the previous sync. status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil { // This is the legacy event thrown by manage pod loop // all other events are now dispatched from syncPodFn p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, \"error determining status: %v\", err) return err } err = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, }) lastSyncTime = time.Now() return err }() // notify the call-back function if the operation succeeded or not if update.OnCompleteFunc != nil { update.OnCompleteFunc(err) } if err != nil { // IMPORTANT: we do not log errors here, the syncPodFn is responsible for logging errors glog.Errorf(\"Error syncing pod %s (%q), skipping: %v\", update.Pod.UID, format.Pod(update.Pod), err) } p.wrapUp(update.Pod.UID, err) } } 7. 总结 syncLoopIteration基本流程如下：\n通过几种channel来对不同类型的事件进行监听并处理。其中channel包括：configCh、plegCh、syncCh、houseKeepingCh、livenessManager.Updates()。 不同的SyncHandler执行不同的增删改查操作。 其中HandlePodAdditions、HandlePodUpdates、HandlePodReconcile、HandlePodSyncs都调用到了dispatchWork来执行pod的相关操作。HandlePodCleanups的pod清理任务，通过channel的方式加需要清理的pod给podKiller来清理。 dispatchWork调用podWorkers.UpdatePod执行异步操作。 podWorkers.UpdatePod中调用managePodLoop来执行pod相关操作循环。 channel类型及作用：\nconfigCh：将配置更改的pod分派给事件类型的相应处理程序回调。 plegCh：更新runtime缓存，同步pod。 syncCh：同步所有等待同步的pod。 houseKeepingCh：触发清理pod。 livenessManager.Updates()：对失败的pod或者liveness检查失败的pod进行sync操作。 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/pod_workers.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析kubelet中syncLoopIteration部 …","ref":"/k8s-source-code-analysis/kubelet/syncloopiteration/","tags":["源码分析"],"title":"kubelet源码分析（四）之 syncLoopIteration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/deployment/","tags":"","title":"版本发布"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/concurrency/","tags":"","title":"并发编程"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/","tags":"","title":"容器网络"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析调度逻辑中的预选策略，即第一步筛选出符合pod调度条件的节点。\n1. 调用入口 预选，通过预选函数来判断每个节点是否适合被该Pod调度。\ngenericScheduler.Schedule中对findNodesThatFit的调用过程如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\nfunc (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ... // 列出所有的节点 nodes, err := nodeLister.List() if err != nil { return \"\", err } if len(nodes) == 0 { return \"\", ErrNoNodesAvailable } // Used for all fit and priority funcs. err = g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return \"\", err } trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() // 调用findNodesThatFit过滤出预选节点 filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", \u0026FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } // metrics metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) ... } 核心代码：\n// 调用findNodesThatFit过滤出预选节点 filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 2. findNodesThatFit findNodesThatFit基于给定的预选函数过滤node，每个node传入到预选函数中来确实该节点是否符合要求。\nfindNodesThatFit的入参是被调度的pod和当前的节点列表，返回预选节点列表和错误。\nfindNodesThatFit基本流程如下：\n设置可行节点的总数，作为预选节点数组的容量，避免总节点过多需要筛选的节点过多。 通过NodeTree不断获取下一个节点来判断该节点是否满足pod的调度条件。 通过之前注册的各种预选函数来判断当前节点是否符合pod的调度条件。 最后返回满足调度条件的node列表，供下一步的优选操作。 findNodesThatFit完整代码如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\n// Filters the nodes to find the ones that fit based on the given predicate functions // Each node is passed through the predicate functions to determine if it is a fit func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) { var filtered []*v1.Node failedPredicateMap := FailedPredicateMap{} if len(g.predicates) == 0 { filtered = nodes } else { allNodes := int32(g.cache.NodeTree().NumNodes) numNodesToFind := g.numFeasibleNodesToFind(allNodes) // Create filtered list with enough space to avoid growing it // and allow assigning. filtered = make([]*v1.Node, numNodesToFind) errs := errors.MessageCountMap{} var ( predicateResultLock sync.Mutex filteredLen int32 equivClass *equivalence.Class ) ctx, cancel := context.WithCancel(context.Background()) // We can use the same metadata producer for all nodes. meta := g.predicateMetaProducer(pod, g.cachedNodeInfoMap) if g.equivalenceCache != nil { // getEquivalenceClassInfo will return immediately if no equivalence pod found equivClass = equivalence.NewClass(pod) } checkNode := func(i int) { var nodeCache *equivalence.NodeCache nodeName := g.cache.NodeTree().Next() if g.equivalenceCache != nil { nodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName) } fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.cache, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) if err != nil { predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return } if fits { length := atomic.AddInt32(\u0026filteredLen, 1) if length \u003e numNodesToFind { cancel() atomic.AddInt32(\u0026filteredLen, -1) } else { filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } else { predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() } } // Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) filtered = filtered[:filteredLen] if len(errs) \u003e 0 { return []*v1.Node{}, FailedPredicateMap{}, errors.CreateAggregateFromMessageCountMap(errs) } } if len(filtered) \u003e 0 \u0026\u0026 len(g.extenders) != 0 { for _, extender := range g.extenders { if !extender.IsInterested(pod) { continue } filteredList, failedMap, err := extender.Filter(pod, filtered, g.cachedNodeInfoMap) if err != nil { if extender.IsIgnorable() { glog.Warningf(\"Skipping extender %v as it returned error %v and has ignorable flag set\", extender, err) continue } else { return []*v1.Node{}, FailedPredicateMap{}, err } } for failedNodeName, failedMsg := range failedMap { if _, found := failedPredicateMap[failedNodeName]; !found { failedPredicateMap[failedNodeName] = []algorithm.PredicateFailureReason{} } failedPredicateMap[failedNodeName] = append(failedPredicateMap[failedNodeName], predicates.NewFailureReason(failedMsg)) } filtered = filteredList if len(filtered) == 0 { break } } } return filtered, failedPredicateMap, nil } 以下对findNodesThatFit分段分析。\n3. numFeasibleNodesToFind findNodesThatFit先基于所有的节点找出可行的节点是总数。numFeasibleNodesToFind的作用主要是避免当节点过多（超过100）影响调度的效率。\nallNodes := int32(g.cache.NodeTree().NumNodes) numNodesToFind := g.numFeasibleNodesToFind(allNodes) // Create filtered list with enough space to avoid growing it // and allow assigning. filtered = make([]*v1.Node, numNodesToFind) numFeasibleNodesToFind基本流程如下：\n如果所有的node节点小于minFeasibleNodesToFind(当前默认为100)则返回节点数。 如果节点数超100，则取指定计分的百分比的节点数，当该百分比后的数目仍小于minFeasibleNodesToFind，则返回minFeasibleNodesToFind。 如果百分比后的数目大于minFeasibleNodesToFind，则返回该百分比。 // numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops // its search for more feasible nodes. func (g *genericScheduler) numFeasibleNodesToFind(numAllNodes int32) int32 { if numAllNodes \u003c minFeasibleNodesToFind || g.percentageOfNodesToScore \u003c= 0 || g.percentageOfNodesToScore \u003e= 100 { return numAllNodes } numNodes := numAllNodes * g.percentageOfNodesToScore / 100 if numNodes \u003c minFeasibleNodesToFind { return minFeasibleNodesToFind } return numNodes } 4. checkNode checkNode是一个校验node是否符合要求的函数，其中实际调用到的核心函数是podFitsOnNode。再通过workqueue并发执行checkNode操作。\ncheckNode主要流程如下：\n通过cache中的nodeTree不断获取下一个node。 将当前node和pod传入podFitsOnNode判断当前node是否符合要求。 如果当前node符合要求就将当前node加入预选节点的数组中filtered。 如果当前node不满足要求，则加入到失败的数组中，并记录原因。 通过workqueue.ParallelizeUntil并发执行checkNode函数，一旦找到配置的可行节点数，就停止搜索更多节点。 checkNode := func(i int) { var nodeCache *equivalence.NodeCache nodeName := g.cache.NodeTree().Next() if g.equivalenceCache != nil { nodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName) } fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.cache, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) if err != nil { predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return } if fits { length := atomic.AddInt32(\u0026filteredLen, 1) if length \u003e numNodesToFind { cancel() atomic.AddInt32(\u0026filteredLen, -1) } else { filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } else { predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() } } workqueue的并发操作：\n// Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) ParallelizeUntil具体代码如下：\n// ParallelizeUntil is a framework that allows for parallelizing N // independent pieces of work until done or the context is canceled. func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) { var stop \u003c-chan struct{} if ctx != nil { stop = ctx.Done() } toProcess := make(chan int, pieces) for i := 0; i \u003c pieces; i++ { toProcess \u003c- i } close(toProcess) if pieces \u003c workers { workers = pieces } wg := sync.WaitGroup{} wg.Add(workers) for i := 0; i \u003c workers; i++ { go func() { defer utilruntime.HandleCrash() defer wg.Done() for piece := range toProcess { select { case \u003c-stop: return default: doWorkPiece(piece) } } }() } wg.Wait() } 5. podFitsOnNode podFitsOnNode主要内容如下：\npodFitsOnNode会检查给定的某个Node是否满足预选的函数。\n对于给定的pod，podFitsOnNode会检查是否有相同的pod存在，尽量复用缓存过的预选结果。\npodFitsOnNode主要在Schedule（调度）和Preempt（抢占）的时候被调用。\n当在Schedule中被调用的时候，主要判断是否可以被调度到当前节点，依据为当前节点上所有已存在的pod及被提名要运行到该节点的具有相等或更高优先级的pod。\n当在Preempt中被调用的时候，即发生抢占的时候，通过SelectVictimsOnNode函数选出需要被移除的pod，移除后然后将预调度的pod调度到该节点上。\npodFitsOnNode基本流程如下：\n遍历之前注册好的预选策略predicates.Ordering，并获取预选策略的执行函数。 遍历执行每个预选函数，并返回是否合适，预选失败的原因和错误。 如果预选函数执行的结果不合适，则加入预选失败的数组中。 最后返回预选失败的个数是否为0，和预选失败的原因。 入参：\npod PredicateMetadata NodeInfo predicateFuncs schedulercache.Cache nodeCache SchedulingQueue alwaysCheckAllPredicates equivClass 出参：\nfit PredicateFailureReason 完整代码如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\n// podFitsOnNode checks whether a node given by NodeInfo satisfies the given predicate functions. // For given pod, podFitsOnNode will check if any equivalent pod exists and try to reuse its cached // predicate results as possible. // This function is called from two different places: Schedule and Preempt. // When it is called from Schedule, we want to test whether the pod is schedulable // on the node with all the existing pods on the node plus higher and equal priority // pods nominated to run on the node. // When it is called from Preempt, we should remove the victims of preemption and // add the nominated pods. Removal of the victims is done by SelectVictimsOnNode(). // It removes victims from meta and NodeInfo before calling this function. func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, cache schedulercache.Cache, nodeCache *equivalence.NodeCache, queue SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) { var ( eCacheAvailable bool failedPredicates []algorithm.PredicateFailureReason ) podsAdded := false // We run predicates twice in some cases. If the node has greater or equal priority // nominated pods, we run them when those pods are added to meta and nodeInfo. // If all predicates succeed in this pass, we run them again when these // nominated pods are not added. This second pass is necessary because some // predicates such as inter-pod affinity may not pass without the nominated pods. // If there are no nominated pods for the node or if the first run of the // predicates fail, we don't run the second pass. // We consider only equal or higher priority pods in the first pass, because // those are the current \"pod\" must yield to them and not take a space opened // for running them. It is ok if the current \"pod\" take resources freed for // lower priority pods. // Requiring that the new pod is schedulable in both circumstances ensures that // we are making a conservative decision: predicates like resources and inter-pod // anti-affinity are more likely to fail when the nominated pods are treated // as running, while predicates like pod affinity are more likely to fail when // the nominated pods are treated as not running. We can't just assume the // nominated pods are running because they are not running right now and in fact, // they may end up getting scheduled to a different node. for i := 0; i \u003c 2; i++ { metaToUse := meta nodeInfoToUse := info if i == 0 { podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(util.GetPodPriority(pod), meta, info, queue) } else if !podsAdded || len(failedPredicates) != 0 { break } // Bypass eCache if node has any nominated pods. // TODO(bsalamat): consider using eCache and adding proper eCache invalidations // when pods are nominated or their nominations change. eCacheAvailable = equivClass != nil \u0026\u0026 nodeCache != nil \u0026\u0026 !podsAdded for _, predicateKey := range predicates.Ordering() { var ( fit bool reasons []algorithm.PredicateFailureReason err error ) //TODO (yastij) : compute average predicate restrictiveness to export it as Prometheus metric if predicate, exist := predicateFuncs[predicateKey]; exist { if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, pod, metaToUse, nodeInfoToUse, equivClass, cache) } else { fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } if err != nil { return false, []algorithm.PredicateFailureReason{}, err } if !fit { // eCache is available and valid, and predicates result is unfit, record the fail reasons failedPredicates = append(failedPredicates, reasons...) // if alwaysCheckAllPredicates is false, short circuit all predicates when one predicate fails. if !alwaysCheckAllPredicates { glog.V(5).Infoln(\"since alwaysCheckAllPredicates has not been set, the predicate \" + \"evaluation is short circuited and there are chances \" + \"of other predicates failing as well.\") break } } } } } return len(failedPredicates) == 0, failedPredicates, nil } 5.1. predicateFuncs 根据之前初注册好的预选策略函数来执行预选，判断节点是否符合调度。\nfor _, predicateKey := range predicates.Ordering() { if predicate, exist := predicateFuncs[predicateKey]; exist { if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, pod, metaToUse, nodeInfoToUse, equivClass, cache) } else { fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } 预选策略如下：\nvar ( predicatesOrdering = []string{CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred} ) 6. PodFitsResources 以下以PodFitsResources这个预选函数为例做分析，其他重要的预选函数待后续单独分析。\nPodFitsResources用来检查一个节点是否有足够的资源来运行当前的pod，包括CPU、内存、GPU等。\nPodFitsResources基本流程如下：\n判断当前节点上pod总数加上预调度pod个数是否大于node的可分配pod总数，若是则不允许调度。 判断pod的request值是否都为0，若是则允许调度。 判断pod的request值加上当前node上所有pod的request值总和是否大于node的可分配资源，若是则不允许调度。 判断pod的拓展资源request值加上当前node上所有pod对应的request值总和是否大于node对应的可分配资源，若是则不允许调度。 PodFitsResources的注册代码如下：\nfactory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) PodFitsResources入参：\npod\nnodeInfo\nPredicateMetadata\nPodFitsResources出参：\nfit PredicateFailureReason PodFitsResources完整代码：\n此部分的代码位于pkg/scheduler/algorithm/predicates/predicates.go\n// PodFitsResources checks if a node has sufficient resources, such as cpu, memory, gpu, opaque int resources etc to run a pod. // First return value indicates whether a node has sufficient resources to run a pod while the second return value indicates the // predicate failure reasons if the node has insufficient resources to run the pod. func PodFitsResources(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } var predicateFails []algorithm.PredicateFailureReason allowedPodNumber := nodeInfo.AllowedPodNumber() if len(nodeInfo.Pods())+1 \u003e allowedPodNumber { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber))) } // No extended resources should be ignored by default. ignoredExtendedResources := sets.NewString() var podRequest *schedulercache.Resource if predicateMeta, ok := meta.(*predicateMetadata); ok { podRequest = predicateMeta.podRequest if predicateMeta.ignoredExtendedResources != nil { ignoredExtendedResources = predicateMeta.ignoredExtendedResources } } else { // We couldn't parse metadata - fallback to computing it. podRequest = GetResourceRequest(pod) } if podRequest.MilliCPU == 0 \u0026\u0026 podRequest.Memory == 0 \u0026\u0026 podRequest.EphemeralStorage == 0 \u0026\u0026 len(podRequest.ScalarResources) == 0 { return len(predicateFails) == 0, predicateFails, nil } allocatable := nodeInfo.AllocatableResource() if allocatable.MilliCPU \u003c podRequest.MilliCPU+nodeInfo.RequestedResource().MilliCPU { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceCPU, podRequest.MilliCPU, nodeInfo.RequestedResource().MilliCPU, allocatable.MilliCPU)) } if allocatable.Memory \u003c podRequest.Memory+nodeInfo.RequestedResource().Memory { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceMemory, podRequest.Memory, nodeInfo.RequestedResource().Memory, allocatable.Memory)) } if allocatable.EphemeralStorage \u003c podRequest.EphemeralStorage+nodeInfo.RequestedResource().EphemeralStorage { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceEphemeralStorage, podRequest.EphemeralStorage, nodeInfo.RequestedResource().EphemeralStorage, allocatable.EphemeralStorage)) } for rName, rQuant := range podRequest.ScalarResources { if v1helper.IsExtendedResourceName(rName) { // If this resource is one of the extended resources that should be // ignored, we will skip checking it. if ignoredExtendedResources.Has(string(rName)) { continue } } if allocatable.ScalarResources[rName] \u003c rQuant+nodeInfo.RequestedResource().ScalarResources[rName] { predicateFails = append(predicateFails, NewInsufficientResourceError(rName, podRequest.ScalarResources[rName], nodeInfo.RequestedResource().ScalarResources[rName], allocatable.ScalarResources[rName])) } } if glog.V(10) { if len(predicateFails) == 0 { // We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is // not logged. There is visible performance gain from it. glog.Infof(\"Schedule Pod %+v on Node %+v is allowed, Node is running only %v out of %v Pods.\", podName(pod), node.Name, len(nodeInfo.Pods()), allowedPodNumber) } } return len(predicateFails) == 0, predicateFails, nil } 6.1. NodeInfo NodeInfo是node的聚合信息，主要包括：\nnode：k8s node的结构体 pods：当前node上pod的数量 requestedResource：当前node上所有pod的request总和 allocatableResource：node的实际所有的可分配资源(对应于Node.Status.Allocatable.*)，可理解为node的资源总量。 此部分代码位于pkg/scheduler/cache/node_info.go\n// NodeInfo is node level aggregated information. type NodeInfo struct { // Overall node information. node *v1.Node pods []*v1.Pod podsWithAffinity []*v1.Pod usedPorts util.HostPortInfo // Total requested resource of all pods on this node. // It includes assumed pods which scheduler sends binding to apiserver but // didn't get it as scheduled yet. requestedResource *Resource nonzeroRequest *Resource // We store allocatedResources (which is Node.Status.Allocatable.*) explicitly // as int64, to avoid conversions and accessing map. allocatableResource *Resource // Cached taints of the node for faster lookup. taints []v1.Taint taintsErr error // imageStates holds the entry of an image if and only if this image is on the node. The entry can be used for // checking an image's existence and advanced usage (e.g., image locality scheduling policy) based on the image // state information. imageStates map[string]*ImageStateSummary // TransientInfo holds the information pertaining to a scheduling cycle. This will be destructed at the end of // scheduling cycle. // TODO: @ravig. Remove this once we have a clear approach for message passing across predicates and priorities. TransientInfo *transientSchedulerInfo // Cached conditions of node for faster lookup. memoryPressureCondition v1.ConditionStatus diskPressureCondition v1.ConditionStatus pidPressureCondition v1.ConditionStatus // Whenever NodeInfo changes, generation is bumped. // This is used to avoid cloning it if the object didn't change. generation int64 } 6.2. Resource Resource是可计算资源的集合体。主要包括：\nMilliCPU Memory EphemeralStorage AllowedPodNumber：允许的pod总数(对应于Node.Status.Allocatable.Pods().Value())，一般为110。 ScalarResources // Resource is a collection of compute resource. type Resource struct { MilliCPU int64 Memory int64 EphemeralStorage int64 // We store allowedPodNumber (which is Node.Status.Allocatable.Pods().Value()) // explicitly as int, to avoid conversions and improve performance. AllowedPodNumber int // ScalarResources ScalarResources map[v1.ResourceName]int64 } 以下分析podFitsOnNode的具体流程。\n6.3. allowedPodNumber 首先获取节点的信息，先判断如果该节点当前所有的pod的个数加上当前预调度的pod是否会大于该节点允许的pod的总数，一般为110个。如果超过，则predicateFails数组增加1，即当前节点不适合该pod。\nnode := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } var predicateFails []algorithm.PredicateFailureReason allowedPodNumber := nodeInfo.AllowedPodNumber() if len(nodeInfo.Pods())+1 \u003e allowedPodNumber { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber))) } 6.4. podRequest 如果podRequest都为0，则允许调度到该节点，直接返回结果。\nif podRequest.MilliCPU == 0 \u0026\u0026 podRequest.Memory == 0 \u0026\u0026 podRequest.EphemeralStorage == 0 \u0026\u0026 len(podRequest.ScalarResources) == 0 { return len(predicateFails) == 0, predicateFails, nil } 6.5. AllocatableResource 如果当前预调度的pod的request资源加上当前node上所有pod的request总和大于该node的可分配资源总量，则不允许调度到该节点，直接返回结果。其中request资源包括CPU、内存、storage。\nallocatable := nodeInfo.AllocatableResource() if allocatable.MilliCPU \u003c podRequest.MilliCPU+nodeInfo.RequestedResource().MilliCPU { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceCPU, podRequest.MilliCPU, nodeInfo.RequestedResource().MilliCPU, allocatable.MilliCPU)) } if allocatable.Memory \u003c podRequest.Memory+nodeInfo.RequestedResource().Memory { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceMemory, podRequest.Memory, nodeInfo.RequestedResource().Memory, allocatable.Memory)) } if allocatable.EphemeralStorage \u003c podRequest.EphemeralStorage+nodeInfo.RequestedResource().EphemeralStorage { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceEphemeralStorage, podRequest.EphemeralStorage, nodeInfo.RequestedResource().EphemeralStorage, allocatable.EphemeralStorage)) } 6.6. ScalarResources 判断其他拓展的标量资源，是否该pod的request值加上当前node上所有pod的对应资源的request总和大于该node上对应资源的可分配总量，如果是，则不允许调度到该节点。\nfor rName, rQuant := range podRequest.ScalarResources { if v1helper.IsExtendedResourceName(rName) { // If this resource is one of the extended resources that should be // ignored, we will skip checking it. if ignoredExtendedResources.Has(string(rName)) { continue } } if allocatable.ScalarResources[rName] \u003c rQuant+nodeInfo.RequestedResource().ScalarResources[rName] { predicateFails = append(predicateFails, NewInsufficientResourceError(rName, podRequest.ScalarResources[rName], nodeInfo.RequestedResource().ScalarResources[rName], allocatable.ScalarResources[rName])) } } 7. 总结 findNodesThatFit基于给定的预选函数过滤node，每个node传入到预选函数中来确实该节点是否符合要求。\nfindNodesThatFit的入参是被调度的pod和当前的节点列表，返回预选节点列表和错误。\nfindNodesThatFit基本流程如下：\n设置可行节点的总数，作为预选节点数组的容量，避免总节点过多导致需要筛选的节点过多，效率低。 通过NodeTree不断获取下一个节点来判断该节点是否满足pod的调度条件。 通过之前注册的各种预选函数来判断当前节点是否符合pod的调度条件。 最后返回满足调度条件的node列表，供下一步的优选操作。 7.1. checkNode checkNode是一个校验node是否符合要求的函数，其中实际调用到的核心函数是podFitsOnNode。再通过workqueue并发执行checkNode操作。\ncheckNode主要流程如下：\n通过cache中的nodeTree不断获取下一个node。 将当前node和pod传入podFitsOnNode判断当前node是否符合要求。 如果当前node符合要求就将当前node加入预选节点的数组中filtered。 如果当前node不满足要求，则加入到失败的数组中，并记录原因。 通过workqueue.ParallelizeUntil并发执行checkNode函数，一旦找到配置的可行节点数，就停止搜索更多节点。 7.2. podFitsOnNode 其中会调用到核心函数podFitsOnNode。\npodFitsOnNode主要内容如下：\npodFitsOnNode会检查给定的某个Node是否满足预选的函数。\n对于给定的pod，podFitsOnNode会检查是否有相同的pod存在，尽量复用缓存过的预选结果。\npodFitsOnNode主要在Schedule（调度）和Preempt（抢占）的时候被调用。\n当在Schedule中被调用的时候，主要判断是否可以被调度到当前节点，依据为当前节点上所有已存在的pod及被提名要运行到该节点的具有相等或更高优先级的pod。\n当在Preempt中被调用的时候，即发生抢占的时候，通过SelectVictimsOnNode函数选出需要被移除的pod，移除后然后将预调度的pod调度到该节点上。\npodFitsOnNode基本流程如下：\n遍历之前注册好的预选策略predicates.Ordering，并获取预选策略的执行函数。 遍历执行每个预选函数，并返回是否合适，预选失败的原因和错误。 如果预选函数执行的结果不合适，则加入预选失败的数组中。 最后返回预选失败的个数是否为0，和预选失败的原因。 7.3. PodFitsResources 本文只示例分析了其中一个重要的预选函数：PodFitsResources\nPodFitsResources用来检查一个节点是否有足够的资源来运行当前的pod，包括CPU、内存、GPU等。\nPodFitsResources基本流程如下：\n判断当前节点上pod总数加上预调度pod个数是否大于node的可分配pod总数，若是则不允许调度。 判断pod的request值是否都为0，若是则允许调度。 判断pod的request值加上当前node上所有pod的request值总和是否大于node的可分配资源，若是则不允许调度。 判断pod的拓展资源request值加上当前node上所有pod对应的request值总和是否大于node对应的可分配资源，若是则不允许调度。 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithm/predicates/predicates.go\n","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析调度逻辑中的预选策略，即第一步筛选出符合pod调度条件的节点。 …","ref":"/k8s-source-code-analysis/kube-scheduler/findnodesthatfit/","tags":["源码分析"],"title":"kube-scheduler源码分析（四）之 预选策略"},{"body":"1. netplan简介 netplan是一个linux网络配置的渲染器，可以通过创建一个网络配置的yaml文件，netplan将该文件渲染成linux network所需的配置。\n2. netplan原理 netplan读取/etc/netplan/*.yaml的配置文件，Netplan在/run（例如：/run/systemd/network/）中生成特定于后端的配置文件，将设备的控制交给特定的网络守护进程。\n3. netplan配置 具体的netplan配置可以参考：\nhttps://netplan.readthedocs.io/en/stable/howto/ https://netplan.readthedocs.io/en/stable/netplan-yaml/ 通用配置：\nnetwork: version: NUMBER # 必填 renderer: STRING # 必填 networkd（默认）或者NetworkManager ethernets: MAPPING # 网卡相关配置 bonds: MAPPING # bond相关配置 vlans: MAPPING # VLAN相关配置 bridges: MAPPING dummy-devices: MAPPING modems: MAPPING tunnels: MAPPING virtual-ethernets: MAPPING vrfs: MAPPING wifis: MAPPING nm-devices: MAPPING 3.1. 使用直连网关配置 具体可参考：https://netplan.readthedocs.io/en/stable/netplan-yaml/#routing\n如果是没有bond设置及VLAN设置，则可用以下网关配置。\n参数说明：\naddresses：网卡的IP地址 routes 路由地址，一般to后面对应0.0.0.0/0， via对应网关地址 dhcp4：布尔类型，是否给IPv4开启DHCP，默认是false。 gateway4: IPv4的网关地址，已经废弃，被routes参数取代。gateway4和route两者配置一个即可。 network: version: 2 renderer: networkd ethernets: ens3: addresses: [ \"192.168.10.1/24\" ] routes: - to: default # or 0.0.0.0/0 via: 9.9.9.9 on-link: true 使用“on-link”关键字，其中网关是直接连接到网络的IP地址，即使该地址与接口上配置的子网不匹配。\n3.2. 配置bond网卡 具体可参考：https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-bonds\n如果有bond网卡则按以下的配置。\nbond参数 parameters 说明：\nmode:网卡的bond模式，包括balance-rr(默认，即轮询), active-backup（主备模式）, balance-xor, broadcast, 802.3ad, balance-tlb , balance-alb mii-monitor-interval：指定MII监控的间隔时间(检查绑定的接口是否有carrier)。默认值为0;这将禁用MII监控。这相当于网络后端的MIIMonitorSec=字段。如果没有指定时间后缀，该值将被解释为毫秒。 lacp-rate：配置lacpdu的传输速率。这只在802.3ad模式下有用。可能的值是slow(默认为30秒)和fast(每秒)。 transmit-hash-policy：指定端口选择的传输哈希策略。这只在balance-xor、802.3ad和balance-tlb模式下有用。取值为layer2、layer3+4、layer2+3、encap2+3、encap3+4。 以下示例配置一个或多个bond网卡：\nnetwork: version: 2 renderer: networkd ethernets: ens1: {} ens2: {} ens3: {} ens4: {} bonds: bond0: # bond0 即网卡名称 interfaces: # 由哪几块网卡做的bond0 - ens1 - ens2 addresses: [ \"192.168.10.1/24\" ] # 配置bond0网卡地址及路由 routes: - to: default # or 0.0.0.0/0 via: 9.9.9.9 parameters: # bond相关参数 mode: \"802.3ad\" mii-monitor-interval: \"100\" # 设置 MII 链路监控间隔为 100 毫秒。 lacp-rate: \"fast\" # 设置 LACP 速率为快速模式（每秒发送 LACPDU 数据包）。 transmit-hash-policy: \"layer3+4\" # 设置传输散列策略为基于第3层和第4层（IP地址和端口）的负载均衡策略。 bond1: # bond1 即网卡名称 interfaces: # 由哪几块网卡做的bond1 - ens3 - ens4 addresses: [ \"192.168.10.2/24\" ] # 配置bond1网卡地址及路由 routes: - to: default # or 0.0.0.0/0 via: 9.9.9.9 parameters: # bond相关参数 mode: \"802.3ad\" mii-monitor-interval: \"100\" lacp-rate: \"fast\" transmit-hash-policy: \"layer3+4\" 3.2.1. bond模式802.3ad说明 在网络配置中，bond（又称为 link aggregation 或 NIC teaming）是将多个网络接口聚合成一个单一的逻辑接口，以提高带宽和提供冗余性。bond模式决定了这些接口如何协同工作。\nmode: \"802.3ad\" 是一种 bond模式，它遵循 IEEE 802.3ad 标准，也称为 LACP（Link Aggregation Control Protocol）。这个模式提供了一种动态协商机制，可以在多个网络接口之间聚合链路。\n802.3ad 模式的特点\n动态协商：使用 LACP 协议动态协商链路聚合，使多个网络接口协同工作。 负载均衡：能够在多个链路上均衡负载，提高整体带宽。 冗余性：在任何一个接口故障时，其他接口仍能继续工作，提供链路冗余。 兼容性：需要交换机端也支持并配置 LACP 协议。 典型应用场景\n高带宽需求：需要聚合多个网络接口来提高带宽，例如服务器对网络带宽有较高要求的情况。 高可用性需求：需要提供网络接口的冗余，确保网络连接的稳定性和可靠性。 常用的模式802.3ad配置\nparameters: # bond相关参数 mode: \"802.3ad\" mii-monitor-interval: \"100\" # 设置 MII 链路监控间隔为 100 毫秒。 lacp-rate: \"fast\" # 设置 LACP 速率为快速模式（每秒发送 LACPDU 数据包）。 transmit-hash-policy: \"layer3+4\" # 设置传输散列策略为基于第3层和第4层（IP地址和端口）的负载均衡策略。 交换机配置要求\n为了使 802.3ad 模式正常工作，需要确保连接的交换机支持并配置了 LACP。通过这种配置，可以实现服务器端和交换机端的链路聚合，提供更高的带宽和冗余性。\n3.3. 配置VLAN 具体可参考：https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-vlans\n如果有配置VLAN ID则按以下配置，\nVLAN参数说明：\nbond0.1000：VLAN的网卡名称 id：VLAN id link：链接的网卡名称，可以是实体网卡也可以是bond的网卡。 以下示例配置一个或多个VLAN网卡。\nnetwork: version: 2 renderer: networkd ethernets: ens1: {} ens2: {} ens3: {} bonds: bond0: interfaces: - ens1 - ens2 parameters: mode: \"802.3ad\" mii-monitor-interval: \"100\" lacp-rate: \"fast\" transmit-hash-policy: \"layer3+4\" vlans: bond0.1000: addresses: - \"192.168.10.1/24\" #配置 bond0.1000 的地址和路由 routes: - to: \"0.0.0.0/0\" via: \"9.9.9.9\" id: 1000 # VLAN ID link: \"bond0\" # 配置VLAN对应的网卡 ens3.2000: #多个VLAN的配置 addresses: - \"192.168.10.2/24\" #配置 bond0.1000 的地址和路由 routes: - to: \"0.0.0.0/0\" via: \"9.9.9.9\" id: 2000 # VLAN ID link: \"ens3\" # 配置VLAN对应的网卡 总结上述三种情况下addresses和routes参数配置的地方：\n没有配置bond和VLAN，则addresses和routes参数配置在ethernets 如果没有配置VLAN，但是配置了bond，则配置在bond下 如果配置了VLAN，不论是否配置bond，都配置在VLAN下 4. netplan命令 netplan -h usage: /usr/sbin/netplan [-h] [--debug] ... Network configuration in YAML options: -h, --help show this help message and exit --debug Enable debug messages Available commands: help Show this help message apply Apply current netplan config to running system generate Generate backend specific configuration files from /etc/netplan/*.yaml get Get a setting by specifying a nested key like \"ethernets.eth0.addresses\", or \"all\" info Show available features ip Retrieve IP information from the system set Add new setting by specifying a dotted key=value pair like ethernets.eth0.dhcp4=true rebind Rebind SR-IOV virtual functions of given physical functions to their driver status Query networking state of the running system try Try to apply a new netplan config to running system, with automatic rollback 4.1. netplan get 查询当前的配置内容：\n$ netplan get network: version: 2 renderer: networkd ethernets: enp24s0f0: {} enp24s0f1: {} bonds: bond0: interfaces: - enp24s0f1 - enp24s0f0 parameters: mode: \"802.3ad\" mii-monitor-interval: \"100\" lacp-rate: \"fast\" transmit-hash-policy: \"layer3+4\" vlans: bond0.1000: addresses: - \"a.x.x.x/26\" dhcp4: false routes: - to: \"0.0.0.0/0\" via: \"b.x.x.x\" id: 1000 link: \"bond0\" 4.2. netplan apply 通过编辑/etc/netplan/*.yaml的文件，再执行netplan apply的命令可以使得网络配置生效。\n5. Systemd-networkd 可以通过man systemd-networkd查看说明\nsystemd-networkd是一种管理网络的系统服务。它检测和配置网络设备，以及创建虚拟网络设备。\nsystemd-networkd将根据systemd.netdev文件中的配置创建网络设备，并遵守这些文件中的[Match]部分。\n当systemd-networkd退出时，它通常会保留现有的网络设备和配置不变。当配置更新并重新启动systemd-networkd时，netdev已删除配置的接口不会被删除，可能需要手动清理。\nsystemd-networkd可以在运行时使用networkctl进行控制。\n5.1. 配置文件 systemd-networkd的配置文件位于/run/systemd/network，部分配置在/etc/systemd/network下。\n主要配置文件路径和用途\n.network 文件：定义网络接口的配置。 路径：/run/systemd/network/*.network 用途：配置网络接口的 IP 地址、网关、DNS 等。 .link 文件：定义网络接口的属性，如名称、MAC 地址等。 路径：/etc/systemd/network/*.link 用途：配置网络接口的属性，比如名称、MAC 地址、MTU 等。 .netdev 文件：定义虚拟网络设备（例如 bridge、bond、vlan 等）的配置。 路径：/run/systemd/network/*.netdev 用途：配置虚拟网络设备。 例如：\n# cat 10-netplan-bond0.netdev [NetDev] Name=bond0 Kind=bond [Bond] Mode=802.3ad LACPTransmitRate=fast MIIMonitorSec=100ms TransmitHashPolicy=layer3+4 # cat 10-netplan-bond0.network [Match] Name=bond0 [Network] LinkLocalAddressing=ipv6 ConfigureWithoutCarrier=yes VLAN=bond0.1000 #cat 10-netplan-bond0.1000.network [Match] Name=bond0.1000 [Network] LinkLocalAddressing=ipv6 Address=192.168.0.1/26 ConfigureWithoutCarrier=yes [Route] Destination=0.0.0.0/0 Gateway=9.9.9.9 # cat 10-netplan-bond0.1000.netdev [NetDev] Name=bond0.1000 Kind=vlan [VLAN] Id=1000 5.2. netdev参数说明 netdev的参数可以参考：https://manpages.ubuntu.com/manpages/bionic/man5/systemd.netdev.5.html\nbond部分的参数说明：\nThe \"[Bond]\" section accepts the following key: Mode= Specifies one of the bonding policies. The default is \"balance-rr\" (round robin). Possible values are \"balance-rr\", \"active-backup\", \"balance-xor\", \"broadcast\", \"802.3ad\", \"balance-tlb\", and \"balance-alb\". TransmitHashPolicy= Selects the transmit hash policy to use for slave selection in balance-xor, 802.3ad, and tlb modes. Possible values are \"layer2\", \"layer3+4\", \"layer2+3\", \"encap2+3\", and \"encap3+4\". LACPTransmitRate= Specifies the rate with which link partner transmits Link Aggregation Control Protocol Data Unit packets in 802.3ad mode. Possible values are \"slow\", which requests partner to transmit LACPDUs every 30 seconds, and \"fast\", which requests partner to transmit LACPDUs every second. The default value is \"slow\". vlan部分说明：\nThe \"[VLAN]\" section only applies for netdevs of kind \"vlan\", and accepts the following key: Id= The VLAN ID to use. An integer in the range 0–4094. This option is compulsory. 6. networkctl命令 # networkctl -h networkctl [OPTIONS...] COMMAND Query and control the networking subsystem. Commands: list [PATTERN...] List links status [PATTERN...] Show link status lldp [PATTERN...] Show LLDP neighbors label Show current address label entries in the kernel delete DEVICES... Delete virtual netdevs up DEVICES... Bring devices up down DEVICES... Bring devices down renew DEVICES... Renew dynamic configurations forcerenew DEVICES... Trigger DHCP reconfiguration of all connected clients reconfigure DEVICES... Reconfigure interfaces reload Reload .network and .netdev files edit FILES|DEVICES... Edit network configuration files cat FILES|DEVICES... Show network configuration files 6.1. networkctl status # networkctl status ● Interfaces: 8, 9, 7, 6, 5, 4, 3, 2, 1 State: routable Online state: online Address: 192.168.0.1 on bond0.1000 xxxx::4c0e:43ff:feba:xxxx on bond0 xxxx::4c0e:43ff:feba:xxxx on bond0.1000 Gateway: 9.9.9.9 on bond0.1000 Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: netdev ready Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: eno5: Gained carrier Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: Configuring with /run/systemd/network/10-netplan-bond0.1000.network. Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: Link UP Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0: Gained carrier Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: Gained carrier Jul 18 02:21:17 192.168.0.1 systemd-networkd[6393]: eno6: Gained carrier Jul 18 02:21:17 192.168.0.1 systemd-networkd[6393]: bond0.1000: Gained IPv6LL Jul 18 02:21:18 192.168.0.1 systemd-networkd[6393]: bond0: Gained IPv6LL Jul 18 02:21:18 192.168.0.1 systemd[1]: Finished systemd-networkd-wait-online.service - Wait for Network to be Configured. 6.2. networkctl list # networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether off unmanaged 3 eno2 ether off unmanaged 4 eno3 ether off unmanaged 5 eno4 ether off unmanaged 6 eno5 ether enslaved configured 7 eno6 ether enslaved configured 8 bond0 bond degraded configured 9 bond0.1000 vlan routable configured 9 links listed. 7. 总结 本文大概介绍了netplan和systemd-networkd两个模块，其中systemd-networkd是主要负责linux机器上的网络管理的后台进程。可以通过networkctl的命令行进行配置和查看。而netplan则是一个网络配置模板渲染工具，通过yaml文件可以生成systemd-networkd所使用的配置文件。其中netplan的配置文件位于/etc/netplan/目录，而systemd-networkd的配置文件为/run/systemd/network目录。两者的配置参数几乎是一一对应的，通过配置netplan的参数则可以配置systemd-networkd的参数。\n参考：\nhttps://netplan.io/ https://netplan.readthedocs.io/en/stable/netplan-yaml/ https://netplan.readthedocs.io/en/stable/netplan-yaml/#routing https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-bonds https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-vlans Systemd-networkd https://manpages.ubuntu.com/manpages/bionic/man5/systemd.netdev.5.html ","categories":"","description":"","excerpt":"1. netplan简介 netplan是一个linux网络配置的渲染器，可以通过创建一个网络配置的yaml文件，netplan将该文件渲染 …","ref":"/linux-notes/network/netplan/","tags":["network"],"title":"netplan介绍"},{"body":"1. 概述 网络接口绑定（Network Interface Bonding），也称为链路聚合（Link Aggregation）或NIC Teaming，是将多个物理网络接口聚合成一个逻辑接口，以提高带宽和提供冗余性的技术。这种技术广泛应用于服务器和高性能计算环境中，以确保网络的高可用性和高性能。\n2. 优势 增加带宽：通过聚合多个网络接口，整体带宽增加，从而提升网络吞吐量。 高可用性：在一个接口发生故障时，其他接口可以继续工作，确保网络连接的连续性。 负载均衡：数据流量可以在多个接口之间均衡分配，避免单一接口成为瓶颈。 简化管理：将多个接口管理为一个逻辑接口，简化了网络配置和管理。 3. Bonding 模式 Linux 支持多种 Bonding 模式，每种模式都有其独特的特点和应用场景：\nmode=0 (balance-rr)：循环方式（Round-robin），每个数据包依次从每个接口发送。提供负载均衡和容错功能。 mode=1 (active-backup)：主备模式（Active-backup），一个接口为主接口，其他接口为备份接口。当主接口失败时，备份接口接管。提供高可用性。 mode=2 (balance-xor)：根据传输散列算法选择接口。提供负载均衡和容错功能。 mode=3 (broadcast)：广播模式，所有数据包通过所有接口发送。提供容错功能。 mode=4 (802.3ad)：动态链路聚合（LACP），需要交换机支持 IEEE 802.3ad。提供负载均衡和高可用性。 mode=5 (balance-tlb)：基于发送负载的自适应传输负载均衡（Adaptive Transmit Load Balancing）。无需特殊交换机支持。 mode=6 (balance-alb)：基于接收负载的自适应负载均衡（Adaptive Load Balancing）。无需特殊交换机支持。 4. 配置示例 以下是使用 systemd-networkd 配置 Bonding 的示例。\n4.1. 配置物理接口 首先，配置要绑定的物理接口。例如，enp26s0f0 和 enp26s0f1：\n创建文件 /etc/systemd/network/10-enp26s0f0.network：\n[Match] Name=enp26s0f0 [Network] Bond=bond0 创建文件 /etc/systemd/network/10-enp26s0f1.network：\n[Match] Name=enp26s0f1 [Network] Bond=bond0 4.2. 配置 Bonding 接口 创建文件 /etc/systemd/network/bond0.netdev 来定义 Bonding 接口：\n[NetDev] Name=bond0 Kind=bond [Bond] Mode=802.3ad MIIMonitorSec=1s LACPTransmitRate=fast 4.3. 配置 Bonding 接口的网络设置 创建文件 /etc/systemd/network/10-bond0.network 来配置 Bonding 接口的网络设置：\n[Match] Name=bond0 [Network] Address=192.168.1.10/24 Gateway=192.168.1.1 DNS=8.8.8.8 DNS=8.8.4.4 4.4. 应用配置 保存配置文件后，重新启动 systemd-networkd 服务以应用新的网络配置：\nsudo systemctl restart systemd-networkd 4.5. 检查配置 或者查看具体接口的详细信息：\n# networkctl status bond0 ● 8: bond0 Link File: /usr/lib/systemd/network/99-default.link Network File: /run/systemd/network/10-netplan-bond0.network State: degraded (configured) Online state: online Type: bond Kind: bond Driver: bonding Hardware Address: 4e:0e:43:ba:f7:82 MTU: 1500 (min: 68, max: 65535) QDisc: noqueue IPv6 Address Generation Mode: eui64 Mode: 802.3ad Miimon: 100ms Updelay: 0 Downdelay: 0 Number of Queues (Tx/Rx): 16/16 Auto negotiation: no Speed: 20Gbps Duplex: full Address: xxx::4c0e:43ff:feba:xxx Activation Policy: up Required For Online: yes DHCP6 Client DUID: DUID-EN/Vendor:0000ab111fbd6366525ac0ea 5. 通过命令配置bond 5.1. 通过IP命令做bond #!/bin/bash # 安装必要的软件包 sudo apt-get update sudo apt-get install -y ifenslave # 创建 Bond 接口 sudo ip link add bond0 type bond # 设置 Bond 模式 sudo ip link set bond0 type bond mode 802.3ad 或者 modprobe bonding mode=4 miimon=100 lacp_rate=1 xmit_hash_policy=1 # 添加从接口到 Bond 接口 sudo ip link set enp26s0f0 down sudo ip link set enp26s0f0 master bond0 sudo ip link set enp26s0f1 down sudo ip link set enp26s0f1 master bond0 # 配置 Bond 接口的 IP 地址 sudo ip addr add 192.168.1.10/24 dev bond0 # 启用 Bond 接口 sudo ip link set bond0 up # 启用从接口 sudo ip link set enp26s0f0 up sudo ip link set enp26s0f1 up echo \"Bond 接口配置完成\" 查看bond状态\ncat /proc/net/bonding/bond0 使用 modprobe 工具配置网络接口的 Bond（绑定）操作是另一种在 Linux 上设置链路聚合的方法。modprobe 用于加载和卸载内核模块，而 bonding 模块是用于实现网络接口绑定的内核模块。\n","categories":"","description":"","excerpt":"1. 概述 网络接口绑定（Network Interface Bonding），也称为链路聚合（Link Aggregation）或NIC …","ref":"/linux-notes/network/bond/","tags":["network"],"title":"网卡Bonding介绍"},{"body":"Docker部署 docker run -d -p 3000:3000 grafana/grafana:latest K8S部署 helm部署\nhelm repo add grafana https://grafana.github.io/helm-charts helm search repo grafana 参考：\nInstall Grafana | Grafana documentation\nDeploy Grafana on Kubernetes | Grafana documentation\nGitHub - grafana/helm-charts\n","categories":"","description":"","excerpt":"Docker部署 docker run -d -p 3000:3000 grafana/grafana:latest K8S部署 helm部 …","ref":"/kubernetes-notes/monitor/grafana-usage/","tags":["Monitor"],"title":"Grafana部署"},{"body":"1. Wasm（WebAssembly）是什么 Wasm，全称为WebAssembly，是基于堆栈的虚拟机的二进制指令格式。Wasm被设计为编程语言的可移植编译目标，支持在Web上部署客户端和服务器应用程序。\nWebAssembly的主要目标是提供一种可移植、高效、安全的执行环境，以在Web浏览器中运行各种编程语言的代码。它不依赖于特定的硬件或操作系统，WebAssembly允许开发人员使用多种编程语言，例如C、C++、Rust等，通过编译成Wasm字节码来在Web上运行。\nWasm的特点：\n高效性能：Wasm被设计为高效执行，并且与底层系统硬件紧密关联，使其在Web浏览器中可以获得接近本机代码的性能。\n安全性：Wasm是一种隔离的执行环境，它运行在浏览器的沙箱中，具有严格的安全性措施，确保Wasm代码不能直接访问Web浏览器的敏感资源和功能。\n可移植性：由于Wasm是一种独立于平台的中间表示，因此可以在各种设备和操作系统上运行，从桌面计算机到移动设备。\n语言无关性：Wasm允许使用多种编程语言编写代码，而不仅限于JavaScript。这为开发人员提供了更多的灵活性，使得在Web上运行高性能应用程序变得更加容易。\n一句话来概括：\nWasm是一种可移植、高效、安全、跨语言的二进制编码格式。它支持在客户端（浏览器）和服务端运行应用程序。\n2. WasmEdge是什么 WasmEdge 是一个轻量级、高性能和可扩展的 WebAssembly 运行时。它是当今最快的Wasm VM。适用于云原生、边缘和去中心化应用程序。它为serverless应用程序、嵌入式功能、微服务、智能合约和 IoT 设备提供支持。\n3. 如何将golang编译成wasm并运行 代码如下：\npackage main func main() { println(\"Hello TinyGo from WasmEdge!\") } 3.1. 编译wasm二进制 使用tinygo编译\n安装tinygo\nubuntu系统\nwget https://github.com/tinygo-org/tinygo/releases/download/v0.28.1/tinygo_0.28.1_amd64.deb sudo dpkg -i tinygo_0.28.1_amd64.deb tinygo编译\ntinygo build -o hello.wasm -target wasm main.go 3.2. 运行wasm二进制 安装wasmedge，参考：https://wasmedge.org/docs/start/install\nwget -qO- https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- -p /usr/local 运行wasm二进制\n参考：\n[Go - WasmEdge Runtime](Go - WasmEdge Runtime)\n# wasmedge hello.wasm Hello TinyGo from WasmEdge! 3.3. 性能提升 要为这些应用程序达到原生 Go 性能，你可以使用 wasmedgec 命令来 AOT 编译 wasm 程序，然后使用 wasmedge 命令运行它。\n$ wasmedgec hello.wasm hello.wasm $ wasmedge hello.wasm Hello TinyGo from WasmEdge! 4. 如何构建wasm的容器镜像 安装buildah，参考：https://github.com/containers/buildah/blob/main/install.md\nsudo apt-get -y update sudo apt-get -y install buildah 步骤如下：\n编译wasm二进制\n编写dockerfile，例如：\nFROM scratch COPY hello.wasm / CMD [\"/hello.wasm\"] 使用buildah构建和发布镜像。\nbuildah build --annotation \"module.wasm.image/variant=compat-smart\" -t wasm-hello . 参考：\nhttps://webassembly.org/\nhttps://github.com/WasmEdge/WasmEdge\nhttps://github.com/second-state/wasmedge-containers-examples\nhttps://github.com/second-state/wasmedge-containers-examples/blob/main/simple_wasi_app-zh.md\nhttps://wasmedge.org/docs/develop/deploy/cri-runtime/containerd-crun\nhttps://wasmedge.org/docs/develop/go/hello_world\nManage WebAssembly Apps Using Container and Kubernetes Tools\n","categories":"","description":"","excerpt":"1. Wasm（WebAssembly）是什么 Wasm，全称为WebAssembly，是基于堆栈的虚拟机的二进制指令格式。Wasm被设计为 …","ref":"/kubernetes-notes/runtime/wasmedge/","tags":["Kubernetes","Runtime"],"title":"WasmEdge介绍"},{"body":"1. 读取数据key 使用以下命令列出所有的key。\nETCDCTL_API=3 etcdctl --endpoints=\u003cetcd-ip-1\u003e:2379,\u003cetcd-ip-2\u003e:2379,\u003cetcd-ip-3\u003e:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt get / --prefix --keys-only 参数说明：\n--cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --key=\"\"\tidentify secure client using this TLS key file --endpoints=[127.0.0.1:2379]\tgRPC endpoints 可以使用alias来重命名etcdctl一串的命令\nalias ectl='ETCDCTL_API=3 etcdctl --endpoints=\u003cetcd-ip-1\u003e:2379,\u003cetcd-ip-2\u003e:2379,\u003cetcd-ip-3\u003e:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt' 2. 集群数据 2.1. node /registry/minions/\u003cnode-ip-1\u003e /registry/minions/\u003cnode-ip-2\u003e /registry/minions/\u003cnode-ip-3\u003e 其他信息：\n/registry/leases/kube-node-lease/\u003cnode-ip-1\u003e /registry/leases/kube-node-lease/\u003cnode-ip-2\u003e /registry/leases/kube-node-lease/\u003cnode-ip-3\u003e /registry/masterleases/\u003cnode-ip-2\u003e /registry/masterleases/\u003cnode-ip-3\u003e 3. k8s对象数据 k8s对象数据的格式\n3.1. namespace /registry/namespaces/default /registry/namespaces/game /registry/namespaces/kube-node-lease /registry/namespaces/kube-public /registry/namespaces/kube-system 3.2. namespace级别对象 /registry/{resource}/{namespace}/{resource_name} 以下以常见k8s对象为例：\n# deployment /registry/deployments/default/game-2048 /registry/deployments/kube-system/prometheus-operator # replicasets /registry/replicasets/default/game-2048-c7d589ccf # pod /registry/pods/default/game-2048-c7d589ccf-8lsbw # statefulsets /registry/statefulsets/kube-system/prometheus-k8s # daemonsets /registry/daemonsets/kube-system/kube-proxy # secrets /registry/secrets/default/default-token-tbfmb # serviceaccounts /registry/serviceaccounts/default/default service\n# service /registry/services/specs/default/game-2048 # endpoints /registry/services/endpoints/default/game-2048 4. 读取数据value 由于k8s默认etcd中的数据是通过protobuf格式存储，因此看到的key和value的值是一串字符串。\nalias ectl='ETCDCTL_API=3 etcdctl --endpoints=:2379,:2379,:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt'\n# ectl get /registry/namespaces/test -w json |jq { \"header\": { \"cluster_id\": 12113422651334595000, \"member_id\": 8381627376898157000, \"revision\": 12321629, \"raft_term\": 20 }, \"kvs\": [ { \"key\": \"L3JlZ2lzdHJ5L25hbWVzcGFjZXMvdGVzdA==\", \"create_revision\": 11670741, \"mod_revision\": 11670741, \"version\": 1, \"value\": \"azhzAAoPCgJ2MRIJTmFtZXNwYWNlElwKQgoEdGVzdBIAGgAiACokYWM1YmJjOTQtNTkxZi0xMWVhLWJiOTQtNmM5MmJmM2I3NmI1MgA4AEIICJuf3fIFEAB6ABIMCgprdWJlcm5ldGVzGggKBkFjdGl2ZRoAIgA=\" } ], \"count\": 1 } 其中key可以通过base64解码出来\necho \"L3JlZ2lzdHJ5L25hbWVzcGFjZXMvdGVzdA==\" | base64 --decode # output /registry/namespaces/test value是值可以通过安装etcdhelper工具解析出来。\nalias ehelper='etcdhelper -key /etc/kubernetes/pki/apiserver-etcd-client.key -cert /etc/kubernetes/pki/apiserver-etcd-client.crt -cacert /etc/kubernetes/pki/etcd/ca.crt'\n# ehelper get /registry/namespaces/test /v1, Kind=Namespace { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"test\", \"uid\": \"ac5bbc94-591f-11ea-bb94-6c92bf3b76b5\", \"creationTimestamp\": \"2020-02-27T05:11:55Z\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Active\" } } 5. 注意事项 由于k8s的etcd数据为了性能考虑，默认通过protobuf格式存储，不要通过手动的方式去修改或添加k8s数据。 不推荐使用json格式存储etcd数据，如果需要json格式，可以使用--storage-media-type=application/json参数存储，参考：https://github.com/kubernetes/kubernetes/issues/44670 6. 快捷命令 由于etcdctl的命令需要添加很多认证参数和endpoints的参数，因此可以使用别名的方式来简化命令。\n# etcdctl alias ectl='ETCDCTL_API=3 etcdctl --endpoints=\u003cetcd-ip-1\u003e:2379,\u003cetcd-ip-2\u003e:2379,\u003cetcd-ip-3\u003e:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt' # etcdhelper alias ehelper='etcdhelper -key /etc/kubernetes/pki/apiserver-etcd-client.key -cert /etc/kubernetes/pki/apiserver-etcd-client.crt -cacert /etc/kubernetes/pki/etcd/ca.crt' 6.1. etcdhelper的使用 etcdhelper文档参考：https://github.com/openshift/origin/tree/master/tools/etcdhelper\n# 必要的认证参数 -key - points to master.etcd-client.key -cert - points to master.etcd-client.crt -cacert - points to ca.crt # 命令操作参数 ls - list all keys starting with prefix get - get the specific value of a key dump - dump the entire contents of the etcd 示例\n$ ehelper ls /registry/leases/ /registry/leases/kube-node-lease/\u003cip-1\u003e /registry/leases/kube-node-lease/\u003cip-2\u003e /registry/leases/kube-node-lease/\u003cip-3\u003e $ ehelper get \u003ckey\u003e 7. RBAC 附RBAC相关的key。\nclusterrolebindings\n/registry/clusterrolebindings/cluster-admin /registry/clusterrolebindings/flannel /registry/clusterrolebindings/galaxy /registry/clusterrolebindings/helm /registry/clusterrolebindings/kube-state-metrics /registry/clusterrolebindings/kubeadm:kubelet-bootstrap /registry/clusterrolebindings/kubeadm:node-autoapprove-bootstrap /registry/clusterrolebindings/kubeadm:node-autoapprove-certificate-rotation /registry/clusterrolebindings/kubeadm:node-proxier /registry/clusterrolebindings/lbcf-controller /registry/clusterrolebindings/prometheus-k8s /registry/clusterrolebindings/prometheus-operator /registry/clusterrolebindings/system:aws-cloud-provider /registry/clusterrolebindings/system:basic-user /registry/clusterrolebindings/system:controller:attachdetach-controller /registry/clusterrolebindings/system:controller:certificate-controller /registry/clusterrolebindings/system:controller:clusterrole-aggregation-controller /registry/clusterrolebindings/system:controller:cronjob-controller /registry/clusterrolebindings/system:controller:daemon-set-controller /registry/clusterrolebindings/system:controller:deployment-controller /registry/clusterrolebindings/system:controller:disruption-controller /registry/clusterrolebindings/system:controller:endpoint-controller /registry/clusterrolebindings/system:controller:expand-controller /registry/clusterrolebindings/system:controller:generic-garbage-collector /registry/clusterrolebindings/system:controller:horizontal-pod-autoscaler /registry/clusterrolebindings/system:controller:job-controller /registry/clusterrolebindings/system:controller:namespace-controller /registry/clusterrolebindings/system:controller:node-controller /registry/clusterrolebindings/system:controller:persistent-volume-binder /registry/clusterrolebindings/system:controller:pod-garbage-collector /registry/clusterrolebindings/system:controller:pv-protection-controller /registry/clusterrolebindings/system:controller:pvc-protection-controller /registry/clusterrolebindings/system:controller:replicaset-controller /registry/clusterrolebindings/system:controller:replication-controller /registry/clusterrolebindings/system:controller:resourcequota-controller /registry/clusterrolebindings/system:controller:route-controller /registry/clusterrolebindings/system:controller:service-account-controller /registry/clusterrolebindings/system:controller:service-controller /registry/clusterrolebindings/system:controller:statefulset-controller /registry/clusterrolebindings/system:controller:ttl-controller /registry/clusterrolebindings/system:coredns /registry/clusterrolebindings/system:discovery /registry/clusterrolebindings/system:kube-controller-manager /registry/clusterrolebindings/system:kube-dns /registry/clusterrolebindings/system:kube-scheduler /registry/clusterrolebindings/system:node /registry/clusterrolebindings/system:node-proxier /registry/clusterrolebindings/system:public-info-viewer /registry/clusterrolebindings/system:volume-scheduler clusterroles\n/registry/clusterroles/admin /registry/clusterroles/cluster-admin /registry/clusterroles/edit /registry/clusterroles/flannel /registry/clusterroles/kube-state-metrics /registry/clusterroles/lbcf-controller /registry/clusterroles/prometheus-k8s /registry/clusterroles/prometheus-operator /registry/clusterroles/system:aggregate-to-admin /registry/clusterroles/system:aggregate-to-edit /registry/clusterroles/system:aggregate-to-view /registry/clusterroles/system:auth-delegator /registry/clusterroles/system:aws-cloud-provider /registry/clusterroles/system:basic-user /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:nodeclient /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient /registry/clusterroles/system:controller:attachdetach-controller /registry/clusterroles/system:controller:certificate-controller /registry/clusterroles/system:controller:clusterrole-aggregation-controller /registry/clusterroles/system:controller:cronjob-controller /registry/clusterroles/system:controller:daemon-set-controller /registry/clusterroles/system:controller:deployment-controller /registry/clusterroles/system:controller:disruption-controller /registry/clusterroles/system:controller:endpoint-controller /registry/clusterroles/system:controller:expand-controller /registry/clusterroles/system:controller:generic-garbage-collector /registry/clusterroles/system:controller:horizontal-pod-autoscaler /registry/clusterroles/system:controller:job-controller /registry/clusterroles/system:controller:namespace-controller /registry/clusterroles/system:controller:node-controller /registry/clusterroles/system:controller:persistent-volume-binder /registry/clusterroles/system:controller:pod-garbage-collector /registry/clusterroles/system:controller:pv-protection-controller /registry/clusterroles/system:controller:pvc-protection-controller /registry/clusterroles/system:controller:replicaset-controller /registry/clusterroles/system:controller:replication-controller /registry/clusterroles/system:controller:resourcequota-controller /registry/clusterroles/system:controller:route-controller /registry/clusterroles/system:controller:service-account-controller /registry/clusterroles/system:controller:service-controller /registry/clusterroles/system:controller:statefulset-controller /registry/clusterroles/system:controller:ttl-controller /registry/clusterroles/system:coredns /registry/clusterroles/system:csi-external-attacher /registry/clusterroles/system:csi-external-provisioner /registry/clusterroles/system:discovery /registry/clusterroles/system:heapster /registry/clusterroles/system:kube-aggregator /registry/clusterroles/system:kube-controller-manager /registry/clusterroles/system:kube-dns /registry/clusterroles/system:kube-scheduler /registry/clusterroles/system:kubelet-api-admin /registry/clusterroles/system:node /registry/clusterroles/system:node-bootstrapper /registry/clusterroles/system:node-problem-detector /registry/clusterroles/system:node-proxier /registry/clusterroles/system:persistent-volume-provisioner /registry/clusterroles/system:public-info-viewer /registry/clusterroles/system:volume-scheduler /registry/clusterroles/view rolebindings\n/registry/rolebindings/kube-public/kubeadm:bootstrap-signer-clusterinfo /registry/rolebindings/kube-public/system:controller:bootstrap-signer /registry/rolebindings/kube-system/kube-proxy /registry/rolebindings/kube-system/kube-state-metrics /registry/rolebindings/kube-system/kubeadm:kubeadm-certs /registry/rolebindings/kube-system/kubeadm:kubelet-config-1.14 /registry/rolebindings/kube-system/kubeadm:nodes-kubeadm-config /registry/rolebindings/kube-system/system::extension-apiserver-authentication-reader /registry/rolebindings/kube-system/system::leader-locking-kube-controller-manager /registry/rolebindings/kube-system/system::leader-locking-kube-scheduler /registry/rolebindings/kube-system/system:controller:bootstrap-signer /registry/rolebindings/kube-system/system:controller:cloud-provider /registry/rolebindings/kube-system/system:controller:token-cleaner roles\n/registry/roles/kube-public/kubeadm:bootstrap-signer-clusterinfo /registry/roles/kube-public/system:controller:bootstrap-signer /registry/roles/kube-system/extension-apiserver-authentication-reader /registry/roles/kube-system/kube-proxy /registry/roles/kube-system/kube-state-metrics-resizer /registry/roles/kube-system/kubeadm:kubeadm-certs /registry/roles/kube-system/kubeadm:kubelet-config-1.14 /registry/roles/kube-system/kubeadm:nodes-kubeadm-config /registry/roles/kube-system/system::leader-locking-kube-controller-manager /registry/roles/kube-system/system::leader-locking-kube-scheduler /registry/roles/kube-system/system:controller:bootstrap-signer /registry/roles/kube-system/system:controller:cloud-provider /registry/roles/kube-system/system:controller:token-cleaner 参考：\nhttps://github.com/etcd-io/etcd/tree/master/etcdctl https://github.com/openshift/origin/tree/master/tools/etcdhelper using-etcdctl-to-access-kubernetes-data 如何读取Kubernetes存储在etcd上的数据 https://github.com/kubernetes/kubernetes/issues/44670 ","categories":"","description":"","excerpt":"1. 读取数据key 使用以下命令列出所有的key。\nETCDCTL_API=3 etcdctl …","ref":"/kubernetes-notes/etcd/k8s-etcd-data/","tags":["Etcd"],"title":"Etcd中的k8s数据"},{"body":"Pod调度 在kubernetes集群中，Pod（container）是应用的载体，一般通过RC、Deployment、DaemonSet、Job等对象来完成Pod的调度与自愈功能。\n1. RC、Deployment:全自动调度 RC的功能即保持集群中始终运行着指定个数的Pod。\n在调度策略上主要有：\n系统内置调度算法[最优Node] NodeSelector[定向调度] NodeAffinity[亲和性调度] 2. NodeSelector[定向调度] k8s中kube-scheduler负责实现Pod的调度，内部系统通过一系列算法最终计算出最佳的目标节点。如果需要将Pod调度到指定Node上，则可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配来达到目的。\n1、kubectl label nodes {node-name} {label-key}={label-value}\n2、nodeSelector: {label-key}:{label-value}\n如果给多个Node打了相同的标签，则scheduler会根据调度算法从这组Node中选择一个可用的Node来调度。\n如果Pod的nodeSelector的标签在Node中没有对应的标签，则该Pod无法被调度成功。\nNode标签的使用场景：\n对集群中不同类型的Node打上不同的标签，可控制应用运行Node的范围。例如role=frontend;role=backend;role=database。\n3. NodeAffinity[亲和性调度] NodeAffinity意为Node亲和性调度策略，NodeSelector为精确匹配，NodeAffinity为条件范围匹配，通过In（属于）、NotIn（不属于）、Exists（存在一个条件）、DoesNotExist（不存在）、Gt（大于）、Lt（小于）等操作符来选择Node，使调度更加灵活。\nRequiredDuringSchedulingRequiredDuringExecution：类似于NodeSelector，但在Node不满足条件时，系统将从该Node上移除之前调度上的Pod。 RequiredDuringSchedulingIgnoredDuringExecution：与上一个类似，区别是在Node不满足条件时，系统不一定从该Node上移除之前调度上的Pod。 PreferredDuringSchedulingIgnoredDuringExecution：指定在满足调度条件的Node中，哪些Node应更优先地进行调度。同时在Node不满足条件时，系统不一定从该Node上移除之前调度上的Pod。 如果同时设置了NodeSelector和NodeAffinity，则系统将需要同时满足两者的设置才能进行调度。\n4. DaemonSet：特定场景调度 DaemonSet是kubernetes1.2版本新增的一种资源对象，用于管理在集群中每个Node上仅运行一份Pod的副本实例。\n该用法适用的应用场景：\n在每个Node上运行一个GlusterFS存储或者Ceph存储的daemon进程。 在每个Node上运行一个日志采集程序：fluentd或logstach。 在每个Node上运行一个健康程序，采集该Node的运行性能数据，例如：Prometheus Node Exportor、collectd、New Relic agent或Ganglia gmond等。 DaemonSet的Pod调度策略与RC类似，除了使用系统内置算法在每台Node上进行调度，也可以通过NodeSelector或NodeAffinity来指定满足条件的Node范围进行调度。\n5. Job：批处理调度 kubernetes从1.2版本开始支持批处理类型的应用，可以通过kubernetes Job资源对象来定义并启动一个批处理任务。批处理任务通常并行（或串行）启动多个计算进程去处理一批工作项（work item），处理完后，整个批处理任务结束。\n5.1. 批处理的三种模式 批处理按任务实现方式不同分为以下几种模式：\nJob Template Expansion模式 一个Job对象对应一个待处理的Work item，有几个Work item就产生几个独立的Job，通过适用于Work item数量少，每个Work item要处理的数据量比较大的场景。例如有10个文件（Work item）,每个文件（Work item）为100G。\nQueue with Pod Per Work Item 采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，其中Job会启动N个Pod，每个Pod对应一个Work item。\nQueue with Variable Pod Count 采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，其中Job会启动N个Pod，每个Pod对应一个Work item。但Pod的数量是可变的。\n5.2. Job的三种类型 1）Non-parallel Jobs\n通常一个Job只启动一个Pod,除非Pod异常才会重启该Pod,一旦此Pod正常结束，Job将结束。\n2）Parallel Jobs with a fixed completion count\n并行Job会启动多个Pod，此时需要设定Job的.spec.completions参数为一个正数，当正常结束的Pod数量达到该值则Job结束。\n3）Parallel Jobs with a work queue\n任务队列方式的并行Job需要一个独立的Queue，Work item都在一个Queue中存放，不能设置Job的.spec.completions参数。\n此时Job的特性：\n每个Pod能独立判断和决定是否还有任务项需要处理 如果某个Pod正常结束，则Job不会再启动新的Pod 如果一个Pod成功结束，则此时应该不存在其他Pod还在干活的情况，它们应该都处于即将结束、退出的状态 如果所有的Pod都结束了，且至少一个Pod成功结束，则整个Job算是成功结束 参考文章\n《Kubernetes权威指南》 ","categories":"","description":"","excerpt":"Pod调度 在kubernetes集群中，Pod（container）是应用的载体，一般通 …","ref":"/kubernetes-notes/concepts/pod/pod-scheduler/","tags":["Kubernetes"],"title":"Pod调度"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/cilium/","tags":"","title":"Cilium"},{"body":"本文主要分析DaemonSetController的源码逻辑，daemonset是运行在指定节点上的服务，常用来作为agent类的服务来配置，也是k8s最常用的控制器之一。\n1. startDaemonSetController startDaemonSetController是入口函数，先New后Run。\nfunc startDaemonSetController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) { dsc, err := daemon.NewDaemonSetsController( ctx, controllerContext.InformerFactory.Apps().V1().DaemonSets(), controllerContext.InformerFactory.Apps().V1().ControllerRevisions(), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.InformerFactory.Core().V1().Nodes(), controllerContext.ClientBuilder.ClientOrDie(\"daemon-set-controller\"), flowcontrol.NewBackOff(1*time.Second, 15*time.Minute), ) if err != nil { return nil, true, fmt.Errorf(\"error creating DaemonSets controller: %v\", err) } go dsc.Run(ctx, int(controllerContext.ComponentConfig.DaemonSetController.ConcurrentDaemonSetSyncs)) return nil, true, nil } 2. NewDaemonSetsController NewDaemonSetsController仍然是常见的k8s controller初始化逻辑:\n常用配置初始化 根据所需要的对象，添加event handler，便于监听所需要对象的事件变化，此处包括ds, pod，node三个对象。 赋值syncHandler，即具体实现是syncDaemonSet函数。 func NewDaemonSetsController( ctx context.Context, daemonSetInformer appsinformers.DaemonSetInformer, historyInformer appsinformers.ControllerRevisionInformer, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, kubeClient clientset.Interface, failedPodsBackoff *flowcontrol.Backoff, ) (*DaemonSetsController, error) { // 常用配置初始化 dsc := \u0026DaemonSetsController{ kubeClient: kubeClient, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"daemonset-controller\"}), podControl: controller.RealPodControl{ KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"daemonset-controller\"}), }, crControl: controller.RealControllerRevisionControl{ KubeClient: kubeClient, }, burstReplicas: BurstReplicas, expectations: controller.NewControllerExpectations(), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"daemonset\"), } // 添加event handler daemonSetInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addDaemonset(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updateDaemonset(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dsc.deleteDaemonset(logger, obj) }, }) dsc.dsLister = daemonSetInformer.Lister() dsc.dsStoreSynced = daemonSetInformer.Informer().HasSynced historyInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addHistory(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updateHistory(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dsc.deleteHistory(logger, obj) }, }) dsc.historyLister = historyInformer.Lister() dsc.historyStoreSynced = historyInformer.Informer().HasSynced // 添加pod的event handler podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addPod(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updatePod(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dsc.deletePod(logger, obj) }, }) dsc.podLister = podInformer.Lister() dsc.podStoreSynced = podInformer.Informer().HasSynced // 添加node的event handler nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addNode(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updateNode(logger, oldObj, newObj) }, }, ) dsc.nodeStoreSynced = nodeInformer.Informer().HasSynced dsc.nodeLister = nodeInformer.Lister() // 赋值syncHandler，具体的controller处理代码 dsc.syncHandler = dsc.syncDaemonSet dsc.enqueueDaemonSet = dsc.enqueue dsc.failedPodsBackoff = failedPodsBackoff return dsc, nil } 3. Run Run函数与其他controller的逻辑一致不再分析，具体可以阅读本源码分析系列的replicaset-controller分析。\nfunc (dsc *DaemonSetsController) Run(ctx context.Context, workers int) { defer utilruntime.HandleCrash() dsc.eventBroadcaster.StartStructuredLogging(0) dsc.eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{Interface: dsc.kubeClient.CoreV1().Events(\"\")}) defer dsc.eventBroadcaster.Shutdown() defer dsc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info(\"Starting daemon sets controller\") defer logger.Info(\"Shutting down daemon sets controller\") if !cache.WaitForNamedCacheSync(\"daemon sets\", ctx.Done(), dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) { return } for i := 0; i \u003c workers; i++ { go wait.UntilWithContext(ctx, dsc.runWorker, time.Second) } go wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, ctx.Done()) \u003c-ctx.Done() } 3.1. processNextWorkItem processNextWorkItem可参考replicaset-controller对该部分的分析。\nfunc (dsc *DaemonSetsController) runWorker(ctx context.Context) { for dsc.processNextWorkItem(ctx) { } } // processNextWorkItem deals with one key off the queue. It returns false when it's time to quit. func (dsc *DaemonSetsController) processNextWorkItem(ctx context.Context) bool { dsKey, quit := dsc.queue.Get() if quit { return false } defer dsc.queue.Done(dsKey) err := dsc.syncHandler(ctx, dsKey.(string)) if err == nil { dsc.queue.Forget(dsKey) return true } utilruntime.HandleError(fmt.Errorf(\"%v failed with : %v\", dsKey, err)) dsc.queue.AddRateLimited(dsKey) return true } 4. syncDaemonSet syncDaemonSet是控制器具体的实现逻辑，即每个控制器的核心实现大部分在syncHandler这个函数中。\nfunc (dsc *DaemonSetsController) syncDaemonSet(ctx context.Context, key string) error { // 为了突出代码重点，已删除非必要部分。 // 获取集群中的ds对象 namespace, name, err := cache.SplitMetaNamespaceKey(key) ds, err := dsc.dsLister.DaemonSets(namespace).Get(name) // 获取机器列表 nodeList, err := dsc.nodeLister.List(labels.Everything()) everything := metav1.LabelSelector{} if reflect.DeepEqual(ds.Spec.Selector, \u0026everything) { dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, SelectingAllReason, \"This daemon set is selecting all pods. A non-empty selector is required.\") return nil } // Construct histories of the DaemonSet, and get the hash of current history cur, old, err := dsc.constructHistory(ctx, ds) hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey] if !dsc.expectations.SatisfiedExpectations(logger, dsKey) { // Only update status. Don't raise observedGeneration since controller didn't process object of that generation. return dsc.updateDaemonSetStatus(ctx, ds, nodeList, hash, false) } // 处理daemonset中的pod创建及删除 err = dsc.updateDaemonSet(ctx, ds, nodeList, hash, dsKey, old) // 更新状态 statusErr := dsc.updateDaemonSetStatus(ctx, ds, nodeList, hash, true) switch { case err != nil \u0026\u0026 statusErr != nil: logger.Error(statusErr, \"Failed to update status\", \"daemonSet\", klog.KObj(ds)) return err case err != nil: return err case statusErr != nil: return statusErr } return nil } 5. manage manage是具体的daemonset的pod创建及删除的具体代码。manage入口在updateDaemonSet的函数中。\nfunc (dsc *DaemonSetsController) updateDaemonSet(ctx context.Context, ds *apps.DaemonSet, nodeList []*v1.Node, hash, key string, old []*apps.ControllerRevision) error { // manage处理pod的创建及删除 err := dsc.manage(ctx, ds, nodeList, hash) err = dsc.cleanupHistory(ctx, ds, old) if err != nil { return fmt.Errorf(\"failed to clean up revisions of DaemonSet: %w\", err) } return nil } 以下是manage的具体代码\nfunc (dsc *DaemonSetsController) manage(ctx context.Context, ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error { // 查找daemonset中pod和node的映射 nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ctx, ds, false) // 计算需要创建和删除的pod var nodesNeedingDaemonPods, podsToDelete []string for _, node := range nodeList { nodesNeedingDaemonPodsOnNode, podsToDeleteOnNode := dsc.podsShouldBeOnNode( logger, node, nodeToDaemonPods, ds, hash) nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, nodesNeedingDaemonPodsOnNode...) podsToDelete = append(podsToDelete, podsToDeleteOnNode...) } // Remove unscheduled pods assigned to not existing nodes when daemonset pods are scheduled by scheduler. // If node doesn't exist then pods are never scheduled and can't be deleted by PodGCController. podsToDelete = append(podsToDelete, getUnscheduledPodsWithoutNode(nodeList, nodeToDaemonPods)...) // 根据上述的计算结果，实现具体创建和删除pod的操作 if err = dsc.syncNodes(ctx, ds, podsToDelete, nodesNeedingDaemonPods, hash); err != nil { return err } return nil } 6. syncNodes syncNodes根据传入的需要创建和删除的pod，实现具体的创建和删除pod的操作。\nfunc (dsc *DaemonSetsController) syncNodes(ctx context.Context, ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error { // 已删除次要代码 dsKey, err := controller.KeyFunc(ds) batchSize := integer.IntMin(createDiff, controller.SlowStartInitialBatchSize) for pos := 0; createDiff \u003e pos; batchSize, pos = integer.IntMin(2*batchSize, createDiff-(pos+batchSize)), pos+batchSize { errorCount := len(errCh) createWait.Add(batchSize) for i := pos; i \u003c pos+batchSize; i++ { go func(ix int) { defer createWait.Done() // 批量创建pod err := dsc.podControl.CreatePods(ctx, ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) }(i) } createWait.Wait() } deleteWait := sync.WaitGroup{} deleteWait.Add(deleteDiff) for i := 0; i \u003c deleteDiff; i++ { go func(ix int) { defer deleteWait.Done() // 批量删除pod if err := dsc.podControl.DeletePod(ctx, ds.Namespace, podsToDelete[ix], ds); err != nil { dsc.expectations.DeletionObserved(logger, dsKey) } }(i) } deleteWait.Wait() // 处理错误 errors := []error{} close(errCh) for err := range errCh { errors = append(errors, err) } return utilerrors.NewAggregate(errors) } 参考：\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go ","categories":"","description":"","excerpt":"本文主要分析DaemonSetController的源码逻辑，daemonset是运行在指定节点上的服务，常用来作为agent类的服务来配 …","ref":"/k8s-source-code-analysis/kube-controller-manager/daemonset-controller/","tags":["源码分析"],"title":"kube-controller-manager源码分析（五）之 DaemonSetController"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/helm/","tags":"","title":"helm工具"},{"body":"1.27 参考：\nKubernetes 在 v1.27 中移除的特性和主要变更\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md#changelog-since-v1260\n202304 | K8s 1.27 正式发布 - DaoCloud Enterprise\n（一）重要更新 k8s.gcr.io 重定向到 registry.k8s.io 相关说明 Kubernetes 项目为了托管其容器镜像，使用社区拥有的一个名为 registry.k8s.io. 的镜像仓库。从 3 月 20 日起，所有来自过期 k8s.gcr.io 仓库的流量将被重定向到 registry.k8s.io。 已弃用的 k8s.gcr.io 仓库最终将被淘汰。Kubernetes v1.27 版本不会发布到旧的仓库。\n原地调整 Pod 资源 (alpha) 参考：Kubernetes 1.27: 原地调整 Pod 资源 (alpha) | Kubernetes\n在 Kubernetes v1.27 中，添加了一个新的 alpha 特性，允许用户调整分配给 Pod 的 CPU 和内存资源大小，而无需重新启动容器。 首先，API 层面现在允许修改 Pod 容器中的 resources 字段下的 cpu 和 memory 资源。资源修改只需 patch 正在运行的 pod 规约即可。\nStatefulSet PVC 自动删除功能特性 Beta 在 v1.23 中引入的 StatefulSetAutoDeletePVC 功能将在 1.27 版本中升级为 Beta，并默认开启。 然而，默认开启并不意味着所有 StatefulSet 的 PVC 都将自动删除。\n优化大型集群中 kube-proxy 的 iptables 模式性能 功能 MinimizeIPTablesRestore 在 1.26 版本中引入，并在 1.27 版本中升级为 Beta 并默认启用。 该功能旨在改善大型集群中 kube-proxy 的 iptables 模式性能。\n如果您遇到 Service 信息未正确同步到 iptables 的问题，您可以通过将 kube-proxy 启动参数设置为 --feature-gates=MinimizeIPTablesRestore=false 来禁用该功能（并向社区提交问题）。 您还可以查看 kube-proxy 的 metrics 信息中的 sync_proxy_rules_iptables_partial_restore_failures_total 指标来监控规则同步失败的次数。\nKubelet 事件驱动 PLEG 升级为 Beta 在节点 Pod 较多的情况下，通过容器运行时的 Event 驱动 Pod 状态更新，能够有效地提升效率。 在 1.27 中，该功能已经达到了 Beta 条件，基础的 E2E 测试任务已经添加。 之所以默认关闭该功能，是因为社区认为该功能还需要补充以下验证：压力测试、恢复测试和带退避逻辑的重试。\nPod 调度就绪态功能增强 调度就绪态功能 PodSchedulingReadiness，在 v1.26 作为 Alpha 功能引入，从 v1.27 开始该功能升级为 Beta，默认开启。\nDeployment 滚动更新过程中的调度优化 在 v1.27 中，PodTopologySpread 调度策略可以区分调度 Pod 标签的值 （这里通常指 Pod 的 pod-template-hash 标签，不同 replica set 对应的 Pod 该标签的值不同）， 这样滚动更新后，新的 Pod 实例会被调度得更加均匀 。\n关于加快 Pod 启动的进展 要启用并行镜像拉取，请在 kubelet 配置中将 serializeImagePulls 字段设置为 false。 当 serializeImagePulls 被禁用时，将立即向镜像服务发送镜像拉取请求，并可以并行拉取多个镜像。\n为了在节点上具有多个 Pod 的场景中加快 Pod 启动，特别是在突然扩缩的情况下， kubelet 需要同步 Pod 状态并准备 ConfigMap、Secret 或卷。这就需要大带宽访问 kube-apiserver。在 v1.27 中，kubelet 为了提高 Pod 启动性能，将这些默认值分别提高到了 50 和 100。\n（二）弃用变更 kubelet 移除了命令行参数 --container-runtime。\n弃用的命令行参数 --pod-eviction-timeout 将被从 kube-controller-manager 中移除。\n1.26 参考：\nKubernetes 1.26 中的移除、弃用和主要变更 | Kubernetes\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#changelog-since-v1250\nhttps://docs.daocloud.io/blogs/221209-k8s-1.26/\nhttps://www.alibabacloud.com/help/zh/ack/product-overview/kubernetes-1-26-release-notes?spm=a2c63.p38356.0.0.64624df9xXfEkZ\n（一）重要更新 Kubelet Evented PLEG for Better Performance 该功能让 kubelet 在跟踪节点中 Pod 状态时，通过尽可能依赖容器运行时接口(CRI) 的通知来减少定期轮训，这会减少 kubelet 对 CPU 的使用\n新增 Alpha Feature Gate —— EventedPLEG 来控制是否开启该功能。\n优化 kube-proxy 性能，它只发送在调用 iptables-restore 中更改的规则，而不是整个规则集 PR#112200 client-go 的 SharedInformerFactory 增加 Shutdown 方法，来等待 Factory 内所有运行的 informer 都结束。 （二）弃用变更 Kubelet 不再支持 v1alpha2 版本的 CRI，接入的容器运行时必须实现 v1 版本的容器运行时接口。 Kubernetes v1.26 将不支持 containerd 1.5.x 及更早的版本；需要升级到 containerd 1.6.x 或更高版本后，才能将该节点的 kubelet 升级到 1.26。\n1.25 参考：\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#whats-new-major-themes\nKubernetes 1.25 的移除说明和主要变更 | Kubernetes\n（一）重要更新 cgroup v2 升级到 GA Kubernetes 1.25 将 cgroup v2 正式发布（GA）， 让kubelet使用最新的容器资源管理能力。一些 Kubernetes 特性专门使用 cgroup v2 来增强资源管理和隔离。 例如，MemoryQoS 特性提高了内存利用率并依赖 cgroup v2 功能来启用它。kubelet 中的新资源管理特性也将利用新的 cgroup v2 特性向前发展。\nCSI 内联存储卷正式发布GA CSI 内联存储卷与其他类型的临时卷相似，如 configMap、downwardAPI 和 secret。 重要的区别是，存储是由 CSI 驱动提供的，它允许使用第三方供应商提供的临时存储。 卷被定义为 Pod 规约的一部分，并遵循 Pod 的生命周期，这意味着卷随着 Pod 的调度而创建，并随着 Pod 的销毁而销毁。\nEphemeral Containers进入稳定版本 当pod crash的时候，无法通过kubectl exec 进入容器，这个时候可以通过临时容器[Ephemeral Containers](临时容器 | Kubernetes)\n（二）弃用变更 Kubernetes v1.25 将移除 PodSecurityPolicy，取而代之的是 Pod Security Admission（即 PodSecurity 安全准入控制器）。\n清理 IPTables 链的所有权 从 v1.25 开始，Kubelet 将逐渐迁移为不在 nat 表中创建以下 iptables 链：\nKUBE-MARK-DROP KUBE-MARK-MASQ KUBE-POSTROUTING 1.24 最新发行版本：1.24.2 (发布日期: 2022-06-15）\n不再支持：2023-09-29\n补丁版本： 1.24.1、 1.24.2\nComplete 1.24 Schedule and Changelog\nKubernetes 1.24 使用 go1.18构建，默认情况下将不再验证使用 SHA-1 哈希算法签名的证书。\n（一）重要更新 1.24.0主要参考kubernetes/CHANGELOG-1.24.major-themes\n1）kubelet完全移除Dockershim【最重大更新】 在 v1.20 中弃用后，dockershim 组件已从 kubelet 中删除。从 v1.24 开始，您将需要使用其他受支持的运行时之一（例如 containerd 或 CRI-O），或者如果您依赖 Docker 引擎作为容器运行时，则使用 cri-dockerd。有关确保您的集群已准备好进行此移除的更多信息，请参阅本[指南](Is Your Cluster Ready for v1.24? | Kubernetes)。\n2）Beta API 默认关闭 默认情况下，不会在集群中启用新的 beta API。默认情况下，现有的 beta API 和现有 beta API 的新版本将继续启用。\n3）存储容量和卷扩展到GA 存储容量跟踪支持通过 CSIStorageCapacity 对象公开当前可用的存储容量，并增强使用具有后期绑定的 CSI 卷的 pod 的调度。\n卷扩展增加了对调整现有持久卷大小的支持。\n4）避免 IP 分配给service的冲突 Kubernetes 1.24 引入了一项新的选择加入功能，允许您为服务的静态 IP 地址分配软预留范围。通过手动启用此功能，集群将更喜欢从服务 IP 地址池中自动分配，从而降低冲突风险。\n可以分配 Service ClusterIP：\n动态，这意味着集群将自动在配置的服务 IP 范围内选择一个空闲 IP。\n静态，这意味着用户将在配置的服务 IP 范围内设置一个 IP。\nService ClusterIP 是唯一的，因此，尝试使用已分配的 ClusterIP 创建 Service 将返回错误。\n（二）弃用变更 1）kubeadm kubeadm.k8s.io/v1beta2 已被弃用，并将在未来的版本中删除，可能在 3 个版本（一年）中。您应该开始将 kubeadm.k8s.io/v1beta3 用于新集群。要迁移磁盘上的旧配置文件，您可以使用 kubeadm config migrate 命令。\n默认 k​​ubeadm 配置为 containerd 套接字（Unix：unix:///var/run/containerd/containerd.sock，Windows：npipe:////./pipe/containerd-containerd）而不是 Docker 的配置.如果在集群创建期间 Init|JoinConfiguration.nodeRegistration.criSocket 字段为空，并且在主机上发现多个套接字，则总是会抛出错误并要求用户通过设置字段中的值来指定要使用的套接字。使用 crictl 与 CRI 套接字进行所有通信，以执行诸如拉取图像和获取正在运行的容器列表等操作，而不是在 Docker 的情况下使用 docker CLI。\nkubeadm 迁移到标签和污点中不再使用 master 一词。对于新的集群，标签 node-role.kubernetes.io/master 将不再添加到控制平面节点，只会添加标签 node-role.kubernetes.io/control-plane。\n2）kube-apiserver 不安全的地址标志 --address、--insecure-bind-address、--port 和 --insecure-port（自 1.20 起惰性）被删除\n弃用了--master-countflag 和--endpoint-reconciler-type=master-countreconciler，转而使用 lease reconciler。\n已弃用Service.Spec.LoadBalancerIP。\n3）kube-controller-manager kube-controller-manager 中的不安全地址标志 --address 和 --port 自 v1.20 起无效，并在 v1.24 中被删除。 4）kubelet --pod-infra-container-image kubelet 标志已弃用，将在未来版本中删除。\n以下与 dockershim 相关的标志也与 dockershim 一起被删除 --experimental-dockershim-root-directory、--docker-endpoint、--image-pull-progress-deadline、--network-plugin、--cni-conf -dir，--cni-bin-dir，--cni-cache-dir，--network-plugin-mtu。(#106907, @cyclinder)\n1.23 最新发行版本：1.23.8 (发布日期: 2022-06-15）\n不再支持：2023-02-28\n补丁版本： 1.23.1、 1.23.2、 1.23.3、 1.23.4、 1.23.5、 1.23.6、 1.23.7、 1.23.8\nComplete 1.23 Schedule and Changelog\nKubernetes 是使用 golang 1.17 构建的。此版本的 go 删除了使用 GODEBUG=x509ignoreCN=0 环境设置来重新启用将 X.509 服务证书的 CommonName 视为主机名的已弃用旧行为的能力。\n（一） 重要更新 1）FlexVolume 已弃用 FlexVolume 已弃用。 Out-of-tree CSI 驱动程序是在 Kubernetes 中编写卷驱动程序的推荐方式。FlexVolume 驱动程序的维护者应实施 CSI 驱动程序并将 FlexVolume 的用户转移到 CSI。 FlexVolume 的用户应将其工作负载转移到 CSI 驱动程序。\n2）IPv4/IPv6 双栈网络到 GA IPv4/IPv6 双栈网络从 GA 毕业。从 1.21 开始，Kubernetes 集群默认启用支持双栈网络。在 1.23 中，移除了 IPv6DualStack 功能门。双栈网络的使用不是强制性的。尽管启用了集群以支持双栈网络，但 Pod 和服务继续默认为单栈。要使用双栈网络：Kubernetes 节点具有可路由的 IPv4/IPv6 网络接口，使用支持双栈的 CNI 网络插件，Pod 配置为双栈，服务的 .spec.ipFamilyPolicy 字段设置为 PreferDualStack 或需要双栈。\n3）Horizo​​ntalPodAutoscaler v2 到 GA Horizo​​ntalPodAutoscaler API 的第 2 版在 1.23 版本中逐渐稳定。 Horizo​​ntalPodAutoscaler autoscaling/v2beta2 API 已弃用，取而代之的是新的 autoscaling/v2 API，Kubernetes 项目建议将其用于所有用例。\n4）Scheduler简化多点插件配置 kube-scheduler 正在为插件添加一个新的、简化的配置字段，以允许在一个位置启用多个扩展点。新的 multiPoint 插件字段旨在为管理员简化大多数调度程序设置。通过 multiPoint 启用的插件将自动为它们实现的每个单独的扩展点注册。例如，实现 Score 和 Filter 扩展的插件可以同时为两者启用。这意味着可以启用和禁用整个插件，而无需手动编辑单个扩展点设置。这些扩展点现在可以被抽象出来，因为它们与大多数用户无关。\n（二）已知问题 在 1.22 Kubernetes 版本附带的 etcd v3.5.0 版本中发现了数据损坏问题。请阅读 etcd 的最新[生产建议](etcd/CHANGELOG at main · etcd-io/etcd · GitHub)。\n运行etcd v3.5.2 v3.5.1和v3.5.0高负荷会导致数据损坏问题。如果etcd进程被杀,偶尔有些已提交的事务并不反映在所有的成员。建议升级到v3.5.3。\n最低推荐etcd版本运行在生产3.3.18 + 3.4.2 + v3.5.3 +。\n1.22 最新发行版本：1.22.11 (发布日期: 2022-06-15）\n不再支持：2022-10-28\n补丁版本： 1.22.1、 1.22.2、 1.22.3、 1.22.4、 1.22.5、 1.22.6、 1.22.7、 1.22.8、 1.22.9、 1.22.10、 1.22.11\nComplete 1.22 Schedule and Changelog\n（—）重要更新 1）kubeadm 允许非root用户允许kubeadm。\n现在V1beta3首选API版本;v1beta2 API也仍然是可用的,并没有弃用。\n移除对docker cgroup driver的检查，kubeadm默认使用systemd cgroup driver，需要手动将runtime配置为systemd。\nv1beta3中删除ClusterConfiguration.DNS字段，因为CoreDNS是唯一支持DNS类型。\n2）etcd etcd使用v3.5.0版本。（但是在1.23版本中发现v3.5.0有数据损坏的问题） 3）kubelet 节点支持swap内存。\n作为α特性,Kubernetes v1.22并且可以使用cgroup v2 API来控制内存分配和隔离。这个功能的目的是改善工作负载和节点可用性时对内存资源的争用。\n参考：\n发行版本 | Kubernetes\nkubernetes/CHANGELOG at master · GitHub\nsig-release/releases at master · kubernetes/sig-release · GitHub\nKubernetes 1.24 正式发布，这里是更新功能总览\n","categories":"","description":"","excerpt":"1.27 参考：\nKubernetes 在 v1.27 …","ref":"/kubernetes-notes/setup/k8s-changelog/","tags":["Kubernetes"],"title":"k8s版本记录"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/gateway/","tags":"","title":"k8s网关"},{"body":"1. kubeconfig说明 默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。 你可以通过设置 KUBECONFIG 环境变量或者设置 --kubeconfig参数来指定其他 kubeconfig 文件。\nkubeconfig内容示例：\n以下证书以文件的形式读取。\napiVersion: v1 kind: Config clusters: - cluster: certificate-authority: */ca.crt server: https://****** name: demo contexts: - context: cluster: demo user: demo name: demo current-context: demo preferences: {} users: - name: demo user: client-certificate: */client.crt client-key: */client.key 2. 将 Kubeconfig 中的证书文件转为证书数据 2.1. 证书文件转为证书数据 如果需要将证书文件替换为证书数据格式，可以通过base64的命令解码成data数据，同时修改相应的参数。\n获取证书文件的 base64 编码：\ncat \"证书文件\" | base64 将输出结果的换行符去掉，填到对应证书的data中。\n将 certificate-authority 改为 certificate-authority-data，并且将 */ca.crt 证书文件经 base64 编码后的字符串填入该位置。\n将 client-certificate 改为 client-certificate-data，并且将 */client.crt 证书文件经 base64 编码后的字符串填入该位置。\n将 client-key 改为 client-key-data，并且将 */client.key 证书文件 base64 编码后的字符串填入该位置。\n例如：\napiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZADQFURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRJd01ESXhNREEwTURJMU1Wb1hEVE13TURJd09EQTBNREkxTVZvd0ZURVRNQkVHQTFVRQpBeE1LYldsdWFXdDFZbVZEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTUc1CjYwU0txcHVXeE1mWlVabmVFakM5bjFseHFQSzdUTlVQbzROejFWcWxaQkt6NzJDVVErZjBtVGNQLy9oS3BQUVAKaG9pNndyaXJRUmVERTErRFIrOTZHVDIrSGZ3L2VHQTI5ZmErNS80UG5PWlpTUEVpS3MxVVdhc0VqSDJVZG4xTwpEejVRZk1ESkFjZlBoTzV0eUZFaGZNa2hid0Y2QkJONnh5RmJJdXl4OThmZGx5SWJJUnpLSml6VWZQcUx2WUZoCmFQbjF4WFZyT2QyMnFtblgzL2VxZXM4aG51SmpJdlVPbWRDRlhjQVRYdE00Wmw2bERvWUs2VS9vaEFzM0x4VzAKWUV4ZkcxMzFXdjIrR0t4WWV2Q0FuMitSQ3NBdFpTZk9zcVljMmorYS9FODVqdzcySlFkNGd6eGlHMCszaU14WApWaGhpcWFrY1owZlRCc0FtZHY4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUIwR0ExVWRKUVFXCk1CUUdDQ3NHQVFVRkJ3TUNCZ2dyQmdFRkJRY0RBVEFQQmdOVkhSTUJBZjhFQlRBREFRSC9NQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDKzFuU2w0dnJDTEV6eWg0VWdXR3ZWSldtV2ltM2dBWFFJU1R2WG56NXZqOXE3Z0JYSwpCRVUyakVHTFF2UEJQWUZwUjhmZllCZCtqT2xtYS9IdU9ISmw0RUxhaHJKbnIwaU9YcytoeVlpV0ZUKzZ2R05RCmY4QnAvNTlkYzY1ejVVMnlUQjd4VkhMcGYzRTRZdUN2NmZhdy9PZTNUUzZUbThZdFBXREgxNDBOR2ZKMHlWRlYKSzZsQnl5THMwMzZzT1V5ZUJpcEduOUxyKytvb09mTVZIU2dpaEJlcEl3ZVVvYk05YU1ram1Hb2VjNk5HTUN3NwpkaFNWTmdMNGxMSnRvRktoVDdTZHFjMmk2SWlwbkJrdUlHUWRJUFliQnF6MkN5eVMyRkZmeEJsV2lmNmcxMTFTClphSUlpQ0lLbXNqeDJvTFBhOUdNSjR6bERNR1hLY1ZyNnhhVQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== server: https://****** name: demo contexts: - context: cluster: demo user: demo name: demo current-context: demo preferences: {} users: - name: demo user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURBRENDQWVpZ0F3SUJBZ0lCQWpBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRJd01ESXhNekV5TXpreU5sb1hEVEl4TURJeE16RXlNemt5Tmxvd01URVhNQlVHQTFVRQpDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGakFVQmdOVkJBTVREVzFwYm1scmRXSmxMWFZ6WlhJd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDdEp5MThYOGVBaVlJc1g3Z0xQazBOZEFuRlNJU0kKeXNGMGIzS21CTk9VclRIcGtnaUFwRldmZDJaVXNZR3Iwd0VBL0FIWm9PMUxPUHdqYzEyb2o1bGIwWlNNWTlMaQpVeW9UQ3huREZIVDJIQnlPNCtGRk5pVCtXTU5FTURURWxERXhlano1WVIwb1pZVjN2V2Z5T3l2SlBna1dFU29IClVVcnVvQmRRbU5LUzhCeXhIUmFvcnFJUFRVRGkzUFJIbTlGZERKV1lNTWpnbDZmbmdHWkRQMnpjTlNFZjRGNzYKc2FUK0VhMU1kbjV5akRCNXczaHJvZXBBclc0QVUyR3NTRFZyTHY2UDFiSWV0RDdONjZrT1BBNklIUlRKbUNLLwp2aEJrbGVPMGFwcERzTERDT3hpMkc2Q1BRNDZVYVZUOEhZMk9sQU5nSmtRRDNwYjFXTnlhQlVpRkFnTUJBQUdqClB6QTlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBTHVrZ1dxWnRtbkZITlcxYgpqL012ekcxTmJyUEtWYXVobml5RzRWWnRZYzR1Uk5iSGhicEhEdThyamc2dVB1Y0xMdHAzblU2UGw4S2J2WFpiCmplNmJQR2xvV3VBcFIrVW9KRFQ2VEpDK2o2Qm5CSXpWQkNOL21lSWVPQ0hEK1k5L2dtbzRnd2Q4c2F3U0Z1bjMKZTFVekF2cHBwdTVZY05wcU92aUkxT2NjNGdxNTd2V1h1MFRIdUJkM0VtQ2JZRXUzYXhOL25ldnhOYnYxbDFRSQovSzRaOWw3MXFqaEp3SVlBaHUzek5pTWpCU1VTRjJkZnd2NmFnclhSUnN6b1Z4ejE5Mm9qM2pWU215cXZxeVFrCmZXckpsc3VhY1NDdTlKUE44OUQrVXkwVnZXZmhPdmp4cXVRSktwUW9hMzlQci81Q3YweXFKUkFIMkk5Wk1IZEYKNkJQRVBRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVQSLS0tLQpNSUlFcEFJQkFBS0NBUUVBclNjdGZGL0hnSW1DTEYrNEN6NU5EWFFKeFVpRWlNckJkRzl5cGdUVGxLMHg2WklJCmdLUlZuM2RtVkxHQnE5TUJBUHdCMmFEdFN6ajhJM05kcUkrWlc5R1VqR1BTNGxNcUV3c1p3eFIwOWh3Y2p1UGgKUlRZay9sakRSREEweEpReE1YbzgrV0VkS0dXRmQ3MW44anNyeVQ0SkZoRXFCMUZLN3FBWFVKalNrdkFjc1IwVwpxSzZpRDAxQTR0ejBSNXZSWFF5Vm1EREk0SmVuNTRCbVF6OXMzRFVoSCtCZStyR2svaEd0VEhaK2Nvd3dlY040CmE2SHFRSzF1QUZOaHJFZzFheTcrajlXeUhyUSt6ZXVwRGp3T2lCMFV5Wmdpdjc0UVpKWGp0R3FhUTdDd3dqc1kKdGh1Z2owT09sR2xVL0IwdGpwUURZQ1pFQTk2VzlWamNtZ1ZJaFFJREFRQUJBb0lCQUdDazVGTnVGaWtkRndYegphd01EZy9oRlV3ckZIZ3hIdHRCcFFBRi80aVF5d3hBT0RTYllFbDVPUTFSME90OFBoNWpvRDVSTHFRWjZTT2owCmhFc0gwMTRYVFNWS3RqTFNua0pBeU9GRWNyL0hFdjJDSFlNRzVJRCtSQWEwTFUrbk13bmRvMWpCcG9lY21uRXAKeTNHOUt3Ukkxc04xVXhNQWdhVk12NWFocGE2UzRTdENpalh3VGVVWUxpc1pSZGp5UGljUWlQN0xaSnhBcjRLTgpTUHlDNE1IZTJtV3F3cjM5cnBrMWZ3WkViMTRPMjR2Z3dMYmROTFJYdVhZSTdicEpOUGRJbEQvRExOQkJSL0FVCjhJYjNDaTZwZ2M4dFA4VzJCeW9TQUJVZUNpWDRFM21wQUttVytKbzFuU3FwQ1FnM2JGV0RpRjFrKzdEZjJZM3IKc09UT0srVUNnWUVBMTBEb3BtRVcrNnowanZadVFZdFlPWlVQQzZwV0dBaDFLWlVHZndYVWVLQ3dnNlNuQW9qRwpuMjR4bWJVdTlzRzBjd2syK0VVcXh3S2IrR2NJVTdyNVdOaUNXNVZYQzV5ZUp3OFZiMWtQekRzMSs4YzA4VjNhCkkzNHluOHpjZm1WTkZOUm1ZS1FMK1FGbnZ3ZXM4NFRleGVBb1hGY2FQcVZTNDNVV2JHRW02ZThDZ1lFQXplNFkKeUxYQ2pWNVppajFsUTdkQ2FweEQvL0dCT2JsemRocXRuSjl1OWxyVkQ4Y3RvUEVKYVkwVFFWc3FaNVhCdTNtVApLb0w4bmZjWHg4cWR3Z3gvZFRHa2c3d00rZFkrUTFuVDNOWnRFSVdVUkR3T0hLT1N1Tm5kdnU1a1Q4aXRrdHhhCktrYVlSMlNOT25iMlFzb1ZHa3F5OS9QK0xWL0FyeWdScmtaYXVNc0NnWUVBaGpUTkdUYzltaXNxeTV2d0FHTzkKM1NFSG9YRlJmbWgvakM2RFAxMUdMUE9iT21qRlREbzFCS0F5d3JBSm1RWUsyUkpzdUh4L2dGY3JJY1F6bCtqaQpvRGRWaDM1a0tEUTlFd00valE0TllIdW1XOVhITjVvWmNMbTFISmNnL3Bsd1pzVkxFNFFVaHVzT1lUZUs2TVgyCkU0OS8rcHJBSFVEOG5oNlpuWGN4U1BjQ2dZRUFub0lQcDZab1MwSjliMi9VbTJ2YS9vNnJ0TDBpOTlpc2JCTWEKNFR6RFAzTXBIc3owYlRZN1JYaW1ncDcybytiY3lUNUtMZVhISnB3RVBPL1R3SUs0Tk8veUxzZzN3TExOR0RCegphRC9Ra1hBUWNQazg3NFJrc2s1WVpkZS9kTDRHQk00QnhScXpxZmhXME5LeXVUUXRUQ0NGWTEvMm5OeGdSekp6CmNZNkwxRU1DZ1lCdXpHRkJVeXM4TFpHekFNTWdmN1VRQ0dVZERrT2dJRHdzd0dxVVlxQ2ZLcnlGYVdCOUJvSi8KMnJMVmVYNDVXTnFpa0tCMlgvckdRbGFIK25YalRBaDlpN0NrWGRySUQzeXA1cGJBa0VnYjg3dGo2Y3hONlBOcQo5cnhzOU1lR0NleFhsdjBkUUpMUkowNXU2OEVxYm44Q0RIOXFRSWZaTml0NXA0S0JFVkp3L1E9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= 2.2. 证书数据转成证书文件 grep 'certificate-authority-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d \u003e ca.crt grep 'client-key-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d \u003e client.key grep 'client-certificate-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d \u003e client.crt 3. 生成集群级别的kubeconfig 3.1. 创建ServiceAccount # 1. 创建用户 cat\u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: namespace: ${ServiceAccountNS} name: ${ServiceAccountName} EOF # 2. 创建Secret绑定ServiceAccount # k8s 1.24后的版本不再自动生成secret，绑定后当删除ServiceAccount时会自动删除secret cat\u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: ${SecretName} namespace: ${ServiceAccountNS} annotations: kubernetes.io/service-account.name: \"${ServiceAccountName}\" type: kubernetes.io/service-account-token EOF 3.2. 创建和绑定权限 如果不创建自定义权限，可以使用自带的ClusterRole: cluster-admin/admin/edit/view。如果需要生成集群admin的权限，可以绑定 cluster-admin的权限。建议权限授权时遵循最小权限原则。即需要使用的权限授权，不需要使用的权限不授权，对于删除等敏感权限慎重授权。\n如果要创建自定义权限，使用ClusterRole和ClusterRoleBinding的对象。\n只读权限：verbs: [\"get\",\"list\",\"watch\"]\n可写权限：verbs: [\"create\",\"update\",\"patch\",\"delete\",\"deletecollection\"]\n全部权限：verbs: [\"*\"]\n# 3. 创建权限: 可以自定义权限，或者使用clusterrole: cluster-admin/admin/edit/view权限 # 只读权限：verbs: [\"get\",\"list\",\"watch\"] # 可写权限：verbs: [\"create\",\"update\",\"patch\",\"delete\",\"deletecollection\"] # 全部权限：verbs: [\"*\"] cat\u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ${ClusterRoleName} rules: - apiGroups: [\"*\"] resources: [\"pods\",\"deployments\"] verbs: [\"*\"] EOF # 4. 绑定权限 cat\u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ${ClusterRoleBindingName} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ${ClusterRoleName} EOF 3.3. 创建kubeconfig # 5. 基于secret获取token TOKEN=$(kubectl get secret ${SecretName} -n ${ServiceAccountNS} -o jsonpath={\".data.token\"} | base64 -d) # 6. 创建kubeconfig文件 kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-cluster ${KubeConfigCluster} \\ --server=https://${APISERVER} \\ --certificate-authority=${CaFilePath} \\ --embed-certs=true kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-credentials ${USER} --token=${TOKEN} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-context ${USER}@${KubeConfigCluster} --cluster=${KubeConfigCluster} --user=${USER} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config use-context ${USER}@${KubeConfigCluster} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config view 4. 生成namespace的kubeconfig 4.1. 创建ServiceAccount 创建ServiceAccout同上，没有区别。\n4.2. 创建和绑定权限 创建namespace级别的权限用Role和RoleBinding的对象。一个ServiceAccount可以绑定多个权限，可以通过创建ClusterRoleBinding或RoleBinding来实现对多个namespace的权限控制。\n# 3. 创建权限: 可以自定义权限，或者使用clusterrole: admin/edit/view权限 cat\u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: ${USER_NAMESPACE} name: ${RoleName} rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"*\"] EOF # 4. 绑定权限 cat\u003c\u003cEOF | kubectl apply -f - kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ${RoleBindingName} namespace: ${USER_NAMESPACE} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: kind: Role name: ${RoleName} apiGroup: rbac.authorization.k8s.io EOF 4.3. 创建kubeconfig # 5. 基于secret获取token TOKEN=$(kubectl get secret ${SecretName} -n ${ServiceAccountNS} -o jsonpath={\".data.token\"} | base64 -d) # 6. 创建kubeconfig文件 kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-cluster ${KubeConfigCluster} \\ --server=https://${APISERVER} \\ --certificate-authority=${CaFilePath} \\ --embed-certs=true kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-credentials ${USER} --token=${TOKEN} # 与cluster级别kubeconfig不同的增加了--namespace kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-context ${USER}@${KubeConfigCluster} \\ --cluster=${KubeConfigCluster} --user=${USER} --namespace=${USER_NAMESPACE} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config use-context ${USER}@${KubeConfigCluster} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config view 5. 创建token来访问apiserver 以上方式是通过创建ServiceAccount的Secret的方式来获取token，该token是永久生效的，如果要使token失效可以删除对应的Secret对象，\n如果要使用指定过期期限的token权限，可以通过以下命令生成：\n# 生成过期时间365天的token TOKEN=$(kubectl -n ${NAMESPACE} create token ${USER} --duration 8760h) kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-credentials ${USER} --token=${TOKEN} 参考：\n如何将 Kubeconfig 中的证书文件转为证书数据 - CODING 帮助中心 ","categories":"","description":"","excerpt":"1. kubeconfig说明 默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。 你可以通过设 …","ref":"/kubernetes-notes/operation/kubectl/kubeconfig/","tags":["Kubernetes"],"title":"kubeconfig的使用"},{"body":"1. if语句 if 语句通过关系运算符判断表达式的真假来决定执行哪个分支。Shell 有三种 if ... else 语句：\nif ... fi 语句； if ... else ... fi 语句； if ... elif ... else ... fi 语句。 1.1. if ... else if ... else 语句的语法：\nif [ expression ] then Statement(s) to be executed if expression is true fi 如果 expression 返回 true，then 后边的语句将会被执行；如果返回 false，不会执行任何语句。 最后必须以 fi 来结尾闭合 if，fi 就是 if 倒过来拼写。 注意：expression 和方括号([ ])之间必须有空格，否则会有语法错误。\n1.2. if ... else ... fi if ... else ... fi 语句的语法：\nif [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 如果 expression 返回 true，那么 then 后边的语句将会被执行；否则，执行 else 后边的语句。\n1.3. if ... elif ... fi 多分枝选择 if ... elif ... fi 语句可以对多个条件进行判断，语法为：\nif [ expression 1 ] then Statement(s) to be executed if expression 1 is true elif [ expression 2 ] then Statement(s) to be executed if expression 2 is true elif [ expression 3 ] then Statement(s) to be executed if expression 3 is true else Statement(s) to be executed if no expression is true fi 哪一个 expression 的值为 true，就执行哪个 expression 后面的语句；如果都为 false，那么不执行任何语句。\nif ... else 语句也可以写成一行，用分号隔开，以命令的方式来运行\nif ... else 语句也经常与 test 命令结合使用，test 命令用于检查某个条件是否成立，与方括号([ ])类似。\nif test $[num1] -eq $[num2] 2. case语句 case ... esac 与其他语言中的 switch ... case 语句类似，是一种多分枝选择结构。\ncase 值 in模式1) #-------\u003e匹配值 command1 command2 command3 ;; #------\u003ebreak 模式2） command1 command2 command3 ;; *) # -------\u003e相当于default command1 command2 command3 ;; esac #-----\u003e结束标志 case工作方式如上所示。\n取值后面必须为关键字 in， 每一模式必须以右括号结束 取值可以为变量或常数。 匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。 ;; 与其他语言中的 break 类似，意思是跳到整个 case 语句的最后。 取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。 如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。 参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. if语句 if 语句通过关系运算符判断表达式的真假来决定执行哪个分支。Shell 有三种 if ... else 语句：\nif ... …","ref":"/linux-notes/shell/shell-if/","tags":["Shell"],"title":"Shell 判断语句"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析kubelet中syncPod的部分。\n1. managePodLoop managePodLoop通过读取podUpdateschannel的信息，执行syncPodFn函数，而syncPodFn函数在newPodWorkers的时候赋值了，即kubelet.syncPod。\nmanagePodLoop完整代码如下：\n此部分代码位于pkg/kubelet/pod_workers.go\nfunc (p *podWorkers) managePodLoop(podUpdates \u003c-chan UpdatePodOptions) { var lastSyncTime time.Time for update := range podUpdates { err := func() error { podUID := update.Pod.UID // This is a blocking call that would return only if the cache // has an entry for the pod that is newer than minRuntimeCache // Time. This ensures the worker doesn't start syncing until // after the cache is at least newer than the finished time of // the previous sync. status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil { // This is the legacy event thrown by manage pod loop // all other events are now dispatched from syncPodFn p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, \"error determining status: %v\", err) return err } // 该部分的syncPodFn实际上的实现函数是kubelet.syncPod err = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, }) lastSyncTime = time.Now() return err }() // notify the call-back function if the operation succeeded or not if update.OnCompleteFunc != nil { update.OnCompleteFunc(err) } if err != nil { // IMPORTANT: we do not log errors here, the syncPodFn is responsible for logging errors glog.Errorf(\"Error syncing pod %s (%q), skipping: %v\", update.Pod.UID, format.Pod(update.Pod), err) } p.wrapUp(update.Pod.UID, err) } } 以下分析syncPod相关逻辑。\n2. syncPod syncPod可以理解为是一个单个pod进行同步任务的事务脚本。其中入参是syncPodOptions，syncPodOptions记录了需要同步的pod的相关信息。具体定义如下：\n// syncPodOptions provides the arguments to a SyncPod operation. type syncPodOptions struct { // the mirror pod for the pod to sync, if it is a static pod mirrorPod *v1.Pod // pod to sync pod *v1.Pod // the type of update (create, update, sync) updateType kubetypes.SyncPodType // the current status podStatus *kubecontainer.PodStatus // if update type is kill, use the specified options to kill the pod. killPodOptions *KillPodOptions } syncPod主要执行以下的工作流：\n如果是正在创建的pod，则记录pod worker的启动latency。 调用generateAPIPodStatus为pod提供v1.PodStatus信息。 如果pod是第一次运行，记录pod的启动latency。 更新status manager中的pod状态。 如果pod不应该被运行则杀死pod。 如果pod是一个static pod，并且没有对应的mirror pod，则创建一个mirror pod。 如果没有pod的数据目录则给pod创建对应的数据目录。 等待volume被attach或mount。 获取pod的secret数据。 调用container runtime的SyncPod函数，执行相关pod操作。 更新pod的ingress和egress的traffic limit。 当以上任务流中有任何的error，则return error。在下一次执行syncPod的任务流会被再次执行。对于错误信息会被记录到event中，方便debug。\n以下对syncPod的执行过程进行分析。\nsyncPod的代码位于pkg/kubelet/kubelet.go\n2.1. SyncPodKill 首先，获取syncPodOptions的pod信息。\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType ... } 如果pod是需要被杀死的，则执行killPod，会在指定的宽限期内杀死pod。\n// if we want to kill a pod, do it now! if updateType == kubetypes.SyncPodKill { killPodOptions := o.killPodOptions if killPodOptions == nil || killPodOptions.PodStatusFunc == nil { return fmt.Errorf(\"kill pod options are required if update type is kill\") } apiPodStatus := killPodOptions.PodStatusFunc(pod, podStatus) kl.statusManager.SetPodStatus(pod, apiPodStatus) // we kill the pod with the specified grace period since this is a termination if err := kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \"error killing pod: %v\", err) // there was an error killing the pod, so we return that error directly utilruntime.HandleError(err) return err } return nil } 2.2. SyncPodCreate 如果pod是需要被创建的，则记录pod的启动latency，latency与pod在apiserver中第一次被记录相关。\n// Latency measurements for the main workflow are relative to the // first time the pod was seen by the API server. var firstSeenTime time.Time if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok { firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get() } // Record pod worker start latency if being created // TODO: make pod workers record their own latencies if updateType == kubetypes.SyncPodCreate { if !firstSeenTime.IsZero() { // This is the first time we are syncing the pod. Record the latency // since kubelet first saw the pod if firstSeenTime is set. metrics.PodWorkerStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime)) } else { glog.V(3).Infof(\"First seen time not recorded for pod %q\", pod.UID) } } 通过pod和pod status生成最终的api pod status并设置pod的IP。\n// Generate final API pod status with pod and status manager status apiPodStatus := kl.generateAPIPodStatus(pod, podStatus) // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576) // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and // set pod IP to hostIP directly in runtime.GetPodStatus podStatus.IP = apiPodStatus.PodIP 记录pod到running状态的时间。\n// Record the time it takes for the pod to become running. existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok || existingStatus.Phase == v1.PodPending \u0026\u0026 apiPodStatus.Phase == v1.PodRunning \u0026\u0026 !firstSeenTime.IsZero() { metrics.PodStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime)) } 如果pod是不可运行的，则更新pod和container的状态和相应的原因。\nrunnable := kl.canRunPod(pod) if !runnable.Admit { // Pod is not runnable; update the Pod and Container statuses to why. apiPodStatus.Reason = runnable.Reason apiPodStatus.Message = runnable.Message // Waiting containers are not creating. const waitingReason = \"Blocked\" for _, cs := range apiPodStatus.InitContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } for _, cs := range apiPodStatus.ContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } } 并更新status manager中的状态信息，杀死不可运行的pod。\n// Update status in the status manager kl.statusManager.SetPodStatus(pod, apiPodStatus) // Kill pod if it should not be running if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { var syncErr error if err := kl.killPod(pod, nil, podStatus, nil); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \"error killing pod: %v\", err) syncErr = fmt.Errorf(\"error killing pod: %v\", err) utilruntime.HandleError(syncErr) } else { if !runnable.Admit { // There was no error killing the pod, but the pod cannot be run. // Return an error to signal that the sync loop should back off. syncErr = fmt.Errorf(\"pod cannot be run: %s\", runnable.Message) } } return syncErr } 如果网络插件还没到Ready状态，则只有在使用host网络模式的情况下才启动pod。\n// If the network plugin is not ready, only start the pod if it uses the host network if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026\u0026 !kubecontainer.IsHostNetworkPod(pod) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, \"%s: %v\", NetworkNotReadyErrorMsg, rs) return fmt.Errorf(\"%s: %v\", NetworkNotReadyErrorMsg, rs) } 2.3. Cgroups 给pod创建Cgroups，如果cgroups-per-qos参数开启，则申请相应的资源。对于terminated的pod不需要创建或更新pod的Cgroups。\n当重新启动kubelet并且启用cgroups-per-qos时，应该间歇性地终止所有pod的运行容器并在qos cgroup hierarchy下重新启动。\n如果pod的cgroup已经存在或者pod第一次运行，不杀死pod中容器。\n// Create Cgroups for the pod and apply resource parameters // to them if cgroups-per-qos flag is enabled. pcm := kl.containerManager.NewPodContainerManager() // If pod has already been terminated then we need not create // or update the pod's cgroup if !kl.podIsTerminated(pod) { // When the kubelet is restarted with the cgroups-per-qos // flag enabled, all the pod's running containers // should be killed intermittently and brought back up // under the qos cgroup hierarchy. // Check if this is the pod's first sync firstSync := true for _, containerStatus := range apiPodStatus.ContainerStatuses { if containerStatus.State.Running != nil { firstSync = false break } } // Don't kill containers in pod if pod's cgroups already // exists or the pod is running for the first time podKilled := false if !pcm.Exists(pod) \u0026\u0026 !firstSync { if err := kl.killPod(pod, nil, podStatus, nil); err == nil { podKilled = true } } ... 如果pod被杀死并且重启策略是Never，则不创建或更新对应的Cgroups，否则创建和更新pod的Cgroups。\n// Create and Update pod's Cgroups // Don't create cgroups for run once pod if it was killed above // The current policy is not to restart the run once pods when // the kubelet is restarted with the new flag as run once pods are // expected to run only once and if the kubelet is restarted then // they are not expected to run again. // We don't create and apply updates to cgroup if its a run once pod and was killed above if !(podKilled \u0026\u0026 pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { if err := kl.containerManager.UpdateQOSCgroups(); err != nil { glog.V(2).Infof(\"Failed to update QoS cgroups while syncing pod: %v\", err) } if err := pcm.EnsureExists(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, \"unable to ensure pod container exists: %v\", err) return fmt.Errorf(\"failed to ensure that the pod: %v cgroups exist and are correctly applied: %v\", pod.UID, err) } } } 其中创建Cgroups是通过containerManager的UpdateQOSCgroups来执行。\nif err := kl.containerManager.UpdateQOSCgroups(); err != nil { glog.V(2).Infof(\"Failed to update QoS cgroups while syncing pod: %v\", err) } 2.4. Mirror Pod 如果pod是一个static pod，没有对应的mirror pod，则创建一个mirror pod；如果存在mirror pod则删除再重建一个mirror pod。\n// Create Mirror Pod for Static Pod if it doesn't already exist if kubepod.IsStaticPod(pod) { podFullName := kubecontainer.GetPodFullName(pod) deleted := false if mirrorPod != nil { if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) { // The mirror pod is semantically different from the static pod. Remove // it. The mirror pod will get recreated later. glog.Warningf(\"Deleting mirror pod %q because it is outdated\", format.Pod(mirrorPod)) if err := kl.podManager.DeleteMirrorPod(podFullName); err != nil { glog.Errorf(\"Failed deleting mirror pod %q: %v\", format.Pod(mirrorPod), err) } else { deleted = true } } } if mirrorPod == nil || deleted { node, err := kl.GetNode() if err != nil || node.DeletionTimestamp != nil { glog.V(4).Infof(\"No need to create a mirror pod, since node %q has been removed from the cluster\", kl.nodeName) } else { glog.V(4).Infof(\"Creating a mirror pod for static pod %q\", format.Pod(pod)) if err := kl.podManager.CreateMirrorPod(pod); err != nil { glog.Errorf(\"Failed creating a mirror pod for %q: %v\", format.Pod(pod), err) } } } } 2.5. makePodDataDirs 给pod创建数据目录。\n// Make data directories for the pod if err := kl.makePodDataDirs(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, \"error making pod data directories: %v\", err) glog.Errorf(\"Unable to make pod data directories for pod %q: %v\", format.Pod(pod), err) return err } 其中数据目录包括\nPodDir：{kubelet.rootDirectory}/pods/podUID PodVolumesDir：{PodDir}/volumes PodPluginsDir：{PodDir}/plugins // makePodDataDirs creates the dirs for the pod datas. func (kl *Kubelet) makePodDataDirs(pod *v1.Pod) error { uid := pod.UID if err := os.MkdirAll(kl.getPodDir(uid), 0750); err != nil \u0026\u0026 !os.IsExist(err) { return err } if err := os.MkdirAll(kl.getPodVolumesDir(uid), 0750); err != nil \u0026\u0026 !os.IsExist(err) { return err } if err := os.MkdirAll(kl.getPodPluginsDir(uid), 0750); err != nil \u0026\u0026 !os.IsExist(err) { return err } return nil } 2.6. mount volumes 对非terminated状态的pod挂载volume。\n// Volume manager will not mount volumes for terminated pods if !kl.podIsTerminated(pod) { // Wait for volumes to attach/mount if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, \"Unable to mount volumes for pod %q: %v\", format.Pod(pod), err) glog.Errorf(\"Unable to mount volumes for pod %q: %v; skipping pod\", format.Pod(pod), err) return err } } 2.7. PullSecretsForPod 获取pod的secret数据。\n// Fetch the pull secrets for the pod pullSecrets := kl.getPullSecretsForPod(pod) getPullSecretsForPod具体实现函数如下：\n// getPullSecretsForPod inspects the Pod and retrieves the referenced pull // secrets. func (kl *Kubelet) getPullSecretsForPod(pod *v1.Pod) []v1.Secret { pullSecrets := []v1.Secret{} for _, secretRef := range pod.Spec.ImagePullSecrets { secret, err := kl.secretManager.GetSecret(pod.Namespace, secretRef.Name) if err != nil { glog.Warningf(\"Unable to retrieve pull secret %s/%s for %s/%s due to %v. The image pull may not succeed.\", pod.Namespace, secretRef.Name, pod.Namespace, pod.Name, err) continue } pullSecrets = append(pullSecrets, *secret) } return pullSecrets } 2.8. containerRuntime.SyncPod 调用container runtime的SyncPod函数，执行相关pod操作，由此kubelet.syncPod的操作逻辑转入containerRuntime.SyncPod函数中。\n// Call the container runtime's SyncPod callback result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { // Do not return error if the only failures were pods in backoff for _, r := range result.SyncResults { if r.Error != kubecontainer.ErrCrashLoopBackOff \u0026\u0026 r.Error != images.ErrImagePullBackOff { // Do not record an event here, as we keep all event logging for sync pod failures // local to container runtime so we get better errors return err } } return nil } 3. Runtime.SyncPod SyncPod主要执行sync操作使得运行的pod达到期望状态的pod。主要执行以下操作：\n计算sandbox和container的变化。 必要的时候杀死pod。 杀死所有不需要运行的container。 必要时创建sandbox。 创建init container。 创建正常的container。 Runtime.SyncPod部分代码位于pkg/kubelet/kuberuntime/kuberuntime_manager.go\n3.1. computePodActions 计算sandbox和container的变化。\n// Step 1: Compute sandbox and container changes. podContainerChanges := m.computePodActions(pod, podStatus) glog.V(3).Infof(\"computePodActions got %+v for pod %q\", podContainerChanges, format.Pod(pod)) if podContainerChanges.CreateSandbox { ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err) } if podContainerChanges.SandboxID != \"\" { m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, \"Pod sandbox changed, it will be killed and re-created.\") } else { glog.V(4).Infof(\"SyncPod received new pod %q, will create a sandbox for it\", format.Pod(pod)) } } 3.2. killPodWithSyncResult 必要的时候杀死pod。\n// Step 2: Kill the pod if the sandbox has changed. if podContainerChanges.KillPod { if !podContainerChanges.CreateSandbox { glog.V(4).Infof(\"Stopping PodSandbox for %q because all other containers are dead.\", format.Pod(pod)) } else { glog.V(4).Infof(\"Stopping PodSandbox for %q, will start new one\", format.Pod(pod)) } killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil { glog.Errorf(\"killPodWithSyncResult failed: %v\", killResult.Error()) return } if podContainerChanges.CreateSandbox { m.purgeInitContainers(pod, podStatus) } } 3.3. killContainer 杀死所有不需要运行的container。\n// Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill { glog.V(3).Infof(\"Killing unwanted container %q(id=%q) for pod %q\", containerInfo.name, containerID, format.Pod(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) glog.Errorf(\"killContainer %q(id=%q) for pod %q failed: %v\", containerInfo.name, containerID, format.Pod(pod), err) return } } 3.4. createPodSandbox 必要时创建sandbox。\n// Step 4: Create a sandbox for the pod if necessary. ... glog.V(4).Infof(\"Creating sandbox for pod %q\", format.Pod(pod)) createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil { createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg) glog.Errorf(\"createPodSandbox for pod %q failed: %v\", format.Pod(pod), err) ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), referr) } m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, \"Failed create pod sandbox: %v\", err) return } glog.V(4).Infof(\"Created PodSandbox %q for pod %q\", podSandboxID, format.Pod(pod)) 3.5. start init container 创建init container。\n// Step 5: start the init container. if container := podContainerChanges.NextInitContainerToStart; container != nil { // Start the next init container. startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) glog.V(4).Infof(\"Backing Off restarting init container %+v in pod %v\", container, format.Pod(pod)) return } glog.V(4).Infof(\"Creating init container %+v in pod %v\", container, format.Pod(pod)) if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil { startContainerResult.Fail(err, msg) utilruntime.HandleError(fmt.Errorf(\"init container start failed: %v: %s\", err, msg)) return } // Successfully started the container; clear the entry in the failure glog.V(4).Infof(\"Completed init container %q for pod %q\", container.Name, format.Pod(pod)) } 3.6. start containers 创建正常的container。\n// Step 6: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart { container := \u0026pod.Spec.Containers[idx] startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) glog.V(4).Infof(\"Backing Off restarting container %+v in pod %v\", container, format.Pod(pod)) continue } glog.V(4).Infof(\"Creating container %+v in pod %v\", container, format.Pod(pod)) // 通过startContainer来运行容器 if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { startContainerResult.Fail(err, msg) // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch { case err == images.ErrImagePullBackOff: glog.V(3).Infof(\"container start failed: %v: %s\", err, msg) default: utilruntime.HandleError(fmt.Errorf(\"container start failed: %v: %s\", err, msg)) } continue } } 4. startContainer startContainer启动一个容器并返回是否成功。\n主要包括以下几个步骤：\n拉取镜像 创建容器 启动容器 运行post start lifecycle hooks(如果有设置此项) startContainer完整代码如下：\nstartContainer部分代码位于pkg/kubelet/kuberuntime/kuberuntime_container.go\n// startContainer starts a container and returns a message indicates why it is failed on error. // It starts the container through the following steps: // * pull the image // * create the container // * start the container // * run the post start lifecycle hooks (if applicable) func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) { // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return msg, err } // Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { glog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } glog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) // For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainerConfig } containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, \"Created container\") if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } // Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, \"Started container\") // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } // Step 4: execute the post start hook. if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { glog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } return \"\", nil } 以下对startContainer分段分析：\n4.1. pull image 通过EnsureImageExists方法拉取拉取指定pod容器的镜像，并返回镜像信息和错误。\n// Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return msg, err } 4.2. CreateContainer 首先生成container的*v1.ObjectReference对象，该对象包括container的相关信息。\n// Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { glog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } glog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) 统计container的重启次数，新的容器默认重启次数为0。\n// For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } 生成container的配置。\ncontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainerConfig } 调用runtimeService，执行CreateContainer的操作。\ncontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, \"Created container\") if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } 4.3. StartContainer 执行runtimeService的StartContainer方法，来启动容器。\n// Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, \"Started container\") // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } 4.4. execute post start hook 如果有指定Lifecycle.PostStart，则执行PostStart操作，PostStart如果执行失败，则容器会根据重启的规则进行重启。\n// Step 4: execute the post start hook. if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { glog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } 5. 总结 kubelet的工作是管理pod在Node上的生命周期（包括增删改查），kubelet通过各种类型的manager异步工作各自执行各自的任务，其中使用到了多种的channel来控制状态信号变化的传递，例如比较重要的channel有podUpdates \u003c-chan UpdatePodOptions，来传递pod的变化情况。\n创建pod的调用逻辑\nsyncLoopIteration--\u003ekubetypes.ADD--\u003eHandlePodAdditions(u.Pods)--\u003edispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)--\u003epodWorkers.UpdatePod--\u003emanagePodLoop(podUpdates)--\u003esyncPod(o syncPodOptions)--\u003econtainerRuntime.SyncPod--\u003estartContainer\n参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/pod_workers.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kuberuntime/kuberuntime_container.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析kubelet中syncPod的部分。\n1. …","ref":"/k8s-source-code-analysis/kubelet/syncpod/","tags":["源码分析"],"title":"kubelet源码分析（五）之 syncPod"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/access/","tags":"","title":"访问控制"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/storage/","tags":"","title":"容器存储"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/text/","tags":"","title":"文本处理"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析优选策略逻辑，即从预选的节点中选择出最优的节点。优选策略的具体实现函数为PrioritizeNodes。PrioritizeNodes最终返回是一个记录了各个节点分数的列表。\n1. 调用入口 genericScheduler.Schedule中对PrioritizeNodes的调用过程如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\nfunc (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ... trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) // 执行优选逻辑的操作，返回记录各个节点分数的列表 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) ... } 核心代码：\n// 基于预选节点filteredNodes进一步筛选优选的节点，返回记录各个节点分数的列表 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 2. PrioritizeNodes 优选，从满足的节点中选择出最优的节点。PrioritizeNodes最终返回是一个记录了各个节点分数的列表。\n具体操作如下：\nPrioritizeNodes通过并行运行各个优先级函数来对节点进行优先级排序。 每个优先级函数会给节点打分，打分范围为0-10分。 0 表示优先级最低的节点，10表示优先级最高的节点。 每个优先级函数也有各自的权重。 优先级函数返回的节点分数乘以权重以获得加权分数。 最后组合（添加）所有分数以获得所有节点的总加权分数。 PrioritizeNodes主要流程如下：\n如果没有设置优选函数和拓展函数，则全部节点设置相同的分数，直接返回。 依次给node执行map函数进行打分。 再对上述map函数的执行结果执行reduce函数计算最终得分。 最后根据不同优先级函数的权重对得分取加权平均数。 入参：\npod nodeNameToInfo meta interface{}, priorityConfigs nodes extenders 出参：\nHostPriorityList：记录节点分数的列表。 HostPriority定义如下:\n// HostPriority represents the priority of scheduling to a particular host, higher priority is better. type HostPriority struct { // Name of the host Host string // Score associated with the host Score int } PrioritizeNodes完整代码如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\n// PrioritizeNodes prioritizes the nodes by running the individual priority functions in parallel. // Each priority function is expected to set a score of 0-10 // 0 is the lowest priority score (least preferred node) and 10 is the highest // Each priority function can also have its own weight // The node scores returned by the priority function are multiplied by the weights to get weighted scores // All scores are finally combined (added) to get the total weighted scores of all nodes func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) { // If no priority configs are provided, then the EqualPriority function is applied // This is required to generate the priority list in the required format if len(priorityConfigs) == 0 \u0026\u0026 len(extenders) == 0 { result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } result = append(result, hostPriority) } return result, nil } var ( mu = sync.Mutex{} wg = sync.WaitGroup{} errs []error ) appendError := func(err error) { mu.Lock() defer mu.Unlock() errs = append(errs, err) } results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) for i, priorityConfig := range priorityConfigs { if priorityConfig.Function != nil { // DEPRECATED wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() var err error results[index], err = config.Function(pod, nodeNameToInfo, nodes) if err != nil { appendError(err) } }(i, priorityConfig) } else { results[i] = make(schedulerapi.HostPriorityList, len(nodes)) } } processNode := func(index int) { nodeInfo := nodeNameToInfo[nodes[index].Name] var err error for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) return } } } workqueue.Parallelize(16, len(nodes), processNode) for i, priorityConfig := range priorityConfigs { if priorityConfig.Reduce == nil { continue } wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() if err := config.Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if glog.V(10) { for _, hostPriority := range results[index] { glog.Infof(\"%v -\u003e %v: %v, Score: (%d)\", pod.Name, hostPriority.Host, config.Name, hostPriority.Score) } } }(i, priorityConfig) } // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } // Summarize all scores. result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) for j := range priorityConfigs { result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } if len(extenders) != 0 \u0026\u0026 nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } // wait for all go routines to finish wg.Wait() for i := range result { result[i].Score += combinedScores[result[i].Host] } } if glog.V(10) { for i := range result { glog.V(10).Infof(\"Host %s =\u003e Score %d\", result[i].Host, result[i].Score) } } return result, nil } 以下对PrioritizeNodes分段进行分析。\n3. EqualPriorityMap 如果没有提供优选函数和拓展函数，则将所有的节点设置为相同的优先级，即节点的score都为1，然后直接返回结果。(但一般情况下优选函数列表都不为空)\n// If no priority configs are provided, then the EqualPriority function is applied // This is required to generate the priority list in the required format if len(priorityConfigs) == 0 \u0026\u0026 len(extenders) == 0 { result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } result = append(result, hostPriority) } return result, nil } EqualPriorityMap具体实现如下：\n// EqualPriorityMap is a prioritizer function that gives an equal weight of one to all nodes func EqualPriorityMap(_ *v1.Pod, _ interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } return schedulerapi.HostPriority{ Host: node.Name, Score: 1, }, nil } 4. processNode processNode就是基于index拿出node的信息，调用之前注册的各种优选函数（此处是mapFunction），通过优选函数对node和pod进行处理，最后返回一个记录node分数的列表result。processNode同样也使用workqueue.Parallelize来进行并行处理。(processNode类似于预选逻辑findNodesThatFit中使用到的checkNode的作用)\n其中优选函数是通过priorityConfigs来记录，每类优选函数包括PriorityMapFunction和PriorityReduceFunction两种函数。优选函数的注册部分可参考registerAlgorithmProvider。\nprocessNode := func(index int) { nodeInfo := nodeNameToInfo[nodes[index].Name] var err error for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) return } } } // 并行执行processNode workqueue.Parallelize(16, len(nodes), processNode) priorityConfigs定义如下：\n核心属性：\nMap ：PriorityMapFunction Reduce：PriorityReduceFunction // PriorityConfig is a config used for a priority function. type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int } 具体的优选函数处理逻辑待下文分析，本文会以NewSelectorSpreadPriority函数为例。\n5. PriorityMapFunction PriorityMapFunction是一个计算给定节点的每个节点结果的函数。\nPriorityMapFunction定义如下：\n// PriorityMapFunction is a function that computes per-node results for a given node. // TODO: Figure out the exact API of this method. // TODO: Change interface{} to a specific type. type PriorityMapFunction func(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) PriorityMapFunction是在processNode中调用的，代码如下：\nresults[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) 下文会分析NewSelectorSpreadPriority在的map函数CalculateSpreadPriorityMap。\n6. PriorityReduceFunction PriorityReduceFunction是一个聚合每个节点结果并计算所有节点的最终得分的函数。\nPriorityReduceFunction定义如下：\n// PriorityReduceFunction is a function that aggregated per-node results and computes // final scores for all nodes. // TODO: Figure out the exact API of this method. // TODO: Change interface{} to a specific type. type PriorityReduceFunction func(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error PrioritizeNodes中对reduce函数调用部分如下：\nfor i, priorityConfig := range priorityConfigs { if priorityConfig.Reduce == nil { continue } wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() if err := config.Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if glog.V(10) { for _, hostPriority := range results[index] { glog.Infof(\"%v -\u003e %v: %v, Score: (%d)\", pod.Name, hostPriority.Host, config.Name, hostPriority.Score) } } }(i, priorityConfig) } 下文会分析NewSelectorSpreadPriority在的reduce函数CalculateSpreadPriorityReduce。\n7. Summarize all scores 先等待计算完成再计算加权平均数。\n// Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } 计算所有节点的加权平均数。\n// Summarize all scores. result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) for j := range priorityConfigs { result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } 当设置了拓展的计算方式，则增加拓展计算方式的加权平均数。\nif len(extenders) != 0 \u0026\u0026 nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } // wait for all go routines to finish wg.Wait() for i := range result { result[i].Score += combinedScores[result[i].Host] } } 8. NewSelectorSpreadPriority 以下以NewSelectorSpreadPriority这个优选函数来做分析，其他重要的优选函数待后续专门分析。\nNewSelectorSpreadPriority主要的功能是将属于相同service和rs下的pod尽量分布在不同的node上。\n该函数的注册代码如下：\n此部分代码位于pkg/scheduler/algorithmprovider/defaults/defaults.go\n// ServiceSpreadingPriority is a priority config factory that spreads pods by minimizing // the number of pods (belonging to the same service) on the same node. // Register the factory so that it's available, but do not include it as part of the default priorities // Largely replaced by \"SelectorSpreadPriority\", but registered for backward compatibility with 1.0 factory.RegisterPriorityConfigFactory( \"ServiceSpreadingPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, algorithm.EmptyControllerLister{}, algorithm.EmptyReplicaSetLister{}, algorithm.EmptyStatefulSetLister{}) }, Weight: 1, }, ) NewSelectorSpreadPriority的具体实现如下：\n此部分代码位于pkg/scheduler/algorithm/priorities/selector_spreading.go\n// NewSelectorSpreadPriority creates a SelectorSpread. func NewSelectorSpreadPriority( serviceLister algorithm.ServiceLister, controllerLister algorithm.ControllerLister, replicaSetLister algorithm.ReplicaSetLister, statefulSetLister algorithm.StatefulSetLister) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { selectorSpread := \u0026SelectorSpread{ serviceLister: serviceLister, controllerLister: controllerLister, replicaSetLister: replicaSetLister, statefulSetLister: statefulSetLister, } return selectorSpread.CalculateSpreadPriorityMap, selectorSpread.CalculateSpreadPriorityReduce } NewSelectorSpreadPriority主要包括map和reduce两种函数，分别对应CalculateSpreadPriorityMap，CalculateSpreadPriorityReduce。\n8.1. CalculateSpreadPriorityMap CalculateSpreadPriorityMap的主要作用是将相同service、RC、RS或statefulset的pod分布在不同的节点上。当调度一个pod的时候，先寻找与该pod匹配的service、RS、RC或statefulset，然后寻找与其selector匹配的已存在的pod，寻找存在这类pod最少的节点。\n基本流程如下：\n寻找与该pod对应的service、RS、RC、statefulset匹配的selector。 遍历当前节点的所有pod，将该节点上已存在的selector匹配到的pod的个数作为该节点的分数（此时，分数大的表示匹配到的pod越多，越不符合被调度的条件，该分数在reduce阶段会被按10分制处理成分数大的越符合被调度的条件）。 此部分代码位于pkg/scheduler/algorithm/priorities/selector_spreading.go\n// CalculateSpreadPriorityMap spreads pods across hosts, considering pods // belonging to the same service,RC,RS or StatefulSet. // When a pod is scheduled, it looks for services, RCs,RSs and StatefulSets that match the pod, // then finds existing pods that match those selectors. // It favors nodes that have fewer existing matching pods. // i.e. it pushes the scheduler towards a node where there's the smallest number of // pods which match the same service, RC,RSs or StatefulSets selectors as the pod being scheduled. func (s *SelectorSpread) CalculateSpreadPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { var selectors []labels.Selector node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } priorityMeta, ok := meta.(*priorityMetadata) if ok { selectors = priorityMeta.podSelectors } else { selectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) } if len(selectors) == 0 { return schedulerapi.HostPriority{ Host: node.Name, Score: int(0), }, nil } count := int(0) for _, nodePod := range nodeInfo.Pods() { if pod.Namespace != nodePod.Namespace { continue } // When we are replacing a failed pod, we often see the previous // deleted version while scheduling the replacement. // Ignore the previous deleted version for spreading purposes // (it can still be considered for resource restrictions etc.) if nodePod.DeletionTimestamp != nil { glog.V(4).Infof(\"skipping pending-deleted pod: %s/%s\", nodePod.Namespace, nodePod.Name) continue } for _, selector := range selectors { if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) { count++ break } } } return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } 以下分段分析：\n先获得selector。\nselectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) 计算节点上匹配selector的pod的个数，作为该节点分数，该分数并不是最终节点的分数，只是中间过渡的记录状态。\ncount := int(0) for _, nodePod := range nodeInfo.Pods() { ... for _, selector := range selectors { if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) { count++ break } } } 8.2. CalculateSpreadPriorityReduce CalculateSpreadPriorityReduce根据节点上现有匹配pod的数量计算每个节点的十分制的分数，具有较少现有匹配pod的节点的分数越高，表示节点越可能被调度到。\n基本流程如下：\n记录所有节点中匹配到pod个数最多的节点的分数（即匹配到的pod最多的个数）。 遍历所有的节点，按比例取十分制的得分，计算方式为：(节点中最多匹配pod的个数-当前节点pod的个数)/节点中最多匹配pod的个数。此时，分数越高表示该节点上匹配到的pod的个数越少，越可能被调度到，即满足把相同selector的pod分散到不同节点的需求。 此部分代码位于pkg/scheduler/algorithm/priorities/selector_spreading.go\n// CalculateSpreadPriorityReduce calculates the source of each node // based on the number of existing matching pods on the node // where zone information is included on the nodes, it favors nodes // in zones with fewer existing matching pods. func (s *SelectorSpread) CalculateSpreadPriorityReduce(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error { countsByZone := make(map[string]int, 10) maxCountByZone := int(0) maxCountByNodeName := int(0) for i := range result { if result[i].Score \u003e maxCountByNodeName { maxCountByNodeName = result[i].Score } zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == \"\" { continue } countsByZone[zoneID] += result[i].Score } for zoneID := range countsByZone { if countsByZone[zoneID] \u003e maxCountByZone { maxCountByZone = countsByZone[zoneID] } } haveZones := len(countsByZone) != 0 maxCountByNodeNameFloat64 := float64(maxCountByNodeName) maxCountByZoneFloat64 := float64(maxCountByZone) MaxPriorityFloat64 := float64(schedulerapi.MaxPriority) for i := range result { // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName \u003e 0 { fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) } // If there is zone information present, incorporate it if haveZones { zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID != \"\" { zoneScore := MaxPriorityFloat64 if maxCountByZone \u003e 0 { zoneScore = MaxPriorityFloat64 * (float64(maxCountByZone-countsByZone[zoneID]) / maxCountByZoneFloat64) } fScore = (fScore * (1.0 - zoneWeighting)) + (zoneWeighting * zoneScore) } } result[i].Score = int(fScore) if glog.V(10) { glog.Infof( \"%v -\u003e %v: SelectorSpreadPriority, Score: (%d)\", pod.Name, result[i].Host, int(fScore), ) } } return nil } 以下分段分析：\n先获取所有节点中匹配到的pod最多的个数。\nfor i := range result { if result[i].Score \u003e maxCountByNodeName { maxCountByNodeName = result[i].Score } zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == \"\" { continue } countsByZone[zoneID] += result[i].Score } 遍历所有的节点，按比例取十分制的得分。\nfor i := range result { // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName \u003e 0 { fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) } ... } 9. 总结 优选，从满足的节点中选择出最优的节点。PrioritizeNodes最终返回是一个记录了各个节点分数的列表。\n9.1. PrioritizeNodes 主要流程如下：\n如果没有设置优选函数和拓展函数，则全部节点设置相同的分数，直接返回。 依次给node执行map函数进行打分。 再对上述map函数的执行结果执行reduce函数计算最终得分。 最后根据不同优先级函数的权重对得分取加权平均数。 其中每类优选函数会包含map函数和reduce函数两种。\n9.2. NewSelectorSpreadPriority 其中以NewSelectorSpreadPriority这个优选函数为例作分析，该函数的功能是将相同service、RS、RC或statefulset下pod尽量分散到不同的节点上。包括map函数和reduce函数两部分，具体如下。\n9.2.1. CalculateSpreadPriorityMap 基本流程如下：\n寻找与该pod对应的service、RS、RC、statefulset匹配的selector。 遍历当前节点的所有pod，将该节点上已存在的selector匹配到的pod的个数作为该节点的分数（此时，分数大的表示匹配到的pod越多，越不符合被调度的条件，该分数在reduce阶段会被按10分制处理成分数大的越符合被调度的条件）。 9.2.2. CalculateSpreadPriorityReduce 基本流程如下：\n记录所有节点中匹配到pod个数最多的节点的分数（即匹配到的pod最多的个数）。 遍历所有的节点，按比例取十分制的得分，计算方式为：(节点中最多匹配pod的个数-当前节点pod的个数)/节点中最多匹配pod的个数。此时，分数越高表示该节点上匹配到的pod的个数越少，越可能被调度到，即满足把相同selector的pod分散到不同节点的需求。 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithm/priorities/selector_spreading.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析优选策略逻辑，即从预选的节点中选择出最优的节点。优选策略的具体 …","ref":"/k8s-source-code-analysis/kube-scheduler/prioritizenodes/","tags":["源码分析"],"title":"kube-scheduler源码分析（五）之 优选策略"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/tools/","tags":"","title":"运维工具"},{"body":" 本文主要介绍etcd-operator的部署及使用\n1. 部署RBAC 下载create_role.sh、cluster-role-binding-template.yaml、cluster-role-template.yaml\n例如：\n|-- cluster-role-binding-template.yaml |-- cluster-role-template.yaml |-- create_role.sh # 部署rbac kubectl create ns operator bash create_role.sh --namespace=operator # namespace与etcd-operator的ns一致 示例：\nbash create_role.sh --namespace=operator + ROLE_NAME=etcd-operator + ROLE_BINDING_NAME=etcd-operator + NAMESPACE=default + for i in '\"$@\"' + case $i in + NAMESPACE=operator + echo 'Creating role with ROLE_NAME=etcd-operator, NAMESPACE=operator' Creating role with ROLE_NAME=etcd-operator, NAMESPACE=operator + sed -e 's/\u003cROLE_NAME\u003e/etcd-operator/g' -e 's/\u003cNAMESPACE\u003e/operator/g' cluster-role-template.yaml + kubectl create -f - clusterrole.rbac.authorization.k8s.io/etcd-operator created + echo 'Creating role binding with ROLE_NAME=etcd-operator, ROLE_BINDING_NAME=etcd-operator, NAMESPACE=operator' Creating role binding with ROLE_NAME=etcd-operator, ROLE_BINDING_NAME=etcd-operator, NAMESPACE=operator + sed -e 's/\u003cROLE_NAME\u003e/etcd-operator/g' -e 's/\u003cROLE_BINDING_NAME\u003e/etcd-operator/g' -e 's/\u003cNAMESPACE\u003e/operator/g' cluster-role-binding-template.yaml + kubectl create -f - clusterrolebinding.rbac.authorization.k8s.io/etcd-operator created 1.1. create_role.sh 脚本 create_role.sh有三个入参，可以指定--namespace参数，该参数与etcd-operator部署的namespace应一致。默认为default。\n#!/usr/bin/env bash set -o errexit set -o nounset set -o pipefail ETCD_OPERATOR_ROOT=$(dirname \"${BASH_SOURCE}\")/../.. print_usage() { echo \"$(basename \"$0\") - Create Kubernetes RBAC role and role bindings for etcd-operator Usage: $(basename \"$0\") [options...] Options: --role-name=STRING Name of ClusterRole to create (default=\\\"etcd-operator\\\", environment variable: ROLE_NAME) --role-binding-name=STRING Name of ClusterRoleBinding to create (default=\\\"etcd-operator\\\", environment variable: ROLE_BINDING_NAME) --namespace=STRING namespace to create role and role binding in. Must already exist. (default=\\\"default\\\", environment variable: NAMESPACE) \" \u003e\u00262 } ROLE_NAME=\"${ROLE_NAME:-etcd-operator}\" ROLE_BINDING_NAME=\"${ROLE_BINDING_NAME:-etcd-operator}\" NAMESPACE=\"${NAMESPACE:-default}\" for i in \"$@\" do case $i in --role-name=*) ROLE_NAME=\"${i#*=}\" ;; --role-binding-name=*) ROLE_BINDING_NAME=\"${i#*=}\" ;; --namespace=*) NAMESPACE=\"${i#*=}\" ;; -h|--help) print_usage exit 0 ;; *) print_usage exit 1 ;; esac done echo \"Creating role with ROLE_NAME=${ROLE_NAME}, NAMESPACE=${NAMESPACE}\" sed -e \"s/\u003cROLE_NAME\u003e/${ROLE_NAME}/g\" \\ -e \"s/\u003cNAMESPACE\u003e/${NAMESPACE}/g\" \\ \"cluster-role-template.yaml\" | \\ kubectl create -f - echo \"Creating role binding with ROLE_NAME=${ROLE_NAME}, ROLE_BINDING_NAME=${ROLE_BINDING_NAME}, NAMESPACE=${NAMESPACE}\" sed -e \"s/\u003cROLE_NAME\u003e/${ROLE_NAME}/g\" \\ -e \"s/\u003cROLE_BINDING_NAME\u003e/${ROLE_BINDING_NAME}/g\" \\ -e \"s/\u003cNAMESPACE\u003e/${NAMESPACE}/g\" \\ \"cluster-role-binding-template.yaml\" | \\ kubectl create -f - 1.2. cluster-role-binding-template.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: \u003cROLE_BINDING_NAME\u003e roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: \u003cROLE_NAME\u003e subjects: - kind: ServiceAccount name: default namespace: \u003cNAMESPACE\u003e 1.3. cluster-role-template.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: \u003cROLE_NAME\u003e rules: - apiGroups: - etcd.database.coreos.com resources: - etcdclusters - etcdbackups - etcdrestores verbs: - \"*\" - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - \"*\" - apiGroups: - \"\" resources: - pods - services - endpoints - persistentvolumeclaims - events verbs: - \"*\" - apiGroups: - apps resources: - deployments verbs: - \"*\" # The following permissions can be removed if not using S3 backup and TLS - apiGroups: - \"\" resources: - secrets verbs: - get 2. 部署etcd-operator kubectl create -f etcd-operator.yaml etcd-operator.yaml如下：\napiVersion: apps/v1 kind: Deployment metadata: name: etcd-operator namespace: operator # 与rbac指定的ns一致 labels: app: etcd-operator spec: replicas: 1 selector: matchLabels: app: etcd-operator template: metadata: labels: app: etcd-operator spec: containers: - name: etcd-operator image: registry.cn-shenzhen.aliyuncs.com/huweihuang/etcd-operator:v0.9.4 command: - etcd-operator # Uncomment to act for resources in all namespaces. More information in doc/user/clusterwide.md - -cluster-wide env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 查看CRD\n#kubectl get customresourcedefinitions NAME CREATED AT etcdclusters.etcd.database.coreos.com 2020-08-01T13:02:18Z 查看etcd-operator的日志是否OK。\nk logs -f etcd-operator-545df8d445-qpf6n -n operator time=\"2020-08-01T13:02:18Z\" level=info msg=\"etcd-operator Version: 0.9.4\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Git SHA: c8a1c64\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Go Version: go1.11.5\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Go OS/Arch: linux/amd64\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Event(v1.ObjectReference{Kind:\\\"Endpoints\\\", Namespace:\\\"operator\\\", Name:\\\"etcd-operator\\\", UID:\\\"7de38cff-1b7b-4bf2-9837-473fa66c9366\\\", APIVersion:\\\"v1\\\", ResourceVersion:\\\"41195930\\\", FieldPath:\\\"\\\"}): type: 'Normal' reason: 'LeaderElection' etcd-operator-545df8d445-qpf6n became leader\" 以上内容表示etcd-operator运行正常。\n3. 部署etcd集群 kubectl create -f etcd-cluster.yaml 当开启clusterwide则etcd集群与etcd-operator的ns可不同。\netcd-cluster.yaml\napiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"default-etcd-cluster\" ## Adding this annotation make this cluster managed by clusterwide operators ## namespaced operators ignore it annotations: etcd.database.coreos.com/scope: clusterwide namespace: etcd # 此处的ns表示etcd集群部署在哪个ns下 spec: size: 3 version: \"v3.3.18\" repository: registry.cn-shenzhen.aliyuncs.com/huweihuang/etcd pod: busyboxImage: registry.cn-shenzhen.aliyuncs.com/huweihuang/busybox:1.28.0-glibc 查看集群部署结果\n$ kgpo -n etcd NAME READY STATUS RESTARTS AGE default-etcd-cluster-b6phnpf8z8 1/1 Running 0 3m3s default-etcd-cluster-hhgq4sbtgr 1/1 Running 0 109s default-etcd-cluster-ttfh5fj92b 1/1 Running 0 2m29s 4. 访问etcd集群 查看service\n$ kgsvc -n etcd NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default-etcd-cluster ClusterIP None \u003cnone\u003e 2379/TCP,2380/TCP 5m37s default-etcd-cluster-client ClusterIP 192.168.255.244 \u003cnone\u003e 2379/TCP 5m37s 使用service地址访问\n# 查看集群健康状态 $ ETCDCTL_API=3 etcdctl --endpoints 192.168.255.244:2379 endpoint health 192.168.255.244:2379 is healthy: successfully committed proposal: took = 1.96126ms # 写数据 $ ETCDCTL_API=3 etcdctl --endpoints 192.168.255.244:2379 put foo bar OK # 读数据 $ ETCDCTL_API=3 etcdctl --endpoints 192.168.255.244:2379 get foo foo bar 5. 销毁etcd-operator kubectl delete -f example/deployment.yaml kubectl delete endpoints etcd-operator kubectl delete crd etcdclusters.etcd.database.coreos.com kubectl delete clusterrole etcd-operator kubectl delete clusterrolebinding etcd-operator 参考：\nhttps://github.com/coreos/etcd-operator https://github.com/coreos/etcd-operator/blob/master/doc/user/install_guide.md https://github.com/coreos/etcd-operator/blob/master/doc/user/client_service.md https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md https://github.com/coreos/etcd-operator/blob/master/doc/user/cluster_tls.md ","categories":"","description":"","excerpt":" 本文主要介绍etcd-operator的部署及使用\n1. 部署RBAC 下 …","ref":"/kubernetes-notes/etcd/etcd-operator-usage/","tags":["Etcd"],"title":"etcd-operator的使用"},{"body":"1. Pod伸缩 k8s中RC的用来保持集群中始终运行指定数目的实例，通过RC的scale机制可以完成Pod的扩容和缩容（伸缩）。\n1.1. 手动伸缩（scale） kubectl scale rc redis-slave --replicas=3 1.2. 自动伸缩（HPA） Horizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。HPA控制器基于Master的kube-controller-manager服务启动参数--horizontal-pod-autoscaler-sync-period定义是时长（默认30秒），周期性监控目标Pod的CPU使用率，并在满足条件时对ReplicationController或Deployment中的Pod副本数进行调整，以符合用户定义的平均Pod CPU使用率。Pod CPU使用率来源于heapster组件，因此需安装该组件。\n可以通过kubectl autoscale命令进行快速创建或者使用yaml配置文件进行创建。创建之前需已存在一个RC或Deployment对象，并且该RC或Deployment中的Pod必须定义resources.requests.cpu的资源请求值，以便heapster采集到该Pod的CPU。\n1.2.1. 通过kubectl autoscale创建 例如：\nphp-apache-rc.yaml\napiVersion: v1 kind: ReplicationController metadata: name: php-apache spec: replicas: 1 template: metadata: name: php-apache labels: app: php-apache spec: containers: - name: php-apache image: gcr.io/google_containers/hpa-example resources: requests: cpu: 200m ports: - containerPort: 80 创建php-apache的RC\nkubectl create -f php-apache-rc.yaml php-apache-svc.yaml\napiVersion: v1 kind: Service metadata: name: php-apache spec: ports: - port: 80 selector: app: php-apache 创建php-apache的Service\nkubectl create -f php-apache-svc.yaml 创建HPA控制器\nkubectl autoscale rc php-apache --min=1 --max=10 --cpu-percent=50 1.2.2. 通过yaml配置文件创建 hpa-php-apache.yaml\napiVersion: v1 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: v1 kind: ReplicationController name: php-apache minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 创建hpa\nkubectl create -f hpa-php-apache.yaml 查看hpa\nkubectl get hpa 2. Pod滚动升级 k8s中的滚动升级通过执行kubectl rolling-update命令完成，该命令创建一个新的RC（与旧的RC在同一个命名空间中），然后自动控制旧的RC中的Pod副本数逐渐减少为0，同时新的RC中的Pod副本数从0逐渐增加到附加值，但滚动升级中Pod副本数（包括新Pod和旧Pod）保持原预期值。\n2.1. 通过配置文件实现 redis-master-controller-v2.yaml\napiVersion: v1 kind: ReplicationController metadata: name: redis-master-v2 labels: name: redis-master version: v2 spec: replicas: 1 selector: name: redis-master version: v2 template: metadata: labels: name: redis-master version: v2 spec: containers: - name: master image: kubeguide/redis-master:2.0 ports: - containerPort: 6371 注意事项：\nRC的名字（name）不能与旧RC的名字相同 在selector中应至少有一个Label与旧的RC的Label不同，以标识其为新的RC。例如本例中新增了version的Label。 运行kubectl rolling-update\nkubectl rolling-update redis-master -f redis-master-controller-v2.yaml 2.2. 通过kubectl rolling-update命令实现 kubectl rolling-update redis-master --image=redis-master:2.0 与使用配置文件实现不同在于，该执行结果旧的RC被删除，新的RC仍使用旧的RC的名字。\n2.3. 升级回滚 kubectl rolling-update加参数--rollback实现回滚操作\nkubectl rolling-update redis-master --image=kubeguide/redis-master:2.0 --rollback 参考文章\n《Kubernetes权威指南》 ","categories":"","description":"","excerpt":"1. Pod伸缩 k8s中RC的用来保持集群中始终运行指定数目的实例，通过RC的scale机制可以完成Pod的扩容和缩容（伸缩）。\n1.1. …","ref":"/kubernetes-notes/concepts/pod/pod-operation/","tags":["Kubernetes"],"title":"Pod伸缩与升级"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/git/","tags":"","title":"GIT命令"},{"body":"runc代码目录结构 ├── create.go # createCommand ├── delete.go # deleteCommand ├── events.go # eventsCommand ├── exec.go # execCommand ├── features.go # featuresCommand ├── kill.go # killCommand ├── libcontainer # 核心实现逻辑 ├── list.go # listCommand ├── main.go # main函数 ├── pause.go # pauseCommand ├── ps.go # psCommand ├── restore.go # restoreCommand ├── run.go # runCommand ├── spec.go # specCommand ├── start.go # startCommand ├── state.go # stateCommand ├── update.go # updateCommand ├── utils_linux.go # startContainer libcontainer目录结构 libcontainer ├── apparmor ├── capabilities ├── cgroups ├── configs ├── console_linux.go ├── container.go ├── container_linux.go ├── container_linux_test.go ├── criu_opts_linux.go ├── devices ├── error.go ├── factory_linux.go ├── factory_linux_test.go ├── init_linux.go ├── integration ├── intelrdt ├── keys ├── logs ├── message_linux.go ├── mount_linux.go ├── network_linux.go ├── notify_linux.go ├── notify_linux_test.go ├── notify_v2_linux.go ├── nsenter ├── process.go ├── process_linux.go ├── restored_process.go ├── rootfs_linux.go ├── rootfs_linux_test.go ├── seccomp ├── setns_init_linux.go ├── specconv ├── standard_init_linux.go ├── state_linux.go ├── state_linux_test.go ├── stats_linux.go ├── sync.go ├── system ├── user ├── userns └── utils Main函数 runc的代码仓库主要使用了github.com/urfave/cli的命令框架（该框架与cobra命令框架类似）。添加了多个重要的子命令。\nfunc main() { app := cli.NewApp() app.Name = \"runc\" app.Usage = usage app.Commands = []cli.Command{ checkpointCommand, createCommand, deleteCommand, eventsCommand, execCommand, killCommand, listCommand, pauseCommand, psCommand, restoreCommand, resumeCommand, runCommand, specCommand, startCommand, stateCommand, updateCommand, featuresCommand, } runCommand 以runCommand为例分析子命令的调用流程。\ngithub.com/urfave/cli命令框架代码格式：\n创建一个Command结构体，包含：\nName：命名名称\nUsage：使用说明\nDescription：描述命令信息\nFlags：解析参数\nAction: command run的核心逻辑。\n// default action is to start a container var runCommand = cli.Command{ Name: \"run\", Usage: \"create and run a container\", // 删除描述信息 ArgsUsage: ``, Description: ``, Flags: []cli.Flag{ cli.StringFlag{ Name: \"bundle, b\", Value: \"\", Usage: `path to the root of the bundle directory, defaults to the current directory`, }, // 删除多余的FLAG代码 }, Action: func(context *cli.Context) error { if err := checkArgs(context, 1, exactArgs); err != nil { return err } // 核心代码，启动容器 status, err := startContainer(context, CT_ACT_RUN, nil) if err == nil { // exit with the container's exit status so any external supervisor is // notified of the exit with the correct exit status. os.Exit(status) } return fmt.Errorf(\"runc run failed: %w\", err) }, } startContainer 启动容器的流程：\nsetup spec信息。\n基于spec信息创建container。\n通过runner启动进程。\n删除error处理代码\nfunc startContainer(context *cli.Context, action CtAct, criuOpts *libcontainer.CriuOpts) (int, error) { if err := revisePidFile(context); err != nil { return -1, err } spec, err := setupSpec(context) id := context.Args().First() notifySocket := newNotifySocket(context, os.Getenv(\"NOTIFY_SOCKET\"), id) if notifySocket != nil { notifySocket.setupSpec(spec) } container, err := createContainer(context, id, spec) if notifySocket != nil { if err := notifySocket.setupSocketDirectory(); err != nil { return -1, err } if action == CT_ACT_RUN { if err := notifySocket.bindSocket(); err != nil { return -1, err } } } // Support on-demand socket activation by passing file descriptors into the container init process. listenFDs := []*os.File{} if os.Getenv(\"LISTEN_FDS\") != \"\" { listenFDs = activation.Files(false) } r := \u0026runner{ enableSubreaper: !context.Bool(\"no-subreaper\"), shouldDestroy: !context.Bool(\"keep\"), container: container, listenFDs: listenFDs, notifySocket: notifySocket, consoleSocket: context.String(\"console-socket\"), detach: context.Bool(\"detach\"), pidFile: context.String(\"pid-file\"), preserveFDs: context.Int(\"preserve-fds\"), action: action, criuOpts: criuOpts, init: true, } return r.run(spec.Process) } 待完善\n","categories":"","description":"","excerpt":"runc代码目录结构 ├── create.go # createCommand ├── delete.go # deleteCommand …","ref":"/k8s-source-code-analysis/kubelet/runc-code-analysis/","tags":["源码分析"],"title":"runc源码分析"},{"body":"1. for for循环一般格式为：\nfor 变量 in 列表 do command1 command2 ... commandN done 列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。每循环一次，就将列表中的下一个值赋给变量。 in 列表是可选的，如果不用它，for 循环使用命令行的位置参数。\n示例：\nfor loop in 1 2 3 4 5 do echo \"The value is: $loop\" done # 运行结果： The value is: 1 The value is: 2 The value is: 3 The value is: 4 The value is: 5 2. while while循环用于不断执行一系列命令，也用于从输入文件中读取数据\nwhile command do Statement(s) to be executed if command is true done 命令执行完毕，控制返回循环顶部，从头开始直至测试条件为假。\n示例：\nCOUNTER=0 while [ $COUNTER -lt 5 ] do COUNTER='expr $COUNTER+1' echo $COUNTER done while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按\u003cCtrl-D\u003e结束循环。\necho 'type \u003cCTRL-D\u003e to terminate' echo -n 'enter your most liked film: ' while read FILM do echo \"Yeah! great film the $FILM\" done 3. until until 循环执行一系列命令直至条件为 true 时停止。until 循环与 while 循环在处理方式上刚好相反。一般while循环优于until循环，但在某些时候，也只是极少数情况下，until 循环更加有用。 until 循环格式为：\nuntil command do Statement(s) to be executed until command is true done command 一般为条件表达式，如果返回值为 false，则继续执行循环体内的语句，否则跳出循环。\n示例：\n#!/bin/bash a=0 until [ ! $a -lt 10 ] do echo $a a=`expr $a + 1` done 4. break命令 break命令允许跳出所有（终止执行后面的所有循环）。\n在嵌套循环中，break 命令后面还可以跟一个整数，表示跳出第几层循环\nbreak n 表示跳出第 n 层循环。\n示例：\n#!/bin/bash while : do echo -n \"Input a number between 1 to 5: \" read aNum case $aNum in 1|2|3|4|5) echo \"Your number is $aNum!\" ;; *) echo \"You do not select a number between 1 to 5, game is over!\" break ;; esac done 5. continue命令 continue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出循环\n同样，continue 后面也可以跟一个数字，表示跳出第几层循环。\n示例：\n#!/bin/bash while : do echo -n \"Input a number between 1 to 5: \" read aNum case $aNum in 1|2|3|4|5) echo \"Your number is $aNum!\" ;; *) echo \"You do not select a number between 1 to 5!\" continue echo \"Game is over!\" ;; esac done 参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. for for循环一般格式为：\nfor 变量 in 列表 do command1 command2 ... commandN done …","ref":"/linux-notes/shell/shell-loop/","tags":["Shell"],"title":"Shell 循环语句"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/test/","tags":"","title":"单元测试"},{"body":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析调度中的抢占逻辑，当pod不适合任何节点的时候，可能pod会调度失败，这时候可能会发生抢占。抢占逻辑的具体实现函数为Scheduler.preempt。\n1. 调用入口 当pod不适合任何节点的时候，可能pod会调度失败。这时候可能会发生抢占。\nscheduleOne函数中关于抢占调用的逻辑如下：\n此部分的代码位于/pkg/scheduler/scheduler.go\n// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. func (sched *Scheduler) scheduleOne() { ... suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() // 执行抢占逻辑 sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } ... } 其中核心代码为：\n// 基于sched.schedule(pod)返回的err和当前待调度的pod执行抢占策略 sched.preempt(pod, fitError) 2. Scheduler.preempt 当pod调度失败的时候，会抢占低优先级pod的空间来给高优先级的pod。其中入参为调度失败的pod对象和调度失败的err。\n抢占的基本流程如下：\n判断是否有关闭抢占机制，如果关闭抢占机制则直接返回。 获取调度失败pod的最新对象数据。 执行抢占算法Algorithm.Preempt，返回预调度节点和需要被剔除的pod列表。 将抢占算法返回的node添加到pod的Status.NominatedNodeName中，并删除需要被剔除的pod。 当抢占算法返回的node是nil的时候，清除pod的Status.NominatedNodeName信息。 整个抢占流程的最终结果实际上是更新Pod.Status.NominatedNodeName属性的信息。如果抢占算法返回的节点不为空，则将该node更新到Pod.Status.NominatedNodeName中，否则就将Pod.Status.NominatedNodeName设置为空。\n2.1. preempt preempt的具体实现函数：\n此部分的代码位于/pkg/scheduler/scheduler.go\n// preempt tries to create room for a pod that has failed to schedule, by preempting lower priority pods if possible. // If it succeeds, it adds the name of the node where preemption has happened to the pod annotations. // It returns the node name and an error if any. func (sched *Scheduler) preempt(preemptor *v1.Pod, scheduleErr error) (string, error) { if !util.PodPriorityEnabled() || sched.config.DisablePreemption { glog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" + \" No preemption is performed.\") return \"\", nil } preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil { glog.Errorf(\"Error getting the updated preemptor pod object: %v\", err) return \"\", err } node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) metrics.PreemptionVictims.Set(float64(len(victims))) if err != nil { glog.Errorf(\"Error preempting victims to make room for %v/%v.\", preemptor.Namespace, preemptor.Name) return \"\", err } var nodeName = \"\" if node != nil { nodeName = node.Name err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil { glog.Errorf(\"Error in preemption process. Cannot update pod %v/%v annotations: %v\", preemptor.Namespace, preemptor.Name, err) return \"\", err } for _, victim := range victims { if err := sched.config.PodPreemptor.DeletePod(victim); err != nil { glog.Errorf(\"Error preempting pod %v/%v: %v\", victim.Namespace, victim.Name, err) return \"\", err } sched.config.Recorder.Eventf(victim, v1.EventTypeNormal, \"Preempted\", \"by %v/%v on node %v\", preemptor.Namespace, preemptor.Name, nodeName) } } // Clearing nominated pods should happen outside of \"if node != nil\". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { glog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } return nodeName, err } 以下对preempt的实现分段分析。\n如果设置关闭抢占机制，则直接返回。\nif !util.PodPriorityEnabled() || sched.config.DisablePreemption { glog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" + \" No preemption is performed.\") return \"\", nil } 获取当前pod的最新状态。\npreemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil { glog.Errorf(\"Error getting the updated preemptor pod object: %v\", err) return \"\", err } GetUpdatedPod的实现就是去拿pod的对象。\nfunc (p *podPreemptor) GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) { return p.Client.CoreV1().Pods(pod.Namespace).Get(pod.Name, metav1.GetOptions{}) } 接着执行抢占的算法。抢占的算法返回预调度节点的信息和因抢占被剔除的pod的信息。具体的抢占算法逻辑下文分析。\nnode, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) 将预调度节点的信息更新到pod的Status.NominatedNodeName属性中。\nerr = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) SetNominatedNodeName的具体实现为：\nfunc (p *podPreemptor) SetNominatedNodeName(pod *v1.Pod, nominatedNodeName string) error { podCopy := pod.DeepCopy() podCopy.Status.NominatedNodeName = nominatedNodeName _, err := p.Client.CoreV1().Pods(pod.Namespace).UpdateStatus(podCopy) return err } 接着删除因抢占而需要被剔除的pod。\nerr := sched.config.PodPreemptor.DeletePod(victim) PodPreemptor.DeletePod的具体实现就是删除具体的pod。\nfunc (p *podPreemptor) DeletePod(pod *v1.Pod) error { return p.Client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, \u0026metav1.DeleteOptions{}) } 如果抢占算法得出的node对象为nil，则将pod的Status.NominatedNodeName属性设置为空。\n// Clearing nominated pods should happen outside of \"if node != nil\". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { glog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } RemoveNominatedNodeName的具体实现如下：\nfunc (p *podPreemptor) RemoveNominatedNodeName(pod *v1.Pod) error { if len(pod.Status.NominatedNodeName) == 0 { return nil } return p.SetNominatedNodeName(pod, \"\") } 2.2. NominatedNodeName Pod.Status.NominatedNodeName的说明：\nnominatedNodeName是调度失败的pod抢占别的pod的时候，被抢占pod的运行节点。但在剔除被抢占pod之前该调度失败的pod不会被调度。同时也不保证最终该pod一定会调度到nominatedNodeName的机器上，也可能因为之后资源充足等原因调度到其他节点上。最终该pod会被加到调度的队列中。\n其中加入到调度队列的具体过程如下：\nfunc NewConfigFactory(args *ConfigFactoryArgs) scheduler.Configurator { ... // unscheduled pod queue args.PodInformer.Informer().AddEventHandler( ... Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToSchedulingQueue, UpdateFunc: c.updatePodInSchedulingQueue, DeleteFunc: c.deletePodFromSchedulingQueue, }, }, ) ... } addPodToSchedulingQueue:\nfunc (c *configFactory) addPodToSchedulingQueue(obj interface{}) { if err := c.podQueue.Add(obj.(*v1.Pod)); err != nil { runtime.HandleError(fmt.Errorf(\"unable to queue %T: %v\", obj, err)) } } PriorityQueue.Add:\n// Add adds a pod to the active queue. It should be called only when a new pod // is added so there is no chance the pod is already in either queue. func (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() err := p.activeQ.Add(pod) if err != nil { glog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { if p.unschedulableQ.get(pod) != nil { glog.Errorf(\"Error: pod %v/%v is already in the unschedulable queue.\", pod.Namespace, pod.Name) p.deleteNominatedPodIfExists(pod) p.unschedulableQ.delete(pod) } p.addNominatedPodIfNeeded(pod) p.cond.Broadcast() } return err } addNominatedPodIfNeeded:\n// addNominatedPodIfNeeded adds a pod to nominatedPods if it has a NominatedNodeName and it does not // already exist in the map. Adding an existing pod is not going to update the pod. func (p *PriorityQueue) addNominatedPodIfNeeded(pod *v1.Pod) { nnn := NominatedNodeName(pod) if len(nnn) \u003e 0 { for _, np := range p.nominatedPods[nnn] { if np.UID == pod.UID { glog.Errorf(\"Pod %v/%v already exists in the nominated map!\", pod.Namespace, pod.Name) return } } p.nominatedPods[nnn] = append(p.nominatedPods[nnn], pod) } } NominatedNodeName:\n// NominatedNodeName returns nominated node name of a Pod. func NominatedNodeName(pod *v1.Pod) string { return pod.Status.NominatedNodeName } 3. genericScheduler.Preempt 抢占算法依然是在ScheduleAlgorithm接口中定义。\n// ScheduleAlgorithm is an interface implemented by things that know how to schedule pods // onto machines. type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt receives scheduling errors for a pod and tries to create room for // the pod by preempting lower priority pods if possible. // It returns the node where preemption happened, a list of preempted pods, a // list of pods whose nominated node name should be removed, and error if any. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) // Predicates() returns a pointer to a map of predicate functions. This is // exposed for testing. Predicates() map[string]FitPredicate // Prioritizers returns a slice of priority config. This is exposed for // testing. Prioritizers() []PriorityConfig } Preempt的具体实现为genericScheduler结构体。\nPreempt的主要实现是找到可以调度的节点和上面因抢占而需要被剔除的pod。\n基本流程如下：\n根据调度失败的原因对所有节点先进行一批筛选，筛选出潜在的被调度节点列表。 通过selectNodesForPreemption筛选出需要牺牲的pod和其节点。 基于拓展抢占逻辑再次对上述筛选出来的牺牲者做过滤。 基于上述的过滤结果，选择一个最终可能因抢占被调度的节点。 基于上述的候选节点，找出该节点上优先级低于当前被调度pod的牺牲者pod列表。 完整代码如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\n// preempt finds nodes with pods that can be preempted to make room for \"pod\" to // schedule. It chooses one of the nodes and preempts the pods on the node and // returns 1) the node, 2) the list of preempted pods if such a node is found, // 3) A list of pods whose nominated node name should be cleared, and 4) any // possible error. func (g *genericScheduler) Preempt(pod *v1.Pod, nodeLister algorithm.NodeLister, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) { // Scheduler may return various types of errors. Consider preemption only if // the error is of type FitError. fitError, ok := scheduleErr.(*FitError) if !ok || fitError == nil { return nil, nil, nil, nil } err := g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return nil, nil, nil, err } if !podEligibleToPreemptOthers(pod, g.cachedNodeInfoMap) { glog.V(5).Infof(\"Pod %v/%v is not eligible for more preemption.\", pod.Namespace, pod.Name) return nil, nil, nil, nil } allNodes, err := nodeLister.List() if err != nil { return nil, nil, nil, err } if len(allNodes) == 0 { return nil, nil, nil, ErrNoNodesAvailable } potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError.FailedPredicates) if len(potentialNodes) == 0 { glog.V(3).Infof(\"Preemption will not help schedule pod %v/%v on any node.\", pod.Namespace, pod.Name) // In this case, we should clean-up any existing nominated node name of the pod. return nil, nil, []*v1.Pod{pod}, nil } pdbs, err := g.cache.ListPDBs(labels.Everything()) if err != nil { return nil, nil, nil, err } // 找出可能被抢占的节点 nodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil { return nil, nil, nil, err } // We will only check nodeToVictims with extenders that support preemption. // Extenders which do not support preemption may later prevent preemptor from being scheduled on the nominated // node. In that case, scheduler will find a different host for the preemptor in subsequent scheduling cycles. nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } // 选出最终被抢占的节点 candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } // Lower priority pods nominated to run on this node, may no longer fit on // this node. So, we should remove their nomination. Removing their // nomination updates these pods and moves them to the active queue. It // lets scheduler find another place for them. // 找出被强占节点上牺牲者pod列表 nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } return nil, nil, nil, fmt.Errorf( \"preemption failed: the target node %s has been deleted from scheduler cache\", candidateNode.Name) } 以下对genericScheduler.Preempt分段进行分析。\n3.1. selectNodesForPreemption selectNodesForPreemption并行地所有节点中找可能被抢占的节点。\nnodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates,g.predicateMetaProducer, g.schedulingQueue, pdbs) selectNodesForPreemption主要基于selectVictimsOnNode构造一个checkNode的函数，然后并发执行该函数。\nselectNodesForPreemption具体实现如下：\n// selectNodesForPreemption finds all the nodes with possible victims for // preemption in parallel. func selectNodesForPreemption(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, potentialNodes []*v1.Node, predicates map[string]algorithm.FitPredicate, metadataProducer algorithm.PredicateMetadataProducer, queue SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) (map[*v1.Node]*schedulerapi.Victims, error) { nodeToVictims := map[*v1.Node]*schedulerapi.Victims{} var resultLock sync.Mutex // We can use the same metadata producer for all nodes. meta := metadataProducer(pod, nodeNameToInfo) checkNode := func(i int) { nodeName := potentialNodes[i].Name var metaCopy algorithm.PredicateMetadata if meta != nil { metaCopy = meta.ShallowCopy() } pods, numPDBViolations, fits := selectVictimsOnNode(pod, metaCopy, nodeNameToInfo[nodeName], predicates, queue, pdbs) if fits { resultLock.Lock() victims := schedulerapi.Victims{ Pods: pods, NumPDBViolations: numPDBViolations, } nodeToVictims[potentialNodes[i]] = \u0026victims resultLock.Unlock() } } workqueue.Parallelize(16, len(potentialNodes), checkNode) return nodeToVictims, nil } 3.1.1. selectVictimsOnNode selectVictimsOnNode找到应该被抢占的给定节点上的最小pod集合，以便给调度失败的pod安排足够的空间。该函数最终返回的是一个pod的数组。当有更低优先级的pod可能被选择的时候，较高优先级的pod不会被选入该待剔除的pod集合。\n基本流程如下：\n先检查当该节点上所有低于预被调度pod优先级的pod移除后，该pod能否被调度到当前节点上。 如果上述检查可以，则将该节点的所有低优先级pod按照优先级来排序。 // selectVictimsOnNode finds minimum set of pods on the given node that should // be preempted in order to make enough room for \"pod\" to be scheduled. The // minimum set selected is subject to the constraint that a higher-priority pod // is never preempted when a lower-priority pod could be (higher/lower relative // to one another, not relative to the preemptor \"pod\"). // The algorithm first checks if the pod can be scheduled on the node when all the // lower priority pods are gone. If so, it sorts all the lower priority pods by // their priority and then puts them into two groups of those whose PodDisruptionBudget // will be violated if preempted and other non-violating pods. Both groups are // sorted by priority. It first tries to reprieve as many PDB violating pods as // possible and then does them same for non-PDB-violating pods while checking // that the \"pod\" can still fit on the node. // NOTE: This function assumes that it is never called if \"pod\" cannot be scheduled // due to pod affinity, node affinity, or node anti-affinity reasons. None of // these predicates can be satisfied by removing more pods from the node. func selectVictimsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, fitPredicates map[string]algorithm.FitPredicate, queue SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) ([]*v1.Pod, int, bool) { potentialVictims := util.SortableList{CompFunc: util.HigherPriorityPod} nodeInfoCopy := nodeInfo.Clone() removePod := func(rp *v1.Pod) { nodeInfoCopy.RemovePod(rp) if meta != nil { meta.RemovePod(rp) } } addPod := func(ap *v1.Pod) { nodeInfoCopy.AddPod(ap) if meta != nil { meta.AddPod(ap, nodeInfoCopy) } } // As the first step, remove all the lower priority pods from the node and // check if the given pod can be scheduled. podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() { if util.GetPodPriority(p) \u003c podPriority { potentialVictims.Items = append(potentialVictims.Items, p) removePod(p) } } potentialVictims.Sort() // If the new pod does not fit after removing all the lower priority pods, // we are almost done and this node is not suitable for preemption. The only condition // that we should check is if the \"pod\" is failing to schedule due to pod affinity // failure. // TODO(bsalamat): Consider checking affinity to lower priority pods if feasible with reasonable performance. if fits, _, err := podFitsOnNode(pod, meta, nodeInfoCopy, fitPredicates, nil, nil, queue, false, nil); !fits { if err != nil { glog.Warningf(\"Encountered error while selecting victims on node %v: %v\", nodeInfo.Node().Name, err) } return nil, 0, false } var victims []*v1.Pod numViolatingVictim := 0 // Try to reprieve as many pods as possible. We first try to reprieve the PDB // violating victims and then other non-violating ones. In both cases, we start // from the highest priority victims. violatingVictims, nonViolatingVictims := filterPodsWithPDBViolation(potentialVictims.Items, pdbs) reprievePod := func(p *v1.Pod) bool { addPod(p) fits, _, _ := podFitsOnNode(pod, meta, nodeInfoCopy, fitPredicates, nil, nil, queue, false, nil) if !fits { removePod(p) victims = append(victims, p) glog.V(5).Infof(\"Pod %v is a potential preemption victim on node %v.\", p.Name, nodeInfo.Node().Name) } return fits } for _, p := range violatingVictims { if !reprievePod(p) { numViolatingVictim++ } } // Now we try to reprieve non-violating victims. for _, p := range nonViolatingVictims { reprievePod(p) } return victims, numViolatingVictim, true } 3.2. processPreemptionWithExtenders processPreemptionWithExtenders基于selectNodesForPreemption选出的牺牲者进行扩展的抢占逻辑继续筛选牺牲者。\n// We will only check nodeToVictims with extenders that support preemption. // Extenders which do not support preemption may later prevent preemptor from being scheduled on the nominated // node. In that case, scheduler will find a different host for the preemptor in subsequent scheduling cycles. nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } processPreemptionWithExtenders完整代码如下：\n// processPreemptionWithExtenders processes preemption with extenders func (g *genericScheduler) processPreemptionWithExtenders( pod *v1.Pod, nodeToVictims map[*v1.Node]*schedulerapi.Victims, ) (map[*v1.Node]*schedulerapi.Victims, error) { if len(nodeToVictims) \u003e 0 { for _, extender := range g.extenders { if extender.SupportsPreemption() \u0026\u0026 extender.IsInterested(pod) { newNodeToVictims, err := extender.ProcessPreemption( pod, nodeToVictims, g.cachedNodeInfoMap, ) if err != nil { if extender.IsIgnorable() { glog.Warningf(\"Skipping extender %v as it returned error %v and has ignorable flag set\", extender, err) continue } return nil, err } // Replace nodeToVictims with new result after preemption. So the // rest of extenders can continue use it as parameter. nodeToVictims = newNodeToVictims // If node list becomes empty, no preemption can happen regardless of other extenders. if len(nodeToVictims) == 0 { break } } } } return nodeToVictims, nil } 3.3. pickOneNodeForPreemption pickOneNodeForPreemption从筛选出的node中再挑选一个节点作为最终调度节点。\ncandidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } pickOneNodeForPreemption完整代码如下：\n// pickOneNodeForPreemption chooses one node among the given nodes. It assumes // pods in each map entry are ordered by decreasing priority. // It picks a node based on the following criteria: // 1. A node with minimum number of PDB violations. // 2. A node with minimum highest priority victim is picked. // 3. Ties are broken by sum of priorities of all victims. // 4. If there are still ties, node with the minimum number of victims is picked. // 5. If there are still ties, the first such node is picked (sort of randomly). // The 'minNodes1' and 'minNodes2' are being reused here to save the memory // allocation and garbage collection time. func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node { if len(nodesToVictims) == 0 { return nil } minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 for node, victims := range nodesToVictims { if len(victims.Pods) == 0 { // We found a node that doesn't need any preemption. Return it! // This should happen rarely when one or more pods are terminated between // the time that scheduler tries to schedule the pod and the time that // preemption logic tries to find nodes for preemption. return node } numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods \u003c minNumPDBViolatingPods { minNumPDBViolatingPods = numPDBViolatingPods minNodes1 = nil lenNodes1 = 0 } if numPDBViolatingPods == minNumPDBViolatingPods { minNodes1 = append(minNodes1, node) lenNodes1++ } } if lenNodes1 == 1 { return minNodes1[0] } // There are more than one node with minimum number PDB violating pods. Find // the one with minimum highest priority victim. minHighestPriority := int32(math.MaxInt32) var minNodes2 = make([]*v1.Node, lenNodes1) lenNodes2 := 0 for i := 0; i \u003c lenNodes1; i++ { node := minNodes1[i] victims := nodesToVictims[node] // highestPodPriority is the highest priority among the victims on this node. highestPodPriority := util.GetPodPriority(victims.Pods[0]) if highestPodPriority \u003c minHighestPriority { minHighestPriority = highestPodPriority lenNodes2 = 0 } if highestPodPriority == minHighestPriority { minNodes2[lenNodes2] = node lenNodes2++ } } if lenNodes2 == 1 { return minNodes2[0] } // There are a few nodes with minimum highest priority victim. Find the // smallest sum of priorities. minSumPriorities := int64(math.MaxInt64) lenNodes1 = 0 for i := 0; i \u003c lenNodes2; i++ { var sumPriorities int64 node := minNodes2[i] for _, pod := range nodesToVictims[node].Pods { // We add MaxInt32+1 to all priorities to make all of them \u003e= 0. This is // needed so that a node with a few pods with negative priority is not // picked over a node with a smaller number of pods with the same negative // priority (and similar scenarios). sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) } if sumPriorities \u003c minSumPriorities { minSumPriorities = sumPriorities lenNodes1 = 0 } if sumPriorities == minSumPriorities { minNodes1[lenNodes1] = node lenNodes1++ } } if lenNodes1 == 1 { return minNodes1[0] } // There are a few nodes with minimum highest priority victim and sum of priorities. // Find one with the minimum number of pods. minNumPods := math.MaxInt32 lenNodes2 = 0 for i := 0; i \u003c lenNodes1; i++ { node := minNodes1[i] numPods := len(nodesToVictims[node].Pods) if numPods \u003c minNumPods { minNumPods = numPods lenNodes2 = 0 } if numPods == minNumPods { minNodes2[lenNodes2] = node lenNodes2++ } } // At this point, even if there are more than one node with the same score, // return the first one. if lenNodes2 \u003e 0 { return minNodes2[0] } glog.Errorf(\"Error in logic of node scoring for preemption. We should never reach here!\") return nil } 3.4. getLowerPriorityNominatedPods getLowerPriorityNominatedPods的基本流程如下：\n获取候选节点上的pod列表。 获取待调度pod的优先级值。 遍历该节点的pod列表，如果低于待调度pod的优先级则放入低优先级pod列表中。 genericScheduler.Preempt中相关代码如下：\n// Lower priority pods nominated to run on this node, may no longer fit on // this node. So, we should remove their nomination. Removing their // nomination updates these pods and moves them to the active queue. It // lets scheduler find another place for them. nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } getLowerPriorityNominatedPods代码如下：\n此部分代码位于pkg/scheduler/core/generic_scheduler.go\n// getLowerPriorityNominatedPods returns pods whose priority is smaller than the // priority of the given \"pod\" and are nominated to run on the given node. // Note: We could possibly check if the nominated lower priority pods still fit // and return those that no longer fit, but that would require lots of // manipulation of NodeInfo and PredicateMeta per nominated pod. It may not be // worth the complexity, especially because we generally expect to have a very // small number of nominated pods per node. func (g *genericScheduler) getLowerPriorityNominatedPods(pod *v1.Pod, nodeName string) []*v1.Pod { pods := g.schedulingQueue.WaitingPodsForNode(nodeName) if len(pods) == 0 { return nil } var lowerPriorityPods []*v1.Pod podPriority := util.GetPodPriority(pod) for _, p := range pods { if util.GetPodPriority(p) \u003c podPriority { lowerPriorityPods = append(lowerPriorityPods, p) } } return lowerPriorityPods } 4. 总结 4.1. Scheduler.preempt 当pod调度失败的时候，会抢占低优先级pod的空间来给高优先级的pod。其中入参为调度失败的pod对象和调度失败的err。\n抢占的基本流程如下：\n判断是否有关闭抢占机制，如果关闭抢占机制则直接返回。 获取调度失败pod的最新对象数据。 执行抢占算法Algorithm.Preempt，返回预调度节点和需要被剔除的pod列表。 将抢占算法返回的node添加到pod的Status.NominatedNodeName中，并删除需要被剔除的pod。 当抢占算法返回的node是nil的时候，清除pod的Status.NominatedNodeName信息。 整个抢占流程的最终结果实际上是更新Pod.Status.NominatedNodeName属性的信息。如果抢占算法返回的节点不为空，则将该node更新到Pod.Status.NominatedNodeName中，否则就将Pod.Status.NominatedNodeName设置为空。\n4.2. genericScheduler.Preempt Preempt的主要实现是找到可以调度的节点和上面因抢占而需要被剔除的pod。\n基本流程如下：\n根据调度失败的原因对所有节点先进行一批筛选，筛选出潜在的被调度节点列表。 通过selectNodesForPreemption筛选出需要牺牲的pod和其节点。 基于拓展抢占逻辑再次对上述筛选出来的牺牲者做过滤。 基于上述的过滤结果，选择一个最终可能因抢占被调度的节点。 基于上述的候选节点，找出该节点上优先级低于当前被调度pod的牺牲者pod列表。 参考：\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go ","categories":"","description":"","excerpt":" 以下代码分析基于 kubernetes v1.12.0 版本。\n本文主要分析调度中的抢占逻辑，当pod不适合任何节点的时候，可能pod会调 …","ref":"/k8s-source-code-analysis/kube-scheduler/preempt/","tags":["源码分析"],"title":"kube-scheduler源码分析（六）之 抢占逻辑"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/principle/","tags":"","title":"原理篇"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/resource/","tags":"","title":"资源隔离"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/mysql/","tags":"","title":"Mysql"},{"body":"1. 函数定义 函数可以让我们将一个复杂功能划分成若干模块，让程序结构更加清晰，代码重复利用率更高。Shell 也支持函数。Shell 函数必须先定义后使用。\nShell 函数的定义格式如下：\nfunction_name () { list of commands [ return value ] } 也可以在函数名前加上关键字 function：\nfunction function_name () { list of commands [ return value ] } 2. 函数返回值 函数返回值，可以显式增加return语句；如果不加，会将最后一条命令运行结果作为返回值。\nShell 函数返回值只能是整数，一般用来表示函数执行成功与否，0表示成功，其他值表示失败。\n如果 return 其他数据，比如一个字符串，往往会得到错误提示：“numeric argument required”。\n如果一定要让函数返回字符串，那么可以先定义一个变量，用来接收函数的计算结果，脚本在需要的时候访问这个变量来获得函数返回值。函数返回值在调用该函数后通过 $?【$?表示上个命令的退出状态，或函数的返回值。】 来获得。\n3. 函数调用 调用函数只需要给出函数名，不需要加括号。\n像删除变量一样，删除函数也可以使用 unset 不过要加上 .f 选项\n$unset .f function_name 如果你希望直接从终端调用函数，可以将函数定义在主目录下的 .profile 文件，这样每次登录后，在命令提示符后面输入函数名字就可以立即调用。\n示例：\n#!/bin/bash # Define your function here Hello () { echo \"Url is http://see.xidian.edu.cn/cpp/shell/\" } # Invoke your function Hello 运行结果：\n$./test.sh Hello World $ 4. 函数参数 在Shell中，调用函数时可以向其传递参数。在函数体内部，通过 $n 的形式来获取参数的值，例如，$1表示第一个参数，$2表示第二个参数...\n注意，$10 不能获取第十个参数，获取第十个参数需要${10}。当n\u003e=10时，需要使用${n}来获取参数。\n特殊变量用来处理参数:\n特殊变量 说明 $# 传递给函数的参数个数。 $* 显示所有传递给函数的参数。 $@ 与$*相同，但是略有区别，请查看Shell特殊变量。 $? 函数的返回值。 带参数的函数示例：\n#!/bin/bash funWithParam(){ echo \"The value of the first parameter is $1 !\" echo \"The value of the second parameter is $2 !\" echo \"The value of the tenth parameter is $10 !\" echo \"The value of the tenth parameter is ${10} !\" echo \"The value of the eleventh parameter is ${11} !\" echo \"The amount of the parameters is $# !\" # 参数个数 echo \"The string of the parameters is $* !\" # 传递给函数的所有参数 } funWithParam 1 2 3 4 5 6 7 8 9 34 73 运行脚本：\nThe value of the first parameter is 1 ! The value of the second parameter is 2 ! The value of the tenth parameter is 10 ! The value of the tenth parameter is 34 ! The value of the eleventh parameter is 73 ! The amount of the parameters is 12 ! The string of the parameters is 1 2 3 4 5 6 7 8 9 34 73 !\" 参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. 函数定义 函数可以让我们将一个复杂功能划分成若干模块，让程序结构更加清晰，代码重复利用率更高。Shell 也支持函数。Shell 函数 …","ref":"/linux-notes/shell/shell-function/","tags":["Shell"],"title":"Shell 函数"},{"body":"Unix 命令默认从标准输入设备(stdin)获取输入，将结果输出到标准输出设备(stdout)显示。一般情况下，标准输入设备就是键盘，标准输出设备就是终端，即显示器。\n1. 输出重定向 命令的输出不仅可以是显示器，还可以很容易的转移向到文件，这被称为输出重定向。\n命令输出重定向的语法为：\n$ command \u003e file 这样，输出到显示器的内容就可以被重定向到文件。\n输出重定向会覆盖文件内容；如果不希望文件内容被覆盖，可以使用 \u003e\u003e 追加到文件末尾\n2. 输入重定向 和输出重定向一样，Unix 命令也可以从文件获取输入，语法为：\n$ command \u003c file 这样，本来需要从键盘获取输入的命令会转移到文件读取内容。\n注意：输出重定向是大于号(\u003e)，输入重定向是小于号(\u003c)。\n3. 重定向深入讲解 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件：\n标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command \u003e file 将 stdout 重定向到 file，command \u003c file 将stdin 重定向到 file。\n如果希望 stderr 重定向到 file，可以这样写：\n$command 2 \u003e file 如果希望 stderr 追加到 file 文件末尾，可以这样写：\n$command 2 \u003e\u003e file 2 表示标准错误文件(stderr)。\n如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写：\n$command \u003e file 2\u003e\u00261 或\n$command \u003e\u003e file 2\u003e\u00261 如果希望对 stdin 和 stdout 都重定向，可以这样写：\n$command \u003c file1 \u003efile2 command 命令将 stdin 重定向到 file1，将 stdout 重定向到 file2。\n全部可用的重定向命令列表\n命令 说明 command \u003e file 将输出重定向到 file。 command \u003c file 将输入重定向到 file。 command \u003e\u003e file 将输出以追加的方式重定向到 file。 n \u003e file 将文件描述符为 n 的文件重定向到 file。 n \u003e\u003e file 将文件描述符为 n 的文件以追加的方式重定向到 file。 n \u003e\u0026 m 将输出文件 m 和 n 合并。 n \u003c\u0026 m 将输入文件 m 和 n 合并。 \u003c\u003c tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 4. Here Document Here Document 目前没有统一的翻译，这里暂译为”嵌入文档“。Here Document 是 Shell 中的一种特殊的重定向方式，它的基本的形式如下：\ncommand \u003c\u003c delimiter document delimiter 它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。 注意：\n结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。 开始的delimiter前后的空格会被忽略掉。 5. /dev/null 文件 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：\n$ command \u003e /dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出“的效果。\n如果希望屏蔽 stdout 和 stderr，可以这样写：\n$ command \u003e /dev/null 2\u003e\u00261 6. 文件包含 像其他语言一样，Shell 也可以包含外部脚本，将外部脚本的内容合并到当前脚本。\nShell 中包含脚本可以使用：\n. filename 或\nsource filename 两种方式的效果相同，简单起见，一般使用点号(.)，但是注意点号(.)和文件名中间有一空格。\n注意：被包含脚本不需要有执行权限。\n参考：\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"Unix 命令默认从标准输入设备(stdin)获取输入，将结果输出到标准输出设备(stdout)显示。一般情况下，标准输入设备就是键盘，标准 …","ref":"/linux-notes/shell/shell-stdout/","tags":["Shell"],"title":"Shell 重定向"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/framework/","tags":"","title":"框架与工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/","tags":"","title":"运维指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/standard/","tags":"","title":"代码规范"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/develop/","tags":"","title":"开发指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/trouble-shooting/","tags":"","title":"问题排查"},{"body":" 本文主要分析controller-runtime的源码，源码版本为v0.16.3\n1. 概述 controller-runtime源码地址：https://github.com/kubernetes-sigs/controller-runtime。\ncontroller-runtime项目是一个用于快速构建k8s operator的工具包。其中kubebuilder和operator-sdk项目都是通过controller-runtime项目来快速编写k8s operator的工具。\n本文以kubebuilder的代码生成架构为例，分析controller-runtime的逻辑。kubebuilder框架生成的代码参考：https://github.com/huweihuang/venus\n2. controller-runtime架构图 代码目录：\npkg ├── builder ├── cache ├── client # client用于操作k8s的对象 ├── cluster ├── config ├── controller # controller逻辑 ├── envtest ├── event ├── finalizer ├── handler ├── internal # 核心代码 controller的具体实现 ├── leaderelection ├── manager # 核心代码 ├── predicate ├── ratelimiter ├── reconcile ├── recorder ├── scheme ├── source └── webhook 3. Operator框架逻辑 代码参考：https://github.com/huweihuang/venus/blob/main/cmd/app/operator.go#L71\noperator代码框架的主体逻辑包括以下几个部分。\nmanager：主要用来管理多个的controller，构建，注册，运行controller。\ncontroller：主要用来封装reconciler的控制器。\nreconciler：具体执行业务逻辑的函数。\nmanger的框架主要包含以下几个部分。\nmgr:=ctrl.NewManager：构建一个manager对象。\nReconciler.SetupWithManager(mgr)：注册controller到manager对象。\nmgr.Start(ctrl.SetupSignalHandler())：运行manager从而运行controller的逻辑。\n代码如下：\n// 构建manager对象，主要的初始化参数包括 // - kubeconfig // - controller的option参数 mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, Metrics: metricsserver.Options{BindAddress: opt.MetricsAddr}, HealthProbeBindAddress: opt.ProbeAddr, LeaderElection: opt.EnableLeaderElection, LeaderElectionID: \"52609143.huweihuang.com\", Controller: config.Controller{ MaxConcurrentReconciles: opt.MaxConcurrentReconciles, }, }) // 将controller注册到manager中，并初始化controller对象。 if err = (\u0026venuscontroller.RedisReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \"unable to create controller\", \"controller\", \"Redis\") return err } // 运行manager对象，从而运行controller中的reconcile逻辑。 if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \"problem running manager\") return err } 4. NewManager NewManager初始化一个manager 用来管理和创建controller对象。一个manager可以关联多个controller对象。manager是一个接口，而最终是实现结构体是controllerManager的对象。\n4.1. Manager接口 type Manager interface { // Cluster holds a variety of methods to interact with a cluster. cluster.Cluster // Add will set requested dependencies on the component, and cause the component to be // started when Start is called. // Depending on if a Runnable implements LeaderElectionRunnable interface, a Runnable can be run in either // non-leaderelection mode (always running) or leader election mode (managed by leader election if enabled). // 通过Runnable接口将具体的controller注册到manager中。 Add(Runnable) error // Start starts all registered Controllers and blocks until the context is cancelled. // Returns an error if there is an error starting any controller. // // If LeaderElection is used, the binary must be exited immediately after this returns, // otherwise components that need leader election might continue to run after the leader // lock was lost. // 运行具体的逻辑 Start(ctx context.Context) error } 4.2. NewControllerManager New构建一个具体的controllerManager的对象。\nfunc New(config *rest.Config, options Options) (Manager, error) { // Set default values for options fields options = setOptionsDefaults(options) ... errChan := make(chan error) runnables := newRunnables(options.BaseContext, errChan) return \u0026controllerManager{ stopProcedureEngaged: pointer.Int64(0), cluster: cluster, runnables: runnables, errChan: errChan, recorderProvider: recorderProvider, resourceLock: resourceLock, metricsServer: metricsServer, controllerConfig: options.Controller, logger: options.Logger, elected: make(chan struct{}), webhookServer: options.WebhookServer, leaderElectionID: options.LeaderElectionID, leaseDuration: *options.LeaseDuration, renewDeadline: *options.RenewDeadline, retryPeriod: *options.RetryPeriod, healthProbeListener: healthProbeListener, readinessEndpointName: options.ReadinessEndpointName, livenessEndpointName: options.LivenessEndpointName, pprofListener: pprofListener, gracefulShutdownTimeout: *options.GracefulShutdownTimeout, internalProceduresStop: make(chan struct{}), leaderElectionStopped: make(chan struct{}), leaderElectionReleaseOnCancel: options.LeaderElectionReleaseOnCancel, }, nil } 5. SetupWithManager SetupWithManager将具体的controller注册到manager中。其中通过Complete完成controller的初始化。\n// SetupWithManager sets up the controller with the Manager. func (r *RedisReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026venusv1.Redis{}). Complete(r) } SetupWithManager通过NewControllerManagedBy方法构建了一个Builder的对象。\n// Builder builds a Controller. type Builder struct { forInput ForInput ownsInput []OwnsInput watchesInput []WatchesInput mgr manager.Manager globalPredicates []predicate.Predicate ctrl controller.Controller ctrlOptions controller.Options name string } // ControllerManagedBy returns a new controller builder that will be started by the provided Manager. func ControllerManagedBy(m manager.Manager) *Builder { return \u0026Builder{mgr: m} } 通过builder对象完成controller的初始化。\n5.1. controller初始化 // Build builds the Application Controller and returns the Controller it created. func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) { // Set the ControllerManagedBy if err := blder.doController(r); err != nil { // 初始化controller return nil, err } // Set the Watch if err := blder.doWatch(); err != nil { // 添加event handler return nil, err } return blder.ctrl, nil } doController最终通过调用NewUnmanaged构建一个controller对象。并传入自定义的reconciler对象。\nfunc New(name string, mgr manager.Manager, options Options) (Controller, error) { c, err := NewUnmanaged(name, mgr, options) ... // Add the controller as a Manager components return c, mgr.Add(c) } // NewUnmanaged returns a new controller without adding it to the manager. The // caller is responsible for starting the returned controller. func NewUnmanaged(name string, mgr manager.Manager, options Options) (Controller, error) { if options.Reconciler == nil { return nil, fmt.Errorf(\"must specify Reconciler\") } ... // Create controller with dependencies set return \u0026controller.Controller{ Do: options.Reconciler, // 将具体的reconciler函数传递到controller的reconciler。 MakeQueue: func() workqueue.RateLimitingInterface { return workqueue.NewRateLimitingQueueWithConfig(options.RateLimiter, workqueue.RateLimitingQueueConfig{ Name: name, }) }, // 初始化任务队列 MaxConcurrentReconciles: options.MaxConcurrentReconciles, // 设置controller的并发数 CacheSyncTimeout: options.CacheSyncTimeout, Name: name, LogConstructor: options.LogConstructor, RecoverPanic: options.RecoverPanic, LeaderElected: options.NeedLeaderElection, }, nil 5.2. 添加event handler doWatch最终会运行informer.start添加event handler。\n// Watch implements controller.Controller. func (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error { c.mu.Lock() defer c.mu.Unlock() // Controller hasn't started yet, store the watches locally and return. // // These watches are going to be held on the controller struct until the manager or user calls Start(...). if !c.Started { c.startWatches = append(c.startWatches, watchDescription{src: src, handler: evthdler, predicates: prct}) return nil } c.LogConstructor(nil).Info(\"Starting EventSource\", \"source\", src) return src.Start(c.ctx, evthdler, c.Queue, prct...) } add event handler\nfunc (is *Informer) Start(ctx context.Context, handler handler.EventHandler, queue workqueue.RateLimitingInterface, prct ...predicate.Predicate) error { // Informer should have been specified by the user. if is.Informer == nil { return fmt.Errorf(\"must specify Informer.Informer\") } _, err := is.Informer.AddEventHandler(internal.NewEventHandler(ctx, queue, handler, prct).HandlerFuncs()) if err != nil { return err } return nil } 6. mgr.Start controllerManager运行之前注册的runnables的函数，其中包括controller的函数。\nfunc (cm *controllerManager) Start(ctx context.Context) (err error) { // Start and wait for caches. if err := cm.runnables.Caches.Start(cm.internalCtx); err != nil { } // Start the non-leaderelection Runnables after the cache has synced. if err := cm.runnables.Others.Start(cm.internalCtx); err != nil { } // Start the leader election and all required runnables. { ctx, cancel := context.WithCancel(context.Background()) cm.leaderElectionCancel = cancel go func() { if cm.resourceLock != nil { if err := cm.startLeaderElection(ctx); err != nil { cm.errChan \u003c- err } } else { // Treat not having leader election enabled the same as being elected. if err := cm.startLeaderElectionRunnables(); err != nil { cm.errChan \u003c- err } close(cm.elected) } }() } ... } 6.1. controller.start start主要包含2个部分\n同步cache：WaitForSync 启动指定并发数的worker：processNextWorkItem 该部分的代码逻辑跟k8s controller-manager中的具体的controller的逻辑类似。\nfunc (c *Controller) Start(ctx context.Context) error { ... err := func() error { ... for _, watch := range c.startWatches { syncingSource, ok := watch.src.(source.SyncingSource) // 同步list-watch中的cache数据。 if err := func() error { if err := syncingSource.WaitForSync(sourceStartCtx); err != nil { ... } } } // 运行指定并发数的processNextWorkItem任务。 // Launch workers to process resources c.LogConstructor(nil).Info(\"Starting workers\", \"worker count\", c.MaxConcurrentReconciles) wg.Add(c.MaxConcurrentReconciles) for i := 0; i \u003c c.MaxConcurrentReconciles; i++ { go func() { defer wg.Done() // Run a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the reconcileHandler is never invoked concurrently with the same object. for c.processNextWorkItem(ctx) { } }() } ... } 6.2. processNextWorkItem 经典的processNextWorkItem函数，最终调用reconcileHandler来处理具体的逻辑。\nfunc (c *Controller) processNextWorkItem(ctx context.Context) bool { obj, shutdown := c.Queue.Get() if shutdown { // Stop working return false } defer c.Queue.Done(obj) ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(1) defer ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(-1) c.reconcileHandler(ctx, obj) return true } 7. reconcileHandler reconcileHandler部分的代码是整个reconciler逻辑中的核心，自定义的reconciler函数最终是调用了reconcileHandler来实现，并且该函数描述了具体的任务队列处理的几种类型。\nerr != nil：如果错误不为空，则重新入队，等待处理。 result.RequeueAfter \u003e 0：如果指定RequeueAfter \u003e 0，则做延迟入队处理。 result.Requeue：如何指定了requeue则表示马上重新入队处理。 err == nil : 如果错误为空，表示reconcile成功，则移除队列的任务。 func (c *Controller) reconcileHandler(ctx context.Context, obj interface{}) { // 获取k8s crd的具体对象 req, ok := obj.(reconcile.Request) if !ok { // As the item in the workqueue is actually invalid, we call // Forget here else we'd go into a loop of attempting to // process a work item that is invalid. c.Queue.Forget(obj) c.LogConstructor(nil).Error(nil, \"Queue item was not a Request\", \"type\", fmt.Sprintf(\"%T\", obj), \"value\", obj) // Return true, don't take a break return } // 调用用户定义的Reconcile函数，并对返回结果进行处理。 result, err := c.Reconcile(ctx, req) switch { case err != nil: // 如果错误不为空，则重新入队，等待处理 if errors.Is(err, reconcile.TerminalError(nil)) { ctrlmetrics.TerminalReconcileErrors.WithLabelValues(c.Name).Inc() } else { c.Queue.AddRateLimited(req) } case result.RequeueAfter \u003e 0: // 如果指定了延迟入队，则做延迟入队处理 c.Queue.Forget(obj) c.Queue.AddAfter(req, result.RequeueAfter) case result.Requeue: // 如果指定了马上入队，则做相应处理 c.Queue.AddRateLimited(req) default: // 如果错误为空，表示reconcile成功，则移除队列的任务 log.V(5).Info(\"Reconcile successful\") // Finally, if no error occurs we Forget this item so it does not // get queued again until another change happens. c.Queue.Forget(obj) } } 8. 总结 controller-runtime封装了k8s-controller-manager控制器的主要逻辑，其中就包括创建list-watch对象，waitForSync等，创建任务队列，将任务处理的goroutine抽象成一个reconcile函数，使用户更方便的编写operator工具。 kubebuilder是一个基于controller-runtime框架的命令生成工具。可以用于快速生成和部署crd对象，快速生成controller-runtime框架的基本代码。 controller-runtime框架的最核心是reconcileHandler函数，该函数定义了reconcile的4种错误处理及入队重试的类型。可以根据具体的业务需求选择合适的方法来处理。 参考：\ncontroller-runtime源码分析\ncontroller-runtime细节分析\n","categories":"","description":"","excerpt":" 本文主要分析controller-runtime的源码，源码版本为v0.16.3\n1. 概述 controller-runtime源码地 …","ref":"/k8s-source-code-analysis/kube-controller-manager/controller-runtime/","tags":["源码分析"],"title":"controller-runtime源码分析"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/memcached/","tags":"","title":"Memcached"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/web/","tags":"","title":"Web编程"},{"body":"本文主要分析client-go中使用的workqueue，从而来分析k8s是如何基于任务队列做并发控制的。其中代码参考：\nhttps://github.com/kubernetes/client-go/tree/release-1.30 1. 概述 k8s的控制器大多是基于任务队列的方式进行并发控制，甚至包括基于controller-manager开发的自定义operator控制器。以下我们以deployment controller为例展示workqueue在k8s中的使用。\n2. Deployment中的workqueue 2.1. 初始化workqueue Deployment 的构建函数NewDeploymentController中初始化了任务队列和对于event事件处理相对应的workqueue的操作。\n初始化workqueue的限速任务队列。 添加event handler使得deployment对象入队，等待处理。 func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { dc := \u0026DeploymentController{ // 初始化一个限速的任务队列 queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } // 添加 add, update, delete的event handler, 其中handler的操作都添加对象到任务队列中。 dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addDeployment(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateDeployment(logger, oldObj, newObj) }, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: func(obj interface{}) { dc.deleteDeployment(logger, obj) }, }) // 定义队列处理函数 dc.enqueueDeployment = dc.enqueue 以下显示event handler具体逻辑，可见，无论是add，update，delete event handler都是调用enqueueDeployment的函数，而enqueueDeployment实际上就是初始化函数中的dc.enqueue。\nfunc (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) { d := obj.(*apps.Deployment) logger.V(4).Info(\"Adding deployment\", \"deployment\", klog.KObj(d)) dc.enqueueDeployment(d) } func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) { oldD := old.(*apps.Deployment) curD := cur.(*apps.Deployment) logger.V(4).Info(\"Updating deployment\", \"deployment\", klog.KObj(oldD)) dc.enqueueDeployment(curD) } func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) { d, ok := obj.(*apps.Deployment) logger.V(4).Info(\"Deleting deployment\", \"deployment\", klog.KObj(d)) dc.enqueueDeployment(d) } 2.2. 添加任务到队列 以下是dc.enqueue的具体实现，可见最终的调用workqueue的Add方法，添加对象到任务队列中。\nfunc (dc *DeploymentController) enqueue(deployment *apps.Deployment) { key, err := controller.KeyFunc(deployment) // 添加到任务队列中。 dc.queue.Add(key) } 其中key是namespace/name的拼接字符串，通过MetaNamespaceKeyFunc函数获取对象的key。\n// 通过该函数获取k8s对象中处理的key。 func MetaNamespaceKeyFunc(obj interface{}) (string, error) { if key, ok := obj.(ExplicitKey); ok { return string(key), nil } objName, err := ObjectToName(obj) if err != nil { return \"\", err } return objName.String(), nil } // 拼接namespace和name func (objName ObjectName) String() string { if len(objName.Namespace) \u003e 0 { return objName.Namespace + \"/\" + objName.Name } return objName.Name } 2.3. 读取任务队列 deployment controller会调用dc.queue.Get()来读取任务队列中的对象。该goroutine是一个常驻的逻辑实时获取。\nfunc (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool { // 读取任务队列 key, quit := dc.queue.Get() if quit { return false } // 返回将key设置为done。 defer dc.queue.Done(key) // 处理任务 err := dc.syncHandler(ctx, key.(string)) dc.handleErr(ctx, err, key) return true } 任务的错误处理：\n如果错误为空，则调用Forget移出队列 如果小于最大重试次数，则加入延迟队列 如果大于最大重试次数，则移出队列。 func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) { // 如果错误为空，则调用Forget移出队列 if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) { dc.queue.Forget(key) return } ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string)) // 如果小于最大重试次数，则加入延迟队列 if dc.queue.NumRequeues(key) \u003c maxRetries { dc.queue.AddRateLimited(key) return } // 如果大于最大重试次数，则移出队列。 utilruntime.HandleError(err) logger.V(2).Info(\"Dropping deployment out of the queue\", \"deployment\", klog.KRef(ns, name), \"err\", err) dc.queue.Forget(key) } syncDeployment获取deployment对象，基于ns和name重新获取deployment。\nfunc (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error { // 拆分key获取ns和name namespace, name, err := cache.SplitMetaNamespaceKey(key) // 基于ns和name获取deployment对象 deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { logger.V(2).Info(\"Deployment has been deleted\", \"deployment\", klog.KRef(namespace, name)) return nil } // deepcopy deployment对象 d := deployment.DeepCopy() 待完善\n","categories":"","description":"","excerpt":"本文主要分析client-go中使用的workqueue，从而来分析k8s是如何基于任务队列做并发控制的。其中代码参考： …","ref":"/k8s-source-code-analysis/kube-controller-manager/workqueue/","tags":["源码分析"],"title":"workqueue源码分析"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/nginx/","tags":"","title":"Nginx"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/","tags":"","title":"Runtime"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/code/","tags":"","title":"源码分析"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/etcd/","tags":"","title":"Etcd集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/keepalived/","tags":"","title":"Keepalived"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/ide/","tags":"","title":"IDE配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/multi-cluster/","tags":"","title":"多集群管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/keymap/","tags":"","title":"快捷键"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/","tags":"","title":"边缘容器"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/baremetal/","tags":"","title":"裸金属"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/llm/","tags":"","title":"大模型"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/kvm/","tags":"","title":"虚拟化"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/monitor/","tags":"","title":"监控体系"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/serverless/","tags":"","title":"Serverless"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/cluster-optimization/","tags":"","title":"集群优化"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/golang/","tags":"","title":"Golang"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cni/","tags":"","title":"CNI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/disk/","tags":"","title":"disk"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/serverless/","tags":"","title":"Serverless"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/network/","tags":"","title":"Network"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubevirt/","tags":"","title":"KubeVirt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/nginx/","tags":"","title":"Nginx"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%A3%B8%E9%87%91%E5%B1%9E/","tags":"","title":"裸金属"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/operator/","tags":"","title":"Operator"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/openyurt/","tags":"","title":"OpenYurt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/monitor/","tags":"","title":"Monitor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/apisix/","tags":"","title":"ApiSix"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/karmada/","tags":"","title":"Karmada"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/runtime/","tags":"","title":"Runtime"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubeedge/","tags":"","title":"Kubeedge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%A4%9A%E9%9B%86%E7%BE%A4/","tags":"","title":"多集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/linux/","tags":"","title":"Linux"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/iptables/","tags":"","title":"iptables"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/etcd/","tags":"","title":"Etcd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/virtualkubelet/","tags":"","title":"VirtualKubelet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git/","tags":"","title":"Git"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/keepalived/","tags":"","title":"Keepalived"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/","tags":"","title":"快捷键"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/memcached/","tags":"","title":"Memcached"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tcpip/","tags":"","title":"TCPIP"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":"","title":"源码分析"},{"body":"1. kubelet简介 在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点资源。可以把kubelet理解成【Server-Agent】架构中的agent，是Node上的pod管家。\n更多kubelet配置参数信息可参考kubelet --help\n2. 节点管理 节点通过设置kubelet的启动参数“--register-node”，来决定是否向API Server注册自己，默认为true。可以通过kubelet --help或者查看kubernetes源码【cmd/kubelet/app/server.go中】来查看该参数。\nkubelet的配置文件\n默认配置文件在/etc/kubernetes/kubelet中，其中\n--api-servers：用来配置Master节点的IP和端口。 --kubeconfig：用来配置kubeconfig的路径，kubeconfig文件常用来指定证书。 --hostname-override：用来配置该节点在集群中显示的主机名。 --node-status-update-frequency：配置kubelet向Master心跳上报的频率，默认为10s。 3. Pod管理 kubelet有几种方式获取自身Node上所需要运行的Pod清单。但本文只讨论通过API Server监听etcd目录，同步Pod列表的方式。\nkubelet通过API Server Client使用WatchAndList的方式监听etcd中/registry/nodes/${当前节点名称}和/registry/pods的目录，将获取的信息同步到本地缓存中。\nkubelet监听etcd，执行对Pod的操作，对容器的操作则是通过Docker Client执行，例如启动删除容器等。\nkubelet创建和修改Pod流程：\n为该Pod创建一个数据目录。 从API Server读取该Pod清单。 为该Pod挂载外部卷（External Volume） 下载Pod用到的Secret。 检查运行的Pod，执行Pod中未完成的任务。 先创建一个Pause容器，该容器接管Pod的网络，再创建其他容器。 Pod中容器的处理流程： 1）比较容器hash值并做相应处理。 2）如果容器被终止了且没有指定重启策略，则不做任何处理。 3）调用Docker Client下载容器镜像，调用Docker Client运行容器。 4. 容器健康检查 Pod通过探针的方式来检查容器的健康状态，具体可参考Pod详解#Pod健康检查。\n5. cAdvisor资源监控 kubelet通过cAdvisor获取本节点信息及容器的数据。cAdvisor为谷歌开源的容器资源分析工具，默认集成到kubernetes中。\ncAdvisor自动采集CPU,内存，文件系统，网络使用情况，容器中运行的进程，默认端口为4194。可以通过Node IP+Port访问。\n更多参考：http://github.com/google/cadvisor\n参考《Kubernetes权威指南》\n","categories":"","description":"","excerpt":"1. kubelet简介 在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任 …","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-kubelet/","tags":["Kubernetes"],"title":"Kubernetes核心原理（四）之kubelet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysql/","tags":"","title":"Mysql"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pod/","tags":"","title":"Pod"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docker/","tags":"","title":"Docker"},{"body":"Summary 目录 序言 云原生体系 12-Factor K8S知识体系 安装与配置 部署k8s集群 使用kubeadm安装生产环境kubernetes 使用kubespray安装kubernetes 使用minikube安装kubernetes 使用kind安装kubernetes 安装k8s dashboard kubeadm升级k8s集群 kubeadm管理证书 k8s证书及秘钥 k8s版本说明 k8s版本记录 基本概念 kubernetes架构 Kubernetes总架构图 基于Docker及Kubernetes技术构建容器云（PaaS）平台概述 kubernetes对象 理解kubernetes对象 kubernetes常用对象说明 Pod Pod介绍 Pod定义文件 Pod生命周期 Pod健康检查 Pod存储卷 Pod控制器 Pod伸缩与升级 配置 ConfigMap Workload 核心原理 核心组件 Api Server Controller Manager Scheduler Kubelet 流程图 Pod创建流程 PVC创建流程 容器网络 Docker网络 K8S网络 Pod的DNS策略 网络插件 Flannel介绍 CNI CNI接口介绍 Macvlan介绍 容器存储 存储卷概念 Volume Persistent Volume Persistent Volume Claim Storage Class Dynamic Volume Provisioning CSI csi-cephfs-plugin 部署csi-cephfs 部署cephfs-provisioner FlexVolume介绍 资源隔离 资源配额 Pod限额 资源服务质量 Lxcfs资源视图隔离 运维指南 kubernetes集群问题排查 kubectl工具 kubectl安装与配置 kubectl命令说明 kubectl命令别名 kubectl进入node shell helm工具 helm的使用 节点迁移 安全迁移节点 指定Node调度与隔离 镜像仓库 配置私有的镜像仓库 拉取私有镜像 访问控制 使用RBAC鉴权 版本发布 金丝雀发布 开发指南 client-go的使用及源码分析 CSI插件开发 nfs-client-provisioner源码分析 csi-provisioner源码分析 operator开发 kubebuilder的使用 如何开发一个Operator k8s社区开发指南 问题排查 节点相关问题 keycreate permission denied Cgroup不支持pid资源 Cgroup子系统无法挂载 Pod驱逐 镜像拉取失败问题 PVC Terminating 源码分析 Kubernetes源码分析笔记 kubelet NewKubeletCommand NewMainKubelet startKubelet syncLoopIteration syncPod kube-controller-manager NewControllerManagerCommand DeploymentController Informer机制 kube-scheduler NewSchedulerCommand registerAlgorithmProvider scheduleOne findNodesThatFit PrioritizeNodes preempt kube-apiserver NewAPIServerCommand Runtime Runtime Runc和Containerd概述 Containerd 安装Containerd Docker Docker学习笔记 Kata Container kata容器简介 kata配置 GPU nvidia-device-plugin介绍 Etcd Etcd介绍 Raft算法 Etcd启动配置参数 Etcd访问控制 etcdctl命令工具 etcdctl命令工具-V3 etcdctl命令工具-V2 Etcd中的k8s数据 Etcd-Operator的使用 多集群管理 k8s多集群管理的思考 Virtual Kubelet Virtual Kubelet介绍 Virtual Kubelet 命令 Karmada Karmada介绍 边缘容器 KubeEdge介绍 KubeEdge源码分析 cloudcore edgecore OpenYurt OpenYurt部署 OpenYurt部署之调整k8s配置 OpenYurt源码分析 YurtHub源码分析（1) TunnelServer源码分析（1） Tunnel-Agent源码分析 虚拟化 虚拟化相关概念 KubeVirt KubeVirt的介绍 KubeVirt的使用 监控体系 监控体系介绍 kube-prometheus-stack的使用 cAdvisor介绍 Heapster介绍 Influxdb介绍 ","categories":"","description":"","excerpt":"Summary 目录 序言 云原生体系 12-Factor K8S知识体系 安装与配置 部署k8s集群 使用kubeadm安装生产环 …","ref":"/kubernetes-notes/summary/","tags":"","title":""},{"body":"Summary 序言 序言 计算 CPU\n内存\n存储 Linux 文件系统 Linux 介绍 文件系统 文件存储结构 文件权限 磁盘 LVM的使用 磁盘命令 Raid介绍 网络 TCP/IP TCP/IP基础 IP协议 TCP与UDP协议 Http Http基础 Http报文 Http状态码 网络命令 iptables介绍 iptables命令 tcpdump命令 wrk压测命令 VLAN介绍 程序 进程\nShell 脚本\nShell简介 Shell变量 Shell运算符 Shell数组 Shell echo命令 Shell判断语句 Shell循环语句 Shell函数 Shell重定向 运维工具 Ansible的使用 Supervisor的使用 Confd的使用 NFS的使用 ceph-fuse的使用 ssh tips Git管理 Git 介绍 Git 常用命令 Git 命令分类 Git commit规范 Git 命令别名 数据库 Mysql 系统管理 数据表操作 表内容操作 Redis Redis介绍 Redis集群模式部署 Redis主从及哨兵模式部署 Redis配置详解(中文版) Redis配置详解(英文版) Memcached Memcached的使用 Memcached命令 网络工具 Nginx Nginx安装与配置 Nginx作为反向代理 Nginx http服务器 配置Nginx免费证书 Keepalived Keepalived简介 Keepalived的安装与配置 Keepalived的相关操作 Keepalived的配置详解 Baremetal bmc概念 工具技巧 快捷键 vscode快捷键 eclipse快捷键 chrome快捷键 tmux快捷键 iterm2 rzsz的使用 IDE工具 vscode配置 Goland配置 vim vim命令 vimrc配置 basic vimrc ","categories":"","description":"","excerpt":"Summary 序言 序言 计算 CPU\n内存\n存储 Linux 文件系统 Linux 介绍 文件系统 文件存储结构 文件权限 磁盘 LVM …","ref":"/linux-notes/summary/","tags":"","title":""},{"body":" About 博客记录 2016年： 使用CSDN博客，网址：https://blog.csdn.net/huwh_ 2017年： 开始研究静态网站技术，采用hexo来搭建个人博客网站。 并且开始基于YuHsuan的博客主题优化了自己个人的博客主题hexo-theme-huweihuang，并将该主题应用到个人博客www.huweihuang.com网站上。 同时该博客主题也被多人用于自己的博客上。 2018年： 为了便于将笔记整理成知识体系，开始使用Gitbook来管理笔记，先后发布了kubernetes-notes，golang-notes，linux-notes等。 2022年： 由于Gitbook官方网站主题没有开源，使用Gitbook官方网站来增加文档美观性：k8s.huweihuang.com，但是定制性受限。 开始研究hugo静态网站技术，使用google开源的文档主题docsy，并发布新的博客地址：blog.huweihuang.com。 微信公众号 搜索：容器云架构\n","categories":"","description":"","excerpt":" About 博客记录 2016年： 使用CSDN博客，网址：https://blog.csdn.net/huwh_ 2017年： 开始研究 …","ref":"/about/","tags":"","title":"About"},{"body":"1. 安装 以centos为例。\nyum install -y ansible 2. 配置 默认配置目录在/etc/ansible/，主要有以下两个配置：\nansible.cfg：ansible的配置文件 hosts：配置ansible所连接的机器IP信息 2.1. ansible.cfg 2.2. hosts # This is the default ansible 'hosts' file. # # It should live in /etc/ansible/hosts # # - Comments begin with the '#' character # - Blank lines are ignored # - Groups of hosts are delimited by [header] elements # - You can enter hostnames or ip addresses # - A hostname/ip can be a member of multiple groups # Ex 1: Ungrouped hosts, specify before any group headers. # green.example.com # blue.example.com # 192.168.100.1 # 192.168.100.10 # Ex 2: A collection of hosts belonging to the 'webservers' group # [webservers] # alpha.example.org # beta.example.org # 192.168.1.100 # 192.168.1.110 # If you have multiple hosts following a pattern you can specify # them like this: # www[001:006].example.com # Ex 3: A collection of database servers in the 'dbservers' group # [dbservers] # # db01.intranet.mydomain.net # db02.intranet.mydomain.net # 10.25.1.56 # 10.25.1.57 # Here's another example of host ranges, this time there are no # leading 0s: # db-[99:101]-node.example.com [k8s] 192.168.201.52 192.168.201.53 192.168.201.54 192.168.201.55 192.168.201.56 192.168.201.57 # password setting [all:vars] ansible_connection=ssh ansible_ssh_user=root ansible_ssh_pass=xxx 3. ansible的命令 命令格式为：ansible [options]\nhost-pattern：即hosts文件中配置的集群名称 options：命令操作符 例如：ansible k8s -a 'uname -r'\n[root@k8s-master ansible]# ansible k8s -a 'uname -r' 172.16.201.56 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.55 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.54 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.53 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.52 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.57 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 具体的命令信息：\nUsage: ansible \u003chost-pattern\u003e [options] Define and run a single task 'playbook' against a set of hosts Options: -a MODULE_ARGS, --args=MODULE_ARGS module arguments --ask-vault-pass ask for vault password -B SECONDS, --background=SECONDS run asynchronously, failing after X seconds (default=N/A) -C, --check don't make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON, if filename prepend with @ -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY specify inventory host path or comma separated host list. --inventory-file is deprecated -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else -m MODULE_NAME, --module-name=MODULE_NAME module name to execute (default=command) -M MODULE_PATH, --module-path=MODULE_PATH prepend colon-separated path(s) to module library (default=[u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']) -o, --one-line condense output --playbook-dir=BASEDIR Since this tool does not use playbooks, use this as a subsitute playbook directory.This sets the relative path for many features including roles/ group_vars/ etc. -P POLL_INTERVAL, --poll=POLL_INTERVAL set the poll interval if using -B (default=15) --syntax-check perform a syntax check on the playbook, but do not execute it -t TREE, --tree=TREE log output to this directory --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=None) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password Some modules do not make sense in Ad-Hoc (include, meta, etc) 4. ansible-playbook Usage: ansible-playbook [options] playbook.yml [playbook2 ...] Runs Ansible playbooks, executing the defined tasks on the targeted hosts. Options: --ask-vault-pass ask for vault password -C, --check don't make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON, if filename prepend with @ --flush-cache clear the fact cache for every host in inventory --force-handlers run handlers even if a task fails -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY specify inventory host path or comma separated host list. --inventory-file is deprecated -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else --list-tags list all available tags --list-tasks list all tasks that would be executed -M MODULE_PATH, --module-path=MODULE_PATH prepend colon-separated path(s) to module library (default=[u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']) --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values --start-at-task=START_AT_TASK start the playbook at the task matching this name --step one-step-at-a-time: confirm each task before running --syntax-check perform a syntax check on the playbook, but do not execute it -t TAGS, --tags=TAGS only run plays and tasks tagged with these values --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=None) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password ","categories":"","description":"","excerpt":"1. 安装 以centos为例。\nyum install -y ansible 2. 配置 默认配置目录在/etc/ansible/，主要有 …","ref":"/linux-notes/tools/ansible-usage/","tags":["Linux"],"title":"ansible的使用"},{"body":" 以下转自https://github.com/amix/vimrc/blob/master/vimrcs/basic.vim\nbasic vimrc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Maintainer: \" Amir Salihefendic — @amix3k \" \" Awesome_version: \" Get this config, nice color schemes and lots of plugins! \" \" Install the awesome version from: \" \" https://github.com/amix/vimrc \" \" Sections: \" -\u003e General \" -\u003e VIM user interface \" -\u003e Colors and Fonts \" -\u003e Files and backups \" -\u003e Text, tab and indent related \" -\u003e Visual mode related \" -\u003e Moving around, tabs and buffers \" -\u003e Status line \" -\u003e Editing mappings \" -\u003e vimgrep searching and cope displaying \" -\u003e Spell checking \" -\u003e Misc \" -\u003e Helper functions \" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e General \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Sets how many lines of history VIM has to remember set history=500 \" Enable filetype plugins filetype plugin on filetype indent on \" Set to auto read when a file is changed from the outside set autoread \" With a map leader it's possible to do extra key combinations \" like \u003cleader\u003ew saves the current file let mapleader = \",\" \" Fast saving nmap \u003cleader\u003ew :w!\u003ccr\u003e \" :W sudo saves the file \" (useful for handling the permission-denied error) command W w !sudo tee % \u003e /dev/null \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e VIM user interface \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Set 7 lines to the cursor - when moving vertically using j/k set so=7 \" Avoid garbled characters in Chinese language windows OS let $LANG='en' set langmenu=en source $VIMRUNTIME/delmenu.vim source $VIMRUNTIME/menu.vim \" Turn on the Wild menu set wildmenu \" Ignore compiled files set wildignore=*.o,*~,*.pyc if has(\"win16\") || has(\"win32\") set wildignore+=.git\\*,.hg\\*,.svn\\* else set wildignore+=*/.git/*,*/.hg/*,*/.svn/*,*/.DS_Store endif \"Always show current position set ruler \" Height of the command bar set cmdheight=2 \" A buffer becomes hidden when it is abandoned set hid \" Configure backspace so it acts as it should act set backspace=eol,start,indent set whichwrap+=\u003c,\u003e,h,l \" Ignore case when searching set ignorecase \" When searching try to be smart about cases set smartcase \" Highlight search results set hlsearch \" Makes search act like search in modern browsers set incsearch \" Don't redraw while executing macros (good performance config) set lazyredraw \" For regular expressions turn magic on set magic \" Show matching brackets when text indicator is over them set showmatch \" How many tenths of a second to blink when matching brackets set mat=2 \" No annoying sound on errors set noerrorbells set novisualbell set t_vb= set tm=500 \" Properly disable sound on errors on MacVim if has(\"gui_macvim\") autocmd GUIEnter * set vb t_vb= endif \" Add a bit extra margin to the left set foldcolumn=1 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Colors and Fonts \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Enable syntax highlighting syntax enable \" Enable 256 colors palette in Gnome Terminal if $COLORTERM == 'gnome-terminal' set t_Co=256 endif try colorscheme desert catch endtry set background=dark \" Set extra options when running in GUI mode if has(\"gui_running\") set guioptions-=T set guioptions-=e set t_Co=256 set guitablabel=%M\\ %t endif \" Set utf8 as standard encoding and en_US as the standard language set encoding=utf8 \" Use Unix as the standard file type set ffs=unix,dos,mac \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Files, backups and undo \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Turn backup off, since most stuff is in SVN, git et.c anyway... set nobackup set nowb set noswapfile \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Text, tab and indent related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Use spaces instead of tabs set expandtab \" Be smart when using tabs ;) set smarttab \" 1 tab == 4 spaces set shiftwidth=4 set tabstop=4 \" Linebreak on 500 characters set lbr set tw=500 set ai \"Auto indent set si \"Smart indent set wrap \"Wrap lines \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Visual mode related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Visual mode pressing * or # searches for the current selection \" Super useful! From an idea by Michael Naumann vnoremap \u003csilent\u003e * :\u003cC-u\u003ecall VisualSelection('', '')\u003cCR\u003e/\u003cC-R\u003e=@/\u003cCR\u003e\u003cCR\u003e vnoremap \u003csilent\u003e # :\u003cC-u\u003ecall VisualSelection('', '')\u003cCR\u003e?\u003cC-R\u003e=@/\u003cCR\u003e\u003cCR\u003e \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Moving around, tabs, windows and buffers \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Map \u003cSpace\u003e to / (search) and Ctrl-\u003cSpace\u003e to ? (backwards search) map \u003cspace\u003e / map \u003cc-space\u003e ? \" Disable highlight when \u003cleader\u003e\u003ccr\u003e is pressed map \u003csilent\u003e \u003cleader\u003e\u003ccr\u003e :noh\u003ccr\u003e \" Smart way to move between windows map \u003cC-j\u003e \u003cC-W\u003ej map \u003cC-k\u003e \u003cC-W\u003ek map \u003cC-h\u003e \u003cC-W\u003eh map \u003cC-l\u003e \u003cC-W\u003el \" Close the current buffer map \u003cleader\u003ebd :Bclose\u003ccr\u003e:tabclose\u003ccr\u003egT \" Close all the buffers map \u003cleader\u003eba :bufdo bd\u003ccr\u003e map \u003cleader\u003el :bnext\u003ccr\u003e map \u003cleader\u003eh :bprevious\u003ccr\u003e \" Useful mappings for managing tabs map \u003cleader\u003etn :tabnew\u003ccr\u003e map \u003cleader\u003eto :tabonly\u003ccr\u003e map \u003cleader\u003etc :tabclose\u003ccr\u003e map \u003cleader\u003etm :tabmove map \u003cleader\u003et\u003cleader\u003e :tabnext \" Let 'tl' toggle between this and the last accessed tab let g:lasttab = 1 nmap \u003cLeader\u003etl :exe \"tabn \".g:lasttab\u003cCR\u003e au TabLeave * let g:lasttab = tabpagenr() \" Opens a new tab with the current buffer's path \" Super useful when editing files in the same directory map \u003cleader\u003ete :tabedit \u003cc-r\u003e=expand(\"%:p:h\")\u003ccr\u003e/ \" Switch CWD to the directory of the open buffer map \u003cleader\u003ecd :cd %:p:h\u003ccr\u003e:pwd\u003ccr\u003e \" Specify the behavior when switching between buffers try set switchbuf=useopen,usetab,newtab set stal=2 catch endtry \" Return to last edit position when opening files (You want this!) au BufReadPost * if line(\"'\\\"\") \u003e 1 \u0026\u0026 line(\"'\\\"\") \u003c= line(\"$\") | exe \"normal! g'\\\"\" | endif \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Status line \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Always show the status line set laststatus=2 \" Format the status line set statusline=\\ %{HasPaste()}%F%m%r%h\\ %w\\ \\ CWD:\\ %r%{getcwd()}%h\\ \\ \\ Line:\\ %l\\ \\ Column:\\ %c \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Editing mappings \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Remap VIM 0 to first non-blank character map 0 ^ \" Move a line of text using ALT+[jk] or Command+[jk] on mac nmap \u003cM-j\u003e mz:m+\u003ccr\u003e`z nmap \u003cM-k\u003e mz:m-2\u003ccr\u003e`z vmap \u003cM-j\u003e :m'\u003e+\u003ccr\u003e`\u003cmy`\u003emzgv`yo`z vmap \u003cM-k\u003e :m'\u003c-2\u003ccr\u003e`\u003emy`\u003cmzgv`yo`z if has(\"mac\") || has(\"macunix\") nmap \u003cD-j\u003e \u003cM-j\u003e nmap \u003cD-k\u003e \u003cM-k\u003e vmap \u003cD-j\u003e \u003cM-j\u003e vmap \u003cD-k\u003e \u003cM-k\u003e endif \" Delete trailing white space on save, useful for some filetypes ;) fun! CleanExtraSpaces() let save_cursor = getpos(\".\") let old_query = getreg('/') silent! %s/\\s\\+$//e call setpos('.', save_cursor) call setreg('/', old_query) endfun if has(\"autocmd\") autocmd BufWritePre *.txt,*.js,*.py,*.wiki,*.sh,*.coffee :call CleanExtraSpaces() endif \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Spell checking \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Pressing ,ss will toggle and untoggle spell checking map \u003cleader\u003ess :setlocal spell!\u003ccr\u003e \" Shortcuts using \u003cleader\u003e map \u003cleader\u003esn ]s map \u003cleader\u003esp [s map \u003cleader\u003esa zg map \u003cleader\u003es? z= \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Misc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Remove the Windows ^M - when the encodings gets messed up noremap \u003cLeader\u003em mmHmt:%s/\u003cC-V\u003e\u003ccr\u003e//ge\u003ccr\u003e'tzt'm \" Quickly open a buffer for scribble map \u003cleader\u003eq :e ~/buffer\u003ccr\u003e \" Quickly open a markdown buffer for scribble map \u003cleader\u003ex :e ~/buffer.md\u003ccr\u003e \" Toggle paste mode on and off map \u003cleader\u003epp :setlocal paste!\u003ccr\u003e \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Helper functions \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Returns true if paste mode is enabled function! HasPaste() if \u0026paste return 'PASTE MODE ' endif return '' endfunction \" Don't close window, when deleting a buffer command! Bclose call \u003cSID\u003eBufcloseCloseIt() function! \u003cSID\u003eBufcloseCloseIt() let l:currentBufNum = bufnr(\"%\") let l:alternateBufNum = bufnr(\"#\") if buflisted(l:alternateBufNum) buffer # else bnext endif if bufnr(\"%\") == l:currentBufNum new endif if buflisted(l:currentBufNum) execute(\"bdelete! \".l:currentBufNum) endif endfunction function! CmdLine(str) call feedkeys(\":\" . a:str) endfunction function! VisualSelection(direction, extra_filter) range let l:saved_reg = @\" execute \"normal! vgvy\" let l:pattern = escape(@\", \"\\\\/.*'$^~[]\") let l:pattern = substitute(l:pattern, \"\\n$\", \"\", \"\") if a:direction == 'gv' call CmdLine(\"Ack '\" . l:pattern . \"' \" ) elseif a:direction == 'replace' call CmdLine(\"%s\" . '/'. l:pattern . '/') endif let @/ = l:pattern let @\" = l:saved_reg endfunction 参考\nhttps://github.com/amix/vimrc ","categories":"","description":"","excerpt":" 以下转自https://github.com/amix/vimrc/blob/master/vimrcs/basic.vim\nbasic …","ref":"/linux-notes/ide/vim/basic-vimrc/","tags":["VIM"],"title":"basic vimrc"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"1. 安装ceph-fuse yum install -y ceph-fuse 如果安装失败，先执行以下命令，再执行上述安装命令\nyum -y install epel-release rpm -Uhv http://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm 2. 配置客户端访问的key mkdir /etc/ceph/ vi /etc/ceph/ceph.client.admin.keyring\n[client.admin] key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx== 3. ceph-fuse 挂载 ceph-fuse -m \u003cmons_IP1\u003e:6789,\u003cmons_IP2\u003e:6789,\u003cmons_IP3\u003e:6789 -r \u003cceph集群中的目录\u003e \u003c宿主机目录\u003e -o nonempty 例如：\n# ceph-fuse -m 192.168.18.3:6789,192.168.18.4:6789,192.168.18.5:6789 -r /pvc-volumes /root/cephfsdir -o nonempty 2019-03-27 17:58:04.435985 7fc61b67cec0 -1 did not load config file, using default settings. ceph-fuse[18051]: starting ceph client 2019-03-27 17:58:04.469144 7fc61b67cec0 -1 init, newargv = 0x55cecaba81c0 newargc=13 ceph-fuse[18051]: starting fuse 4. 查看是否挂载成功 # df -h Filesystem Size Used Avail Use% Mounted on ... ceph-fuse 1.6T 8.8G 1.6T 1% /root/cephfsdir 5. ceph-fuse命令说明 # ceph-fuse --help 2019-03-27 18:01:16.421376 7fae11998ec0 -1 did not load config file, using default settings. usage: ceph-fuse [-m mon-ip-addr:mon-port] \u003cmount point\u003e [OPTIONS] --client_mountpoint/-r \u003croot_directory\u003e use root_directory as the mounted root, rather than the full Ceph tree. usage: ceph-fuse mountpoint [options] general options: -o opt,[opt...] mount options -h --help print help -V --version print version FUSE options: -d -o debug enable debug output (implies -f) -f foreground operation -s disable multi-threaded operation --conf/-c FILE read configuration from the given configuration file --id/-i ID set ID portion of my name --name/-n TYPE.ID set name --cluster NAME set cluster name (default: ceph) --setuser USER set uid to user or uid (and gid to user's gid) --setgroup GROUP set gid to group or gid --version show version and quit ","categories":"","description":"","excerpt":"1. 安装ceph-fuse yum install -y ceph-fuse 如果安装失败，先执行以下命令，再执行上述安装命令\nyum …","ref":"/linux-notes/tools/ceph-fuse/","tags":["Linux"],"title":"ceph-fuse的使用"},{"body":" confd的源码参考：https://github.com/kelseyhightower/confd\n1. confd的部署 以下Linux系统为例。\n下载confd的二进制文件，下载地址为：https://github.com/kelseyhightower/confd/releases。例如：\n# Download the binary wget https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64 # 重命名二进制文件，并移动到PATH的目录下 mv confd-0.16.0-linux-amd64 /usr/local/bin/confd chmod +x /usr/local/bin/confd # 验证是否安装成功 confd --help 2. confd的配置 Confd通过读取后端存储的配置信息来动态更新对应的配置文件，对应的后端存储可以是etcd，redis等，其中etcd的v3版本对应的存储后端为etcdv3。\n2.1. confd.toml confd.toml为confd服务本身的配置文件，主要记录了使用的存储后端、协议、confdir等参数。\n示例：\n存储后端etcdv3： backend = \"etcdv3\" confdir = \"/etc/confd\" log-level = \"debug\" interval = 5 nodes = [ \"http://192.168.10.4:12379\", ] scheme = \"http\" watch = true 其中watch参数表示实时监听后端存储的变化，如有变化则更新confd管理的配置。\n存储后端为redis backend = \"redis\" confdir = \"/etc/confd\" log-level = \"debug\" interval = 1 # 间隔 1 秒同步一次配置文件 nodes = [ \"127.0.0.1:6379\", ] scheme = \"http\" client_key = \"123456\" # redis的密码，不是 password 参数 #watch = true 如果没有启动watch参数，则会依据interval参数定期去redis存储后端拿取数据，并比较与当前配置数据是否有变化（主要比较md5值），如果有变化则更新配置，没有变化则定期再去拿取数据，以此循环。\n如果启动了watch参数，则修改redis存储数据的同时，还要执行publish的操作，促使confd去触发比较配置并更新配置的操作。\npublish的命令格式如下:\npublish __keyspace@0__:{prefix}/{key} set(or del) 2.2. 创建confdir confdir底下包含两个目录:\nconf.d:confd的配置文件，主要包含配置的生成逻辑，例如模板源，后端存储对应的keys，命令执行等。 templates:配置模板Template，即基于不同组件的配置，修改为符合 Golang text templates的模板文件。 sudo mkdir -p /etc/confd/{conf.d,templates} 2.2.1. Template Resources 模板源配置文件是TOML格式的文件，主要包含配置的生成逻辑，例如模板源，后端存储对应的keys，命令执行等。默认目录在/etc/confd/conf.d。\n参数说明：\n必要参数\ndest (string) - The target file. keys (array of strings) - An array of keys. src (string) - The relative path of a configuration template. 可选参数\ngid (int) - The gid that should own the file. Defaults to the effective gid. mode (string) - The permission mode of the file. uid (int) - The uid that should own the file. Defaults to the effective uid. reload_cmd (string) - The command to reload config. check_cmd (string) - The command to check config. Use {{src}} to reference the rendered source template. prefix (string) - The string to prefix to keys. 例子\n例如：/etc/confd/conf.d/myapp-nginx.toml\n[template] prefix = \"/myapp\" src = \"nginx.tmpl\" dest = \"/tmp/myapp.conf\" owner = \"nginx\" mode = \"0644\" keys = [ \"/services/web\" ] check_cmd = \"/usr/sbin/nginx -t -c {{.src}}\" reload_cmd = \"/usr/sbin/service nginx reload\" 2.2.2. Template Template定义了单一应用配置的模板，默认存储在/etc/confd/templates目录下，模板文件符合Go的text/template格式。\n模板文件常用函数有base，get，gets，lsdir，json等。具体可参考https://github.com/kelseyhightower/confd/blob/master/docs/templates.md。\n例子：\n/etc/confd/templates/nginx.tmpl\n{{range $dir := lsdir \"/services/web\"}} upstream {{base $dir}} { {{$custdir := printf \"/services/web/%s/*\" $dir}}{{range gets $custdir}} server {{$data := json .Value}}{{$data.IP}}:80; {{end}} } server { server_name {{base $dir}}.example.com; location / { proxy_pass {{base $dir}}; } } {{end}} 3. 创建后端存储的配置数据 以etcdv3存储为例，在etcd中创建以下数据。\netcdctl --endpoints=$endpoints put /services/web/cust1/2 '{\"IP\": \"10.0.0.2\"}' etcdctl --endpoints=$endpoints put /services/web/cust2/2 '{\"IP\": \"10.0.0.4\"}' etcdctl --endpoints=$endpoints put /services/web/cust2/1 '{\"IP\": \"10.0.0.3\"}' etcdctl --endpoints=$endpoints put /services/web/cust1/1 '{\"IP\": \"10.0.0.1\"}' 4. 启动confd的服务 confd支持以daemon或者onetime两种模式运行，当以daemon模式运行时，confd会监听后端存储的配置变化，并根据配置模板动态生成目标配置文件。\nconfd可以使用-config-file参数来指定confd的配置文件，而将其他参数写在配置文件中。\n/usr/local/bin/confd -config-file /etc/confd/conf/confd.toml 如果以daemon模式运行，在命令后面添加\u0026符号，例如：\nconfd -watch -backend etcdv3 -node http://172.16.5.4:12379 \u0026 以下以onetime模式运行为例。其中对应的后端存储类型是etcdv3。\n# 执行命令 confd -onetime -backend etcdv3 -node http://172.16.5.4:12379 # output 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Backend set to etcdv3 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Starting confd 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Backend source(s) set to http://172.16.5.4:12379 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO /root/myapp/twemproxy/conf/twemproxy.conf has md5sum 6f0f43abede612c75cb840a4840fbea3 should be 32f48664266e3fd6b56ee73a314ee272 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Target config /root/myapp/twemproxy/conf/twemproxy.conf out of sync 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Target config /root/myapp/twemproxy/conf/twemproxy.conf has been updated 5. 查看生成的配置文件 在/etc/confd/conf.d/myapp-nginx.toml中定义的配置文件的生成路径为/tmp/myapp.conf。\n[root@k8s-dbg-master-1 dest]# cat myapp.conf upstream cust1 { server 10.0.0.1:80; server 10.0.0.2:80; } server { server_name cust1.example.com; location / { proxy_pass cust1; } } upstream cust2 { server 10.0.0.3:80; server 10.0.0.4:80; } server { server_name cust2.example.com; location / { proxy_pass cust2; } } 6. confd动态更新twemproxy 6.1. twemproxy.toml confd的模板源文件配置：/etc/confd/conf.d/twemproxy.toml\n[template] src = \"twemproxy.tmpl\" dest = \"/root/myapp/twemproxy/conf/twemproxy.conf\" keys = [ \"/twemproxy/pool\" ] check_cmd = \"/usr/local/bin/nutcracker -t -c /root/myapp/twemproxy/conf/twemproxy.conf\" reload_cmd = \"bash /root/myapp/twemproxy/reload.sh\" 6.2. twemproxy.tmpl 模板文件：/etc/confd/templates/twemproxy.tmpl\nglobal: worker_processes: 4 # 并发进程数, 如果为0, 这 fallback 回原来的单进程模型(不支持 config reload!) user: nobody # worker 进程的用户, 默认 nobody. 只要主进程是 root 用户启动才生效. group: nobody # worker 进程的用户组 worker_shutdown_timeout: 30 # 单位为秒. 用于 reload 过程中在改时间段之后强制退出旧的 worker 进程. pools: {{range gets \"/twemproxy/pool/*\"}} {{base .Key}}: {{$pool := json .Value}} listen: {{$pool.ListenAddr.IP}}:{{$pool.ListenAddr.Port}} hash: fnv1a_64 # 选择实例的 hash 规则 distribution: ketama auto_eject_hosts: true # server 有问题是否剔除 redis: true # 是否为 Redis 协议 {{if $pool.Password}}redis_auth: {{$pool.Password}}{{end}} server_retry_timeout: 5000 # 被剔除多长时间后会重试 server_connections: 25 # NOTE: server 连接池的大小, 默认为 1, 建议调整 server_failure_limit: 5 # 失败多少次后暂时剔除 timeout: 1000 # Server 超时时间, 1 sec backlog: 1024 # 连接队列大小 preconnect: true # 预连接大小 servers:{{range $server := $pool.Servers}} - {{$server.IP}}:{{$server.Port}}:1 {{if $server.Master}}master{{end}} {{end}} {{end}} 6.3. etcd中的配置格式 etcd中的配置通过一个map来定义为完整的配置内容。其中key是twemproxy中pool的名称，value是pool的所有内容。\n配置对应go结构体如下：\ntype Pool struct{ ListenAddr ListenAddr `json:\"ListenAddr,omitempty\"` Servers []Server `json:\"Servers,omitempty\"` Password string `json:\"Password,omitempty\"` } type ListenAddr struct { IP string `json:\"IP,omitempty\"` Port string `json:\"Port,omitempty\"` } type Server struct { IP string `json:\"IP,omitempty\"` Port string `json:\"Port,omitempty\"` Master bool `json:\"Master,omitempty\"` } 配置对应JSON格式如下：\n{ \"ListenAddr\": { \"IP\": \"192.168.5.7\", \"Port\": \"22225\" }, \"Servers\": [ { \"IP\": \"10.233.116.168\", \"Port\": \"6379\", \"Master\": true }, { \"IP\": \"10.233.110.207\", \"Port\": \"6379\", \"Master\": false } ], \"Password\": \"987654\" } 6.4. 生成twemproxy配置文件 global: worker_processes: 4 # 并发进程数, 如果为0, 这 fallback 回原来的单进程模型(不支持 config reload!) user: nobody # worker 进程的用户, 默认 nobody. 只要主进程是 root 用户启动才生效. group: nobody # worker 进程的用户组 worker_shutdown_timeout: 30 # 单位为秒. 用于 reload 过程中在改时间段之后强制退出旧的 worker 进程. pools: redis1: listen: 192.168.5.7:22223 hash: fnv1a_64 # 选择实例的 hash 规则 distribution: ketama auto_eject_hosts: true # server 有问题是否剔除 redis: true # 是否为 Redis 协议 redis_auth: 987654 server_retry_timeout: 5000 # 被剔除多长时间后会重试 server_connections: 25 # NOTE: server 连接池的大小, 默认为 1, 建议调整 server_failure_limit: 5 # 失败多少次后暂时剔除 timeout: 1000 # Server 超时时间, 1 sec backlog: 1024 # 连接队列大小 preconnect: true # 预连接大小 servers: - 10.233.116.169:6379:1 redis2: listen: 192.168.5.7:22224 hash: fnv1a_64 # 选择实例的 hash 规则 distribution: ketama auto_eject_hosts: true # server 有问题是否剔除 redis: true # 是否为 Redis 协议 redis_auth: 987654 server_retry_timeout: 5000 # 被剔除多长时间后会重试 server_connections: 25 # NOTE: server 连接池的大小, 默认为 1, 建议调整 server_failure_limit: 5 # 失败多少次后暂时剔除 timeout: 1000 # Server 超时时间, 1 sec backlog: 1024 # 连接队列大小 preconnect: true # 预连接大小 servers: - 10.233.110.223:6379:1 master - 10.233.111.21:6379:1 7. confd的命令 $ confd --help Usage of confd: -app-id string Vault app-id to use with the app-id backend (only used with -backend=vault and auth-type=app-id) -auth-token string Auth bearer token to use -auth-type string Vault auth backend type to use (only used with -backend=vault) -backend string backend to use (default \"etcd\") -basic-auth Use Basic Auth to authenticate (only used with -backend=consul and -backend=etcd) -client-ca-keys string client ca keys -client-cert string the client cert -client-key string the client key -confdir string confd conf directory (default \"/etc/confd\") -config-file string the confd config file (default \"/etc/confd/confd.toml\") -file value the YAML file to watch for changes (only used with -backend=file) -filter string files filter (only used with -backend=file) (default \"*\") -interval int backend polling interval (default 600) -keep-stage-file keep staged files -log-level string level which confd should log messages -node value list of backend nodes -noop only show pending changes -onetime run once and exit -password string the password to authenticate with (only used with vault and etcd backends) -path string Vault mount path of the auth method (only used with -backend=vault) -prefix string key path prefix -role-id string Vault role-id to use with the AppRole, Kubernetes backends (only used with -backend=vault and either auth-type=app-role or auth-type=kubernetes) -scheme string the backend URI scheme for nodes retrieved from DNS SRV records (http or https) (default \"http\") -secret-id string Vault secret-id to use with the AppRole backend (only used with -backend=vault and auth-type=app-role) -secret-keyring string path to armored PGP secret keyring (for use with crypt functions) -separator string the separator to replace '/' with when looking up keys in the backend, prefixed '/' will also be removed (only used with -backend=redis) -srv-domain string the name of the resource record -srv-record string the SRV record to search for backends nodes. Example: _etcd-client._tcp.example.com -sync-only sync without check_cmd and reload_cmd -table string the name of the DynamoDB table (only used with -backend=dynamodb) -user-id string Vault user-id to use with the app-id backend (only used with -backend=value and auth-type=app-id) -username string the username to authenticate as (only used with vault and etcd backends) -version print version and exit -watch enable watch support 参考文章：\nhttps://github.com/kelseyhightower/confd/blob/master/docs/installation.md\nhttps://github.com/kelseyhightower/confd/blob/master/docs/quick-start-guide.md\nhttps://github.com/kelseyhightower/confd/blob/master/docs/template-resources.md\nhttps://github.com/kelseyhightower/confd/blob/master/docs/templates.md\n","categories":"","description":"","excerpt":" confd的源码参考：https://github.com/kelseyhightower/confd\n1. confd的部署 以 …","ref":"/linux-notes/tools/confd-usage/","tags":["Linux"],"title":"confd的使用"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/containerd/","tags":"","title":"Containerd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/csi/","tags":"","title":"CSI"},{"body":" 胡伟煌个人博客 Learn More Download Welcome to huweihuang's personal blog !\n","categories":"","description":"","excerpt":" 胡伟煌个人博客 Learn More Download Welcome to huweihuang's personal blog !\n","ref":"/","tags":"","title":"Goldydocs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ide/","tags":"","title":"IDE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubeadm/","tags":"","title":"kubeadm"},{"body":"1. NFS简介 NFS，是Network File System的简写，即网络文件系统。网络文件系统是FreeBSD支持的文件系统中的一种，也被称为NFS. NFS允许一个系统在网络上与他人共享目录和文件。 通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。\n2. NFS的安装与配置 2.1 服务端 NFS需要安装nfs-utils、rpcbind两个包。\n#可以先检查下本地是否已经安装，如果安装则无需重复安装包 [root@k8s-dbg-master-1 build]# rpm -qa|grep rpcbind rpcbind-0.2.0-42.el7.x86_64 [root@k8s-dbg-master-1 build]# rpm -qa|grep nfs libnfsidmap-0.25-17.el7.x86_64 nfs-utils-1.3.0-0.48.el7_4.x86_64 2.1.1. 安装nfs-utils、rpcbind两个包 #centos系统 yum -y install nfs-utils rpcbind #Ubuntu系统 #服务端 apt-get install nfs-kernel-server #客户端 apt-get install nfs-common 2.1.2. 创建共享目录 服务端共享目录：/data/nfs-storage/\nmkdir /data/nfs-storage/ 2.1.3. NFS共享目录文件配置 vi /etc/exports #添加以下信息 /data/nfs-storage *(rw,insecure,sync,no_subtree_check,no_root_squash) 以上配置分为三个部分：\n第一部分就是本地要共享出去的目录。 第二部分为允许访问的主机（可以是一个IP也可以是一个IP段），*代表允许所有的网段访问。 第三部分小括号里面的，为一些权限选项。 权限说明\nrw ：读写； ro ：只读； sync ：同步模式，内存中数据时时写入磁盘； async ：不同步，把内存中数据定期写入磁盘中； secure ：nfs通过1024以下的安全TCP/IP端口发送 insecure ：nfs通过1024以上的端口发送 no_root_squash ：加上这个选项后，root用户就会对共享的目录拥有至高的权限控制，就像是对本机的目录操作一样。不安全，不建议使用； root_squash ：和上面的选项对应，root用户对共享目录的权限不高，只有普通用户的权限，即限制了root； subtree_check ：如果共享/usr/bin之类的子目录时，强制nfs检查父目录的权限（默认） no_subtree_check ：和上面相对，不检查父目录权限 all_squash ：不管使用NFS的用户是谁，他的身份都会被限定成为一个指定的普通用户身份； anonuid/anongid ：要和root_squash 以及 all_squash一同使用，用于指定使用NFS的用户限定后的uid和gid，前提是本机的/etc/passwd中存在这个uid和gid。 2.1.4. 启动NFS服务 #先启动rpcbind service rpcbind start #后启动nfs service nfs start #可以设置开机启动 chkconfig rpcbind on chkconfig nfs on 2.1.5. 服务端验证 通过showmount -e命令如果正常显示共享目录，表示安装正常。\n[root@k8s-dbg-master-1 build]# showmount -e Export list for k8s-dbg-master-1: /data/nfs-storage * 2.2 客户端 2.2.1. 安装nfs-utils的包 yum install nfs-utils.x86_64 -y 2.2.2. 创建挂载点 客户端挂载目录：/mnt/store\nmkdir /mnt/store 2.2.3. 查看NFS服务器的共享 root@k8s-dbg-node-5:~# showmount -e 172.16.5.4 Export list for 172.16.5.4: /data/nfs-storage * 2.2.4. 挂载 mount -t nfs \u003cNFS_SERVER_IP\u003e:\u003cNFS_SERVER_SHARED_DIR\u003e \u003cNFS_CLIENT_MOUNT_DIR\u003e #例如： mount -t nfs 172.16.5.4:/data/nfs-storage /mnt/store 2.2.5. 验证挂载信息 使用mount命令\nroot@k8s-dbg-node-5:~# mount |grep /mnt/store 172.16.5.4:/data/nfs-storage/k8s-storage/ssd on /mnt/store type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.16.200.24,local_lock=none,addr=172.16.5.4) 使用df -h命令\nroot@k8s-dbg-node-5:~# df -h|grep nfs 172.16.5.4:/data/nfs-storage 40G 25G 13G 67% /mnt/store 创建文件测试\n#进入客户端的挂载目录，创建文件 cd /mnt/store touch test.txt #进入服务端的共享目录，查看客户端创建的文件是否同步 cd /data/nfs-storage ls ","categories":"","description":"","excerpt":"1. NFS简介 NFS，是Network File System的简写，即网络文件系统。网络文件系统是FreeBSD支持的文件系统中的一 …","ref":"/linux-notes/tools/nfs-usage/","tags":["Linux"],"title":"NFS的使用"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/shell/","tags":"","title":"Shell"},{"body":"1. ssh/scp免密码 A服务器地址：10.8.216.25，下面简称A\nB服务器地址：10.8.216.26，下面简称B\n实现A登录B免密码。\n1.1. 在A生成密钥对 无密码方式：\nssh-keygen -t rsa -P 自定义密码参数：\nssh-keygen -C \u003ccomment\u003e -f \u003ckeyfile\u003e -t rsa -P \"\u003cpassphrase\u003e\" 执行上述命令，一路回车，会在当前登录用户的home目录下的.ssh目录下生成id_rsa和id_rsa.pub两个文件，分别代表密钥对的私钥和公钥，如下图所示：\n1.2. 拷贝A的公钥（id_rsa.pub）到B 这里拷贝到B的root用户home目录下为例：\nscp /root/.ssh/id_rsa.pub root@10.8.216.26:/root 1.3. 登录B 拷贝A的id_rsa.pub内容到.ssh目录下的authorized_keys文件中\ncd /root cat id_rsa.pub \u003e\u003e .ssh/authorized_keys 如图：\n1.4. 登录或拷贝 此时在A中用SSH登录B或向B拷贝文件，将不需要密码\nssh root@10.8.216.26 scp abc.txt root@10.8.216.26:/root 2. 配置跳板机快速登录 2.1. 配置ssh config文件 ssh config 路径：~/.ssh/config\nAddKeysToAgent yes ServerAliveInterval 3 Host jump HostName {jump_ip} Port {port} User {username} forwardagent yes identityfile ~/.ssh/id_rsa Host *.gw user {username} port {port} proxycommand ssh -W $(echo %h | sed -e \"s/.gw$//\"):%p jump Host bj* User {username} Port {port} proxycommand ssh -W 192.168.123.$(echo %h | awk -F 'bj' '{print $2}'):%p jump 多层跳板机\nHost jump1 Hostname {jump1_ip} Port {port} User {username} forwardagent yes identityfile ~/.ssh/id_rsa Host jump2 Hostname {jump2_ip} Port {port} User {username} ProxyCommand ssh -q -x -W %h:%p jump1 Host * Hostname %h Port {port} User {username} ProxyCommand ssh -q -x -W %h:%p jump2 2.2. 记录机器文件 将关键字和IP写入文件记录，例如 ~/.my_hosts。\n示例：可以是IP + 环境等关键字，中间用空格隔开。\n# release 192.168.123.11 rel-node-11 192.168.123.12 rel-node-12 # pre 192.168.321.13 pre-node-13 192.168.321.14 pre-node-14 192.168.321.15 pre-node-15 # dev 192.168.111.16 dev-node-16 192.168.111.17 dev-node-17 2.3. 安装fzf # for mac brew install fzf 2.4. 设置命令别名 设置 alias 到shell rc 文件(.bashrc / .zshrc)\nalias goto=\"ssh \\$(cat ~/.my_hosts | fzf | awk '{ printf(\\\"%s.gw\\\", \\$1)}')\" 2.5. 使用 使用别名命令，输入关键字搜索，点击回车进入指定机器。\n也可以使用ssh命令登录机器别名。\nssh bj11 3. ssh配置项说明 可以通过man查看ssh配置说明\nman ssh_config 配置文件示例：\nHost jump port 22 Host * !jump StrictHostKeyChecking no HostName %h UserKnownHostsFile /dev/null LogLevel ERROR IdentityFile ~/.ssh/id_rsa ProxyCommand ssh -p 22 -F /dev/null jump -W %h:%p SendEnv LANG LC_* 配置项说明：\nHost: 标识设备，*表示通配所有字符，!表示例外通配。\nStrictHostKeyChecking no：连接时不进行公钥交互确认操作。\nUserKnownHostsFile /dev/null：不提示确认known_hosts文件。\nProxyCommand：代理命令\n如果使用命令加参数的方式：\nssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"ssh -p 22 jump -W %h:%p\" ","categories":"","description":"","excerpt":"1. ssh/scp免密码 A服务器地址：10.8.216.25，下面简称A\nB服务器地址：10.8.216.26，下面简称B\n实现A登录B …","ref":"/linux-notes/tools/ssh-tips/","tags":["Linux"],"title":"ssh tips"},{"body":"1. Supervisor简介 Supervisord 是用 Python 实现的一款的进程管理工具，supervisord 要求管理的程序是非 daemon 程序，supervisord 会帮你把它转成 daemon 程序，因此如果用 supervisord 来管理进程，进程需要以非daemon的方式启动。\n例如：管理nginx 的话，必须在 nginx 的配置文件里添加一行设置 daemon off 让 nginx 以非 daemon 方式启动。\n2. Supervisor安装 以centos系统为例，以下两种方式选择其一。\n# yum install 的方式 yum install -y supervisor # easy_install的方式 yum install -y python-setuptools easy_install supervisor echo_supervisord_conf \u003e/etc/supervisord.conf 3. Supervisor的配置 3.1. supervisord.conf的配置 如果使用yum install -y supervisor的命令安装，会生成默认配置/etc/supervisord.conf和目录/etc/supervisord.d，如果没有则自行创建。\n在/etc/supervisord.d的目录下创建conf和log两个目录，conf用于存放管理进程的配置，log用于存放管理进程的日志。\ncd /etc/supervisord.d mkdir conf log 修改/etc/supervisord.conf的[include]部分，即载入/etc/supervisord.d/conf目录下的所有配置。\nvi /etc/supervisord.conf ... [include] files = supervisord.d/conf/*.conf ... 也可以修改supervisor应用日志的目录，默认日志路径为/var/log/supervisor/supervisord.log。\nvi /etc/supervisord.conf ... [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ... 3.2. 管理应用的配置 进入到/etc/supervisord.d/conf目录，创建管理应用的配置，可以创建多个应用配置。\n例如，创建confd.conf配置。\n[program:confd] directory = /usr/local/bin ; 程序的启动目录 command = /usr/local/bin/confd -config-file /etc/confd/confd.toml ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = root ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 20 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /etc/supervisord.d/log/confd.log ;日志统一放在log目录下 ; 可以通过 environment 来添加需要的环境变量，一种常见的用法是修改 PYTHONPATH ; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere 4. Surpervisor的启动 # supervisord二进制启动 supervisord -c /etc/supervisord.conf # 检查进程 ps aux | grep supervisord 或者以systemd的方式管理\nvi /etc/rc.d/init.d/supervisord\n#!/bin/sh # # /etc/rc.d/init.d/supervisord # # Supervisor is a client/server system that # allows its users to monitor and control a # number of processes on UNIX-like operating # systems. # # chkconfig: - 64 36 # description: Supervisor Server # processname: supervisord # Source init functions . /etc/rc.d/init.d/functions prog=\"supervisord\" prefix=\"/usr\" exec_prefix=\"${prefix}\" prog_bin=\"${exec_prefix}/bin/supervisord\" PIDFILE=\"/var/run/$prog.pid\" start() { echo -n $\"Starting $prog: \" daemon $prog_bin --pidfile $PIDFILE -c /etc/supervisord.conf [ -f $PIDFILE ] \u0026\u0026 success $\"$prog startup\" || failure $\"$prog startup\" echo } stop() { echo -n $\"Shutting down $prog: \" [ -f $PIDFILE ] \u0026\u0026 killproc $prog || success $\"$prog shutdown\" echo } case \"$1\" in start) start ;; stop) stop ;; status) status $prog ;; restart) stop start ;; *) echo \"Usage: $0 {start|stop|restart|status}\" ;; esac 设置开机启动及systemd方式启动。\nsudo chmod +x /etc/rc.d/init.d/supervisord sudo chkconfig --add supervisord sudo chkconfig supervisord on sudo service supervisord start 5. supervisorctl\u0026supervisord Supervisord 安装完成后有两个可用的命令行 supervisord 和 supervisorctl，命令使用解释如下：\n5.1. supervisorctl supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。 supervisorctl start programxxx，启动某个进程。 supervisorctl restart programxxx，重启某个进程。 supervisorctl status，查看进程状态。 supervisorctl stop groupworker ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)。 supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。 supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。 supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。 更多参考：\n$ supervisorctl --help supervisorctl -- control applications run by supervisord from the cmd line. Usage: /usr/bin/supervisorctl [options] [action [arguments]] Options: -c/--configuration -- configuration file path (default /etc/supervisord.conf) -h/--help -- print usage message and exit -i/--interactive -- start an interactive shell after executing commands -s/--serverurl URL -- URL on which supervisord server is listening (default \"http://localhost:9001\"). -u/--username -- username to use for authentication with server -p/--password -- password to use for authentication with server -r/--history-file -- keep a readline history (if readline is available) action [arguments] -- see below Actions are commands like \"tail\" or \"stop\". If -i is specified or no action is specified on the command line, a \"shell\" interpreting actions typed interactively is started. Use the action \"help\" to find out about available actions. 例如：\n# supervisorctl status confd RUNNING pid 31256, uptime 0:11:24 twemproxy RUNNING pid 31255, uptime 0:11:24 5.2. supervisord supervisord，初始启动 Supervisord，启动、管理配置中设置的进程。 $ supervisord --help supervisord -- run a set of applications as daemons. Usage: /usr/bin/supervisord [options] Options: -c/--configuration FILENAME -- configuration file -n/--nodaemon -- run in the foreground (same as 'nodaemon true' in config file) -h/--help -- print this usage message and exit -v/--version -- print supervisord version number and exit -u/--user USER -- run supervisord as this user (or numeric uid) -m/--umask UMASK -- use this umask for daemon subprocess (default is 022) -d/--directory DIRECTORY -- directory to chdir to when daemonized -l/--logfile FILENAME -- use FILENAME as logfile path -y/--logfile_maxbytes BYTES -- use BYTES to limit the max size of logfile -z/--logfile_backups NUM -- number of backups to keep when max bytes reached -e/--loglevel LEVEL -- use LEVEL as log level (debug,info,warn,error,critical) -j/--pidfile FILENAME -- write a pid file for the daemon process to FILENAME -i/--identifier STR -- identifier used for this instance of supervisord -q/--childlogdir DIRECTORY -- the log directory for child process logs -k/--nocleanup -- prevent the process from performing cleanup (removal of old automatic child log files) at startup. -a/--minfds NUM -- the minimum number of file descriptors for start success -t/--strip_ansi -- strip ansi escape codes from process output --minprocs NUM -- the minimum number of processes available for start success --profile_options OPTIONS -- run supervisord under profiler and output results based on OPTIONS, which is a comma-sep'd list of 'cumulative', 'calls', and/or 'callers', e.g. 'cumulative,callers') 6. Supervisor控制台 在/etc/supervisord.conf中修改[inet_http_server] 的参数，具体如下：\n[inet_http_server] ; inet (TCP) server disabled by default port=*:9001 ; ip_address:port specifier, *:port for all iface username=root ; default is no username (open server) password=xxxx ; default is no password (open server) 修改后重启supervisor进程，在浏览器访问 http://\u003chost-ip\u003e:9001。\n具体如下：\n7. supervisor.conf详细配置 cat /etc/supervisord.conf\n; Sample supervisor config file. [unix_http_server] file=/var/run/supervisor/supervisor.sock ; (the path to the socket file) ;chmod=0700 ; sockef file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) nodaemon=false ; (start in foreground if true;default false) minfds=1024 ; (min. avail startup file descriptors;default 1024) minprocs=200 ; (min. avail process descriptors;default 200) ;umask=022 ; (process file creation umask;default 022) ;user=chrism ; (default is current user, required if root) ;identifier=supervisor ; (supervisord identifier, default is 'supervisor') ;directory=/tmp ; (default is not to cd during start) ;nocleanup=true ; (don't clean up tempfiles at start;default false) ;childlogdir=/tmp ; ('AUTO' child log dir, default $TEMP) ;environment=KEY=value ; (key value pairs to add to environment) ;strip_ansi=false ; (strip ansi escape codes in logs; def. false) ; the below section must remain in the config file for RPC ; (supervisorctl/web interface) to work, additional interfaces may be ; added by defining them in separate rpcinterface: sections [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///var/run/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ; The below sample program section shows all possible program subsection values, ; create one or more 'real' program: sections to be able to control them under ; supervisor. ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;autorestart=true ; retstart at unexpected quit (default: true) ;startsecs=10 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; 'expected' exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=1,B=2 ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample eventlistener section shows all possible ; eventlistener subsection values, create one or more 'real' ; eventlistener: sections to be able to handle event notifications ; sent by supervisor. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;autorestart=unexpected ; restart at unexpected quit (default: unexpected) ;startsecs=10 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; 'expected' exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups ; # of stderr logfile backups (default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=1,B=2 ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample group section shows all possible group values, ; create one or more 'real' group: sections to create \"heterogeneous\" ; process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/conf/*.conf 参考：\nhttp://supervisord.org/\n","categories":"","description":"","excerpt":"1. Supervisor简介 Supervisord 是用 Python 实现的一款的进程管理工具，supervisord 要求管理的程序 …","ref":"/linux-notes/tools/supervisor-usage/","tags":["Linux"],"title":"Supervisor的使用"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vim/","tags":"","title":"VIM"},{"body":"1. vi的模式 1.1. 普通模式 由Shell进入vi编辑器时，首先进入普通模式。在普通模式下，从键盘输入任何字符都被当作命令来解释。普通模式下没有任何提示符，输入命令后立即执行，不需要回车，而且输入的字符不会在屏幕上显示出来。\n1.2. 编辑模式 编辑模式主要用于文本的编辑。该模式下用户输入的任何字符都被作为文件的内容保存起来，并在屏幕上显示出来。\n1.3. 命令模式 命令模式下，用户可以对文件进行一些高级处理。尽管普通模式下的命令可以完成很多功能，但要执行一些如字符串查找、替换、显示行号等操作还是必须要进入命令模式。\n也有文章称为两种工作模式，即把命令模式合并到普通模式。\n如果不确定当前处于哪种模式，按两次 Esc 键将回到普通模式。\n2. vim命令汇总 高级汇总\n3. vim命令分类 3.1. 基础编辑、移动光标 指令 解释 $ 行尾 ^ 行首 w 下一个单词 (词首） e 下一个单词（词尾） b 前一个单词 x del 删除后一个字符 X backspace 删除前一个字符 u 撤销 ctrl + r 重做 k 上 h 下 g 左 l 右 i 插入，开始写东西 s 覆盖 esc 退出输入模式，进入普通模式，可执行各种命令 3.2. 操作和重复操作 指令 解释 f 查找字符，按f后再按需要移动到的字符，光标就会移动到那 f; 就会移动到下一个 ;的位置 F 反向查找字符 . 重复上一个操作 v 选择模式，用上下左右选择文本，按相应的指令直接执行，如：选中后执行 d 就直接删除选中的文本 ctrl + v 块状选择模式，可以纵向选择文本块，而非以行的形式 d 高级删除指令： dw 删除一个单词 df( 配合 f ，删除从光标处到 ( 的字符，单行操作 dd 删除当前行 d2w 删除两个单词 d2t, 删除当前位置到后面第二个 , 之间的内容，不包含 , （t = to） 3.3. 复制 和 粘贴 指令 解释 y 复制 yy 复制当前行 p 粘贴到后面 P 粘贴到前面 o 在当前行的下一行添加空行并开始输入 O 在当前行的上一行添加空行并开始输入 所有经过 d x e 处理的字符串都已经复制到了粘贴板上。\n3.4. 搜索 指令 解释 / 从当前位置向后搜索 ？ 从当前位置后前搜索 n 搜索完之后，如果有多个结果，跳到 下一个匹 配项 N 跳到 上一个 匹配项 * 直接匹配当前光标下面的字符串，移到下一个匹配项，跟/ ? 没有关系 # 上一个匹配项 3.5. 标记 和 宏 标记\nm 后跟 a - z 任意字符来设置一个标记\n`` `后跟 字符来跳到这个标记点\n大写 A - Z 是全局的，小写 a - z\n'. 代表最后编辑位置\n宏\nq 后接 a - z 开始录制宏\nq 结束宏的录制\n@ 后接 a - z 读取宏\n@@ 代表最后一个宏\n3.6. 高级移动 % 在配对的 () [] 之间移动\nH M L 移动到编辑器可视范围的头部，中间，尾部\nG 到文件的尾部，前面添加数字再按 G 跳到输入的行，写行号的时候是看不见的\n- + 跳到上一行，下一行\n( ) 跳到当前句子的 首 / 尾\n{ } 跳到 前一个 / 后一个 空行\n[[ jumps to the previous { in column 0\n]] jumps to the next } column 0\n3.7. 高级指令 J 合并当前行与下一行。合并已选中的所有行。\nr 替换当前字符到下一个输入的字符。如： r 后接 4 会把当前字符替换成 4\nC 是 c$ 的缩写：修改从光标到结尾\nD 是 d$ 的缩写：删除从光标到结尾\nY 是 yy 的缩写：复制当前行\ns 删除光标下字符，并开始编辑\nS 删除当前行，并开始编辑\n\u003c 向前缩进，一行，或多行，范围设置在前面提到了，t等等\n\u003e 向后缩进，一行，或多行\n= 格式化，一行，或多行\n~ 切换光标下的字符大小写\n参考：\n本文由以下文章整理得\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.html https://segmentfault.com/a/1190000016056004#articleHeader15 ","categories":"","description":"","excerpt":"1. vi的模式 1.1. 普通模式 由Shell进入vi编辑器时，首先进入普通模式。在普通模式下，从键盘输入任何字符都被当作命令来解释。普 …","ref":"/linux-notes/ide/vim/vim-keymap/","tags":["VIM"],"title":"vim 命令"},{"body":"vimrc 中文版 由 https://blog.51cto.com/zpf666/2335640 转载\n\"~/.vimrc \"vim config file \"date 2018-12-26 \"Created by bert \"blog:https://blog.51cto.com/zpf666 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e全局配置\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"关闭vi兼容模式\" set nocompatible \"设置历史记录步数\" set history=1000 \"开启相关插件\" \"侦测文件类型\" filetype on \"载入文件类型插件\" filetype plugin on \"为特定文件类型载入相关缩进文件\" filetype indent on \"当文件在外部被修改时，自动更新该文件\" set autoread \"激活鼠标的使用\" set mouse=a set selection=exclusive set selectmode=mouse,key \"保存全局变量\" set viminfo+=! \"带有如下符号的单词不要被换行分割\" set iskeyword+=_,$,@,%,#,- \"通过使用: commands命令，告诉我们文件的哪一行被改变过\" set report=0 \"被分割的窗口间显示空白，便于阅读\" set fillchars=vert:\\ ,stl:\\ ,stlnc:\\ \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e字体和颜色\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"自动开启语法高亮\" syntax enable \"设置字体\" \"set guifont=dejaVu\\ Sans\\ MONO\\ 10 set guifont=Courier_New:h10:cANSI \"设置颜色\" \"colorscheme desert \"高亮显示当前行\" set cursorline hi cursorline guibg=#00ff00 hi CursorColumn guibg=#00ff00 \"高亮显示普通txt文件（需要txt.vim脚本）\" au BufRead,BufNewFile * setfiletype txt \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e代码折叠功能\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"激活折叠功能\" set foldenable \"set nofen（这个是关闭折叠功能）\" \"设置按照语法方式折叠（可简写set fdm=XX）\" \"有6种折叠方法： \"manual 手工定义折叠\" \"indent 更多的缩进表示更高级别的折叠\" \"expr 用表达式来定义折叠\" \"syntax 用语法高亮来定义折叠\" \"diff 对没有更改的文本进行折叠\" \"marker 对文中的标志进行折叠\" set foldmethod=manual \"set fdl=0（这个是不选用任何折叠方法）\" \"设置折叠区域的宽度\" \"如果不为0，则在屏幕左侧显示一个折叠标识列 \"分别用“-”和“+”来表示打开和关闭的折叠 set foldcolumn=0 \"设置折叠层数为3\" setlocal foldlevel=3 \"设置为自动关闭折叠\" set foldclose=all \"用空格键来代替zo和zc快捷键实现开关折叠\" \"zo O-pen a fold (打开折叠) \"zc C-lose a fold (关闭折叠) \"zf F-old creation (创建折叠) \"nnoremap \u003cspace\u003e @=((foldclosed(line('.')) \u003c 0) ? 'zc' : 'zo')\u003cCR\u003e \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e文字处理\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"使用空格来替换Tab\" set expandtab \"设置所有的Tab和缩进为4个空格\" set tabstop=4 \"设定\u003c\u003c和\u003e\u003e命令移动时的宽度为4\" set shiftwidth=4 \"使得按退格键时可以一次删除4个空格\" set softtabstop=4 set smarttab \"缩进，自动缩进（继承前一行的缩进）\" \"set autoindent 命令打开自动缩进，是下面配置的缩写 \"可使用autoindent命令的简写，即“:set ai”和“:set noai” \"还可以使用“:set ai sw=4”在一个命令中打开缩进并设置缩进级别 set ai set cindent \"智能缩进\" set si \"自动换行” set wrap \"设置软宽度\" set sw=4 \"行内替换\" set gdefault \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eVim 界面\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"增强模式中的命令行自动完成操作\" set wildmenu \"显示标尺\" set ruler \"设置命令行的高度\" set cmdheight=1 \"显示行数\" set nu \"不要图形按钮\" set go= \"在执行宏命令时，不进行显示重绘；在宏命令执行完成后，一次性重绘，以便提高性能\" set lz \"使回格键（backspace）正常处理indent, eol, start等\" set backspace=eol,start,indent \"允许空格键和光标键跨越行边界\" set whichwrap+=\u003c,\u003e,h,l \"设置魔术\" set magic \"关闭遇到错误时的声音提示\" \"关闭错误信息响铃\" set noerrorbells \"关闭使用可视响铃代替呼叫\" set novisualbell \"高亮显示匹配的括号([{和}])\" set showmatch \"匹配括号高亮的时间（单位是十分之一秒）\" set mat=2 \"光标移动到buffer的顶部和底部时保持3行距离\" set scrolloff=3 \"搜索逐字符高亮\" set hlsearch set incsearch \"搜索时不区分大小写\" \"还可以使用简写（“:set ic”和“:set noic”）\" set ignorecase \"用浅色高亮显示当前行\" autocmd InsertLeave * se nocul autocmd InsertEnter * se cul \"输入的命令显示出来，看的清楚\" set showcmd \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e编码设置\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"设置编码\" set encoding=utf-8 set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936 \"设置文件编码\" set fileencodings=utf-8 \"设置终端编码\" set termencoding=utf-8 \"设置语言编码\" set langmenu=zh_CN.UTF-8 set helplang=cn \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e其他设置\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"开启新行时使用智能自动缩进\" set smartindent set cin set showmatch \"在处理未保存或只读文件的时候，弹出确认\" set confirm \"隐藏工具栏\" set guioptions-=T \"隐藏菜单栏\" set guioptions-=m \"置空错误铃声的终端代码\" set vb t_vb= \"显示状态栏（默认值为1，表示无法显示状态栏）\" set laststatus=2 \"状态行显示的内容\" set statusline=%F%m%r%h%w\\ [FORMAT=%{\u0026ff}]\\ [TYPE=%Y]\\ [POS=%l,%v][%p%%]\\ %{strftime(\\\"%d/%m/%y\\ -\\ %H:%M\\\")} \"粘贴不换行问题的解决方法\" set pastetoggle=\u003cF9\u003e \"设置背景颜色\" set background=dark \"文件类型自动检测，代码智能补全\" set completeopt=longest,preview,menu \"共享剪切板\" set clipboard+=unnamed \"从不备份\" set nobackup set noswapfile \"自动保存\" set autowrite \"显示中文帮助\" if version \u003e= 603 set helplang=cn set encoding=utf-8 endif \"设置高亮相关项\" highlight Search ctermbg=black ctermfg=white guifg=white guibg=black \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003e在shell脚本开头自动增加解释器以及作者等版权信息\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"新建.py,.cc,.sh,.java文件，自动插入文件头\" autocmd BufNewFile *.py,*.cc,*.sh,*.java exec \":call SetTitle()\" \"定义函数SetTitle，自动插入文件头\" func SetTitle() if expand (\"%:e\") == 'sh' call setline(1, \"#!/bin/bash\") call setline(2, \"#Author:bert\") call setline(3, \"#Blog:https://blog.51cto.com/zpf666\") call setline(4, \"#Time:\".strftime(\"%F %T\")) call setline(5, \"#Name:\".expand(\"%\")) call setline(6, \"#Version:V1.0\") call setline(7, \"#Description:This is a production script.\") endif endfunc ","categories":"","description":"","excerpt":"vimrc 中文版 由 https://blog.51cto.com/zpf666/2335640 转载\n\"~/.vimrc \"vim …","ref":"/linux-notes/ide/vim/vimrc-cn/","tags":["VIM"],"title":"vim 配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","tags":"","title":"大模型"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":"","title":"问题排查"}]