[{"body":"å‡è®¾åœ¨ Kubernetes é›†ç¾¤ä¸­ä¸€æ¬¡æ€§è°ƒåº¦ 1 ä¸‡ä¸ª Podï¼Œ è¿™æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å¦‚æœç®¡ç†ä¸å½“ï¼Œå¯èƒ½ä¼šå¯¼è‡´è°ƒåº¦å™¨ç“¶é¢ˆã€API Server è¿‡è½½ï¼Œç”šè‡³æ•´ä¸ªé›†ç¾¤å´©æºƒã€‚\næœ¬æ–‡å°†æ¢è®¨ä¼˜åŒ–å¤§è§„æ¨¡ Pod è°ƒåº¦çš„æœ€ä½³å®è·µä¸æŠ€æœ¯æ‰‹æ®µã€‚\nğŸš€ é¢ä¸´çš„æŒ‘æˆ˜ è°ƒåº¦å™¨å‹åŠ›å¤§ï¼šå¤§é‡ Pod åŒæ—¶è¿›å…¥ Pending çŠ¶æ€ï¼Œè°ƒåº¦å™¨å¤„ç†ä¸è¿‡æ¥ã€‚ API Server å‹åŠ›å¤§ï¼šé«˜é¢‘çš„ CREATE/GET/LIST è¯·æ±‚å¯èƒ½è§¦å‘é™æµã€‚ etcd å»¶è¿Ÿå¢åŠ ï¼šå†™å…¥åŠçŠ¶æ€å˜åŒ–é¢‘ç¹ï¼Œå¯¼è‡´å­˜å‚¨åç«¯å‹åŠ›è¿‡å¤§ã€‚ èŠ‚ç‚¹å‹åŠ›ä¸å‡è¡¡ï¼šè°ƒåº¦ä¸å‡å¯å¯¼è‡´éƒ¨åˆ†èŠ‚ç‚¹ CPU/å†…å­˜/ç£ç›˜ IO èµ„æºæ‰“çˆ†ã€‚ ç½‘ç»œæ’ä»¶ç“¶é¢ˆï¼šCNI æ’ä»¶æ— æ³•å¤„ç†å¤§é‡å¹¶å‘çš„ IP åˆ†é…ã€‚ 1. è°ƒåº¦å™¨ä¼˜åŒ– ğŸ”§ 1.1. æ§åˆ¶ Pod åˆ›å»ºé€Ÿç‡ ä¸è¦ä¸€æ¬¡æ€§å¯åŠ¨ 10,000 ä¸ª Podï¼Œè€Œæ˜¯ï¼š\nåˆ†æ‰¹åˆ›å»ºï¼šä¾‹å¦‚æ¯æ‰¹åˆ›å»º 500â€“1000 ä¸ª Podã€‚ æ§åˆ¶é€Ÿç‡ï¼šé€šè¿‡è„šæœ¬æˆ– Job æ§åˆ¶å™¨å¼•å…¥ sleep ç­‰æ—¶é—´é—´éš”ã€‚ ç¤ºä¾‹ Shell è„šæœ¬ï¼š for file in batches/batch_*.yaml; do kubectl apply -f \"$file\" sleep 10 done âš™ï¸ 1.2. è°ƒä¼˜é»˜è®¤è°ƒåº¦å™¨ é»˜è®¤è°ƒåº¦å™¨ï¼ˆkube-schedulerï¼‰åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚ä¸ºäº†é«˜æ•ˆè°ƒåº¦ 1 ä¸‡ä¸ªä»¥ä¸Šçš„ Podï¼Œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ·±å…¥è°ƒä¼˜ï¼š\nâœ… 1. æé«˜å¹¶å‘è°ƒåº¦èƒ½åŠ› Kubernetes v1.19+ æ”¯æŒé…ç½®å¹¶è¡Œè°ƒåº¦çº¿ç¨‹æ•°ï¼š\napiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler parallelism: 64 æ¨èå€¼ä¸º CPU æ ¸å¿ƒæ•°çš„ 2~4 å€ï¼ˆè§†è°ƒåº¦å¯†é›†åº¦è€Œå®šï¼‰ã€‚\næ³¨æ„ï¼šå¹¶å‘è¿‡é«˜å¯èƒ½å¯¼è‡´å†…å­˜æ¿€å¢æˆ– etcd å‹åŠ›è¿‡å¤§ï¼Œå»ºè®®ç»“åˆå‹æµ‹è¯„ä¼°ã€‚\nâœ… 2. å¯ç”¨ç¼“å­˜è°ƒåº¦å™¨ï¼ˆScheduling Queue ä¼˜åŒ–ï¼‰ è°ƒåº¦å™¨å†…éƒ¨ç»´æŠ¤äº† Pending Pod çš„ä¼˜å…ˆé˜Ÿåˆ—ï¼ˆPriorityQueueï¼‰ä¸ Node ä¿¡æ¯ç¼“å­˜ã€‚\nç¡®ä¿ä½¿ç”¨ä¼˜å…ˆçº§è°ƒåº¦ï¼ˆPodPriorityï¼‰å¯å¸®åŠ©è°ƒåº¦å™¨ä¼˜å…ˆå¤„ç†é‡è¦ä»»åŠ¡ã€‚\né…ç½®è°ƒåº¦å™¨æ—¶å¯å¯ç”¨ permit æ’ä»¶é˜¶æ®µï¼Œåœ¨è°ƒåº¦å†³ç­–å‰æå‰æ§åˆ¶è°ƒåº¦æµé‡ã€‚\nâœ… 3. å…³é—­æˆ–ç²¾ç®€è€—æ—¶æ’ä»¶ æŸäº›é»˜è®¤å¯ç”¨çš„æ’ä»¶åœ¨è°ƒåº¦é«˜å³°æ—¶ä¼šå¸¦æ¥æ€§èƒ½è´Ÿæ‹…ï¼š\næ’ä»¶ ç±»å‹ è¯´æ˜ VolumeBinding Bind æŒä¹…åŒ–å·ç»‘å®šï¼Œéœ€è®¿é—® API NodeResourcesFit Filter æ£€æŸ¥èµ„æºæ˜¯å¦æ»¡è¶³ InterPodAffinity Filter Pod ä¹‹é—´äº²å’Œæ€§è®¡ç®—å¤æ‚ âš ï¸ ä¼˜åŒ–å»ºè®®ï¼š\næ— çŠ¶æ€æœåŠ¡å»ºè®® å…³é—­ VolumeBinding æ’ä»¶ã€‚\nåªä½¿ç”¨å¿…è¦çš„ Score æ’ä»¶ï¼ˆå¦‚ LeastAllocatedã€BalancedAllocationï¼‰ã€‚\né…ç½®æ–¹å¼ï¼š\nplugins: score: disabled: - name: \"NodeResourcesBalancedAllocation\" enabled: - name: \"NodeResourcesLeastAllocated\" âœ… 4. é¢„é€‰èŠ‚ç‚¹é›†èŒƒå›´ï¼ˆèŠ‚ç‚¹å‰ªæï¼‰ è°ƒåº¦å™¨é»˜è®¤ä¼šè¯„ä¼°æ‰€æœ‰å¯è°ƒåº¦èŠ‚ç‚¹ï¼Œ1 ä¸‡ Pod Ã— 1 åƒèŠ‚ç‚¹çš„ç»„åˆæå…¶è€—æ—¶ã€‚\nä¼˜åŒ–æ–¹æ³•ï¼š\nNodeAffinityï¼šæå‰é€šè¿‡æ ‡ç­¾ç­›æ‰ä¸ç¬¦åˆçš„èŠ‚ç‚¹ã€‚\nä½¿ç”¨ preFilter æ’ä»¶ è‡ªå®šä¹‰èŠ‚ç‚¹é›†åˆã€‚\nç¤ºä¾‹ï¼š\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/worker operator: In values: - batch âœ… 5. å¯ç”¨æ‹“æ‰‘æ„ŸçŸ¥ä¸äº²å’Œæ€§ç¼“å­˜ ä½¿ç”¨æ‹“æ‰‘è°ƒåº¦å»ºè®®ï¼š\ntopologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway âœ… 6. æ§åˆ¶è°ƒåº¦é˜Ÿåˆ—å‹åŠ›ï¼ˆBackoff \u0026 Retryï¼‰ Pod å¤šæ¬¡è°ƒåº¦å¤±è´¥ä¼šè¿›å…¥ backoff é˜Ÿåˆ—ï¼Œé»˜è®¤é€€é¿æ—¶é—´ä¸ºï¼š\nInitialBackoff = 1 * time.Second MaxBackoff = 10 * time.Second è°ƒå¤§ MaxBackoff å¯å‡ç¼“é«˜é¢‘é‡è¯•å¯¹è°ƒåº¦å™¨çš„å‹åŠ›ã€‚\nğŸ§ª è°ƒä¼˜æ•ˆæœéªŒè¯å»ºè®®ï¼š ä½¿ç”¨ --v=5 çº§åˆ«è¿è¡Œ kube-schedulerï¼Œè¾“å‡ºè°ƒåº¦è¯¦ç»†æ—¥å¿—ã€‚\nè§‚å¯Ÿè°ƒåº¦å»¶è¿ŸæŒ‡æ ‡ï¼ˆSchedulingLatencySecondsï¼‰ï¼š\nframework_extension_point_duration_seconds\nscheduler_scheduling_duration_seconds_bucket\nğŸ§© 1.3. æ‰©å±•è°ƒåº¦å™¨ï¼ˆScheduling Framework æ’ä»¶ï¼‰ Kubernetes æ”¯æŒé€šè¿‡è°ƒåº¦æ¡†æ¶æ’ä»¶æœºåˆ¶è‡ªå®šä¹‰è°ƒåº¦é€»è¾‘ã€‚\nâœ¨ ç¤ºä¾‹ï¼šå¿«é€Ÿ Filter æ’ä»¶ è‡ªå®šä¹‰è¿‡æ»¤æ’ä»¶ï¼Œå¯åªè¯„ä¼°éƒ¨åˆ†èŠ‚ç‚¹ï¼Œä»è€Œå‡å°‘è°ƒåº¦å»¶è¿Ÿï¼š\nfunc (f *FastFilterPlugin) Filter(...) *framework.Status { if strings.HasPrefix(node.Name, \"compute-node-\") { return framework.NewStatus(framework.Success) } return framework.NewStatus(framework.Unschedulable) } ğŸ§  1.4. ä½¿ç”¨å¤šä¸ªè°ƒåº¦å™¨ï¼ˆè°ƒåº¦å™¨éš”ç¦»ï¼‰ å¹¶è¡Œéƒ¨ç½²å¤šä¸ªè°ƒåº¦å™¨è¿›ç¨‹ï¼š\nspec: schedulerName: batch-scheduler æ¯ç±»å·¥ä½œè´Ÿè½½ä½¿ç”¨ä¸åŒè°ƒåº¦å™¨è¿›è¡Œå¤„ç†ï¼Œå®ç°å¹¶è¡Œè°ƒåº¦å’Œèµ„æºéš”ç¦»ã€‚\nğŸ›  1.5. ä½¿ç”¨é«˜æ€§èƒ½è°ƒåº¦ç³»ç»Ÿ Koordinator æ”¯æŒæ‰¹é‡è°ƒåº¦ã€NUMA æ„ŸçŸ¥ã€QoS èµ„æºåˆ†çº§ ä¸ Kubernetes è°ƒåº¦æ¡†æ¶å…¼å®¹ï¼Œéƒ¨ç½²ç®€å• Volcano é¢å‘å¤§è§„æ¨¡æ‰¹å¤„ç†ã€AI/MLã€HPC ä»»åŠ¡è°ƒåº¦ æ”¯æŒæŠ¢å ã€ä»»åŠ¡ä¼˜å…ˆçº§ã€ä¾èµ–å…³ç³»ç­‰ ğŸ“Š 1.6. ç›‘æ§ä¸éªŒè¯å»ºè®® ä½¿ç”¨ kubectl get pods -w å®æ—¶è§‚å¯Ÿ Pending çŠ¶æ€ å…³æ³¨è°ƒåº¦äº‹ä»¶ FailedScheduling è·Ÿè¸ª API Server æŒ‡æ ‡ï¼šQPSã€å»¶è¿Ÿã€å†…å­˜å ç”¨ éƒ¨ç½² Prometheus + Grafana è¿›è¡Œç³»ç»Ÿç›‘æ§ä¸å¯è§†åŒ– âœ… 1.7. æ€»ç»“å¯¹æ¯” ä¼˜åŒ–æ–¹å¼ æ•ˆæœ æ§åˆ¶ Pod åˆ›å»ºé€Ÿç‡ é¿å…æ§åˆ¶é¢ç»„ä»¶è¿‡è½½ æé«˜è°ƒåº¦å™¨å¹¶å‘åº¦ æå‡æ¯ç§’è°ƒåº¦åå ç¼–å†™è°ƒåº¦å™¨æ’ä»¶ é™ä½å•æ¬¡è°ƒåº¦å¤æ‚åº¦ å¤šè°ƒåº¦å™¨æ¶æ„ å®ç°ä»»åŠ¡éš”ç¦»ä¸å¹¶è¡Œè°ƒåº¦ ä½¿ç”¨ Koordinator/Volcano é¢å‘ AI/æ‰¹å¤„ç†ç­‰é«˜è´Ÿè½½åœºæ™¯ 2. Etcdä¼˜åŒ– etcd æ˜¯ Kubernetes æ§åˆ¶å¹³é¢çš„æ ¸å¿ƒå­˜å‚¨å¼•æ“ï¼Œä¸€æ—¦åœ¨å¤§è§„æ¨¡ Pod åˆ›å»ºã€è°ƒåº¦è¿‡ç¨‹ä¸­å‡ºç° å†™å…¥å»¶è¿Ÿå¢åŠ ï¼Œä¼šç›´æ¥å½±å“ API Server æ€§èƒ½ï¼Œè¿›è€Œæ‹–æ…¢è°ƒåº¦å™¨å’Œæ§åˆ¶å™¨ååº”é€Ÿåº¦ï¼Œç”šè‡³å¼•å‘é›†ç¾¤ä¸å¯ç”¨ã€‚\nğŸ”§ 2.1. åŸºç¡€é…ç½®ä¼˜åŒ– âœ… 1. å¯ç”¨è‡ªåŠ¨å‹ç¼©å†å²æ•°æ® etcd é»˜è®¤ä¼šä¿ç•™å†å²ç‰ˆæœ¬ï¼Œéšç€å¯¹è±¡å˜åŒ–å¢å¤šï¼Œå­˜å‚¨è†¨èƒ€ï¼Œå¯¼è‡´å»¶è¿Ÿå‡é«˜ã€‚\n--auto-compaction-retention=1h # æ¯å°æ—¶æ¸…ç†å†å² --snapshot-count=10000 # æ§åˆ¶ä½•æ—¶è§¦å‘å¿«ç…§ âœ… 2. å¯ç”¨ WAL å‹ç¼© å‹ç¼© Write-Ahead Logï¼Œå‡å°‘ç£ç›˜ I/O å¼€é”€ï¼š\n--experimental-initial-corrupt-check=true --experimental-compact-hash-check-enabled=true ğŸ’½ 2.2. ç¡¬ä»¶å±‚ä¼˜åŒ–ï¼ˆéå¸¸å…³é”®ï¼‰ etcd å¯¹ ç£ç›˜ IOPS å’Œå»¶è¿Ÿ æ•æ„Ÿï¼Œæ¨èï¼š\nä½¿ç”¨ SSDï¼ˆNVMe æœ€ä½³ï¼‰\netcd ç‹¬ç«‹ç£ç›˜ï¼Œé¿å…å’Œ kubelet æˆ– container runtime å…±ç”¨\næå‡å†…å­˜ï¼ˆå»ºè®® 16G+ï¼‰ã€CPU æ€§èƒ½ï¼ˆè‡³å°‘ 4 æ ¸ï¼‰\nå¼€å¯ NUMA äº²å’Œé…ç½®ï¼Œå‡å°‘è·¨æ ¸è°ƒåº¦\nğŸ§± 2.3. é›†ç¾¤éƒ¨ç½²æ¶æ„ä¼˜åŒ– âœ… 1. éš”ç¦»éƒ¨ç½² etcd etcdä¸è¦ä¸ kube-apiserverã€controller-manager ç­‰ç»„ä»¶å…±èŠ‚ç‚¹è¿è¡Œã€‚\n1ï¼‰etcd å¯¹ç£ç›˜ IOã€å†…å­˜å’Œ CPU çš„æ€§èƒ½éå¸¸æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯ç£ç›˜å»¶è¿Ÿå¯¹ etcd æ€§èƒ½å’Œç¨³å®šæ€§æœ‰æ˜¾è‘—å½±å“ã€‚\nkube-apiserverã€controller-managerã€scheduler ç­‰ç»„ä»¶ä¹Ÿä¼šé¢‘ç¹è®¿é—® etcdï¼Œäº§ç”Ÿè¾ƒå¤§ CPU å’Œå†…å­˜è´Ÿè½½ã€‚\nå¦‚æœå®ƒä»¬éƒ¨ç½²åœ¨åŒä¸€èŠ‚ç‚¹ï¼Œå®¹æ˜“å¯¼è‡´ èµ„æºç«äº‰ï¼ˆå°¤å…¶æ˜¯ IOï¼‰ï¼Œå½±å“ etcd çš„å“åº”èƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œè¿›è€Œå½±å“æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶é¢ã€‚\n2ï¼‰ Kubernetes é€šå¸¸è¿è¡Œåœ¨é«˜é€Ÿå±€åŸŸç½‘å†…ï¼Œè®¿é—® etcd çš„å»¶è¿Ÿå¾ˆå°\nç½‘ç»œå»¶è¿Ÿåœ¨ç°ä»£æ•°æ®ä¸­å¿ƒæˆ–äº‘ç¯å¢ƒä¸­é€šå¸¸æ˜¯å¾®ç§’åˆ°æ¯«ç§’çº§åˆ«ã€‚\nåªè¦ä¿è¯ etcd é›†ç¾¤ç½‘ç»œç¨³å®šï¼Œæ§åˆ¶é¢ç»„ä»¶å³ä½¿ä¸åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¹Ÿèƒ½å¿«é€Ÿè®¿é—®ã€‚\n3ï¼‰ ä¸åœ¨ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¯ä»¥é¿å…â€œå±€éƒ¨é«˜è´Ÿè½½â€å¯¼è‡´è¿é”å½±å“\nå¦‚æœ kube-apiserver è·Ÿ etcd åŒèŠ‚ç‚¹ï¼Œä¸€æ—¦ kube-apiserver çªå‘æµé‡ï¼ˆæ¯”å¦‚åˆ›å»ºå¤§é‡èµ„æºï¼‰ï¼Œä¼šå¯¼è‡´ etcd æ‰€åœ¨èŠ‚ç‚¹èµ„æºè¢«å æ»¡ï¼Œä»è€Œå½±å“ etcd å“åº”ã€‚\nåä¹‹äº¦ç„¶ï¼Œetcd çš„ GC æˆ– compaction æ“ä½œä¹Ÿå¯èƒ½å½±å“ apiserver çš„æ€§èƒ½ã€‚\nâœ… 2. å¤šå‰¯æœ¬éƒ¨ç½²ï¼ˆ3~5ä¸ªèŠ‚ç‚¹ï¼‰ é¿å…å•ç‚¹ç“¶é¢ˆï¼Œå¹¶å¯ç”¨é«˜å¯ç”¨ã€‚\n3. kube-apiserverä¼˜åŒ– åœ¨å¤§é‡ Pod åŒæ—¶è°ƒåº¦æ—¶ï¼ŒAPI Server å‹åŠ›å¤§ æ˜¯é€ æˆé›†ç¾¤å¡é¡¿æˆ–å¼‚å¸¸çš„æ ¸å¿ƒç“¶é¢ˆä¹‹ä¸€ï¼Œä¸»è¦è¡¨ç°ä¸ºï¼š\nåˆ›å»ºã€æ›´æ–°ã€æŸ¥è¯¢ Pod ç­‰è¯·æ±‚å“åº”å˜æ…¢\nkubeletã€controller-manager ä¸ API Server é€šä¿¡è¶…æ—¶\netcd QPS æ¿€å¢ã€å»¶è¿Ÿå‡é«˜ï¼Œç”šè‡³è§¦å‘ç†”æ–­\nä»¥ä¸‹æ˜¯å…·ä½“çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»é›†ç¾¤å‚æ•°ã€é™æµã€ç»„ä»¶è§£è€¦ç­‰å¤šä¸ªå±‚é¢å±•å¼€ï¼š\nğŸ§± 3.1. æ§åˆ¶è¯·æ±‚é€Ÿç‡ï¼ˆé™æµï¼‰ âœ… 1. æ§åˆ¶å®¢æˆ·ç«¯åˆ›å»ºé€Ÿç‡ æ¯”å¦‚å¤§é‡ Job/Deployment æ§åˆ¶å™¨ã€è„šæœ¬åŒæ—¶å‘å‡º kubectl apply è¯·æ±‚ï¼š\nè§£å†³æ–¹æ³•ï¼š\né‡‡ç”¨ kubectl --wait=false å¼‚æ­¥åˆ›å»º\nä½¿ç”¨åˆ†æ‰¹ apply æˆ– sleep æ§åˆ¶é€Ÿç‡\nä½¿ç”¨ controllerï¼ˆä¾‹å¦‚è‡ªå®šä¹‰ CRD + controllerï¼‰åˆ†æ‰¹åˆ†ç»„ç®¡ç† pod/job\nâš™ï¸ 3.2. è°ƒä¼˜ API Server å‚æ•° åœ¨ kube-apiserver å¯åŠ¨å‚æ•°ä¸­ï¼š\nâœ… 1. å¢åŠ æœ€å¤§å¹¶å‘ QPS --max-requests-inflight=4000 # é»˜è®¤ 400ï¼Œå¢åŠ ååèƒ½åŠ› --max-mutating-requests-inflight=2000 # é»˜è®¤ 200ï¼Œè°ƒå¤§å†™è¯·æ±‚å®¹é‡ âœ… 2. å¢åŠ ç¼“å­˜æ—¶é—´ä¸å“åº”çª—å£ --request-timeout=1m --min-request-timeout=300 3.3. operatorä¼˜åŒ– å¦‚æœæœ‰å¼€å‘è‡ªå®šä¹‰çš„operatorï¼Œåˆ™éœ€è¦å¯¹operatorçš„é€»è¾‘è¿›è¡Œä¼˜åŒ–ã€‚\nâœ… 1. ä½¿ç”¨ informer ç¼“å­˜æœºåˆ¶ï¼ˆclient-go é»˜è®¤æ”¯æŒï¼‰ è‡ªå®šä¹‰æ§åˆ¶å™¨æˆ–è°ƒåº¦æ’ä»¶ä¸­åº”ä½¿ç”¨å…±äº«ç¼“å­˜ï¼Œè€Œéé¢‘ç¹ GETï¼š\ninformer := factory.Core().V1().Pods().Informer() âœ… 2. å‡å°‘ä¸å¿…è¦çš„ Watch æˆ–é¢‘ç¹ List è¯·æ±‚ è°ƒåº¦å™¨æ’ä»¶ä¸­ä¸è¦é¢‘ç¹è®¿é—® Pod åˆ—è¡¨\nå‡å°‘ metrics æˆ–å®¡è®¡æ—¥å¿—ç³»ç»Ÿå¯¹ API çš„é«˜é¢‘é‡‡é›†\n4. ç½‘ç»œæ’ä»¶ä¼˜åŒ– Kubernetes ç½‘ç»œæ’ä»¶ï¼ˆCNIï¼‰åœ¨å¤§è§„æ¨¡éƒ¨ç½²æˆ–é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œè‹¥å¤„ç†èƒ½åŠ›è·Ÿä¸ä¸Šï¼Œä¼šå‡ºç° è°ƒåº¦æˆåŠŸä½†ç½‘ç»œä¸é€šã€æœåŠ¡è¿æ¥æ…¢ã€è·¨èŠ‚ç‚¹é€šä¿¡å¼‚å¸¸ ç­‰é—®é¢˜ã€‚\nğŸ§­ 4.1. ç½‘ç»œç“¶é¢ˆè¡¨ç° ç°è±¡ å¯èƒ½åŸå›  Pod åˆ›å»ºå¡ä½åœ¨ ContainerCreating CNI æ’ä»¶è°ƒç”¨è¶…æ—¶ï¼Œç½‘ç»œè®¾å¤‡æœªåˆå§‹åŒ– è·¨èŠ‚ç‚¹æœåŠ¡è®¿é—®æ…¢æˆ–è¶…æ—¶ ç½‘ç»œæ’ä»¶è½¬å‘è·¯å¾„æ€§èƒ½ä¸è¶³ï¼Œiptables/ebpf ç´¯ç§¯ é›†ç¾¤ä¸­ ping æŸäº› Pod æ…¢ æŸäº›èŠ‚ç‚¹æµé‡ç“¶é¢ˆï¼Œæˆ–è€… VXLAN éš§é“é«˜å»¶è¿Ÿ kube-proxy CPU å æ»¡ iptables è§„åˆ™è¿‡å¤šæˆ–é¢‘ç¹å˜æ›´ âœ… 4.2. é€‰å‹ä¼˜åŒ–ï¼šé€‰æ‹©é«˜æ€§èƒ½ CNI æ’ä»¶ æ’ä»¶ æ€§èƒ½ç‰¹ç‚¹ Cilium eBPF é©±åŠ¨ï¼Œæ— éœ€ iptablesï¼Œæé«˜æ€§èƒ½ï¼Œæ”¯æŒå¤§è§„æ¨¡èŠ‚ç‚¹ Calico (BPF æ¨¡å¼) æ”¯æŒ eBPF æ¨¡å¼ï¼Œæ€§èƒ½æ›´å¥½äºä¼ ç»Ÿ iptables Flannel é€‚ç”¨äºå°è§„æ¨¡é›†ç¾¤ï¼Œæ€§èƒ½æ™®é€šï¼Œä¸æ¨èå¤§é›†ç¾¤ä½¿ç”¨ Multus æ”¯æŒå¤šç½‘å¡/å¤š CNIï¼Œé€‚åˆè¾¹ç¼˜åœºæ™¯ä½†è°ƒè¯•å¤æ‚ ğŸ’¡ æ¨èä½¿ç”¨ Cilium æˆ– Calicoï¼ˆeBPF æ¨¡å¼ï¼‰ï¼Œé¿å…ä½¿ç”¨ä¼ ç»Ÿ Flannel/VXLANã€‚\nâš™ï¸ 4.3. ç½‘ç»œæ’ä»¶å‚æ•°è°ƒä¼˜ ğŸ”¹ Cilium ç¤ºä¾‹ é…ç½® /etc/cilium/cilium-configï¼š\nenable-bpf-masquerade: \"true\" enable-ipv4-masquerade: \"false\" bpf-lb-map-max: \"65536\" bpf-ct-global-tcp-max: \"524288\" bpf-ct-global-any-max: \"262144\" å¹¶å¯ç”¨ kube-proxy æ›¿ä»£æ¨¡å¼ï¼ˆkube-proxy-freeï¼‰ï¼š\nkubeProxyReplacement: \"strict\" ğŸ”„ 4.4. è·¨èŠ‚ç‚¹é€šä¿¡ä¼˜åŒ– âœ… 1. å‡å°‘ VXLAN å°è£…ï¼ˆæˆ–æ”¹ä¸º Native Routingï¼‰ Flannel/Calico VXLAN æ¨¡å¼æ€§èƒ½å·®\næ¨èåˆ‡æ¢ä¸º Calico çš„ BGP æ¨¡å¼ï¼ˆè·¯ç”±ç›´è¾¾ï¼Œæ— å°è£…ï¼‰\nâœ… 2. ä½¿ç”¨ Direct Server Returnï¼ˆDSRï¼‰+ ECMP è·¯ç”± å¤§æµé‡æœåŠ¡éƒ¨ç½²æ—¶ï¼Œé¿å…ä¸­å¿ƒåŒ–è½¬å‘ã€‚\nğŸ”ƒ 4.5. kube-proxy ä¼˜åŒ– âœ… 1. ä½¿ç”¨ ipvs æ¨¡å¼ä»£æ›¿ iptables --proxy-mode=ipvs --ipvs-scheduler=rr ç›¸æ¯” iptablesï¼Œipvs å¤„ç†æœåŠ¡è½¬å‘åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸‹ CPU æ›´çœã€å»¶è¿Ÿæ›´ä½ã€‚\nâœ… 2. é…åˆ eBPF æ›¿ä»£ kube-proxyï¼ˆCilium æ¨èï¼‰ Cilium çš„ kube-proxy-replacement=strict ç›´æ¥ä½¿ç”¨ BPF åŠ é€ŸæœåŠ¡è°ƒåº¦ã€‚\nğŸ”§ 4.6. èŠ‚ç‚¹ç³»ç»Ÿå‚æ•°ä¼˜åŒ– è®¾ç½®èŠ‚ç‚¹çš„å†…æ ¸å‚æ•°ï¼Œæå‡å¤§æµé‡ä¸‹ç³»ç»Ÿå¤„ç†èƒ½åŠ›ï¼š\n# æé«˜ conntrack è¡¨å®¹é‡ sysctl -w net.netfilter.nf_conntrack_max=262144 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=86400 # å…è®¸æ›´å¤šæ–‡ä»¶æè¿°ç¬¦ ulimit -n 1048576 # è°ƒé«˜é˜Ÿåˆ—é•¿åº¦ sysctl -w net.core.somaxconn=1024 sysctl -w net.core.netdev_max_backlog=250000 ğŸ“Š 4.7. ç›‘æ§å…³é”®æŒ‡æ ‡ é€šè¿‡ Cilium/Calico è‡ªå¸¦ metrics æˆ– Prometheus é‡‡é›†ï¼š\næŒ‡æ ‡ è¯´æ˜ cilium_forwarding_latency_seconds è½¬å‘å»¶è¿Ÿ cilium_drop_count_total æ•°æ®åŒ…è¢«ä¸¢å¼ƒçš„åŸå›  iptables_rule_count kube-proxy ä¸­è§„åˆ™æ•°é‡ conntrack_entries å½“å‰è¿æ¥è·Ÿè¸ªè¡¨å¤§å° âœ… 4.8. æ€»ç»“ä¼˜åŒ–å»ºè®®è¡¨ æ–¹å‘ æ–¹æ¡ˆ æ’ä»¶é€‰å‹ ä½¿ç”¨ Cilium/Calico eBPFï¼Œé¿å… Flannel æ’ä»¶é…ç½® ä¼˜åŒ–è½¬å‘è¡¨ã€è¿æ¥è·Ÿè¸ªè¡¨å¤§å° ç½‘ç»œæ¶æ„ BGP æ›¿ä»£ VXLANï¼Œå¼€å¯ kube-proxy-free ç³»ç»Ÿå†…æ ¸ è°ƒé«˜ conntrack / backlog ç­‰å‚æ•° è½¬å‘æ¨¡å¼ ä½¿ç”¨ ipvs æˆ– eBPF åŠ é€Ÿ kube-proxy ç›‘æ§æ’æŸ¥ å¼€å¯ drop åˆ†æã€BPF è·¯å¾„è¿½è¸ª 5. æ€»ç»“ å¤§è§„æ¨¡ Pod è°ƒåº¦ä¸ä»…ä»…æ˜¯è¿½æ±‚é€Ÿåº¦ï¼Œæ›´é‡è¦çš„æ˜¯åœ¨é«˜å‹ä¸‹ä¿æŒç³»ç»Ÿçš„ç¨³å®šæ€§ä¸æ­£ç¡®æ€§ã€‚éœ€è¦ä»å„ä¸ªæ–¹é¢è¿›è¡Œé›†ç¾¤ä¼˜åŒ–æ‰èƒ½æ‰¿å—å¤§è§„æ¨¡podé›†ç¾¤çš„æ€§èƒ½å‹åŠ›ã€‚æœ¬æ–‡åˆ†åˆ«ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œä¼˜åŒ–ï¼š\nè°ƒåº¦å™¨åŠæ‰©å±•è°ƒåº¦å™¨\nETCDä¼˜åŒ–\nkube-apiserverä¼˜åŒ–\nç½‘ç»œæ’ä»¶åŠèŠ‚ç‚¹ä¼˜åŒ–\nåªè¦è®¾è®¡åˆç†ï¼ŒKubernetes å®Œå…¨å¯ä»¥ç¨³å®šé«˜æ•ˆåœ°è°ƒåº¦æ•°ä¸‡ä¸ª Podã€‚\n","categories":"","description":"","excerpt":"å‡è®¾åœ¨ Kubernetes é›†ç¾¤ä¸­ä¸€æ¬¡æ€§è°ƒåº¦ 1 ä¸‡ä¸ª Podï¼Œ è¿™æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å¦‚æœç®¡ç†ä¸å½“ï¼Œå¯èƒ½ä¼šå¯¼è‡´è°ƒåº¦å™¨ç“¶é¢ˆã€API â€¦","ref":"/kubernetes-notes/cluster-optimization/massive-pod-scheduling/","tags":["Kubernetes"],"title":"å¤§è§„æ¨¡Podè°ƒåº¦ä¼˜åŒ–"},{"body":"1. Flannelç®€ä»‹ Flannel æ˜¯ä¸€ä¸ªç®€å•çš„ã€æ˜“äºä½¿ç”¨çš„ Kubernetes ç½‘ç»œæ’ä»¶ï¼Œç”¨äºä¸ºå®¹å™¨é›†ç¾¤æä¾›ç½‘ç»œåŠŸèƒ½ã€‚å®ƒä¸»è¦è§£å†³çš„æ˜¯ Kubernetes é›†ç¾¤ä¸­è·¨èŠ‚ç‚¹å®¹å™¨é—´é€šä¿¡çš„é—®é¢˜ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…ä¸€ä¸ªç‹¬ç«‹çš„å­ç½‘ï¼Œç¡®ä¿å®¹å™¨ä¹‹é—´å¯ä»¥ä½¿ç”¨è™šæ‹Ÿç½‘ç»œè¿›è¡Œæ— éšœç¢é€šä¿¡ã€‚\n1.1. Flannel çš„ç‰¹ç‚¹ä¸ä¼˜åŠ¿ æ˜“äºé…ç½®å’Œä½¿ç”¨ æä¾›ç®€å•çš„é…ç½®æ–‡ä»¶ï¼Œæ˜“äºé›†æˆåˆ° Kubernetes é›†ç¾¤ä¸­ã€‚ æ”¯æŒå¤šç§åç«¯ï¼ˆå¦‚ VXLANã€host-gwã€AWS VPC ç­‰ï¼‰ï¼Œçµæ´»æ»¡è¶³ä¸åŒç¯å¢ƒéœ€æ±‚ã€‚ è·¨èŠ‚ç‚¹ç½‘ç»œé€šä¿¡ ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…ç‹¬ç«‹çš„å­ç½‘ï¼Œå®¹å™¨ä¹‹é—´ä½¿ç”¨è™šæ‹Ÿç½‘ç»œ IP ç›´æ¥é€šä¿¡ï¼Œè€Œæ— éœ€ NATã€‚ è½»é‡çº§è®¾è®¡ è¿è¡Œæ—¶èµ„æºå ç”¨å°‘ï¼Œé€‚åˆèµ„æºæœ‰é™çš„ç¯å¢ƒã€‚ ç¨³å®šå…¼å®¹æ€§å¼º æ”¯æŒå¤šç§ Linux å‘è¡Œç‰ˆï¼Œå…¼å®¹ Kubernetes å’Œ Dockerï¼Œé€‚åº”å¹¿æ³›çš„å®¹å™¨åŒ–åœºæ™¯ã€‚ å¤šåç«¯æ”¯æŒ æä¾› VXLANã€host-gwã€AWS VPCã€UDP ç­‰å¤šç§ç½‘ç»œåç«¯ï¼Œä»¥é€‚åº”ä¸åŒåœºæ™¯å’Œéœ€æ±‚ã€‚ 1.2. ä½¿ç”¨åœºæ™¯ ä¸­å°è§„æ¨¡ Kubernetes é›†ç¾¤ Flannel æ˜“äºéƒ¨ç½²å’Œç®¡ç†ï¼Œéå¸¸é€‚åˆä¸­å°è§„æ¨¡é›†ç¾¤ä½¿ç”¨ã€‚ è·¨èŠ‚ç‚¹å®¹å™¨é€šä¿¡ åœ¨éœ€è¦å®¹å™¨é—´æ— éšœç¢é€šä¿¡çš„åœºæ™¯ä¸­ï¼ŒFlannel æä¾›å¯é çš„è™šæ‹Ÿç½‘ç»œæ”¯æŒã€‚ éé«˜æ€§èƒ½æ•æ„Ÿçš„åœºæ™¯ ç”±äº Flannel ä½¿ç”¨å°åŒ…å°è£…æŠ€æœ¯ï¼ˆå¦‚ VXLANï¼‰ï¼Œåœ¨æ€§èƒ½è¦æ±‚ä¸æ˜¯ç‰¹åˆ«é«˜çš„åœºæ™¯ä¸­éå¸¸é€‚ç”¨ã€‚ æ··åˆäº‘/å¤šäº‘éƒ¨ç½² Flannel çš„å¤šåç«¯æ”¯æŒå’Œçµæ´»é…ç½®ï¼Œä½¿å…¶åœ¨å¤šç§åŸºç¡€è®¾æ–½ä¸­æ˜“äºéƒ¨ç½²ã€‚ 1.3. Flannel çš„å±€é™æ€§ å°½ç®¡ Flannel æ˜“ç”¨ä¸”è½»é‡ï¼Œä½†å®ƒä¹Ÿå­˜åœ¨ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼š\næ€§èƒ½é™åˆ¶ ä½¿ç”¨ VXLAN æˆ– UDP åç«¯æ—¶ï¼Œç”±äºå°åŒ…å’Œè§£å°åŒ…æ“ä½œä¼šæ¶ˆè€—é¢å¤–èµ„æºï¼Œç½‘ç»œæ€§èƒ½å¯èƒ½ä¸å¦‚ç›´æ¥è·¯ç”±çš„æ–¹æ¡ˆï¼ˆå¦‚ Calico çš„ BGPï¼‰ã€‚ åœ¨é«˜æµé‡æˆ–ä½å»¶è¿Ÿåœºæ™¯ä¸‹ï¼ŒFlannel å¯èƒ½ä¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚ ç¼ºä¹é«˜çº§ç½‘ç»œåŠŸèƒ½ ä¸æ”¯æŒç½‘ç»œç­–ç•¥ï¼ˆNetwork Policyï¼‰åŠŸèƒ½ï¼Œæ— æ³•å®ç°ç»†ç²’åº¦çš„è®¿é—®æ§åˆ¶ã€‚ å¯¹äºéœ€è¦å¤æ‚ç½‘ç»œåŠŸèƒ½ï¼ˆå¦‚æµé‡åŠ å¯†ã€å¤šç§Ÿæˆ·éš”ç¦»ï¼‰çš„åœºæ™¯ï¼ŒCalico æˆ– Cilium æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚ ä¾èµ– etcd Flannel å¼ºä¾èµ–äº etcdã€‚å¦‚æœ etcd å‡ºç°æ•…éšœï¼Œå¯èƒ½å½±å“ç½‘ç»œç®¡ç†å’Œå­ç½‘åˆ†é…ã€‚ éœ€è¦é¢å¤–æ³¨æ„ etcd çš„é«˜å¯ç”¨æ€§å’Œæ€§èƒ½ã€‚ è¿ç»´å¤æ‚åº¦éšç€è§„æ¨¡å¢é•¿ éšç€é›†ç¾¤è§„æ¨¡æ‰©å¤§ï¼ˆå¦‚ 1000+ èŠ‚ç‚¹ï¼‰ï¼ŒFlannel çš„èµ„æºæ¶ˆè€—å’Œé…ç½®å¤æ‚åº¦å¯èƒ½å¢åŠ ï¼Œä¸å¦‚æ›´é«˜æ€§èƒ½çš„ç½‘ç»œæ–¹æ¡ˆã€‚ 2. Flannel çš„æ¶æ„ä¸é…ç½® flannelçš„æ¶æ„æ¯”è¾ƒç®€å•ï¼Œåªæœ‰æ¯ä¸ªèŠ‚ç‚¹ä¸€ä¸ªçš„flanneldç»„ä»¶ï¼Œé€šè¿‡daemonsetéƒ¨ç½²ï¼Œå¹¶æ²¡æœ‰è·Ÿcalicoæˆ–ciliumçš„æ¶æ„ä¸­æœ‰ä¸­æ§ç»„ä»¶ã€‚å…¶ä»–ç»„ä»¶åˆ™ä½¿ç”¨k8sçš„etcdã€‚\nflanneld ç»„ä»¶ æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œä¸€ä¸ª flanneld æœåŠ¡ï¼Œè´Ÿè´£ç®¡ç†è¯¥èŠ‚ç‚¹çš„ç½‘ç»œé…ç½®å’Œæ•°æ®å°åŒ…è§£å°åŒ…ã€‚ etcd é›†æˆ Flannel ä½¿ç”¨ etcd å­˜å‚¨ç½‘ç»œé…ç½®å’Œå­ç½‘åˆ†é…ä¿¡æ¯ã€‚ æ‰€æœ‰èŠ‚ç‚¹é€šè¿‡ etcd åè°ƒåˆ†é…ç½‘ç»œèµ„æºã€‚ ç½‘ç»œåç«¯ Flannel æ”¯æŒå¤šç§åç«¯æŠ€æœ¯ï¼Œå¦‚ VXLANã€UDPã€host-gw ç­‰ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©ã€‚ Kubernetes é›†æˆ Flannel é€šè¿‡ Kubernetes çš„ CNI æ’ä»¶æ¥å£æ— ç¼é›†æˆï¼Œç¡®ä¿ä¸ Kubernetes ç½‘ç»œéœ€æ±‚çš„å…¼å®¹æ€§ã€‚ 2.1. éƒ¨ç½²flannel é€šè¿‡ä»¥ä¸‹çš„yamlæ–‡ä»¶å¯ä»¥å¿«é€Ÿçš„éƒ¨ç½²flannelç»„ä»¶\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml éƒ¨ç½²å®Œæˆåä¼šç”Ÿæˆé»˜è®¤é…ç½®ï¼škube-flannel-cfgï¼Œ å…¶ä¸­é»˜è®¤ä½¿ç”¨vxlançš„åç«¯æ¨¡å¼ã€‚\napiVersion: v1 kind: ConfigMap data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" // é»˜è®¤ä¸ºvxlançš„æ¨¡å¼ } } 2.2. èŠ‚ç‚¹é…ç½® åœ¨/etc/cni/net.dè·¯å¾„ä¸‹ä¼šç”Ÿæˆflannelçš„cnié…ç½®ã€‚\n{ \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } åŒæ—¶åœ¨èŠ‚ç‚¹ä¼šç”Ÿæˆä¸€ä¸ªå­ç½‘é…ç½®/var/run/flannel/subnet.env\nFLANNEL_NETWORK=10.244.0.0/16 # æ•´ä¸ªé›†ç¾¤çš„Podç½‘æ®µ FLANNEL_SUBNET=10.244.3.1/24 # è¯¥èŠ‚ç‚¹çš„å­ç½‘Podç½‘æ®µ FLANNEL_MTU=1450 FLANNEL_IPMASQ=true èŠ‚ç‚¹ä¼šç”Ÿæˆcni0å’Œflannel.1çš„ç½‘å¡ï¼Œå…¶ä¸­ç½‘å¡çš„ç½‘æ®µè·Ÿè¯¥èŠ‚ç‚¹çš„FLANNEL_SUBNETç½‘æ®µä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™éœ€è¦é‡å»ºç½‘å¡ã€‚\nflannel.1ï¼šèŠ‚ç‚¹ç½‘å…³ï¼Œ10.244.3.0 cni0: 10.244.3.1 FLANNEL_SUBNET=10.244.3.1/24 cni0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1450 inet 10.244.3.1 netmask 255.255.255.0 broadcast 10.244.3.255 inet6 fe80::828:abff:fe83:34ac prefixlen 64 scopeid 0x20\u003clink\u003e ether 0a:28:ab:83:34:ac txqueuelen 1000 (Ethernet) RX packets 16220840959 bytes 2329280828193 (2.3 TB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 16382181068 bytes 43297103465563 (43.2 TB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 flannel.1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1450 inet 10.244.3.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::d875:a3ff:fe8b:1e64 prefixlen 64 scopeid 0x20\u003clink\u003e ether da:75:a3:8b:1e:64 txqueuelen 0 (Ethernet) RX packets 225837271 bytes 33216374045 (33.2 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 204036495 bytes 396891087255 (396.8 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 3. Flannelç½‘ç»œåŸç† 3.1. åŸç†å›¾ å›¾ä¸­docker0ç­‰ä»·äºcni0, flannel0ç­‰ä»·äºflannel.1ç½‘å¡\nå…³é”®ç½‘å¡å’Œè·¯ç”±è¡¨çš„è§’è‰²\ncni0 ç½‘æ¡¥ æ˜¯ä¸€ä¸ª Linux ç½‘æ¡¥ï¼Œè¿æ¥åŒä¸€èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod ç½‘ç»œæ¥å£ï¼ˆveth å¯¹ï¼‰ã€‚ å°†æ•°æ®åŒ…ä» Pod è½¬å‘åˆ°æœ¬åœ°çš„å…¶ä»– Pod æˆ–ä¸Šäº¤ç»™ flannel.1ã€‚ flannel.1 ç½‘å¡ æ˜¯ä¸€ä¸ªè™šæ‹Ÿç½‘å¡ï¼ˆVXLAN æ¥å£ï¼‰ï¼Œç”¨äºå°è£…å’Œè§£å°è£…è·¨èŠ‚ç‚¹çš„ Pod æ•°æ®åŒ…ã€‚ è¿æ¥åˆ°ç‰©ç†ç½‘ç»œï¼Œé€šè¿‡å°è£…çš„æ–¹å¼å°†æ•°æ®åŒ…å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹ã€‚ è·¯ç”±è¡¨ å®šä¹‰äº†å¦‚ä½•è½¬å‘æ•°æ®åŒ…ï¼ŒåŒ…æ‹¬ Pod å­ç½‘çš„è·¯ç”±è§„åˆ™å’Œé»˜è®¤è·¯ç”±ã€‚ Flannel ä¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹é…ç½®è·¯ç”±è¡¨ï¼Œä½¿å¾—æœ¬åœ° Pod çš„å­ç½‘å¯ä»¥é€šè¿‡ cni0 è®¿é—®ï¼Œè¿œç¨‹ Pod çš„å­ç½‘é€šè¿‡ flannel.1 è®¿é—®ã€‚ 3.2. æ•°æ®åŒ…è·¯å¾„ 3.2.1. æ•°æ®åŒ…ç¯å¢ƒ å‡è®¾æœ‰ä»¥ä¸‹ç¯å¢ƒï¼š\nNode Aï¼šå­ç½‘ 10.244.1.0/24ï¼ŒPod1 çš„ IP æ˜¯ 10.244.1.2ã€‚ Node Bï¼šå­ç½‘ 10.244.2.0/24ï¼ŒPod2 çš„ IP æ˜¯ 10.244.2.3ã€‚ ä¸¤ä¸ªèŠ‚ç‚¹é€šè¿‡ç‰©ç†ç½‘ç»œäº’è”ã€‚ å…¶ä¸­èŠ‚ç‚¹è·¯ç”±è¡¨ä¿¡æ¯å¦‚ä¸‹ï¼š\nA èŠ‚ç‚¹ï¼ˆPod IPï¼š10.244.1.2ï¼‰ï¼š\n# ç‰©ç†æœºè·¯ç”± default via \u003cç‰©ç†æœºç½‘å…³\u003e dev bond0 proto static \u003cç‰©ç†æœºç›®æ ‡ç½‘æ®µ\u003e dev bond0 proto kernel scope link src \u003cæœ¬æœºèŠ‚ç‚¹IP\u003e # flannelè·¯ç”± 10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink 10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1 #æœ¬åœ°å­ç½‘ï¼Œç›´æ¥é€šè¿‡ cni0 å¤„ç†ã€‚ 10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink # è¿œç¨‹å­ç½‘ï¼Œæ•°æ®åŒ…é€šè¿‡ flannel.1 å°è£… BèŠ‚ç‚¹ï¼ˆPod IPï¼š10.244.2.3ï¼‰ï¼š\n# ç‰©ç†æœºè·¯ç”± default via \u003cç‰©ç†æœºç½‘å…³\u003e dev bond0 proto static \u003cç‰©ç†æœºç›®æ ‡ç½‘æ®µ\u003e dev bond0 proto kernel scope link src \u003cæœ¬æœºèŠ‚ç‚¹IP\u003e # flannelè·¯ç”± 10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink # è¿œç¨‹å­ç½‘ï¼Œæ•°æ®åŒ…é€šè¿‡ flannel.1 å°è£… 10.244.2.0/24 dev cni0 proto kernel scope link src 10.244.2.1 #æœ¬åœ°å­ç½‘ï¼Œç›´æ¥é€šè¿‡ cni0 å¤„ç†ã€‚ 3.2.1. æ•°æ®è·¯å¾„ 1. ä» Pod å‘å‡ºçš„æ•°æ®åŒ…\n(1) Pod1 å‘é€æ•°æ®åŒ…\nPod1 å‘å¾€ Pod2 çš„æ•°æ®åŒ…ï¼š æº IPï¼š10.244.1.2 ç›®æ ‡ IPï¼š10.244.2.3 æ•°æ®åŒ…é€šè¿‡ Pod1 çš„ veth è®¾å¤‡å‘é€åˆ°æœ¬åœ°çš„ cni0 ç½‘æ¡¥ã€‚ 2. cni0 ç½‘æ¡¥å¤„ç†\nåˆ¤æ–­ç›®æ ‡ IP å±äºå“ªä¸ªå­ç½‘ï¼š æœ¬èŠ‚ç‚¹å­ç½‘ï¼ˆ10.244.1.0/24ï¼‰ï¼šç›´æ¥è½¬å‘åˆ°å¯¹åº”çš„ vethã€‚ å…¶ä»–å­ç½‘ï¼ˆ10.244.2.0/24ï¼‰ï¼šè·¯ç”±è¡¨æŒ‡å‘ flannel.1ã€‚ åœ¨æœ¬ä¾‹ä¸­ï¼Œç›®æ ‡ IP å±äº 10.244.2.0/24ï¼Œå› æ­¤æ•°æ®åŒ…é€šè¿‡è·¯ç”±è§„åˆ™è½¬å‘åˆ° flannel.1ã€‚ 3. flannel.1 ç½‘å¡å°è£…\n(1) æ•°æ®å°è£…\nFlannel ä»£ç†ï¼ˆflanneldï¼‰ä¼šæ£€æµ‹ç›®æ ‡å­ç½‘å±äºè¿œç¨‹èŠ‚ç‚¹ï¼Œè§¦å‘å°è£…æµç¨‹ã€‚ æ•°æ®åŒ…å°è£…ä¸º VXLAN åŒ…ï¼Œå¤–å±‚ IP æ ‡å¤´ï¼š æº IPï¼šNode A çš„ç‰©ç† IPï¼ˆä¾‹å¦‚ 192.168.1.1ï¼‰ã€‚ ç›®æ ‡ IPï¼šNode B çš„ç‰©ç† IPï¼ˆä¾‹å¦‚ 192.168.1.2ï¼‰ã€‚ VXLAN Headerï¼šæ ‡è®°è™šæ‹Ÿç½‘ç»œ ID å’Œå…¶ä»–ä¿¡æ¯ã€‚ (2) è·¯ç”±è½¬å‘\nå°è£…åçš„æ•°æ®åŒ…é€šè¿‡ä¸»æœºçš„ç‰©ç†ç½‘å¡ï¼ˆå¦‚ eth0ï¼‰å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹ã€‚ 4. åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹ (Node B)\n(1) flannel.1 æ¥æ”¶æ•°æ®åŒ…\nNode B çš„ç‰©ç†ç½‘å¡æ¥æ”¶å°è£…çš„ VXLAN æ•°æ®åŒ…ï¼Œäº¤ç”± flannel.1ã€‚ flannel.1 è§£å°æ•°æ®åŒ…ï¼Œè¿˜åŸå‡ºåŸå§‹çš„ Pod æ•°æ®åŒ…ï¼š æº IPï¼š10.244.1.2 ç›®æ ‡ IPï¼š10.244.2.3 (2) è·¯ç”±åˆ° cni0\næ ¹æ®è·¯ç”±è¡¨ï¼Œç›®æ ‡å­ç½‘ 10.244.2.0/24 å±äºæœ¬èŠ‚ç‚¹ï¼Œé€šè¿‡ cni0 è½¬å‘æ•°æ®åŒ…ã€‚ cni0 æ ¹æ®ç›®æ ‡ IPï¼Œæ‰¾åˆ° Pod2 çš„ veth è®¾å¤‡ã€‚ 5. æ•°æ®åŒ…åˆ°è¾¾ç›®æ ‡ Pod\næ•°æ®åŒ…æœ€ç»ˆé€šè¿‡ cni0 ç½‘æ¡¥é€åˆ° Pod2 çš„ veth æ¥å£ï¼ŒPod2 æ¥æ”¶åˆ°æ¥è‡ª Pod1 çš„é€šä¿¡ã€‚ 3.2.3. æ•°æ®æµæ€»ç»“ Pod1 æ•°æ®åŒ…å…ˆè¿›å…¥æœ¬åœ°çš„ cni0 ç½‘æ¡¥ã€‚ cni0 ç½‘æ¡¥é€šè¿‡è·¯ç”±è¡¨ï¼Œå‘ç°ç›®æ ‡ IP å±äºå…¶ä»–å­ç½‘ï¼Œäº¤ç”± flannel.1ã€‚ flannel.1 å°è£…æ•°æ®åŒ…ï¼Œå¹¶é€šè¿‡ç‰©ç†ç½‘å¡å‘å¾€ç›®æ ‡èŠ‚ç‚¹ã€‚ ç›®æ ‡èŠ‚ç‚¹çš„ flannel.1 è§£å°æ•°æ®åŒ…ï¼Œäº¤ç»™ cni0ã€‚ cni0 ç½‘æ¡¥å°†æ•°æ®åŒ…è½¬å‘åˆ°ç›®æ ‡ Pod çš„ vethã€‚ é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä¸åŒèŠ‚ç‚¹çš„ Pod å®ç°äº†é€æ˜çš„äº’é€šã€‚\n4. æ€»ç»“ Flannelæ˜¯ä¸€ä¸ªéå¸¸ç®€å•ï¼Œç¨³å®šçš„CNIæ’ä»¶ï¼Œå…¶ä¸­éƒ¨ç½²å’Œé…ç½®æ–¹å¼éƒ½éå¸¸ç®€å•ï¼Œç½‘ç»œåŸç†ä¹Ÿç®€å•ï¼Œå‡ºç°é—®é¢˜æ’æŸ¥æ¯”è¾ƒæ–¹ä¾¿ã€‚ç‰¹åˆ«é€‚åˆk8sé›†ç¾¤è§„æ¨¡ä¸å¤§ï¼ˆ1000ä¸ªèŠ‚ç‚¹ä»¥å†…ï¼‰ï¼Œç½‘ç»œæ€§èƒ½è¦æ±‚ä¸æ˜¯éå¸¸ä¸¥æ ¼ï¼Œä¸”å›¢é˜Ÿä¸­ç½‘ç»œç›¸å…³äººå‘˜è¾ƒå°‘ä¸”æ— æ³•æ”¯æŒç»´æŠ¤å¤æ‚ç½‘ç»œæ’ä»¶çš„å›¢é˜Ÿä½¿ç”¨ã€‚å› ä¸ºé€‰æ‹©æ–¹æ¡ˆæœ‰ä¸€ä¸ªåŸºæœ¬çš„è€ƒè™‘ç‚¹æ˜¯è¯¥æ–¹æ¡ˆç¨³å®šä¸”å›¢é˜Ÿä¸­æœ‰äººå¯ç»´æŠ¤ï¼Œè€ŒFlannelæ˜¯ä¸€ä¸ªç»´æŠ¤æˆæœ¬ç›¸å¯¹æ¯”è¾ƒä½çš„ç½‘ç»œæ–¹æ¡ˆã€‚\nå‚è€ƒï¼š\nhttps://github.com/flannel-io/flannel ","categories":"","description":"","excerpt":"1. Flannelç®€ä»‹ Flannel æ˜¯ä¸€ä¸ªç®€å•çš„ã€æ˜“äºä½¿ç”¨çš„ Kubernetes ç½‘ç»œæ’ä»¶ï¼Œç”¨äºä¸ºå®¹å™¨é›†ç¾¤æä¾›ç½‘ç»œåŠŸèƒ½ã€‚å®ƒä¸»è¦è§£å†³çš„ â€¦","ref":"/kubernetes-notes/network/flannel/flannel/","tags":["Kubernetes","CNI"],"title":"Flannelä»‹ç»"},{"body":"1. Ciliumç®€ä»‹ Cilium æ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨ç½‘ç»œæ’ä»¶ï¼ˆCNIï¼‰ï¼Œä¸“ä¸º Kubernetes å’Œäº‘åŸç”Ÿç¯å¢ƒè®¾è®¡ï¼ŒåŸºäº eBPFï¼ˆExtended Berkeley Packet Filterï¼‰ å®ç°é«˜æ€§èƒ½ã€å¯æ‰©å±•çš„ç½‘ç»œå’Œå®‰å…¨åŠŸèƒ½ã€‚å®ƒæ”¯æŒå¾®æœåŠ¡é—´çš„ç»†ç²’åº¦æµé‡æ§åˆ¶ï¼Œèƒ½å¤Ÿåœ¨ L3/L4/L7 å±‚æä¾›ç½‘ç»œç­–ç•¥ï¼ŒåŒæ—¶å…·æœ‰å¼ºå¤§çš„å¯è§‚æµ‹æ€§å·¥å…·ï¼ˆå¦‚ Hubbleï¼‰ä»¥å¸®åŠ©è¿ç»´äººå‘˜ç›‘æ§å’Œä¼˜åŒ–æµé‡ã€‚\n1.1. æ ¸å¿ƒç‰¹æ€§ åŸºäº eBPF çš„é«˜æ€§èƒ½æ•°æ®å¹³é¢\neBPFï¼šCilium é€šè¿‡ eBPF åœ¨ Linux å†…æ ¸ä¸­ç›´æ¥è¿è¡Œæ•°æ®åŒ…å¤„ç†é€»è¾‘ï¼Œé¿å…äº†å†…æ ¸ä¸ç”¨æˆ·æ€çš„é¢‘ç¹åˆ‡æ¢ï¼Œå¤§å¹…æé«˜äº†æ€§èƒ½ã€‚\né«˜æ•ˆæµé‡è½¬å‘ï¼šæ”¯æŒ BPF çš„å¿«é€Ÿè·¯å¾„ä¼˜åŒ–ï¼ˆZero-Copy è½¬å‘ï¼‰ï¼Œåœ¨é«˜æµé‡ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ã€‚\nå¤šå±‚ç½‘ç»œç­–ç•¥\nL3/L4 ç­–ç•¥ï¼šåŸºäº IP å’Œç«¯å£çš„åŸºæœ¬æµé‡æ§åˆ¶ã€‚\nL7 ç­–ç•¥ï¼šæ”¯æŒåº”ç”¨å±‚åè®®ï¼ˆå¦‚ HTTPã€gRPCï¼‰çš„è®¿é—®æ§åˆ¶ï¼Œå¯ä»¥æ ¹æ®è¯·æ±‚è·¯å¾„ã€æ–¹æ³•æˆ–å†…å®¹è¿‡æ»¤æµé‡ã€‚\nå¾®æœåŠ¡å‹å¥½ï¼šç‰¹åˆ«é€‚åˆéœ€è¦ç»†ç²’åº¦ç½‘ç»œç­–ç•¥çš„å¾®æœåŠ¡æ¶æ„ã€‚\nå¯è§‚æµ‹æ€§\nHubbleï¼šCilium å†…ç½®çš„å¯è§‚æµ‹æ€§å¹³å°ï¼Œå¯ä»¥å®æ—¶ç›‘æ§æœåŠ¡é—´çš„ç½‘ç»œæµé‡ã€å»¶è¿Ÿå’Œé”™è¯¯ç‡ï¼Œå¸®åŠ©å¼€å‘å’Œè¿ç»´å›¢é˜Ÿå¿«é€Ÿå®šä½é—®é¢˜ã€‚\næµé‡è·¯å¾„è¿½è¸ªï¼šæ”¯æŒæµé‡è·¯å¾„çš„å…¨é“¾è·¯è¿½è¸ªï¼Œä¾¿äºæ’æŸ¥ç½‘ç»œç“¶é¢ˆæˆ–ç­–ç•¥å†²çªã€‚\næ‹“å±•æ€§\næ”¯æŒè‡ªå®šä¹‰ eBPF ç¨‹åºï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚æ‰©å±•ç½‘ç»œåŠŸèƒ½ã€‚\nä¸å…¶ä»–äº‘åŸç”Ÿå·¥å…·ï¼ˆå¦‚ Prometheusã€Grafanaã€Istioï¼‰æ— ç¼é›†æˆã€‚\nè·¨äº‘å’Œæ··åˆäº‘æ”¯æŒ\næ”¯æŒ Kubernetes é›†ç¾¤çš„å¤šç½‘ç»œç¯å¢ƒï¼Œä¾‹å¦‚åœ¨è·¨äº‘å’Œæ··åˆäº‘åœºæ™¯ä¸­æä¾›ç»Ÿä¸€çš„ç½‘ç»œç­–ç•¥å’Œæµé‡æ§åˆ¶ã€‚ æœåŠ¡å‘ç°ä¸è´Ÿè½½å‡è¡¡\nå†…ç½®æœåŠ¡è´Ÿè½½å‡è¡¡ï¼šæä¾›å†…æ ¸çº§çš„æµé‡è´Ÿè½½å‡è¡¡ï¼Œæ¯”ä¼ ç»Ÿçš„ kube-proxy æ€§èƒ½æ›´é«˜ã€‚\næœåŠ¡å‘ç°æ”¯æŒï¼šå¯ä»¥ä¸ Kubernetes çš„ Service èµ„æºååŒå·¥ä½œï¼Œè‡ªåŠ¨å®ç° Pod é—´é€šä¿¡ã€‚\n1.2. é€‚ç”¨åœºæ™¯ äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„ åœ¨éœ€è¦ä¸¥æ ¼æµé‡æ§åˆ¶å’Œä¸°å¯Œå¯è§‚æµ‹æ€§çš„ç¯å¢ƒä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚ è¾¹ç¼˜è®¡ç®— ä½å»¶è¿Ÿéœ€æ±‚è¾ƒé«˜çš„åœºæ™¯ï¼Œå¦‚ CDN è¾¹ç¼˜èŠ‚ç‚¹å’Œ IoT ç¯å¢ƒã€‚ é«˜æµé‡é›†ç¾¤ é€‚ç”¨äºå¯¹ååé‡å’Œæ€§èƒ½è¦æ±‚æé«˜çš„ç”Ÿäº§é›†ç¾¤ï¼Œä¾‹å¦‚ç”µå•†ã€æµåª’ä½“å’Œé‡‘èæœåŠ¡ã€‚ å¤šç§Ÿæˆ·éš”ç¦» æ”¯æŒå¤šç§Ÿæˆ·ç½‘ç»œç¯å¢ƒä¸­çš„å¼ºéš”ç¦»éœ€æ±‚ã€‚ 1.3. Ciliumçš„å±€é™æ€§ è™½ç„¶ Cilium åœ¨ç°ä»£ Kubernetes ç½‘ç»œä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºç‚¹æˆ–éœ€è¦æ³¨æ„çš„é™åˆ¶ã€‚\n1. é«˜æ€§èƒ½æ¶ˆè€—\nå†…å­˜å’Œ CPU å ç”¨ï¼šç”±äºéœ€è¦åœ¨æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ Cilium Agent å’Œä¾èµ– eBPF åŠ è½½ç¨‹åºï¼Œå¯èƒ½å¯¹èŠ‚ç‚¹èµ„æºæ¶ˆè€—è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜æµé‡åœºæ™¯ä¸‹ã€‚ èµ„æºå¯†é›†åŠŸèƒ½ï¼šå¦‚ Hubbleï¼ˆCilium çš„å¯è§‚æµ‹æ€§å·¥å…·ï¼‰å¯èƒ½è¿›ä¸€æ­¥å¢åŠ èµ„æºä½¿ç”¨ã€‚ 2. ä¾èµ– Linux å†…æ ¸ç‰ˆæœ¬\neBPF é™åˆ¶ï¼šCilium ä¾èµ– eBPF æŠ€æœ¯ï¼Œå¯¹ Linux å†…æ ¸ç‰ˆæœ¬æœ‰è¦æ±‚ï¼Œæœ€ä½éœ€è¦ 4.19+ï¼Œéƒ¨åˆ†åŠŸèƒ½ï¼ˆå¦‚é«˜çº§è´Ÿè½½å‡è¡¡ï¼‰éœ€è¦ 5.x æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚ å†…æ ¸å‡çº§æˆæœ¬ï¼šåœ¨æŸäº›ç¯å¢ƒï¼ˆå¦‚è€æ—§ç³»ç»Ÿæˆ–ä¼ä¸šçº§ç¯å¢ƒï¼‰ä¸­ï¼Œå‡çº§å†…æ ¸å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ 3. å­¦ä¹ æ›²çº¿é™¡å³­\nå¤æ‚æ€§ï¼šCilium å¼•å…¥äº† eBPF æŠ€æœ¯ï¼Œä¸ä¼ ç»Ÿ CNIï¼ˆå¦‚ Calicoã€Flannelï¼‰ç›¸æ¯”æŠ€æœ¯æ›´å¤æ‚ï¼Œéœ€è¦æ·±å…¥ç†è§£ eBPFã€Linux å†…æ ¸ç½‘ç»œæ ˆå’Œ Cilium çš„é…ç½®æ–¹å¼ã€‚ 4. éƒ¨ç½²å’Œç®¡ç†å¤æ‚\né«˜çº§åŠŸèƒ½é…ç½®ç¹çï¼šå¦‚æ›¿ä»£ kube-proxy æˆ–é…ç½®é«˜æ€§èƒ½è´Ÿè½½å‡è¡¡ï¼Œéœ€è¦äº†è§£åº•å±‚ç½‘ç»œå’Œ Kubernetes çš„ç»†èŠ‚ã€‚ ç›‘æ§å’Œæ•…éšœæ’æŸ¥éš¾åº¦ï¼šeBPF ç¨‹åºè¿è¡Œåœ¨å†…æ ¸ä¸­ï¼Œæ’æŸ¥é—®é¢˜æ—¶æ— æ³•ç›´æ¥æŸ¥çœ‹ä¼ ç»Ÿç”¨æˆ·æ€æ—¥å¿—ï¼Œéœ€ä½¿ç”¨ä¸“ç”¨å·¥å…·å¦‚ bpftool æˆ– Hubbleã€‚ 2. Ciliuméƒ¨ç½² 2.1. éƒ¨ç½² éƒ¨ç½²æ–‡æ¡£å¯å‚è€ƒï¼š\nhttps://docs.cilium.io/en/stable/installation/k8s-install-helm/ https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#k8s-install-quick å¯ä»¥ä½¿ç”¨helmæ¥éƒ¨ç½²\né»˜è®¤å€¼ï¼š\né»˜è®¤çš„clusterPoolIPv4PodCIDRListæ˜¯10.0.0.0/8ï¼Œéœ€è¦ä¿è¯pod CIDRè·Ÿnodeçš„CIDRä¸å†²çªã€‚ é»˜è®¤ipam.modeæ˜¯cluster-poolï¼Œå¯ä¸ä¿®æ”¹è®¾ç½®ã€‚ helm repo add cilium https://helm.cilium.io/ helm repo update # éƒ¨ç½²cilium kubectl create ns cilium-system || true helm install cilium cilium/cilium --namespace cilium-system \\ --set ipam.mode=cluster-pool \\ --set ipam.operator.clusterPoolIPv4PodCIDRList=\"10.244.0.0/16\" \\ --set ipam.operator.clusterPoolIPv4MaskSize=24 æˆ–è€…ä½¿ç”¨Cilium CLI éƒ¨ç½²\n# å®‰è£…cilium cli curl -L --remote-name https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz tar xzvf cilium-linux-amd64.tar.gz sudo mv cilium /usr/local/bin # éƒ¨ç½²ciliumå¥—ä»¶ kubectl create ns cilium-system || true cilium install --namespace=cilium-system \\ --set ipam.mode=cluster-pool \\ --set ipam.operator.clusterPoolIPv4PodCIDRList=\"10.244.0.0/16\" \\ --set ipam.operator.clusterPoolIPv4MaskSize=24 2.2. éƒ¨ç½²æ£€æŸ¥ $ cilium status --wait --namespace cilium-system /Â¯Â¯\\ /Â¯Â¯\\__/Â¯Â¯\\ Cilium: OK \\__/Â¯Â¯\\__/ Operator: OK /Â¯Â¯\\__/Â¯Â¯\\ Hubble: disabled \\__/Â¯Â¯\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium-operator Running: 2 cilium Running: 2 Image versions cilium quay.io/cilium/cilium:v1.9.5: 2 cilium-operator quay.io/cilium/operator-generic:v1.9.5: 2 ç½‘ç»œè¿é€šæ€§æµ‹è¯•\n$ cilium connectivity test â„¹ï¸ Monitor aggregation detected, will skip some flow validation steps âœ¨ [k8s-cluster] Creating namespace for connectivity check... (...) --------------------------------------------------------------------------------------------------------------------- ğŸ“‹ Test Report --------------------------------------------------------------------------------------------------------------------- âœ… 69/69 tests successful (0 warnings) 2.3. Cilium IPAM æ¨¡å¼ Cilium æ”¯æŒä»¥ä¸‹ä¸¤ç§å¸¸è§çš„ IPAM æ¨¡å¼ï¼š\n2.3.1. Cluster-Pool æ¨¡å¼ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰ï¼š ç”± Cilium è‡ªå·±ç®¡ç† Pod IP åœ°å€èŒƒå›´ã€‚ å¯ä»¥åœ¨éƒ¨ç½²æ—¶æŒ‡å®š CIDR èŒƒå›´ã€‚ é«˜çµæ´»æ€§ï¼šæ”¯æŒä¸ºä¸åŒèŠ‚ç‚¹æˆ–åŒºåŸŸå®šä¹‰ç‹¬ç«‹çš„ IP åœ°å€æ± ã€‚ åŠ¨æ€ç®¡ç†ï¼šå¯ä»¥åŠ¨æ€è°ƒæ•´ IP åœ°å€æ± å¤§å°ï¼Œé€‚åº”é›†ç¾¤çš„æ‰©å±•éœ€æ±‚ã€‚ æ— å†²çªè®¾è®¡ï¼šé¿å…å› èŠ‚ç‚¹å¢åŠ æˆ–åˆ é™¤å¼•èµ·çš„ IP åœ°å€å†²çªã€‚ ä¼˜åŒ–æ€§èƒ½ï¼šå‡å°‘å¯¹ Kubernetes æ§åˆ¶å™¨çš„ä¾èµ–ï¼Œæé«˜ç½‘ç»œæ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚ ä¾èµ– Cilium Operatorï¼šéœ€è¦è¿è¡Œ Cilium Operator æ¥ç®¡ç† IP åœ°å€æ± ï¼Œå¢åŠ è¿ç»´å¤æ‚æ€§ã€‚ æ¨èåœºæ™¯\nå¤§å‹é›†ç¾¤ï¼ˆ\u003e 500 èŠ‚ç‚¹ï¼‰æˆ–è¶…å¤§è§„æ¨¡é›†ç¾¤ã€‚ éœ€è¦è·¨åŒºåŸŸæˆ–å¤šèŠ‚ç‚¹æ± çš„å¤æ‚ç½‘ç»œè§„åˆ’ã€‚ éœ€è¦åŠ¨æ€æ‰©å±• Pod IP åœ°å€èŒƒå›´çš„é›†ç¾¤ã€‚ é›†ç¾¤è¿è¡Œ Cilium å¹¶éœ€è¦å……åˆ†åˆ©ç”¨å…¶é«˜çº§åŠŸèƒ½ï¼ˆå¦‚ eBPF åŠ é€Ÿã€æœåŠ¡ç½‘æ ¼ç­‰ï¼‰ã€‚ 2.3.2. Kubernetes æ¨¡å¼ï¼š ä½¿ç”¨ Kubernetes è‡ªèº«çš„ IP åœ°å€åˆ†é…æ–¹å¼ï¼Œä¾‹å¦‚ç”± kube-controller-manager é€šè¿‡ --cluster-cidr è¿›è¡Œç®¡ç†ã€‚ Cilium ä» Kubernetes ä¸­è·å–åˆ†é…ç»™ Pod çš„ IP åœ°å€ã€‚ å…¼å®¹æ€§å¼ºï¼šé€‚é…å¤§å¤šæ•° CNI æ’ä»¶ï¼ˆåŒ…æ‹¬ Ciliumï¼‰ï¼Œæ— éœ€é¢å¤–é…ç½®ã€‚ çµæ´»æ€§æœ‰é™ï¼šä¸æ”¯æŒç»†ç²’åº¦çš„ IP åœ°å€æ± ç®¡ç†ï¼Œæ— æ³•ä¸ºç‰¹å®šèŠ‚ç‚¹æˆ–åŒºåŸŸåˆ†é…ç‰¹å®šçš„ IP èŒƒå›´ã€‚ æ¨èåœºæ™¯\nå°å‹æˆ–ä¸­å‹é›†ç¾¤ï¼ˆ\u003c 500 èŠ‚ç‚¹ï¼‰ã€‚ ç½‘ç»œè§„åˆ’è¾ƒä¸ºç®€å•ï¼Œæ— éœ€å¤æ‚çš„ IP åœ°å€ç®¡ç†ã€‚ éœ€è¦å¿«é€Ÿéƒ¨ç½²å¹¶ä¿æŒä¸ Kubernetes é»˜è®¤è¡Œä¸ºä¸€è‡´ã€‚ 3. Ciliumæ¶æ„åŠç»„ä»¶ä»‹ç» 3.1. æ¶æ„å›¾ å‚è€ƒå®˜ç½‘ï¼š https://docs.cilium.io/en/stable/overview/component-overview/\n3.2. æ ¸å¿ƒç»„ä»¶ 1. Cilium Agent\nè¿è¡Œåœ¨æ¯ä¸ª Kubernetes èŠ‚ç‚¹ä¸Šï¼Œæ˜¯ Cilium çš„æ ¸å¿ƒå®ˆæŠ¤è¿›ç¨‹ã€‚ åŠŸèƒ½ï¼š ä» Kubernetes API Server è·å–èµ„æºï¼ˆå¦‚ Podã€Service å’Œç½‘ç»œç­–ç•¥ï¼‰å¹¶è½¬æ¢ä¸º eBPF ç¨‹åºã€‚ åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šç®¡ç†å’ŒåŠ è½½ eBPF ç¨‹åºåˆ°å†…æ ¸ã€‚ å®ç° L3/L4 å’Œ L7 ç½‘ç»œç­–ç•¥ï¼Œå¹¶å°†ç­–ç•¥ä¸‹å‘åˆ°æ•°æ®å¹³é¢ã€‚ è´Ÿè´£æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡ï¼ˆå–ä»£ kube-proxyï¼‰ã€‚ 2. Cilium CLI\nå‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºå®‰è£…ã€é…ç½®å’Œè°ƒè¯• Ciliumã€‚ åŠŸèƒ½ï¼š é…ç½® Cilium çš„ç½‘ç»œç­–ç•¥ã€‚ æŸ¥çœ‹ Cilium å’Œ Hubble çš„è¿è¡ŒçŠ¶æ€ã€‚ è°ƒè¯• eBPF ç¨‹åºã€‚ 3. CNI Plugin\nå½“åœ¨èŠ‚ç‚¹ä¸Šè°ƒåº¦æˆ–ç»ˆæ­¢podæ—¶ï¼ŒKubernetesä¼šè°ƒç”¨CNIæ’ä»¶ï¼ˆcilium-cniï¼‰ã€‚å®ƒä¸èŠ‚ç‚¹çš„Cilium APIäº¤äº’ï¼Œè§¦å‘å¿…è¦çš„æ•°æ®è·¯å¾„é…ç½®ï¼Œä¸ºpodæä¾›ç½‘ç»œã€è´Ÿè½½å¹³è¡¡å’Œç½‘ç»œç­–ç•¥ã€‚ 4. Cilium Operator\nåœ¨ Kubernetes ä¸­è¿è¡Œçš„æ§åˆ¶å¹³é¢ç»„ä»¶ã€‚ åŠŸèƒ½ï¼š å¤„ç† IP åœ°å€ç®¡ç†ï¼šç®¡ç† Pod çš„ IP æ± ã€‚ ç»´æŠ¤ Cilium Agent ä¸ Kubernetes çš„é›†æˆã€‚ å¤„ç†èŠ‚ç‚¹é—´çš„æ‹“æ‰‘å˜åŒ–ï¼ˆå¦‚èŠ‚ç‚¹åŠ å…¥æˆ–ç¦»å¼€ï¼‰ã€‚ 3.3. å…¶ä»–ç»„ä»¶ 1. eBPF ç¨‹åº\nCilium çš„æ•°æ®å¹³é¢æ ¸å¿ƒï¼Œè¿è¡Œåœ¨ Linux å†…æ ¸ä¸­ã€‚ åŠŸèƒ½ï¼š è·¯ç”±å’Œè½¬å‘ï¼šåœ¨èŠ‚ç‚¹é—´å¤„ç† Pod çš„ç½‘ç»œæµé‡ã€‚ ç½‘ç»œç­–ç•¥ï¼šå®ç° L3/L4 å’Œ L7 çš„è®¿é—®æ§åˆ¶ã€‚ æœåŠ¡è´Ÿè½½å‡è¡¡ï¼šæä¾›ç±»ä¼¼ kube-proxy çš„æœåŠ¡è½¬å‘åŠŸèƒ½ï¼Œä½†æ€§èƒ½æ›´é«˜ã€‚ ç›‘æ§å’Œå¯è§‚æµ‹æ€§ï¼šæ”¶é›†ç½‘ç»œæµé‡æ•°æ®ï¼Œä¾› Hubble æˆ–å…¶ä»–å·¥å…·åˆ†æã€‚ 2. Hubble\nCilium çš„å¯è§‚æµ‹æ€§å¹³å°ï¼Œç”¨äºå®æ—¶ç›‘æ§å’Œåˆ†æç½‘ç»œæµé‡ã€‚ åŠŸèƒ½ï¼š æµé‡å¯è§†åŒ–ï¼šå±•ç¤ºæœåŠ¡é—´çš„æµé‡è·¯å¾„å’Œç»Ÿè®¡ã€‚ æµé‡è¿½è¸ªï¼šæ•è·å’Œè°ƒè¯•ç½‘ç»œé—®é¢˜ã€‚ å»¶è¿Ÿå’Œé”™è¯¯ç‡ç›‘æ§ã€‚ 4. Cilium çš„å·¥ä½œæµ åˆå§‹åŒ–ï¼š Cilium Agent å¯åŠ¨åï¼Œä¸ Kubernetes API Server å»ºç«‹è¿æ¥ï¼Œç›‘å¬èµ„æºå˜åŒ–ã€‚ ç­–ç•¥é…ç½®ï¼š ç”¨æˆ·å®šä¹‰çš„ NetworkPolicy æˆ– CiliumNetworkPolicy é€šè¿‡ Cilium Agent ä¼ é€’åˆ° eBPF ç¨‹åºã€‚ eBPF ç¨‹åºåœ¨å†…æ ¸ä¸­ç›´æ¥æ‰§è¡Œæµé‡æ§åˆ¶é€»è¾‘ã€‚ æµé‡å¤„ç†ï¼š æ•°æ®å¹³é¢é€šè¿‡ eBPF ç¨‹åºå¯¹ç½‘ç»œæµé‡è¿›è¡Œè·¯ç”±ã€ç­–ç•¥åŒ¹é…å’Œè´Ÿè½½å‡è¡¡ã€‚ ç›‘æ§å’Œåˆ†æï¼š eBPF ç¨‹åºæ”¶é›†æµé‡æ•°æ®ï¼Œå‘é€åˆ° Hubbleã€‚ Hubble å°†æ•°æ®å¯è§†åŒ–ï¼Œä¾›ç”¨æˆ·ç›‘æ§å’Œè°ƒè¯•ã€‚ å‚è€ƒï¼š\nhttps://docs.cilium.io/en/stable/ https://docs.cilium.io/en/stable/overview/component-overview/ https://docs.cilium.io/en/stable/network/concepts/ipam/ ","categories":"","description":"","excerpt":"1. Ciliumç®€ä»‹ Cilium æ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨ç½‘ç»œæ’ä»¶ï¼ˆCNIï¼‰ï¼Œä¸“ä¸º Kubernetes å’Œäº‘åŸç”Ÿç¯å¢ƒè®¾è®¡ï¼Œ â€¦","ref":"/kubernetes-notes/network/cilium/cilium/","tags":["Kubernetes","CNI"],"title":"Ciliumä»‹ç»"},{"body":"1. Calicoç®€ä»‹ Calico æ˜¯ä¸€ä¸ªå¼€æºçš„ç½‘ç»œå’Œç½‘ç»œå®‰å…¨è§£å†³æ–¹æ¡ˆï¼Œä¸»è¦ç”¨äº Kubernetes ç­‰å®¹å™¨ç¼–æ’ç³»ç»Ÿã€‚å®ƒé€šè¿‡æä¾›é«˜æ•ˆçš„ç½‘ç»œè¿æ¥å’Œå¼ºå¤§çš„å®‰å…¨æ§åˆ¶æ¥æ»¡è¶³å®¹å™¨åŒ–å’Œå¾®æœåŠ¡æ¶æ„çš„éœ€æ±‚ã€‚Calico ä»¥å…¶çµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œæ€§èƒ½è‘—ç§°ï¼Œæ˜¯è®¸å¤šä¼ä¸šå’Œäº‘åŸç”Ÿåº”ç”¨çš„é¦–é€‰ç½‘ç»œæ’ä»¶ã€‚\n1.1. ä¸»è¦åŠŸèƒ½å’Œç‰¹ç‚¹ ç½‘ç»œæ¶æ„ï¼š\nL3 è·¯ç”±æ¶æ„ï¼šCalico åŸºäºç¬¬ä¸‰å±‚ï¼ˆL3ï¼‰ç½‘ç»œæ„å»ºï¼Œä¸ä¾èµ–ä¼ ç»Ÿçš„è¦†ç›–ç½‘ç»œï¼ˆoverlay networkï¼‰ï¼Œä½¿å…¶å¯ä»¥åˆ©ç”¨ IP è·¯ç”±æ¥å®ç°è·¨èŠ‚ç‚¹çš„ç›´æ¥é€šä¿¡ã€‚ BGP æ”¯æŒï¼šCalico ä½¿ç”¨ BGPï¼ˆè¾¹ç•Œç½‘å…³åè®®ï¼‰æ¥åˆ†å‘å’ŒåŒæ­¥è·¯ç”±ä¿¡æ¯ï¼Œä½¿å¾—å®¹å™¨å’Œ Pod èƒ½å¤Ÿè·¨èŠ‚ç‚¹ç›´æ¥é€šä¿¡ï¼Œå‡å°‘ç½‘ç»œå»¶è¿Ÿï¼Œæå‡æ€§èƒ½ã€‚ æ”¯æŒå¤šç§åç«¯ï¼šé™¤äº†é»˜è®¤çš„ IP è·¯ç”±æ¨¡å¼ï¼ŒCalico ä¹Ÿæ”¯æŒ VXLANã€IPIPã€WireGuard å’Œ eBPF åç«¯ï¼Œé€‚åº”ä¸åŒçš„ç½‘ç»œç¯å¢ƒå’Œéœ€æ±‚ã€‚ ç½‘ç»œå®‰å…¨ï¼š\nç½‘ç»œç­–ç•¥ï¼šCalico æ”¯æŒæ ‡å‡†çš„ Kubernetes NetworkPolicyï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç­–ç•¥æ§åˆ¶ Pod ä¹‹é—´çš„é€šä¿¡æƒé™ã€‚ GlobalNetworkPolicyï¼šCalico æä¾› GlobalNetworkPolicyï¼Œå¯ä»¥å®ç°è·¨å‘½åç©ºé—´ã€è·¨é›†ç¾¤çš„ç»Ÿä¸€ç­–ç•¥ç®¡ç†ï¼Œç”¨äºå¤šç§Ÿæˆ·éš”ç¦»æˆ–æ›´é«˜çš„å®‰å…¨æ§åˆ¶ã€‚ æ”¯æŒ eBPFï¼šCalico åœ¨å¯ç”¨ eBPF æ¨¡å¼æ—¶ï¼Œå¯ä»¥é€šè¿‡ eBPF æä¾›æ›´é«˜æ•ˆçš„ç½‘ç»œæ•°æ®å¤„ç†ï¼Œå¹¶æ”¯æŒ L4 å±‚çš„è´Ÿè½½å‡è¡¡å’Œç½‘ç»œç­–ç•¥æ§åˆ¶ã€‚ å¯è§‚æµ‹æ€§å’Œå¯è§†åŒ–ï¼š\nCalico é›†æˆäº†å¤šç§å¯è§‚æµ‹æ€§å·¥å…·ï¼Œæ”¯æŒ Prometheus ç­‰ç›‘æ§ç³»ç»Ÿï¼Œå¹¶å¯ä»¥ç”Ÿæˆç½‘ç»œæµé‡å’Œç­–ç•¥çš„ç›‘æ§æŒ‡æ ‡ï¼Œå¸®åŠ©è¿ç»´äººå‘˜äº†è§£ç½‘ç»œçŠ¶å†µã€‚ æ”¯æŒä¸ EFKï¼ˆElasticsearch, Fluentd, Kibanaï¼‰é›†æˆï¼Œä¾¿äºå¯è§†åŒ–ç½‘ç»œæµé‡å’Œç­–ç•¥ã€‚ é«˜æ‰©å±•æ€§ï¼š\nCalico å¯ä»¥åœ¨ä¸åŒçš„åŸºç¡€è®¾æ–½ä¸­è¿è¡Œï¼ŒåŒ…æ‹¬æœ¬åœ°æ•°æ®ä¸­å¿ƒå’Œäº‘å¹³å°ï¼ˆAWSã€GCPã€Azureï¼‰ï¼Œå¹¶æ”¯æŒä¸ VM åŠè£¸é‡‘å±æœåŠ¡å™¨äº’è”ã€‚ ä½¿ç”¨ IP è·¯ç”±å’Œ BGPï¼Œä½¿å¾— Calico åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸­ä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½å’Œæ‰©å±•æ€§ã€‚ 1.2. é€‚ç”¨åœºæ™¯ å®¹å™¨åŒ–åº”ç”¨çš„ç½‘ç»œç®¡ç†ï¼šé€‚ç”¨äº Kubernetes ç­‰å®¹å™¨ç¼–æ’å¹³å°ï¼Œä¸ºå®¹å™¨æä¾›é«˜æ•ˆã€å®‰å…¨çš„ç½‘ç»œè¿æ¥ã€‚ æ··åˆäº‘å’Œå¤šé›†ç¾¤éƒ¨ç½²ï¼šCalico æ”¯æŒè·¨æ•°æ®ä¸­å¿ƒå’Œäº‘ç¯å¢ƒçš„å¤šé›†ç¾¤éƒ¨ç½²ï¼Œéå¸¸é€‚åˆæ··åˆäº‘å’Œå¤šç§Ÿæˆ·ç¯å¢ƒã€‚ ç½‘ç»œå®‰å…¨ï¼šå¯¹äºéœ€è¦ç»†ç²’åº¦çš„ç½‘ç»œå®‰å…¨ç­–ç•¥æ§åˆ¶å’Œå¤šç§Ÿæˆ·éš”ç¦»çš„åœºæ™¯ï¼ŒCalico æä¾›å¤šå±‚æ¬¡çš„å®‰å…¨æ§åˆ¶å’Œéš”ç¦»ã€‚ 1.3. Calicoçš„å±€é™æ€§ calicoä¹Ÿå­˜åœ¨ä¸€äº›ä¸è¶³å’Œå±€é™æ€§ã€‚\n1. å¯¹ç½‘ç»œæ‹“æ‰‘çš„ä¾èµ–\nBGP é…ç½®å¤æ‚æ€§ï¼š Calico é»˜è®¤ä½¿ç”¨ BGP åè®®è¿›è¡Œè·¯ç”±ï¼Œå¦‚æœç½‘ç»œç¯å¢ƒä¸­æ²¡æœ‰ç°æˆçš„ BGP æ”¯æŒï¼ˆå¦‚éç”Ÿäº§ç¯å¢ƒæˆ–ç½‘ç»œç®¡ç†å‘˜ç»éªŒä¸è¶³ï¼‰ï¼Œé…ç½®å¯èƒ½è¾ƒå¤æ‚ã€‚æ­¤å¤–ï¼ŒBGP çš„ç®¡ç†å¯¹éƒ¨åˆ†è¿ç»´äººå‘˜æœ‰ä¸€å®šé—¨æ§›ã€‚ å¯¹åº•å±‚ç½‘ç»œè¦æ±‚è¾ƒé«˜ï¼š Calico çš„æ— éš§é“è®¾è®¡ä¾èµ–åº•å±‚ç½‘ç»œçš„æ­£å¸¸è¿è¡Œã€‚å¦‚æœåº•å±‚ç½‘ç»œä¸æ”¯æŒé«˜æ•ˆçš„è·¯ç”±æˆ–ä¸èƒ½å¾ˆå¥½åœ°ç®¡ç†å¤šæ’­æµé‡ï¼Œå¯èƒ½ä¼šå½±å“æ•´ä½“ç½‘ç»œæ€§èƒ½ã€‚ 2. å¤§è§„æ¨¡é›†ç¾¤æ€§èƒ½é—®é¢˜\netcd è´Ÿè½½ï¼š åœ¨å¤§è§„æ¨¡ Kubernetes é›†ç¾¤ä¸­ï¼ˆå¦‚å‡ åƒä¸ªèŠ‚ç‚¹ï¼‰ï¼ŒCalico å¯¹ etcd çš„è®¿é—®é¢‘ç‡è¾ƒé«˜ï¼Œè¿™å¯èƒ½ä¼šç»™ etcd å¸¦æ¥è¾ƒå¤§å‹åŠ›ã€‚å°½ç®¡ Typha å¯ä»¥ç¼“è§£éƒ¨åˆ†é—®é¢˜ï¼Œä½†ä¾ç„¶å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚ è·¯ç”±è¡¨è†¨èƒ€ï¼š åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸­ï¼ŒBGP æ¨¡å¼ä¸‹æ¯ä¸ªèŠ‚ç‚¹éœ€è¦ç»´æŠ¤å¤§é‡è·¯ç”±ä¿¡æ¯ï¼ˆä¸é›†ç¾¤ä¸­ Pod çš„æ•°é‡ç›¸å…³ï¼‰ï¼Œå¯èƒ½å¯¼è‡´è·¯ç”±è¡¨è†¨èƒ€å¹¶å¯¹å†…å­˜å’Œ CPU èµ„æºäº§ç”Ÿè¾ƒå¤§å‹åŠ›ã€‚ 2. Calicoæ¶æ„å›¾ æ¥è‡ªå®˜ç½‘\n2.1. Calico æ¶æ„æ¦‚è§ˆ Calico é€šè¿‡ BGPï¼ˆè¾¹ç•Œç½‘å…³åè®®ï¼‰åœ¨ Kubernetes é›†ç¾¤èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œè·¯ç”±å¹¿æ’­ï¼Œæ— éœ€ä½¿ç”¨å¤æ‚çš„éš§é“åè®®ï¼ˆå¦‚ VXLAN æˆ– GREï¼‰ï¼Œè¿™ä½¿å¾— Calico çš„ç½‘ç»œæ€§èƒ½è¾ƒé«˜ã€‚å®ƒä¸»è¦ç”±ä»¥ä¸‹ç»„ä»¶ç»„æˆï¼š\ncalico-nodeï¼šè¿™æ˜¯ Calico çš„æ ¸å¿ƒç»„ä»¶ï¼Œè¿è¡Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œè´Ÿè´£è®¾ç½®è·¯ç”±ã€ç®¡ç† IP åœ°å€ï¼Œå¹¶é€šè¿‡ Felix å®ç°ç½‘ç»œç­–ç•¥çš„æ‰§è¡Œã€‚ BGP Daemon (Bird)ï¼šè´Ÿè´£èŠ‚ç‚¹ä¹‹é—´çš„è·¯ç”±ä¼ æ’­ï¼Œä»¥å®ç°ä¸åŒèŠ‚ç‚¹ Pod ä¹‹é—´çš„æµé‡è·¯ç”±ã€‚ Felixï¼šä½œä¸º Calico çš„ä¸»ä»£ç†ï¼Œè´Ÿè´£ç›‘æ§ etcd ä¸­å­˜å‚¨çš„ç½‘ç»œç­–ç•¥å¹¶å°†å…¶åº”ç”¨äºèŠ‚ç‚¹çš„ç½‘ç»œæ¥å£ï¼ŒåŒæ—¶è´Ÿè´£è®¾ç½® IP è·¯ç”±ã€ç®¡ç† ACLs ä»¥ç¡®ä¿æµé‡ç¬¦åˆç­–ç•¥ã€‚ Typhaï¼šå½“èŠ‚ç‚¹æ•°è¾ƒå¤šæ—¶ï¼Œå¯ä»¥é€šè¿‡ Typha ç»„ä»¶æ¥å‡å°‘ etcd çš„è®¿é—®è´Ÿè½½ã€‚å®ƒä¼šå°† etcd ä¸­çš„ç­–ç•¥å˜åŒ–ç¼“å­˜èµ·æ¥ï¼Œå¹¶åŒæ­¥ç»™æ¯ä¸ªèŠ‚ç‚¹çš„ Felix ä»£ç†ã€‚ etcdï¼šCalico ä½¿ç”¨ etcd ä½œä¸ºå­˜å‚¨åç«¯ï¼Œç”¨äºå­˜å‚¨ç½‘ç»œç­–ç•¥ã€IP æ± ç­‰é…ç½®ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ Kubernetes çš„ API Server ä½œä¸ºå­˜å‚¨åç«¯ï¼Œä¾¿äºé›†æˆã€‚ 2.2. Calico çš„æ ¸å¿ƒç»„ä»¶ calicoctlï¼šè¿™æ˜¯ Calico æä¾›çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºé…ç½®å’Œç®¡ç† Calico çš„èµ„æºï¼Œæ¯”å¦‚ IP æ± ã€ç­–ç•¥ã€ç½‘ç»œè®¾ç½®ç­‰ã€‚å¯ä»¥é€šè¿‡è¯¥å·¥å…·æŸ¥çœ‹ã€åˆ›å»ºã€æ›´æ–°å’Œåˆ é™¤ç½‘ç»œç­–ç•¥ã€‚ IPAM (IP Address Management)ï¼šCalico è‡ªå¸¦çš„ IP åœ°å€ç®¡ç†æ¨¡å—ï¼Œå¯ä»¥ä¸ºé›†ç¾¤ä¸­çš„ Pod è‡ªåŠ¨åˆ†é… IP åœ°å€ã€‚IPAM æ”¯æŒçµæ´»çš„ IP æ± è®¾ç½®ï¼Œå¯ä»¥å¯¹ä¸åŒçš„èŠ‚ç‚¹ã€å‘½åç©ºé—´æˆ–å·¥ä½œè´Ÿè½½åˆ†é…ç‰¹å®šçš„ IP èŒƒå›´ã€‚ 2.3. å·¥ä½œæµç¨‹ æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œçš„ calico-node ç»„ä»¶ä¼šå’Œå…¶ä»–èŠ‚ç‚¹è¿›è¡Œ BGP è·¯ç”±ä¿¡æ¯äº¤æ¢ï¼Œç¡®ä¿ä¸åŒèŠ‚ç‚¹çš„ Pod å¯ä»¥äº’ç›¸é€šä¿¡ã€‚ Felix ç»„ä»¶è´Ÿè´£å°†ç½‘ç»œç­–ç•¥çš„å®šä¹‰åº”ç”¨åˆ°å®é™…çš„ç½‘ç»œæ¥å£ä¸Šï¼Œä»¥ç¡®ä¿æµé‡ç¬¦åˆé¢„è®¾çš„ç­–ç•¥ã€‚ Typha ç»„ä»¶åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸­å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘ etcd çš„å‹åŠ›ï¼Œå¸®åŠ© Felix å¿«é€ŸåŒæ­¥ç½‘ç»œç­–ç•¥ã€‚ 2.4. Calico ç½‘ç»œç­–ç•¥ (Network Policy) Calico æ”¯æŒä¸°å¯Œçš„ç½‘ç»œç­–ç•¥ï¼Œç”¨äºå®šä¹‰ä¸åŒçš„ Pod æˆ–æœåŠ¡ä¹‹é—´çš„ç½‘ç»œè®¿é—®è§„åˆ™ã€‚ç½‘ç»œç­–ç•¥çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\nåŸºäºæ ‡ç­¾çš„ç­–ç•¥ï¼šå¯ä»¥æ ¹æ® Pod æˆ–å‘½åç©ºé—´çš„æ ‡ç­¾æ¥æ§åˆ¶ç½‘ç»œæµé‡çš„å…è®¸å’Œæ‹’ç»ã€‚ æ”¯æŒ Egress å’Œ Ingress ç­–ç•¥ï¼šä¸ä»…å¯ä»¥æ§åˆ¶è¿›å…¥ Pod çš„æµé‡ï¼Œè¿˜å¯ä»¥æ§åˆ¶ Pod å‘å‡ºçš„æµé‡ã€‚ çµæ´»çš„è§„åˆ™å®šä¹‰ï¼šæ”¯æŒåŸºäº IP åœ°å€ã€ç«¯å£ã€åè®®çš„è§„åˆ™é…ç½®ï¼Œèƒ½å¤Ÿç²¾ç»†åœ°æ§åˆ¶ç½‘ç»œæµé‡ã€‚ 3. Calicoç»„ä»¶åŠé…ç½® calicoçš„éƒ¨ç½²å¯å‚è€ƒï¼škubeadm-scripts/cni/install-calico.sh\néƒ¨ç½²å®Œæˆåå¯ä»¥åœ¨k8sé›†ç¾¤ä¸­çœ‹åˆ°ä»¥ä¸‹ç»„ä»¶ï¼š\nä¸­æ§ç»„ä»¶ï¼šcalico-kube-controllers\nèŠ‚ç‚¹ç»„ä»¶ï¼šcalico-node\ncalico-system calico-kube-controllers-8945657f7-ntbxm 1/1 Running 0 421d calico-system calico-node-2df8c 1/1 Running 0 421d calico-system calico-node-5vq6z 1/1 Running 0 421d calico-system calico-node-dpnkd 1/1 Running 0 421d calico-system calico-node-sms2h 1/1 Running 0 421d calico-system calico-node-w95l2 1/1 Running 0 414d 3.1. calico nodeè¿›ç¨‹æ ‘ \\_ /usr/local/bin/runsvdir -P /etc/service/enabled \\_ runsv confd | \\_ calico-node -confd \\_ runsv allocate-tunnel-addrs | \\_ calico-node -allocate-tunnel-addrs \\_ runsv monitor-addresses | \\_ calico-node -monitor-addresses \\_ runsv bird | \\_ bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg \\_ runsv felix | \\_ calico-node -felix \\_ runsv cni | \\_ calico-node -monitor-token \\_ runsv node-status-reporter | \\_ calico-node -status-reporter \\_ runsv bird6 \\_ bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg 3.2. calico-kube-controllersè¿›ç¨‹æ ‘ /usr/bin/kube-controllers 3.3. CNI calicoäºŒè¿›åˆ¶ cd /opt/cni/bin |-- calico |-- calico-ipam |-- install 3.4. CNI calicoé…ç½® 10-calico.conflist\ncd /etc/cni/net.d # cat 10-calico.conflist { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"log_file_path\": \"/var/log/calico/cni/cni.log\", \"datastore_type\": \"kubernetes\", \"nodename\": \"node1\", \"mtu\": 0, \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"snat\": true, \"capabilities\": {\"portMappings\": true} }, { \"type\": \"bandwidth\", \"capabilities\": {\"bandwidth\": true} } ] } calico-kubeconfig\n# Kubeconfig file for Calico CNI plugin. Installed by calico/node. apiVersion: v1 kind: Config clusters: - name: local cluster: server: https://10.96.0.1:443 certificate-authority-data: \"xxx\" users: - name: calico user: token:xxx contexts: - name: calico-context context: cluster: local user: calico å‚è€ƒï¼š\nhttps://docs.tigera.io/calico/latest/about/\nhttps://docs.tigera.io/calico/latest/reference/architecture/overview\nhttps://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart\nLive Migration from Flannel to Calico\n","categories":"","description":"","excerpt":"1. Calicoç®€ä»‹ Calico æ˜¯ä¸€ä¸ªå¼€æºçš„ç½‘ç»œå’Œç½‘ç»œå®‰å…¨è§£å†³æ–¹æ¡ˆï¼Œä¸»è¦ç”¨äº Kubernetes ç­‰å®¹å™¨ç¼–æ’ç³»ç»Ÿã€‚å®ƒé€šè¿‡æä¾›é«˜æ•ˆçš„ç½‘ â€¦","ref":"/kubernetes-notes/network/calico/calico/","tags":["Kubernetes","CNI"],"title":"Calicoä»‹ç»"},{"body":"1. knativeç®€ä»‹ knativeæ˜¯ä¸€ä¸ªå°†serverlessçš„èƒ½åŠ›æ‰©å±•åˆ°k8sä¸­çš„å¼€æºé¡¹ç›®ã€‚serverlessè®©å¼€å‘è€…æ— éœ€å…³æ³¨å®¹å™¨ã€é•œåƒã€è¿ç»´ç­‰äº‹é¡¹ï¼Œé›†ä¸­ç²¾åŠ›äºå¼€å‘ä»£ç æœ¬èº«ï¼Œå³å°†ä»£ç é€šè¿‡å…è¿ç»´çš„å½¢å¼äº¤ä»˜ç»™serverlesså¹³å°ã€‚ä»£ç ä¼šåœ¨è®¾å®šçš„æ¡ä»¶ä¸‹è¿è¡Œï¼Œå¹¶è‡ªåŠ¨å®ç°æ‰©ç¼©å®¹ã€‚\n2. knativeçš„ç»„ä»¶ knativeä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š\nbuild: å°†ä»£ç è½¬æ¢ä¸ºå®¹å™¨ï¼Œä¸»è¦åŒ…æ‹¬\nå°†æºä»£ç ä»gitä»“åº“æ‹‰å–ä¸‹æ¥ï¼Œå®‰è£…ç›¸å…³çš„ä¾èµ– æ„å»ºå®¹å™¨é•œåƒ å°†å®¹å™¨é•œåƒæ¨é€åˆ°é•œåƒä»“åº“ Servingï¼šåˆ›å»ºä¸€ä¸ªå¯ä¼¸ç¼©çš„éƒ¨ç½²ã€‚\né…ç½®å®šä¹‰äº†æœåŠ¡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬ç‰ˆæœ¬ç®¡ç†ï¼Œæ¯æ¬¡ä¿®æ”¹éƒ½åˆ›å»ºä¸€ä¸ªæ–°ç‰ˆæœ¬éƒ¨ç½²ï¼Œå¹¶ä¿ç•™æ—§ç‰ˆæœ¬ã€‚ çµæ´»çš„è·¯ç”±æ§åˆ¶ï¼Œå¯ä»¥æ§åˆ¶ç™¾åˆ†æ¯”çš„è·¯ç”±åˆ°æ–°ç‰ˆæœ¬å’Œæ—§ç‰ˆæœ¬æœåŠ¡ã€‚ è‡ªåŠ¨å¼¹æ€§ä¼¸ç¼©ï¼Œå¯ä»¥å¿«é€Ÿåˆ›å»ºä¸Šåƒä¸ªå®ä¾‹æˆ–å¿«é€Ÿè°ƒæ•´å®ä¾‹æ•°ä¸º0ã€‚ Eventingï¼šäº‹ä»¶è§¦å‘ï¼Œé€šè¿‡å®šä¹‰å„ç§äº‹ä»¶ä½¿ç”¨knativeè‡ªåŠ¨æ¥å®Œæˆè¿™äº›ä»»åŠ¡ï¼Œè€Œæ— éœ€æ‰‹åŠ¨ç¼–å†™è„šæœ¬ã€‚\n3. éƒ¨ç½²knative éƒ¨ç½²knativeä¸»è¦æ˜¯éƒ¨ç½²Servingå’ŒEventingä¸¤ä¸ªç»„ä»¶ï¼Œå¯ä»¥å•ç‹¬éƒ¨ç½²ä¹Ÿå¯ä»¥åŒæ—¶éƒ¨ç½²ã€‚\n3.1. éƒ¨ç½²Serving éƒ¨ç½²CRD\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-crds.yaml éƒ¨ç½²Serving\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-core.yaml éƒ¨ç½²HPA autoscalingï¼ˆå¯é€‰ï¼‰\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-hpa.yaml æŸ¥çœ‹éƒ¨ç½²ç»“æœ\n# kgdep -n knative-serving NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/activator 1/1 1 1 107s deployment.apps/autoscaler 1/1 1 1 107s deployment.apps/autoscaler-hpa 1/1 1 1 107s deployment.apps/controller 1/1 1 1 107s deployment.apps/domain-mapping 1/1 1 1 107s deployment.apps/domainmapping-webhook 1/1 1 1 107s deployment.apps/webhook 1/1 1 1 107s 3.2. éƒ¨ç½²Eventing éƒ¨ç½²CRD\nkubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.10.0/eventing-crds.yaml éƒ¨ç½²Eventing\nkubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.10.0/eventing-core.yaml æŸ¥çœ‹éƒ¨ç½²ç»“æœ\n# kgdep -n knative-eventing NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1/1 1 1 2m13s eventing-webhook 1/1 1 1 2m13s pingsource-mt-adapter 0/0 0 0 2m13s 4. éƒ¨ç½²knativeå®¢æˆ·ç«¯ wget https://github.com/knative/client/releases/download/knative-v1.10.0/kn-linux-amd64 chmod +x kn-linux-amd64 mv kn-linux-amd64 /usr/bin/kn knå‘½ä»¤ï¼š\nkn kn is the command line interface for managing Knative Serving and Eventing resources Find more information about Knative at: https://knative.dev Serving Commands: service Manage Knative services revision Manage service revisions route List and describe service routes domain Manage domain mappings container Manage service's containers (experimental) Eventing Commands: source Manage event sources broker Manage message brokers trigger Manage event triggers channel Manage event channels subscription Manage event subscriptions eventtype Manage eventtypes Other Commands: plugin Manage kn plugins secret Manage secrets completion Output shell completion code version Show the version of this client 5. åˆ›å»ºç¤ºä¾‹æœåŠ¡ ä»¥ä¸‹é€šè¿‡yamlçš„æ–¹å¼æ¼”ç¤ºã€‚\nvi hello.yaml\napiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: ghcr.io/knative/helloworld-go:latest ports: - containerPort: 8080 env: - name: TARGET value: \"World\" åˆ›å»ºæ–‡ä»¶\nkubectl apply -f hello.yaml æŸ¥çœ‹æœåŠ¡\n# kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default.svc.cluster.local hello-00001 hello-00001 Unknown IngressNotConfigured # kubectl get po NAME READY STATUS RESTARTS AGE hello-00001-deployment-6469df75c-qpp5v 2/2 Running 0 15m å‚è€ƒï¼š\nhttps://www.ibm.com/topics/knative https://knative.dev/docs/concepts/ https://knative.dev/docs/serving/ ","categories":"","description":"","excerpt":"1. knativeç®€ä»‹ knativeæ˜¯ä¸€ä¸ªå°†serverlessçš„èƒ½åŠ›æ‰©å±•åˆ°k8sä¸­çš„å¼€æºé¡¹ç›®ã€‚serverlessè®©å¼€å‘è€…æ— éœ€å…³æ³¨å®¹ â€¦","ref":"/kubernetes-notes/serverless/knative/","tags":["Serverless"],"title":"knativeä»‹ç»"},{"body":"1. å‰è¨€ æœ¬è§„èŒƒåœ¨ Google Golang ä»£ç è§„èŒƒ çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œäº†è°ƒæ•´å’Œè¡¥å……ã€‚\næ¯é¡¹è§„èŒƒå†…å®¹ï¼Œç»™å‡ºäº†è¦æ±‚ç­‰çº§ï¼Œå…¶å®šä¹‰ä¸ºï¼š\nå¿…é¡»ï¼ˆMandatoryï¼‰ï¼šç”¨æˆ·å¿…é¡»é‡‡ç”¨ï¼› æ¨èï¼ˆPreferableï¼‰ï¼šç”¨æˆ·ç†åº”é‡‡ç”¨ï¼Œä½†å¦‚æœ‰ç‰¹æ®Šæƒ…å†µï¼Œå¯ä»¥ä¸é‡‡ç”¨ï¼› å¯é€‰ï¼ˆOptionalï¼‰ï¼šç”¨æˆ·å¯å‚è€ƒï¼Œè‡ªè¡Œå†³å®šæ˜¯å¦é‡‡ç”¨ï¼› 2. ä»£ç é£æ ¼ 2.1 ã€å¿…é¡»ã€‘æ ¼å¼åŒ– ä»£ç éƒ½å¿…é¡»ç”¨ gofmt æ ¼å¼åŒ–ã€‚ 2.2 ã€æ¨èã€‘æ¢è¡Œ å»ºè®®ä¸€è¡Œä»£ç ä¸è¦è¶…è¿‡120åˆ—ï¼Œè¶…è¿‡çš„æƒ…å†µï¼Œä½¿ç”¨åˆç†çš„æ¢è¡Œæ–¹æ³•æ¢è¡Œã€‚ ä¾‹å¤–åœºæ™¯ï¼š import æ¨¡å—è¯­å¥ å·¥å…·ç”Ÿæˆä»£ç  struct tag 2.3 ã€å¿…é¡»ã€‘æ‹¬å·å’Œç©ºæ ¼ éµå¾ª gofmt çš„é€»è¾‘ã€‚\nè¿ç®—ç¬¦å’Œæ“ä½œæ•°ä¹‹é—´è¦ç•™ç©ºæ ¼ã€‚\nä½œä¸ºè¾“å…¥å‚æ•°æˆ–è€…æ•°ç»„ä¸‹æ ‡æ—¶ï¼Œè¿ç®—ç¬¦å’Œè¿ç®—æ•°ä¹‹é—´ä¸éœ€è¦ç©ºæ ¼ï¼Œç´§å‡‘å±•ç¤ºã€‚\n2.4 ã€å¿…é¡»ã€‘import è§„èŒƒ ä½¿ç”¨ goimports è‡ªåŠ¨æ ¼å¼åŒ–å¼•å…¥çš„åŒ…åï¼Œimport è§„èŒƒåŸåˆ™ä¸Šä»¥ goimports è§„åˆ™ä¸ºå‡†ã€‚\ngoimports ä¼šè‡ªåŠ¨æŠŠä¾èµ–åŒ…æŒ‰é¦–å­—æ¯æ’åºï¼Œå¹¶å¯¹åŒ…è¿›è¡Œåˆ†ç»„ç®¡ç†ï¼Œé€šè¿‡ç©ºè¡Œéš”å¼€ï¼Œé»˜è®¤åˆ†ä¸ºæœ¬åœ°åŒ…ï¼ˆæ ‡å‡†åº“ã€å†…éƒ¨åŒ…ï¼‰ã€ç¬¬ä¸‰æ–¹åŒ…ã€‚\næ ‡å‡†åŒ…æ°¸è¿œä½äºæœ€ä¸Šé¢çš„ç¬¬ä¸€ç»„ã€‚\nå†…éƒ¨åŒ…æ˜¯æŒ‡ä¸èƒ½è¢«å¤–éƒ¨ import çš„åŒ…ï¼Œå¦‚ GoPath æ¨¡å¼ä¸‹çš„åŒ…åæˆ–è€…éåŸŸåå¼€å¤´çš„å½“å‰é¡¹ç›®çš„ GoModules åŒ…åã€‚\ngoimports é»˜è®¤æœ€å°‘åˆ†æˆæœ¬åœ°åŒ…å’Œç¬¬ä¸‰æ–¹åŒ…ä¸¤å¤§ç±»ï¼Œè¿™ä¸¤ç±»åŒ…å¿…é¡»åˆ†å¼€ä¸èƒ½æ”¾åœ¨ä¸€èµ·ã€‚æœ¬åœ°åŒ…æˆ–è€…ç¬¬ä¸‰æ–¹åŒ…å†…éƒ¨å¯ä»¥ç»§ç»­æŒ‰å®é™…æƒ…å†µç»†åˆ†ä¸åŒå­ç±»ã€‚\nä¸è¦ä½¿ç”¨ç›¸å¯¹è·¯å¾„å¼•å…¥åŒ…ï¼š\n// ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ import ( \"../net\" ) åº”è¯¥ä½¿ç”¨å®Œæ•´çš„è·¯å¾„å¼•å…¥åŒ…ï¼š import ( \"xxxx.com/proj/net\" ) åŒ…åå’Œ git è·¯å¾„åä¸ä¸€è‡´æ—¶ï¼Œæˆ–è€…å¤šä¸ªç›¸åŒåŒ…åå†²çªæ—¶ï¼Œä½¿ç”¨åˆ«åä»£æ›¿ï¼Œåˆ«åå‘½åè§„èŒƒå’ŒåŒ…å‘½åè§„èŒƒä¿æŒä¸€è‡´ï¼š // åˆç†ç”¨æ³•ï¼šåŒ…åå’Œ git è·¯å¾„åä¸ä¸€è‡´ï¼Œä½¿ç”¨åˆ«å import ( opentracing \"github.com/opentracing/opentracing-go\" ) // åˆç†ç”¨æ³•ï¼šå¤šä¸ªç›¸åŒåŒ…åå†²çªï¼Œä½¿ç”¨åˆ«å import ( \"fmt\" \"os\" \"runtime/trace\" nettrace \"golang.net/x/trace\" ) // ä¸åˆç†ç”¨æ³•ï¼šåŒ…åå’Œè·¯å¾„åä¸€è‡´ï¼Œä¹Ÿä¸å­˜åœ¨å¤šåŒ…åå†²çªï¼Œä¸åº”è¯¥ä½¿ç”¨åˆ«å import ( \"fmt\" \"os\" nettrace \"golang.net/x/trace\" ) ã€å¯é€‰ã€‘åŒ¿ååŒ…çš„å¼•ç”¨å»ºè®®ä½¿ç”¨ä¸€ä¸ªæ–°çš„åˆ†ç»„å¼•å…¥ï¼Œå¹¶åœ¨åŒ¿ååŒ…ä¸Šå†™ä¸Šæ³¨é‡Šè¯´æ˜ã€‚ å®Œæ•´ç¤ºä¾‹å¦‚ä¸‹ï¼š\nimport ( // standard package \u0026 inner package \"encoding/json\" \"myproject/models\" \"myproject/controller\" \"strings\" // third-party package \"git.obc.im/obc/utils\" \"git.obc.im/dep/beego\" \"git.obc.im/dep/mysql\" opentracing \"github.com/opentracing/opentracing-go\" // anonymous import package // import filesystem storage driver _ \"github.com/org/repo/pkg/storage/filesystem ) 2.5 ã€å¿…é¡»ã€‘é”™è¯¯å¤„ç† 2.5.1 ã€å¿…é¡»ã€‘error å¤„ç† error ä½œä¸ºå‡½æ•°çš„å€¼è¿”å›ï¼Œå¿…é¡»å¯¹ error è¿›è¡Œå¤„ç†, æˆ–å°†è¿”å›å€¼èµ‹å€¼ç»™æ˜ç¡®å¿½ç•¥ã€‚å¯¹äº defer xx.Close()å¯ä»¥ä¸ç”¨æ˜¾å¼å¤„ç†ã€‚\nerror ä½œä¸ºå‡½æ•°çš„å€¼è¿”å›ä¸”æœ‰å¤šä¸ªè¿”å›å€¼çš„æ—¶å€™ï¼Œerror å¿…é¡»æ˜¯æœ€åä¸€ä¸ªå‚æ•°ã€‚\n// ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ func do() (error, int) { } // è¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ func do() (int, error) { } é”™è¯¯æè¿°ä¸éœ€è¦æ ‡ç‚¹ç»“å°¾ã€‚\né‡‡ç”¨ç‹¬ç«‹çš„é”™è¯¯æµè¿›è¡Œå¤„ç†ã€‚\n// ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ if err != nil { // error handling } else { // normal code } // è€Œè¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ if err != nil { // error handling return // or continue, etc. } // normal code å¦‚æœè¿”å›å€¼éœ€è¦åˆå§‹åŒ–ï¼Œåˆ™é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ï¼š x, err := f() if err != nil { // error handling return // or continue, etc. } // use x é”™è¯¯è¿”å›çš„åˆ¤æ–­ç‹¬ç«‹å¤„ç†ï¼Œä¸ä¸å…¶ä»–å˜é‡ç»„åˆé€»è¾‘åˆ¤æ–­ã€‚ // ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ï¼š x, y, err := f() if err != nil || y == nil { return err // å½“yä¸erréƒ½ä¸ºç©ºæ—¶ï¼Œå‡½æ•°çš„è°ƒç”¨è€…ä¼šå‡ºç°é”™è¯¯çš„è°ƒç”¨é€»è¾‘ } // åº”å½“ä½¿ç”¨å¦‚ä¸‹æ–¹å¼ï¼š x, y, err := f() if err != nil { return err } if y == nil { return errors.New(\"some error\") } ã€æ¨èã€‘å¯¹äºä¸éœ€è¦æ ¼å¼åŒ–çš„é”™è¯¯ï¼Œç”Ÿæˆæ–¹å¼ä¸ºï¼šerrors.New(\"xxxx\")ã€‚ ã€æ¨èã€‘å»ºè®®go1.13 ä»¥ä¸Šï¼Œerror ç”Ÿæˆæ–¹å¼ä¸ºï¼šfmt.Errorf(\"module xxx: %w\", err)ã€‚ 2.5.2 ã€å¿…é¡»ã€‘panic å¤„ç† åœ¨ä¸šåŠ¡é€»è¾‘å¤„ç†ä¸­ç¦æ­¢ä½¿ç”¨ panicã€‚\nåœ¨ main åŒ…ä¸­åªæœ‰å½“å®Œå…¨ä¸å¯è¿è¡Œçš„æƒ…å†µå¯ä½¿ç”¨ panicï¼Œä¾‹å¦‚ï¼šæ–‡ä»¶æ— æ³•æ‰“å¼€ï¼Œæ•°æ®åº“æ— æ³•è¿æ¥å¯¼è‡´ç¨‹åºæ— æ³•æ­£å¸¸è¿è¡Œã€‚\nå¯¹äºå…¶å®ƒçš„åŒ…ï¼Œå¯å¯¼å‡ºçš„æ¥å£ä¸€å®šä¸èƒ½æœ‰ panicï¼›åœ¨åŒ…å†…ä¼ é€’é”™è¯¯æ—¶ï¼Œä¸æ¨èä½¿ç”¨ panic æ¥ä¼ é€’ errorã€‚\n// ä¸æ¨èä¸ºä¼ é€’errorè€Œåœ¨åŒ…å†…ä½¿ç”¨panic,ä»¥ä¸‹ä¸ºç¤ºä¾‹ // PError åŒ…å†…å®šä¹‰çš„é”™è¯¯ç±»å‹ type PError string // Error erroræ¥å£æ–¹æ³• func (e PError) Error() string { return string(e) } func do(str string) { // ... // æ­¤å¤„çš„panicç”¨äºä¼ é€’error panic(PError(\"é”™è¯¯ä¿¡æ¯\")) // ... } // Do åŒ…çº§è®¿é—®å…¥å£ func Do(str string) error { var err error defer func() { if e := recover(); e != nil { err = e.(PError) } }() do(str) return err } å»ºè®®åœ¨ main åŒ…ä¸­ä½¿ç”¨ log.Fatal æ¥è®°å½•é”™è¯¯ï¼Œè¿™æ ·å°±å¯ä»¥ç”± log æ¥ç»“æŸç¨‹åºï¼Œæˆ–è€…å°† panic æŠ›å‡ºçš„å¼‚å¸¸è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿æ’æŸ¥é—®é¢˜ã€‚\npanic æ•è·åªèƒ½åˆ° goroutine æœ€é¡¶å±‚ï¼Œæ¯ä¸ªè‡ªè¡Œå¯åŠ¨çš„ goroutineï¼Œå¿…é¡»åœ¨å…¥å£å¤„æ•è· panicï¼Œå¹¶æ‰“å°è¯¦ç»†å †æ ˆä¿¡æ¯æˆ–è¿›è¡Œå…¶å®ƒå¤„ç†ã€‚\n2.5.3 ã€å¿…é¡»ã€‘recover å¤„ç† recover ç”¨äºæ•è· runtime çš„å¼‚å¸¸ï¼Œç¦æ­¢æ»¥ç”¨ recoverã€‚\nå¿…é¡»åœ¨ defer ä¸­ä½¿ç”¨ï¼Œä¸€èˆ¬ç”¨æ¥æ•è·ç¨‹åºè¿è¡ŒæœŸé—´å‘ç”Ÿå¼‚å¸¸æŠ›å‡ºçš„ panic æˆ–ç¨‹åºä¸»åŠ¨æŠ›å‡ºçš„ panicã€‚\npackage main import ( \"log\" ) func main() { defer func() { if err := recover(); err != nil { // do something or record log log.Println(\"exec panic error: \", err) // log.Println(debug.Stack()) } }() getOne() panic(11) // æ‰‹åŠ¨æŠ›å‡ºpanic } // getOne æ¨¡æ‹Ÿsliceè¶Šç•Œ runtimeè¿è¡Œæ—¶æŠ›å‡ºçš„panic func getOne() { defer func() { if err := recover(); err != nil { // do something or record log log.Println(\"exec panic error: \", err) // log.Println(debug.Stack()) } }() var arr = []string{\"a\", \"b\", \"c\"} log.Println(\"hello,\", arr[4]) } // æ‰§è¡Œç»“æœï¼š // 2020/01/02 17:18:53 exec panic error: runtime error: index out of range // 2020/01/02 17:18:53 exec panic error: 11 2.6 ã€å¿…é¡»ã€‘å•å…ƒæµ‹è¯• å•å…ƒæµ‹è¯•æ–‡ä»¶åå‘½åè§„èŒƒä¸º example_test.goã€‚\næµ‹è¯•ç”¨ä¾‹çš„å‡½æ•°åç§°å¿…é¡»ä»¥ Test å¼€å¤´ï¼Œä¾‹å¦‚ TestExampleã€‚\nå¦‚æœå­˜åœ¨ func Fooï¼Œå•æµ‹å‡½æ•°å¯ä»¥å¸¦ä¸‹åˆ’çº¿ï¼Œä¸º func Test_Fooã€‚å¦‚æœå­˜åœ¨ func (b *Bar) Fooï¼Œå•æµ‹å‡½æ•°å¯ä»¥ä¸º func TestBar_Fooã€‚ä¸‹åˆ’çº¿ä¸èƒ½å‡ºç°åœ¨å‰é¢æè¿°æƒ…å†µä»¥å¤–çš„ä½ç½®ã€‚\nå•æµ‹æ–‡ä»¶è¡Œæ•°é™åˆ¶æ˜¯æ™®é€šæ–‡ä»¶çš„2å€ï¼Œå³1600è¡Œã€‚å•æµ‹å‡½æ•°è¡Œæ•°é™åˆ¶ä¹Ÿæ˜¯æ™®é€šå‡½æ•°çš„2å€ï¼Œå³ä¸º160è¡Œã€‚åœˆå¤æ‚åº¦ã€åˆ—æ•°é™åˆ¶ã€ import åˆ†ç»„ç­‰å…¶ä»–è§„èŒƒç»†èŠ‚å’Œæ™®é€šæ–‡ä»¶ä¿æŒä¸€è‡´ã€‚\nç”±äºå•æµ‹æ–‡ä»¶å†…çš„å‡½æ•°éƒ½æ˜¯ä¸å¯¹å¤–çš„ï¼Œæ‰€æœ‰å¯å¯¼å‡ºå‡½æ•°å¯ä»¥æ²¡æœ‰æ³¨é‡Šï¼Œä½†æ˜¯ç»“æ„ä½“å®šä¹‰æ—¶å°½é‡ä¸è¦å¯¼å‡ºã€‚\næ¯ä¸ªé‡è¦çš„å¯å¯¼å‡ºå‡½æ•°éƒ½è¦é¦–å…ˆç¼–å†™æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹è¯•ç”¨ä¾‹å’Œæ­£è§„ä»£ç ä¸€èµ·æäº¤æ–¹ä¾¿è¿›è¡Œå›å½’æµ‹è¯•ã€‚\n2.7 ã€å¿…é¡»ã€‘ç±»å‹æ–­è¨€å¤±è´¥å¤„ç† type assertion çš„å•ä¸ªè¿”å›å€¼å½¢å¼é’ˆå¯¹ä¸æ­£ç¡®çš„ç±»å‹å°†äº§ç”Ÿ panicã€‚å› æ­¤ï¼Œè¯·å§‹ç»ˆä½¿ç”¨ â€œcomma okâ€ çš„æƒ¯ç”¨æ³•ã€‚ // ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ t := i.(string) // è€Œè¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ t, ok := i.(string) if !ok { // ä¼˜é›…åœ°å¤„ç†é”™è¯¯ } 3. æ³¨é‡Š åœ¨ç¼–ç é˜¶æ®µåŒæ­¥å†™å¥½å˜é‡ã€å‡½æ•°ã€åŒ…æ³¨é‡Šï¼Œæ³¨é‡Šå¯ä»¥é€šè¿‡ godoc å¯¼å‡ºç”Ÿæˆæ–‡æ¡£ã€‚ ç¨‹åºä¸­æ¯ä¸€ä¸ªè¢«å¯¼å‡ºçš„(å¤§å†™çš„)åå­—ï¼Œéƒ½åº”è¯¥æœ‰ä¸€ä¸ªæ–‡æ¡£æ³¨é‡Šã€‚ æ‰€æœ‰æ³¨é‡Šæ‰çš„ä»£ç åœ¨æäº¤ code review å‰éƒ½åº”è¯¥è¢«åˆ é™¤ï¼Œé™¤éæ·»åŠ æ³¨é‡Šè®²è§£ä¸ºä»€ä¹ˆä¸åˆ é™¤ï¼Œ å¹¶ä¸”æ ‡æ˜åç»­å¤„ç†å»ºè®®ï¼ˆæ¯”å¦‚åˆ é™¤è®¡åˆ’ï¼‰ã€‚ 3.1 ã€å¿…é¡»ã€‘åŒ…æ³¨é‡Š æ¯ä¸ªåŒ…éƒ½åº”è¯¥æœ‰ä¸€ä¸ªåŒ…æ³¨é‡Šã€‚\nåŒ…å¦‚æœæœ‰å¤šä¸ª go æ–‡ä»¶ï¼Œåªéœ€è¦å‡ºç°åœ¨ä¸€ä¸ª go æ–‡ä»¶ä¸­ï¼ˆä¸€èˆ¬æ˜¯å’ŒåŒ…åŒåçš„æ–‡ä»¶ï¼‰å³å¯ï¼Œæ ¼å¼ä¸ºï¼šâ€œ// Package åŒ…å åŒ…ä¿¡æ¯æè¿°â€ã€‚\n// Package math provides basic constants and mathematical functions. package math // æˆ–è€… /* Package template implements data-driven templates for generating textual output such as HTML. .... */ package template 3.2 ã€å¿…é¡»ã€‘ç»“æ„ä½“æ³¨é‡Š æ¯ä¸ªéœ€è¦å¯¼å‡ºçš„è‡ªå®šä¹‰ç»“æ„ä½“æˆ–è€…æ¥å£éƒ½å¿…é¡»æœ‰æ³¨é‡Šè¯´æ˜ã€‚\næ³¨é‡Šå¯¹ç»“æ„è¿›è¡Œç®€è¦ä»‹ç»ï¼Œæ”¾åœ¨ç»“æ„ä½“å®šä¹‰çš„å‰ä¸€è¡Œã€‚\næ ¼å¼ä¸ºï¼š\"// ç»“æ„ä½“å ç»“æ„ä½“ä¿¡æ¯æè¿°\"ã€‚\nç»“æ„ä½“å†…çš„å¯å¯¼å‡ºæˆå‘˜å˜é‡åï¼Œå¦‚æœæ˜¯ä¸ªç”Ÿåƒ»è¯ï¼Œæˆ–è€…æ„ä¹‰ä¸æ˜ç¡®çš„è¯ï¼Œå°±å¿…é¡»è¦ç»™å‡ºæ³¨é‡Šï¼Œæ”¾åœ¨æˆå‘˜å˜é‡çš„å‰ä¸€è¡Œæˆ–åŒä¸€è¡Œçš„æœ«å°¾ã€‚\n// User ç”¨æˆ·ç»“æ„å®šä¹‰äº†ç”¨æˆ·åŸºç¡€ä¿¡æ¯ type User struct { Name string Email string // Demographic æ—ç¾¤ Demographic string } 3.3 ã€å¿…é¡»ã€‘æ–¹æ³•æ³¨é‡Š æ¯ä¸ªéœ€è¦å¯¼å‡ºçš„å‡½æ•°æˆ–è€…æ–¹æ³•ï¼ˆç»“æ„ä½“æˆ–è€…æ¥å£ä¸‹çš„å‡½æ•°ç§°ä¸ºæ–¹æ³•ï¼‰éƒ½å¿…é¡»æœ‰æ³¨é‡Šã€‚æ³¨æ„ï¼Œå¦‚æœæ–¹æ³•çš„æ¥æ”¶å™¨ä¸ºä¸å¯å¯¼å‡ºç±»å‹ï¼Œå¯ä»¥ä¸æ³¨é‡Šï¼Œä½†éœ€è¦è´¨ç–‘è¯¥æ–¹æ³•å¯å¯¼å‡ºçš„å¿…è¦æ€§ã€‚\næ³¨é‡Šæè¿°å‡½æ•°æˆ–æ–¹æ³•åŠŸèƒ½ã€è°ƒç”¨æ–¹ç­‰ä¿¡æ¯ã€‚\næ ¼å¼ä¸ºï¼š\"// å‡½æ•°å å‡½æ•°ä¿¡æ¯æè¿°\"ã€‚\n// NewtAttrModel æ˜¯å±æ€§æ•°æ®å±‚æ“ä½œç±»çš„å·¥å‚æ–¹æ³• func NewAttrModel(ctx *common.Context) *AttrModel { // TODO } 3.4 ã€å¿…é¡»ã€‘å˜é‡å’Œå¸¸é‡æ³¨é‡Š æ¯ä¸ªéœ€è¦å¯¼å‡ºçš„å¸¸é‡å’Œå˜é‡éƒ½å¿…é¡»æœ‰æ³¨é‡Šè¯´æ˜ã€‚\nè¯¥æ³¨é‡Šå¯¹å¸¸é‡æˆ–å˜é‡è¿›è¡Œç®€è¦ä»‹ç»ï¼Œæ”¾åœ¨å¸¸é‡æˆ–è€…å˜é‡å®šä¹‰çš„å‰ä¸€è¡Œã€‚\nå¤§å—å¸¸é‡æˆ–å˜é‡å®šä¹‰æ—¶ï¼Œå¯åœ¨å‰é¢æ³¨é‡Šä¸€ä¸ªæ€»çš„è¯´æ˜ï¼Œç„¶åæ¯ä¸€è¡Œå¸¸é‡çš„æœ«å°¾è¯¦ç»†æ³¨é‡Šè¯¥å¸¸é‡çš„å®šä¹‰ã€‚\næ ¼å¼ä¸ºï¼š\"// å˜é‡å å˜é‡ä¿¡æ¯æè¿°\"ï¼Œæ–œçº¿åé¢ç´§è·Ÿä¸€ä¸ªç©ºæ ¼ã€‚\n// FlagConfigFile é…ç½®æ–‡ä»¶çš„å‘½ä»¤è¡Œå‚æ•°å const FlagConfigFile = \"--config\" // å‘½ä»¤è¡Œå‚æ•° const ( FlagConfigFile1 = \"--config\" // é…ç½®æ–‡ä»¶çš„å‘½ä»¤è¡Œå‚æ•°å1 FlagConfigFile2 = \"--config\" // é…ç½®æ–‡ä»¶çš„å‘½ä»¤è¡Œå‚æ•°å2 FlagConfigFile3 = \"--config\" // é…ç½®æ–‡ä»¶çš„å‘½ä»¤è¡Œå‚æ•°å3 FlagConfigFile4 = \"--config\" // é…ç½®æ–‡ä»¶çš„å‘½ä»¤è¡Œå‚æ•°å4 ) // FullName è¿”å›æŒ‡å®šç”¨æˆ·åçš„å®Œæ•´åç§° var FullName = func(username string) string { return fmt.Sprintf(\"fake-%s\", username) } 3.5 ã€å¿…é¡»ã€‘ç±»å‹æ³¨é‡Š æ¯ä¸ªéœ€è¦å¯¼å‡ºçš„ç±»å‹å®šä¹‰ï¼ˆtype definitionï¼‰å’Œç±»å‹åˆ«åï¼ˆtype aliasesï¼‰éƒ½å¿…é¡»æœ‰æ³¨é‡Šè¯´æ˜ã€‚\nè¯¥æ³¨é‡Šå¯¹ç±»å‹è¿›è¡Œç®€è¦ä»‹ç»ï¼Œæ”¾åœ¨å®šä¹‰çš„å‰ä¸€è¡Œã€‚\næ ¼å¼ä¸ºï¼š\"// ç±»å‹å ç±»å‹ä¿¡æ¯æè¿°\"ã€‚\n// StorageClass å­˜å‚¨ç±»å‹ type StorageClass string // FakeTime æ ‡å‡†åº“æ—¶é—´çš„ç±»å‹åˆ«å type FakeTime = time.Time 4. å‘½åè§„èŒƒ å‘½åæ˜¯ä»£ç è§„èŒƒä¸­å¾ˆé‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œç»Ÿä¸€çš„å‘½åè§„èŒƒæœ‰åˆ©äºæé«˜ä»£ç çš„å¯è¯»æ€§ï¼Œå¥½çš„å‘½åä»…ä»…é€šè¿‡å‘½åå°±å¯ä»¥è·å–åˆ°è¶³å¤Ÿå¤šçš„ä¿¡æ¯ã€‚\n4.1 ã€æ¨èã€‘åŒ…å‘½å ä¿æŒ package çš„åå­—å’Œç›®å½•ä¸€è‡´ã€‚\nå°½é‡é‡‡å–æœ‰æ„ä¹‰ã€ç®€çŸ­çš„åŒ…åï¼Œå°½é‡ä¸è¦å’Œæ ‡å‡†åº“å†²çªã€‚\nåŒ…ååº”è¯¥ä¸ºå°å†™å•è¯ï¼Œä¸è¦ä½¿ç”¨ä¸‹åˆ’çº¿æˆ–è€…æ··åˆå¤§å°å†™ï¼Œä½¿ç”¨å¤šçº§ç›®å½•æ¥åˆ’åˆ†å±‚çº§ã€‚\nåŒ…åå¯è°¨æ…åœ°ä½¿ç”¨ç¼©å†™ã€‚å½“ç¼©å†™æ˜¯ç¨‹åºå‘˜å¹¿æ³›ç†ŸçŸ¥çš„è¯æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ç¼©å†™ã€‚ä¾‹å¦‚ï¼š\nstrconv (string conversion) syscall (system call) fmt (formatted I/O) å¦‚æœç¼©å†™æœ‰æ­§ä¹‰æˆ–ä¸æ¸…æ™°ï¼Œä¸ç”¨ç¼©å†™ã€‚\né¡¹ç›®åå¯ä»¥é€šè¿‡ä¸­åˆ’çº¿æ¥è¿æ¥å¤šä¸ªå•è¯ã€‚\nç®€å•æ˜äº†çš„åŒ…å‘½åï¼Œå¦‚ï¼štimeã€listã€httpã€‚\nä¸è¦ä½¿ç”¨æ— æ„ä¹‰çš„åŒ…åï¼Œå¦‚ï¼šutilã€commonã€miscã€globalã€‚packageåå­—åº”è¯¥è¿½æ±‚æ¸…æ™°ä¸”è¶Šæ¥è¶Šæ”¶æ•›ï¼Œç¬¦åˆâ€˜å•ä¸€èŒè´£â€™åŸåˆ™ã€‚è€Œä¸æ˜¯åƒcommonä¸€æ ·ï¼Œä»€ä¹ˆéƒ½èƒ½å¾€é‡Œé¢æ”¾ï¼Œè¶Šæ¥è¶Šè†¨èƒ€ï¼Œè®©ä¾èµ–å…³ç³»å˜å¾—å¤æ‚ï¼Œä¸åˆ©äºé˜…è¯»ã€å¤ç”¨ã€é‡æ„ã€‚æ³¨æ„ï¼Œxx/util/encryptionè¿™æ ·çš„åŒ…åæ˜¯å…è®¸çš„ã€‚\n4.2 ã€å¿…é¡»ã€‘æ–‡ä»¶å‘½å é‡‡ç”¨æœ‰æ„ä¹‰ï¼Œç®€çŸ­çš„æ–‡ä»¶åã€‚\næ–‡ä»¶ååº”è¯¥é‡‡ç”¨å°å†™ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†å‰²å„ä¸ªå•è¯ã€‚\n4.3 ã€å¿…é¡»ã€‘ç»“æ„ä½“å‘½å é‡‡ç”¨é©¼å³°å‘½åæ–¹å¼ï¼Œé¦–å­—æ¯æ ¹æ®è®¿é—®æ§åˆ¶é‡‡ç”¨å¤§å†™æˆ–è€…å°å†™ã€‚\nç»“æ„ä½“ååº”è¯¥æ˜¯åè¯æˆ–åè¯çŸ­è¯­ï¼Œå¦‚ Customerã€WikiPageã€Accountã€AddressParserï¼Œå®ƒä¸åº”æ˜¯åŠ¨è¯ã€‚\né¿å…ä½¿ç”¨ Dataã€Info è¿™ç±»æ„ä¹‰å¤ªå®½æ³›çš„ç»“æ„ä½“åã€‚\nç»“æ„ä½“çš„å£°æ˜å’Œåˆå§‹åŒ–æ ¼å¼é‡‡ç”¨å¤šè¡Œï¼Œä¾‹å¦‚ï¼š\n// User å¤šè¡Œå£°æ˜ type User struct { Name string Email string } // å¤šè¡Œåˆå§‹åŒ– u := User{ Name: \"john\", Email: \"john@example.com\", } 4.4 ã€æ¨èã€‘æ¥å£å‘½å å‘½åè§„åˆ™åŸºæœ¬ä¿æŒå’Œç»“æ„ä½“å‘½åè§„åˆ™ä¸€è‡´ã€‚\nå•ä¸ªå‡½æ•°çš„æ¥å£åä»¥ er ä½œä¸ºåç¼€ï¼Œä¾‹å¦‚ Readerï¼ŒWriterã€‚\n// Reader å­—èŠ‚æ•°ç»„è¯»å–æ¥å£ type Reader interface { // Read è¯»å–æ•´ä¸ªç»™å®šçš„å­—èŠ‚æ•°æ®å¹¶è¿”å›è¯»å–çš„é•¿åº¦ Read(p []byte) (n int, err error) } ä¸¤ä¸ªå‡½æ•°çš„æ¥å£åç»¼åˆä¸¤ä¸ªå‡½æ•°åã€‚\nä¸‰ä¸ªä»¥ä¸Šå‡½æ•°çš„æ¥å£åï¼Œç±»ä¼¼äºç»“æ„ä½“åã€‚\n// Car å°æ±½è½¦ç»“æ„ç”³æ˜ type Car interface { // Start ... Start([]byte) // Stop ... Stop() error // Recover ... Recover() } 4.5 ã€å¿…é¡»ã€‘å˜é‡å‘½å å˜é‡åå¿…é¡»éµå¾ªé©¼å³°å¼ï¼Œé¦–å­—æ¯æ ¹æ®è®¿é—®æ§åˆ¶å†³å®šä½¿ç”¨å¤§å†™æˆ–å°å†™ã€‚\nç‰¹æœ‰åè¯æ—¶ï¼Œéœ€è¦éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\nå¦‚æœå˜é‡ä¸ºç§æœ‰ï¼Œä¸”ç‰¹æœ‰åè¯ä¸ºé¦–ä¸ªå•è¯ï¼Œåˆ™ä½¿ç”¨å°å†™ï¼Œå¦‚ apiClientï¼› å…¶ä»–æƒ…å†µéƒ½åº”è¯¥ä½¿ç”¨è¯¥åè¯åŸæœ‰çš„å†™æ³•ï¼Œå¦‚ APIClientã€repoIDã€UserIDï¼› é”™è¯¯ç¤ºä¾‹ï¼šUrlArrayï¼Œåº”è¯¥å†™æˆ urlArray æˆ–è€… URLArrayï¼› è¯¦ç»†çš„ä¸“æœ‰åè¯åˆ—è¡¨å¯å‚è€ƒè¿™é‡Œã€‚ ç§æœ‰å…¨å±€å˜é‡å’Œå±€éƒ¨å˜é‡è§„èŒƒä¸€è‡´ï¼Œå‡ä»¥å°å†™å­—æ¯å¼€å¤´ã€‚\nä»£ç ç”Ÿæˆå·¥å…·è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç å¯æ’é™¤æ­¤è§„åˆ™ï¼ˆå¦‚ xxx.pb.go é‡Œé¢çš„ Idï¼‰ã€‚\nå˜é‡åæ›´å€¾å‘äºé€‰æ‹©çŸ­å‘½åã€‚ç‰¹åˆ«æ˜¯å¯¹äºå±€éƒ¨å˜é‡ã€‚ cæ¯”lineCountè¦å¥½ï¼Œiæ¯”sliceIndexè¦å¥½ã€‚åŸºæœ¬åŸåˆ™æ˜¯ï¼šå˜é‡çš„ä½¿ç”¨å’Œå£°æ˜çš„ä½ç½®è¶Šè¿œï¼Œå˜é‡åå°±éœ€è¦å…·å¤‡è¶Šå¼ºçš„æè¿°æ€§ã€‚\n4.6 ã€å¿…é¡»ã€‘å¸¸é‡å‘½å å¸¸é‡å‡éœ€éµå¾ªé©¼å³°å¼ã€‚ // AppVersion åº”ç”¨ç¨‹åºç‰ˆæœ¬å·å®šä¹‰ const AppVersion = \"1.0.0\" å¦‚æœæ˜¯æšä¸¾ç±»å‹çš„å¸¸é‡ï¼Œéœ€è¦å…ˆåˆ›å»ºç›¸åº”ç±»å‹ï¼š // Scheme ä¼ è¾“åè®® type Scheme string const ( // HTTP è¡¨ç¤ºHTTPæ˜æ–‡ä¼ è¾“åè®® HTTP Scheme = \"http\" // HTTPS è¡¨ç¤ºHTTPSåŠ å¯†ä¼ è¾“åè®® HTTPS Scheme = \"https\" ) ç§æœ‰å…¨å±€å¸¸é‡å’Œå±€éƒ¨å˜é‡è§„èŒƒä¸€è‡´ï¼Œå‡ä»¥å°å†™å­—æ¯å¼€å¤´ã€‚ const appVersion = \"1.0.0\" 4.7 ã€å¿…é¡»ã€‘å‡½æ•°å‘½å å‡½æ•°åå¿…é¡»éµå¾ªé©¼å³°å¼ï¼Œé¦–å­—æ¯æ ¹æ®è®¿é—®æ§åˆ¶å†³å®šä½¿ç”¨å¤§å†™æˆ–å°å†™ã€‚ ä»£ç ç”Ÿæˆå·¥å…·è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç å¯æ’é™¤æ­¤è§„åˆ™ï¼ˆå¦‚åè®®ç”Ÿæˆæ–‡ä»¶ xxx.pb.go , gotests è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶ xxx_test.go é‡Œé¢çš„ä¸‹åˆ’çº¿ï¼‰ã€‚ 5. æ§åˆ¶ç»“æ„ 5.1 ã€æ¨èã€‘if if æ¥å—åˆå§‹åŒ–è¯­å¥ï¼Œçº¦å®šå¦‚ä¸‹æ–¹å¼å»ºç«‹å±€éƒ¨å˜é‡ï¼š if err := file.Chmod(0664); err != nil { return err } if å¯¹ä¸¤ä¸ªå€¼è¿›è¡Œåˆ¤æ–­æ—¶ï¼Œçº¦å®šå¦‚ä¸‹é¡ºåºï¼šå˜é‡åœ¨å·¦ï¼Œå¸¸é‡åœ¨å³ï¼š // ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ if nil != err { // error handling } // ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ if 0 == errorCode { // do something } // è€Œè¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ if err != nil { // error handling } // è€Œè¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ if errorCode == 0 { // do something } if å¯¹äºboolç±»å‹çš„å˜é‡ï¼Œåº”ç›´æ¥è¿›è¡ŒçœŸå‡åˆ¤æ–­ï¼š var allowUserLogin bool // ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ if allowUserLogin == true { // do something } // ä¸è¦é‡‡ç”¨è¿™ç§æ–¹å¼ if allowUserLogin == false { // do something } // è€Œè¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ if allowUserLogin { // do something } // è€Œè¦é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼ if !allowUserLogin { // do something } 5.2\tã€æ¨èã€‘for é‡‡ç”¨çŸ­å£°æ˜å»ºç«‹å±€éƒ¨å˜é‡ï¼š sum := 0 for i := 0; i \u003c 10; i++ { sum += 1 } 5.3\tã€å¿…é¡»ã€‘range å¦‚æœåªéœ€è¦ç¬¬ä¸€é¡¹ï¼ˆkeyï¼‰ï¼Œå°±ä¸¢å¼ƒç¬¬äºŒä¸ªï¼š for key := range m { if key.expired() { delete(m, key) } } å¦‚æœåªéœ€è¦ç¬¬äºŒé¡¹ï¼Œåˆ™æŠŠç¬¬ä¸€é¡¹ç½®ä¸ºä¸‹åˆ’çº¿ï¼š sum := 0 for _, value := range array { sum += value } 5.4\tã€å¿…é¡»ã€‘switch è¦æ±‚å¿…é¡»æœ‰ defaultï¼š switch os := runtime.GOOS; os { case \"darwin\": fmt.Println(\"OS X.\") case \"linux\": fmt.Println(\"Linux.\") default: // freebsd, openbsd, // plan9, windows... fmt.Printf(\"%s.\\n\", os) } 5.5 ã€æ¨èã€‘return å°½æ—© returnï¼Œä¸€æ—¦æœ‰é”™è¯¯å‘ç”Ÿï¼Œé©¬ä¸Šè¿”å›ï¼š f, err := os.Open(name) if err != nil { return err } defer f.Close() d, err := f.Stat() if err != nil { return err } codeUsing(f, d) 5.6 ã€å¿…é¡»ã€‘goto ä¸šåŠ¡ä»£ç ç¦æ­¢ä½¿ç”¨ gotoï¼Œå…¶ä»–æ¡†æ¶æˆ–åº•å±‚æºç æ¨èå°½é‡ä¸ç”¨ã€‚ 6. å‡½æ•° 6.1 ã€æ¨èã€‘å‡½æ•°å‚æ•° å‡½æ•°è¿”å›ç›¸åŒç±»å‹çš„ä¸¤ä¸ªæˆ–ä¸‰ä¸ªå‚æ•°ï¼Œæˆ–è€…å¦‚æœä»ä¸Šä¸‹æ–‡ä¸­ä¸æ¸…æ¥šç»“æœçš„å«ä¹‰ï¼Œä½¿ç”¨å‘½åè¿”å›ï¼Œå…¶å®ƒæƒ…å†µä¸å»ºè®®ä½¿ç”¨å‘½åè¿”å›ã€‚ // Parent1 ... func (n *Node) Parent1() *Node // Parent2 ... func (n *Node) Parent2() (*Node, error) // Location ... func (f *Foo) Location() (lat, long float64, err error) ä¼ å…¥å˜é‡å’Œè¿”å›å˜é‡ä»¥å°å†™å­—æ¯å¼€å¤´ã€‚\nå‚æ•°æ•°é‡å‡ä¸èƒ½è¶…è¿‡5ä¸ªã€‚\nå°½é‡ç”¨å€¼ä¼ é€’ï¼ŒéæŒ‡é’ˆä¼ é€’ã€‚\nä¼ å…¥å‚æ•°æ˜¯ mapï¼Œsliceï¼Œchanï¼Œinterface ä¸è¦ä¼ é€’æŒ‡é’ˆã€‚\n6.2 ã€å¿…é¡»ã€‘defer å½“å­˜åœ¨èµ„æºç®¡ç†æ—¶ï¼Œåº”ç´§è·Ÿ defer å‡½æ•°è¿›è¡Œèµ„æºçš„é‡Šæ”¾ã€‚\nåˆ¤æ–­æ˜¯å¦æœ‰é”™è¯¯å‘ç”Ÿä¹‹åï¼Œå† defer é‡Šæ”¾èµ„æºã€‚\nresp, err := http.Get(url) if err != nil { return err } // å¦‚æœæ“ä½œæˆåŠŸï¼Œå†defer Close() defer resp.Body.Close() ç¦æ­¢åœ¨å¾ªç¯ä¸­ä½¿ç”¨ deferï¼Œä¸¾ä¾‹å¦‚ä¸‹ï¼š // ä¸è¦è¿™æ ·ä½¿ç”¨ func filterSomething(values []string) { for _, v := range values { fields, err := db.Query(v) // ç¤ºä¾‹ï¼Œå®é™…ä¸è¦è¿™ä¹ˆæŸ¥è¯¢ï¼Œé˜²æ­¢sqlæ³¨å…¥ if err != nil { // xxx } defer fields.Close() // ç»§ç»­ä½¿ç”¨fields } } // åº”å½“ä½¿ç”¨å¦‚ä¸‹çš„æ–¹å¼ï¼š func filterSomething(values []string) { for _, v := range values { func() { fields, err := db.Query(v) // ç¤ºä¾‹ï¼Œå®é™…ä¸è¦è¿™ä¹ˆæŸ¥è¯¢ï¼Œé˜²æ­¢sqlæ³¨å…¥ if err != nil { ... } defer fields.Close() // ç»§ç»­ä½¿ç”¨fields }() } } 6.3 ã€æ¨èã€‘æ–¹æ³•çš„æ¥æ”¶å™¨ ã€æ¨èã€‘æ¨èä»¥ç±»åç¬¬ä¸€ä¸ªè‹±æ–‡é¦–å­—æ¯çš„å°å†™ä½œä¸ºæ¥æ”¶å™¨çš„å‘½åã€‚\nã€æ¨èã€‘æ¥æ”¶å™¨çš„å‘½ååœ¨å‡½æ•°è¶…è¿‡20è¡Œçš„æ—¶å€™ä¸è¦ç”¨å•å­—ç¬¦ã€‚\nã€å¿…é¡»ã€‘å‘½åä¸èƒ½é‡‡ç”¨ meï¼Œthisï¼Œself è¿™ç±»æ˜“æ··æ·†åç§°ã€‚\n6.4 ã€æ¨èã€‘ä»£ç è¡Œæ•° ã€å¿…é¡»ã€‘æ–‡ä»¶é•¿åº¦ä¸èƒ½è¶…è¿‡800è¡Œã€‚\nã€æ¨èã€‘å‡½æ•°é•¿åº¦ä¸èƒ½è¶…è¿‡80è¡Œï¼ˆå‡½æ•°é•¿åº¦ä¸ºå‡½æ•°ç­¾åå·¦æ‹¬å·ä¸‹ä¸€è¡Œå¼€å§‹åˆ°å³æ‹¬å·ä¸Šä¸€è¡Œç»“æŸéƒ¨åˆ†çš„è¡Œæ•°ï¼ŒåŒ…æ‹¬ä»£ç è¡Œï¼Œæ³¨é‡Šè¡Œï¼Œç©ºè¡Œï¼‰ã€‚\n6.5 ã€å¿…é¡»ã€‘åµŒå¥— åµŒå¥—æ·±åº¦ä¸èƒ½è¶…è¿‡4å±‚ï¼š // AddArea æ·»åŠ æˆåŠŸæˆ–å‡ºé”™ func (s *BookingService) AddArea(areas ...string) error { s.Lock() defer s.Unlock() for _, area := range areas { for _, has := range s.areas { if area == has { return srverr.ErrAreaConflict } } s.areas = append(s.areas, area) s.areaOrders[area] = new(order.AreaOrder) } return nil } // å»ºè®®è°ƒæ•´ä¸ºè¿™æ ·ï¼š // AddArea æ·»åŠ æˆåŠŸæˆ–å‡ºé”™ func (s *BookingService) AddArea(areas ...string) error { s.Lock() defer s.Unlock() for _, area := range areas { if s.HasArea(area) { return srverr.ErrAreaConflict } s.areas = append(s.areas, area) s.areaOrders[area] = new(order.AreaOrder) } return nil } // HasArea ... func (s *BookingService) HasArea(area string) bool { for _, has := range s.areas { if area == has { return true } } return false } 6.6 ã€æ¨èã€‘å˜é‡å£°æ˜ å˜é‡å£°æ˜å°½é‡æ”¾åœ¨å˜é‡ç¬¬ä¸€æ¬¡ä½¿ç”¨å‰é¢ï¼Œå°±è¿‘åŸåˆ™ã€‚ 6.7 ã€å¿…é¡»ã€‘é­”æ³•æ•°å­— å¦‚æœé­”æ³•æ•°å­—å‡ºç°è¶…è¿‡2æ¬¡ï¼Œåˆ™ç¦æ­¢ä½¿ç”¨ã€‚ func getArea(r float64) float64 { return 3.14 * r * r } func getLength(r float64) float64 { return 3.14 * 2 * r } ç”¨ä¸€ä¸ªå¸¸é‡ä»£æ›¿ï¼š // PI ... const PI = 3.14 func getArea(r float64) float64 { return PI * r * r } func getLength(r float64) float64 { return PI * 2 * r } 7. ä¾èµ–ç®¡ç† 7.1 ã€å¿…é¡»ã€‘go1.11 ä»¥ä¸Šå¿…é¡»ä½¿ç”¨ go modules æ¨¡å¼ï¼š go mod init git.xxx.com/group/myrepo 7.2 ã€æ¨èã€‘ä»£ç æäº¤ å»ºè®®æ‰€æœ‰ä¸å¯¹å¤–å¼€æºçš„å·¥ç¨‹çš„ module name ä½¿ç”¨ git.xxx.com/group/repo ï¼Œæ–¹ä¾¿ä»–äººç›´æ¥å¼•ç”¨ã€‚\nå»ºè®®ä½¿ç”¨ go modules ä½œä¸ºä¾èµ–ç®¡ç†çš„é¡¹ç›®ä¸æäº¤ vendor ç›®å½•ã€‚\nå»ºè®®ä½¿ç”¨ go modules ç®¡ç†ä¾èµ–çš„é¡¹ç›®ï¼Œ go.sum æ–‡ä»¶å¿…é¡»æäº¤ï¼Œä¸è¦æ·»åŠ åˆ° .gitignore è§„åˆ™ä¸­ã€‚\n8. åº”ç”¨æœåŠ¡ 8.1 ã€æ¨èã€‘åº”ç”¨æœåŠ¡æ¥å£å»ºè®®æœ‰ README.md å…¶ä¸­å»ºè®®åŒ…æ‹¬æœåŠ¡åŸºæœ¬æè¿°ã€ä½¿ç”¨æ–¹æ³•ã€éƒ¨ç½²æ—¶çš„é™åˆ¶ä¸è¦æ±‚ã€åŸºç¡€ç¯å¢ƒä¾èµ–ï¼ˆä¾‹å¦‚æœ€ä½ go ç‰ˆæœ¬ã€æœ€ä½å¤–éƒ¨é€šç”¨åŒ…ç‰ˆæœ¬ï¼‰ç­‰ã€‚ 8.2 ã€å¿…é¡»ã€‘åº”ç”¨æœåŠ¡å¿…é¡»è¦æœ‰æ¥å£æµ‹è¯•ã€‚ é™„ï¼šå¸¸ç”¨å·¥å…· go è¯­è¨€æœ¬èº«åœ¨ä»£ç è§„èŒƒæ€§è¿™æ–¹é¢ä¹Ÿåšäº†å¾ˆå¤šåŠªåŠ›ï¼Œå¾ˆå¤šé™åˆ¶éƒ½æ˜¯å¼ºåˆ¶è¯­æ³•è¦æ±‚ï¼Œä¾‹å¦‚å·¦å¤§æ‹¬å·ä¸æ¢è¡Œï¼Œå¼•ç”¨çš„åŒ…æˆ–è€…å®šä¹‰çš„å˜é‡ä¸ä½¿ç”¨ä¼šæŠ¥é”™ï¼Œæ­¤å¤– go è¿˜æ˜¯æä¾›äº†å¾ˆå¤šå¥½ç”¨çš„å·¥å…·å¸®åŠ©æˆ‘ä»¬è¿›è¡Œä»£ç çš„è§„èŒƒã€‚\ngofmt ï¼Œå¤§éƒ¨åˆ†çš„æ ¼å¼é—®é¢˜å¯ä»¥é€šè¿‡ gofmt è§£å†³ï¼Œ gofmt è‡ªåŠ¨æ ¼å¼åŒ–ä»£ç ï¼Œä¿è¯æ‰€æœ‰çš„ go ä»£ç ä¸å®˜æ–¹æ¨èçš„æ ¼å¼ä¿æŒä¸€è‡´ï¼Œäºæ˜¯æ‰€æœ‰æ ¼å¼æœ‰å…³é—®é¢˜ï¼Œéƒ½ä»¥ gofmt çš„ç»“æœä¸ºå‡†ã€‚ goimports ï¼Œæ­¤å·¥å…·åœ¨ gofmt çš„åŸºç¡€ä¸Šå¢åŠ äº†è‡ªåŠ¨åˆ é™¤å’Œå¼•å…¥åŒ…ã€‚ go vet ï¼Œvet å·¥å…·å¯ä»¥å¸®æˆ‘ä»¬é™æ€åˆ†ææˆ‘ä»¬çš„æºç å­˜åœ¨çš„å„ç§é—®é¢˜ï¼Œä¾‹å¦‚å¤šä½™çš„ä»£ç ï¼Œæå‰ return çš„é€»è¾‘ï¼Œ struct çš„ tag æ˜¯å¦ç¬¦åˆæ ‡å‡†ç­‰ã€‚ç¼–è¯‘å‰å…ˆæ‰§è¡Œä»£ç é™æ€åˆ†æã€‚ golint ï¼Œç±»ä¼¼ javascript ä¸­çš„ jslint çš„å·¥å…·ï¼Œä¸»è¦åŠŸèƒ½å°±æ˜¯æ£€æµ‹ä»£ç ä¸­ä¸è§„èŒƒçš„åœ°æ–¹ã€‚ ","categories":"","description":"","excerpt":"1. å‰è¨€ æœ¬è§„èŒƒåœ¨ Google Golang ä»£ç è§„èŒƒ çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œäº†è°ƒæ•´å’Œè¡¥å……ã€‚\næ¯é¡¹è§„èŒƒå†…å®¹ï¼Œç»™å‡ºäº†è¦æ±‚ç­‰çº§ï¼Œå…¶å®šä¹‰ä¸ºï¼š\nå¿… â€¦","ref":"/golang-notes/standard/go-style-guide/","tags":["Golang"],"title":"Golang ä»£ç è§„èŒƒ"},{"body":"æœ¬æ–‡ä¸»è¦ä»‹ç»è·Ÿbaremetalç›¸å…³çš„åŸºæœ¬æ¦‚å¿µ\nBMCï¼ˆBaseboard Management Controllerï¼‰ åœ¨ä»‹ç»BMCä¹‹å‰éœ€è¦äº†è§£ä¸€ä¸ªæ¦‚å¿µï¼Œå³å¹³å°ç®¡ç†ï¼ˆplatform managementï¼‰ã€‚å¹³å°ç®¡ç†è¡¨ç¤ºçš„æ˜¯ä¸€ç³»åˆ—çš„ç›‘è§†å’Œæ§åˆ¶åŠŸèƒ½ï¼Œæ“ä½œçš„å¯¹è±¡æ˜¯ç³»ç»Ÿç¡¬ä»¶ã€‚æ¯”å¦‚é€šè¿‡ç›‘è§†ç³»ç»Ÿçš„æ¸©åº¦ï¼Œç”µå‹ï¼Œé£æ‰‡ã€ç”µæºç­‰ç­‰ï¼Œå¹¶åšç›¸åº”çš„è°ƒèŠ‚å·¥ä½œï¼Œä»¥ä¿è¯ç³»ç»Ÿå¤„äºå¥åº·çš„çŠ¶æ€ã€‚åŒæ—¶å¹³å°ç®¡ç†è¿˜è´Ÿè´£è®°å½•å„ç§ç¡¬ä»¶çš„ä¿¡æ¯å’Œæ—¥å¿—è®°å½•ï¼Œç”¨äºæç¤ºç”¨æˆ·å’Œåç»­é—®é¢˜çš„å®šä½ã€‚\nä»¥ä¸Šçš„è¿™äº›åŠŸèƒ½å¯ä»¥é›†æˆåˆ°ä¸€ä¸ªæ§åˆ¶å™¨ä¸Šæ¥å®ç°ï¼Œè¿™ä¸ªæ§åˆ¶å™¨è¢«ç§°ä¸ºåŸºæ¿ç®¡ç†æ§åˆ¶å™¨ï¼ˆBaseboard Manager Controllerï¼Œç®€ç§°BMCï¼‰ã€‚\nBMCÂ æ˜¯ç‹¬ç«‹äºæœåŠ¡å™¨ç³»ç»Ÿä¹‹å¤–çš„å°å‹æ“ä½œç³»ç»Ÿï¼Œæ˜¯ä¸€ä¸ªé›†æˆåœ¨ä¸»æ¿ä¸Šçš„èŠ¯ç‰‡ï¼Œä¹Ÿæœ‰äº§å“æ˜¯é€šè¿‡ PCIE ç­‰å½¢å¼æ’åœ¨ä¸»æ¿ä¸Šï¼Œå¯¹å¤–è¡¨ç°å½¢å¼åªæ˜¯ä¸€ä¸ªæ ‡å‡†çš„ RJ45 ç½‘å£ï¼Œæ‹¥æœ‰ç‹¬ç«‹ IP çš„å›ºä»¶ç³»ç»Ÿã€‚æœåŠ¡å™¨é›†ç¾¤ä¸€èˆ¬ä½¿ç”¨ BMC æŒ‡ä»¤è¿›è¡Œå¤§è§„æ¨¡æ— äººå€¼å®ˆæ“ä½œï¼ŒåŒ…æ‹¬æœåŠ¡å™¨çš„è¿œç¨‹ç®¡ç†ã€ç›‘æ§ã€å®‰è£…ã€é‡å¯ç­‰ã€‚\nBMCæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç³»ç»Ÿï¼Œå®ƒä¸ä¾èµ–ä¸ç³»ç»Ÿä¸Šçš„å…¶å®ƒç¡¬ä»¶ï¼ˆæ¯”å¦‚CPUã€å†…å­˜ç­‰ï¼‰ï¼Œä¹Ÿä¸ä¾èµ–ä¸BIOSã€OSç­‰ã€‚\nBMCé€šè¿‡ä¸åŒçš„æ¥å£ä¸ç³»ç»Ÿä¸­çš„å…¶å®ƒç»„ä»¶è¿æ¥ã€‚LPCã€I2Cã€SMBUSï¼ŒSerialç­‰ï¼Œè¿™äº›éƒ½æ˜¯æ¯”è¾ƒåŸºæœ¬çš„æ¥å£ï¼Œè€ŒIPMIï¼Œå®ƒæ˜¯ä¸BMCåŒ¹é…çš„æ¥å£ï¼Œæ‰€æœ‰çš„BMCéƒ½éœ€è¦å®ç°è¿™ç§æ¥å£ã€‚\nBIOSï¼ˆBasic Input Output Systemï¼‰ BIOSï¼ˆBasic Input Output Systemï¼‰ï¼Œå³åŸºç¡€è¾“å…¥è¾“å‡ºç³»ç»Ÿï¼Œæ˜¯åˆ»åœ¨ä¸»æ¿ ROM èŠ¯ç‰‡ä¸Šä¸å¯ç¯¡æ”¹çš„å¯åŠ¨ç¨‹åºï¼ŒBIOS è´Ÿè´£è®¡ç®—ç³»ç»Ÿè‡ªæ£€ç¨‹åºï¼ˆPOSTï¼ŒPower On Self Testï¼‰å’Œç³»ç»Ÿè‡ªå¯åŠ¨ç¨‹åºï¼Œå› æ­¤æ˜¯è®¡ç®—æœºç³»ç»Ÿå¯åŠ¨åçš„ç¬¬ä¸€é“ç¨‹å¼ã€‚ç”±äºä¸å¯ç¯¡æ”¹æ€§ï¼Œæ•…ç¨‹åºå­˜å‚¨åœ¨ ROM èŠ¯ç‰‡ä¸­ï¼Œå¹¶ä¸”åœ¨æ–­ç”µåï¼Œä¾ç„¶å¯ä»¥ç»´æŒåŸæœ‰è®¾ç½®ã€‚\nBIOS ä¸»è¦åŠŸèƒ½æ˜¯æ§åˆ¶è®¡ç®—æœºå¯åŠ¨åçš„åŸºæœ¬ç¨‹å¼ï¼ŒåŒ…æ‹¬ç¡¬ç›˜é©±åŠ¨ï¼ˆå¦‚è£…æœºè¿‡ç¨‹ä¸­ä¼˜å…ˆé€‰æ‹© DVD æˆ–è€… USB å¯åŠ¨ç›˜ï¼‰ï¼Œé”®ç›˜è®¾ç½®ï¼Œè½¯ç›˜é©±åŠ¨ï¼Œå†…å­˜å’Œç›¸å…³è®¾å¤‡ã€‚\nIPMIï¼ˆIntelligent Platform Management Interfaceï¼‰ IPMI: æ™ºæ…§å¹³å°ç®¡ç†æ¥å£ï¼ˆIntelligent Platform Management Interfaceï¼‰åŸºäºç¡¬ä»¶çš„å¹³å°ç®¡ç†ç³»ç»Ÿçš„ä¸€ç»„æ ‡å‡†åŒ–è§„èŒƒï¼Œå¯ä»¥é›†ä¸­æ§åˆ¶å’Œç›‘è§†æœåŠ¡å™¨ã€‚\nIPMIå°±æ˜¯å¯¹â€œå¹³å°ç®¡ç†â€è¿™ä¸ªæ¦‚å¿µçš„å…·ä½“çš„è§„èŒƒå®šä¹‰ï¼Œè¯¥è§„èŒƒå®šä¹‰äº†â€œå¹³å°ç®¡ç†â€çš„è½¯ç¡¬ä»¶æ¶æ„ï¼Œäº¤äº’æŒ‡ä»¤ï¼Œäº‹ä»¶æ ¼å¼ï¼Œæ•°æ®è®°å½•ï¼Œèƒ½åŠ›é›†ç­‰ã€‚è€ŒBMCæ˜¯IPMIä¸­çš„ä¸€ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼Œå±äºIPMIç¡¬ä»¶æ¶æ„ã€‚\nIPMIæ˜¯ç‹¬ç«‹äºä¸»æœºç³»ç»Ÿ CPUã€BIOS/UEFI å’Œ OS ä¹‹å¤–ï¼Œå¯ç‹¬ç«‹è¿è¡Œçš„æ¿ä¸Šéƒ¨ä»¶ï¼Œå…¶æ ¸å¿ƒéƒ¨ä»¶å³ä¸º BMCã€‚æˆ–è€…è¯´ï¼ŒBMC ä¸å…¶ä»–ç»„ä»¶å¦‚ BIOS/UEFIã€CPU ç­‰äº¤äº’ï¼Œéƒ½æ˜¯ç»ç”± IPMI æ¥å®Œæˆã€‚åœ¨ IPMI ååŠ©ä¸‹ï¼Œç”¨æˆ·å¯ä»¥è¿œç¨‹å¯¹å…³é—­çš„æœåŠ¡å™¨è¿›è¡Œå¯åŠ¨ã€é‡è£…ã€æŒ‚è½½ ISO é•œåƒç­‰ã€‚\nRedFish Redfishæ˜¯ä¸€ç§åŸºäºHTTPsæœåŠ¡çš„ç®¡ç†æ ‡å‡†ï¼Œåˆ©ç”¨RESTfulæ¥å£å®ç°è®¾å¤‡ç®¡ç†ã€‚æ¯ä¸ªHTTPsæ“ä½œéƒ½ä»¥UTF-8ç¼–ç çš„JSONçš„å½¢å¼ï¼Œæäº¤æˆ–è¿”å›ä¸€ä¸ªèµ„æºã€‚ç”¨äºæ‰§è¡Œå¸¦å¤–ç³»ç»Ÿç®¡ç†ï¼ˆout-of-band systems managementï¼‰ï¼Œå…¶é€‚ç”¨äºå¤§è§„æ¨¡çš„æœåŠ¡å™¨ã€‚\nRedfishæ˜¯ç›¸å½“äºIPMIè§„èŒƒçš„ä¸€ç§æ¼”åŒ–ã€‚\n","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦ä»‹ç»è·Ÿbaremetalç›¸å…³çš„åŸºæœ¬æ¦‚å¿µ\nBMCï¼ˆBaseboard Management Controllerï¼‰ åœ¨ä»‹ç»BMCä¹‹å‰éœ€ â€¦","ref":"/linux-notes/baremetal/bmc/","tags":["è£¸é‡‘å±"],"title":"BMCæ¦‚å¿µ"},{"body":"Deploymenté…ç½®é‡‘ä¸é›€å‘å¸ƒ é‡‘ä¸é›€å‘å¸ƒæ˜¯æŒ‡æ§åˆ¶æ›´æ–°è¿‡ç¨‹ä¸­çš„æ»šåŠ¨èŠ‚å¥ï¼Œé€šè¿‡â€œæš‚åœâ€ï¼ˆpauseï¼‰æˆ–â€œç»§ç»­â€ï¼ˆresumeï¼‰æ›´æ–°å‘å¸ƒæ“ä½œã€‚é€šè¿‡ä¸€å°éƒ¨åˆ†çš„ç‰ˆæœ¬å‘å¸ƒå®ä¾‹æ¥è§‚å¯Ÿæ–°ç‰ˆæœ¬æ˜¯å¦æœ‰å¼‚å¸¸ï¼Œå¦‚æœæ²¡æœ‰å¼‚å¸¸åˆ™ä¾æ¬¡å‘å¸ƒå‰©ä½™çš„å®ä¾‹ã€‚\n1. è®¾ç½®å‘ç‰ˆèŠ‚å¥ ä¸»è¦æ˜¯ä»¥ä¸‹å­—æ®µçš„è®¾ç½®ï¼š\nmaxSurgeï¼šæœ€å¤§å‘ç‰ˆå®ä¾‹æ•°ï¼Œå¯ä»¥åˆ›å»ºçš„è¶…å‡ºæœŸæœ› Pod ä¸ªæ•°çš„ Pod æ•°é‡ã€‚å¯ä»¥æ˜¯ç™¾åˆ†æ¯”æˆ–è€…æ˜¯æ•°å­—ã€‚ maxUnavailableï¼šæœ€å¤§ä¸å¯ç”¨å®ä¾‹æ•°ã€‚å¯ä»¥æ˜¯ç™¾åˆ†æ¯”æˆ–æ•°å­—ã€‚ minReadySeconds ï¼šæ–°å»ºçš„ Pod åœ¨æ²¡æœ‰ä»»ä½•å®¹å™¨å´©æºƒçš„æƒ…å†µä¸‹å°±ç»ªå¹¶è¢«ç³»ç»Ÿè§†ä¸ºå¯ç”¨çš„æœ€çŸ­ç§’æ•°ã€‚ é»˜è®¤ä¸º 0ï¼ˆPod å°±ç»ªåå³è¢«è§†ä¸ºå¯ç”¨ï¼‰ã€‚å¯å°†è¯¥å€¼è®¾ç½®ä¸º5-10ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢æ–°èµ·çš„podå‘ç”Ÿcrashï¼Œè¿›è€Œå½±å“æœåŠ¡çš„å¯ç”¨æ€§ï¼Œä¿è¯é›†ç¾¤åœ¨æ›´æ–°è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚ # ä¾‹å¦‚å°†maxSurgeè®¾ç½®ä¸º1ï¼ŒmaxUnavailableè®¾ç½®ä¸º0 $ kubectl patch deployment myapp-deploy -p '{\"spec\": {\"strategy\": {\"rollingUpdate\": {\"maxSurge\": 1, \"maxUnavailable\": 0}}}}' 2. å‡çº§ç‰ˆæœ¬å¹¶æš‚åœ kubectl set image deployment myapp-deploy myapp=kubernetes/myapp:v3 \u0026\u0026 \\ kubectl rollout pause deployments myapp-deploy 3. æŸ¥çœ‹å‡çº§çŠ¶æ€ $ kubectl rollout status deployment myapp-deploy Waiting for deployment \"myapp-deploy\" rollout to finish: 1 out of 3 new replicas have been updated... 4. æ¢å¤ç»§ç»­å‘ç‰ˆ è§‚å¯Ÿç°åº¦çš„å®ä¾‹çš„æµé‡æ˜¯å¦æ­£å¸¸ï¼Œå¦‚æœæ­£å¸¸åˆ™ç»§ç»­å‘ç‰ˆï¼Œå¦‚æœä¸æ­£å¸¸åˆ™å›æ»šä¹‹å‰çš„å‡çº§ã€‚\n$ kubectl rollout resume deployments myapp-deploy 5. å›æ»šå‘å¸ƒ 5.1. å›æ»šä¸Šä¸€ä¸ªç‰ˆæœ¬ kubectl rollout undo deployments myapp-deploy 5.2. æŸ¥çœ‹å†å²ç‰ˆæœ¬ $ kubectl rollout history deployment myapp-deploy deployment.apps/myapp-deploy REVISION CHANGE-CAUSE 3 \u003cnone\u003e 5 \u003cnone\u003e 6 \u003cnone\u003e 5.3. å›æ»šæŒ‡å®šç‰ˆæœ¬ kubectl rollout undo deployment myapp-deploy --to-revision 3 å‚è€ƒï¼š\nhttps://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/manage-deployment/#canary-deployments https://kubernetes.renkeju.com/chapter_5/5.3.4.Canary_release.html https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/deployment-v1/#DeploymentSpec ","categories":"","description":"","excerpt":"Deploymenté…ç½®é‡‘ä¸é›€å‘å¸ƒ é‡‘ä¸é›€å‘å¸ƒæ˜¯æŒ‡æ§åˆ¶æ›´æ–°è¿‡ç¨‹ä¸­çš„æ»šåŠ¨èŠ‚å¥ï¼Œé€šè¿‡â€œæš‚åœâ€ï¼ˆpauseï¼‰æˆ–â€œç»§ç»­â€ï¼ˆresumeï¼‰æ›´æ–°å‘å¸ƒæ“ â€¦","ref":"/kubernetes-notes/operation/deployment/canary-deployment/","tags":["Kubernetes"],"title":"é‡‘ä¸é›€å‘å¸ƒ"},{"body":"1. kube-prometheus-stackç®€ä»‹ kube-prometheus-stackæ˜¯prometheusç›‘æ§k8sé›†ç¾¤çš„å¥—ä»¶ï¼Œå¯ä»¥é€šè¿‡helmä¸€é”®å®‰è£…ï¼ŒåŒæ—¶å¸¦æœ‰ç›‘æ§çš„æ¨¡æ¿ã€‚\nå„ç»„ä»¶åŒ…æ‹¬\ngrafana kube-state-metrics prometheus alertmanager node-exporter 2. å®‰è£…kube-prometheus-stack æ‰§è¡Œä»¥ä¸‹å‘½ä»¤\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n prometheus ç¤ºä¾‹ï¼š\n# helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n prometheus NAME: kube-prometheus-stack LAST DEPLOYED: Wed May 17 17:12:24 2023 NAMESPACE: prometheus STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace prometheus get pods -l \"release=kube-prometheus-stack\" Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026 configure Alertmanager and Prometheus instances using the Operator. 3. æŸ¥çœ‹å®‰è£…ç»“æœ deployment\nkube-prometheus-stack-grafana kube-prometheus-stack-kube-state-metrics kube-prometheus-stack-operator statefulset\nprometheus-kube-prometheus-stack-prometheus alertmanager-kube-prometheus-stack-alertmanager daemonset\nkube-prometheus-stack-prometheus-node-exporter # kg all -n prometheus NAME READY STATUS RESTARTS AGE pod/alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 0 9m34s pod/kube-prometheus-stack-grafana-5bb7689dc8-lgrws 3/3 Running 0 9m35s pod/kube-prometheus-stack-kube-state-metrics-5d6578867c-25xbq 1/1 Running 0 9m35s pod/kube-prometheus-stack-operator-9c5fbdc68-nrn7h 1/1 Running 0 9m35s pod/kube-prometheus-stack-prometheus-node-exporter-8ghd8 1/1 Running 0 48s pod/kube-prometheus-stack-prometheus-node-exporter-brtp9 1/1 Running 0 29s pod/kube-prometheus-stack-prometheus-node-exporter-n4kdp 1/1 Running 0 88s pod/kube-prometheus-stack-prometheus-node-exporter-ttksv 1/1 Running 0 35s pod/prometheus-kube-prometheus-stack-prometheus-0 2/2 Running 0 9m34s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,9094/TCP,9094/UDP 9m34s service/kube-prometheus-stack-alertmanager ClusterIP 10.99.108.180 \u003cnone\u003e 9093/TCP 9m36s service/kube-prometheus-stack-grafana ClusterIP 10.110.62.28 \u003cnone\u003e 80/TCP 9m36s service/kube-prometheus-stack-kube-state-metrics ClusterIP 10.110.105.139 \u003cnone\u003e 8080/TCP 9m35s service/kube-prometheus-stack-operator ClusterIP 10.96.147.204 \u003cnone\u003e 443/TCP 9m36s service/kube-prometheus-stack-prometheus ClusterIP 10.98.235.203 \u003cnone\u003e 9090/TCP 9m36s service/kube-prometheus-stack-prometheus-node-exporter ClusterIP 10.105.99.77 \u003cnone\u003e 9100/TCP 9m36s service/prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 9m34s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/kube-prometheus-stack-prometheus-node-exporter 4 4 4 4 4 \u003cnone\u003e 9m35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/kube-prometheus-stack-grafana 1/1 1 1 9m35s deployment.apps/kube-prometheus-stack-kube-state-metrics 1/1 1 1 9m35s deployment.apps/kube-prometheus-stack-operator 1/1 1 1 9m35s NAME READY AGE statefulset.apps/alertmanager-kube-prometheus-stack-alertmanager 1/1 9m34s statefulset.apps/prometheus-kube-prometheus-stack-prometheus 1/1 9m34s 4. ç™»å½•grafana é»˜è®¤è´¦å·å¯†ç \nè´¦å·ï¼šadmin å¯†ç ï¼šprom-operator é»˜è®¤è´¦å·å¯†ç ä½äºsecretä¸­ï¼Œé€šè¿‡base64è§£ç å¯å¾—ä¸Šè¿°å¯†ç ã€‚\nkg secret -n prometheus kube-prometheus-stack-grafana -oyaml apiVersion: v1 data: admin-password: cHJvbS1vcGVyYXRvcg== admin-user: YWRtaW4= æ¨¡æ¿å†…å®¹ï¼š\npodæ•°æ®ï¼š\nå‚è€ƒï¼š\nhttps://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack https://github.com/prometheus-operator/kube-prometheus ","categories":"","description":"","excerpt":"1. kube-prometheus-stackç®€ä»‹ kube-prometheus-stackæ˜¯prometheusç›‘æ§k8sé›†ç¾¤çš„å¥—ä»¶ï¼Œ â€¦","ref":"/kubernetes-notes/monitor/kube-promethus-stack/","tags":["Monitor"],"title":"kube-prometheus-stackçš„ä½¿ç”¨"},{"body":" æœ¬æ–‡åˆ†æyurthubæºç ï¼Œç¬¬ä¸€éƒ¨åˆ†ã€‚\næœ¬æ–‡ä»¥commit idï¼š180282663457080119a1bc6076cce20c922b5c50ï¼Œ å¯¹åº”ç‰ˆæœ¬tag: v1.2.1 çš„æºç åˆ†æyurthubçš„å®ç°é€»è¾‘ã€‚\nyurthubæ˜¯éƒ¨ç½²åœ¨æ¯ä¸ªè¾¹ç¼˜èŠ‚ç‚¹ä¸Šç”¨æ¥å®ç°è¾¹ç¼˜è‡ªæ²»çš„ç»„ä»¶ã€‚åœ¨äº‘è¾¹é€šä¿¡æ­£å¸¸çš„æƒ…å†µä¸‹å®ç°apiserverçš„è¯·æ±‚è½¬å‘ï¼Œæ–­ç½‘çš„æƒ…å†µä¸‹é€šè¿‡æœ¬åœ°çš„ç¼“å­˜æ•°æ®ä¿è¯èŠ‚ç‚¹ä¸Šå®¹å™¨çš„æ­£å¸¸è¿è¡Œã€‚\nåŸºæœ¬æ¶æ„å›¾ï¼š\npkgåŒ…ä¸­yurthubä»£ç ç›®å½•ç»“æ„ï¼›\nyurthub â”œâ”€â”€ cachemanager # cache ç®¡ç†å™¨ï¼Œ â”œâ”€â”€ certificate # è¯ä¹¦tokenç®¡ç† â”œâ”€â”€ filter â”œâ”€â”€ gc # GCManager â”œâ”€â”€ healthchecker # cloud apiserver æ¢ç«æœºåˆ¶ â”œâ”€â”€ kubernetes # â”œâ”€â”€ metrics â”œâ”€â”€ network # ç½‘ç»œiptablesé…ç½® â”œâ”€â”€ otaupdate â”œâ”€â”€ poolcoordinator â”œâ”€â”€ proxy # æ ¸å¿ƒä»£ç ï¼Œåå‘ä»£ç†æœºåˆ¶ï¼ŒåŒ…æ‹¬remote proxyå’Œlocal proxy â”œâ”€â”€ server # yurthub server â”œâ”€â”€ storage # æœ¬åœ°å­˜å‚¨çš„å®ç° â”œâ”€â”€ tenant â”œâ”€â”€ transport â””â”€â”€ util 1. NewCmdStartYurtHub openyurtçš„ä»£ç é£æ ¼ä¸k8sçš„ä¸€è‡´ï¼Œç”±cmdä¸ºå…¥å£ï¼Œpkgä¸ºä¸»è¦çš„å®ç°é€»è¾‘ã€‚\nä»¥ä¸‹æ˜¯cmdçš„mainå‡½æ•°ã€‚\nfunc main() { newRand := rand.New(rand.NewSource(time.Now().UnixNano())) newRand.Seed(time.Now().UnixNano()) cmd := app.NewCmdStartYurtHub(server.SetupSignalContext()) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { panic(err) } } main å‡½æ•°ä¸»è¦åˆ›å»º NewCmdStartYurtHub å¯¹è±¡ã€‚NewCmdçš„å‡½æ•°ä¸€èˆ¬éƒ½åŒ…å«ä»¥ä¸‹çš„å‡ ä¸ªéƒ¨åˆ†ï¼Œè¿è¡Œé¡ºåºä»ä¸Šåˆ°ä¸‹ï¼š\nNewYurtHubOptionsï¼šåˆ›å»ºoptionå‚æ•°å¯¹è±¡ï¼Œä¸»è¦ç”¨äºflagå‚æ•°è§£æåˆ°optionçš„ç»“æ„ä½“ã€‚\nyurtHubOptions.AddFlags(cmd.Flags())ï¼šæ·»åŠ AddFlagsï¼Œè®¾ç½®flagå‚æ•°ä¿¡æ¯ã€‚\nyurtHubOptions.Validate()ï¼šæ ¡éªŒflagè§£æåçš„optionçš„å‚æ•°åˆæ³•æ€§ã€‚\nyurtHubCfg, err := config.Complete(yurtHubOptions)ï¼šå°†optionçš„å‚æ•°è½¬æ¢ä¸ºconfigçš„å¯¹è±¡ã€‚\nRun(ctx, yurtHubCfg)ï¼šåŸºäºconfigæ‰§è¡Œrunå‡½æ•°ï¼Œè¿è¡Œcmdçš„æ ¸å¿ƒé€»è¾‘ã€‚\n// NewCmdStartYurtHub creates a *cobra.Command object with default parameters func NewCmdStartYurtHub(ctx context.Context) *cobra.Command { yurtHubOptions := options.NewYurtHubOptions() cmd := \u0026cobra.Command{ Use: projectinfo.GetHubName(), Short: \"Launch \" + projectinfo.GetHubName(), Long: \"Launch \" + projectinfo.GetHubName(), Run: func(cmd *cobra.Command, args []string) { if yurtHubOptions.Version { fmt.Printf(\"%s: %#v\\n\", projectinfo.GetHubName(), projectinfo.Get()) return } fmt.Printf(\"%s version: %#v\\n\", projectinfo.GetHubName(), projectinfo.Get()) cmd.Flags().VisitAll(func(flag *pflag.Flag) { klog.V(1).Infof(\"FLAG: --%s=%q\", flag.Name, flag.Value) }) if err := yurtHubOptions.Validate(); err != nil { klog.Fatalf(\"validate options: %v\", err) } yurtHubCfg, err := config.Complete(yurtHubOptions) if err != nil { klog.Fatalf(\"complete %s configuration error, %v\", projectinfo.GetHubName(), err) } klog.Infof(\"%s cfg: %#+v\", projectinfo.GetHubName(), yurtHubCfg) if err := Run(ctx, yurtHubCfg); err != nil { klog.Fatalf(\"run %s failed, %v\", projectinfo.GetHubName(), err) } }, } yurtHubOptions.AddFlags(cmd.Flags()) return cmd } ä»¥ä¸Šflagã€optionã€configçš„æ„å»ºå‡½æ•°æ­¤å¤„ä¸åšåˆ†æï¼Œä»¥ä¸‹åˆ†ærunå‡½æ•°çš„é€»è¾‘ã€‚\n2. Run(ctx, yurtHubCfg) Runå‡½æ•°éƒ¨åˆ†ä¸»è¦æ„å»ºäº†å‡ ä¸ªmanagerï¼Œæ¯ä¸ªmanagerå„å¸å…¶èŒï¼Œè´Ÿè´£å¯¹åº”çš„é€»è¾‘ã€‚æœ‰çš„manageråœ¨è¯¥å‡½æ•°ä¸­ç›´æ¥æ„å»ºåæ‰§è¡Œmanager.runçš„é€»è¾‘ã€‚æœ‰çš„åˆ™ä½œä¸ºå‚æ•°ä¼ å…¥ä¸‹ä¸€çº§å‡½æ•°ä¸­å†æ‰§è¡Œmanager.runå‡½æ•°ã€‚\nä¸»è¦åŒ…æ‹¬ä»¥ä¸‹çš„managerï¼š\ntransportManager\ncloudHealthChecker\nrestConfigMgr\ncacheMgr\ngcMgr\ntenantMgr\nNetworkMgr\næ¯ä¸ªmanagerçš„å®ç°ç»†èŠ‚æ­¤å¤„æš‚ä¸åšåˆ†æã€‚\næ­¤å¤„å…ˆè´´ä¸€ä¸‹å®Œæ•´æºç ï¼Œé¿å…è¯»è€…è¿˜éœ€è¦å»ç¿»ä»£ç ã€‚\nä»£ç ï¼š/cmd/yurthub/app/start.go\n// Run runs the YurtHubConfiguration. This should never exit func Run(ctx context.Context, cfg *config.YurtHubConfiguration) error { defer cfg.CertManager.Stop() trace := 1 klog.Infof(\"%d. new transport manager\", trace) // æ„é€ NewTransportManager transportManager, err := transport.NewTransportManager(cfg.CertManager, ctx.Done()) if err != nil { return fmt.Errorf(\"could not new transport manager, %w\", err) } trace++ klog.Infof(\"%d. prepare cloud kube clients\", trace) cloudClients, err := createClients(cfg.HeartbeatTimeoutSeconds, cfg.RemoteServers, cfg.CoordinatorServerURL, transportManager) if err != nil { return fmt.Errorf(\"failed to create cloud clients, %w\", err) } trace++ var cloudHealthChecker healthchecker.MultipleBackendsHealthChecker if cfg.WorkingMode == util.WorkingModeEdge { klog.Infof(\"%d. create health checkers for remote servers and pool coordinator\", trace) cloudHealthChecker, err = healthchecker.NewCloudAPIServerHealthChecker(cfg, cloudClients, ctx.Done()) if err != nil { return fmt.Errorf(\"could not new cloud health checker, %w\", err) } } else { klog.Infof(\"%d. disable health checker for node %s because it is a cloud node\", trace, cfg.NodeName) // In cloud mode, cloud health checker is not needed. // This fake checker will always report that the cloud is healthy and pool coordinator is unhealthy. cloudHealthChecker = healthchecker.NewFakeChecker(true, make(map[string]int)) } trace++ klog.Infof(\"%d. new restConfig manager\", trace) restConfigMgr, err := hubrest.NewRestConfigManager(cfg.CertManager, cloudHealthChecker) if err != nil { return fmt.Errorf(\"could not new restConfig manager, %w\", err) } trace++ var cacheMgr cachemanager.CacheManager if cfg.WorkingMode == util.WorkingModeEdge { klog.Infof(\"%d. new cache manager with storage wrapper and serializer manager\", trace) cacheMgr = cachemanager.NewCacheManager(cfg.StorageWrapper, cfg.SerializerManager, cfg.RESTMapperManager, cfg.SharedFactory) } else { klog.Infof(\"%d. disable cache manager for node %s because it is a cloud node\", trace, cfg.NodeName) } trace++ if cfg.WorkingMode == util.WorkingModeEdge { klog.Infof(\"%d. new gc manager for node %s, and gc frequency is a random time between %d min and %d min\", trace, cfg.NodeName, cfg.GCFrequency, 3*cfg.GCFrequency) gcMgr, err := gc.NewGCManager(cfg, restConfigMgr, ctx.Done()) if err != nil { return fmt.Errorf(\"could not new gc manager, %w\", err) } // ç›´æ¥è¿è¡Œmanager gcMgr.Run() } else { klog.Infof(\"%d. disable gc manager for node %s because it is a cloud node\", trace, cfg.NodeName) } trace++ klog.Infof(\"%d. new tenant sa manager\", trace) tenantMgr := tenant.New(cfg.TenantNs, cfg.SharedFactory, ctx.Done()) trace++ var coordinatorHealthCheckerGetter func() healthchecker.HealthChecker = getFakeCoordinatorHealthChecker var coordinatorTransportManagerGetter func() transport.Interface = getFakeCoordinatorTransportManager var coordinatorGetter func() poolcoordinator.Coordinator = getFakeCoordinator if cfg.EnableCoordinator { klog.Infof(\"%d. start to run coordinator\", trace) trace++ coordinatorInformerRegistryChan := make(chan struct{}) // coordinatorRun will register secret informer into sharedInformerFactory, and start a new goroutine to periodically check // if certs has been got from cloud APIServer. It will close the coordinatorInformerRegistryChan if the secret channel has // been registered into informer factory. coordinatorHealthCheckerGetter, coordinatorTransportManagerGetter, coordinatorGetter = coordinatorRun(ctx, cfg, restConfigMgr, cloudHealthChecker, coordinatorInformerRegistryChan) // wait for coordinator informer registry klog.Infof(\"waiting for coordinator informer registry\") \u003c-coordinatorInformerRegistryChan klog.Infof(\"coordinator informer registry finished\") } // Start the informer factory if all informers have been registered cfg.SharedFactory.Start(ctx.Done()) cfg.YurtSharedFactory.Start(ctx.Done()) klog.Infof(\"%d. new reverse proxy handler for remote servers\", trace) // å°†ä¹‹å‰æ„é€ çš„managerä½œä¸ºå‚æ•°æ„å»ºyurtProxyHandler yurtProxyHandler, err := proxy.NewYurtReverseProxyHandler( cfg, cacheMgr, transportManager, cloudHealthChecker, tenantMgr, coordinatorGetter, coordinatorTransportManagerGetter, coordinatorHealthCheckerGetter, ctx.Done()) if err != nil { return fmt.Errorf(\"could not create reverse proxy handler, %w\", err) } trace++ if cfg.NetworkMgr != nil { cfg.NetworkMgr.Run(ctx.Done()) } klog.Infof(\"%d. new %s server and begin to serve\", trace, projectinfo.GetHubName()) // åŸºäºyurtProxyHandlerè¿è¡Œä¸€ä¸ªhttp server. if err := server.RunYurtHubServers(cfg, yurtProxyHandler, restConfigMgr, ctx.Done()); err != nil { return fmt.Errorf(\"could not run hub servers, %w\", err) } \u003c-ctx.Done() klog.Infof(\"hub agent exited\") return nil } é™¤äº†ä¸Šè¿°çš„å„ç§managerçš„æ„é€ åŠè¿è¡Œå¤–ï¼Œrunå‡½æ•°ä¸­è¿˜æ„å»ºäº†yurtProxyHandlerï¼Œæœ€ç»ˆæ‰§è¡ŒRunYurtHubServersè¿è¡Œä¸€ç»„ä¸ä¼šé€€å‡ºçš„http serverã€‚ä»¥ä¸‹å…ˆä¸å¯¹managerçš„å®ç°åšå±•å¼€ï¼Œè€Œç›´æ¥åˆ†æRunYurtHubServersçš„é€»è¾‘ã€‚RunYurtHubServersçš„ä»£ç åœ¨pkgåŒ…ä¸­ã€‚\n3. RunYurtHubServers RunYurtHubServerså°±æ˜¯ä¸€ä¸ªä¼ ç»Ÿçš„http serverçš„è¿è¡Œé€»è¾‘ï¼Œä¸»è¦åŒ…æ‹¬å‡ ä¸ªä¸åŒç±»å‹çš„http serverã€‚http serverçš„è¿è¡Œé€»è¾‘å¯ä»¥æ¦‚æ‹¬å¦‚ä¸‹ï¼š\nhubServerHandler := mux.NewRouter()ï¼š æ–°å»ºè·¯ç”±åˆ›å»ºhandler\nregisterHandlers(hubServerHandler, cfg, rest)ï¼š æ³¨å†Œè·¯ç”±\nYurtHubServerServing.Serveï¼šæ‰§è¡Œhttp server.Serveå‡½æ•°å¯åŠ¨ä¸€ä¸ªserveræœåŠ¡ã€‚\nhttp serveråˆ†ä¸ºä¸¤ç±»ï¼š\nyurthub http server: yurthub metrics, healthzçš„æ¥å£ã€‚\nyurthub proxy server: ä»£ç†kube-apiserverçš„è¯·æ±‚ã€‚\n3.1. YurtHubServerServing hubServerHandler := mux.NewRouter() registerHandlers(hubServerHandler, cfg, rest) // start yurthub http server for serving metrics, pprof. if cfg.YurtHubServerServing != nil { if err := cfg.YurtHubServerServing.Serve(hubServerHandler, 0, stopCh); err != nil { return err } } registerHandlersçš„è·¯ç”±å†…å®¹å¦‚ä¸‹ï¼š\n// registerHandler registers handlers for yurtHubServer, and yurtHubServer can handle requests like profiling, healthz, update token. func registerHandlers(c *mux.Router, cfg *config.YurtHubConfiguration, rest *rest.RestConfigManager) { // register handlers for update join token c.Handle(\"/v1/token\", updateTokenHandler(cfg.CertManager)).Methods(\"POST\", \"PUT\") // register handler for health check c.HandleFunc(\"/v1/healthz\", healthz).Methods(\"GET\") c.Handle(\"/v1/readyz\", readyz(cfg.CertManager)).Methods(\"GET\") // register handler for profile if cfg.EnableProfiling { profile.Install(c) } // register handler for metrics c.Handle(\"/metrics\", promhttp.Handler()) // register handler for ota upgrade c.Handle(\"/pods\", ota.GetPods(cfg.StorageWrapper)).Methods(\"GET\") c.Handle(\"/openyurt.io/v1/namespaces/{ns}/pods/{podname}/upgrade\", ota.HealthyCheck(rest, cfg.NodeName, ota.UpdatePod)).Methods(\"POST\") } ä»¥ä¸Šè·¯ç”±ä¸åšæ·±å…¥åˆ†æã€‚\n3.2. YurtHubProxyServerServing YurtHubProxyServerServingä¸»è¦ä»£ç†kube-apiserverçš„è½¬å‘è¯·æ±‚ã€‚\n// start yurthub proxy servers for forwarding requests to cloud kube-apiserver if cfg.WorkingMode == util.WorkingModeEdge { proxyHandler = wrapNonResourceHandler(proxyHandler, cfg, rest) } if cfg.YurtHubProxyServerServing != nil { if err := cfg.YurtHubProxyServerServing.Serve(proxyHandler, 0, stopCh); err != nil { return err } } ä»¥ä¸‹åˆ†æyurtProxyHandlerçš„é€»è¾‘ã€‚\n3.3. NewYurtReverseProxyHandler NewYurtReverseProxyHandlerä¸»è¦åˆ›å»ºäº†http handler ä»£ç†æ‰€æœ‰è½¬å‘è¯·æ±‚ã€‚\n1ã€åˆ›å»ºLoad Balancerï¼Œä¸»è¦ç”¨æ¥è½¬å‘apiserverçš„è¯·æ±‚ã€‚\nlb, err := remote.NewLoadBalancer( yurtHubCfg.LBMode, yurtHubCfg.RemoteServers, localCacheMgr, transportMgr, coordinatorGetter, cloudHealthChecker, yurtHubCfg.FilterManager, yurtHubCfg.WorkingMode, stopCh) 2ã€åˆ›å»ºlocal Proxyï¼Œä¸»è¦ç”¨æ¥è½¬å‘æœ¬åœ°ç¼“å­˜çš„è¯·æ±‚ã€‚\n// When yurthub works in Edge mode, we may use local proxy or pool proxy to handle // the request when offline. localProxy = local.NewLocalProxy(localCacheMgr, cloudHealthChecker.IsHealthy, isCoordinatorHealthy, yurtHubCfg.MinRequestTimeout, ) localProxy = local.WithFakeTokenInject(localProxy, yurtHubCfg.SerializerManager) 3ã€åˆ›å»ºyurtReverseProxy\nyurtProxy := \u0026yurtReverseProxy{ resolver: resolver, loadBalancer: lb, cloudHealthChecker: cloudHealthChecker, coordinatorHealtCheckerGetter: coordinatorHealthCheckerGetter, localProxy: localProxy, poolProxy: poolProxy, maxRequestsInFlight: yurtHubCfg.MaxRequestInFlight, isCoordinatorReady: isCoordinatorReady, enablePoolCoordinator: yurtHubCfg.EnableCoordinator, tenantMgr: tenantMgr, workingMode: yurtHubCfg.WorkingMode, } return yurtProxy.buildHandlerChain(yurtProxy), nil 4. yurtReverseProxy yurtReverseProxyä¸»è¦æ˜¯ä½œä¸ºå®ç°åå‘ä»£ç†çš„ç»“æ„ä½“ã€‚\ntype yurtReverseProxy struct { resolver apirequest.RequestInfoResolver loadBalancer remote.LoadBalancer cloudHealthChecker healthchecker.MultipleBackendsHealthChecker coordinatorHealtCheckerGetter func() healthchecker.HealthChecker localProxy http.Handler poolProxy http.Handler maxRequestsInFlight int tenantMgr tenant.Interface isCoordinatorReady func() bool workingMode hubutil.WorkingMode enablePoolCoordinator bool } åå‘ä»£ç†æœåŠ¡\nfunc (p *yurtReverseProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) { if p.workingMode == hubutil.WorkingModeCloud { p.loadBalancer.ServeHTTP(rw, req) return } switch { case util.IsKubeletLeaseReq(req): p.handleKubeletLease(rw, req) case util.IsEventCreateRequest(req): p.eventHandler(rw, req) case util.IsPoolScopedResouceListWatchRequest(req): p.poolScopedResouceHandler(rw, req) case util.IsSubjectAccessReviewCreateGetRequest(req): p.subjectAccessReviewHandler(rw, req) default: // For resource request that do not need to be handled by pool-coordinator, // handling the request with cloud apiserver or local cache. if p.cloudHealthChecker.IsHealthy() { p.loadBalancer.ServeHTTP(rw, req) } else { p.localProxy.ServeHTTP(rw, req) } } } æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæ˜¯äº‘ç«¯apiserverå¯ä»¥è®¿é—®çš„é€šï¼Œåˆ™é€šè¿‡loadbalaneræ¥è½¬å‘ï¼Œå¦åˆ™å°±é€šè¿‡localproxyæ¥è½¬å‘è¯»å–æœ¬åœ°èŠ‚ç‚¹çš„æ•°æ®ã€‚\n5. LoadBalancer LoadBalanceræ˜¯ä¸ªæœ¬åœ°çš„è´Ÿè½½å‡è¡¡é€»è¾‘ï¼Œé€šè¿‡è½®è¯¢çš„æ–¹å¼å»è¯·æ±‚cloudçš„apiserverï¼Œå½“äº‘è¾¹ç½‘ç»œé€šä¿¡æ˜¯æ­£å¸¸çš„æ—¶å€™åå‘ä»£ç†apiserverçš„è¯·æ±‚ï¼Œå¹¶åšæœ¬åœ°ç¼“å­˜æŒä¹…åŒ–ã€‚æ–­ç½‘çš„æ—¶å€™åˆ™è¯»å–æœ¬åœ°çš„ç¼“å­˜æ•°æ®ã€‚\nbackendsï¼šçœŸå®åå‘ä»£ç†çš„åç«¯\nalgo: å¤„ç†è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œè½®è¯¢æˆ–è€…æŒ‰ä¼˜å…ˆçº§\nlocalCacheMgr: æœ¬åœ°ç¼“å­˜ç®¡ç†çš„manager\nä»£ç ï¼š/pkg/yurthub/proxy/remote/loadbalancer.go\ntype loadBalancer struct { backends []*util.RemoteProxy algo loadBalancerAlgo localCacheMgr cachemanager.CacheManager filterManager *manager.Manager coordinatorGetter func() poolcoordinator.Coordinator workingMode hubutil.WorkingMode stopCh \u003c-chan struct{} } 5.1. NewLoadBalancer NewLoadBalanceræ„å»ºä¸€ä¸ªremoteçš„åå‘ä»£ç†ï¼Œä¸»è¦åŒ…å«æ·»åŠ romote server proxyå’Œå¤„ç†è´Ÿè½½å‡è¡¡ç­–ç•¥ä¸¤éƒ¨åˆ†ã€‚\n1ã€æ·»åŠ å¤šä¸ªapiserverçš„åœ°å€ï¼Œåˆ›å»ºremote proxyå®ç°åå‘ä»£ç†æ“ä½œã€‚\nbackends := make([]*util.RemoteProxy, 0, len(remoteServers)) for i := range remoteServers { b, err := util.NewRemoteProxy(remoteServers[i], lb.modifyResponse, lb.errorHandler, transportMgr, stopCh) if err != nil { klog.Errorf(\"could not new proxy backend(%s), %v\", remoteServers[i].String(), err) continue } backends = append(backends, b) } 2ã€å¤„ç†è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼š\nvar algo loadBalancerAlgo switch lbMode { case \"rr\": algo = \u0026rrLoadBalancerAlgo{backends: backends, checker: healthChecker} case \"priority\": algo = \u0026priorityLoadBalancerAlgo{backends: backends, checker: healthChecker} default: algo = \u0026rrLoadBalancerAlgo{backends: backends, checker: healthChecker} } 5.2. loadBalancer.ServeHTTP loadBalancerå®ç°ServeHTTPçš„æ¥å£ï¼Œé€šè¿‡è´Ÿè½½å‡è¡¡ç­–ç•¥æŒ‘é€‰å‡ºä¸€ä¸ªå¯ç”¨çš„åå‘ä»£ç†backendã€‚å†è°ƒç”¨backendçš„ServeHTTPæ–¹æ³•å®ç°å…·ä½“çš„åå‘ä»£ç†æ“ä½œã€‚\n// pick a remote proxy based on the load balancing algorithm. rp := lb.algo.PickOne() rp.ServeHTTP(rw, req) 5.3. errorHandler å¦‚æœè¯·æ±‚apiserverå¤±è´¥ï¼Œå½“verb=get/list, åˆ™è¯»å–cacheä¸­çš„å†…å®¹ã€‚\nfunc (lb *loadBalancer) errorHandler(rw http.ResponseWriter, req *http.Request, err error) { klog.Errorf(\"remote proxy error handler: %s, %v\", hubutil.ReqString(req), err) if lb.localCacheMgr == nil || !lb.localCacheMgr.CanCacheFor(req) { rw.WriteHeader(http.StatusBadGateway) return } ctx := req.Context() if info, ok := apirequest.RequestInfoFrom(ctx); ok { if info.Verb == \"get\" || info.Verb == \"list\" { # è¯»å–cacheå†…å®¹ if obj, err := lb.localCacheMgr.QueryCache(req); err == nil { hubutil.WriteObject(http.StatusOK, obj, rw, req) return } } } rw.WriteHeader(http.StatusBadGateway) } 5.4. QueryCache // QueryCache get runtime object from backend storage for request func (cm *cacheManager) QueryCache(req *http.Request) (runtime.Object, error) { ctx := req.Context() info, ok := apirequest.RequestInfoFrom(ctx) if !ok || info == nil || info.Resource == \"\" { return nil, fmt.Errorf(\"failed to get request info for request %s\", util.ReqString(req)) } if !info.IsResourceRequest { return nil, fmt.Errorf(\"failed to QueryCache for getting non-resource request %s\", util.ReqString(req)) } # æ ¹æ®verbæŸ¥è¯¢storageä¸­çš„æ•°æ® switch info.Verb { case \"list\": return cm.queryListObject(req) case \"get\", \"patch\", \"update\": return cm.queryOneObject(req) default: return nil, fmt.Errorf(\"failed to QueryCache, unsupported verb %s of request %s\", info.Verb, util.ReqString(req)) } } 5.5. æŸ¥è¯¢storageä¸­çš„æ•°æ® func (cm *cacheManager) queryOneObject(req *http.Request) (runtime.Object, error) { ... klog.V(4).Infof(\"component: %s try to get key: %s\", comp, key.Key()) obj, err := cm.storage.Get(key) if err != nil { klog.Errorf(\"failed to get obj %s from storage, %v\", key.Key(), err) return nil, err } ... } ç›®å‰å­˜å‚¨æœ‰ä¸¤ç§æ¥å£å®ç°ï¼Œä¸€ä¸ªæ˜¯æœ¬åœ°ç£ç›˜å­˜å‚¨ï¼Œä¸€ä¸ªæ˜¯etcdå­˜å‚¨ã€‚ä»¥ä¸‹ä»¥ç£ç›˜å­˜å‚¨ä¸ºä¾‹åˆ†æã€‚\nä»£ç ï¼š/pkg/yurthub/storage/disk/storage.go\n// Get will get content from the regular file that specified by key. // If key points to a dir, return ErrKeyHasNoContent. func (ds *diskStorage) Get(key storage.Key) ([]byte, error) { if err := utils.ValidateKey(key, storageKey{}); err != nil { return []byte{}, storage.ErrKeyIsEmpty } storageKey := key.(storageKey) if !ds.lockKey(storageKey) { return nil, storage.ErrStorageAccessConflict } defer ds.unLockKey(storageKey) path := filepath.Join(ds.baseDir, storageKey.Key()) buf, err := ds.fsOperator.Read(path) switch err { case nil: return buf, nil case fs.ErrNotExists: return nil, storage.ErrStorageNotFound case fs.ErrIsNotFile: return nil, storage.ErrKeyHasNoContent default: return buf, fmt.Errorf(\"failed to read file at %s, %v\", path, err) } } 6. RemoteProxy RemoteProxyå®ç°ä¸€ä¸ªå…·ä½“çš„åå‘ä»£ç†æ“ä½œã€‚\nå­—æ®µè¯´æ˜ï¼š\nreverseProxyï¼šhttpçš„ReverseProxy\nremoteServerï¼šapiserverçš„åœ°å€\nä»£ç ï¼š/pkg/yurthub/proxy/util/remote.go\n// RemoteProxy is an reverse proxy for remote server type RemoteProxy struct { reverseProxy *httputil.ReverseProxy remoteServer *url.URL currentTransport http.RoundTripper bearerTransport http.RoundTripper upgradeHandler *proxy.UpgradeAwareHandler bearerUpgradeHandler *proxy.UpgradeAwareHandler stopCh \u003c-chan struct{} } å®ç°ReverseProxyçš„ServeHTTPæ¥å£ã€‚\nfunc (rp *RemoteProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) { if httpstream.IsUpgradeRequest(req) { klog.V(5).Infof(\"get upgrade request %s\", req.URL) if isBearerRequest(req) { rp.bearerUpgradeHandler.ServeHTTP(rw, req) } else { rp.upgradeHandler.ServeHTTP(rw, req) } return } rp.reverseProxy.ServeHTTP(rw, req) } å®ç°é”™è¯¯å¤„ç†çš„æ¥å£ã€‚\nfunc (r *responder) Error(w http.ResponseWriter, req *http.Request, err error) { klog.Errorf(\"failed while proxying request %s, %v\", req.URL, err) http.Error(w, err.Error(), http.StatusInternalServerError) } 7. LocalProxy LocalProxyæ˜¯ä¸€ä¸ªå½“äº‘è¾¹ç½‘ç»œæ–­å¼€çš„æ—¶å€™ï¼Œç”¨äºå¤„ç†æœ¬åœ°kubeletè¯·æ±‚çš„æ•°æ®çš„ä»£ç†ã€‚\nå­—æ®µè¯´æ˜ï¼š\ncacheMgrï¼šä¸»è¦åŒ…å«æœ¬åœ°cacheçš„ä¸€ä¸ªå¤„ç†ç®¡ç†å™¨ã€‚ ä»£ç ï¼š/pkg/yurthub/proxy/local/local.go\n// LocalProxy is responsible for handling requests when remote servers are unhealthy type LocalProxy struct { cacheMgr manager.CacheManager isCloudHealthy IsHealthy isCoordinatorReady IsHealthy minRequestTimeout time.Duration } LocalProxyå®ç°ServeHTTPæ¥å£ï¼Œæ ¹æ®ä¸åŒçš„k8sè¯·æ±‚ç±»å‹ï¼Œæ‰§è¡Œä¸åŒçš„æ“ä½œï¼š\nwatchï¼šlp.localWatch(w, req)\ncreateï¼šlp.localPost(w, req)\ndelete, deletecollection: localDelete(w, req)\nlist., get, updateï¼šlp.localReqCache(w, req)\n// ServeHTTP implements http.Handler for LocalProxy func (lp *LocalProxy) ServeHTTP(w http.ResponseWriter, req *http.Request) { var err error ctx := req.Context() if reqInfo, ok := apirequest.RequestInfoFrom(ctx); ok \u0026\u0026 reqInfo != nil \u0026\u0026 reqInfo.IsResourceRequest { klog.V(3).Infof(\"go into local proxy for request %s\", hubutil.ReqString(req)) switch reqInfo.Verb { case \"watch\": err = lp.localWatch(w, req) case \"create\": err = lp.localPost(w, req) case \"delete\", \"deletecollection\": err = localDelete(w, req) default: // list, get, update err = lp.localReqCache(w, req) } if err != nil { klog.Errorf(\"could not proxy local for %s, %v\", hubutil.ReqString(req), err) util.Err(err, w, req) } } else { klog.Errorf(\"local proxy does not support request(%s), requestInfo: %s\", hubutil.ReqString(req), hubutil.ReqInfoString(reqInfo)) util.Err(apierrors.NewBadRequest(fmt.Sprintf(\"local proxy does not support request(%s)\", hubutil.ReqString(req))), w, req) } } 7.1. localReqCache å½“è¾¹ç¼˜ç½‘ç»œæ–­è¿çš„æ—¶å€™ï¼Œkubeletæ‰§è¡Œget listçš„æ“ä½œæ—¶ï¼Œé€šè¿‡localReqCacheè¯·æ±‚æœ¬åœ°ç¼“å­˜çš„æ•°æ®ï¼Œè¿”å›ç»™kubeletå¯¹åº”çš„k8så…ƒæ•°æ®ã€‚\n// localReqCache handles Get/List/Update requests when remote servers are unhealthy func (lp *LocalProxy) localReqCache(w http.ResponseWriter, req *http.Request) error { if !lp.cacheMgr.CanCacheFor(req) { klog.Errorf(\"can not cache for %s\", hubutil.ReqString(req)) return apierrors.NewBadRequest(fmt.Sprintf(\"can not cache for %s\", hubutil.ReqString(req))) } obj, err := lp.cacheMgr.QueryCache(req) if errors.Is(err, storage.ErrStorageNotFound) || errors.Is(err, hubmeta.ErrGVRNotRecognized) { klog.Errorf(\"object not found for %s\", hubutil.ReqString(req)) reqInfo, _ := apirequest.RequestInfoFrom(req.Context()) return apierrors.NewNotFound(schema.GroupResource{Group: reqInfo.APIGroup, Resource: reqInfo.Resource}, reqInfo.Name) } else if err != nil { klog.Errorf(\"failed to query cache for %s, %v\", hubutil.ReqString(req), err) return apierrors.NewInternalError(err) } else if obj == nil { klog.Errorf(\"no cache object for %s\", hubutil.ReqString(req)) return apierrors.NewInternalError(fmt.Errorf(\"no cache object for %s\", hubutil.ReqString(req))) } return util.WriteObject(http.StatusOK, obj, w, req) } æ ¸å¿ƒä»£ç ä¸ºï¼š\næŸ¥è¯¢æœ¬åœ°ç¼“å­˜ï¼Œè¿”å›ç¼“å­˜æ•°æ®ã€‚\nobj, err := lp.cacheMgr.QueryCache(req) return util.WriteObject(http.StatusOK, obj, w, req) æ€»ç»“ yurthubæ˜¯å®ç°è¾¹ç¼˜æ–­ç½‘è‡ªæ²»çš„æ ¸å¿ƒç»„ä»¶ï¼Œæ ¸å¿ƒé€»è¾‘æ˜¯kubeletå‘apiserverçš„è¯·æ±‚ä¼šé€šè¿‡yurhubè¿›è¡Œè½¬å‘ï¼Œå¦‚æœapiserverçš„æ¥å£å¯é€šï¼Œåˆ™å°†è¯·æ±‚ç»“æœè¿”å›ï¼Œå¹¶å­˜å‚¨åˆ°æœ¬åœ°ï¼Œå¦‚æœæ¥å£ä¸å¯é€šï¼Œåˆ™è¯»å–æœ¬åœ°çš„æ•°æ®ã€‚\nyurthubæœ¬è´¨æ˜¯ä¸€ä¸ªåå‘ä»£ç†çš„http server, æ ¸å¿ƒé€»è¾‘ä¸»è¦åŒ…æ‹¬ ï¼š\n- proxy: åå‘ä»£ç†çš„å®ç° - cachemanagerï¼šcacheçš„å®ç° - storageï¼šæœ¬åœ°å­˜å‚¨çš„å®ç° å‚è€ƒï¼š\nhttps://github.com/openyurtio/openyurt ","categories":"","description":"","excerpt":" æœ¬æ–‡åˆ†æyurthubæºç ï¼Œç¬¬ä¸€éƒ¨åˆ†ã€‚\næœ¬æ–‡ä»¥commit â€¦","ref":"/kubernetes-notes/edge/openyurt/code-analysis/yurthub-code-analysis-1/","tags":["OpenYurt"],"title":"OpenYurtä¹‹YurtHubæºç åˆ†æ"},{"body":"1. ç®€ä»‹ linuxç³»ç»Ÿä¸Šå¸¸ç”¨tcpdumpæŠ“åŒ…æ¥åˆ†æç½‘ç»œé—®é¢˜ã€‚æœ¬æ–‡åŸºäºç½‘ç»œæ–‡ç« æ•´ç†ï¼Œä¸»è¦ä»‹ç»tcpdumpæŠ“åŒ…çš„å¸¸ç”¨å‘½ä»¤åŠå‚æ•°ã€‚\nä»¥ä¸‹æ˜¯æ•°æ®åŒ…åœ¨æ“ä½œç³»ç»Ÿå±‚é¢çš„æµç¨‹ï¼š\nç½‘å¡nic -\u003e tcpdump -\u003e iptables(netfilter) -\u003e app -\u003e iptables(netfilter) -\u003e tcpdump -\u003e ç½‘å¡nic\n2. tcpdumpå¸¸ç”¨å‚æ•°åŠå‘½ä»¤ 2.1. æŒ‡å®šç½‘å¡(-i)å’Œä¸»æœº(host) tcpdumpé»˜è®¤ä¼šå°†IPåå‘è§£æä¸ºåŸŸåï¼Œå¯ä»¥ç”¨-nnç¦æ­¢åå‘è§£æã€‚\n-iï¼šæŒ‡å®šç½‘å¡\nhostï¼šæŒ‡å®šä¸»æœº\n-nnï¼šç¦æ­¢åå‘è§£æåŸŸå\n-væˆ–-vvï¼šæ˜¾ç¤ºæŠ“åŒ…çš„è¯¦ç»†ä¿¡æ¯\n-w: å†™å…¥æ–‡ä»¶ï¼ˆ.pcapæˆ–.capï¼‰ï¼Œä¾›wiresharkåˆ†æã€‚\ntcpdump -i any host 192.168.1.1 #-iæŒ‡å®šç½‘å¡ä¸ºæ‰€æœ‰ tcpdump -i eth0 host 192.168.1.1 #-iæŒ‡å®šç½‘å¡ä¸ºeth0 tcpdump -i eth0 host 192.168.1.1 -nn -v -w client.pcap # å†™å…¥æ–‡ä»¶ 2.2. æŒ‡å®šæ¥æºIPæˆ–ç›®çš„IPã€ç½‘æ®µ srcï¼šæŒ‡å®šæ¥æºIP\ndst: æŒ‡å®šç›®æ ‡IP\nnet: æŒ‡å®šç½‘æ®µ\n-s : æŒ‡å®šæŠ“åŒ…å­—èŠ‚æ•°ï¼Œ-s 0ä¸é™å­—èŠ‚æ•°ï¼ŒæŠ“å®Œæ•´çš„åŒ…ã€‚ä¾‹å¦‚icmp å¤§å°ä¸º84å­—èŠ‚\nport: æŒ‡å®šç«¯å£\nportrange: æŒ‡å®šç«¯å£èŒƒå›´\nåè®®ï¼štcp, udp, icmp\n# æŒ‡å®šæºIP tcpdump -nn -i any src host 192.168.1.1 # æŒ‡å®šç›®æ ‡IP tcpdump -nn -i any dst 192.168.1.1 # æŒ‡å®šç½‘æ®µ tcpdump -nn -i any net 192.168.1.1/32 # æŒ‡å®šå­—èŠ‚æ•° tcpdump -nn -i any -s 84 host 192.168.1.1 # 84è¡¨ç¤ºicmpçš„åŒ… # æŒ‡å®šåè®® tcpdump -nn -i any -s 0 icmp #åªæŠ“icmpåè®® tcpdump -nn -i any -s 60 tcp port 80 #tcpåè®®ï¼Œè¿™é‡ŒåªæŠ“60ä¸ªå¤´éƒ¨å­—èŠ‚ tcpdump -nn -i any -s 0 udp port 22 # udpåè®® # æŒ‡å®šç«¯å£æˆ–èŒƒå›´ tcpdump -nn -i any -s 0 port 22 tcpdump -nn -i any tcp portrange 53-80 2.3. æŒ‡å®šæŠ“åŒ…æ•°é‡ã€æŠ“åŒ…å¤§å°ã€åŠè½®è¯¢æŠ“åŒ… -c: æŒ‡å®šæŠ“å¤šå°‘ä¸ªåŒ…\n-W: æœ€å¤šå†™å…¥å¤šå°‘ä¸ªæŠ“åŒ…æ–‡ä»¶ï¼Œä»¥MBä¸ºå•ä½\n-Cï¼šå†™å…¥åˆ°æŠ“åŒ…æ–‡ä»¶çš„å¤§å°ä¸Šé™\n-Gï¼šå‚æ•°æŒ‡å®šé—´éš”å¤šå°‘ç§’è½®è¯¢ä¿å­˜ä¸€æ¬¡æ–‡ä»¶ï¼Œé€šå¸¸æ˜¯ä»¥æ—¶é—´æ ¼å¼å‘½ä»¤\n# æŒ‡å®šæŠ“2ä¸ªåŒ… tcpdump -i any -s 0 net 192.168.1.1/32 -c 2 # æŒ‡å®šå†™å…¥åˆ°æ–‡ä»¶çš„å¤§å°ä¸Šé™ä¸º1M tcpdump -i any -s 0 -C 1 -v -w client.pcap # æŒ‡å®šå†™å…¥åˆ°10ä¸ªæŠ“åŒ…æ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶åªæŠ“1Mï¼Œå¾ªç¯å†™å…¥ tcpdump -i any -s 0 -C 1M -v -W 10 -w client.pcap # å‚æ•°æŒ‡å®šé—´éš”å¤šå°‘ç§’è½®è¯¢ä¿å­˜ä¸€æ¬¡æ–‡ä»¶ï¼Œé€šå¸¸æ˜¯ä»¥æ—¶é—´æ ¼å¼å‘½ä»¤ tcpdump -nn -i any -s 0 -G 5 -Z root -v -w %m-%d-%H:%M:%S.pcap #æ¯éš”äº”ç§’ä¿å­˜ä¸€æ¬¡æ–‡ä»¶ 2.4. tcpdumpçš„é€»è¾‘è¡¨è¾¾å¼(orã€andã€not) and: ä¸\nor: æˆ–\nnot: é\n# ä¸ tcpdump -nn -i any -s 0 host 192.168.1.1 and icmp # æˆ– tcpdump -nn -i any -s 0 host 192.168.1.1 or icmp or src net 192.168.1.1/32 # é tcpdump -nn -i any -s 0 ! net 172.16.0.0/16 and icmp and ! tcp 3. Flagsæ ‡è®°è§£è¯» Flags å«ä¹‰ [S] SYN [.] ACK [S.] SYNã€ACK [P.] PUSH [R.] RST [F.] FIN [DF] Don't Fragment(ä¸åˆ†ç‰‡)ï¼Œå½“DF=0æ—¶ï¼Œå…è®¸åˆ†ç‰‡ [FP.] FINã€PUSHã€ACK å‚è€ƒï¼š\nhttps://www.tcpdump.org/manpages/tcpdump.1.html\næŠ“åŒ…ç¥å™¨TCPDUMPçš„åˆ†ææ€»ç»“-æ¶µç›–å„å¤§ä½¿ç”¨åœºæ™¯ã€é«˜çº§ç”¨æ³•\n","categories":"","description":"","excerpt":"1. ç®€ä»‹ linuxç³»ç»Ÿä¸Šå¸¸ç”¨tcpdumpæŠ“åŒ…æ¥åˆ†æç½‘ç»œé—®é¢˜ã€‚æœ¬æ–‡åŸºäºç½‘ç»œæ–‡ç« æ•´ç†ï¼Œä¸»è¦ä»‹ç»tcpdumpæŠ“åŒ…çš„å¸¸ç”¨å‘½ä»¤åŠå‚æ•°ã€‚\nä»¥ä¸‹æ˜¯ â€¦","ref":"/linux-notes/network/tcpdump/","tags":["network"],"title":"tcpdumpæŠ“åŒ…æµç¨‹"},{"body":"1. å®‰è£…helm curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash 2. åŸºæœ¬æ¦‚å¿µ Helmæ˜¯ç”¨æ¥ç®¡ç†k8sé›†ç¾¤ä¸Šçš„è½¯ä»¶åŒ…ã€‚\nChart:ä»£è¡¨helmè½¯ä»¶åŒ…\nRepositoryï¼šè½¯ä»¶åŒ…çš„å­˜æ”¾ä»“åº“\nRelease:è¿è¡Œåœ¨k8sä¸Šçš„ä¸€ä¸ªå‘å¸ƒå®ä¾‹ã€‚\n3. helmå‘½ä»¤ Usage: helm [command] Available Commands: completion generate autocompletion scripts for the specified shell create create a new chart with the given name dependency manage a chart's dependencies env helm client environment information get download extended information of a named release help Help about any command history fetch release history install install a chart lint examine a chart for possible issues list list releases package package a chart directory into a chart archive plugin install, list, or uninstall Helm plugins pull download a chart from a repository and (optionally) unpack it in local directory push push a chart to remote registry login to or logout from a registry repo add, list, remove, update, and index chart repositories rollback roll back a release to a previous revision search search for a keyword in charts show show information of a chart status display the status of the named release template locally render templates test run tests for a release uninstall uninstall a release upgrade upgrade a release verify verify that a chart at the given path has been signed and is valid version print the client version information 4. å¸¸ç”¨å‘½ä»¤ 4.1. helm search helm search hubï¼šä»Â Artifact HubÂ ä¸­æŸ¥æ‰¾å¹¶åˆ—å‡º helm chartsã€‚æ”¯æŒæ¨¡ç³ŠåŒ¹é…ã€‚ helm search hub wordpress helm search repoï¼šåŸºäºæŒ‡å®šä»“åº“è¿›è¡Œæœç´¢ã€‚ helm repo add brigade https://brigadecore.github.io/charts helm search repo brigade # åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬ helm search repo apisix -l 4.2. helm install/uninstall helm install \u003crelease_name\u003e \u003cchart_name\u003e # ç¤ºä¾‹ helm install happy-panda bitnami/wordpress - # uninstall helm uninstall RELEASE_NAME å®‰è£…è‡ªå®šä¹‰chart\nhelm install -f values.yaml bitnami/wordpress --generate-name # æœ¬åœ° chart å‹ç¼©åŒ… helm install foo foo-0.1.1.tgz # è§£å‹åçš„ chart ç›®å½• helm install foo path/to/foo # å®Œæ•´çš„ URL helm install foo https://example.com/charts/foo-1.2.3.tgz 4.3. helm upgrade helm upgrade happy-panda bitnami/wordpress 4.4. helm rollback helm rollback \u003cRELEASE\u003e [REVISION] [flags] 4.5. helm repo helm repo add dev https://example.com/dev-charts helm repo list helm repo remove 4.6. helm pull ä»ä»“åº“ä¸‹è½½å¹¶ï¼ˆå¯é€‰ï¼‰åœ¨æœ¬åœ°ç›®å½•è§£å‹ã€‚\nhelm pull [chart URL | repo/chartname] helm pull [chart URL | repo/chartname] --version 5. åˆ›å»ºchart 5.1. åˆå§‹åŒ–chart helm create mychart æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶ç›®å½•ï¼š\nmychart |-- charts # ç›®å½•ç”¨äºå­˜æ”¾æ‰€ä¾èµ–çš„å­chart |-- Chart.yaml # æè¿°è¿™ä¸ª Chart çš„ç›¸å…³ä¿¡æ¯ã€åŒ…æ‹¬åå­—ã€æè¿°ä¿¡æ¯ã€ç‰ˆæœ¬ç­‰ |-- templates | |-- deployment.yaml | |-- _helpers.tpl # æ¨¡æ¿åŠ©æ‰‹æ–‡ä»¶ï¼Œå®šä¹‰çš„å€¼å¯åœ¨æ¨¡æ¿ä¸­ä½¿ç”¨ | |-- hpa.yaml | |-- ingress.yaml | |-- NOTES.txt # Chart éƒ¨ç½²åˆ°é›†ç¾¤åçš„ä¸€äº›ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼šå¦‚ä½•ä½¿ç”¨ã€åˆ—å‡ºç¼ºçœå€¼ | |-- serviceaccount.yaml | |-- service.yaml | `-- tests | `-- test-connection.yaml `-- values.yaml # æ¨¡æ¿çš„å€¼æ–‡ä»¶ï¼Œè¿™äº›å€¼ä¼šåœ¨å®‰è£…æ—¶åº”ç”¨åˆ° GO æ¨¡æ¿ç”Ÿæˆéƒ¨ç½²æ–‡ä»¶ ç§»é™¤é»˜è®¤æ¨¡æ¿æ–‡ä»¶ï¼Œå¹¶æ·»åŠ è‡ªå·±çš„æ¨¡æ¿æ–‡ä»¶ã€‚\nrm -rf mysubchart/templates/* 5.2. è°ƒè¯•æ¨¡æ¿ helm lintÂ æ˜¯éªŒè¯chartæ˜¯å¦éµå¾ªæœ€ä½³å®è·µçš„é¦–é€‰å·¥å…·ã€‚ helm template --debugÂ åœ¨æœ¬åœ°æµ‹è¯•æ¸²æŸ“chartæ¨¡æ¿ã€‚ helm install --dry-run --debugï¼šæˆ‘ä»¬å·²ç»çœ‹åˆ°è¿‡è¿™ä¸ªæŠ€å·§äº†ï¼Œè¿™æ˜¯è®©æœåŠ¡å™¨æ¸²æŸ“æ¨¡æ¿çš„å¥½æ–¹æ³•ï¼Œç„¶åè¿”å›ç”Ÿæˆçš„æ¸…å•æ–‡ä»¶ã€‚ helm get manifest: è¿™æ˜¯æŸ¥çœ‹å®‰è£…åœ¨æœåŠ¡å™¨ä¸Šçš„æ¨¡æ¿çš„å¥½æ–¹æ³•ã€‚ å‚è€ƒï¼š\nHelm | å®‰è£…Helm Helm | ä½¿ç”¨Helm ","categories":"","description":"","excerpt":"1. å®‰è£…helm curl â€¦","ref":"/kubernetes-notes/operation/helm/helm-usage/","tags":["Kubernetes"],"title":"helmçš„ä½¿ç”¨"},{"body":" æœ¬æ–‡ä¸»è¦ä»‹ç»é€šè¿‡k8sæ¥éƒ¨ç½²apisixåŠapisix-ingress-controllerï¼Œä½¿ç”¨apisixä½œä¸ºk8så†…Podäº’ç›¸è®¿é—®çš„ç½‘å…³ã€‚\n1. ç¯å¢ƒå‡†å¤‡ 1.1. å®‰è£…helm å‚è€ƒï¼šHelm | å®‰è£…\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash helmæ·»åŠ ä»“åº“\nhelm repo add apisix https://charts.apiseven.com helm repo update 1.2. å®‰è£…ETCD å¯ä»¥æå‰å‡†å¤‡å¥½etcdç¯å¢ƒï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨apisixå®˜æ–¹çš„helmå‘½ä»¤å®‰è£…ï¼Œä½†æ˜¯éœ€è¦å­˜åœ¨é»˜è®¤æ˜¯storageclassæ¥æä¾›pvæŒ‚è½½ã€‚\n2. ä¸€é”®å®‰è£…å…¨éƒ¨ helm install apisix apisix/apisix --set gateway.type=NodePort --set ingress-controller.enabled=true --namespace=apisix --create-namespace æˆ–é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ†åˆ«å®‰è£…å„ç»„ä»¶ã€‚\n3. å®‰è£…apisix å‚è€ƒï¼šapisix-helm-chart/apisix.md at master Â· apache/apisix-helm-chart Â· GitHub\n3.1. å®‰è£… helm install apisix apisix/apisix --namespace apisix --create-namespace å¸è½½\nhelm uninstall apisix --namespace apisix 3.2. ä¿®æ”¹é…ç½® kubectl edit cm apisix-napisix å¯é€‰ï¼š\nä¿®æ”¹apisixç«¯å£ã€‚\nä¿®æ”¹etcdåœ°å€ã€‚\nä¿®æ”¹admin keyå€¼ã€‚\nä¿®æ”¹æ—¥å¿—è·¯å¾„ã€‚\nä½¿ç”¨etcdå­˜å‚¨streamé…ç½®æˆ–è€…é™æ€æ–‡ä»¶å­˜å‚¨streamé…ç½®\napisix: node_listen: 8000 # APISIX listening port config_center: etcd # etcd: use etcd to store the config value # yaml: fetch the config value from local yaml file `/your_path/conf/apisix.yaml` etcd: host: \"http://foo:2379\" # etcd address admin_key - name: \"admin\" key: newsupersecurekey # è¯·ä¿®æ”¹ key çš„å€¼ role: admin nginx_config: error_log: /var/log/apisix_error.log http: access_log: /var/log/apisix_access.log access_log_format: \"$time_iso8601|$remote_addr - $remote_user|$http_host|\\\"$request\\\"|$status|$body_bytes_sent|$request_time|\\\"$http_referer\\\"|\\\"$http_user_agent\\\"|$upstream_addr|$upstream_status|$upstream_response_time|\\\"$upstream_scheme://$upstream_host$upstream_uri\\\"\" plugin_attr: log-rotate: interval: 3600 # rotate interval (unit: second) max_kept: 48 # max number of log files will be kept enable_compression: false 4. å®‰è£…apisix-ingress-controller å‚è€ƒï¼šapisix-helm-chart/apisix-ingress-controller.md at master Â· apache/apisix-helm-chart Â· GitHub\n4.1. å®‰è£… helm install apisix-ingress-controller apisix/apisix-ingress-controller --namespace apisix --create-namespace å¸è½½\nhelm uninstall apisix-ingress-controller --namespace apisix 4.2. ä¿®æ”¹é…ç½® kubectl edit cm apisix-configmap -napisix é…ç½®\napisixåœ°å€\napisix admin key\ndefault_cluster_base_url: http://apisix-admin.apisix.svc.cluster.local:9180/apisix/admin default_cluster_admin_key: \"edd1c9f034335f136f87ad84b625c8f1\" 5. å®‰è£…dashboard 5.1. å®‰è£… helm repo add apisix https://charts.apiseven.com helm repo update helm install apisix-dashboard apisix/apisix-dashboard --namespace apisix --create-namespace å¸è½½\nhelm uninstall apisix-dashboard --namespace apisix 5.2. ä¿®æ”¹é…ç½® kubectl edit cm apisix-dashboard -napisix ç«¯å£\netcdåœ°å€\nç™»å½•è´¦å·å¯†ç \ndata: conf.yaml: |- conf: listen: host: 0.0.0.0 port: 9000 etcd: prefix: \"/apisix\" endpoints: - 10.65.240.210:2379 log: error_log: level: warn file_path: /dev/stderr access_log: file_path: /dev/stdout authentication: secert: secert expire_time: 3600 users: - username: admin password: admin 6. æŸ¥çœ‹helmå®‰è£…åˆ—è¡¨ # helm list -n apisix NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION apisix apisix 1 2022-08-23 16:19:47.678174579 +0800 +08 deployed apisix-0.11.0 2.15.0 apisix-dashboard apisix 1 2022-08-23 20:36:37.55042356 +0800 +08 deployed apisix-dashboard-0.6.0 2.13.0 å‚è€ƒï¼š\nhttps://github.com/apache/apisix-helm-chart\nhttps://apisix.apache.org/zh/docs/apisix/installation-guide/\nhttps://github.com/apache/apisix-ingress-controller/blob/master/install.md\n","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦ä»‹ç»é€šè¿‡k8sæ¥éƒ¨ç½²apisixåŠapisix-ingress-controllerï¼Œä½¿ç”¨apisixä½œä¸ºk8så†…Podäº’ç›¸è®¿é—®çš„ â€¦","ref":"/kubernetes-notes/network/gateway/install/","tags":["ApiSix"],"title":"å®‰è£…APISIX"},{"body":" æœ¬æ–‡ä¸»è¦ä»‹ç»éƒ¨ç½²openyurtç»„ä»¶åˆ°k8sé›†ç¾¤ä¸­ã€‚\n1. ç»™äº‘ç«¯èŠ‚ç‚¹å’Œè¾¹ç¼˜èŠ‚ç‚¹æ‰“æ ‡ç­¾ openyurtå°†k8sèŠ‚ç‚¹åˆ†ä¸ºäº‘ç«¯èŠ‚ç‚¹å’Œè¾¹ç¼˜èŠ‚ç‚¹ï¼Œäº‘ç«¯èŠ‚ç‚¹ä¸»è¦è¿è¡Œä¸€äº›äº‘ç«¯çš„ä¸šåŠ¡ï¼Œè¾¹ç¼˜èŠ‚ç‚¹è¿è¡Œè¾¹ç¼˜ä¸šåŠ¡ã€‚å½“ä¸Â apiserverÂ æ–­å¼€è¿æ¥æ—¶ï¼Œåªæœ‰è¿è¡Œåœ¨è¾¹ç¼˜è‡ªæ²»çš„èŠ‚ç‚¹ä¸Šçš„Podæ‰ä¸ä¼šè¢«é©±é€ã€‚é€šè¿‡æ‰“Â openyurt.io/is-edge-workerÂ çš„æ ‡ç­¾çš„æ–¹å¼æ¥åŒºåˆ†ï¼Œfalseè¡¨ç¤ºäº‘ç«¯èŠ‚ç‚¹ï¼Œtrueè¡¨ç¤ºè¾¹ç¼˜èŠ‚ç‚¹ã€‚\näº‘ç«¯ç»„ä»¶ï¼š\nyurt-controller-manager\nyurt-tunnel-server\nè¾¹ç¼˜ç»„ä»¶ï¼š\nyurt-hub\nyurt-tunnel-agent\n1.1. openyurt.io/is-edge-workerèŠ‚ç‚¹æ ‡ç­¾ # äº‘ç«¯èŠ‚ç‚¹ï¼Œå€¼ä¸ºfalse kubectl label node us-west-1.192.168.0.87 openyurt.io/is-edge-worker=false # è¾¹ç¼˜èŠ‚ç‚¹ï¼Œå€¼ä¸ºtrue kubectl label node us-west-1.192.168.0.88 openyurt.io/is-edge-worker=true 1.2. ç»™è¾¹ç¼˜èŠ‚ç‚¹å¼€å¯è‡ªæ²»æ¨¡å¼ kubectl annotate node us-west-1.192.168.0.88 node.beta.openyurt.io/autonomy=true 2. å®‰è£…å‡†å¤‡ 2.1. è°ƒæ•´k8sç»„ä»¶çš„é…ç½® å‚è€ƒè°ƒæ•´k8sç»„ä»¶çš„é…ç½®\n2.2. éƒ¨ç½²tunnel-dns wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurt-tunnel-dns.yaml kubectl apply -f yurt-tunnel-dns.yaml è·å–clusterIPï¼Œä½œä¸ºkube-apiserverçš„ä¸“ç”¨nameserveråœ°å€ã€‚\nkubectl -n kube-system get svc yurt-tunnel-dns -o=jsonpath='{.spec.clusterIP}' 3. éƒ¨ç½²openyurtæ§åˆ¶é¢ é€šè¿‡helmæ¥éƒ¨ç½²æ§åˆ¶é¢ï¼Œæ‰€æœ‰helm chartséƒ½å¯ä»¥åœ¨openyurt-helm ä»“åº“ä¸­æ‰¾åˆ°ã€‚\nå¿«æ·å®‰è£…å¯å‚è€ƒè„šæœ¬ï¼šhelm-install-openyurt.sh\nhelm repo add openyurt https://openyurtio.github.io/openyurt-helm 3.1. yurt-app-manager helm upgrade --install yurt-app-manager -n kube-system openyurt/yurt-app-manager 3.2. openyurt åœ¨openyurt/openyurtä¸­çš„ç»„ä»¶åŒ…æ‹¬ï¼š\nyurt-controller-manager: é˜²æ­¢apiserveråœ¨æ–­å¼€è¿æ¥æ—¶é©±é€è¿è¡Œåœ¨è¾¹ç¼˜èŠ‚ç‚¹ä¸Šçš„pod yurt-tunnel-server: åœ¨äº‘ç«¯æ„å»ºäº‘è¾¹éš§é“ yurt-tunnel-agent: åœ¨è¾¹ç¼˜ä¾§æ„å»ºäº‘è¾¹éš§é“ ç”±äºyurt-tunnel-serveré»˜è®¤ä½¿ç”¨hostæ¨¡å¼ï¼Œå› æ­¤å¯èƒ½å­˜åœ¨è¾¹ç¼˜ç«¯çš„agentæ— æ³•è®¿é—®äº‘ç«¯çš„tunnel-serverï¼Œéœ€è¦ä¸ºtunnel-serveré…ç½®ä¸€ä¸ªå¯è®¿é—®çš„åœ°å€ã€‚\n# ä¸‹è½½å¹¶è§£å‹ helm pull openyurt/openyurt --untar # ä¿®æ”¹tunnelç›¸å…³é…ç½® cd openyurt vi values.yaml # ç¤ºä¾‹ï¼š yurtTunnelServer: replicaCount: 1 tolerations: [] parameters: certDnsNames: \"\u003ctunnel serverçš„åŸŸå\u003e\" tunnelAgentConnectPort: \u003ctunnel serverç«¯å£ï¼Œé»˜è®¤ä¸º10262\u003e certIps: \"\" yurtTunnelAgent: replicaCount: 1 tolerations: [] parameters: tunnelserverAddr: \"\u003ctunnel serverçš„åœ°å€ï¼ŒåŒ…æ‹¬ç«¯å£\u003e\" # install helm install openyurt ./openyurt 4. éƒ¨ç½² Yurthub(edge) åœ¨Â yurt-controller-managerÂ å¯åŠ¨å¹¶æ­£å¸¸è¿è¡Œåï¼Œä»¥é™æ€ pod çš„æ–¹å¼éƒ¨ç½²Â Yurthubã€‚\nä¸º yurthub åˆ›å»ºå…¨å±€é…ç½®(å³RBAC, configmap) wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub-cfg.yaml kubectl apply -f yurthub-cfg.yaml åœ¨è¾¹ç¼˜èŠ‚ç‚¹ä»¥static podæ–¹å¼åˆ›å»ºyurthub mkdir -p /etc/kubernetes/manifests/ cd /etc/kubernetes/manifests/ wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub.yaml # è·å–bootstrap token kubeadm token create # å‡è®¾ apiserver çš„åœ°å€æ˜¯ 1.2.3.4:6443ï¼Œbootstrap token æ˜¯ 07401b.f395accd246ae52d sed -i 's|__kubernetes_master_address__|1.2.3.4:6443|; s|__bootstrap_token__|07401b.f395accd246ae52d|' /etc/kubernetes/manifests/yurthub.yaml 5. é‡ç½® Kubelet é‡ç½® kubelet æœåŠ¡ï¼Œè®©å®ƒé€šè¿‡ yurthub è®¿é—®apiserverã€‚ä¸º kubelet æœåŠ¡åˆ›å»ºä¸€ä¸ªæ–°çš„ kubeconfig æ–‡ä»¶æ¥è®¿é—®apiserverã€‚\nmkdir -p /var/lib/openyurt cat \u003c\u003c EOF \u003e /var/lib/openyurt/kubelet.conf apiVersion: v1 clusters: - cluster: server: http://127.0.0.1:10261 name: default-cluster contexts: - context: cluster: default-cluster namespace: default user: default-auth name: default-context current-context: default-context kind: Config preferences: {} EOF ä¿®æ”¹/etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nsed -i \"s|KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=\\/etc\\/kubernetes\\/bootstrap-kubelet.conf\\ --kubeconfig=\\/etc\\/kubernetes\\/kubelet.conf|KUBELET_KUBECONFIG_ARGS=--kubeconfig=\\/var\\/lib\\/openyurt\\/kubelet.conf|g\" \\ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf é‡å¯kubeletæœåŠ¡\nsystemctl daemon-reload \u0026\u0026 systemctl restart kubelet 6. yurthubéƒ¨ç½²è„šæœ¬ æ ¹æ®ä»¥ä¸Šéƒ¨ç½²æ­¥éª¤ï¼Œæ•´ç†éƒ¨ç½²è„šæœ¬ã€‚éœ€è¦ä¿®æ”¹è„šæœ¬å†…å®¹çš„master-addrå’Œtokenå­—æ®µã€‚\n#!/bin/bash set -e set -x ### install yurthub ### mkdir -p /etc/kubernetes/manifests/ cd /etc/kubernetes/manifests/ wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub.yaml ### ä¿®æ”¹masterå’Œtokenå­—æ®µ sed -i 's|__kubernetes_master_address__|\u003cmaster-addr\u003e:6443|; s|__bootstrap_token__|\u003ctoken\u003e|' /etc/kubernetes/manifests/yurthub.yaml mkdir -p /var/lib/openyurt cat \u003c\u003c EOF \u003e /var/lib/openyurt/kubelet.conf apiVersion: v1 clusters: - cluster: server: http://127.0.0.1:10261 name: default-cluster contexts: - context: cluster: default-cluster namespace: default user: default-auth name: default-context current-context: default-context kind: Config preferences: {} EOF cp /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.bak sed -i \"s|KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=\\/etc\\/kubernetes\\/bootstrap-kubelet.conf\\ --kubeconfig=\\/etc\\/kubernetes\\/kubelet.conf|KUBELET_KUBECONFIG_ARGS=--kubeconfig=\\/var\\/lib\\/openyurt\\/kubelet.conf|g\" \\ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl daemon-reload \u0026\u0026 systemctl restart kubelet å‚è€ƒï¼š\nhttps://openyurt.io/zh/docs/installation/manually-setup https://openyurt.io/zh/docs/installation/openyurt-prepare åœ¨å­˜é‡çš„K8sèŠ‚ç‚¹ä¸Šå®‰è£…OpenYurt Nodeç»„ä»¶ ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦ä»‹ç»éƒ¨ç½²openyurtç»„ä»¶åˆ°k8sé›†ç¾¤ä¸­ã€‚\n1. ç»™äº‘ç«¯èŠ‚ç‚¹å’Œè¾¹ç¼˜èŠ‚ç‚¹æ‰“æ ‡ç­¾ openyurtå°†k8sèŠ‚ç‚¹åˆ†ä¸ºäº‘ç«¯èŠ‚ç‚¹å’Œè¾¹ç¼˜èŠ‚ â€¦","ref":"/kubernetes-notes/edge/openyurt/install-openyurt/","tags":["OpenYurt"],"title":"OpenYurtéƒ¨ç½²"},{"body":" æœ¬æ–‡åŸºäºhttps://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/ æ•´ç†ã€‚\n1. RBACä»‹ç» åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ã€Role-based access control (RBAC)ã€‘æ˜¯ä¸€ç§åŸºäºç»„ç»‡ä¸­ç”¨æˆ·çš„è§’è‰²æ¥è°ƒèŠ‚æ§åˆ¶å¯¹ è®¡ç®—æœºæˆ–ç½‘ç»œèµ„æºçš„è®¿é—®çš„æ–¹æ³•ã€‚\nRBAC é‰´æƒæœºåˆ¶ä½¿ç”¨Â rbac.authorization.k8s.ioÂ API ç»„æ¥é©±åŠ¨é‰´æƒå†³å®šï¼Œ å…è®¸ä½ é€šè¿‡ Kubernetes API åŠ¨æ€é…ç½®ç­–ç•¥ã€‚\nè¦å¯ç”¨ RBACï¼Œåœ¨å¯åŠ¨Â API æœåŠ¡å™¨æ—¶å°†Â --authorization-modeÂ å‚æ•°è®¾ç½®ä¸ºä¸€ä¸ªé€—å·åˆ†éš”çš„åˆ—è¡¨å¹¶ç¡®ä¿å…¶ä¸­åŒ…å«Â RBACã€‚\nkube-apiserver --authorization-mode=Example,RBAC --\u003cå…¶ä»–é€‰é¡¹\u003e --\u003cå…¶ä»–é€‰é¡¹\u003e ç¼–å†™è‡ªå®šä¹‰CRDæ§åˆ¶å™¨æˆ–éƒ¨ç½²å…¶ä»–å¼€æºç»„ä»¶æ—¶ï¼Œç»å¸¸éœ€è¦ç»™ç»„ä»¶é…ç½®RBACæƒé™ã€‚\nç†è§£RBACæƒé™ä½“ç³»ï¼Œåªéœ€è¦ç†è§£ä»¥ä¸‹ä¸‰ä¸ªæ¦‚å¿µå¯¹è±¡å³å¯ï¼š\nã€æƒé™ã€‘Roleï¼šè§’è‰²ï¼Œå®ƒå…¶å®æ˜¯ä¸€ç»„è§„åˆ™ï¼Œå®šä¹‰äº†ä¸€ç»„å¯¹ Kubernetes API å¯¹è±¡çš„æ“ä½œæƒé™ã€‚\nã€ç”¨æˆ·ã€‘Subjectï¼šè¢«ä½œç”¨è€…ï¼Œæ—¢å¯ä»¥æ˜¯â€œäººâ€ï¼Œä¹Ÿå¯ä»¥æ˜¯â€œæœºå™¨â€ï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ åœ¨ Kubernetes é‡Œå®šä¹‰çš„â€œç”¨æˆ·â€ã€‚\nã€æˆæƒã€‘RoleBindingï¼šå®šä¹‰äº†â€œè¢«ä½œç”¨è€…â€å’Œâ€œè§’è‰²â€çš„ç»‘å®šå…³ç³»ã€‚\nä¸€å¥è¯ç†è§£RBACï¼Œå°±æ˜¯å°†å®šä¹‰çš„æƒé™ä¸å®šä¹‰çš„ç”¨æˆ·ä¹‹é—´çš„å…³ç³»è¿›è¡Œç»‘å®šï¼Œå³æˆæƒæŸä¸ªç”¨æˆ·æŸäº›æƒé™ã€‚\nå¿«é€Ÿæˆæƒè„šæœ¬å¯ä»¥å‚è€ƒï¼šhttps://github.com/huweihuang/kubeadm-scripts/tree/main/kubeconfig/token\n2. APIå¯¹è±¡ è§’è‰²ï¼ˆæƒé™ï¼‰---è§’è‰²ï¼ˆæƒé™ï¼‰ç»‘å®š---ç”¨æˆ·ï¼ˆsubjectï¼‰\né›†ç¾¤çº§åˆ«èŒƒå›´ å‘½åç©ºé—´èŒƒå›´ æƒé™ ClusterRole Role æˆæƒ ClusterRoleBinding RoleBinding ç”¨æˆ· ServiceAccout ä»¥ä¸‹ä»æƒé™ã€ç”¨æˆ·ã€æˆæƒä¸‰ä¸ªæ¦‚å¿µè¿›è¡Œè¯´æ˜ã€‚å®Œæˆä¸€å¥—æˆæƒé€»è¾‘ï¼Œä¸»è¦åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤\nåˆ›å»ºæƒé™ï¼Œå³åˆ›å»ºRoleæˆ–ClusterRoleå¯¹è±¡ã€‚\nåˆ›å»ºç”¨æˆ·ï¼Œå³åˆ›å»ºServiceAccountå¯¹è±¡ã€‚\nåˆ†é…æƒé™ï¼Œå³åˆ›å»ºRoleBindingæˆ–ClusterRoleBindingå¯¹è±¡ã€‚\n3. æƒé™ RBAC çš„Â RoleÂ æˆ–Â ClusterRoleÂ ä¸­åŒ…å«ä¸€ç»„ä»£è¡¨ç›¸å…³æƒé™çš„è§„åˆ™ã€‚ è¿™äº›æƒé™æ˜¯çº¯ç²¹ç´¯åŠ çš„ï¼ˆä¸å­˜åœ¨æ‹’ç»æŸæ“ä½œçš„è§„åˆ™ï¼‰ã€‚\n3.1. å‘½åç©ºé—´æƒé™[Role] Roleæ˜¯é’ˆå¯¹æŒ‡å®šnamespaceçš„æƒé™ï¼Œå³åˆ›å»ºçš„æ—¶å€™éœ€è¦æŒ‡å®šnamespaceã€‚\nç¤ºä¾‹ï¼š\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\"\"] # \"\" æ ‡æ˜ core API ç»„ resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 3.2. é›†ç¾¤çº§åˆ«æƒé™[ClusterRole] ClusterRoleç”¨äºæŒ‡å®šé›†ç¾¤å†…çš„èµ„æºï¼š\né›†ç¾¤èŒƒå›´èµ„æºï¼ˆæ¯”å¦‚èŠ‚ç‚¹ï¼ˆNodeï¼‰ï¼‰\néèµ„æºç«¯ç‚¹ï¼ˆæ¯”å¦‚Â /healthzï¼‰\nè·¨åå­—ç©ºé—´è®¿é—®çš„åå­—ç©ºé—´ä½œç”¨åŸŸçš„èµ„æºï¼ˆå¦‚ è®¿é—®æ‰€æœ‰namespaceä¸‹çš„Podï¼‰\nç¤ºä¾‹ï¼š\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # \"namespace\" è¢«å¿½ç•¥ï¼Œå› ä¸º ClusterRoles ä¸å—åå­—ç©ºé—´é™åˆ¶ name: secret-reader rules: - apiGroups: [\"\"] # åœ¨ HTTP å±‚é¢ï¼Œç”¨æ¥è®¿é—® Secret èµ„æºçš„åç§°ä¸º \"secrets\" resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] 3.3. é»˜è®¤æƒé™è§’è‰² åœ¨ Kubernetes ä¸­å·²ç»å†…ç½®äº†å¾ˆå¤šä¸ªä¸ºç³»ç»Ÿä¿ç•™çš„ ClusterRoleï¼Œå®ƒä»¬çš„åå­—éƒ½ä»¥system:å¼€å¤´ã€‚å¯ä»¥é€šè¿‡ kubectl get clusterroles æŸ¥çœ‹åˆ°å®ƒä»¬ã€‚\nè¶…çº§ç”¨æˆ·ï¼ˆSuper-Userï¼‰è§’è‰²ï¼ˆcluster-adminï¼‰ã€ ä½¿ç”¨ ClusterRoleBinding åœ¨é›†ç¾¤èŒƒå›´å†…å®Œæˆæˆæƒçš„è§’è‰²ï¼ˆcluster-statusï¼‰ã€ ä»¥åŠä½¿ç”¨ RoleBinding åœ¨ç‰¹å®šåå­—ç©ºé—´ä¸­æˆäºˆçš„è§’è‰²ï¼ˆadminã€editã€viewï¼‰ã€‚\né»˜è®¤ ClusterRole é»˜è®¤ ClusterRoleBinding æè¿° cluster-admin system:mastersÂ ç»„ å…è®¸è¶…çº§ç”¨æˆ·åœ¨å¹³å°ä¸Šçš„ä»»ä½•èµ„æºä¸Šæ‰§è¡Œæ‰€æœ‰æ“ä½œã€‚ å½“åœ¨Â ClusterRoleBindingÂ ä¸­ä½¿ç”¨æ—¶ï¼Œå¯ä»¥æˆæƒå¯¹é›†ç¾¤ä¸­ä»¥åŠæ‰€æœ‰åå­—ç©ºé—´ä¸­çš„å…¨éƒ¨èµ„æºè¿›è¡Œå®Œå…¨æ§åˆ¶ã€‚ å½“åœ¨Â RoleBindingÂ ä¸­ä½¿ç”¨æ—¶ï¼Œå¯ä»¥æˆæƒæ§åˆ¶è§’è‰²ç»‘å®šæ‰€åœ¨åå­—ç©ºé—´ä¸­çš„æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬åå­—ç©ºé—´æœ¬èº«ã€‚ admin æ—  å…è®¸ç®¡ç†å‘˜è®¿é—®æƒé™ï¼Œæ—¨åœ¨ä½¿ç”¨Â RoleBindingÂ åœ¨åå­—ç©ºé—´å†…æ‰§è¡Œæˆæƒã€‚å¦‚æœåœ¨Â RoleBindingÂ ä¸­ä½¿ç”¨ï¼Œåˆ™å¯æˆäºˆå¯¹åå­—ç©ºé—´ä¸­çš„å¤§å¤šæ•°èµ„æºçš„è¯»/å†™æƒé™ï¼Œ åŒ…æ‹¬åˆ›å»ºè§’è‰²å’Œè§’è‰²ç»‘å®šçš„èƒ½åŠ›ã€‚ æ­¤è§’è‰²ä¸å…è®¸å¯¹èµ„æºé…é¢æˆ–è€…åå­—ç©ºé—´æœ¬èº«è¿›è¡Œå†™æ“ä½œã€‚ æ­¤è§’è‰²ä¹Ÿä¸å…è®¸å¯¹ Kubernetes v1.22+ åˆ›å»ºçš„ Endpoints è¿›è¡Œå†™æ“ä½œã€‚ æ›´å¤šä¿¡æ¯å‚é˜…Â â€œEndpoints å†™æƒé™â€å°èŠ‚ã€‚ edit æ—  å…è®¸å¯¹åå­—ç©ºé—´çš„å¤§å¤šæ•°å¯¹è±¡è¿›è¡Œè¯»/å†™æ“ä½œã€‚æ­¤è§’è‰²ä¸å…è®¸æŸ¥çœ‹æˆ–è€…ä¿®æ”¹è§’è‰²æˆ–è€…è§’è‰²ç»‘å®šã€‚ ä¸è¿‡ï¼Œæ­¤è§’è‰²å¯ä»¥è®¿é—® Secretï¼Œä»¥åå­—ç©ºé—´ä¸­ä»»ä½• ServiceAccount çš„èº«ä»½è¿è¡Œ Podï¼Œ æ‰€ä»¥å¯ä»¥ç”¨æ¥äº†è§£åå­—ç©ºé—´å†…æ‰€æœ‰æœåŠ¡è´¦æˆ·çš„ API è®¿é—®çº§åˆ«ã€‚ æ­¤è§’è‰²ä¹Ÿä¸å…è®¸å¯¹ Kubernetes v1.22+ åˆ›å»ºçš„ Endpoints è¿›è¡Œå†™æ“ä½œã€‚ æ›´å¤šä¿¡æ¯å‚é˜…Â â€œEndpoints å†™æ“ä½œâ€å°èŠ‚ã€‚ view æ—  å…è®¸å¯¹åå­—ç©ºé—´çš„å¤§å¤šæ•°å¯¹è±¡æœ‰åªè¯»æƒé™ã€‚ å®ƒä¸å…è®¸æŸ¥çœ‹è§’è‰²æˆ–è§’è‰²ç»‘å®šã€‚æ­¤è§’è‰²ä¸å…è®¸æŸ¥çœ‹ Secretsï¼Œå› ä¸ºè¯»å– Secret çš„å†…å®¹æ„å‘³ç€å¯ä»¥è®¿é—®åå­—ç©ºé—´ä¸­ ServiceAccount çš„å‡­æ®ä¿¡æ¯ï¼Œè¿›è€Œå…è®¸åˆ©ç”¨åå­—ç©ºé—´ä¸­ä»»ä½• ServiceAccount çš„èº«ä»½è®¿é—® APIï¼ˆè¿™æ˜¯ä¸€ç§ç‰¹æƒæå‡ï¼‰ã€‚ 4. ç”¨æˆ· 4.1. ServiceAccount åˆ›å»ºæŒ‡å®šnamespaceçš„ServiceAccountå¯¹è±¡ã€‚ServiceAccountå¯ä»¥åœ¨podä¸­è¢«ä½¿ç”¨ã€‚\napiVersion: v1 kind: ServiceAccount metadata: namespace: \u003cNamespace\u003e name: \u003cServiceAccountName\u003e 4.2. Secret åˆ›å»ºsecretï¼Œç»‘å®šserviceaccountï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆtokenã€‚\nk8s 1.24åçš„ç‰ˆæœ¬ä¸å†è‡ªåŠ¨ç”Ÿæˆsecretï¼Œç»‘å®šåå½“åˆ é™¤ServiceAccountæ—¶ä¼šè‡ªåŠ¨åˆ é™¤secret\napiVersion: v1 kind: Secret metadata: name: \u003cSecretName\u003e namespace: \u003cNamespace\u003e annotations: kubernetes.io/service-account.name: \"ServiceAccountName\" type: kubernetes.io/service-account-token 5. æˆæƒ 5.1. æˆæƒå‘½åç©ºé—´æƒé™[RoleBinding] RoleBindingè§’è‰²ç»‘å®šï¼ˆRole Bindingï¼‰æ˜¯å°†è§’è‰²ä¸­å®šä¹‰çš„æƒé™èµ‹äºˆä¸€ä¸ªæˆ–è€…ä¸€ç»„ç”¨æˆ·ã€‚ å®ƒåŒ…å«è‹¥å¹²Â ä¸»ä½“ï¼ˆç”¨æˆ·ã€ç»„æˆ–æœåŠ¡è´¦æˆ·ï¼‰çš„åˆ—è¡¨å’Œå¯¹è¿™äº›ä¸»ä½“æ‰€è·å¾—çš„è§’è‰²çš„å¼•ç”¨ã€‚ RoleBinding åœ¨æŒ‡å®šçš„åå­—ç©ºé—´ä¸­æ‰§è¡Œæˆæƒï¼Œè€Œ ClusterRoleBinding åœ¨é›†ç¾¤èŒƒå›´æ‰§è¡Œæˆæƒã€‚\nå­—æ®µè¯´æ˜ï¼š\nsubjectsï¼šè¡¨ç¤ºæƒé™æ‰€æˆäºˆçš„ç”¨æˆ·ï¼ŒåŒ…æ‹¬ServiceAccountï¼ŒGroupï¼ŒUserã€‚\nroleRefï¼šè¡¨ç¤ºæƒé™å¯¹åº”çš„è§’è‰²ï¼ŒåŒ…æ‹¬Roleï¼ŒClusterRoleã€‚\nç¤ºä¾‹ï¼š\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ${USER}-rolebinding namespace: ${NAMESPACE} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: kind: ClusterRole name: ${ROLE} apiGroup: rbac.authorization.k8s.io å¯ä»¥ç»™æŒ‡å®šå‘½åç©ºé—´ä¸‹çš„serviceaccountæˆæƒå…¶ä»–å‘½åç©ºé—´çš„æƒé™ã€‚åªéœ€è¦æ–°å¢RoleBindingåœ¨é¢„æˆæƒçš„å‘½åç©ºé—´ä¸‹å³å¯ã€‚å¯ä»¥ç†è§£ä¸ºå¯ä»¥ç»™ä¸€ä¸ªç”¨æˆ·åˆ†é…å¤šä¸ªå‘½åç©ºé—´çš„æƒé™ã€‚\n#!/bin/bash set -e # ç»™å·²å­˜åœ¨çš„ç”¨æˆ·USER æ·»åŠ å…¶ä»–NAMESPACEçš„æƒé™ USER=$1 NAMESPACE=$2 ROLE=$3 ROLE=${ROLE:-edit} ServiceAccountName=\"${USER}-user\" ServiceAccountNS=\"kubernetes-dashboard\" cat\u003c\u003cEOF | kubectl apply -f - kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ${USER}-rolebinding namespace: ${NAMESPACE} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: kind: ClusterRole name: ${ROLE} apiGroup: rbac.authorization.k8s.io EOF 5.2. æˆæƒé›†ç¾¤çº§åˆ«æƒé™[ClusterRoleBinding] è¦è·¨æ•´ä¸ªé›†ç¾¤å®Œæˆè®¿é—®æƒé™çš„æˆäºˆï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ª ClusterRoleBindingã€‚\nç¤ºä¾‹ï¼š\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ${USER} subjects: - kind: ServiceAccount name: ${USER} namespace: ${NAMESPACE} roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ${ROLE} å‚è€ƒï¼š\nä½¿ç”¨ RBAC é‰´æƒ | Kubernetes ç”¨æˆ·è®¤è¯ | Kubernetes k8s serviceaccountåˆ›å»ºåæ²¡æœ‰ç”Ÿæˆå¯¹åº”çš„secret ","categories":"","description":"","excerpt":" æœ¬æ–‡åŸº â€¦","ref":"/kubernetes-notes/operation/access/rbac-auth/","tags":["Kubernetes"],"title":"ä½¿ç”¨ RBAC é‰´æƒ"},{"body":" æœ¬æ–‡ç”±ç½‘ç»œèµ„æºæ•´ç†ä»¥ä½œè®°å½•\nç®€ä»‹ Karmadaï¼ˆKubernetes Armadaï¼‰æ˜¯åŸºäºKubernetesåŸç”ŸAPIçš„å¤šé›†ç¾¤ç®¡ç†ç³»ç»Ÿã€‚åœ¨å¤šäº‘å’Œæ··åˆäº‘åœºæ™¯ä¸‹ï¼ŒKarmadaæä¾›å¯æ’æ‹”ï¼Œå…¨è‡ªåŠ¨åŒ–ç®¡ç†å¤šé›†ç¾¤åº”ç”¨ï¼Œå®ç°å¤šäº‘é›†ä¸­ç®¡ç†ã€é«˜å¯ç”¨æ€§ã€æ•…éšœæ¢å¤å’Œæµé‡è°ƒåº¦ã€‚\nç‰¹æ€§ åŸºäºK8såŸç”ŸAPIçš„è·¨é›†ç¾¤åº”ç”¨ç®¡ç†ï¼Œç”¨æˆ·å¯ä»¥æ–¹ä¾¿å¿«æ·åœ°å°†åº”ç”¨ä»å•é›†ç¾¤è¿ç§»åˆ°å¤šé›†ç¾¤ã€‚ ä¸­å¿ƒå¼æ“ä½œå’Œç®¡ç†Kubernetesé›†ç¾¤ã€‚ è·¨é›†ç¾¤åº”ç”¨å¯åœ¨å¤šé›†ç¾¤ä¸Šè‡ªåŠ¨æ‰©å±•ï¼Œæ•…éšœè½¬ç§»å’Œè´Ÿè½½å‡è¡¡ã€‚ é«˜çº§çš„è°ƒåº¦ç­–ç•¥ï¼šåŒºåŸŸï¼Œå¯ç”¨åŒºï¼Œäº‘æä¾›å•†ï¼Œé›†ç¾¤äº²å’Œæ€§/åäº²å’Œæ€§ã€‚ æ”¯æŒåˆ›å»ºåˆ†å‘ç”¨æˆ·è‡ªå®šä¹‰ï¼ˆCustomResourceDefinitionsï¼‰èµ„æºã€‚ æ¡†æ¶ç»“æ„ ETCDï¼šå­˜å‚¨Karmada APIå¯¹è±¡ã€‚ Karmada Schedulerï¼šæä¾›é«˜çº§çš„å¤šé›†ç¾¤è°ƒåº¦ç­–ç•¥ã€‚ Karmada Controller Manager: åŒ…å«å¤šä¸ªControllerï¼ŒControllerç›‘å¬karmadaå¯¹è±¡å¹¶ä¸”ä¸æˆå‘˜é›†ç¾¤API serverè¿›è¡Œé€šä¿¡å¹¶åˆ›å»ºæˆå‘˜é›†ç¾¤çš„k8så¯¹è±¡ã€‚ Cluster Controllerï¼šæˆå‘˜é›†ç¾¤çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ä¸å¯¹è±¡ç®¡ç†ã€‚ Policy Controllerï¼šç›‘å¬PropagationPolicyå¯¹è±¡ï¼Œåˆ›å»ºResourceBindingï¼Œé…ç½®èµ„æºåˆ†å‘ç­–ç•¥ã€‚ Binding Controllerï¼šç›‘å¬ResourceBindingå¯¹è±¡ï¼Œå¹¶åˆ›å»ºworkå¯¹è±¡å“åº”èµ„æºæ¸…å•ã€‚ Execution Controllerï¼šç›‘å¬workå¯¹è±¡ï¼Œå¹¶å°†èµ„æºåˆ†å‘åˆ°æˆå‘˜é›†ç¾¤ä¸­ã€‚ èµ„æºåˆ†å‘æµç¨‹ åŸºæœ¬æ¦‚å¿µ\nèµ„æºæ¨¡æ¿ï¼ˆResource Templateï¼‰ï¼šKarmadaä½¿ç”¨K8såŸç”ŸAPIå®šä¹‰ä½œä¸ºèµ„æºæ¨¡æ¿ï¼Œä¾¿äºå¿«é€Ÿå¯¹æ¥K8sç”Ÿæ€å·¥å…·é“¾ã€‚ åˆ†å‘ç­–ç•¥ï¼ˆPropagaion Policyï¼‰ï¼šKarmadaæä¾›ç‹¬ç«‹çš„ç­–ç•¥APIï¼Œç”¨æ¥é…ç½®èµ„æºåˆ†å‘ç­–ç•¥ã€‚ å·®å¼‚åŒ–ç­–ç•¥ï¼ˆOverride Policyï¼‰ï¼šKarmadaæä¾›ç‹¬ç«‹çš„å·®å¼‚åŒ–APIï¼Œç”¨æ¥é…ç½®ä¸é›†ç¾¤ç›¸å…³çš„å·®å¼‚åŒ–é…ç½®ã€‚æ¯”å¦‚é…ç½®ä¸åŒé›†ç¾¤ä½¿ç”¨ä¸åŒçš„é•œåƒã€‚ Karmadaèµ„æºåˆ†å‘æµç¨‹å›¾ï¼š\nå‚è€ƒï¼š\nhttps://github.com/karmada-io/karmada https://support.huaweicloud.com/productdesc-mcp/mcp_productdesc_0001.html ","categories":"","description":"","excerpt":" æœ¬æ–‡ç”±ç½‘ç»œèµ„æºæ•´ç†ä»¥ä½œè®°å½•\nç®€ä»‹ Karmadaï¼ˆKubernetes Armadaï¼‰æ˜¯åŸºäºKubernetesåŸç”ŸAPIçš„å¤šé›†ç¾¤ç®¡ç†ç³» â€¦","ref":"/kubernetes-notes/multi-cluster/karmada/karmada-introduction/","tags":["Karmada"],"title":"Karmadaä»‹ç»"},{"body":" æœ¬æ–‡ä¸»è¦ç”±äº‘åŸç”Ÿè™šæ‹ŸåŒ–ï¼šåŸºäº Kubevirt æ„å»ºè¾¹ç¼˜è®¡ç®—å®ä¾‹æ–‡ç« é‡æ–°æ•´ç†è€Œæˆã€‚\n1. kubevirtç®€ä»‹ kubevirtæ˜¯åŸºäºk8sä¹‹ä¸Šï¼Œæä¾›äº†ä¸€ç§é€šè¿‡k8sæ¥ç¼–æ’å’Œç®¡ç†è™šæ‹Ÿæœºçš„æ–¹å¼ã€‚\n2. æ¶æ„å›¾ 2.1. ç»„ä»¶è¯´æ˜ åˆ†ç±» ç»„ä»¶ éƒ¨ç½²æ–¹å¼ åŠŸèƒ½è¯´æ˜ æ§åˆ¶é¢ virt-api deployment è‡ªå®šä¹‰APIï¼Œå¼€æœºã€å…³æœºã€é‡å¯ç­‰ï¼Œä½œä¸ºapiserverçš„æ’ä»¶ï¼Œä¸šåŠ¡é€šè¿‡k8s apiserverè¯·æ±‚virt-apiã€‚ virt-controller deployment ç®¡ç†å’Œç›‘æ§VMIå¯¹è±¡çš„çŠ¶æ€ï¼Œæ§åˆ¶VMIä¸‹çš„podã€‚ èŠ‚ç‚¹ä¾§ virt-handler daemonset ç±»ä¼¼kubeletï¼Œç®¡ç†å®¿ä¸»æœºä¸Šçš„æ‰€æœ‰è™šæ‹Ÿæœºå®ä¾‹ã€‚ virt-launcher virt-handler pod è°ƒç”¨libvirtå’Œqemuåˆ›å»ºè™šæ‹Ÿæœºè¿›ç¨‹ã€‚ virt-launcherä¸libvirté€»è¾‘ï¼š\n2.2. è‡ªå®šä¹‰CRDå¯¹è±¡ åˆ†ç±» CRDå¯¹è±¡ åŠŸèƒ½è¯´æ˜ è™šæœº VirtualMachineInstanceï¼ˆVMIï¼‰ ä»£è¡¨è¿è¡Œçš„è™šæ‹Ÿæœºå®ä¾‹ VirtualMachineï¼ˆVMï¼‰ è™šæœºå¯¹è±¡ï¼Œæä¾›å¼€æœºã€å…³æœºã€é‡å¯ï¼Œç®¡ç†VMIå®ä¾‹ï¼Œä¸VMIçš„å…³ç³»æ˜¯1ï¼š1 3. åˆ›å»ºè™šæ‹Ÿæœºæµç¨‹ å¾…è¡¥å……\nå‚è€ƒï¼š\nhttps://github.com/kubevirt/kubevirt\nArchitecture - KubeVirt User-Guide\nhttps://mp.weixin.qq.com/s/IwA1QcGaooZAL96YjvTqjA\n","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦ç”±äº‘åŸç”Ÿè™šæ‹ŸåŒ–ï¼šåŸºäº Kubevirt æ„å»ºè¾¹ç¼˜è®¡ç®—å®ä¾‹æ–‡ç« é‡æ–°æ•´ç†è€Œæˆã€‚\n1. kubevirtç®€ä»‹ kubevirtæ˜¯åŸº â€¦","ref":"/kubernetes-notes/kvm/kubevirt/kubevirt-introduction/","tags":["KubeVirt"],"title":"KubeVirtçš„ä»‹ç»"},{"body":" æœ¬æ–‡ä¸»è¦åˆ†æOCIï¼ŒCRIï¼Œruncï¼Œcontainerdï¼Œcri-containerdï¼Œdockershimç­‰ç»„ä»¶è¯´æ˜åŠè°ƒç”¨å…³ç³»ã€‚\n1. æ¦‚è¿° å„ä¸ªç»„ä»¶è°ƒç”¨å…³ç³»å›¾å¦‚ä¸‹ï¼š\nå›¾ç‰‡æ¥æºï¼šhttps://www.jianshu.com/p/62e71584d1cb\n2. OCIï¼ˆOpen Container Initiativeï¼‰ OCIï¼ˆOpen Container Initiativeï¼‰å³å¼€æ”¾çš„å®¹å™¨è¿è¡Œæ—¶è§„èŒƒï¼Œç›®çš„åœ¨äºå®šä¹‰ä¸€ä¸ªå®¹å™¨è¿è¡Œæ—¶åŠé•œåƒçš„ç›¸å…³æ ‡å‡†å’Œè§„èŒƒï¼Œå…¶ä¸­åŒ…æ‹¬\nruntime-specï¼šå®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œå…·ä½“å‚è€ƒruntime-specã€‚ image-specï¼šé•œåƒçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œå…·ä½“å‚è€ƒimage-specã€‚ å®ç°OCIæ ‡å‡†çš„å®¹å™¨è¿è¡Œæ—¶æœ‰runcï¼Œkataç­‰ã€‚\n3. RunC runc(run container)æ˜¯ä¸€ä¸ªåŸºäºOCIæ ‡å‡†å®ç°çš„ä¸€ä¸ªè½»é‡çº§å®¹å™¨è¿è¡Œå·¥å…·ï¼Œç”¨æ¥åˆ›å»ºå’Œè¿è¡Œå®¹å™¨ã€‚è€ŒContainerdæ˜¯ç”¨æ¥ç»´æŒé€šè¿‡runcåˆ›å»ºçš„å®¹å™¨çš„è¿è¡ŒçŠ¶æ€ã€‚å³runcç”¨æ¥åˆ›å»ºå’Œè¿è¡Œå®¹å™¨ï¼Œcontainerdä½œä¸ºå¸¸é©»è¿›ç¨‹ç”¨æ¥ç®¡ç†å®¹å™¨ã€‚\nruncåŒ…å«libcontainerï¼ŒåŒ…æ‹¬å¯¹namespaceå’Œcgroupçš„è°ƒç”¨æ“ä½œã€‚\nå‘½ä»¤å‚æ•°ï¼š\nTo start a new instance of a container: # runc run [ -b bundle ] \u003ccontainer-id\u003e USAGE: runc [global options] command [command options] [arguments...] COMMANDS: checkpoint checkpoint a running container create create a container delete delete any resources held by the container often used with detached container events display container events such as OOM notifications, cpu, memory, and IO usage statistics exec execute new process inside the container init initialize the namespaces and launch the process (do not call it outside of runc) kill kill sends the specified signal (default: SIGTERM) to the container's init process list lists containers started by runc with the given root pause pause suspends all processes inside the container ps ps displays the processes running inside a container restore restore a container from a previous checkpoint resume resumes all processes that have been previously paused run create and run a container spec create a new specification file start executes the user defined process in a created container state output the state of a container update update container resource constraints help, h Shows a list of commands or help for one command 4. Containerd containerdï¼ˆcontainer daemonï¼‰æ˜¯ä¸€ä¸ªdaemonè¿›ç¨‹ç”¨æ¥ç®¡ç†å’Œè¿è¡Œå®¹å™¨ï¼Œå¯ä»¥ç”¨æ¥æ‹‰å–/æ¨é€é•œåƒå’Œç®¡ç†å®¹å™¨çš„å­˜å‚¨å’Œç½‘ç»œã€‚å…¶ä¸­å¯ä»¥è°ƒç”¨runcæ¥åˆ›å»ºå’Œè¿è¡Œå®¹å™¨ã€‚\n4.1. containerdçš„æ¶æ„å›¾ 4.2. dockerä¸containerdã€runcçš„å…³ç³»å›¾ æ›´å…·ä½“çš„è°ƒç”¨é€»è¾‘ï¼š\n5. CRIï¼ˆContainer Runtime Interface ï¼‰ CRIå³å®¹å™¨è¿è¡Œæ—¶æ¥å£ï¼Œä¸»è¦ç”¨æ¥å®šä¹‰k8sä¸å®¹å™¨è¿è¡Œæ—¶çš„APIè°ƒç”¨ï¼Œkubeleté€šè¿‡CRIæ¥è°ƒç”¨å®¹å™¨è¿è¡Œæ—¶ï¼Œåªè¦å®ç°äº†CRIæ¥å£çš„å®¹å™¨è¿è¡Œæ—¶å°±å¯ä»¥å¯¹æ¥åˆ°k8sçš„kubeletç»„ä»¶ã€‚\n5.1. dockerä¸k8sè°ƒç”¨containerdçš„å…³ç³»å›¾ 5.2. cri-api 5.2.1. runtime service // Runtime service defines the public APIs for remote container runtimes service RuntimeService { // Version returns the runtime name, runtime version, and runtime API version. rpc Version(VersionRequest) returns (VersionResponse) {} // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure // the sandbox is in the ready state on success. rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {} // StopPodSandbox stops any running process that is part of the sandbox and // reclaims network resources (e.g., IP addresses) allocated to the sandbox. // If there are any running containers in the sandbox, they must be forcibly // terminated. // This call is idempotent, and must not return an error if all relevant // resources have already been reclaimed. kubelet will call StopPodSandbox // at least once before calling RemovePodSandbox. It will also attempt to // reclaim resources eagerly, as soon as a sandbox is not needed. Hence, // multiple StopPodSandbox calls are expected. rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {} // RemovePodSandbox removes the sandbox. If there are any running containers // in the sandbox, they must be forcibly terminated and removed. // This call is idempotent, and must not return an error if the sandbox has // already been removed. rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {} // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not // present, returns an error. rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {} // ListPodSandbox returns a list of PodSandboxes. rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {} // CreateContainer creates a new container in specified PodSandbox rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {} // StartContainer starts the container. rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {} // StopContainer stops a running container with a grace period (i.e., timeout). // This call is idempotent, and must not return an error if the container has // already been stopped. // The runtime must forcibly kill the container after the grace period is // reached. rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {} // RemoveContainer removes the container. If the container is running, the // container must be forcibly removed. // This call is idempotent, and must not return an error if the container has // already been removed. rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {} // ListContainers lists all containers by filters. rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {} // ContainerStatus returns status of the container. If the container is not // present, returns an error. rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {} // UpdateContainerResources updates ContainerConfig of the container. rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {} // ReopenContainerLog asks runtime to reopen the stdout/stderr log file // for the container. This is often called after the log file has been // rotated. If the container is not running, container runtime can choose // to either create a new log file and return nil, or return an error. // Once it returns error, new container log file MUST NOT be created. rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {} // ExecSync runs a command in a container synchronously. rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {} // Exec prepares a streaming endpoint to execute a command in the container. rpc Exec(ExecRequest) returns (ExecResponse) {} // Attach prepares a streaming endpoint to attach to a running container. rpc Attach(AttachRequest) returns (AttachResponse) {} // PortForward prepares a streaming endpoint to forward ports from a PodSandbox. rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {} // ContainerStats returns stats of the container. If the container does not // exist, the call returns an error. rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {} // ListContainerStats returns stats of all running containers. rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {} // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} // Status returns the status of the runtime. rpc Status(StatusRequest) returns (StatusResponse) {} } 5.2.2. image service // ImageService defines the public APIs for managing images. service ImageService { // ListImages lists existing images. rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {} // ImageStatus returns the status of the image. If the image is not // present, returns a response with ImageStatusResponse.Image set to // nil. rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {} // PullImage pulls an image with authentication config. rpc PullImage(PullImageRequest) returns (PullImageResponse) {} // RemoveImage removes the image. // This call is idempotent, and must not return an error if the image has // already been removed. rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {} // ImageFSInfo returns information of the filesystem that is used to store images. rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {} } 5.3. cri-containerd 5.3.1. CRI Pluginè°ƒç”¨æµç¨‹ kubeletè°ƒç”¨CRIæ’ä»¶ï¼Œé€šè¿‡CRI Runtime Serviceæ¥å£åˆ›å»ºpod crié€šè¿‡CNIæ¥å£åˆ›å»ºå’Œé…ç½®podçš„network namespace criè°ƒç”¨containerdåˆ›å»ºsandbox containerï¼ˆpause container ï¼‰å¹¶å°†å®¹å™¨æ”¾å…¥podçš„cgroupå’Œnamespaceä¸­ kubeletè°ƒç”¨CRIæ’ä»¶ï¼Œé€šè¿‡image serviceæ¥å£æ‹‰å–é•œåƒï¼Œæ¥ç€é€šè¿‡containerdæ¥æ‹‰å–é•œåƒ kubeletè°ƒç”¨CRIæ’ä»¶ï¼Œé€šè¿‡runtime serviceæ¥å£è¿è¡Œæ‹‰å–ä¸‹æ¥çš„é•œåƒæœåŠ¡ï¼Œæœ€åé€šè¿‡containerdæ¥è¿è¡Œä¸šåŠ¡å®¹å™¨ï¼Œå¹¶å°†å®¹å™¨æ”¾å…¥podçš„cgroupå’Œnamespaceä¸­ã€‚ å…·ä½“å‚è€ƒï¼šhttps://github.com/containerd/cri/blob/release/1.4/docs/architecture.md\n5.3.2. k8så¯¹runtimeè°ƒç”¨çš„æ¼”è¿› ç”±åŸæ¥é€šè¿‡dockershimè°ƒç”¨dockerå†è°ƒç”¨containerdï¼Œç›´æ¥å˜æˆé€šè¿‡cri-containerdè°ƒç”¨containerdï¼Œä»è€Œå‡å°‘äº†ä¸€å±‚dockerè°ƒç”¨é€»è¾‘ã€‚\nå…·ä½“å‚è€ƒï¼šhttps://github.com/containerd/cri/blob/release/1.4/docs/proposal.md\n5.4. Dockershim åœ¨æ—§ç‰ˆæœ¬çš„k8sä¸­ï¼Œç”±äºdockeræ²¡æœ‰å®ç°CRIæ¥å£ï¼Œå› æ­¤å¢åŠ ä¸€ä¸ªDockershimæ¥å®ç°k8så¯¹dockerçš„è°ƒç”¨ã€‚ï¼ˆshimï¼šå«ç‰‡ï¼Œä¸€èˆ¬ç”¨æ¥è¡¨ç¤ºå¯¹ç¬¬ä¸‰æ–¹ç»„ä»¶APIè°ƒç”¨çš„é€‚é…æ’ä»¶ï¼Œä¾‹å¦‚k8sä½¿ç”¨Dockershimæ¥å®ç°å¯¹dockeræ¥å£çš„é€‚é…è°ƒç”¨ï¼‰\n5.5. CRI-O cri-oä¸containerdç±»ä¼¼ï¼Œç”¨æ¥å®ç°å®¹å™¨çš„ç®¡ç†ï¼Œå¯æ›¿æ¢containerdçš„ä½¿ç”¨ã€‚\nå‚è€ƒï¼š\nhttps://opencontainers.org/about/overview/ https://github.com/opencontainers/runtime-spec https://github.com/kubernetes/kubernetes/blob/242a97307b34076d5d8f5bbeb154fa4d97c9ef1d/docs/devel/container-runtime-interface.md https://github.com/containerd/containerd/blob/main/docs/cri/architecture.md https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/ ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦åˆ†æOCIï¼ŒCRIï¼Œruncï¼Œcontainerdï¼Œcri-containerdï¼Œdockershimç­‰ç»„ä»¶è¯´æ˜åŠè°ƒç”¨å…³ç³»ã€‚\n1. â€¦","ref":"/kubernetes-notes/runtime/runtime/","tags":["Kubernetes","Runtime"],"title":"Runcå’ŒContainerdæ¦‚è¿°"},{"body":"1. Go modulesç®€ä»‹ Go 1.11ç‰ˆæœ¬å¼€å§‹æ”¯æŒGo modulesæ–¹å¼çš„ä¾èµ–åŒ…ç®¡ç†åŠŸèƒ½ï¼Œå®˜ç½‘å‚è€ƒï¼šhttps://github.com/golang/go/wiki/Modules ã€‚\n2. go modçš„ä½¿ç”¨ é¡¹ç›®æ–‡ä»¶å¦‚ä¸‹ï¼š\nhello.go\npackage main import ( \"fmt\" \"rsc.io/quote\" ) func main() { fmt.Println(quote.Hello()) } æ“ä½œè®°å½•ï¼š\n# å®‰è£…GO 1.11åŠä»¥ä¸Šç‰ˆæœ¬ go version go version go1.12.5 darwin/amd64 # å¼€å¯moduleåŠŸèƒ½ export GO111MODULE=on # è¿›å…¥åˆ°é¡¹ç›®ç›®å½• cd /home/gopath/src/hello # åˆå§‹åŒ– go mod init go: creating new go.mod: module hello # ç¼–è¯‘ go build go: finding rsc.io/quote v1.5.2 go: downloading rsc.io/quote v1.5.2 go: extracting rsc.io/quote v1.5.2 go: finding rsc.io/sampler v1.3.0 go: finding golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: downloading rsc.io/sampler v1.3.0 go: extracting rsc.io/sampler v1.3.0 go: downloading golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: extracting golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c # ä¸æ·»åŠ vendorç›®å½• go mod tidy -v # å¦‚æœæ·»åŠ vendorç›®å½•ï¼Œåˆ™æ‰§è¡Œvendorå‚æ•° go mod vendor -v # å‘½ä»¤è¾“å‡ºå¦‚ä¸‹: # golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c golang.org/x/text/language golang.org/x/text/internal/tag # rsc.io/quote v1.5.2 rsc.io/quote # rsc.io/sampler v1.3.0 rsc.io/sampler # æ–‡ä»¶ç›®å½•ç»“æ„ ./ â”œâ”€â”€ go.mod â”œâ”€â”€ go.sum â”œâ”€â”€ hello # äºŒè¿›åˆ¶æ–‡ä»¶ â”œâ”€â”€ hello.go â””â”€â”€ vendor â”œâ”€â”€ golang.org â”œâ”€â”€ modules.txt â””â”€â”€ rsc.io 3. go modçš„ç›¸å…³æ–‡ä»¶ 3.1. go.mod æ–‡ä»¶è·¯å¾„ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹\nmodule hello go 1.12 require rsc.io/quote v1.5.2 3.2. go.sum æ–‡ä»¶è·¯å¾„ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c h1:qgOY6WgZOaTkIIMiVjBQcw93ERBE4m30iBm00nkL0i8= golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ= rsc.io/quote v1.5.2 h1:w5fcysjrx7yqtD/aO+QwRjYZOKnaM9Uh2b40tElTs3Y= rsc.io/quote v1.5.2/go.mod h1:LzX7hefJvL54yjefDEDHNONDjII0t9xZLPXsUe+TKr0= rsc.io/sampler v1.3.0 h1:7uVkIFmeBqHfdjD+gZwtXXI+RODJ2Wc4O7MPEh/QiW4= rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA= 3.3. modules.txt æ–‡ä»¶è·¯å¾„ï¼š/{project}/vendor/modules.txt\n# golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c golang.org/x/text/language golang.org/x/text/internal/tag # rsc.io/quote v1.5.2 rsc.io/quote # rsc.io/sampler v1.3.0 rsc.io/sampler 4. go modçš„å¸®åŠ©ä¿¡æ¯ go help mod Go mod provides access to operations on modules. Note that support for modules is built into all the go commands, not just 'go mod'. For example, day-to-day adding, removing, upgrading, and downgrading of dependencies should be done using 'go get'. See 'go help modules' for an overview of module functionality. Usage: go mod \u003ccommand\u003e [arguments] The commands are: download download modules to local cache edit edit go.mod from tools or scripts graph print module requirement graph init initialize new module in current directory tidy add missing and remove unused modules vendor make vendored copy of dependencies verify verify dependencies have expected content why explain why packages or modules are needed Use \"go help mod \u003ccommand\u003e\" for more information about a command. 4.1. go mod init go help mod init usage: go mod init [module] Init initializes and writes a new go.mod to the current directory, in effect creating a new module rooted at the current directory. The file go.mod must not already exist. If possible, init will guess the module path from import comments (see 'go help importpath') or from version control configuration. To override this guess, supply the module path as an argument. 4.2. go mod tidy usage: go mod tidy [-v] Tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module's packages and dependencies, and it removes unused modules that don't provide any relevant packages. It also adds any missing entries to go.sum and removes any unnecessary ones. The -v flag causes tidy to print information about removed modules to standard error. 4.3. go mod vendor go help mod vendor usage: go mod vendor [-v] Vendor resets the main module's vendor directory to include all packages needed to build and test all the main module's packages. It does not include test code for vendored packages. The -v flag causes vendor to print the names of vendored modules and packages to standard error. å‚è€ƒï¼š\nhttps://github.com/golang/go/wiki/Modules https://blog.golang.org/modules2019 https://blog.golang.org/using-go-modules ","categories":"","description":"","excerpt":"1. Go modulesç®€ä»‹ Go 1.11ç‰ˆæœ¬å¼€å§‹æ”¯æŒGo modulesæ–¹å¼çš„ä¾èµ–åŒ…ç®¡ç†åŠŸèƒ½ï¼Œå®˜ç½‘å‚ â€¦","ref":"/golang-notes/introduction/package/go-modules/","tags":["Golang"],"title":"go modulesçš„ä½¿ç”¨"},{"body":"kubeedgeæºç åˆ†æä¹‹cloudcore æœ¬æ–‡æºç åˆ†æåŸºäºkubeedge v1.1.0\næœ¬æ–‡ä¸»è¦åˆ†æcloudcoreä¸­CloudCoreCommandçš„åŸºæœ¬æµç¨‹ï¼Œå…·ä½“çš„cloudhubã€edgecontrollerã€devicecontrolleræ¨¡å—çš„å®ç°é€»è¾‘å¾…åç»­å•ç‹¬æ–‡ç« åˆ†æã€‚\nç›®å½•ç»“æ„ï¼š\ncloud/cmd/cloudcore\ncloudcore â”œâ”€â”€ app â”‚Â â”œâ”€â”€ options â”‚Â â”‚Â â””â”€â”€ options.go â”‚Â â””â”€â”€ server.go # NewCloudCoreCommandã€registerModules â””â”€â”€ cloudcore.go # mainå‡½æ•° cloudcoreéƒ¨åˆ†åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š\ncloudhub edgecontroller devicecontroller 1. mainå‡½æ•° kubeedgeçš„ä»£ç é‡‡ç”¨cobraå‘½ä»¤æ¡†æ¶ï¼Œä»£ç é£æ ¼ä¸k8sæºç é£æ ¼ç±»ä¼¼ã€‚cmdç›®å½•ä¸»è¦ä¸ºcobra commandçš„åŸºæœ¬å†…å®¹åŠå‚æ•°è§£æï¼Œpkgç›®å½•åŒ…å«å…·ä½“çš„å®ç°é€»è¾‘ã€‚\ncloud/cmd/cloudcore/cloudcore.go\nfunc main() { command := app.NewCloudCoreCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 2. NewCloudCoreCommand NewCloudCoreCommandä¸ºcobra commandçš„æ„é€ å‡½æ•°ï¼Œè¯¥ç±»å‡½æ•°ä¸€èˆ¬åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\næ„é€ option æ·»åŠ Flags è¿è¡ŒRunå‡½æ•°ï¼ˆæ ¸å¿ƒï¼‰ cloud/cmd/cloudcore/app/server.go\nfunc NewCloudCoreCommand() *cobra.Command { opts := options.NewCloudCoreOptions() cmd := \u0026cobra.Command{ Use: \"cloudcore\", Long: `CloudCore is the core cloud part of KubeEdge, which contains three modules: cloudhub, edgecontroller, and devicecontroller. Cloudhub is a web server responsible for watching changes at the cloud side, caching and sending messages to EdgeHub. EdgeController is an extended kubernetes controller which manages edge nodes and pods metadata so that the data can be targeted to a specific edge node. DeviceController is an extended kubernetes controller which manages devices so that the device metadata/status date can be synced between edge and cloud.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() flag.PrintFlags(cmd.Flags()) // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) registerModules() // start all modules core.Run() }, } fs := cmd.Flags() namedFs := opts.Flags() verflag.AddFlags(namedFs.FlagSet(\"global\")) globalflag.AddGlobalFlags(namedFs.FlagSet(\"global\"), cmd.Name()) for _, f := range namedFs.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStderr(), namedFs, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStdout(), namedFs, cols) }) return cmd } æ ¸å¿ƒä»£ç ï¼š\n// æ„é€ option opts := options.NewCloudCoreOptions() // æ‰§è¡Œrunå‡½æ•° registerModules() core.Run() // æ·»åŠ flags fs.AddFlagSet(f) 3. registerModules ç”±äºkubeedgeçš„ä»£ç çš„å¤§éƒ¨åˆ†æ¨¡å—éƒ½é‡‡ç”¨äº†åŸºäºgo-channelçš„æ¶ˆæ¯é€šä¿¡æ¡†æ¶Beehiveï¼ˆå¾…åç»­å•ç‹¬æ–‡ç« åˆ†æï¼‰ï¼Œå› æ­¤åœ¨å„æ¨¡å—å¯åŠ¨ä¹‹å‰ï¼Œéœ€è¦å°†è¯¥æ¨¡å—æ³¨å†Œåˆ°beehiveçš„æ¡†æ¶ä¸­ã€‚\nå…¶ä¸­cloudcoreéƒ¨åˆ†æ¶‰åŠçš„æ¨¡å—æœ‰ï¼š\ncloudhub edgecontroller devicecontroller cloud/cmd/cloudcore/app/server.go\n// registerModules register all the modules started in cloudcore func registerModules() { cloudhub.Register() edgecontroller.Register() devicecontroller.Register() } ä»¥ä¸‹ä»¥cloudhubä¸ºä¾‹è¯´æ˜æ³¨å†Œçš„è¿‡ç¨‹ã€‚\ncloudhubç»“æ„ä½“ä¸»è¦åŒ…å«ï¼š\ncontextï¼šä¸Šä¸‹æ–‡ï¼Œç”¨æ¥ä¼ é€’æ¶ˆæ¯ä¸Šä¸‹æ–‡ stopChanï¼šgo channelé€šä¿¡ beehiveæ¡†æ¶ä¸­çš„æ¨¡å—éœ€è¦å®ç°Moduleæ¥å£ï¼Œå› æ­¤cloudhubä¹Ÿå®ç°äº†è¯¥æ¥å£ï¼Œå…¶ä¸­æ ¸å¿ƒæ–¹æ³•ä¸ºStartï¼Œç”¨æ¥å¯åŠ¨ç›¸åº”æ¨¡å—çš„è¿è¡Œã€‚\nvendor/github.com/kubeedge/beehive/pkg/core/module.go\n// Module interface type Module interface { Name() string Group() string Start(c *context.Context) Cleanup() } ä»¥ä¸‹ä¸ºcloudHubç»“æ„ä½“åŠæ³¨å†Œå‡½æ•°ã€‚\ncloud/pkg/cloudhub/cloudhub.go\ntype cloudHub struct { context *context.Context stopChan chan bool } func Register() { core.Register(\u0026cloudHub{}) } å…·ä½“çš„æ³¨å†Œå®ç°å‡½æ•°ä¸ºcore.Registerï¼Œæ³¨å†Œè¿‡ç¨‹å®é™…ä¸Šå°±æ˜¯å°†å…·ä½“çš„æ¨¡å—ç»“æ„ä½“æ”¾å…¥ä¸€ä¸ªä»¥æ¨¡å—åä¸ºkeyçš„mapæ˜ å°„ä¸­ï¼Œå¾…åç»­è°ƒç”¨ã€‚\nvendor/github.com/kubeedge/beehive/pkg/core/module.go\n// Register register module func Register(m Module) { if isModuleEnabled(m.Name()) { modules[m.Name()] = m //å°†å…·ä½“çš„æ¨¡å—ç»“æ„ä½“æ”¾å…¥ä¸€ä¸ªä»¥æ¨¡å—åä¸ºkeyçš„mapæ˜ å°„ä¸­ log.LOGGER.Info(\"module \" + m.Name() + \" registered\") } else { disabledModules[m.Name()] = m log.LOGGER.Info(\"module \" + m.Name() + \" is not register, please check modules.yaml\") } } 4. core.Run CloudCoreCommandå‘½ä»¤çš„Runå‡½æ•°å®é™…ä¸Šæ˜¯è¿è¡Œbeehiveæ¡†æ¶ä¸­æ³¨å†Œçš„æ‰€æœ‰æ¨¡å—ã€‚\nå…¶ä¸­åŒ…æ‹¬ä¸¤éƒ¨åˆ†é€»è¾‘ï¼š\nå¯åŠ¨è¿è¡Œæ‰€æœ‰æ³¨å†Œæ¨¡å— ç›‘å¬ä¿¡å·å¹¶åšä¼˜é›…æ¸…ç† vendor/github.com/kubeedge/beehive/pkg/core/core.go\n//Run starts the modules and in the end does module cleanup func Run() { //Address the module registration and start the core StartModules() // monitor system signal and shutdown gracefully GracefulShutdown() } 5. StartModules StartModulesè·å–contextä¸Šä¸‹æ–‡ï¼Œå¹¶ä»¥goroutineçš„æ–¹å¼è¿è¡Œæ‰€æœ‰å·²æ³¨å†Œçš„æ¨¡å—ã€‚å…¶ä¸­Startå‡½æ•°å³æ¯ä¸ªæ¨¡å—çš„å…·ä½“å®ç°Moduleæ¥å£ä¸­çš„Startæ–¹æ³•ã€‚ä¸åŒæ¨¡å—å„è‡ªå®šä¹‰è‡ªå·±çš„å…·ä½“Startæ–¹æ³•å®ç°ã€‚\ncoreContext := context.GetContext(context.MsgCtxTypeChannel) go module.Start(coreContext) å…·ä½“å®ç°å¦‚ä¸‹ï¼š\nvendor/github.com/kubeedge/beehive/pkg/core/core.go\n// StartModules starts modules that are registered func StartModules() { coreContext := context.GetContext(context.MsgCtxTypeChannel) modules := GetModules() for name, module := range modules { //Init the module coreContext.AddModule(name) //Assemble typeChannels for send2Group coreContext.AddModuleGroup(name, module.Group()) go module.Start(coreContext) log.LOGGER.Info(\"starting module \" + name) } } 6. GracefulShutdown å½“æ”¶åˆ°ç›¸å…³ä¿¡å·ï¼Œåˆ™æ‰§è¡Œå„ä¸ªæ¨¡å—å®ç°çš„Cleanupæ–¹æ³•ã€‚\nvendor/github.com/kubeedge/beehive/pkg/core/core.go\n// GracefulShutdown is if it gets the special signals it does modules cleanup func GracefulShutdown() { c := make(chan os.Signal) signal.Notify(c, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM, syscall.SIGQUIT, syscall.SIGILL, syscall.SIGTRAP, syscall.SIGABRT) select { case s := \u003c-c: log.LOGGER.Info(\"got os signal \" + s.String()) //Cleanup each modules modules := GetModules() for name, module := range modules { log.LOGGER.Info(\"Cleanup module \" + name) module.Cleanup() } } } å‚è€ƒï¼š\nhttps://github.com/kubeedge/kubeedge/tree/release-1.1/cloud/cmd/cloudcore https://github.com/kubeedge/kubeedge/tree/release-1.1/vendor/github.com/kubeedge/beehive/pkg/core ","categories":"","description":"","excerpt":"kubeedgeæºç åˆ†æä¹‹cloudcore æœ¬æ–‡æºç åˆ†æåŸºäºkubeedge v1.1.0\næœ¬æ–‡ä¸»è¦åˆ†æcloudcore â€¦","ref":"/kubernetes-notes/edge/kubeedge/code-analysis/cloudcore/","tags":["Kubeedge"],"title":"Kubeedgeä¹‹cloudcore æºç åˆ†æ"},{"body":"kubeedgeæºç åˆ†æä¹‹edgecore æœ¬æ–‡æºç åˆ†æåŸºäºkubeedge v1.1.0\næœ¬æ–‡ä¸»è¦åˆ†æedgecoreä¸­EdgeCoreCommandçš„åŸºæœ¬æµç¨‹ï¼Œå…·ä½“çš„edgedã€edgehubã€metamanagerç­‰æ¨¡å—çš„å®ç°é€»è¾‘å¾…åç»­å•ç‹¬æ–‡ç« åˆ†æã€‚\nç›®å½•ç»“æ„ï¼š\nedgecore â”œâ”€â”€ app â”‚Â â”œâ”€â”€ options â”‚Â â”‚Â â””â”€â”€ options.go â”‚Â â””â”€â”€ server.go # NewEdgeCoreCommand ã€registerModules â””â”€â”€ edgecore.go # main edgecoreæ¨¡å—åŒ…å«ï¼š\nedged edgehub metamanager eventbus servicebus devicetwin edgemesh 1. mainå‡½æ•° mainå…¥å£å‡½æ•°ï¼Œä»ç„¶æ˜¯cobraå‘½ä»¤æ¡†æ¶æ ¼å¼ã€‚\nedge/cmd/edgecore/edgecore.go\nfunc main() { command := app.NewEdgeCoreCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 2. NewEdgeCoreCommand NewEdgeCoreCommandä¸NewCloudCoreCommandä¸€æ ·æ„é€ å¯¹åº”çš„cobra commandç»“æ„ä½“ã€‚\nedge/cmd/edgecore/app/server.go\n// NewEdgeCoreCommand create edgecore cmd func NewEdgeCoreCommand() *cobra.Command { opts := options.NewEdgeCoreOptions() cmd := \u0026cobra.Command{ Use: \"edgecore\", Long: `Edgecore is the core edge part of KubeEdge, which contains six modules: devicetwin, edged, edgehub, eventbus, metamanager, and servicebus. DeviceTwin is responsible for storing device status and syncing device status to the cloud. It also provides query interfaces for applications. Edged is an agent that runs on edge nodes and manages containerized applications and devices. Edgehub is a web socket client responsible for interacting with Cloud Service for the edge computing (like Edge Controller as in the KubeEdge Architecture). This includes syncing cloud-side resource updates to the edge, and reporting edge-side host and device status changes to the cloud. EventBus is a MQTT client to interact with MQTT servers (mosquito), offering publish and subscribe capabilities to other components. MetaManager is the message processor between edged and edgehub. It is also responsible for storing/retrieving metadata to/from a lightweight database (SQLite).ServiceBus is a HTTP client to interact with HTTP servers (REST), offering HTTP client capabilities to components of cloud to reach HTTP servers running at edge. `, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() flag.PrintFlags(cmd.Flags()) // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) registerModules() // start all modules core.Run() }, } fs := cmd.Flags() namedFs := opts.Flags() verflag.AddFlags(namedFs.FlagSet(\"global\")) globalflag.AddGlobalFlags(namedFs.FlagSet(\"global\"), cmd.Name()) for _, f := range namedFs.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStderr(), namedFs, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) cliflag.PrintSections(cmd.OutOrStdout(), namedFs, cols) }) return cmd } æ ¸å¿ƒä»£ç ï¼š\nopts := options.NewEdgeCoreOptions() registerModules() core.Run() 3. registerModules edgecoreä»ç„¶é‡‡ç”¨Beehiveé€šä¿¡æ¡†æ¶ï¼Œæ¨¡å—è°ƒç”¨å‰å…ˆæ³¨å†Œå¯¹åº”çš„æ¨¡å—ã€‚å…·ä½“å‚è€ƒcloudcore.registerModuleså¤„çš„åˆ†æï¼Œæ­¤å¤„ä¸å†å±•å¼€åˆ†ææ³¨å†Œæµç¨‹ã€‚æ­¤å¤„æ³¨å†Œçš„æ˜¯edgecoreä¸­æ¶‰åŠçš„ç»„ä»¶ã€‚\nedge/cmd/edgecore/app/server.go\n// registerModules register all the modules started in edgecore func registerModules() { devicetwin.Register() edged.Register() edgehub.Register() eventbus.Register() edgemesh.Register() metamanager.Register() servicebus.Register() test.Register() dbm.InitDBManager() } 4. core.Run core.Runä¸cloudcore.runå¤„é€»è¾‘ä¸€è‡´ä¸å†å±•å¼€åˆ†æã€‚\nvendor/github.com/kubeedge/beehive/pkg/core/core.go\n//Run starts the modules and in the end does module cleanup func Run() { //Address the module registration and start the core StartModules() // monitor system signal and shutdown gracefully GracefulShutdown() } å‚è€ƒï¼š\nhttps://github.com/kubeedge/kubeedge/tree/release-1.1/edge/cmd/edgecore ","categories":"","description":"","excerpt":"kubeedgeæºç åˆ†æä¹‹edgecore æœ¬æ–‡æºç åˆ†æåŸºäºkubeedge v1.1.0\næœ¬æ–‡ä¸»è¦åˆ†æedgecore â€¦","ref":"/kubernetes-notes/edge/kubeedge/code-analysis/edgecore/","tags":["Kubeedge"],"title":"Kubeedgeä¹‹edgecore æºç åˆ†æ"},{"body":"1. kubebuilder 1.1. å®‰è£…kubebuilder # download kubebuilder and install locally. curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder \u0026\u0026 mv kubebuilder /usr/local/bin/ 1.2. kubebuilderå‘½ä»¤ Development kit for building Kubernetes extensions and tools. Provides libraries and tools to create new projects, APIs and controllers. Includes tools for packaging artifacts into an installer container. Typical project lifecycle: - initialize a project: kubebuilder init --domain example.com --license apache2 --owner \"The Kubernetes authors\" - create one or more a new resource APIs and add your code to them: kubebuilder create api --group \u003cgroup\u003e --version \u003cversion\u003e --kind \u003cKind\u003e Create resource will prompt the user for if it should scaffold the Resource and / or Controller. To only scaffold a Controller for an existing Resource, select \"n\" for Resource. To only define the schema for a Resource without writing a Controller, select \"n\" for Controller. After the scaffold is written, api will run make on the project. Usage: kubebuilder [command] Available Commands: create Scaffold a Kubernetes API or webhook. edit This command will edit the project configuration help Help about any command init Initialize a new project version Print the kubebuilder version Flags: -h, --help help for kubebuilder Use \"kubebuilder [command] --help\" for more information about a command. 2. æ“ä½œæ­¥éª¤ 2.1. åˆå§‹åŒ– mkdir $GOPATH/src/github.com/huweihuang/operator-example cd $GOPATH/src/github.com/huweihuang/operator-example go mod init github.com/huweihuang/operator-example 2.2. åˆ›å»ºé¡¹ç›® # kubebuilder init --domain github.com --license apache2 --owner \"Hu Weihuang\" Writing scaffold for you to edit... Get controller runtime: $ go get sigs.k8s.io/controller-runtime@v0.5.0 Update go.mod: $ go mod tidy Running make: $ make go: creating new go.mod: module tmp go: finding sigs.k8s.io v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd/controller-gen v0.2.5 /Users/weihuanghu/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\" go fmt ./... go vet ./... go build -o bin/manager main.go Next: define a resource with: $ kubebuilder create api æŸ¥çœ‹ç”Ÿæˆæ–‡ä»¶ï¼š\n./ â”œâ”€â”€ Dockerfile â”œâ”€â”€ Makefile â”œâ”€â”€ PROJECT â”œâ”€â”€ bin â”‚Â â””â”€â”€ manager â”œâ”€â”€ config â”‚Â â”œâ”€â”€ certmanager â”‚Â â”‚Â â”œâ”€â”€ certificate.yaml â”‚Â â”‚Â â”œâ”€â”€ kustomization.yaml â”‚Â â”‚Â â””â”€â”€ kustomizeconfig.yaml â”‚Â â”œâ”€â”€ default â”‚Â â”‚Â â”œâ”€â”€ kustomization.yaml â”‚Â â”‚Â â”œâ”€â”€ manager_auth_proxy_patch.yaml â”‚Â â”‚Â â”œâ”€â”€ manager_webhook_patch.yaml â”‚Â â”‚Â â””â”€â”€ webhookcainjection_patch.yaml â”‚Â â”œâ”€â”€ manager â”‚Â â”‚Â â”œâ”€â”€ kustomization.yaml â”‚Â â”‚Â â””â”€â”€ manager.yaml â”‚Â â”œâ”€â”€ prometheus â”‚Â â”‚Â â”œâ”€â”€ kustomization.yaml â”‚Â â”‚Â â””â”€â”€ monitor.yaml â”‚Â â”œâ”€â”€ rbac â”‚Â â”‚Â â”œâ”€â”€ auth_proxy_client_clusterrole.yaml â”‚Â â”‚Â â”œâ”€â”€ auth_proxy_role.yaml â”‚Â â”‚Â â”œâ”€â”€ auth_proxy_role_binding.yaml â”‚Â â”‚Â â”œâ”€â”€ auth_proxy_service.yaml â”‚Â â”‚Â â”œâ”€â”€ kustomization.yaml â”‚Â â”‚Â â”œâ”€â”€ leader_election_role.yaml â”‚Â â”‚Â â”œâ”€â”€ leader_election_role_binding.yaml â”‚Â â”‚Â â””â”€â”€ role_binding.yaml â”‚Â â””â”€â”€ webhook â”‚Â â”œâ”€â”€ kustomization.yaml â”‚Â â”œâ”€â”€ kustomizeconfig.yaml â”‚Â â””â”€â”€ service.yaml â”œâ”€â”€ go.mod â”œâ”€â”€ go.sum â”œâ”€â”€ hack â”‚Â â””â”€â”€ boilerplate.go.txt â””â”€â”€ main.go 2.3. åˆ›å»ºAPI # kubebuilder create api --group webapp --version v1 --kind Guestbook Create Resource [y/n] y Create Controller [y/n] y Writing scaffold for you to edit... api/v1/guestbook_types.go controllers/guestbook_controller.go Running make: $ make go: creating new go.mod: module tmp go: finding sigs.k8s.io/controller-tools/cmd v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd/controller-gen v0.2.5 go: finding sigs.k8s.io v0.2.5 /Users/weihuanghu/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\" go fmt ./... go vet ./... go build -o bin/manager main.go æŸ¥çœ‹åˆ›å»ºæ–‡ä»¶\napi â””â”€â”€ v1 â”œâ”€â”€ groupversion_info.go â”œâ”€â”€ guestbook_types.go â””â”€â”€ zz_generated.deepcopy.go controllers â”œâ”€â”€ guestbook_controller.go â””â”€â”€ suite_test.go æŸ¥çœ‹api/v1/guestbook_types.go\n// GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \"make\" to regenerate code after modifying this file // Quantity of instances // +kubebuilder:validation:Minimum=1 // +kubebuilder:validation:Maximum=10 Size int32 `json:\"size\"` // Name of the ConfigMap for GuestbookSpec's configuration // +kubebuilder:validation:MaxLength=15 // +kubebuilder:validation:MinLength=1 ConfigMapName string `json:\"configMapName\"` // +kubebuilder:validation:Enum=Phone;Address;Name Type string `json:\"alias,omitempty\"` } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run \"make\" to regenerate code after modifying this file // PodName of the active Guestbook node. Active string `json:\"active\"` // PodNames of the standby Guestbook nodes. Standby []string `json:\"standby\"` } // +kubebuilder:object:root=true // +kubebuilder:subresource:status // +kubebuilder:resource:scope=Cluster // Guestbook is the Schema for the guestbooks API type Guestbook struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } 3. troubleshooting 3.1. controller-gen: No such file or directory âœ operator-example kubebuilder init --domain github.com --license apache2 --owner \"Hu Weihuang\" Writing scaffold for you to edit... Get controller runtime: $ go get sigs.k8s.io/controller-runtime@v0.5.0 Update go.mod: $ go mod tidy Running make: $ make go: creating new go.mod: module tmp go: finding sigs.k8s.io v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd v0.2.5 go: finding sigs.k8s.io/controller-tools/cmd/controller-gen v0.2.5 /Users/weihuanghu/go:/Users/weihuanghu/k8spath/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\" /bin/sh: /Users/weihuanghu/go:/Users/weihuanghu/k8spath/bin/controller-gen: No such file or directory make: *** [generate] Error 127 2020/04/13 14:34:47 failed to initialize project: exit status 2 ç”±äºæœ¬åœ°å­˜åœ¨å¤šä¸ªGOPATHçš„ç›®å½•ï¼Œè€Œè·å–äº†éå½“å‰é¡¹ç›®ä¸‹çš„GOPATHç›®å½•ï¼Œå› æ­¤å°†å½“å‰é¡¹ç›®æ‰€åœ¨çš„GOPATHç›®å½•exportåˆ°GOPATHç¯å¢ƒå˜é‡ä¸­ï¼Œå°±å¯ä»¥è§£å†³ã€‚\nexport GOPATH=\"/path/to/gopath\" å‚è€ƒï¼š\nhttps://kubernetes.io/zh/docs/concepts/extend-kubernetes/operator/ https://github.com/kubernetes-sigs/kubebuilder https://book.kubebuilder.io/quick-start.html https://operatorhub.io/ https://devops.college/developing-kubernetes-operator-is-now-easy-with-operator-framework-d3194a7428ff ","categories":"","description":"","excerpt":"1. kubebuilder 1.1. å®‰è£…kubebuilder # download kubebuilder and install â€¦","ref":"/kubernetes-notes/develop/operator/kubebuilder/","tags":["Operator"],"title":"kubebuilderçš„ä½¿ç”¨"},{"body":"1. KubeEdgeç®€ä»‹ KubeEdgeæ˜¯åŸºäºkubernetesä¹‹ä¸Šå°†å®¹å™¨åŒ–åº”ç”¨çš„ç¼–æ’èƒ½åŠ›æ‹“å±•åˆ°è¾¹ç¼˜ä¸»æœºæˆ–è¾¹ç¼˜è®¾å¤‡ï¼Œåœ¨äº‘ç«¯å’Œè¾¹ç¼˜ç«¯æä¾›ç½‘ç»œé€šä¿¡ï¼Œåº”ç”¨éƒ¨ç½²ã€å…ƒæ•°æ®åŒæ­¥ç­‰åŠŸèƒ½ã€‚åŒæ—¶æ”¯æŒMQTTåè®®ï¼Œå…è®¸å¼€å‘è€…åœ¨è¾¹ç¼˜ç«¯è‡ªå®šä¹‰æ¥å…¥è¾¹ç¼˜è®¾å¤‡ã€‚\n2. åŠŸèƒ½ è¾¹ç¼˜è®¡ç®—ï¼šæä¾›è¾¹ç¼˜èŠ‚ç‚¹è‡ªæ²»èƒ½åŠ›ï¼Œè¾¹ç¼˜èŠ‚ç‚¹æ•°æ®å¤„ç†èƒ½åŠ›ã€‚ ä¾¿æ·éƒ¨ç½²ï¼šå¼€å‘è€…å¯ä»¥å¼€å‘httpæˆ–mqttåè®®çš„åº”ç”¨ï¼Œè¿è¡Œåœ¨äº‘ç«¯å’Œè¾¹ç¼˜ç«¯ã€‚ k8såŸç”Ÿæ”¯æŒï¼šå¯ä»¥é€šè¿‡k8sç®¡ç†å’Œç›‘æ§è¾¹ç¼˜è®¾å¤‡å’Œè¾¹ç¼˜èŠ‚ç‚¹ã€‚ ä¸°å¯Œçš„åº”ç”¨ç±»å‹ï¼šå¯ä»¥åœ¨è¾¹ç¼˜ç«¯éƒ¨ç½²æœºå™¨å­¦ä¹ ã€å›¾ç‰‡è¯†åˆ«ã€äº‹ä»¶å¤„ç†ç­‰åº”ç”¨ã€‚ 3. ç»„ä»¶ 3.1. äº‘ç«¯ CloudHubï¼šä¸€ä¸ªweb socketæœåŠ¡å™¨ï¼Œè´Ÿè´£ç›‘å¬äº‘ç«¯çš„æ›´æ–°ã€ç¼“å­˜åŠå‘EdgeHubå‘é€æ¶ˆæ¯ã€‚\nEdgeControllerï¼šä¸€ä¸ªæ‰©å±•çš„k8sæ§åˆ¶å™¨ï¼Œè´Ÿè´£ç®¡ç†è¾¹ç¼˜èŠ‚ç‚¹å’Œpodå…ƒæ•°æ®ï¼ŒåŒæ­¥è¾¹ç¼˜èŠ‚ç‚¹çš„æ•°æ®ï¼Œæ˜¯k8s-apiserver ä¸EdgeCoreçš„é€šä¿¡æ¡¥æ¢ã€‚\nDeviceControllerï¼šä¸€ä¸ªæ‰©å±•çš„k8sæ§åˆ¶å™¨ï¼Œè´Ÿè´£ç®¡ç†èŠ‚ç‚¹è®¾å¤‡ï¼ŒåŒæ­¥äº‘ç«¯å’Œè¾¹ç¼˜ç«¯çš„è®¾å¤‡å…ƒæ•°æ®å’ŒçŠ¶æ€ã€‚\n3.2. è¾¹ç¼˜ç«¯ EdgeHubï¼šä¸€ä¸ªweb socketå®¢æˆ·ç«¯ï¼Œè´Ÿè´£äº‘ç«¯ä¸è¾¹ç¼˜ç«¯çš„ä¿¡æ¯äº¤äº’ï¼Œå…¶ä¸­åŒ…æ‹¬å°†äº‘ç«¯çš„èµ„æºå˜æ›´åŒæ­¥åˆ°è¾¹ç¼˜ç«¯åŠè¾¹ç¼˜ç«¯çš„çŠ¶æ€å˜åŒ–åŒæ­¥åˆ°äº‘ç«¯ã€‚ Edgedï¼šè¿è¡Œåœ¨è¾¹ç¼˜èŠ‚ç‚¹ï¼Œç®¡ç†å®¹å™¨åŒ–åº”ç”¨çš„agentï¼Œè´Ÿè´£podç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†ï¼Œç±»ä¼¼kubeletã€‚ EventBusï¼šä¸€ä¸ªMQTTå®¢æˆ·ç«¯ï¼Œä¸MQTTæœåŠ¡ç«¯äº¤äº’ï¼Œæä¾›å‘å¸ƒ/è®¢é˜…çš„èƒ½åŠ›ã€‚ ServiceBusï¼šä¸€ä¸ªHTTPå®¢æˆ·ç«¯ï¼Œä¸HTTPæœåŠ¡ç«¯äº¤äº’ã€‚ä¸ºäº‘ç»„ä»¶æä¾›HTTPå®¢æˆ·ç«¯åŠŸèƒ½ï¼Œä»¥è®¿é—®åœ¨è¾¹ç¼˜è¿è¡Œçš„HTTPæœåŠ¡å™¨ã€‚ DeviceTwinï¼šè´Ÿè´£å­˜å‚¨è®¾å¤‡çŠ¶æ€å¹¶åŒæ­¥è®¾å¤‡çŠ¶æ€åˆ°äº‘ç«¯ï¼ŒåŒæ—¶æä¾›åº”ç”¨çš„æ¥å£æŸ¥è¯¢ã€‚ MetaManagerï¼šedgedå’Œedgehubä¹‹é—´çš„æ¶ˆæ¯å¤„ç†å™¨ï¼Œè´Ÿè´£å‘è½»é‡æ•°æ®åº“ï¼ˆSQLiteï¼‰å­˜å‚¨æˆ–æŸ¥è¯¢å…ƒæ•°æ®ã€‚ 4. æ¶æ„å›¾ å‚è€ƒï¼š\nhttps://github.com/kubeedge/kubeedge\nhttps://kubeedge.readthedocs.io/en/latest/modules/kubeedge.html\n","categories":"","description":"","excerpt":"1. KubeEdgeç®€ä»‹ KubeEdgeæ˜¯åŸºäºkubernetesä¹‹ä¸Šå°†å®¹å™¨åŒ–åº”ç”¨çš„ç¼–æ’èƒ½åŠ›æ‹“å±•åˆ°è¾¹ç¼˜ä¸»æœºæˆ–è¾¹ç¼˜è®¾å¤‡ï¼Œåœ¨äº‘ç«¯å’Œè¾¹ç¼˜ç«¯æä¾› â€¦","ref":"/kubernetes-notes/edge/kubeedge/kubeedge-arch/","tags":["Kubeedge"],"title":"KubeEdgeä»‹ç»"},{"body":"Kata-containerç®€ä»‹ kata-containeré€šè¿‡è½»é‡å‹è™šæ‹ŸæœºæŠ€æœ¯æ„å»ºä¸€ä¸ªå®‰å…¨çš„å®¹å™¨è¿è¡Œæ—¶ï¼Œè¡¨ç°åƒå®¹å™¨ä¸€æ ·ï¼Œä½†é€šç¡¬ä»¶è™šæ‹ŸåŒ–æŠ€æœ¯æä¾›å¼ºéš”ç¦»ï¼Œä½œä¸ºç¬¬äºŒå±‚çš„å®‰å…¨é˜²æŠ¤ã€‚\nç‰¹ç‚¹ï¼š\nå®‰å…¨ï¼šç‹¬ç«‹çš„å†…æ ¸ï¼Œæä¾›ç½‘ç»œã€I/Oã€å†…å­˜çš„éš”ç¦»ã€‚ å…¼å®¹æ€§ï¼šæ”¯æŒOCIå®¹å™¨æ ‡å‡†ï¼Œk8sçš„CRIæ¥å£ã€‚ æ€§èƒ½ï¼šå…¼å®¹è™šæ‹Ÿæœºçš„å®‰å…¨å’Œå®¹å™¨çš„è½»é‡ç‰¹ç‚¹ã€‚ ç®€å•ï¼šä½¿ç”¨æ ‡å‡†çš„æ¥å£ã€‚ 1. kata-containeræ¶æ„ kata-containerä¸ä¼ ç»Ÿcontainerçš„æ¯”è¾ƒ\n2. kata-runtime Kata Containers runtime (kata-runtime)é€šè¿‡QEMU*/KVMæŠ€æœ¯åˆ›å»ºäº†ä¸€ç§è½»é‡å‹çš„è™šæ‹Ÿæœºï¼Œå…¼å®¹ OCI runtime specification æ ‡å‡†ï¼Œæ”¯æŒKubernetes* Container Runtime Interface (CRI)æ¥å£ï¼Œå¯æ›¿æ¢CRI shim runtime (runc) é€šè¿‡k8sæ¥åˆ›å»ºpodæˆ–å®¹å™¨ã€‚\n3. shim shimç±»ä¼¼Dockerçš„ containerd-shim æˆ–CRI-Oçš„ conmonï¼Œä¸»è¦ç”¨æ¥ç›‘æ§å’Œå›æ”¶å®¹å™¨çš„è¿›ç¨‹ï¼Œkata-shiméœ€è¦å¤„ç†æ‰€æœ‰çš„å®¹å™¨çš„IOæµ(stdout, stdin and stderr)å’Œè½¬å‘ç›¸å…³ä¿¡å·ã€‚\ncontainerd-shim-kata-v2å®ç°äº†Containerd Runtime V2 (Shim API)ï¼Œk8så¯ä»¥é€šè¿‡containerd-shim-kata-v2ï¼ˆæ›¿ä»£2N+1ä¸ªshims[ç”±ä¸€ä¸ªcontainerd-shimå’Œkata-shimç»„æˆ]ï¼‰æ¥åˆ›å»ºpodã€‚\n4. kata-agent åœ¨è™šæ‹Ÿæœºå†…kata-agentä½œä¸ºä¸€ä¸ªdaemonè¿›ç¨‹è¿è¡Œï¼Œå¹¶æ‹‰èµ·å®¹å™¨çš„è¿›ç¨‹ã€‚kata-agentä½¿ç”¨VIRTIOæˆ–VSOCKæ¥å£ï¼ˆQEMUåœ¨ä¸»æœºä¸Šæš´éœ²çš„socketæ–‡ä»¶ï¼‰åœ¨guestè™šæ‹Ÿæœºä¸­è¿è¡ŒgRPCæœåŠ¡å™¨ã€‚kata-runtimeé€šè¿‡grpcåè®®ä¸kata-agenté€šä¿¡ï¼Œå‘kata-agentå‘é€ç®¡ç†å®¹å™¨çš„å‘½ä»¤ã€‚è¯¥åè®®è¿˜ç”¨äºå®¹å™¨å’Œç®¡ç†å¼•æ“ï¼ˆä¾‹å¦‚Docker Engineï¼‰ä¹‹é—´ä¼ é€I / Oæµï¼ˆstdoutï¼Œstderrï¼Œstdinï¼‰ã€‚\nå®¹å™¨å†…æ‰€æœ‰çš„æ‰§è¡Œå‘½ä»¤å’Œç›¸å…³çš„IOæµéƒ½éœ€è¦é€šè¿‡QEMUåœ¨å®¿ä¸»æœºæš´éœ²çš„virtio-serialæˆ–vsockæ¥å£ï¼Œå½“ä½¿ç”¨VIRTIOçš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè™šæ‹Ÿæœºä¼šåˆ›å»ºä¸€ä¸ªKata Containers proxy (kata-proxy) æ¥å¤„ç†å‘½ä»¤å’ŒIOæµã€‚\nkata-agentä½¿ç”¨libcontainer æ¥ç®¡ç†å®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸï¼Œå¤ç”¨äº†runcçš„éƒ¨åˆ†ä»£ç ã€‚\n5. kata-proxy kata-proxyæä¾›äº† kata-shim å’Œ kata-runtime ä¸VMä¸­çš„kata-agenté€šä¿¡çš„æ–¹å¼ï¼Œå…¶ä¸­é€šä¿¡æ–¹å¼æ˜¯ä½¿ç”¨virtio-serialæˆ–vsockï¼Œé»˜è®¤æ˜¯ä½¿ç”¨virtio-serialã€‚\n6. Hypervisor kata-containeré€šè¿‡QEMU/KVMæ¥åˆ›å»ºè™šæ‹Ÿæœºç»™å®¹å™¨è¿è¡Œï¼Œå¯ä»¥æ”¯æŒå¤šç§hypervisorsã€‚\n7. QEMU/KVM å¾…è¡¥å……\nå‚è€ƒæ–‡æ¡£ï¼š\nhttps://katacontainers.io/\nhttps://github.com/kata-containers/documentation/blob/master/design/architecture.md\n","categories":"","description":"","excerpt":"Kata-containerç®€ä»‹ kata-containeré€šè¿‡è½»é‡å‹è™šæ‹ŸæœºæŠ€æœ¯æ„å»ºä¸€ä¸ªå®‰å…¨çš„å®¹å™¨è¿è¡Œæ—¶ï¼Œè¡¨ç°åƒå®¹å™¨ä¸€æ ·ï¼Œä½†é€šç¡¬ä»¶è™šæ‹ŸåŒ–æŠ€ â€¦","ref":"/kubernetes-notes/runtime/kata/kata-container/","tags":["Kubernetes","Runtime"],"title":"Kataå®¹å™¨ç®€ä»‹"},{"body":"1. CNIï¼ˆContainer Network Interfaceï¼‰ CNIï¼ˆContainer Network Interfaceï¼‰å³å®¹å™¨ç½‘ç»œæ¥å£ï¼Œé€šè¿‡çº¦å®šç»Ÿä¸€çš„å®¹å™¨ç½‘ç»œæ¥å£ï¼Œä»è€Œkubeletå¯ä»¥é€šè¿‡è¿™ä¸ªæ ‡å‡†çš„APIæ¥è°ƒç”¨ä¸åŒçš„ç½‘ç»œæ’ä»¶å®ç°ä¸åŒçš„ç½‘ç»œåŠŸèƒ½ã€‚\nkubeletå¯åŠ¨å‚æ•°--network-plugin=cniæ¥æŒ‡å®šCNIæ’ä»¶ï¼Œkubeletä»--cni-conf-dir ï¼ˆé»˜è®¤æ˜¯ /etc/cni/net.dï¼‰ è¯»å–æ–‡ä»¶å¹¶ä½¿ç”¨ è¯¥æ–‡ä»¶ä¸­çš„ CNI é…ç½®æ¥è®¾ç½®å„ä¸ª Pod çš„ç½‘ç»œã€‚ CNI é…ç½®æ–‡ä»¶å¿…é¡»ä¸ CNI è§„çº¦ åŒ¹é…ï¼Œå¹¶ä¸”é…ç½®æ‰€å¼•ç”¨çš„æ‰€æœ‰æ‰€éœ€çš„ CNI æ’ä»¶éƒ½åº”å­˜åœ¨äº --cni-bin-dirï¼ˆé»˜è®¤æ˜¯ /opt/cni/binï¼‰ä¸‹ã€‚å¦‚æœæœ‰å¤šä¸ªCNIé…ç½®æ–‡ä»¶ï¼Œkubelet å°†ä¼šä½¿ç”¨æŒ‰æ–‡ä»¶åçš„å­—å…¸é¡ºåºæ’åˆ— çš„ç¬¬ä¸€ä¸ªä½œä¸ºé…ç½®æ–‡ä»¶ã€‚\nCNIè§„èŒƒå®šä¹‰ï¼š\nç½‘ç»œé…ç½®æ–‡ä»¶çš„æ ¼å¼\nå®¹å™¨runtimeä¸CNIæ’ä»¶çš„é€šä¿¡åè®®\nåŸºäºæä¾›çš„é…ç½®æ‰§è¡Œç½‘ç»œæ’ä»¶çš„æ­¥éª¤\nç½‘ç»œæ’ä»¶è°ƒç”¨å…¶ä»–åŠŸèƒ½æ’ä»¶çš„æ­¥éª¤\næ’ä»¶è¿”å›ç»™runtimeç»“æœçš„æ•°æ®æ ¼å¼\n2. CNIé…ç½®æ–‡ä»¶æ ¼å¼ CNIé…ç½®æ–‡ä»¶çš„æ ¼å¼ä¸ºJSONæ ¼å¼ï¼Œé…ç½®æ–‡ä»¶çš„é»˜è®¤è·¯å¾„ï¼š/etc/cni/net.dã€‚æ’ä»¶äºŒè¿›åˆ¶é»˜è®¤çš„è·¯å¾„ä¸ºï¼š/opt/cni/binã€‚\n2.1. ä¸»é…ç½®çš„å­—æ®µ cniVersion (string)ï¼šCNIè§„èŒƒä½¿ç”¨çš„ç‰ˆæœ¬ï¼Œä¾‹å¦‚ç‰ˆæœ¬ä¸º0.4.0ã€‚\nname (string)ï¼šç›®æ ‡ç½‘ç»œçš„åç§°ã€‚\ndisableCheck (boolean)ï¼šå…³é—­CHECKæ“ä½œã€‚\nplugins (list)ï¼šCNIæ’ä»¶åˆ—è¡¨åŠæ’ä»¶é…ç½®ã€‚\n2.2. æ’ä»¶é…ç½®å­—æ®µ æ ¹æ®ä¸åŒçš„æ’ä»¶ï¼Œæ’ä»¶é…ç½®æ‰€éœ€çš„å­—æ®µä¸åŒã€‚\nå¿…é€‰å­—æ®µï¼š\ntype (string)ï¼šèŠ‚ç‚¹ä¸Šæ’ä»¶äºŒè¿›åˆ¶çš„åç§°ï¼Œæ¯”å¦‚bridgeï¼Œsriovï¼Œmacvlanç­‰ã€‚ å¯é€‰å­—æ®µï¼š\ncapabilities (dictionary)\nipMasq (boolean)ï¼šä¸ºç›®æ ‡ç½‘ç»œé…ä¸ŠOutbound Masquerade(åœ°å€ä¼ªè£…)ï¼Œå³ï¼šç”±å®¹å™¨å†…éƒ¨é€šè¿‡ç½‘å…³å‘å¤–å‘é€æ•°æ®åŒ…æ—¶ï¼Œå¯¹æ•°æ®åŒ…çš„æºIPåœ°å€è¿›è¡Œä¿®æ”¹ã€‚\nå½“æˆ‘ä»¬çš„å®¹å™¨ä»¥å®¿ä¸»æœºä½œä¸ºç½‘å…³æ—¶ï¼Œè¿™ä¸ªå‚æ•°æ˜¯å¿…é¡»è¦è®¾ç½®çš„ã€‚å¦åˆ™ï¼Œä»å®¹å™¨å†…éƒ¨å‘å‡ºçš„æ•°æ®åŒ…å°±æ²¡æœ‰åŠæ³•é€šè¿‡ç½‘å…³è·¯ç”±åˆ°å…¶ä»–ç½‘æ®µã€‚å› ä¸ºå®¹å™¨å†…éƒ¨çš„IPåœ°å€æ— æ³•è¢«ç›®æ ‡ç½‘æ®µè¯†åˆ«ï¼Œæ‰€ä»¥è¿™äº›æ•°æ®åŒ…æœ€ç»ˆä¼šè¢«ä¸¢å¼ƒæ‰ã€‚\nipam (dictionary)ï¼šIPAM(IP Adderss Management)å³IPåœ°å€ç®¡ç†ï¼Œæä¾›äº†ä¸€ç³»åˆ—æ–¹æ³•ç”¨äºå¯¹IPå’Œè·¯ç”±è¿›è¡Œç®¡ç†ã€‚å®ƒå¯¹åº”çš„æ˜¯ç”±CNIæä¾›çš„ä¸€ç»„æ ‡å‡†IPAMæ’ä»¶ï¼Œæ¯”å¦‚åƒhost-localï¼Œdhcpï¼Œstaticç­‰ã€‚æ¯”å¦‚æ–‡ä¸­ç”¨åˆ°çš„bridgeæ’ä»¶ï¼Œä¼šè°ƒç”¨æˆ‘ä»¬æ‰€æŒ‡å®šçš„IPAMæ’ä»¶ï¼Œå®ç°å¯¹ç½‘ç»œè®¾å¤‡IPåœ°å€çš„åˆ†é…å’Œç®¡ç†ã€‚**å¦‚æœæ˜¯è‡ªå·±å¼€å‘çš„ipamæ’ä»¶ï¼Œåˆ™ç›¸å…³çš„å…¥å‚å¯ä»¥è‡ªå·±å®šä¹‰å’Œå®ç°ã€‚\nä»¥ä¸‹ä»¥host-localä¸ºä¾‹è¯´æ˜ã€‚\ntypeï¼šæŒ‡å®šæ‰€ç”¨IPAMæ’ä»¶çš„åç§°ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­é‡Œï¼Œç”¨çš„æ˜¯host-localã€‚ subnetï¼šä¸ºç›®æ ‡ç½‘ç»œåˆ†é…ç½‘æ®µï¼ŒåŒ…æ‹¬ç½‘ç»œIDå’Œå­ç½‘æ©ç ï¼Œä»¥CIDRå½¢å¼æ ‡è®°ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­é‡Œä¸º10.15.10.0/24ï¼Œä¹Ÿå°±æ˜¯ç›®æ ‡ç½‘æ®µä¸º10.15.10.0ï¼Œå­ç½‘æ©ç ä¸º255.255.255.0ã€‚ routesï¼šç”¨äºæŒ‡å®šè·¯ç”±è§„åˆ™ï¼Œæ’ä»¶ä¼šä¸ºæˆ‘ä»¬åœ¨å®¹å™¨çš„è·¯ç”±è¡¨é‡Œç”Ÿæˆç›¸åº”çš„è§„åˆ™ã€‚å…¶ä¸­ï¼Œdstè¡¨ç¤ºå¸Œæœ›åˆ°è¾¾çš„ç›®æ ‡ç½‘æ®µï¼Œä»¥CIDRå½¢å¼æ ‡è®°ã€‚gwå¯¹åº”ç½‘å…³çš„IPåœ°å€ï¼Œä¹Ÿå°±æ˜¯è¦åˆ°è¾¾ç›®æ ‡ç½‘æ®µæ‰€è¦ç»è¿‡çš„â€œnext hop(ä¸‹ä¸€è·³)â€ã€‚å¦‚æœçœç•¥gwçš„è¯ï¼Œé‚£ä¹ˆæ’ä»¶ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬é€‰æ‹©é»˜è®¤ç½‘å…³ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­é‡Œï¼Œgwé€‰æ‹©çš„æ˜¯é»˜è®¤ç½‘å…³ï¼Œè€Œdstä¸º0.0.0.0/0åˆ™ä»£è¡¨â€œä»»ä½•ç½‘ç»œâ€ï¼Œè¡¨ç¤ºæ•°æ®åŒ…å°†é€šè¿‡é»˜è®¤ç½‘å…³å‘å¾€ä»»ä½•ç½‘ç»œã€‚å®é™…ä¸Šï¼Œè¿™å¯¹åº”çš„æ˜¯ä¸€æ¡é»˜è®¤è·¯ç”±è§„åˆ™ï¼Œå³ï¼šå½“æ‰€æœ‰å…¶ä»–è·¯ç”±è§„åˆ™éƒ½ä¸åŒ¹é…æ—¶ï¼Œå°†é€‰æ‹©è¯¥è·¯ç”±ã€‚ rangeStartï¼šå…è®¸åˆ†é…çš„IPåœ°å€èŒƒå›´çš„èµ·å§‹å€¼ rangeEndï¼šå…è®¸åˆ†é…çš„IPåœ°å€èŒƒå›´çš„ç»“æŸå€¼ gatewayï¼šä¸ºç½‘å…³ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å°†è¦åœ¨å®¿ä¸»æœºä¸Šåˆ›å»ºçš„bridgeï¼‰æŒ‡å®šçš„IPåœ°å€ã€‚å¦‚æœçœç•¥çš„è¯ï¼Œé‚£ä¹ˆæ’ä»¶ä¼šè‡ªåŠ¨ä»å…è®¸åˆ†é…çš„IPåœ°å€èŒƒå›´å†…é€‰æ‹©èµ·å§‹å€¼ä½œä¸ºç½‘å…³çš„IPåœ°å€ã€‚ dns (dictionary, optional)ï¼šdnsé…ç½®\nnameservers (list of strings, optional)\ndomain (string, optional)\nsearch (list of strings, optional)\noptions (list of strings, optional)\n2.3. é…ç½®æ–‡ä»¶ç¤ºä¾‹ $ mkdir -p /etc/cni/net.d $ cat \u003e/etc/cni/net.d/10-mynet.conf \u003c\u003cEOF { \"cniVersion\": \"1.0.0\", \"name\": \"dbnet\", \"plugins\": [ { \"type\": \"bridge\", // plugin specific parameters \"bridge\": \"cni0\", \"keyA\": [\"some more\", \"plugin specific\", \"configuration\"], \"ipam\": { \"type\": \"host-local\", // ipam specific \"subnet\": \"10.1.0.0/16\", \"gateway\": \"10.1.0.1\", \"routes\": [ {\"dst\": \"0.0.0.0/0\"} ] }, \"dns\": { \"nameservers\": [ \"10.1.0.1\" ] } }, { \"type\": \"tuning\", \"capabilities\": { \"mac\": true }, \"sysctl\": { \"net.core.somaxconn\": \"500\" } }, { \"type\": \"portmap\", \"capabilities\": {\"portMappings\": true} } ] } 3. CNIæ’ä»¶ 3.1. å®‰è£…æ’ä»¶ å®‰è£…CNIäºŒè¿›åˆ¶æ’ä»¶ï¼Œæ’ä»¶ä¸‹è½½åœ°ï¼šhttps://github.com/containernetworking/plugins/releases\n# ä¸‹è½½äºŒè¿›åˆ¶ wget https://github.com/containernetworking/plugins/releases/download/v1.1.0/cni-plugins-linux-amd64-v1.1.0.tgz # è§£å‹æ–‡ä»¶ tar -zvxf cni-plugins-linux-amd64-v1.1.0.tgz -C /opt/cni/bin/ # æŸ¥çœ‹è§£å‹æ–‡ä»¶ # ll -h æ€»ç”¨é‡ 63M -rwxr-xr-x 1 root root 3.7M 2æœˆ 24 01:01 bandwidth -rwxr-xr-x 1 root root 4.1M 2æœˆ 24 01:01 bridge -rwxr-xr-x 1 root root 9.3M 2æœˆ 24 01:01 dhcp -rwxr-xr-x 1 root root 4.2M 2æœˆ 24 01:01 firewall -rwxr-xr-x 1 root root 3.7M 2æœˆ 24 01:01 host-device -rwxr-xr-x 1 root root 3.1M 2æœˆ 24 01:01 host-local -rwxr-xr-x 1 root root 3.8M 2æœˆ 24 01:01 ipvlan -rwxr-xr-x 1 root root 3.2M 2æœˆ 24 01:01 loopback -rwxr-xr-x 1 root root 3.8M 2æœˆ 24 01:01 macvlan -rwxr-xr-x 1 root root 3.6M 2æœˆ 24 01:01 portmap -rwxr-xr-x 1 root root 4.0M 2æœˆ 24 01:01 ptp -rwxr-xr-x 1 root root 3.4M 2æœˆ 24 01:01 sbr -rwxr-xr-x 1 root root 2.7M 2æœˆ 24 01:01 static -rwxr-xr-x 1 root root 3.3M 2æœˆ 24 01:01 tuning -rwxr-xr-x 1 root root 3.8M 2æœˆ 24 01:01 vlan -rwxr-xr-x 1 root root 3.4M 2æœˆ 24 01:01 vrf 3.2. æ’ä»¶åˆ†ç±» å‚è€ƒï¼šhttps://www.cni.dev/plugins/current/\nåˆ†ç±» æ’ä»¶ è¯´æ˜ main bridge Creates a bridge, adds the host and the container to it ipvlan Adds an ipvlan interface in the container macvlan Creates a new MAC address, forwards all traffic to that to the container ptp Creates a veth pair host-device Moves an already-existing device into a container vlan Creates a vlan interface off a master IPAM dhcp Runs a daemon on the host to make DHCP requests on behalf of a container host-local Maintains a local database of allocated IPs static Allocates static IPv4/IPv6 addresses to containers meta tuning Changes sysctl parameters of an existing interface portmap An iptables-based portmapping plugin. Maps ports from the hostâ€™s address space to the container bandwidth Allows bandwidth-limiting through use of traffic control tbf (ingress/egress) sbr A plugin that configures source based routing for an interface (from which it is chained) firewall A firewall plugin which uses iptables or firewalld to add rules to allow traffic to/from the container 4. CNIæ’ä»¶æ¥å£ å…·ä½“å¯å‚è€ƒï¼šhttps://github.com/containernetworking/cni/blob/master/SPEC.md#cni-operations\nCNIå®šä¹‰çš„æ¥å£æ“ä½œæœ‰ï¼š\nADDï¼šæ·»åŠ å®¹å™¨ç½‘ç»œï¼Œåœ¨å®¹å™¨å¯åŠ¨æ—¶è°ƒç”¨ã€‚ DELï¼šåˆ é™¤å®¹å™¨ç½‘ç»œï¼Œåœ¨å®¹å™¨åˆ é™¤æ—¶è°ƒç”¨ã€‚ CHECKï¼šæ£€æŸ¥å®¹å™¨ç½‘ç»œæ˜¯å¦æ­£å¸¸ã€‚ VERSIONï¼šæ˜¾ç¤ºæ’ä»¶ç‰ˆæœ¬ã€‚ è¿™äº›æ“ä½œé€šè¿‡CNI_COMMANDç¯å¢ƒå˜é‡æ¥ä¼ é€’ç»™CNIæ’ä»¶äºŒè¿›åˆ¶ã€‚\nå…¶ä¸­ç¯å¢ƒå˜é‡åŒ…æ‹¬ï¼š\nCNI_COMMANDï¼šå‘½ä»¤æ“ä½œï¼ŒåŒ…æ‹¬ ADD, DEL, CHECK, or VERSIONã€‚\nCNI_CONTAINERID:å®¹å™¨çš„IDï¼Œæœ‰runtimeåˆ†é…ï¼Œä¸ä¸ºç©ºã€‚\nCNI_NETNS:å®¹å™¨çš„ç½‘ç»œå‘½åç©ºé—´ï¼Œå‘½åç©ºé—´è·¯å¾„ï¼Œä¾‹å¦‚ï¼š/run/netns/[nsname]\nCNI_IFNAME:å®¹å™¨å†…çš„ç½‘å¡åç§°ã€‚\nCNI_ARGS:å…¶ä»–å‚æ•°ã€‚\nCNI_PATH:CNIæ’ä»¶äºŒè¿›åˆ¶çš„è·¯å¾„ã€‚\n4.1. ADDæ¥å£ï¼šæ·»åŠ å®¹å™¨ç½‘ç»œ åœ¨å®¹å™¨çš„ç½‘ç»œå‘½åç©ºé—´CNI_NETNSä¸­åˆ›å»ºCNI_IFNAMEç½‘å¡è®¾å¤‡ï¼Œæˆ–è€…è°ƒæ•´ç½‘å¡é…ç½®ã€‚\nå¿…é€‰å‚æ•°ï¼š\nCNI_COMMAND CNI_CONTAINERID CNI_NETNS CNI_IFNAME å¯é€‰å‚æ•°ï¼š\nCNI_ARGS CNI_PATH 4.2. DELæ¥å£ï¼šåˆ é™¤å®¹å™¨ç½‘ç»œ åˆ é™¤å®¹å™¨ç½‘ç»œå‘½åç©ºé—´CNI_NETNSä¸­çš„å®¹å™¨ç½‘å¡CNI_IFNAMEï¼Œæˆ–è€…æ’¤é”€ADDä¿®æ”¹æ“ä½œã€‚\nå¿…é€‰å‚æ•°ï¼š\nCNI_COMMAND CNI_CONTAINERID CNI_IFNAME å¯é€‰å‚æ•°ï¼š\nCNI_NETNS CNI_ARGS CNI_PATH 4.3. CHECKæ¥å£ï¼šæ£€æŸ¥å®¹å™¨ç½‘ç»œ å¿…é€‰å‚æ•°ï¼š\nCNI_COMMAND CNI_CONTAINERID CNI_NETNS CNI_IFNAME å¯é€‰å‚æ•°ï¼š\nCNI_ARGS CNI_PATH 4.4. VERSIONæ¥å£ï¼šè¾“å‡ºCNIçš„ç‰ˆæœ¬ å‚è€ƒï¼š\nhttps://www.cni.dev/docs/spec/ https://github.com/containernetworking/cni https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ https://github.com/containernetworking/plugins/tree/master/plugins https://www.cni.dev/plugins/current/ https://cloud.tencent.com/developer/news/600713 é…ç½®CNIæ’ä»¶ ","categories":"","description":"","excerpt":"1. CNIï¼ˆContainer Network Interfaceï¼‰ CNIï¼ˆContainer Network Interfaceï¼‰å³å®¹ â€¦","ref":"/kubernetes-notes/network/cni/cni/","tags":["CNI"],"title":"CNIæ¥å£ä»‹ç»"},{"body":"k8så¤šé›†ç¾¤çš„æ€è€ƒ 1. ä¸ºä»€ä¹ˆéœ€è¦å¤šé›†ç¾¤ 1ã€k8så•é›†ç¾¤çš„æ‰¿è½½èƒ½åŠ›æœ‰é™ã€‚\nKubernetes v1.21 æ”¯æŒçš„æœ€å¤§èŠ‚ç‚¹æ•°ä¸º 5000ã€‚ æ›´å…·ä½“åœ°è¯´ï¼ŒKubernetesæ—¨åœ¨é€‚åº”æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ ‡å‡†çš„é…ç½®ï¼š\næ¯ä¸ªèŠ‚ç‚¹çš„ Pod æ•°é‡ä¸è¶…è¿‡ 100 èŠ‚ç‚¹æ•°ä¸è¶…è¿‡ 5000 Pod æ€»æ•°ä¸è¶…è¿‡ 150000 å®¹å™¨æ€»æ•°ä¸è¶…è¿‡ 300000 å‚è€ƒï¼šhttps://kubernetes.io/zh/docs/setup/best-practices/cluster-large/\nä¸”å½“èŠ‚ç‚¹æ•°é‡è¾ƒå¤§æ—¶ï¼Œä¼šå‡ºç°è°ƒåº¦å»¶è¿Ÿï¼Œetcdè¯»å†™å»¶è¿Ÿï¼Œapiserverè´Ÿè½½é«˜ç­‰é—®é¢˜ï¼Œå½±å“æœåŠ¡çš„æ­£å¸¸åˆ›å»ºã€‚\n2ã€åˆ†æ•£é›†ç¾¤æœåŠ¡é£é™©ã€‚\nå…¨éƒ¨æœåŠ¡éƒ½æ”¾åœ¨ä¸€ä¸ªk8sé›†ç¾¤ä¸­ï¼Œå½“è¯¥é›†ç¾¤å‡ºç°å¼‚å¸¸ï¼ŒçŸ­æœŸæ— æ³•æ¢å¤çš„æƒ…å†µä¸‹ï¼Œåˆ™å½±å“å…¨éƒ¨æœåŠ¡å’Œå½±å“éƒ¨ç½²ã€‚ä¸ºäº†é¿å…æœºæˆ¿ç­‰æ•…éšœå¯¼è‡´å•é›†ç¾¤å¼‚å¸¸ï¼Œå»ºè®®å°†k8sçš„masteråœ¨åˆ†æ•£åœ¨å»¶è¿Ÿè¾ƒä½çš„ä¸åŒå¯ç”¨åŒºéƒ¨ç½²ï¼Œä¸”åœ¨ä¸åŒregionéƒ¨ç½²å¤šä¸ªk8sé›†ç¾¤æ¥è¿›è¡Œé›†ç¾¤çº§åˆ«çš„å®¹ç¾ã€‚\n3ã€å½“å‰æ··åˆäº‘çš„ä½¿ç”¨æ–¹å¼å’Œæ¶æ„\nå½“å‰éƒ¨åˆ†å…¬å¸ä¼šå­˜åœ¨è‡ªå»ºæœºæˆ¿+ä¸åŒäº‘å‚å•†çš„å…¬æœ‰äº‘ä»è€Œæ¥å®ç°æ··éƒ¨äº‘çš„è¿è¥æ¨¡å¼ï¼Œé‚£ä¹ˆè‡ªç„¶ä¼šå¼•å…¥å¤šé›†ç¾¤ç®¡ç†çš„é—®é¢˜ã€‚\n2. å¤šé›†ç¾¤éƒ¨ç½²éœ€è¦è§£å†³å“ªäº›é—®é¢˜ ç›®æ ‡ï¼šè®©ç”¨æˆ·åƒä½¿ç”¨å•é›†ç¾¤ä¸€æ ·æ¥ä½¿ç”¨å¤šé›†ç¾¤ã€‚\næ‰©å±•é›†ç¾¤çš„è¾¹ç•Œï¼ŒæœåŠ¡çš„è¾¹ç•Œä»å•å°ç‰©ç†æœºå¤šä¸ªè¿›ç¨‹ï¼Œå‘å±•åˆ°é€šè¿‡k8sé›†ç¾¤æ¥ç®¡ç†å¤šå°çš„ç‰©ç†æœºï¼Œå†å‘å±•åˆ°ç®¡ç†å¤šä¸ªçš„k8sé›†ç¾¤ã€‚æœåŠ¡çš„è¾¹ç•Œä»ç‰©ç†æœºå‘å±•åˆ°é›†ç¾¤ã€‚\nè€Œå¤šé›†ç¾¤ç®¡ç†éœ€è¦è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š\nå¤šé›†ç¾¤æœåŠ¡çš„åˆ†å‘éƒ¨ç½²ï¼ˆdeploymentã€daemonsetç­‰ï¼‰ è·¨é›†ç¾¤è‡ªåŠ¨è¿ç§»ä¸è°ƒåº¦ï¼ˆå½“æŸä¸ªé›†ç¾¤å¼‚å¸¸ï¼ŒæœåŠ¡å¯ä»¥åœ¨å…¶ä»–é›†ç¾¤è‡ªåŠ¨éƒ¨ç½²ï¼‰ å¤šé›†ç¾¤æœåŠ¡å‘ç°ï¼Œç½‘ç»œé€šä¿¡åŠè´Ÿè½½å‡è¡¡ï¼ˆserviceï¼Œingressç­‰ï¼‰ è€Œå¤šé›†ç¾¤æœåŠ¡çš„ç½‘ç»œé€šä¿¡å¯ä»¥ç”±Service meshç­‰æ¥è§£å†³ï¼Œæœ¬æ–‡ä¸åšé‡ç‚¹è®¨è®ºã€‚\nä»¥ä¸Šå‡ ä¸ªé—®é¢˜ï¼Œå¯ä»¥å…ˆä»k8sç®¡ç†èŠ‚ç‚¹çš„æ€ç»´è¿›è¡Œåˆ†æ\nç‰©ç†æœºè§†è§’ å•é›†ç¾¤è§†è§’ å¤šé›†ç¾¤è§†è§’ è¿›ç¨‹çš„è¾¹ç•Œ ç‰©ç†æœº k8sé›†ç¾¤ å¤šé›†ç¾¤ è°ƒåº¦å•å…ƒ è¿›ç¨‹æˆ–çº¿ç¨‹ å®¹å™¨æˆ–pod å·¥ä½œè´Ÿè½½ï¼ˆdeploymentï¼‰ æœåŠ¡çš„é›†åˆ å·¥ä½œè´Ÿè½½ï¼ˆdeploymentï¼‰ ä¸åŒé›†ç¾¤å·¥ä½œè´Ÿè½½çš„é›†åˆä½“ï¼ˆworkloadGroupï¼‰ æœåŠ¡å‘ç° service ä¸åŒé›†ç¾¤serviceçš„é›†åˆä½“ æœåŠ¡è¿ç§» å·¥ä½œè´Ÿè½½ï¼ˆdeploymentï¼‰æ§åˆ¶å™¨ ä¸åŒé›†ç¾¤å·¥ä½œè´Ÿè½½çš„é›†åˆä½“æ§åˆ¶å™¨ æœåŠ¡è°ƒåº¦ nodenameæˆ–è€…node selector clusternameæˆ–cluster selector podçš„åäº²å’Œï¼ˆç›¸åŒdeploymentä¸‹çš„podä¸è°ƒåº¦åœ¨ç›¸åŒèŠ‚ç‚¹ï¼‰ workloadåäº²å’Œï¼ˆç›¸åŒworkloadGroupåˆ†æ•£åœ¨ä¸åŒé›†ç¾¤ï¼‰ 2.1. å¤šé›†ç¾¤å·¥ä½œè´Ÿè½½çš„åˆ†å‘ å•é›†ç¾¤ä¸­k8sçš„è°ƒåº¦å•å…ƒæ˜¯podï¼Œå³ä¸€ä¸ªpodåªèƒ½è·‘åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œä¸€ä¸ªèŠ‚ç‚¹å¯ä»¥è¿è¡Œå¤šä¸ªpodï¼Œè€Œä¸åŒèŠ‚ç‚¹ä¸Šçš„ä¸€ç»„podæ˜¯é€šè¿‡ä¸€ä¸ªworkloadæ¥æ§åˆ¶å’Œåˆ†å‘ã€‚ç±»ä¼¼è¿™ä¸ªé€»è¾‘ï¼Œé‚£ä¹ˆåœ¨å¤šé›†ç¾¤çš„è§†è§’ä¸‹ï¼Œå¤šé›†ç¾¤çš„è°ƒåº¦å•å…ƒæ˜¯ä¸€ä¸ªé›†ç¾¤çš„workloadï¼Œä¸€ä¸ªworkloadåªèƒ½è·‘åœ¨ä¸€ä¸ªé›†ç¾¤ä¸­ï¼Œä¸€ä¸ªé›†ç¾¤å¯ä»¥è¿è¡Œå¤šä¸ªworkloadã€‚\né‚£ä¹ˆå°±éœ€è¦æœ‰ä¸€ä¸ªæ§åˆ¶å™¨æ¥ç®¡ç†ä¸åŒk8sé›†ç¾¤çš„ç›¸åŒworkloadã€‚ä¾‹å¦‚ workloadGroupã€‚è€Œè¯¥workloadGroupåœ¨ä¸ä¾µå…¥k8såŸç”ŸAPIçš„æƒ…å†µä¸‹ï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ã€‚\nworkloadGroup:\nèµ„æºæ¨¡æ¿ï¼ˆResource Templateï¼‰ï¼šæœåŠ¡çš„æè¿°ï¼ˆworkloadï¼‰ åˆ†å‘ç­–ç•¥ï¼ˆPropagaion Policyï¼‰ï¼šæœåŠ¡åˆ†å‘çš„é›†ç¾¤ï¼ˆå³å¤šä¸ªworkloadåº”è¯¥è¢«åˆ†å‘åˆ°å“ªäº›é›†ç¾¤è¿è¡Œï¼‰ workloadæè¿°çš„æ˜¯ä»€ä¹ˆæœåŠ¡è¿è¡Œåœ¨ä»€ä¹ˆèŠ‚ç‚¹ï¼ŒworkloadGroupæè¿°çš„æ˜¯ä»€ä¹ˆæœåŠ¡è¿è¡Œåœ¨ä»€ä¹ˆé›†ç¾¤ã€‚\nå®ç°workloadGroupæœ‰ä¸¤ç§æ–¹å¼ï¼š\nä¸€ç§æ˜¯è‡ªå®šä¹‰APIå°†workloadGroupä¸­çš„Resource Templateå’ŒPropagaion Policyåˆæˆåœ¨ä¸€ä¸ªè‡ªå®šä¹‰çš„å¯¹è±¡ä¸­ï¼Œç”±ç”¨æˆ·ç›´æ¥æŒ‡å®šè¯¥workloadGroupä¿¡æ¯ï¼Œä»è€Œå°†ä¸åŒçš„workloadåˆ†å‘åˆ°ä¸åŒçš„é›†ç¾¤ä¸­ã€‚ å¦ä¸€ç§æ–¹å¼æ˜¯é€šè¿‡ä¸€ä¸ªk8sè½½ä½“æ¥è®°å½•ä¸€ä¸ªå…·ä½“çš„workloadå¯¹è±¡ï¼Œå†ç”±ç”¨æˆ·æŒ‡å®šPropagaion Policyå…³è”è¯¥workloadå¯¹è±¡ï¼Œä»è€Œè®©æ§åˆ¶å™¨è‡ªåŠ¨æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„Propagaion Policyå°†workloadåˆ†å‘åˆ°ä¸åŒçš„é›†ç¾¤ä¸­ã€‚ 2.2. è·¨é›†ç¾¤è‡ªåŠ¨è¿ç§»ä¸è°ƒåº¦ å•é›†ç¾¤ä¸­k8sä¸­é€šè¿‡workloadä¸­çš„nodeselectoræˆ–è€…nodenameä»¥åŠäº²å’Œæ€§æ¥æ§åˆ¶podè¿è¡Œåœ¨å“ªä¸ªèŠ‚ç‚¹ä¸Šã€‚è€Œå¤šé›†ç¾¤çš„è§†è§’ä¸‹ï¼Œåˆ™éœ€è¦æœ‰ä¸€ä¸ªæ§åˆ¶å™¨æ¥å®ç°é›†ç¾¤çº§åˆ«çš„è°ƒåº¦é€»è¾‘ï¼Œä¾‹å¦‚clusternameï¼Œcluster selectorï¼Œcluster AntiAffinityï¼Œä»è€Œæ¥è‡ªåŠ¨æ§åˆ¶workloadGroupä¸‹çš„workloadåˆ†æ•£åœ¨ä»€ä¹ˆé›†ç¾¤ä¸Šã€‚\n3. ç›®å‰çš„å¤šé›†ç¾¤æ–¹æ¡ˆ 3.1. Kubefed[Federation v2] ç®€ä»‹\nåŸºæœ¬æ€æƒ³\n3.2. virtual kubelet ç®€ä»‹\nåŸºæœ¬æ€æƒ³\n3.3. Karmada ç®€ä»‹\nåŸºæœ¬æ€æƒ³\nå‚è€ƒï¼š\nhttps://kubernetes.io/zh/docs/setup/best-practices/cluster-large/ CoreOS æ˜¯å¦‚ä½•å°† Kubernetes çš„æ€§èƒ½æé«˜ 10 å€çš„? å½“ K8s é›†ç¾¤è¾¾åˆ°ä¸‡çº§è§„æ¨¡ï¼Œé˜¿é‡Œå·´å·´å¦‚ä½•è§£å†³ç³»ç»Ÿå„ç»„ä»¶æ€§èƒ½é—®é¢˜ï¼Ÿ https://github.com/kubernetes-sigs/kubefed https://jimmysong.io/kubernetes-handbook/practice/federation.html https://kubernetes.io/blog/2018/12/12/kubernetes-federation-evolution/ https://zhuanlan.zhihu.com/p/355193315 ","categories":"","description":"","excerpt":"k8så¤šé›†ç¾¤çš„æ€è€ƒ 1. ä¸ºä»€ä¹ˆéœ€è¦å¤šé›†ç¾¤ 1ã€k8så•é›†ç¾¤çš„æ‰¿è½½èƒ½åŠ›æœ‰é™ã€‚\nKubernetes v1.21 â€¦","ref":"/kubernetes-notes/multi-cluster/k8s-multi-cluster-thinking/","tags":["å¤šé›†ç¾¤"],"title":"k8så¤šé›†ç¾¤ç®¡ç†çš„æ€è€ƒ"},{"body":"1. Linuxç®€ä»‹ ä¸¥æ ¼æ¥è®²ï¼ŒLinuxï¼ˆå†…æ ¸ï¼‰æ˜¯è®¡ç®—æœºè½¯ä»¶ä¸ç¡¬ä»¶é€šä¿¡ä¹‹é—´çš„å¹³å°ï¼Œä¸æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„æ“ä½œç³»ç»Ÿï¼Œè€Œä¸€äº›å‚å®¶å°†Linuxå†…æ ¸å’ŒGNUè½¯ä»¶ï¼ˆç³»ç»Ÿè½¯ä»¶å’Œå·¥å…·ï¼‰æ•´åˆèµ·æ¥ï¼Œå¹¶æä¾›ä¸€äº›å®‰è£…ç•Œé¢å’Œç³»ç»Ÿè®¾å®šä¸ç®¡ç†å·¥å…·ï¼Œå°±æ„æˆä¸€äº›å‘è¡Œå¥—ä»¶ï¼ˆç³»ç»Ÿï¼‰ï¼Œä¾‹å¦‚ï¼šUbuntuã€CentOSã€Red Hatã€Debianç­‰ã€‚\nLinuxå†…æ ¸ç‰ˆæœ¬\nLinuxå†…æ ¸ç‰ˆæœ¬ä¸€èˆ¬æ ¼å¼ä¸ºï¼šx.y.zz-www ï¼Œä¾‹å¦‚ï¼šKernel2.6.15\nx.yï¼šLinuxå†…æ ¸ä¸»ç‰ˆæœ¬å·ï¼Œyè‹¥ä¸ºå¥‡æ•°åˆ™è¡¨ç¤ºæ˜¯æµ‹è¯•ç‰ˆ zzï¼šæ¬¡ç‰ˆæœ¬å¥½ wwwï¼šä»£è¡¨å‘è¡Œå· 2. Linuxä½“ç³»ç»“æ„ Linuxä½“ç³»ç»“æ„å¦‚ä¸‹ï¼š\nå‡ ä¸ªé‡è¦æ¦‚å¿µï¼š\nå†…æ ¸ï¼šå†…æ ¸æ˜¯æ“ä½œç³»ç»Ÿçš„æ ¸å¿ƒã€‚å†…æ ¸ç›´æ¥ä¸ç¡¬ä»¶äº¤äº’ï¼Œå¹¶å¤„ç†å¤§éƒ¨åˆ†è¾ƒä½å±‚çš„ä»»åŠ¡ï¼Œå¦‚å†…å­˜ç®¡ç†ã€è¿›ç¨‹è°ƒåº¦ã€æ–‡ä»¶ç®¡ç†ç­‰ã€‚ Shellï¼šShellæ˜¯ä¸€ä¸ªå¤„ç†ç”¨æˆ·è¯·æ±‚çš„å·¥å…·ï¼Œå®ƒè´Ÿè´£è§£é‡Šç”¨æˆ·è¾“å…¥çš„å‘½ä»¤ï¼Œè°ƒç”¨ç”¨æˆ·å¸Œæœ›ä½¿ç”¨çš„ç¨‹åºã€‚ å‘½ä»¤å’Œå·¥å…·ï¼šæ—¥å¸¸å·¥ä½œä¸­ï¼Œä½ ä¼šç”¨åˆ°å¾ˆå¤šç³»ç»Ÿå‘½ä»¤å’Œå·¥å…·ï¼Œå¦‚cpã€mvã€catå’Œgrepç­‰ã€‚ æ–‡ä»¶å’Œç›®å½•ï¼šLinuxç³»ç»Ÿä¸­æ‰€æœ‰çš„æ•°æ®éƒ½è¢«å­˜å‚¨åˆ°æ–‡ä»¶ä¸­ï¼Œè¿™äº›æ–‡ä»¶è¢«åˆ†é…åˆ°å„ä¸ªç›®å½•ï¼Œæ„æˆæ–‡ä»¶ç³»ç»Ÿã€‚ 3. ç³»ç»Ÿæ“ä½œ 3.1. ç™»å½•Linux ç™»å½•éœ€è¦è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ï¼Œç”¨æˆ·åå’Œå¯†ç æ˜¯åŒºåˆ†å¤§å°å†™ã€‚\nlogin : amrood amrood's password: Last login: Sun Jun 14 09:32:32 2009 from 62.61.164.73 $ 3.2. ä¿®æ”¹å¯†ç  è¾“å…¥passwordå‘½ä»¤åï¼Œè¾“å…¥åŸå¯†ç å’Œæ–°å¯†ç ï¼Œç¡®è®¤å¯†ç å³å¯ã€‚\n$ passwd Changing password for amrood (current) Linux password:****** New Linux password:******* Retype new Linux password:******* passwd: all authentication tokens updated successfully 3.3. æŸ¥çœ‹å½“å‰ç”¨æˆ· 1ã€æŸ¥çœ‹è‡ªå·±çš„ç”¨æˆ·å\n$ whoami amrood 2ã€æŸ¥çœ‹å½“å‰åœ¨çº¿ç”¨æˆ·\nå¯ä»¥ä½¿ç”¨users ã€whoã€wå‘½ä»¤ã€‚\n$ users amrood bablu qadir $ who amrood ttyp0 Oct 8 14:10 (limbo) bablu ttyp2 Oct 4 09:08 (calliope) qadir ttyp4 Oct 8 12:09 (dent) $ w 13:58:53 up 158 days, 22:07, 3 users, load average: 0.72, 0.99, 1.11 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/1 172.16.20.65 13:40 0.00s 0.22s 0.02s w root pts/2 172.16.20.65 Fri15 43:17m 1.04s 1.04s -bash 3.4. å…³é—­ç³»ç»Ÿ å…³é—­ç³»ç»Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤\nå‘½ä»¤ è¯´æ˜ halt ç›´æ¥å…³é—­ç³»ç»Ÿ init 0 ä½¿ç”¨é¢„å…ˆå®šä¹‰çš„è„šæœ¬å…³é—­ç³»ç»Ÿï¼Œå…³é—­å‰å¯ä»¥æ¸…ç†å’Œæ›´æ–°æœ‰å…³ä¿¡æ¯ init 6 é‡æ–°å¯åŠ¨ç³»ç»Ÿ poweroff é€šè¿‡æ–­ç”µæ¥å…³é—­ç³»ç»Ÿ reboot é‡æ–°å¯åŠ¨ç³»ç»Ÿ shutdown å®‰å…¨å…³é—­ç³»ç»Ÿ ä¸€èˆ¬åªæœ‰rootæœ‰å…³é—­ç³»ç»Ÿçš„æƒé™ï¼Œæ™®é€šç”¨æˆ·è¢«èµ‹äºˆç›¸åº”æƒé™ä¹Ÿå¯ä»¥å…³é—­ç³»ç»Ÿã€‚\n","categories":"","description":"","excerpt":"1. Linuxç®€ä»‹ ä¸¥æ ¼æ¥è®²ï¼ŒLinuxï¼ˆå†…æ ¸ï¼‰æ˜¯è®¡ç®—æœºè½¯ä»¶ä¸ç¡¬ä»¶é€šä¿¡ä¹‹é—´çš„å¹³å°ï¼Œä¸æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„æ“ä½œç³»ç»Ÿï¼Œè€Œä¸€äº›å‚å®¶å°†Linuxå†…æ ¸ â€¦","ref":"/linux-notes/file/linux-introduction/","tags":["Linux"],"title":"Linuxä»‹ç»"},{"body":"1. åˆ¤æ–­ç£ç›˜æ˜¯SSDæˆ–HDDç›˜ 1ã€æ²¡æœ‰ä½¿ç”¨raidæ–¹æ¡ˆ\nlsblk -d -o name,rotaå‘½ä»¤ï¼Œ0è¡¨ç¤ºSSDï¼Œ1è¡¨ç¤ºHDD\n# lsblk -d -o name,rota NAME ROTA sda 0 sdb 1 sdc 1 2ã€ä½¿ç”¨raidæ–¹æ¡ˆ\nä¸‹è½½å·¥å…·\nwget https://raw.githubusercontent.com/eLvErDe/hwraid/master/wrapper-scripts/megaclisas-status æ‰§è¡Œæ£€æµ‹å‘½ä»¤\n$ megaclisas-status -- Controller information -- -- ID | H/W Model | RAM | Temp | BBU | Firmware c0 | SAS3508 | 2048MB | 55C | Good | FW: 50.6.3-0109 -- Array information -- -- ID | Type | Size | Strpsz | Flags | DskCache | Status | OS Path | CacheCade |InProgress c0u0 | RAID-1 | 1089G | 256 KB | RA,WB | Default | Optimal | /dev/sda | None |None c0u1 | RAID-5 | 2616G | 256 KB | RA,WB | Default | Optimal | /dev/sdb | None |None -- Disk information -- -- ID | Type | Drive Model | Size | Status | Speed | Temp | Slot ID | LSI ID c0u0p0 | HDD | TOSHIBA AL15SEB120N 080710R0A0LJFDWG | 1.089 TB | Online, Spun Up | 12.0Gb/s | 27C | [134:4] | 0 c0u0p1 | HDD | TOSHIBA AL15SEB120N 080710S0A10SFDWG | 1.089 TB | Online, Spun Up | 12.0Gb/s | 28C | [134:5] | 5 c0u1p0 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002816 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:0] | 2 c0u1p1 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002799 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 30C | [134:1] | 4 c0u1p2 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002805 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:2] | 1 c0u1p3 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002797 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:3] | 3 2. è§£å†³umount target is busyæŒ‚è½½ç›˜å¸è½½ä¸æ‰é—®é¢˜ é—®é¢˜æè¿°:\nç”±äºæœ‰è¿›ç¨‹å ç”¨ç›®å½•ï¼Œå› æ­¤æ— æ³•umountç›®å½•ï¼Œéœ€è¦å…ˆå°†å ç”¨è¿›ç¨‹æ€æ­»ï¼Œå†umountç›®å½•ã€‚\n$ umount /data umount: /data: target is busy. æŸ¥çœ‹ç›®å½•å ç”¨è¿›ç¨‹ï¼š\n# fuser -mv /mnt/ USER PID ACCESS COMMAND /mnt: root kernel mount /mnt root 13830 ..c.. bash æ€æ­»ç›®å½•å ç”¨è¿›ç¨‹\n# fuser -kv /mnt/ USER PID ACCESS COMMAND /mnt: root kernel mount /mnt root 13830 ..c.. bash # æ£€æŸ¥ç›®å½•å ç”¨è¿›ç¨‹ # fuser -mv /mnt/ # umount /mnt fuserå‘½ä»¤å‚æ•°è¯´æ˜\n-k,--kill kill processes accessing the named file -m,--mount show all processes using the named filesystems or block device -v,--verbose verbose output 3. åˆ é™¤å¤§æ–‡ä»¶åç£ç›˜ç©ºé—´æ²¡å‡å°‘ é—®é¢˜æè¿°ï¼š\nä¸ºäº†æ¸…ç†ç£ç›˜ç©ºé—´ï¼Œåœ¨åˆ é™¤æŸäº›å¤§æ–‡ä»¶åï¼ŒæŸ¥çœ‹df -hå‘ç°ç£ç›˜çš„ç©ºé—´å¹¶æ²¡å‡å°‘ã€‚\nåŸå› ï¼š\næœ‰è¿›ç¨‹å ç”¨äº†è¢«åˆ é™¤çš„æ–‡ä»¶ï¼Œå¯¼è‡´ç©ºé—´æ²¡æœ‰æ¸…ç†ã€‚\nè§£å†³æ–¹æ¡ˆ:\næŸ¥çœ‹æ‰€å ç”¨æ–‡ä»¶çš„è¿›ç¨‹ï¼Œå¹¶æ€æ‰è¯¥è¿›ç¨‹ã€‚\n# lsof æŸ¥çœ‹åˆ é™¤æ–‡ä»¶å¯¹åº”çš„è¿›ç¨‹ lsof | grep deleted|grep \"/data/xxxx\" # æ€æ‰è¿›ç¨‹ kill -9 {pid} # æŸ¥çœ‹ç©ºé—´,æ­¤æ—¶ç©ºé—´å·²ç»è…¾å‡ºæ¥ df -h ","categories":"","description":"","excerpt":"1. åˆ¤æ–­ç£ç›˜æ˜¯SSDæˆ–HDDç›˜ 1ã€æ²¡æœ‰ä½¿ç”¨raidæ–¹æ¡ˆ\nlsblk -d -o name,rotaå‘½ä»¤ï¼Œ0è¡¨ç¤ºSSDï¼Œ1è¡¨ç¤ºHDD\n# â€¦","ref":"/linux-notes/disk/disk-command/","tags":["disk"],"title":"ç£ç›˜å‘½ä»¤"},{"body":"1. Cobraç®€ä»‹ Cobraæ˜¯ä¸€ä¸ªcliæ¥å£æ¨¡å¼çš„åº”ç”¨ç¨‹åºæ¡†æ¶ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç”Ÿæˆè¯¥æ¡†æ¶çš„å‘½ä»¤è¡Œå·¥å…·ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡helpæ–¹å¼å¿«é€ŸæŸ¥çœ‹è¯¥äºŒè¿›åˆ¶çš„ä½¿ç”¨æ–¹å¼ã€‚\nCobraä¸»è¦åŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†\nCommand:ä¸€èˆ¬è¡¨ç¤ºactionï¼Œå³è¿è¡Œçš„äºŒè¿›åˆ¶å‘½ä»¤æœåŠ¡ã€‚åŒæ—¶å¯ä»¥æ‹¥æœ‰å­å‘½ä»¤ï¼ˆchildren commandsï¼‰ã€‚ Args:å‘½ä»¤æ‰§è¡Œç›¸å…³å‚æ•°ã€‚ Flags:äºŒè¿›åˆ¶å‘½ä»¤çš„é…ç½®å‚æ•°ï¼Œå¯å¯¹åº”é…ç½®æ–‡ä»¶ã€‚å‚æ•°å¯åˆ†ä¸ºå…¨å±€å‚æ•°å’Œå­å‘½ä»¤å‚æ•°ã€‚å‚è€ƒï¼špflag libraryã€‚ 2. å®‰è£… é€šè¿‡ä»¥ä¸‹æ“ä½œå¯ä»¥åœ¨$GOPATH/binå®‰è£…cobraçš„äºŒè¿›åˆ¶å‘½ä»¤ã€‚\ngo get -u github.com/spf13/cobra/cobra 3. ä½¿ç”¨ cobraå‘½ä»¤è¡Œå¸®åŠ©ä¿¡æ¯å¦‚ä¸‹ï¼š\n# cobra Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobra [command] Available Commands: add Add a command to a Cobra Application help Help about any command init Initialize a Cobra Application Flags: -a, --author string author name for copyright attribution (default \"YOUR NAME\") --config string config file (default is $HOME/.cobra.yaml) -h, --help help for cobra -l, --license string name of license for the project --viper use Viper for configuration (default true) Use \"cobra [command] --help\" for more information about a command. 3.1. cobra init # cobra init --help Initialize (cobra init) will create a new application, with a license and the appropriate structure for a Cobra-based CLI application. * If a name is provided, it will be created in the current directory; * If no name is provided, the current directory will be assumed; * If a relative path is provided, it will be created inside $GOPATH (e.g. github.com/spf13/hugo); * If an absolute path is provided, it will be created; * If the directory already exists but is empty, it will be used. Init will not use an existing directory with contents. Usage: cobra init [name] [flags] Aliases: init, initialize, initialise, create Flags: -h, --help help for init --pkg-name string fully qualified pkg name Global Flags: -a, --author string author name for copyright attribution (default \"YOUR NAME\") --config string config file (default is $HOME/.cobra.yaml) -l, --license string name of license for the project --viper use Viper for configuration (default true) 3.2. cobra add # cobra add --help Add (cobra add) will create a new command, with a license and the appropriate structure for a Cobra-based CLI application, and register it to its parent (default rootCmd). If you want your command to be public, pass in the command name with an initial uppercase letter. Example: cobra add server -\u003e resulting in a new cmd/server.go Usage: cobra add [command name] [flags] Aliases: add, command Flags: -h, --help help for add -p, --parent string variable name of parent command for this command (default \"rootCmd\") Global Flags: -a, --author string author name for copyright attribution (default \"YOUR NAME\") --config string config file (default is $HOME/.cobra.yaml) -l, --license string name of license for the project --viper use Viper for configuration (default true) å‚è€ƒï¼š\nhttps://github.com/spf13/cobra https://github.com/spf13/cobra/blob/master/cobra/README.md ","categories":"","description":"","excerpt":"1. Cobraç®€ä»‹ Cobraæ˜¯ä¸€ä¸ªcliæ¥å£æ¨¡å¼çš„åº”ç”¨ç¨‹åºæ¡†æ¶ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç”Ÿæˆè¯¥æ¡†æ¶çš„å‘½ä»¤è¡Œå·¥å…·ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡helpæ–¹å¼å¿«é€ŸæŸ¥çœ‹è¯¥äºŒè¿›åˆ¶ â€¦","ref":"/golang-notes/framework/cobra/cobra-usage/","tags":["Golang"],"title":"cobra ä»‹ç»"},{"body":" æœ¬æ–‡ä¸»è¦ä»‹ç»Goçš„è°ƒåº¦æ¨¡å‹ã€‚\n1. çº¿ç¨‹å®ç°æ¨¡å‹ çº¿ç¨‹æ¨¡å‹æœ‰ä¸‰ç±»ï¼šå†…æ ¸çº§çº¿ç¨‹æ¨¡å‹ã€ç”¨æˆ·çº§çº¿ç¨‹æ¨¡å‹ã€æ··åˆå‹çº¿ç¨‹æ¨¡å‹ã€‚ä¸‰è€…çš„åŒºåˆ«ä¸»è¦åœ¨äºçº¿ç¨‹ä¸å†…æ ¸è°ƒåº¦å®ä½“KSE(Kernel Scheduling Entity)ä¹‹é—´çš„å¯¹åº”å…³ç³»ä¸Šã€‚\nå†…æ ¸è°ƒåº¦å®ä½“KSEæŒ‡æ“ä½œç³»ç»Ÿå†…æ ¸è°ƒåº¦å™¨è°ƒåº¦çš„å¯¹è±¡å®ä½“ï¼Œæ˜¯å†…æ ¸è°ƒåº¦çš„æœ€å°å•å…ƒã€‚\n1.1. çº¿ç¨‹æ¨¡å‹å¯¹æ¯” çº¿ç¨‹æ¨¡å‹ ç”¨æˆ·çº¿ç¨‹ä¸KSEä¹‹é—´çš„å¯¹åº”å…³ç³» ç‰¹ç‚¹ ä¼˜ç‚¹ ç¼ºç‚¹ å†…æ ¸çº§çº¿ç¨‹æ¨¡å‹ 1:1 1æ¡ç”¨æˆ·çº¿ç¨‹å¯¹åº”ä¸€æ¡å†…æ ¸è¿›ç¨‹/çº¿ç¨‹æ¥è°ƒåº¦ï¼Œå³ä»¥æ ¸å¿ƒæ€çº¿ç¨‹å®ç°ã€‚ å…·æœ‰å’Œå†…æ ¸çº¿ç¨‹ä¸€è‡´çš„ä¼˜ç‚¹ï¼Œä¸åŒç”¨æˆ·çº¿ç¨‹ä¹‹é—´ä¸ä¼šäº’ç›¸å½±å“ã€‚å¯ä»¥åˆ©ç”¨å¤šæ ¸ç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚ åœ¨å¤§é‡çº¿ç¨‹çš„æƒ…å†µä¸‹ï¼Œçº¿ç¨‹çš„åˆ›å»ºã€åˆ é™¤ã€åˆ‡æ¢çš„ä»£ä»·æ›´æ˜‚è´µï¼Œå½±å“æ€§èƒ½ã€‚ ç”¨æˆ·çº§çº¿ç¨‹æ¨¡å‹ M:1 Næ¡ç”¨æˆ·çº¿ç¨‹åªç”±ä¸€æ¡å†…æ ¸è¿›ç¨‹/çº¿ç¨‹è°ƒåº¦ï¼Œå³ä»¥ç”¨æˆ·æ€çº¿ç¨‹å®ç°ã€‚ çº¿ç¨‹çš„åˆ›å»ºã€åˆ é™¤å’Œç¯å¢ƒåˆ‡æ¢éƒ½å¾ˆé«˜æ•ˆã€‚ ä¸€æ—¦ä¸€ä¸ªçº¿ç¨‹å‘ç”Ÿé˜»å¡ï¼Œæ•´ä¸ªè¿›ç¨‹ä¸‹çš„å…¶ä»–çº¿ç¨‹ä¹Ÿä¼šè¢«é˜»å¡ã€‚ä¸èƒ½åˆ©ç”¨å¤šæ ¸ç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚ æ··åˆå‹çº¿ç¨‹æ¨¡å‹ M:N Mæ¡ç”¨æˆ·çº¿ç¨‹ç”±Næ¡å†…æ ¸çº¿ç¨‹åŠ¨æ€å…³è”ã€‚åˆç§°ä¸¤çº§çº¿ç¨‹æ¨¡å‹ å¯ä»¥å¿«é€Ÿåœ°æ‰§è¡Œä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œè€Œä¸”å¯ä»¥åˆ©ç”¨å¤šæ ¸çš„ä¼˜åŠ¿ã€‚å½“æŸä¸ªçº¿ç¨‹å‘ç”Ÿé˜»å¡å¯ä»¥è°ƒåº¦å‡ºCPUå…³è”åˆ°å¯ä»¥æ‰§è¡Œçš„çº¿ç¨‹ä¸Šã€‚ç›®å‰Goå°±æ˜¯é‡‡ç”¨è¿™ç§çº¿ç¨‹æ¨¡å‹ã€‚ åŠ¨æ€å…³è”æœºåˆ¶å®ç°å¤æ‚ï¼Œéœ€è¦ç”¨æˆ·æˆ–runtimeè‡ªå·±å»å®ç°ã€‚ 1.2. çº¿ç¨‹æ¨¡å‹ç¤ºæ„å›¾ 2. G-P-Mè°ƒåº¦æ¨¡å‹ è°ƒåº¦æ¨¡å‹:\nG-P-Må¯¹åº”å…³ç³»ï¼š\n2.1. åŸºæœ¬æ¦‚å¿µ Mï¼šmachineï¼Œä»£è¡¨ç³»ç»Ÿå†…æ ¸è¿›ç¨‹ï¼Œç”¨æ¥æ‰§è¡ŒGã€‚ï¼ˆå·¥äººï¼‰ Pï¼šprocessorï¼Œä»£è¡¨è°ƒåº¦æ‰§è¡Œçš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œç»´æŠ¤äº†ä¸€ä¸ªæœ¬åœ°çš„goroutineçš„é˜Ÿåˆ—ã€‚ï¼ˆå°æ¨è½¦ï¼‰ Gï¼šgoroutineï¼Œä»£è¡¨goroutineï¼Œå³æ‰§è¡Œçš„goroutineçš„æ•°æ®ç»“æ„åŠæ ˆç­‰ã€‚ï¼ˆç –å¤´ï¼‰ 2.2. åŸºæœ¬æµç¨‹ è°ƒåº¦çš„æœ¬è´¨æ˜¯å°†Gå°½é‡å‡åŒ€åˆç†åœ°å®‰æ’ç»™Mæ¥æ‰§è¡Œï¼Œå…¶ä¸­Pçš„ä½œç”¨å°±æ˜¯æ¥å®ç°åˆç†å®‰æ’é€»è¾‘ã€‚\nPçš„æ•°é‡é€šè¿‡ GOMAXPROCS() æ¥è®¾ç½®ï¼Œä¸€èˆ¬ç­‰äºCPUçš„æ ¸æ•°ï¼Œå¯¹äºä¸€æ¬¡ä»£ç æ‰§è¡Œè®¾ç½®å¥½ä¸€èˆ¬ä¸ä¼šå˜ã€‚ Pç»´æŠ¤äº†ä¸€ä¸ªæœ¬åœ°çš„Gé˜Ÿåˆ—ï¼ˆrunqueueï¼‰ï¼ŒåŒ…æ‹¬æ­£åœ¨æ‰§è¡Œå’Œå¾…æ‰§è¡Œçš„Gï¼Œå°½é‡ä¿è¯æ‰€æœ‰çš„Péƒ½åŒ¹é…ä¸€ä¸ªMåŒæ—¶åœ¨æ‰§è¡ŒGã€‚ å½“Pæœ¬åœ°goroutineé˜Ÿåˆ—æ¶ˆè´¹å®Œï¼Œä¼šä»å…¨å±€çš„goroutineé˜Ÿåˆ—ï¼ˆglobal runqueueï¼‰ä¸­æ‹¿goroutineåˆ°æœ¬åœ°é˜Ÿåˆ—ã€‚Pä¹Ÿä¼šå®šæœŸæ£€æŸ¥å…¨å±€çš„goroutineé˜Ÿåˆ—ï¼Œé¿å…å­˜åœ¨å…¨å±€çš„goroutineæ²¡æœ‰è¢«æ‰§è¡Œè€Œ\"é¥¿æ­»\"çš„ç°è±¡ã€‚ På’ŒMæ˜¯åŠ¨æ€å½¢å¼çš„ä¸€å¯¹ä¸€çš„å…³ç³»ï¼ŒPå’ŒGæ˜¯åŠ¨æ€å½¢å¼çš„ä¸€å¯¹å¤šçš„å…³ç³»ã€‚ 2.3. æŠ¢å å¼è°ƒåº¦ï¼ˆé˜»å¡ï¼‰ å½“goroutineå‘ç”Ÿé˜»å¡çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡På°†å‰©ä½™çš„Gåˆ‡æ¢ç»™æ–°çš„Mæ¥æ‰§è¡Œï¼Œè€Œä¸ä¼šå¯¼è‡´å‰©ä½™çš„Gæ— æ³•æ‰§è¡Œï¼Œå¦‚æœæ²¡æœ‰Måˆ™åˆ›å»ºMæ¥åŒ¹é…Pã€‚\nå½“é˜»å¡çš„goroutineè¿”å›åï¼Œè¿›ç¨‹ä¼šå°è¯•è·å–ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼ˆContextï¼‰æ¥æ‰§è¡Œè¿™ä¸ªgoroutineã€‚ä¸€èˆ¬æ˜¯å…ˆä»å…¶ä»–è¿›ç¨‹ä¸­\"å·å–\"ä¸€ä¸ªContextï¼Œå¦‚æœ\"å·å–\"ä¸æˆåŠŸï¼Œåˆ™å°†goroutineæ”¾å…¥å…¨å±€çš„goroutineä¸­ã€‚\n2.4. å·ä»»åŠ¡ På¯ä»¥å·ä»»åŠ¡å³goroutineï¼Œå½“æŸä¸ªPçš„æœ¬åœ°Gæ‰§è¡Œå®Œï¼Œä¸”å…¨å±€æ²¡æœ‰Géœ€è¦æ‰§è¡Œçš„æ—¶å€™ï¼ŒPå¯ä»¥å»å·åˆ«çš„Pè¿˜æ²¡æœ‰æ‰§è¡Œå®Œçš„ä¸€åŠçš„Gæ¥ç»™Mæ‰§è¡Œï¼Œæé«˜äº†Gçš„æ‰§è¡Œæ•ˆç‡ã€‚\nå‚è€ƒï¼š\nhttp://morsmachine.dk/go-scheduler Scalable Go Scheduler Design Doc Go Preemptive Scheduler Design Doc k2huang/blogpost/Goå¹¶å‘æœºåˆ¶ ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦ä»‹ç»Goçš„è°ƒåº¦æ¨¡å‹ã€‚\n1. çº¿ç¨‹å®ç°æ¨¡å‹ çº¿ç¨‹æ¨¡å‹æœ‰ä¸‰ç±»ï¼šå†…æ ¸çº§çº¿ç¨‹æ¨¡å‹ã€ç”¨æˆ·çº§çº¿ç¨‹æ¨¡å‹ã€æ··åˆå‹çº¿ç¨‹æ¨¡å‹ã€‚ä¸‰è€…çš„åŒºåˆ«ä¸»è¦åœ¨äºçº¿ç¨‹ä¸ â€¦","ref":"/golang-notes/principle/go-scheduler/","tags":["Golang"],"title":"Goroutineè°ƒåº¦"},{"body":"1. ç®€ä»‹ Virtual Kubeletæ˜¯ Kubernetes kubelet çš„ä¸€ç§å®ç°ï¼Œä½œä¸ºä¸€ç§è™šæ‹Ÿçš„kubeletç”¨æ¥è¿æ¥k8sé›†ç¾¤å’Œå…¶ä»–å¹³å°çš„APIã€‚è¿™å…è®¸k8sçš„èŠ‚ç‚¹ç”±å…¶ä»–æä¾›è€…ï¼ˆproviderï¼‰æä¾›æ”¯æŒï¼Œè¿™äº›æä¾›è€…ä¾‹å¦‚serverlesså¹³å°ï¼ˆACI, AWS Fargateï¼‰ã€IoT Edgeç­‰ã€‚\nä¸€å¥è¯æ¦‚æ‹¬ï¼šKubernetes API on top, programmable backã€‚\n2. æ¶æ„å›¾ 3. åŠŸèƒ½ virtual kubeletæä¾›ä¸€ä¸ªå¯ä»¥è‡ªå®šä¹‰k8s nodeçš„ä¾èµ–åº“ã€‚\nç›®å‰æ”¯æŒçš„åŠŸèƒ½å¦‚ä¸‹ï¼š\nåˆ›å»ºã€åˆ é™¤ã€æ›´æ–° pod å®¹å™¨çš„æ—¥å¿—ã€execå‘½ä»¤ã€metrics è·å–podã€podåˆ—è¡¨ã€pod status nodeçš„åœ°å€ã€å®¹é‡ã€daemon æ“ä½œç³»ç»Ÿ è‡ªå®šä¹‰virtual network 4. Providers virtual kubeletæä¾›ä¸€ä¸ªæ’ä»¶å¼çš„provideræ¥å£ï¼Œè®©å¼€å‘è€…å¯ä»¥è‡ªå®šä¹‰å®ç°ä¼ ç»Ÿkubeletçš„åŠŸèƒ½ã€‚è‡ªå®šä¹‰çš„providerå¯ä»¥ç”¨è‡ªå·±çš„é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå‚æ•°ã€‚\nè‡ªå®šä¹‰çš„providerå¿…é¡»æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š\næä¾›podã€å®¹å™¨ã€èµ„æºçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„åŠŸèƒ½ ç¬¦åˆvirtual kubeletæä¾›çš„API ä¸ç›´æ¥è®¿é—®k8s apiserverï¼Œå®šä¹‰è·å–æ•°æ®çš„å›è°ƒæœºåˆ¶ï¼Œä¾‹å¦‚configmapã€secrets å¼€æºçš„provider\nAlibaba Cloud ECI Provider Azure Container Instances Provider AWS Fargate Provider 5. è‡ªå®šä¹‰provider åˆ›å»ºè‡ªå®šä¹‰providerçš„ç›®å½•ã€‚\ngit clone https://github.com/virtual-kubelet/virtual-kubelet cd virtual-kubelet mkdir providers/my-provider 5.1. PodLifecylceHandler å½“podè¢«k8såˆ›å»ºã€æ›´æ–°ã€åˆ é™¤æ—¶ï¼Œä¼šè°ƒç”¨ä»¥ä¸‹æ–¹æ³•ã€‚\ntype PodLifecycleHandler interface { // CreatePod takes a Kubernetes Pod and deploys it within the provider. CreatePod(ctx context.Context, pod *corev1.Pod) error // UpdatePod takes a Kubernetes Pod and updates it within the provider. UpdatePod(ctx context.Context, pod *corev1.Pod) error // DeletePod takes a Kubernetes Pod and deletes it from the provider. DeletePod(ctx context.Context, pod *corev1.Pod) error // GetPod retrieves a pod by name from the provider (can be cached). GetPod(ctx context.Context, namespace, name string) (*corev1.Pod, error) // GetPodStatus retrieves the status of a pod by name from the provider. GetPodStatus(ctx context.Context, namespace, name string) (*corev1.PodStatus, error) // GetPods retrieves a list of all pods running on the provider (can be cached). GetPods(context.Context) ([]*corev1.Pod, error) } PodLifecycleHandleræ˜¯è¢«PodControlleræ¥è°ƒç”¨ï¼Œæ¥ç®¡ç†è¢«åˆ†é…åˆ°nodeä¸Šçš„podã€‚\npc, _ := node.NewPodController(podControllerConfig) // \u003c-- instatiates the pod controller pc.Run(ctx) // \u003c-- starts watching for pods to be scheduled on the node 5.2. PodNotifier(optional) PodNotifieræ˜¯å¯é€‰å®ç°ï¼Œè¯¥æ¥å£ä¸»è¦ç”¨æ¥é€šçŸ¥virtual kubeletçš„podçŠ¶æ€å˜åŒ–ã€‚å¦‚æœæ²¡æœ‰å®ç°è¯¥æ¥å£ï¼Œvirtual-kubeletä¼šå®šæœŸæ£€æŸ¥æ‰€æœ‰podçš„çŠ¶æ€ã€‚\ntype PodNotifier interface { // NotifyPods instructs the notifier to call the passed in function when // the pod status changes. // // NotifyPods should not block callers. NotifyPods(context.Context, func(*corev1.Pod)) } 5.3. NodeProvider NodeProviderç”¨æ¥é€šçŸ¥virtual-kubeletå…³äºnodeçŠ¶æ€çš„å˜åŒ–ï¼Œvirtual-kubeletä¼šå®šæœŸæ£€æŸ¥nodeæ˜¯çŠ¶æ€å¹¶ç›¸åº”åœ°æ›´æ–°k8sã€‚\ntype NodeProvider interface { // Ping checks if the node is still active. // This is intended to be lightweight as it will be called periodically as a // heartbeat to keep the node marked as ready in Kubernetes. Ping(context.Context) error // NotifyNodeStatus is used to asynchronously monitor the node. // The passed in callback should be called any time there is a change to the // node's status. // This will generally trigger a call to the Kubernetes API server to update // the status. // // NotifyNodeStatus should not block callers. NotifyNodeStatus(ctx context.Context, cb func(*corev1.Node)) } NodeProvideræ˜¯è¢«NodeControllerè°ƒç”¨ï¼Œæ¥ç®¡ç†k8sä¸­çš„nodeå¯¹è±¡ã€‚\nnc, _ := node.NewNodeController(nodeProvider, nodeSpec) // \u003c-- instantiate a node controller from a node provider and a kubernetes node spec nc.Run(ctx) // \u003c-- creates the node in kubernetes and starts up he controller 5.4. æµ‹è¯• è¿›å…¥åˆ°é¡¹ç›®æ ¹ç›®å½•\nmake test 5.5. ç¤ºä¾‹ä»£ç  Azure Container Instances Provider\nhttps://github.com/virtual-kubelet/azure-aci/blob/master/aci.go#L541\nAlibaba Cloud ECI Provider\nhttps://github.com/virtual-kubelet/alibabacloud-eci/blob/master/eci.go#L177\nAWS Fargate Provider\nhttps://github.com/virtual-kubelet/aws-fargate/blob/master/provider.go#L110\nå‚è€ƒï¼š\nhttps://github.com/virtual-kubelet/virtual-kubelet https://virtual-kubelet.io/docs/ ","categories":"","description":"","excerpt":"1. ç®€ä»‹ Virtual Kubeletæ˜¯ Kubernetes kubelet çš„ä¸€ç§å®ç°ï¼Œä½œä¸ºä¸€ç§è™šæ‹Ÿçš„kubeletç”¨æ¥è¿æ¥k8sé›† â€¦","ref":"/kubernetes-notes/multi-cluster/virtual-kubelet/virtual-kubelet/","tags":["VirtualKubelet"],"title":"Virtual Kubeletä»‹ç»"},{"body":"1. Gitæ˜¯ä»€ä¹ˆ 1.1. æ¦‚è¿° Gitæ˜¯åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œä¸SVNç±»ä¼¼çš„é›†ä¸­åŒ–ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿç›¸æ¯”ï¼Œé›†ä¸­åŒ–ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå¦‚æœä¸­å¤®æœåŠ¡å™¨å®•æœºåˆ™ä¼šå½±å“æ•°æ®å’ŒååŒå¼€å‘ã€‚\nGitæ˜¯åˆ†å¸ƒå¼çš„ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œå®¢æˆ·ç«¯ä¸åªæ˜¯æå–æœ€æ–°ç‰ˆæœ¬çš„å¿«ç…§ï¼Œè€Œä¸”å°†æ•´ä¸ªä»£ç ä»“åº“é•œåƒå¤åˆ¶ä¸‹æ¥ã€‚å¦‚æœä»»ä½•ååŒå·¥ä½œç”¨çš„æœåŠ¡å™¨å‘ç”Ÿæ•…éšœäº†ï¼Œä¹Ÿå¯ä»¥ç”¨ä»»ä½•ä¸€ä¸ªä»£ç ä»“åº“æ¥æ¢å¤ã€‚è€Œä¸”åœ¨åä½œæœåŠ¡å™¨å®•æœºæœŸé—´ï¼Œä½ ä¹Ÿå¯ä»¥æäº¤ä»£ç åˆ°æœ¬åœ°ä»“åº“ï¼Œå½“åä½œæœåŠ¡å™¨æ­£å¸¸å·¥ä½œåï¼Œä½ å†å°†æœ¬åœ°ä»“åº“åŒæ­¥åˆ°è¿œç¨‹ä»“åº“ã€‚\n1.2. ç‰¹æ€§ èƒ½å¤Ÿå¯¹æ–‡ä»¶ç‰ˆæœ¬æ§åˆ¶å’Œå¤šäººåä½œå¼€å‘ æ‹¥æœ‰å¼ºå¤§çš„åˆ†æ”¯ç‰¹æ€§ï¼Œæ‰€ä»¥èƒ½å¤Ÿçµæ´»åœ°ä»¥ä¸åŒçš„å·¥ä½œæµååŒå¼€å‘ åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œå³ä½¿åä½œæœåŠ¡å™¨å®•æœºï¼Œä¹Ÿèƒ½ç»§ç»­æäº¤ä»£ç æˆ–æ–‡ä»¶åˆ°æœ¬åœ°ä»“åº“ï¼Œå½“åä½œæœåŠ¡å™¨æ¢å¤æ­£å¸¸å·¥ä½œæ—¶ï¼Œå†å°†æœ¬åœ°ä»“åº“åŒæ­¥åˆ°è¿œç¨‹ä»“åº“ã€‚ å½“å›¢é˜Ÿä¸­æŸä¸ªæˆå‘˜å®ŒæˆæŸä¸ªåŠŸèƒ½æ—¶ï¼Œé€šè¿‡pull requestæ“ä½œæ¥é€šçŸ¥å…¶ä»–å›¢é˜Ÿæˆå‘˜ï¼Œå…¶ä»–å›¢é˜Ÿæˆå‘˜èƒ½å¤Ÿreview codeåå†åˆå¹¶ä»£ç ã€‚ 2. ä¸ºä»€ä¹ˆè¦ç”¨Git èƒ½å¤Ÿå¯¹æ–‡ä»¶ç‰ˆæœ¬æ§åˆ¶å’Œå¤šäººåä½œå¼€å‘ æ‹¥æœ‰å¼ºå¤§çš„åˆ†æ”¯ç‰¹æ€§ï¼Œæ‰€ä»¥èƒ½å¤Ÿçµæ´»åœ°ä»¥ä¸åŒçš„å·¥ä½œæµååŒå¼€å‘ åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œå³ä½¿åä½œæœåŠ¡å™¨å®•æœºï¼Œä¹Ÿèƒ½ç»§ç»­æäº¤ä»£ç æˆ–æ–‡ä»¶åˆ°æœ¬åœ°ä»“åº“ï¼Œå½“åä½œæœåŠ¡å™¨æ¢å¤æ­£å¸¸å·¥ä½œæ—¶ï¼Œå†å°†æœ¬åœ°ä»“åº“åŒæ­¥åˆ°è¿œç¨‹ä»“åº“ã€‚ å½“å›¢é˜Ÿä¸­æŸä¸ªæˆå‘˜å®ŒæˆæŸä¸ªåŠŸèƒ½æ—¶ï¼Œé€šè¿‡pull requestæ“ä½œæ¥é€šçŸ¥å…¶ä»–å›¢é˜Ÿæˆå‘˜ï¼Œå…¶ä»–å›¢é˜Ÿæˆå‘˜èƒ½å¤Ÿreview codeåå†åˆå¹¶ä»£ç ã€‚ 3. Git å‘½ä»¤æ€ç»´å¯¼å›¾ ","categories":"","description":"","excerpt":"1. Gitæ˜¯ä»€ä¹ˆ 1.1. æ¦‚è¿° Gitæ˜¯åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œä¸SVNç±»ä¼¼çš„é›†ä¸­åŒ–ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿç›¸æ¯”ï¼Œé›†ä¸­åŒ–ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå¦‚æœä¸­å¤®æœåŠ¡å™¨å®•æœºåˆ™ â€¦","ref":"/linux-notes/git/git/","tags":["Git"],"title":"Gitä»‹ç»"},{"body":"1. Keepalivedç®€ä»‹ 1.1. æ¦‚è¿° Keepalivedä¸€ä¸ªåŸºäºVRRPåè®®æ¥å®ç°çš„LVSæœåŠ¡é«˜å¯ç”¨æ–¹æ¡ˆï¼Œå¯ä»¥åˆ©ç”¨å…¶æ¥é¿å…å•ç‚¹æ•…éšœã€‚ä¸€ä¸ªLVSæœåŠ¡ä¼šæœ‰2å°æœåŠ¡å™¨è¿è¡ŒKeepalivedï¼Œä¸€å°ä¸ºä¸»æœåŠ¡å™¨ï¼ˆMASTERï¼‰ï¼Œä¸€å°ä¸ºå¤‡ä»½æœåŠ¡å™¨ï¼ˆBACKUPï¼‰ï¼Œä½†æ˜¯å¯¹å¤–è¡¨ç°ä¸ºä¸€ä¸ªè™šæ‹ŸIPï¼Œä¸»æœåŠ¡å™¨ä¼šå‘é€ç‰¹å®šçš„æ¶ˆæ¯ç»™å¤‡ä»½æœåŠ¡å™¨ï¼Œå½“å¤‡ä»½æœåŠ¡å™¨æ”¶ä¸åˆ°è¿™ä¸ªæ¶ˆæ¯çš„æ—¶å€™ï¼Œå³ä¸»æœåŠ¡å™¨å®•æœºçš„æ—¶å€™ï¼Œ å¤‡ä»½æœåŠ¡å™¨å°±ä¼šæ¥ç®¡è™šæ‹ŸIPï¼Œç»§ç»­æä¾›æœåŠ¡ï¼Œä»è€Œä¿è¯äº†é«˜å¯ç”¨æ€§ã€‚\n1.2. keepalivedçš„ä½œç”¨ Keepalivedçš„ä½œç”¨æ˜¯æ£€æµ‹æœåŠ¡å™¨çš„çŠ¶æ€ï¼Œå¦‚æœæœ‰ä¸€å°webæœåŠ¡å™¨æ­»æœºï¼Œæˆ–å·¥ä½œå‡ºç°æ•…éšœï¼ŒKeepalivedå°†æ£€æµ‹åˆ°ï¼Œå¹¶å°†æœ‰æ•…éšœçš„æœåŠ¡å™¨ä»ç³»ç»Ÿä¸­å‰”é™¤ï¼ŒåŒæ—¶ä½¿ç”¨å…¶ä»–æœåŠ¡å™¨ä»£æ›¿è¯¥æœåŠ¡å™¨çš„å·¥ä½œï¼Œå½“æœåŠ¡å™¨å·¥ä½œæ­£å¸¸åKeepalivedè‡ªåŠ¨å°†æœåŠ¡å™¨åŠ å…¥åˆ°æœåŠ¡å™¨ç¾¤ä¸­ã€‚\n2. å¦‚ä½•å®ç°Keepalived 2.1. åŸºäºVRRPåè®®çš„ç†è§£ Keepalivedæ˜¯ä»¥VRRPåè®®ä¸ºå®ç°åŸºç¡€çš„ï¼ŒVRRPå…¨ç§°Virtual Router Redundancy Protocolï¼Œå³è™šæ‹Ÿè·¯ç”±å†—ä½™åè®®ã€‚\nè™šæ‹Ÿè·¯ç”±å†—ä½™åè®®ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯å®ç°è·¯ç”±å™¨é«˜å¯ç”¨çš„åè®®ï¼Œå³å°†Nå°æä¾›ç›¸åŒåŠŸèƒ½çš„è·¯ç”±å™¨ç»„æˆä¸€ä¸ªè·¯ç”±å™¨ç»„ï¼Œè¿™ä¸ªç»„é‡Œé¢æœ‰ä¸€ä¸ªmasterå’Œå¤šä¸ªbackupï¼Œmasterä¸Šé¢æœ‰ä¸€ä¸ªå¯¹å¤–æä¾›æœåŠ¡çš„vipï¼ˆè¯¥è·¯ç”±å™¨æ‰€åœ¨å±€åŸŸç½‘å†…å…¶ä»–æœºå™¨çš„é»˜è®¤è·¯ç”±ä¸ºè¯¥vipï¼‰ï¼Œmasterä¼šå‘ç»„æ’­ï¼Œå½“backupæ”¶ä¸åˆ°vrrpåŒ…æ—¶å°±è®¤ä¸ºmasterå®•æ‰äº†ï¼Œè¿™æ—¶å°±éœ€è¦æ ¹æ®VRRPçš„ä¼˜å…ˆçº§æ¥é€‰ä¸¾ä¸€ä¸ªbackupå½“masterã€‚è¿™æ ·çš„è¯å°±å¯ä»¥ä¿è¯è·¯ç”±å™¨çš„é«˜å¯ç”¨äº†ã€‚\nkeepalivedä¸»è¦æœ‰ä¸‰ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«æ˜¯coreã€checkå’Œvrrpã€‚coreæ¨¡å—ä¸ºkeepalivedçš„æ ¸å¿ƒï¼Œè´Ÿè´£ä¸»è¿›ç¨‹çš„å¯åŠ¨ã€ç»´æŠ¤ä»¥åŠå…¨å±€é…ç½®æ–‡ä»¶çš„åŠ è½½å’Œè§£æã€‚checkè´Ÿè´£å¥åº·æ£€æŸ¥ï¼ŒåŒ…æ‹¬å¸¸è§çš„å„ç§æ£€æŸ¥æ–¹å¼ã€‚vrrpæ¨¡å—æ˜¯æ¥å®ç°VRRPåè®®çš„ã€‚\n2.2. åŸºäºTCP/IPåè®®çš„ç†è§£ Layer3,4\u00267å·¥ä½œåœ¨IP/TCPåè®®æ ˆçš„IPå±‚ï¼ŒTCPå±‚ï¼ŒåŠåº”ç”¨å±‚,åŸç†åˆ†åˆ«å¦‚ä¸‹ï¼š\nLayer3ï¼š\nKeepalivedä½¿ç”¨Layer3çš„æ–¹å¼å·¥ä½œå¼æ—¶ï¼ŒKeepalivedä¼šå®šæœŸå‘æœåŠ¡å™¨ç¾¤ä¸­çš„æœåŠ¡å™¨å‘é€ä¸€ä¸ªICMPçš„æ•°æ®åŒ…ï¼ˆæ—¢æˆ‘ä»¬å¹³æ—¶ç”¨çš„Pingç¨‹åºï¼‰,å¦‚æœå‘ç°æŸå°æœåŠ¡çš„IPåœ°å€æ²¡æœ‰æ¿€æ´»ï¼ŒKeepalivedä¾¿æŠ¥å‘Šè¿™å°æœåŠ¡å™¨å¤±æ•ˆï¼Œå¹¶å°†å®ƒä»æœåŠ¡å™¨ç¾¤ä¸­å‰”é™¤ï¼Œè¿™ç§æƒ…å†µçš„å…¸å‹ä¾‹å­æ˜¯æŸå°æœåŠ¡å™¨è¢«éæ³•å…³æœºã€‚Layer3çš„æ–¹å¼æ˜¯ä»¥æœåŠ¡å™¨çš„IPåœ°å€æ˜¯å¦æœ‰æ•ˆä½œä¸ºæœåŠ¡å™¨å·¥ä½œæ­£å¸¸ä¸å¦çš„æ ‡å‡†ã€‚\nLayer4:\nå¦‚æœæ‚¨ç†è§£äº†Layer3çš„æ–¹å¼ï¼ŒLayer4å°±å®¹æ˜“äº†ã€‚Layer4ä¸»è¦ä»¥TCPç«¯å£çš„çŠ¶æ€æ¥å†³å®šæœåŠ¡å™¨å·¥ä½œæ­£å¸¸ä¸å¦ã€‚å¦‚web serverçš„æœåŠ¡ç«¯å£ä¸€èˆ¬æ˜¯80ï¼Œå¦‚æœKeepalivedæ£€æµ‹åˆ°80ç«¯å£æ²¡æœ‰å¯åŠ¨ï¼Œåˆ™Keepalivedå°†æŠŠè¿™å°æœåŠ¡å™¨ä»æœåŠ¡å™¨ç¾¤ä¸­å‰”é™¤ã€‚\nLayer7ï¼š\nLayer7å°±æ˜¯å·¥ä½œåœ¨å…·ä½“çš„åº”ç”¨å±‚äº†ï¼Œæ¯”Layer3,Layer4è¦å¤æ‚ä¸€ç‚¹ï¼Œåœ¨ç½‘ç»œä¸Šå ç”¨çš„å¸¦å®½ä¹Ÿè¦å¤§ä¸€äº›ã€‚Keepalivedå°†æ ¹æ®ç”¨æˆ·çš„è®¾å®šæ£€æŸ¥æœåŠ¡å™¨ç¨‹åºçš„è¿è¡Œæ˜¯å¦æ­£å¸¸ï¼Œå¦‚æœä¸ç”¨æˆ·çš„è®¾å®šä¸ç›¸ç¬¦ï¼Œåˆ™Keepalivedå°†æŠŠæœåŠ¡å™¨ä»æœåŠ¡å™¨ç¾¤ä¸­å‰”é™¤ã€‚\n3. Keepalivedé€‰ä¸¾ç­–ç•¥ 3.1. é€‰ä¸¾ç­–ç•¥ é¦–å…ˆï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªåˆå§‹ä¼˜å…ˆçº§ï¼Œç”±é…ç½®æ–‡ä»¶ä¸­çš„priorityé…ç½®é¡¹æŒ‡å®šï¼ŒMASTERèŠ‚ç‚¹çš„priorityåº”æ¯”BAKCUPé«˜ã€‚è¿è¡Œè¿‡ç¨‹ä¸­keepalivedæ ¹æ®vrrp_scriptçš„weightè®¾å®šï¼Œå¢åŠ æˆ–å‡å°èŠ‚ç‚¹ä¼˜å…ˆçº§ã€‚è§„åˆ™å¦‚ä¸‹ï¼š\nâ€œweightâ€å€¼ä¸ºæ­£æ—¶,è„šæœ¬æ£€æµ‹æˆåŠŸæ—¶â€weightâ€å€¼ä¼šåŠ åˆ°â€priorityâ€ä¸Š,æ£€æµ‹å¤±è´¥æ˜¯ä¸åŠ  ä¸»å¤±è´¥: ä¸»priority\u003cå¤‡priority+weightä¹‹å’Œæ—¶ä¼šåˆ‡æ¢\nä¸»æˆåŠŸ: ä¸»priority+weightä¹‹å’Œ\u003eå¤‡priority+weightä¹‹å’Œæ—¶,ä¸»ä¾ç„¶ä¸ºä¸»,å³ä¸å‘ç”Ÿåˆ‡æ¢\nâ€œweightâ€ä¸ºè´Ÿæ•°æ—¶,è„šæœ¬æ£€æµ‹æˆåŠŸæ—¶â€weightâ€ä¸å½±å“â€priorityâ€,æ£€æµ‹å¤±è´¥æ—¶,MasterèŠ‚ç‚¹çš„æƒå€¼å°†æ˜¯â€œpriorityâ€œå€¼ä¸â€œweightâ€å€¼ä¹‹å·® ä¸»å¤±è´¥: ä¸»priotity-abs(weight) \u003c å¤‡priorityæ—¶ä¼šå‘ç”Ÿåˆ‡æ¢\nä¸»æˆåŠŸ: ä¸»priority \u003e å¤‡priority ä¸åˆ‡æ¢\nå½“ä¸¤ä¸ªèŠ‚ç‚¹çš„ä¼˜å…ˆçº§ç›¸åŒæ—¶ï¼Œä»¥èŠ‚ç‚¹å‘é€VRRPé€šå‘Šçš„IPä½œä¸ºæ¯”è¾ƒå¯¹è±¡ï¼ŒIPè¾ƒå¤§è€…ä¸ºMASTERã€‚ 3.2. priorityå’Œweightçš„è®¾å®š ä¸»ä»çš„ä¼˜å…ˆçº§åˆå§‹å€¼priorityå’Œå˜åŒ–é‡weightè®¾ç½®éå¸¸å…³é”®ï¼Œé…é”™çš„è¯ä¼šå¯¼è‡´æ— æ³•è¿›è¡Œä¸»ä»åˆ‡æ¢ã€‚æ¯”å¦‚ï¼Œå½“MASTERåˆå§‹å€¼å®šå¾—å¤ªé«˜ï¼Œå³ä½¿scriptè„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œä¹Ÿæ¯”BACKUPçš„priority + weightå¤§ï¼Œå°±æ²¡æ³•è¿›è¡ŒVIPæ¼‚ç§»äº†ã€‚\næ‰€ä»¥priorityå’Œweightå€¼çš„è®¾å®šåº”éµå¾ª: abs(MASTER priority - BAKCUP priority) \u003c abs(weight)ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œåˆå§‹å€¼MASTERçš„priorityå€¼åº”è¯¥æ¯”è¾ƒBACKUPå¤§ï¼Œä½†ä¸èƒ½è¶…è¿‡weightçš„ç»å¯¹å€¼ã€‚ å¦å¤–ï¼Œå½“ç½‘ç»œä¸­ä¸æ”¯æŒå¤šæ’­(ä¾‹å¦‚æŸäº›äº‘ç¯å¢ƒ)ï¼Œæˆ–è€…å‡ºç°ç½‘ç»œåˆ†åŒºçš„æƒ…å†µï¼Œkeepalived BACKUPèŠ‚ç‚¹æ”¶ä¸åˆ°MASTERçš„VRRPé€šå‘Šï¼Œå°±ä¼šå‡ºç°è„‘è£‚(split brain)ç°è±¡ï¼Œæ­¤æ—¶é›†ç¾¤ä¸­ä¼šå­˜åœ¨å¤šä¸ªMASTERèŠ‚ç‚¹ã€‚\n","categories":"","description":"","excerpt":"1. Keepalivedç®€ä»‹ 1.1. æ¦‚è¿° Keepalivedä¸€ä¸ªåŸºäºVRRPåè®®æ¥å®ç°çš„LVSæœåŠ¡é«˜å¯ç”¨æ–¹æ¡ˆï¼Œå¯ä»¥åˆ©ç”¨å…¶æ¥é¿å…å•ç‚¹æ•… â€¦","ref":"/linux-notes/keepalived/keepalived-introduction/","tags":["Keepalived"],"title":"Keepalivedç®€ä»‹"},{"body":"vscodeå¿«æ·é”® 1. åŸºæœ¬å¿«æ·é”® 1.1. VsCode å¿«æ·é”®æœ‰äº”ç§ç»„åˆæ–¹å¼ Ctrl + Shift + ? : è¿™ç§å¸¸è§„ç»„åˆæŒ‰é’® Ctrl + V Ctrl +V : åŒæ—¶ä¾èµ–ä¸€ä¸ªæŒ‰é”®çš„ç»„åˆ Shift + V c : å…ˆç»„åˆåå•é”®çš„è¾“å…¥ Ctrl + Click: é”®ç›˜ + é¼ æ ‡ç‚¹å‡» Ctrl + DragMouse : é”®ç›˜ + é¼ æ ‡æ‹–åŠ¨ 1.2. Ctrl+P æ¨¡å¼ åœ¨Ctrl+Pä¸‹è¾“å…¥\u003eåˆå¯ä»¥å›åˆ°ä¸»å‘½ä»¤æ¡† Ctrl+Shift+Pæ¨¡å¼ã€‚\nåœ¨Ctrl+Pçª—å£ä¸‹è¿˜å¯ä»¥\nç›´æ¥è¾“å…¥æ–‡ä»¶åï¼Œå¿«é€Ÿæ‰“å¼€æ–‡ä»¶ ? åˆ—å‡ºå½“å‰å¯æ‰§è¡Œçš„åŠ¨ä½œ ! æ˜¾ç¤ºErrorsæˆ–Warningsï¼Œä¹Ÿå¯ä»¥Ctrl+Shift+M : è·³è½¬åˆ°è¡Œæ•°ï¼Œä¹Ÿå¯ä»¥Ctrl+Gç›´æ¥è¿›å…¥ @ è·³è½¬åˆ°symbolï¼ˆæœç´¢å˜é‡æˆ–è€…å‡½æ•°ï¼‰ï¼Œä¹Ÿå¯ä»¥Ctrl+Shift+Oç›´æ¥è¿›å…¥ @:æ ¹æ®åˆ†ç±»è·³è½¬symbolï¼ŒæŸ¥æ‰¾å±æ€§æˆ–å‡½æ•°ï¼Œä¹Ÿå¯ä»¥Ctrl+Shift+Oåè¾“å…¥:è¿›å…¥ # æ ¹æ®åå­—æŸ¥æ‰¾symbolï¼Œä¹Ÿå¯ä»¥Ctrl+T 2. å¿«æ·é”®åˆ†ç±» 2.1. é€šç”¨å¿«æ·é”® å¿«æ·é”® ä½œç”¨ Ctrl+Shift+P,F1 å±•ç¤ºå…¨å±€å‘½ä»¤é¢æ¿ Ctrl+P å¿«é€Ÿæ‰“å¼€æœ€è¿‘æ‰“å¼€çš„æ–‡ä»¶ Ctrl+Shift+N æ‰“å¼€æ–°çš„ç¼–è¾‘å™¨çª—å£ Ctrl+Shift+W å…³é—­ç¼–è¾‘å™¨ 2.2. åŸºç¡€ç¼–è¾‘ å¿«æ·é”® ä½œç”¨ Ctrl + X å‰ªåˆ‡ Ctrl + C å¤åˆ¶ Alt + up/down ç§»åŠ¨è¡Œä¸Šä¸‹ Shift + Alt up/down åœ¨å½“å‰è¡Œä¸Šä¸‹å¤åˆ¶å½“å‰è¡Œ Ctrl + Shift + K åˆ é™¤è¡Œ Ctrl + Enter åœ¨å½“å‰è¡Œä¸‹æ’å…¥æ–°çš„ä¸€è¡Œ Ctrl + Shift + Enter åœ¨å½“å‰è¡Œä¸Šæ’å…¥æ–°çš„ä¸€è¡Œ Ctrl + Shift + | åŒ¹é…èŠ±æ‹¬å·çš„é—­åˆå¤„ï¼Œè·³è½¬ Ctrl + ] / [ è¡Œç¼©è¿› Home å…‰æ ‡è·³è½¬åˆ°è¡Œå¤´ End å…‰æ ‡è·³è½¬åˆ°è¡Œå°¾ Ctrl + Home è·³è½¬åˆ°é¡µå¤´ Ctrl + End è·³è½¬åˆ°é¡µå°¾ Ctrl + up/down è¡Œè§†å›¾ä¸Šä¸‹åç§» Alt + PgUp/PgDown å±è§†å›¾ä¸Šä¸‹åç§» Ctrl + Shift + [ æŠ˜å åŒºåŸŸä»£ç  Ctrl + Shift + ] å±•å¼€åŒºåŸŸä»£ç  Ctrl + K Ctrl + [ æŠ˜å æ‰€æœ‰å­åŒºåŸŸä»£ç  Ctrl + k Ctrl + ] å±•å¼€æ‰€æœ‰æŠ˜å çš„å­åŒºåŸŸä»£ç  Ctrl + K Ctrl + 0 æŠ˜å æ‰€æœ‰åŒºåŸŸä»£ç  Ctrl + K Ctrl + J å±•å¼€æ‰€æœ‰æŠ˜å åŒºåŸŸä»£ç  Ctrl + K Ctrl + C æ·»åŠ è¡Œæ³¨é‡Š Ctrl + K Ctrl + U åˆ é™¤è¡Œæ³¨é‡Š Ctrl + / æ·»åŠ å…³é—­è¡Œæ³¨é‡Š Shift + Alt +A å—åŒºåŸŸæ³¨é‡Š Alt + Z æ·»åŠ å…³é—­è¯æ±‡åŒ…å« 2.3. å¯¼èˆª å¿«æ·é”® ä½œç”¨ Ctrl + T åˆ—å‡ºæ‰€æœ‰ç¬¦å· Ctrl + G è·³è½¬è¡Œ Ctrl + P è·³è½¬æ–‡ä»¶ Ctrl + Shift + O è·³è½¬åˆ°ç¬¦å·å¤„ Ctrl + Shift + M æ‰“å¼€é—®é¢˜å±•ç¤ºé¢æ¿ F8 è·³è½¬åˆ°ä¸‹ä¸€ä¸ªé”™è¯¯æˆ–è€…è­¦å‘Š Shift + F8 è·³è½¬åˆ°ä¸Šä¸€ä¸ªé”™è¯¯æˆ–è€…è­¦å‘Š Ctrl + Shift + Tab åˆ‡æ¢åˆ°æœ€è¿‘æ‰“å¼€çš„æ–‡ä»¶ Alt + left / right å‘åã€å‘å‰ Ctrl + M è¿›å…¥ç”¨Tabæ¥ç§»åŠ¨ç„¦ç‚¹ 2.4. æŸ¥è¯¢ä¸æ›¿æ¢ å¿«æ·é”® ä½œç”¨ Ctrl + F æŸ¥è¯¢ Ctrl + H æ›¿æ¢ F3 / Shift + F3 æŸ¥è¯¢ä¸‹ä¸€ä¸ª/ä¸Šä¸€ä¸ª Alt + Enter é€‰ä¸­æ‰€æœ‰å‡ºç°åœ¨æŸ¥è¯¢ä¸­çš„ Ctrl + D åŒ¹é…å½“å‰é€‰ä¸­çš„è¯æ±‡æˆ–è€…è¡Œï¼Œå†æ¬¡é€‰ä¸­-å¯æ“ä½œ Ctrl + K Ctrl + D ç§»åŠ¨å½“å‰é€‰æ‹©åˆ°ä¸‹ä¸ªåŒ¹é…é€‰æ‹©çš„ä½ç½®(å…‰æ ‡é€‰å®š) Alt + C / R / W 2.5. å¤šè¡Œå…‰æ ‡æ“ä½œäºé€‰æ‹© å¿«æ·é”® ä½œç”¨ Alt + Click æ’å…¥å…‰æ ‡-æ”¯æŒå¤šä¸ª Ctrl + Alt + up/down ä¸Šä¸‹æ’å…¥å…‰æ ‡-æ”¯æŒå¤šä¸ª Ctrl + U æ’¤é”€æœ€åä¸€æ¬¡å…‰æ ‡æ“ä½œ Shift + Alt + I æ’å…¥å…‰æ ‡åˆ°é€‰ä¸­èŒƒå›´å†…æ‰€æœ‰è¡Œç»“æŸç¬¦ Ctrl + I é€‰ä¸­å½“å‰è¡Œ Ctrl + Shift + L é€‰æ‹©æ‰€æœ‰å‡ºç°åœ¨å½“å‰é€‰ä¸­çš„è¡Œ-æ“ä½œ Ctrl + F2 é€‰æ‹©æ‰€æœ‰å‡ºç°åœ¨å½“å‰é€‰ä¸­çš„è¯æ±‡-æ“ä½œ Shift + Alt + right ä»å…‰æ ‡å¤„æ‰©å±•é€‰ä¸­å…¨è¡Œ Shift + Alt + left æ”¶ç¼©é€‰æ‹©åŒºåŸŸ Shift + Alt + (drag mouse) é¼ æ ‡æ‹–åŠ¨åŒºåŸŸï¼ŒåŒæ—¶åœ¨å¤šä¸ªè¡Œç»“æŸç¬¦æ’å…¥å…‰æ ‡ Ctrl + Shift + Alt + (Arrow Key) ä¹Ÿæ˜¯æ’å…¥å¤šè¡Œå…‰æ ‡çš„[æ–¹å‘é”®æ§åˆ¶] Ctrl + Shift + Alt + PgUp/PgDown ä¹Ÿæ˜¯æ’å…¥å¤šè¡Œå…‰æ ‡çš„[æ•´å±ç”Ÿæ•ˆ] Shift + Alt +é¼ æ ‡é€‰æ‹©å— å¤šè¡Œ å—é€‰æ‹©ç¼–è¾‘ 2.6. ä¸°å¯Œçš„è¯­è¨€æ“ä½œ å¿«æ·é”® ä½œç”¨ Ctrl + Space è¾“å…¥å»ºè®®[æ™ºèƒ½æç¤º] Ctrl + Shift + Space å‚æ•°æç¤º Tab EmmetæŒ‡ä»¤è§¦å‘/ç¼©è¿› Shift + Alt + F æ ¼å¼åŒ–ä»£ç  Ctrl + K Ctrl + F æ ¼å¼åŒ–é€‰ä¸­éƒ¨åˆ†çš„ä»£ç  F12 è·³è½¬åˆ°å®šä¹‰å¤„ ctrl + - è·³å›åŸå¤„ï¼ˆè·³è½¬å‰ä½ç½®ï¼‰ Alt + F12 ä»£ç ç‰‡æ®µæ˜¾ç¤ºå®šä¹‰ Ctrl + K F12 åœ¨å…¶ä»–çª—å£æ‰“å¼€å®šä¹‰å¤„ Ctrl + . å¿«é€Ÿä¿®å¤éƒ¨åˆ†å¯ä»¥ä¿®å¤çš„è¯­æ³•é”™è¯¯ Shift + F12 æ˜¾ç¤ºæ‰€æœ‰å¼•ç”¨ F2 é‡å‘½åç¬¦å· Ctrl + Shift + . / , æ›¿æ¢ä¸‹ä¸ªå€¼ Ctrl + K Ctrl + X ç§»é™¤ç©ºç™½å­—ç¬¦ Ctrl + K M æ›´æ”¹é¡µé¢æ–‡æ¡£æ ¼å¼ 2.7. ç¼–è¾‘å™¨ç®¡ç† å¿«æ·é”® ä½œç”¨ Ctrl + F4, Ctrl + W å…³é—­ç¼–è¾‘å™¨ Ctrl + k F å…³é—­å½“å‰æ‰“å¼€çš„æ–‡ä»¶å¤¹ Ctrl + |åˆ‡å‰²ç¼–è¾‘çª—å£ Ctrl + 1/2/3 åˆ‡æ¢ç„¦ç‚¹åœ¨ä¸åŒçš„åˆ‡å‰²çª—å£ Ctrl + K Ctrl \u003c-/-\u003e åˆ‡æ¢ç„¦ç‚¹åœ¨ä¸åŒçš„åˆ‡å‰²çª—å£ Ctrl + Shift + PgUp/PgDown åˆ‡æ¢æ ‡ç­¾é¡µçš„ä½ç½® Ctrl + K \u003c-/-\u003e åˆ‡å‰²çª—å£ä½ç½®è°ƒæ¢ 2.8. æ–‡ä»¶ç®¡ç† å¿«æ·é”® ä½œç”¨ Ctrl + N æ–°å»ºæ–‡ä»¶ Ctrl + O æ‰“å¼€æ–‡ä»¶ Ctrl + S ä¿å­˜æ–‡ä»¶ Ctrl + Shift + S å¦å­˜ä¸º Ctrl + K S ä¿å­˜æ‰€æœ‰å½“å‰å·²ç»æ‰“å¼€çš„æ–‡ä»¶ Ctrl + F4 å…³é—­å½“å‰ç¼–è¾‘çª—å£ Ctrl + K Ctrl + W å…³é—­æ‰€æœ‰ç¼–è¾‘çª—å£ Ctrl + Shift + T æ’¤é”€æœ€è¿‘å…³é—­çš„ä¸€ä¸ªæ–‡ä»¶ç¼–è¾‘çª—å£ Ctrl + K Enter ä¿æŒå¼€å¯ Ctrl + Shift + Tab è°ƒå‡ºæœ€è¿‘æ‰“å¼€çš„æ–‡ä»¶åˆ—è¡¨ï¼Œé‡å¤æŒ‰ä¼šåˆ‡æ¢ Ctrl + Tab ä¸ä¸Šé¢ä¸€è‡´ï¼Œé¡ºåºä¸ä¸€è‡´ Ctrl + K P å¤åˆ¶å½“å‰æ‰“å¼€æ–‡ä»¶çš„å­˜æ”¾è·¯å¾„ Ctrl + K R æ‰“å¼€å½“å‰ç¼–è¾‘æ–‡ä»¶å­˜æ”¾ä½ç½®ã€æ–‡ä»¶ç®¡ç†å™¨ã€‘ Ctrl + K O åœ¨æ–°çš„ç¼–è¾‘å™¨ä¸­æ‰“å¼€å½“å‰ç¼–è¾‘çš„æ–‡ä»¶ 2.9. æ˜¾ç¤º å¿«æ·é”® ä½œç”¨ F11 åˆ‡æ¢å…¨å±æ¨¡å¼ Shift + Alt + 1 åˆ‡æ¢ç¼–è¾‘å¸ƒå±€ã€ç›®å‰æ— æ•ˆã€‘ Ctrl + =/- æ”¾å¤§ / ç¼©å° Ctrl + B ä¾§è¾¹æ æ˜¾ç¤ºéšè— Ctrl + Shift + E èµ„æºè§†å›¾å’Œç¼–è¾‘è§†å›¾çš„ç„¦ç‚¹åˆ‡æ¢ Ctrl + Shift + F æ‰“å¼€å…¨å±€æœç´¢ Ctrl + Shift + G æ‰“å¼€Gitå¯è§†ç®¡ç† Ctrl + Shift + D æ‰“å¼€DeBugé¢æ¿ Ctrl + Shift + X æ‰“å¼€æ’ä»¶å¸‚åœºé¢æ¿ Ctrl + Shift + H åœ¨å½“å‰æ–‡ä»¶æ›¿æ¢æŸ¥è¯¢æ›¿æ¢ Ctrl + Shift + J å¼€å¯è¯¦ç»†æŸ¥è¯¢ Ctrl + Shift + V é¢„è§ˆMarkdownæ–‡ä»¶ã€ç¼–è¯‘åã€‘ Ctrl + K v åœ¨è¾¹æ æ‰“å¼€æ¸²æŸ“åçš„è§†å›¾ã€æ–°å»ºã€‘ 2.10. è°ƒè¯• å¿«æ·é”® ä½œç”¨ F9 æ·»åŠ è§£é™¤æ–­ç‚¹ F5 å¯åŠ¨è°ƒè¯•ã€ç»§ç»­ F11 / Shift + F11 å•æ­¥è¿›å…¥ / å•æ­¥è·³å‡º F10 å•æ­¥è·³è¿‡ Ctrl + K Ctrl + I æ˜¾ç¤ºæ‚¬æµ® 2.11. é›†æˆç»ˆç«¯ å¿«æ·é”® ä½œç”¨ Ctrl + ` æ‰“å¼€é›†æˆç»ˆç«¯ Ctrl + Shift + ` åˆ›å»ºä¸€ä¸ªæ–°çš„ç»ˆç«¯ Ctrl + Shift + C å¤åˆ¶æ‰€é€‰ Ctrl + Shift + V å¤åˆ¶åˆ°å½“å‰æ¿€æ´»çš„ç»ˆç«¯ Shift + PgUp / PgDown é¡µé¢ä¸Šä¸‹ç¿»å± Ctrl + Home / End æ»šåŠ¨åˆ°é¡µé¢å¤´éƒ¨æˆ–å°¾éƒ¨ ","categories":"","description":"","excerpt":"vscodeå¿«æ·é”® 1. åŸºæœ¬å¿«æ·é”® 1.1. VsCode å¿«æ·é”®æœ‰äº”ç§ç»„åˆæ–¹å¼ Ctrl + Shift + ? : â€¦","ref":"/linux-notes/keymap/vscode-keymap/","tags":["å¿«æ·é”®"],"title":"vscodeå¿«æ·é”®"},{"body":"1. kubectlçš„å®‰è£… curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„kubectlï¼Œä¾‹å¦‚ï¼šv1.9.0\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ 2. é…ç½®k8sé›†ç¾¤ç¯å¢ƒ 2.1. å‘½ä»¤è¡Œæ–¹å¼ 2.1.1 éå®‰å…¨æ–¹å¼ kubectl config set-cluster k8s --server=http://\u003curl\u003e kubectl config set-context \u003cNAMESPACE\u003e --cluster=k8s --namespace=\u003cNAMESPACE\u003e kubectl config use-context \u003cNAMESPACE\u003e 2.1.2 å®‰å…¨æ–¹å¼ kubectl config set-cluster k8s --server=https://\u003curl\u003e --insecure-skip-tls-verify=true kubectl config set-credentials k8s-user --username=\u003cusername\u003e --password=\u003cpassword\u003e kubectl config set-context \u003cNAMESPACE\u003e --cluster=k8s --user=k8s-user --namespace=\u003cNAMESPACE\u003e kubectl config use-context \u003cNAMESPACE\u003e 2.1.3 æŸ¥è¯¢å½“å‰é…ç½®ç¯å¢ƒ [root@test ]# kubectl cluster-info Kubernetes master is running at http://192.168.10.3:8081 2.2. æ·»åŠ é…ç½®æ–‡ä»¶çš„æ–¹å¼ å½“æ²¡æœ‰æŒ‡å®š --kubeconfigå‚æ•°å’Œ$KUBECONFIGçš„ç¯å¢ƒå˜é‡çš„æ—¶å€™ï¼Œä¼šé»˜è®¤è¯»å–${HOME}/.kube/configã€‚\nå› æ­¤åˆ›å»º${HOME}/.kube/configæ–‡ä»¶ï¼Œå¹¶åœ¨``${HOME}/.kube/ssl`ç›®å½•ä¸‹åˆ›å»ºca.pemã€cert.pemã€key.pemæ–‡ä»¶ã€‚\nå†…å®¹å¦‚ä¸‹ï¼š\napiVersion: v1 kind: Config clusters: - name: local cluster: certificate-authority: ./ssl/ca.pem server: https://192.168.10.3:6443 users: - name: kubelet user: client-certificate: ./ssl/cert.pem client-key: ./ssl/key.pem contexts: - context: cluster: local user: kubelet name: kubelet-cluster.local current-context: kubelet-cluster.local 3. kubectl config kubectl configå‘½ä»¤è¯´æ˜\n$ kubectl config --help Modify kubeconfig files using subcommands like \"kubectl config set current-context my-context\" The loading order follows these rules: 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used a list of paths (normal path delimitting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Available Commands: current-context Displays the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts rename-context Renames a context from the kubeconfig file. set Sets an individual value in a kubeconfig file set-cluster Sets a cluster entry in kubeconfig set-context Sets a context entry in kubeconfig set-credentials Sets a user entry in kubeconfig unset Unsets an individual value in a kubeconfig file use-context Sets the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file Usage: kubectl config SUBCOMMAND [options] Use \"kubectl \u003ccommand\u003e --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). 4. shellè‡ªåŠ¨è¡¥é½ source \u003c(kubectl completion bash) echo \"source \u003c(kubectl completion bash)\" \u003e\u003e ~/.bashrc å¦‚æœå‡ºç°ä»¥ä¸‹æŠ¥é”™\n# kubectlè‡ªåŠ¨è¡¥é½å¤±è´¥ kubectl _get_comp_words_by_ref : command not found è§£å†³æ–¹æ³•ï¼š\nyum install bash-completion -y source /etc/profile.d/bash_completion.sh å‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl/ ","categories":"","description":"","excerpt":"1. kubectlçš„å®‰è£… curl -LO â€¦","ref":"/kubernetes-notes/operation/kubectl/install-kubectl/","tags":["Kubernetes"],"title":"kubectlå®‰è£…ä¸é…ç½®"},{"body":"1. è™šæ‹ŸåŒ– å€ŸåŠ©è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œç”¨æˆ·èƒ½ä»¥å•ä¸ªç‰©ç†ç¡¬ä»¶ç³»ç»Ÿä¸ºåŸºç¡€ï¼Œåˆ›å»ºå¤šä¸ªæ¨¡æ‹Ÿç¯å¢ƒæˆ–ä¸“ç”¨èµ„æºï¼Œå¹¶ä½¿ç”¨ä¸€æ¬¾åä¸ºâ€œHypervisorâ€ï¼ˆè™šæ‹Ÿæœºç›‘æ§ç¨‹åºï¼‰çš„è½¯ä»¶ç›´æ¥è¿æ¥åˆ°ç¡¬ä»¶ï¼Œä»è€Œå°†ä¸€ä¸ªç³»ç»Ÿåˆ’åˆ†ä¸ºä¸åŒã€å•ç‹¬è€Œå®‰å…¨çš„ç¯å¢ƒï¼Œå³è™šæ‹Ÿæœº (VM)ã€‚\nè™šæ‹ŸåŒ–æŠ€æœ¯å¯ä»¥é‡æ–°åˆ’åˆ†ITèµ„æºï¼Œæé«˜èµ„æºçš„åˆ©ç”¨ç‡ã€‚\n2. è™šæ‹ŸåŒ–çš„ç±»å‹ å…¨è™šæ‹ŸåŒ–ï¼ˆFull virtualizationï¼‰\nå…¨è™šæ‹ŸåŒ–ä½¿ç”¨æœªä¿®æ”¹çš„guestæ“ä½œç³»ç»Ÿç‰ˆæœ¬ï¼Œguestç›´æ¥ä¸CPUé€šä¿¡ï¼Œæ˜¯æœ€å¿«çš„è™šæ‹ŸåŒ–æ–¹æ³•ã€‚\nåŠè™šæ‹ŸåŒ–ï¼ˆParavirtualizationï¼‰\nåŠè™šæ‹ŸåŒ–ä½¿ç”¨ä¿®æ”¹è¿‡çš„guestæ“ä½œç³»ç»Ÿï¼Œguestä¸hypervisoré€šä¿¡ï¼Œhypervisorå°†guestçš„è°ƒç”¨ä¼ é€’ç»™CPUå’Œå…¶ä»–æ¥å£ã€‚å› ä¸ºé€šä¿¡ç»è¿‡hypervisorï¼Œå› æ­¤æ¯”å…¨è™šæ‹ŸåŒ–æ…¢ã€‚\n3. hypervisor hypervisoråˆç§°ä¸º virtual machine monitor (VMM)ï¼Œæ˜¯ä¸€ä¸ªåˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿæœºçš„ç¨‹åºã€‚è¢« hypervisor ç”¨æ¥è¿è¡Œä¸€ä¸ªæˆ–å¤šä¸ªè™šæ‹Ÿæœºçš„è®¡ç®—æœºç§°ä¸ºå®¿ä¸»æœºï¼ˆhost machineï¼‰ï¼Œè¿™äº›è™šæ‹Ÿæœºåˆ™ç§°ä¸ºå®¢æˆ·æœºï¼ˆguest machineï¼‰ã€‚\nåœ¨è™šæ‹ŸåŒ–æŠ€æœ¯ä¸­ï¼ŒHypervisorï¼ˆè™šæ‹Ÿæœºç›‘è§†å™¨ï¼‰æ˜¯ä¸€ç§è½¯ä»¶ã€å›ºä»¶æˆ–ç¡¬ä»¶ï¼Œèƒ½å¤Ÿåˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿæœºï¼ˆVMï¼‰ã€‚Hypervisor èµ·åˆ°ç‰©ç†ç¡¬ä»¶ä¸è™šæ‹Ÿæœºä¹‹é—´çš„ä¸­ä»‹ä½œç”¨ï¼Œç®¡ç†å’Œåˆ†é…ç‰©ç†èµ„æºï¼ˆå¦‚ CPUã€å†…å­˜ã€å­˜å‚¨å’Œç½‘ç»œï¼‰ç»™è™šæ‹Ÿæœºï¼Œå¹¶ç¡®ä¿æ¯ä¸ªè™šæ‹Ÿæœºçš„éš”ç¦»æ€§å’Œå®‰å…¨æ€§ã€‚\nHypervisor çš„åŠŸèƒ½\nè™šæ‹ŸåŒ–èµ„æºç®¡ç†ï¼š åˆ†é…ç‰©ç†ç¡¬ä»¶èµ„æºï¼ˆCPUã€å†…å­˜ã€å­˜å‚¨ã€ç½‘ç»œï¼‰ç»™è™šæ‹Ÿæœºã€‚ ç®¡ç†å’Œè°ƒåº¦è™šæ‹Ÿæœºçš„è¿è¡Œã€‚ è™šæ‹Ÿæœºéš”ç¦»ï¼š ç¡®ä¿è™šæ‹Ÿæœºä¹‹é—´çš„éš”ç¦»æ€§ï¼Œé˜²æ­¢ä¸€ä¸ªè™šæ‹Ÿæœºçš„æ•…éšœæˆ–å®‰å…¨é—®é¢˜å½±å“å…¶ä»–è™šæ‹Ÿæœºã€‚ ç¡¬ä»¶æŠ½è±¡ï¼š æŠ½è±¡åº•å±‚ç¡¬ä»¶ï¼Œä½¿è™šæ‹Ÿæœºå¯ä»¥è¿è¡Œåœ¨ä¸åŒçš„ç¡¬ä»¶å¹³å°ä¸Šï¼Œè€Œæ— éœ€ä¿®æ”¹è™šæ‹Ÿæœºå†…çš„æ“ä½œç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºã€‚ èµ„æºä¼˜åŒ–ï¼š åŠ¨æ€è°ƒæ•´è™šæ‹Ÿæœºèµ„æºï¼Œæä¾›è´Ÿè½½å‡è¡¡ã€èµ„æºå…±äº«å’ŒèŠ‚èƒ½åŠŸèƒ½ã€‚ é«˜å¯ç”¨æ€§å’Œå®¹é”™ï¼š æä¾›è™šæ‹Ÿæœºå¿«ç…§ã€å¤‡ä»½å’Œæ¢å¤åŠŸèƒ½ï¼Œç¡®ä¿é«˜å¯ç”¨æ€§å’Œæ•°æ®å®‰å…¨ã€‚ æ”¯æŒè™šæ‹Ÿæœºè¿ç§»å’Œå…‹éš†ï¼Œæ–¹ä¾¿è¿ç»´ç®¡ç†ã€‚ Hypervisor çš„ä¼˜ç‚¹\nç¡¬ä»¶åˆ©ç”¨ç‡ï¼šé€šè¿‡è™šæ‹ŸåŒ–ï¼Œå¯ä»¥æ›´é«˜æ•ˆåœ°åˆ©ç”¨ç‰©ç†ç¡¬ä»¶èµ„æºï¼Œå‡å°‘ç¡¬ä»¶æµªè´¹ã€‚ çµæ´»æ€§ï¼šè™šæ‹Ÿæœºå¯ä»¥è½»æ¾åˆ›å»ºã€åˆ é™¤å’Œè¿ç§»ï¼Œæ–¹ä¾¿å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²ã€‚ éš”ç¦»æ€§ï¼šè™šæ‹Ÿæœºä¹‹é—´ç›¸äº’éš”ç¦»ï¼Œæé«˜ç³»ç»Ÿå®‰å…¨æ€§å’Œç¨³å®šæ€§ã€‚ æˆæœ¬æ•ˆç›Šï¼šå‡å°‘å¯¹ç‰©ç†ç¡¬ä»¶çš„éœ€æ±‚ï¼Œé™ä½ç¡¬ä»¶å’Œç»´æŠ¤æˆæœ¬ã€‚ 4. kvm kvm(Kernel-based Virtual Machine)æ˜¯Linuxå†…æ ¸çš„è™šæ‹ŸåŒ–æ¨¡å—ï¼Œå¯ä»¥åˆ©ç”¨Linuxå†…æ ¸çš„åŠŸèƒ½æ¥ä½œä¸ºhypervisorã€‚\nKVMæœ¬èº«ä¸è¿›è¡Œæ¨¡æ‹Ÿï¼Œè€Œæ˜¯æš´éœ²ä¸€ä¸ª/dev/kvmæ¥å£ã€‚\nä½¿ç”¨KVMï¼Œå¯ä»¥åœ¨Linuxçš„é•œåƒä¸Š\n5. qemu QEMUï¼ˆQuick Emulatorï¼‰æ˜¯ä¸€ä¸ªå¼€æºçš„ç¡¬ä»¶è™šæ‹ŸåŒ–å’Œä»¿çœŸå™¨è½¯ä»¶ã€‚å®ƒè¢«å¹¿æ³›ç”¨äºåˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿæœºï¼Œæ”¯æŒå¤šç§ä¸åŒçš„ç¡¬ä»¶å¹³å°å’Œæ“ä½œç³»ç»Ÿã€‚ä»¥ä¸‹æ˜¯å¯¹ QEMU çš„è¯¦ç»†ä»‹ç»ï¼š\nQEMU çš„ä¸»è¦ç‰¹ç‚¹\nè™šæ‹ŸåŒ–å’Œä»¿çœŸï¼š è™šæ‹ŸåŒ–ï¼šQEMU èƒ½å¤Ÿåˆ©ç”¨ç¡¬ä»¶è™šæ‹ŸåŒ–æŠ€æœ¯ï¼ˆå¦‚ Intel VT-x å’Œ AMD-Vï¼‰æ¥è¿è¡Œè™šæ‹Ÿæœºã€‚é€šè¿‡ç¡¬ä»¶è¾…åŠ©è™šæ‹ŸåŒ–ï¼ŒQEMU å¯ä»¥æä¾›æ¥è¿‘åŸç”Ÿæ€§èƒ½çš„è™šæ‹Ÿæœºè¿è¡Œç¯å¢ƒã€‚ ä»¿çœŸï¼šQEMU è¿˜å¯ä»¥å®Œå…¨åœ¨è½¯ä»¶ä¸­ä»¿çœŸç¡¬ä»¶ï¼Œæ— éœ€ç¡¬ä»¶è™šæ‹ŸåŒ–æ”¯æŒã€‚è¿™ç§æ¨¡å¼ä¸‹ï¼ŒQEMU èƒ½å¤Ÿä»¿çœŸå¤šç§ä¸åŒçš„ CPU æ¶æ„ï¼ˆå¦‚ x86ã€ARMã€MIPSã€PowerPC ç­‰ï¼‰ï¼Œé€‚ç”¨äºè·¨å¹³å°å¼€å‘å’Œæµ‹è¯•ã€‚ å¤šç§å¹³å°æ”¯æŒï¼š QEMU æ”¯æŒå¤šç§ä¸åŒçš„ä¸»æœºå¹³å°å’Œç›®æ ‡å¹³å°ï¼Œèƒ½å¤Ÿåœ¨ä¸€ä¸ªå¹³å°ä¸Šè¿è¡Œä¸åŒæ¶æ„çš„æ“ä½œç³»ç»Ÿã€‚è¿™ä½¿å¾— QEMU æˆä¸ºè·¨å¹³å°å¼€å‘å’Œæµ‹è¯•çš„ç†æƒ³å·¥å…·ã€‚ ä¸ KVM é›†æˆï¼š QEMU å¯ä»¥ä¸å†…æ ¸è™šæ‹Ÿæœºï¼ˆKVMï¼‰é›†æˆä½¿ç”¨ï¼Œä»¥æé«˜è™šæ‹ŸåŒ–æ€§èƒ½ã€‚KVM æä¾›äº†åŸºäº Linux å†…æ ¸çš„é«˜æ•ˆè™šæ‹ŸåŒ–è§£å†³æ–¹æ¡ˆï¼Œè€Œ QEMU æä¾›äº†å¼ºå¤§çš„è™šæ‹Ÿæœºç®¡ç†å’Œä»¿çœŸåŠŸèƒ½ã€‚ è®¾å¤‡ä»¿çœŸï¼š QEMU æä¾›äº†ä¸°å¯Œçš„è®¾å¤‡ä»¿çœŸåŠŸèƒ½ï¼ŒåŒ…æ‹¬ç½‘ç»œæ¥å£ã€ç£ç›˜å­˜å‚¨ã€å›¾å½¢æ˜¾ç¤ºã€USB è®¾å¤‡ç­‰ã€‚è¿™äº›è®¾å¤‡ä»¿çœŸä½¿å¾—è™šæ‹Ÿæœºèƒ½å¤Ÿæ¨¡æ‹ŸçœŸå®ç¡¬ä»¶ç¯å¢ƒï¼Œæ–¹ä¾¿å¼€å‘å’Œæµ‹è¯•ã€‚ å¿«ç…§å’Œæ¢å¤ï¼š QEMU æ”¯æŒè™šæ‹Ÿæœºå¿«ç…§åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·åœ¨ç‰¹å®šæ—¶åˆ»ä¿å­˜è™šæ‹Ÿæœºçš„çŠ¶æ€ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ¢å¤åˆ°è¯¥çŠ¶æ€ã€‚è¿™å¯¹äºè°ƒè¯•å’Œæµ‹è¯•éå¸¸æœ‰ç”¨ã€‚ 6. libvirt Libvirt æ˜¯ä¸€ä¸ªå¼€æºçš„è™šæ‹ŸåŒ–ç®¡ç†æ¡†æ¶å’Œå·¥å…·é›†ï¼Œç”¨äºç®¡ç†ä¸åŒçš„è™šæ‹ŸåŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ KVMã€QEMUã€Xenã€VMware ESXiã€Hyper-V ä»¥åŠå…¶ä»–ä¸€äº›è™šæ‹ŸåŒ–å¹³å°ã€‚å®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ APIï¼Œç”¨äºåˆ›å»ºã€ç®¡ç†å’Œç›‘æ§è™šæ‹Ÿæœºï¼Œä½¿å¾—å¯¹ä¸åŒè™šæ‹ŸåŒ–æŠ€æœ¯çš„æ“ä½œæ›´åŠ ç®€åŒ–å’Œä¸€è‡´ã€‚\n6.1. ç‰¹ç‚¹ ç»Ÿä¸€æ¥å£ æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ API å’Œå‘½ä»¤è¡Œå·¥å…·ï¼Œä½¿ç”¨æˆ·å’Œå¼€å‘è€…å¯ä»¥é€šè¿‡ä¸€è‡´çš„æ¥å£ç®¡ç†ä¸åŒçš„è™šæ‹ŸåŒ–æŠ€æœ¯ã€‚ å¤šç§è™šæ‹ŸåŒ–æŠ€æœ¯æ”¯æŒï¼š æ”¯æŒå¤šç§è™šæ‹ŸåŒ–å¹³å°ï¼Œå¦‚ KVMã€QEMUã€Xenã€VMware ESXiã€Hyper-Vã€LXCï¼ˆLinux å®¹å™¨ï¼‰ç­‰ï¼Œæä¾›è·¨å¹³å°çš„è™šæ‹ŸåŒ–ç®¡ç†åŠŸèƒ½ã€‚ ç®¡ç†å’Œç›‘æ§ï¼š æä¾›å¼ºå¤§çš„ç®¡ç†å’Œç›‘æ§åŠŸèƒ½ï¼ŒåŒ…æ‹¬åˆ›å»ºã€å¯åŠ¨ã€åœæ­¢ã€è¿ç§»è™šæ‹Ÿæœºï¼Œä»¥åŠç®¡ç†å­˜å‚¨å’Œç½‘ç»œèµ„æºã€‚ XML é…ç½®ï¼š ä½¿ç”¨ XML é…ç½®æ–‡ä»¶æ¥å®šä¹‰è™šæ‹Ÿæœºå’Œèµ„æºï¼Œæä¾›çµæ´»å’Œå¯æ‰©å±•çš„é…ç½®æ–¹å¼ï¼Œæ˜“äºä¿å­˜å’Œç‰ˆæœ¬æ§åˆ¶ã€‚ å®‰å…¨æ€§ï¼š æ”¯æŒå¤šç§å®‰å…¨æœºåˆ¶ï¼Œå¦‚åŸºäº SELinux çš„å®‰å…¨ç­–ç•¥ï¼Œç¡®ä¿è™šæ‹Ÿæœºå’Œå®¿ä¸»ç³»ç»Ÿçš„éš”ç¦»æ€§å’Œå®‰å…¨æ€§ã€‚ ç½‘ç»œç®¡ç†ï¼š æä¾›ç½‘ç»œæ¡¥æ¥ã€NATã€è™šæ‹Ÿäº¤æ¢æœºç­‰ç½‘ç»œç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒå¤æ‚çš„ç½‘ç»œæ‹“æ‰‘é…ç½®ã€‚ å­˜å‚¨ç®¡ç†ï¼š æ”¯æŒå¤šç§å­˜å‚¨åç«¯ï¼Œå¦‚æœ¬åœ°ç£ç›˜ã€ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿï¼ˆNFSï¼‰ã€iSCSIã€LVM ç­‰ï¼Œæ–¹ä¾¿è™šæ‹Ÿæœºçš„å­˜å‚¨èµ„æºç®¡ç†ã€‚ 6.2. ä¸»è¦ç»„ä»¶ libvirtdï¼š Libvirt çš„å®ˆæŠ¤è¿›ç¨‹ï¼Œè´Ÿè´£å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚å’Œç®¡ç†è™šæ‹Ÿæœºã€‚é€šè¿‡è¿™ä¸ªå®ˆæŠ¤è¿›ç¨‹ï¼Œå¯ä»¥ä¸ä¸åŒçš„è™šæ‹ŸåŒ–åç«¯è¿›è¡Œäº¤äº’ã€‚ virshï¼š Libvirt çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œæä¾›äº†ä¸€ç³»åˆ—å‘½ä»¤ç”¨äºç®¡ç†è™šæ‹Ÿæœºå’Œèµ„æºã€‚é€šè¿‡ virshï¼Œç”¨æˆ·å¯ä»¥æ‰§è¡Œåˆ›å»ºã€å¯åŠ¨ã€åœæ­¢ã€è¿ç§»è™šæ‹Ÿæœºç­‰æ“ä½œã€‚ libvirt APIï¼š æä¾› C è¯­è¨€çš„åº“å’Œ APIï¼ŒåŒæ—¶æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€çš„ç»‘å®šï¼Œå¦‚ Pythonã€Perlã€Rubyã€Javaã€Go ç­‰ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥åœ¨ä¸åŒçš„ç¼–ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨ Libvirt çš„åŠŸèƒ½ã€‚ 6.3. virshå‘½ä»¤ # åˆ—å‡ºæ‰€æœ‰è™šæ‹Ÿæœº virsh list --all # å¯åŠ¨è™šæ‹Ÿæœº virsh start \u003cvm_name\u003e # å…³é—­è™šæ‹Ÿæœº virsh shutdown \u003cvm_name\u003e # å®šä¹‰æ–°çš„è™šæ‹Ÿæœº virsh define /path/to/vm.xml # åˆ›å»ºå¹¶å¯åŠ¨è™šæ‹Ÿæœº virsh create /path/to/vm.xml # è¿ç§»è™šæ‹Ÿæœº virsh migrate --live \u003cvm_name\u003e qemu+ssh://destination_host/system å‚è€ƒï¼š\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_getting_started_guide/chap-virtualization_getting_started-what_is_it https://en.wikipedia.org/wiki/Hypervisor https://www.linux-kvm.org/page/Main_Page http://www.linux-kvm.org/page/Documents https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine https://wiki.qemu.org/Index.html ","categories":"","description":"","excerpt":"1. è™šæ‹ŸåŒ– å€ŸåŠ©è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œç”¨æˆ·èƒ½ä»¥å•ä¸ªç‰©ç†ç¡¬ä»¶ç³»ç»Ÿä¸ºåŸºç¡€ï¼Œåˆ›å»ºå¤šä¸ªæ¨¡æ‹Ÿç¯å¢ƒæˆ–ä¸“ç”¨èµ„æºï¼Œå¹¶ä½¿ç”¨ä¸€æ¬¾åä¸ºâ€œHypervisorâ€ï¼ˆè™šæ‹Ÿæœºç›‘æ§ â€¦","ref":"/kubernetes-notes/kvm/vm-concept/","tags":["KubeVirt"],"title":"è™šæ‹ŸåŒ–ç›¸å…³æ¦‚å¿µ"},{"body":"1. Memcachedç®€ä»‹ Memcachedæ˜¯ä¸€ä¸ªå¼€æºçš„ï¼Œé«˜æ€§èƒ½ï¼Œåˆ†å¸ƒå¼å†…å­˜å¯¹è±¡ç¼“å­˜ç³»ç»Ÿã€‚\nMemcachedæ˜¯ä¸€ç§åŸºäºå†…å­˜çš„key-valueå­˜å‚¨ï¼Œç”¨æ¥å­˜å‚¨å°å—çš„ä»»æ„æ•°æ®ï¼ˆå­—ç¬¦ä¸²ã€å¯¹è±¡ï¼‰ã€‚è¿™äº›æ•°æ®å¯ä»¥æ˜¯æ•°æ®åº“è°ƒç”¨ã€APIè°ƒç”¨æˆ–è€…æ˜¯é¡µé¢æ¸²æŸ“çš„ç»“æœã€‚\nä¸€èˆ¬çš„ä½¿ç”¨ç›®çš„æ˜¯ï¼Œé€šè¿‡ç¼“å­˜æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼Œå‡å°‘æ•°æ®åº“è®¿é—®æ¬¡æ•°ï¼Œä»¥æé«˜åŠ¨æ€Webåº”ç”¨çš„é€Ÿåº¦ã€æé«˜å¯æ‰©å±•æ€§ã€‚\n1.1. ç‰¹å¾ memcachedä½œä¸ºé«˜é€Ÿè¿è¡Œçš„åˆ†å¸ƒå¼ç¼“å­˜æœåŠ¡å™¨ï¼Œå…·æœ‰ä»¥ä¸‹çš„ç‰¹ç‚¹ã€‚\nåè®®ç®€å• åŸºäºlibeventçš„äº‹ä»¶å¤„ç† å†…ç½®å†…å­˜å­˜å‚¨æ–¹å¼ memcachedä¸äº’ç›¸é€šä¿¡çš„åˆ†å¸ƒå¼ 2. å®‰è£…ä¸è¿è¡Œ 2.1. è‡ªåŠ¨å®‰è£… # For Redhat/Fedora yum install -y memcached # For Debian or Ubuntu apt-get install memcached 2.2. æºç å®‰è£… å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„Memcachedå¯ä»¥ä» https://github.com/memcached/memcached/wiki/ReleaseNotes åœ°å€ä¸‹è½½ã€‚\n# Memcached depends on libevent yum install libevent-devel # install wget https://memcached.org/latest [you might need to rename the file] tar -zxf memcached-1.x.x.tar.gz cd memcached-1.x.x ./configure --prefix=/usr/local/memcached make \u0026\u0026 make test \u0026\u0026 sudo make install é—®é¢˜\nå¦‚é‡ä»¥ä¸‹æŠ¥é”™ï¼Œå¯å†æ‰§è¡Œmake installã€‚\nSignal handled: Interrupt. ok 51 - shutdown ok 52 - stop_server /bin/sh:è¡Œ3: prove: æœªæ‰¾åˆ°å‘½ä»¤ make: *** [test] Error 127 2.3. éªŒè¯ ç¡®è®¤æ˜¯å¦å®‰è£…æˆåŠŸï¼Œå¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤\n/usr/local/memcached/bin/memcached -h 2.4. è¿è¡Œ 2.4.1. å‰å°è¿è¡Œ /usr/local/memcached/bin/memcached -p 11211 -m 64m -vv 2.4.2. åå°è¿è¡Œ /usr/local/memcached/bin/memcached -p 11211 -m 64m -d -c 102400 -t 8 -P /tmp/memcached.pid 2.5. è¿æ¥ $ telnet 127.0.0.1 11211 Trying 127.0.0.1... Connected to 127.0.0.1. Escape character is '^]'. set foo 0 0 3 ä¿å­˜å‘½ä»¤ bar æ•°æ® STORED ç»“æœ get foo å–å¾—å‘½ä»¤ VALUE foo 0 3 æ•°æ® bar æ•°æ® END ç»“æŸè¡Œ quit é€€å‡º 3. Memcachedè¿è¡Œå‚æ•° # /usr/local/memcached/bin/memcached -h memcached 1.5.12 -p, --port=\u003cnum\u003e TCP port to listen on (default: 11211) -U, --udp-port=\u003cnum\u003e UDP port to listen on (default: 0, off) -s, --unix-socket=\u003cfile\u003e UNIX socket to listen on (disables network support) -A, --enable-shutdown enable ascii \"shutdown\" command -a, --unix-mask=\u003cmask\u003e access mask for UNIX socket, in octal (default: 0700) -l, --listen=\u003caddr\u003e interface to listen on (default: INADDR_ANY) -d, --daemon run as a daemon -r, --enable-coredumps maximize core file limit -u, --user=\u003cuser\u003e assume identity of \u003cusername\u003e (only when run as root) -m, --memory-limit=\u003cnum\u003e item memory in megabytes (default: 64 MB) -M, --disable-evictions return error on memory exhausted instead of evicting -c, --conn-limit=\u003cnum\u003e max simultaneous connections (default: 1024) -k, --lock-memory lock down all paged memory -v, --verbose verbose (print errors/warnings while in event loop) -vv very verbose (also print client commands/responses) -vvv extremely verbose (internal state transitions) -h, --help print this help and exit -i, --license print memcached and libevent license -V, --version print version and exit -P, --pidfile=\u003cfile\u003e save PID in \u003cfile\u003e, only used with -d option -f, --slab-growth-factor=\u003cnum\u003e chunk size growth factor (default: 1.25) -n, --slab-min-size=\u003cbytes\u003e min space used for key+value+flags (default: 48) -L, --enable-largepages try to use large memory pages (if available) -D \u003cchar\u003e Use \u003cchar\u003e as the delimiter between key prefixes and IDs. This is used for per-prefix stats reporting. The default is \":\" (colon). If this option is specified, stats collection is turned on automatically; if not, then it may be turned on by sending the \"stats detail on\" command to the server. -t, --threads=\u003cnum\u003e number of threads to use (default: 4) -R, --max-reqs-per-event maximum number of requests per event, limits the requests processed per connection to prevent starvation (default: 20) -C, --disable-cas disable use of CAS -b, --listen-backlog=\u003cnum\u003e set the backlog queue limit (default: 1024) -B, --protocol=\u003cname\u003e protocol - one of ascii, binary, or auto (default) -I, --max-item-size=\u003cnum\u003e adjusts max item size (default: 1mb, min: 1k, max: 128m) -F, --disable-flush-all disable flush_all command -X, --disable-dumping disable stats cachedump and lru_crawler metadump -o, --extended comma separated list of extended options most options have a 'no_' prefix to disable - maxconns_fast: immediately close new connections after limit - hashpower: an integer multiplier for how large the hash table should be. normally grows at runtime. set based on \"STAT hash_power_level\" - tail_repair_time: time in seconds for how long to wait before forcefully killing LRU tail item. disabled by default; very dangerous option. - hash_algorithm: the hash table algorithm default is murmur3 hash. options: jenkins, murmur3 - lru_crawler: enable LRU Crawler background thread - lru_crawler_sleep: microseconds to sleep between items default is 100. - lru_crawler_tocrawl: max items to crawl per slab per run default is 0 (unlimited) - lru_maintainer: enable new LRU system + background thread - hot_lru_pct: pct of slab memory to reserve for hot lru. (requires lru_maintainer) - warm_lru_pct: pct of slab memory to reserve for warm lru. (requires lru_maintainer) - hot_max_factor: items idle \u003e cold lru age * drop from hot lru. - warm_max_factor: items idle \u003e cold lru age * this drop from warm. - temporary_ttl: TTL's below get separate LRU, can't be evicted. (requires lru_maintainer) - idle_timeout: timeout for idle connections - slab_chunk_max: (EXPERIMENTAL) maximum slab size. use extreme care. - watcher_logbuf_size: size in kilobytes of per-watcher write buffer. - worker_logbuf_size: size in kilobytes of per-worker-thread buffer read by background thread, then written to watchers. - track_sizes: enable dynamic reports for 'stats sizes' command. - no_inline_ascii_resp: save up to 24 bytes per item. small perf hit in ASCII, no perf difference in binary protocol. speeds up all sets. - no_hashexpand: disables hash table expansion (dangerous) - modern: enables options which will be default in future. currently: nothing - no_modern: uses defaults of previous major version (1.4.x) å¸¸ç”¨å‚æ•°ï¼š\n-dæ˜¯å¯åŠ¨ä¸€ä¸ªå®ˆæŠ¤è¿›ç¨‹ï¼› -mæ˜¯åˆ†é…ç»™Memcacheä½¿ç”¨çš„å†…å­˜æ•°é‡ï¼Œå•ä½æ˜¯MBï¼› -uæ˜¯è¿è¡ŒMemcacheçš„ç”¨æˆ·ï¼› -læ˜¯ç›‘å¬çš„æœåŠ¡å™¨IPåœ°å€ï¼Œå¯ä»¥æœ‰å¤šä¸ªåœ°å€ï¼› -pæ˜¯è®¾ç½®Memcacheç›‘å¬çš„ç«¯å£ï¼Œï¼Œæœ€å¥½æ˜¯1024ä»¥ä¸Šçš„ç«¯å£ï¼› -cæ˜¯æœ€å¤§è¿è¡Œçš„å¹¶å‘è¿æ¥æ•°ï¼Œé»˜è®¤æ˜¯1024ï¼› -tæ˜¯çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4ï¼› -Pæ˜¯è®¾ç½®ä¿å­˜Memcacheçš„pidæ–‡ä»¶ã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/memcached/memcached/wiki/Overview https://github.com/memcached/memcached/wiki/Install http://www.runoob.com/memcached/memcached-tutorial.html ","categories":"","description":"","excerpt":"1. Memcachedç®€ä»‹ Memcachedæ˜¯ä¸€ä¸ªå¼€æºçš„ï¼Œé«˜æ€§èƒ½ï¼Œåˆ†å¸ƒå¼å†…å­˜å¯¹è±¡ç¼“å­˜ç³»ç»Ÿã€‚\nMemcachedæ˜¯ä¸€ç§åŸºäºå†…å­˜ â€¦","ref":"/linux-notes/memcached/memcached/","tags":["Memcached"],"title":"Memcachedçš„ä½¿ç”¨"},{"body":"1. åŸºç¡€çŸ¥è¯† 1.1. åè®® è®¡ç®—æœºä¸ç½‘ç»œè®¾å¤‡è¦ç›¸äº’é€šä¿¡ï¼Œå¿…é¡»åŸºäºç›¸åŒçš„æ–¹æ³•ã€‚æ¯”å¦‚ï¼Œå¦‚ä½•æ¢æµ‹åˆ°é€šä¿¡ç›®æ ‡ï¼Œä½¿ç”¨å“ªç§è¯­è¨€é€šä¿¡ï¼Œå¦‚ä½•ç»“æŸé€šä¿¡ç­‰è§„åˆ™è¦äº‹å…ˆç¡®å®šã€‚\nä¸åŒç¡¬ä»¶ï¼Œæ“ä½œç³»ç»Ÿä¹‹é—´çš„é€šä¿¡éƒ½éœ€è¦ä¸€ç§è§„åˆ™ï¼Œæˆ‘ä»¬å°†è¿™ç§äº‹å…ˆçº¦å®šå¥½çš„è§„åˆ™ç§°ä¹‹ä¸ºåè®®ã€‚\n1.2. åœ°å€ åœ°å€ï¼šåœ¨æŸä¸€èŒƒå›´å†…ç¡®è®¤çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå³æ•°æ®åŒ…ä¼ åˆ°æŸä¸€ä¸ªèŒƒå›´ï¼Œéœ€è¦æœ‰ä¸€ä¸ªæ˜ç¡®å”¯ä¸€çš„ç›®æ ‡åœ°å€ã€‚\nç±»å‹ å±‚ åœ°å€ è¯´æ˜ ç«¯å£å· ä¼ è¾“å±‚ ç¨‹åºåœ°å€ åŒä¸€ä¸ªè®¡ç®—æœºä¸­ä¸åŒçš„åº”ç”¨ç¨‹åº IPåœ°å€ ç½‘ç»œå±‚ ä¸»æœºåœ°å€ è¯†åˆ«TCP/IPç½‘ç»œä¸­ä¸åŒçš„ä¸»æœºæˆ–è·¯ç”±å™¨ MACåœ°å€ æ•°æ®é“¾è·¯å±‚ ç‰©ç†åœ°å€ åœ¨åŒä¸€ä¸ªæ•°æ®é“¾è·¯ä¸­è¯†åˆ«ä¸åŒçš„è®¡ç®—æœº 1.3. ç½‘ç»œæ„æˆ æ„æˆè¦ç´  è¯´æ˜ ç½‘å¡ è¿å…¥ç½‘ç»œå¿…é¡»ä½¿ç”¨ç½‘å¡ï¼Œåˆç§°ç½‘ç»œæ¥å£å¡ã€‚ ä¸­ç»§å™¨ OSIç¬¬1å±‚ï¼Œç‰©ç†å±‚ä¸Šå»¶é•¿ç½‘ç»œçš„è®¾å¤‡ï¼Œå°†ç”µç¼†çš„ä¿¡å·æ”¾å¤§ä¼ ç»™å¦ä¸€ä¸ªç”µç¼†ã€‚ ç½‘æ¡¥/2å±‚äº¤æ¢æœº OSIç¬¬2å±‚ï¼Œæ•°æ®é“¾è·¯å±‚é¢ä¸Šè¿æ¥ä¸¤ä¸ªç½‘ç»œçš„è®¾å¤‡ï¼Œè¯†åˆ«æ•°æ®å¸§çš„å†…å®¹å¹¶è½¬å‘ç»™ç›¸é‚»çš„ç½‘æ®µï¼Œæ ¹æ®MACåœ°å€è¿›è¡Œå¤„ç†ã€‚ è·¯ç”±å™¨/3å±‚äº¤æ¢æœº OSIç¬¬3å±‚ï¼Œç½‘ç»œå±‚é¢è¿æ¥ä¸¤ä¸ªç½‘ç»œå¹¶å¯¹åˆ†ç»„æŠ¥æ–‡è¿›è¡Œè½¬å‘ï¼Œæ ¹æ®IPè¿›è¡Œå¤„ç†ã€‚ 4-7å±‚äº¤æ¢æœº ä¼ è¾“å±‚åˆ°åº”ç”¨å±‚ï¼Œä»¥TCPç­‰åè®®åˆ†ææ”¶å‘æ•°æ®ï¼Œè´Ÿè½½å‡è¡¡å™¨å°±æ˜¯å…¶ä¸­ä¸€ç§ã€‚ ç½‘å…³ å¯¹ä¼ è¾“å±‚åˆ°åº”ç”¨å±‚çš„æ•°æ®è¿›è¡Œè½¬æ¢å’Œè½¬å‘çš„è®¾å¤‡ï¼Œé€šå¸¸ä¼šä½¿ç”¨è¡¨ç¤ºå±‚æˆ–åº”ç”¨å±‚çš„ç½‘å…³æ¥å¤„ç†ä¸åŒåè®®ä¹‹é—´çš„ç¿»è¯‘å’Œé€šä¿¡ï¼Œä»£ç†æœåŠ¡å™¨ï¼ˆproxyï¼‰å°±æ˜¯åº”ç”¨ç½‘å…³çš„ä¸€ç§ã€‚ 2. OSIä¸TCP/IPå‚è€ƒæ¨¡å‹ 2.1. OSIä¸TCP/IPå‚è€ƒæ¨¡å‹å›¾ 2.2. OSIå‚è€ƒæ¨¡å‹åˆ†å±‚è¯´æ˜ 2.3. OSIå‚è€ƒæ¨¡å‹é€šä¿¡è¿‡ç¨‹ 1ã€æ‰“åŒ…æ•°æ®æ—¶ï¼Œæ¯ä¸€å±‚åœ¨å¤„ç†ä¸Šä¸€å±‚ä¼ è¿‡æ¥çš„æ•°æ®æ—¶ï¼Œä¼šåœ¨æ•°æ®ä¸Šé™„ä¸Šå½“å‰å±‚çš„é¦–éƒ¨ä¿¡æ¯åä¼ ç»™ä¸‹ä¸€å±‚ï¼›\n2ã€è§£åŒ…æ•°æ®æ—¶ï¼Œæ¯ä¸€å±‚åœ¨å¤„ç†ä¸‹ä¸€å±‚ä¼ è¿‡æ¥çš„æ•°æ®æ—¶ï¼Œä¼šå°†å½“å‰å±‚çš„é¦–éƒ¨ä¿¡æ¯ä¸æ•°æ®åˆ†å¼€ï¼Œå°†æ•°æ®ä¼ ç»™ä¸Šä¸€å±‚ã€‚\n3ã€æ•°æ®é€šä¿¡è¿‡ç¨‹\nåˆ†å±‚ æ¯å±‚çš„æ“ä½œ åº”ç”¨å±‚ åœ¨æ•°æ®å‰é¢åŠ é¦–éƒ¨ï¼Œé¦–éƒ¨åŒ…æ‹¬æ•°æ®å†…å®¹ã€æºåœ°å€å’Œç›®æ ‡åœ°å€ï¼ŒåŒæ—¶ä¹Ÿä¼šå¤„ç†å¼‚å¸¸çš„åé¦ˆä¿¡æ¯ã€‚ è¡¨ç¤ºå±‚ å°†ç‰¹æœ‰çš„æ•°æ®æ ¼å¼è½¬æ¢ä¸ºé€šç”¨çš„æ•°æ®æ ¼å¼ï¼ŒåŒæ—¶ä¹Ÿä¼šåŠ ä¸Šè¡¨ç¤ºå±‚çš„é¦–éƒ¨ä¿¡æ¯ä»¥ä¾›è§£æã€‚ ä¼šè¯å±‚ å¯¹ä½•æ—¶è¿æ¥ï¼Œä»¥ä½•ç§æ–¹å¼è¿æ¥ï¼Œè¿æ¥å¤šä¹…ï¼Œä½•æ—¶æ–­å¼€ç­‰åšè®°å½•ã€‚åŒæ—¶ä¹Ÿä¼šåŠ ä¼šè¯å±‚çš„é¦–éƒ¨ä¿¡æ¯ã€‚ ä¼ è¾“å±‚ å»ºç«‹è¿æ¥ï¼Œæ–­å¼€è¿æ¥ï¼Œç¡®è®¤æ•°æ®æ˜¯å¦å‘é€æˆåŠŸå’Œæ‰§è¡Œå¤±è´¥é‡å‘ä»»åŠ¡ã€‚ ç½‘ç»œå±‚ è´Ÿè´£å°†æ•°æ®å‘åˆ°ç›®æ ‡åœ°å€ï¼Œä¹ŸåŒ…å«é¦–éƒ¨ä¿¡æ¯ã€‚ æ•°æ®é“¾è·¯å±‚ é€šè¿‡ç‰©ç†çš„ä¼ è¾“ä»‹è´¨å®ç°æ•°æ®çš„ä¼ è¾“ã€‚ ç‰©ç†å±‚ å°†0/1è½¬æ¢æˆç‰©ç†çš„ä¼ è¾“ä»‹è´¨ï¼Œé€šè¿‡MACåœ°å€è¿›è¡Œä¼ è¾“ã€‚ 2.4. TCP/IPåº”ç”¨å±‚åè®® 2.4.1. é€šä¿¡æ¨¡å‹ 2.4.2. åº”ç”¨å±‚åè®®è¯´æ˜ åº”ç”¨ç±»å‹ åè®® åè®®è¯´æ˜ WWW HTTP,HTML ç”µå­é‚®ä»¶ SMTPï¼ŒMIME æ–‡ä»¶ä¼ è¾“ FTP è¿œç¨‹ç™»å½• TELNET,SSH ç½‘ç»œç®¡ç† SNMP,MIB 3. TCP/IPé€šä¿¡è¿‡ç¨‹ 3.1. æ•°æ®åŒ…ç»“æ„ 3.2. æ•°æ®æ‰“åŒ…å’Œè§£åŒ…è¿‡ç¨‹ 3.2.1. åŒ…çš„å°è£… 3.2.2. å‘é€ä¸æ¥æ”¶ 3.3. æ•°æ®åŒ…ä¼ è¾“è¿‡ç¨‹ æ–‡ç« ï¼š\nã€Šå›¾è§£TCP/IPã€‹ ","categories":"","description":"","excerpt":"1. åŸºç¡€çŸ¥è¯† 1.1. åè®® è®¡ç®—æœºä¸ç½‘ç»œè®¾å¤‡è¦ç›¸äº’é€šä¿¡ï¼Œå¿…é¡»åŸºäºç›¸åŒçš„æ–¹æ³•ã€‚æ¯”å¦‚ï¼Œå¦‚ä½•æ¢æµ‹åˆ°é€šä¿¡ç›®æ ‡ï¼Œä½¿ç”¨å“ªç§è¯­è¨€é€šä¿¡ï¼Œå¦‚ä½•ç»“æŸé€šä¿¡ç­‰è§„ â€¦","ref":"/linux-notes/tcpip/tcpip-basics/","tags":["TCPIP"],"title":"TCPIPåŸºç¡€"},{"body":"1. client-goç®€ä»‹ 1.1 client-goè¯´æ˜ â€‹\tclient-goæ˜¯ä¸€ä¸ªè°ƒç”¨kubernetesé›†ç¾¤èµ„æºå¯¹è±¡APIçš„å®¢æˆ·ç«¯ï¼Œå³é€šè¿‡client-goå®ç°å¯¹kubernetesé›†ç¾¤ä¸­èµ„æºå¯¹è±¡ï¼ˆåŒ…æ‹¬deploymentã€serviceã€ingressã€replicaSetã€podã€namespaceã€nodeç­‰ï¼‰çš„å¢åˆ æ”¹æŸ¥ç­‰æ“ä½œã€‚å¤§éƒ¨åˆ†å¯¹kubernetesè¿›è¡Œå‰ç½®APIå°è£…çš„äºŒæ¬¡å¼€å‘éƒ½é€šè¿‡client-goè¿™ä¸ªç¬¬ä¸‰æ–¹åŒ…æ¥å®ç°ã€‚\nâ€‹\tclient-goå®˜æ–¹æ–‡æ¡£ï¼šhttps://github.com/kubernetes/client-go\n1.2 ç¤ºä¾‹ä»£ç  git clone https://github.com/huweihuang/client-go.git cd client-go #ä¿è¯æœ¬åœ°HOMEç›®å½•æœ‰é…ç½®kubernetesé›†ç¾¤çš„é…ç½®æ–‡ä»¶ go run client-go.go client-go.go\npackage main import ( \"flag\" \"fmt\" \"os\" \"path/filepath\" \"time\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/clientcmd\" ) func main() { var kubeconfig *string if home := homeDir(); home != \"\" { kubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\") } else { kubeconfig = flag.String(\"kubeconfig\", \"\", \"absolute path to the kubeconfig file\") } flag.Parse() // uses the current context in kubeconfig config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) if err != nil { panic(err.Error()) } // creates the clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } for { pods, err := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{}) if err != nil { panic(err.Error()) } fmt.Printf(\"There are %d pods in the cluster\\n\", len(pods.Items)) time.Sleep(10 * time.Second) } } func homeDir() string { if h := os.Getenv(\"HOME\"); h != \"\" { return h } return os.Getenv(\"USERPROFILE\") // windows } 1.3 è¿è¡Œç»“æœ âœ go run client-go.go There are 9 pods in the cluster There are 7 pods in the cluster There are 7 pods in the cluster There are 7 pods in the cluster There are 7 pods in the cluster 2. client-goæºç åˆ†æ client-goæºç ï¼šhttps://github.com/kubernetes/client-go\nclient-goæºç ç›®å½•ç»“æ„\nThe kubernetes package contains the clientset to access Kubernetes API. The discovery package is used to discover APIs supported by a Kubernetes API server. The dynamic package contains a dynamic client that can perform generic operations on arbitrary Kubernetes API objects. The transport package is used to set up auth and start a connection. The tools/cache package is useful for writing controllers. 2.1 kubeconfig kubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\") è·å–kubernetesé…ç½®æ–‡ä»¶kubeconfigçš„ç»å¯¹è·¯å¾„ã€‚ä¸€èˆ¬è·¯å¾„ä¸º$HOME/.kube/configã€‚è¯¥æ–‡ä»¶ä¸»è¦ç”¨æ¥é…ç½®æœ¬åœ°è¿æ¥çš„kubernetesé›†ç¾¤ã€‚\nconfigå†…å®¹å¦‚ä¸‹ï¼š\napiVersion: v1 clusters: - cluster: server: http://\u003ckube-master-ip\u003e:8080 name: k8s contexts: - context: cluster: k8s namespace: default user: \"\" name: default current-context: default kind: Config preferences: {} users: [] 2.2 rest.config é€šè¿‡å‚æ•°ï¼ˆmasterçš„urlæˆ–è€…kubeconfigè·¯å¾„ï¼‰å’ŒBuildConfigFromFlagsæ–¹æ³•æ¥è·å–rest.Configå¯¹è±¡ï¼Œä¸€èˆ¬æ˜¯é€šè¿‡å‚æ•°kubeconfigçš„è·¯å¾„ã€‚\nconfig, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) BuildConfigFromFlagså‡½æ•°æºç \nk8s.io/client-go/tools/clientcmd/client_config.go\n// BuildConfigFromFlags is a helper function that builds configs from a master // url or a kubeconfig filepath. These are passed in as command line flags for cluster // components. Warnings should reflect this usage. If neither masterUrl or kubeconfigPath // are passed in we fallback to inClusterConfig. If inClusterConfig fails, we fallback // to the default config. func BuildConfigFromFlags(masterUrl, kubeconfigPath string) (*restclient.Config, error) { if kubeconfigPath == \"\" \u0026\u0026 masterUrl == \"\" { glog.Warningf(\"Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work.\") kubeconfig, err := restclient.InClusterConfig() if err == nil { return kubeconfig, nil } glog.Warning(\"error creating inClusterConfig, falling back to default config: \", err) } return NewNonInteractiveDeferredLoadingClientConfig( \u0026ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}, \u0026ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: masterUrl}}).ClientConfig() } 2.3 clientset é€šè¿‡*rest.Configå‚æ•°å’ŒNewForConfigæ–¹æ³•æ¥è·å–clientsetå¯¹è±¡ï¼Œclientsetæ˜¯å¤šä¸ªclientçš„é›†åˆï¼Œæ¯ä¸ªclientå¯èƒ½åŒ…å«ä¸åŒç‰ˆæœ¬çš„æ–¹æ³•è°ƒç”¨ã€‚\nclientset, err := kubernetes.NewForConfig(config) 2.3.1 NewForConfig NewForConfigå‡½æ•°å°±æ˜¯åˆå§‹åŒ–clientsetä¸­çš„æ¯ä¸ªclientã€‚\nk8s.io/client-go/kubernetes/clientset.go\n// NewForConfig creates a new Clientset for the given config. func NewForConfig(c *rest.Config) (*Clientset, error) { configShallowCopy := *c ... var cs Clientset cs.appsV1beta1, err = appsv1beta1.NewForConfig(\u0026configShallowCopy) ... cs.coreV1, err = corev1.NewForConfig(\u0026configShallowCopy) ... } 2.3.2 clientsetçš„ç»“æ„ä½“ k8s.io/client-go/kubernetes/clientset.go\n// Clientset contains the clients for groups. Each group has exactly one // version included in a Clientset. type Clientset struct { *discovery.DiscoveryClient admissionregistrationV1alpha1 *admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Client appsV1beta1 *appsv1beta1.AppsV1beta1Client appsV1beta2 *appsv1beta2.AppsV1beta2Client authenticationV1 *authenticationv1.AuthenticationV1Client authenticationV1beta1 *authenticationv1beta1.AuthenticationV1beta1Client authorizationV1 *authorizationv1.AuthorizationV1Client authorizationV1beta1 *authorizationv1beta1.AuthorizationV1beta1Client autoscalingV1 *autoscalingv1.AutoscalingV1Client autoscalingV2beta1 *autoscalingv2beta1.AutoscalingV2beta1Client batchV1 *batchv1.BatchV1Client batchV1beta1 *batchv1beta1.BatchV1beta1Client batchV2alpha1 *batchv2alpha1.BatchV2alpha1Client certificatesV1beta1 *certificatesv1beta1.CertificatesV1beta1Client coreV1 *corev1.CoreV1Client extensionsV1beta1 *extensionsv1beta1.ExtensionsV1beta1Client networkingV1 *networkingv1.NetworkingV1Client policyV1beta1 *policyv1beta1.PolicyV1beta1Client rbacV1 *rbacv1.RbacV1Client rbacV1beta1 *rbacv1beta1.RbacV1beta1Client rbacV1alpha1 *rbacv1alpha1.RbacV1alpha1Client schedulingV1alpha1 *schedulingv1alpha1.SchedulingV1alpha1Client settingsV1alpha1 *settingsv1alpha1.SettingsV1alpha1Client storageV1beta1 *storagev1beta1.StorageV1beta1Client storageV1 *storagev1.StorageV1Client } 2.3.3 clientset.Interface clientsetå®ç°äº†ä»¥ä¸‹çš„Interfaceï¼Œå› æ­¤å¯ä»¥é€šè¿‡è°ƒç”¨ä»¥ä¸‹æ–¹æ³•è·å¾—å…·ä½“çš„clientã€‚ä¾‹å¦‚ï¼š\npods, err := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{}) clientsetçš„æ–¹æ³•é›†æ¥å£\nk8s.io/client-go/kubernetes/clientset.go\ntype Interface interface { Discovery() discovery.DiscoveryInterface AdmissionregistrationV1alpha1() admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Interface // Deprecated: please explicitly pick a version if possible. Admissionregistration() admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Interface AppsV1beta1() appsv1beta1.AppsV1beta1Interface AppsV1beta2() appsv1beta2.AppsV1beta2Interface // Deprecated: please explicitly pick a version if possible. Apps() appsv1beta2.AppsV1beta2Interface AuthenticationV1() authenticationv1.AuthenticationV1Interface // Deprecated: please explicitly pick a version if possible. Authentication() authenticationv1.AuthenticationV1Interface AuthenticationV1beta1() authenticationv1beta1.AuthenticationV1beta1Interface AuthorizationV1() authorizationv1.AuthorizationV1Interface // Deprecated: please explicitly pick a version if possible. Authorization() authorizationv1.AuthorizationV1Interface AuthorizationV1beta1() authorizationv1beta1.AuthorizationV1beta1Interface AutoscalingV1() autoscalingv1.AutoscalingV1Interface // Deprecated: please explicitly pick a version if possible. Autoscaling() autoscalingv1.AutoscalingV1Interface AutoscalingV2beta1() autoscalingv2beta1.AutoscalingV2beta1Interface BatchV1() batchv1.BatchV1Interface // Deprecated: please explicitly pick a version if possible. Batch() batchv1.BatchV1Interface BatchV1beta1() batchv1beta1.BatchV1beta1Interface BatchV2alpha1() batchv2alpha1.BatchV2alpha1Interface CertificatesV1beta1() certificatesv1beta1.CertificatesV1beta1Interface // Deprecated: please explicitly pick a version if possible. Certificates() certificatesv1beta1.CertificatesV1beta1Interface CoreV1() corev1.CoreV1Interface // Deprecated: please explicitly pick a version if possible. Core() corev1.CoreV1Interface ExtensionsV1beta1() extensionsv1beta1.ExtensionsV1beta1Interface // Deprecated: please explicitly pick a version if possible. Extensions() extensionsv1beta1.ExtensionsV1beta1Interface NetworkingV1() networkingv1.NetworkingV1Interface // Deprecated: please explicitly pick a version if possible. Networking() networkingv1.NetworkingV1Interface PolicyV1beta1() policyv1beta1.PolicyV1beta1Interface // Deprecated: please explicitly pick a version if possible. Policy() policyv1beta1.PolicyV1beta1Interface RbacV1() rbacv1.RbacV1Interface // Deprecated: please explicitly pick a version if possible. Rbac() rbacv1.RbacV1Interface RbacV1beta1() rbacv1beta1.RbacV1beta1Interface RbacV1alpha1() rbacv1alpha1.RbacV1alpha1Interface SchedulingV1alpha1() schedulingv1alpha1.SchedulingV1alpha1Interface // Deprecated: please explicitly pick a version if possible. Scheduling() schedulingv1alpha1.SchedulingV1alpha1Interface SettingsV1alpha1() settingsv1alpha1.SettingsV1alpha1Interface // Deprecated: please explicitly pick a version if possible. Settings() settingsv1alpha1.SettingsV1alpha1Interface StorageV1beta1() storagev1beta1.StorageV1beta1Interface StorageV1() storagev1.StorageV1Interface // Deprecated: please explicitly pick a version if possible. Storage() storagev1.StorageV1Interface } 2.4 CoreV1Client æˆ‘ä»¬ä»¥clientsetä¸­çš„CoreV1Clientä¸ºä¾‹åšåˆ†æã€‚\né€šè¿‡ä¼ å…¥çš„é…ç½®ä¿¡æ¯rest.Configåˆå§‹åŒ–CoreV1Clientå¯¹è±¡ã€‚\nk8s.io/client-go/kubernetes/clientset.go\ncs.coreV1, err = corev1.NewForConfig(\u0026configShallowCopy) 2.4.1 corev1.NewForConfig k8s.io/client-go/kubernetes/typed/core/v1/core_client.go\n// NewForConfig creates a new CoreV1Client for the given config. func NewForConfig(c *rest.Config) (*CoreV1Client, error) { config := *c if err := setConfigDefaults(\u0026config); err != nil { return nil, err } client, err := rest.RESTClientFor(\u0026config) if err != nil { return nil, err } return \u0026CoreV1Client{client}, nil } corev1.NewForConfigæ–¹æ³•æœ¬è´¨æ˜¯è°ƒç”¨äº†rest.RESTClientFor(\u0026config)æ–¹æ³•åˆ›å»ºRESTClientå¯¹è±¡ï¼Œå³CoreV1Clientçš„æœ¬è´¨å°±æ˜¯ä¸€ä¸ªRESTClientå¯¹è±¡ã€‚\n2.4.2 CoreV1Clientç»“æ„ä½“ ä»¥ä¸‹æ˜¯CoreV1Clientç»“æ„ä½“çš„å®šä¹‰ï¼š\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\n// CoreV1Client is used to interact with features provided by the group. type CoreV1Client struct { restClient rest.Interface } CoreV1Clientå®ç°äº†CoreV1Interfaceçš„æ¥å£ï¼Œå³ä»¥ä¸‹æ–¹æ³•ï¼Œä»è€Œå¯¹kubernetesçš„èµ„æºå¯¹è±¡è¿›è¡Œå¢åˆ æ”¹æŸ¥çš„æ“ä½œã€‚\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\n//CoreV1Clientçš„æ–¹æ³• func (c *CoreV1Client) ComponentStatuses() ComponentStatusInterface {...} //ConfigMaps func (c *CoreV1Client) ConfigMaps(namespace string) ConfigMapInterface {...} //Endpoints func (c *CoreV1Client) Endpoints(namespace string) EndpointsInterface {...} func (c *CoreV1Client) Events(namespace string) EventInterface {...} func (c *CoreV1Client) LimitRanges(namespace string) LimitRangeInterface {...} //Namespaces func (c *CoreV1Client) Namespaces() NamespaceInterface {...} //Nodes func (c *CoreV1Client) Nodes() NodeInterface {...} func (c *CoreV1Client) PersistentVolumes() PersistentVolumeInterface {...} func (c *CoreV1Client) PersistentVolumeClaims(namespace string) PersistentVolumeClaimInterface {...} //Pods func (c *CoreV1Client) Pods(namespace string) PodInterface {...} func (c *CoreV1Client) PodTemplates(namespace string) PodTemplateInterface {...} //ReplicationControllers func (c *CoreV1Client) ReplicationControllers(namespace string) ReplicationControllerInterface {...} func (c *CoreV1Client) ResourceQuotas(namespace string) ResourceQuotaInterface {...} func (c *CoreV1Client) Secrets(namespace string) SecretInterface {...} //Services func (c *CoreV1Client) Services(namespace string) ServiceInterface {...} func (c *CoreV1Client) ServiceAccounts(namespace string) ServiceAccountInterface {...} 2.4.3 CoreV1Interface k8s.io/client-go/kubernetes/typed/core/v1/core_client.go\ntype CoreV1Interface interface { RESTClient() rest.Interface ComponentStatusesGetter ConfigMapsGetter EndpointsGetter EventsGetter LimitRangesGetter NamespacesGetter NodesGetter PersistentVolumesGetter PersistentVolumeClaimsGetter PodsGetter PodTemplatesGetter ReplicationControllersGetter ResourceQuotasGetter SecretsGetter ServicesGetter ServiceAccountsGetter } CoreV1Interfaceä¸­åŒ…å«äº†å„ç§kuberneteså¯¹è±¡çš„è°ƒç”¨æ¥å£ï¼Œä¾‹å¦‚PodsGetteræ˜¯å¯¹kubernetesä¸­podå¯¹è±¡å¢åˆ æ”¹æŸ¥æ“ä½œçš„æ¥å£ã€‚ServicesGetteræ˜¯å¯¹serviceå¯¹è±¡çš„æ“ä½œçš„æ¥å£ã€‚\n2.4.4 PodsGetter ä»¥ä¸‹æˆ‘ä»¬ä»¥PodsGetteræ¥å£ä¸ºä¾‹åˆ†æCoreV1Clientå¯¹podå¯¹è±¡çš„å¢åˆ æ”¹æŸ¥æ¥å£è°ƒç”¨ã€‚\nç¤ºä¾‹ä¸­çš„ä»£ç å¦‚ä¸‹ï¼š\npods, err := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{}) CoreV1().Pods()\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\nfunc (c *CoreV1Client) Pods(namespace string) PodInterface { return newPods(c, namespace) } newPods()\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// newPods returns a Pods func newPods(c *CoreV1Client, namespace string) *pods { return \u0026pods{ client: c.RESTClient(), ns: namespace, } } CoreV1().Pods()çš„æ–¹æ³•å®é™…ä¸Šæ˜¯è°ƒç”¨äº†newPods()çš„æ–¹æ³•ï¼Œåˆ›å»ºäº†ä¸€ä¸ªpodså¯¹è±¡ï¼Œpodså¯¹è±¡ç»§æ‰¿äº†rest.Interfaceæ¥å£ï¼Œå³æœ€ç»ˆçš„å®ç°æœ¬è´¨æ˜¯RESTClientçš„HTTPè°ƒç”¨ã€‚\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// pods implements PodInterface type pods struct { client rest.Interface ns string } podså¯¹è±¡å®ç°äº†PodInterfaceæ¥å£ã€‚PodInterfaceå®šä¹‰äº†podså¯¹è±¡çš„å¢åˆ æ”¹æŸ¥ç­‰æ–¹æ³•ã€‚\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// PodInterface has methods to work with Pod resources. type PodInterface interface { Create(*v1.Pod) (*v1.Pod, error) Update(*v1.Pod) (*v1.Pod, error) UpdateStatus(*v1.Pod) (*v1.Pod, error) Delete(name string, options *meta_v1.DeleteOptions) error DeleteCollection(options *meta_v1.DeleteOptions, listOptions meta_v1.ListOptions) error Get(name string, options meta_v1.GetOptions) (*v1.Pod, error) List(opts meta_v1.ListOptions) (*v1.PodList, error) Watch(opts meta_v1.ListOptions) (watch.Interface, error) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *v1.Pod, err error) PodExpansion } PodsGetter\nPodsGetterç»§æ‰¿äº†PodInterfaceçš„æ¥å£ã€‚\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// PodsGetter has a method to return a PodInterface. // A group's client should implement this interface. type PodsGetter interface { Pods(namespace string) PodInterface } Pods().List()\npods.List()æ–¹æ³•é€šè¿‡RESTClientçš„HTTPè°ƒç”¨æ¥å®ç°å¯¹kubernetesçš„podèµ„æºçš„è·å–ã€‚\nk8s.io/client-go/kubernetes/typed/core/v1/pod.go\n// List takes label and field selectors, and returns the list of Pods that match those selectors. func (c *pods) List(opts meta_v1.ListOptions) (result *v1.PodList, err error) { result = \u0026v1.PodList{} err = c.client.Get(). Namespace(c.ns). Resource(\"pods\"). VersionedParams(\u0026opts, scheme.ParameterCodec). Do(). Into(result) return } ä»¥ä¸Šåˆ†æäº†clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{})å¯¹podèµ„æºè·å–çš„è¿‡ç¨‹ï¼Œæœ€ç»ˆæ˜¯è°ƒç”¨RESTClientçš„æ–¹æ³•å®ç°ã€‚\n2.5 RESTClient ä»¥ä¸‹åˆ†æRESTClientçš„åˆ›å»ºè¿‡ç¨‹åŠä½œç”¨ã€‚\nRESTClientå¯¹è±¡çš„åˆ›å»ºåŒæ ·æ˜¯ä¾èµ–ä¼ å…¥çš„configä¿¡æ¯ã€‚\nk8s.io/client-go/kubernetes/typed/core/v1/core_client.go\nclient, err := rest.RESTClientFor(\u0026config) 2.5.1 rest.RESTClientFor k8s.io/client-go/rest/config.go\n// RESTClientFor returns a RESTClient that satisfies the requested attributes on a client Config // object. Note that a RESTClient may require fields that are optional when initializing a Client. // A RESTClient created by this method is generic - it expects to operate on an API that follows // the Kubernetes conventions, but may not be the Kubernetes API. func RESTClientFor(config *Config) (*RESTClient, error) { ... qps := config.QPS ... burst := config.Burst ... baseURL, versionedAPIPath, err := defaultServerUrlFor(config) ... transport, err := TransportFor(config) ... var httpClient *http.Client if transport != http.DefaultTransport { httpClient = \u0026http.Client{Transport: transport} if config.Timeout \u003e 0 { httpClient.Timeout = config.Timeout } } return NewRESTClient(baseURL, versionedAPIPath, config.ContentConfig, qps, burst, config.RateLimiter, httpClient) } RESTClientForå‡½æ•°è°ƒç”¨äº†NewRESTClientçš„åˆå§‹åŒ–å‡½æ•°ã€‚\n2.5.2 NewRESTClient k8s.io/client-go/rest/client.go\n// NewRESTClient creates a new RESTClient. This client performs generic REST functions // such as Get, Put, Post, and Delete on specified paths. Codec controls encoding and // decoding of responses from the server. func NewRESTClient(baseURL *url.URL, versionedAPIPath string, config ContentConfig, maxQPS float32, maxBurst int, rateLimiter flowcontrol.RateLimiter, client *http.Client) (*RESTClient, error) { base := *baseURL ... serializers, err := createSerializers(config) ... return \u0026RESTClient{ base: \u0026base, versionedAPIPath: versionedAPIPath, contentConfig: config, serializers: *serializers, createBackoffMgr: readExpBackoffConfig, Throttle: throttle, Client: client, }, nil } 2.5.3 RESTClientç»“æ„ä½“ ä»¥ä¸‹ä»‹ç»RESTClientçš„ç»“æ„ä½“å®šä¹‰ï¼ŒRESTClientç»“æ„ä½“ä¸­åŒ…å«äº†http.Clientï¼Œå³æœ¬è´¨ä¸ŠRESTClientå°±æ˜¯ä¸€ä¸ªhttp.Clientçš„å°è£…å®ç°ã€‚\nk8s.io/client-go/rest/client.go\n// RESTClient imposes common Kubernetes API conventions on a set of resource paths. // The baseURL is expected to point to an HTTP or HTTPS path that is the parent // of one or more resources. The server should return a decodable API resource // object, or an api.Status object which contains information about the reason for // any failure. // // Most consumers should use client.New() to get a Kubernetes API client. type RESTClient struct { // base is the root URL for all invocations of the client base *url.URL // versionedAPIPath is a path segment connecting the base URL to the resource root versionedAPIPath string // contentConfig is the information used to communicate with the server. contentConfig ContentConfig // serializers contain all serializers for underlying content type. serializers Serializers // creates BackoffManager that is passed to requests. createBackoffMgr func() BackoffManager // TODO extract this into a wrapper interface via the RESTClient interface in kubectl. Throttle flowcontrol.RateLimiter // Set specific behavior of the client. If not set http.DefaultClient will be used. Client *http.Client } 2.5.4 RESTClient.Interface RESTClientå®ç°äº†ä»¥ä¸‹çš„æ¥å£æ–¹æ³•ï¼š\nk8s.io/client-go/rest/client.go\n// Interface captures the set of operations for generically interacting with Kubernetes REST apis. type Interface interface { GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion } åœ¨è°ƒç”¨HTTPæ–¹æ³•ï¼ˆPost()ï¼ŒPut()ï¼ŒGet()ï¼ŒDelete() ï¼‰æ—¶ï¼Œå®é™…ä¸Šè°ƒç”¨äº†Verb(verb string)å‡½æ•°ã€‚\nk8s.io/client-go/rest/client.go\n// Verb begins a request with a verb (GET, POST, PUT, DELETE). // // Example usage of RESTClient's request building interface: // c, err := NewRESTClient(...) // if err != nil { ... } // resp, err := c.Verb(\"GET\"). // Path(\"pods\"). // SelectorParam(\"labels\", \"area=staging\"). // Timeout(10*time.Second). // Do() // if err != nil { ... } // list, ok := resp.(*api.PodList) // func (c *RESTClient) Verb(verb string) *Request { backoff := c.createBackoffMgr() if c.Client == nil { return NewRequest(nil, verb, c.base, c.versionedAPIPath, c.contentConfig, c.serializers, backoff, c.Throttle) } return NewRequest(c.Client, verb, c.base, c.versionedAPIPath, c.contentConfig, c.serializers, backoff, c.Throttle) } Verbå‡½æ•°è°ƒç”¨äº†NewRequestæ–¹æ³•ï¼Œæœ€åè°ƒç”¨Do()æ–¹æ³•å®ç°ä¸€ä¸ªHTTPè¯·æ±‚è·å–Resultã€‚\n2.6 æ€»ç»“ client-goå¯¹kubernetesèµ„æºå¯¹è±¡çš„è°ƒç”¨ï¼Œéœ€è¦å…ˆè·å–kubernetesçš„é…ç½®ä¿¡æ¯ï¼Œå³$HOME/.kube/configã€‚\næ•´ä¸ªè°ƒç”¨çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š\nkubeconfigâ†’rest.configâ†’clientsetâ†’å…·ä½“çš„client(CoreV1Client)â†’å…·ä½“çš„èµ„æºå¯¹è±¡(pod)â†’RESTClientâ†’http.Clientâ†’HTTPè¯·æ±‚çš„å‘é€åŠå“åº”\né€šè¿‡clientsetä¸­ä¸åŒçš„clientå’Œclientä¸­ä¸åŒèµ„æºå¯¹è±¡çš„æ–¹æ³•å®ç°å¯¹kubernetesä¸­èµ„æºå¯¹è±¡çš„å¢åˆ æ”¹æŸ¥ç­‰æ“ä½œï¼Œå¸¸ç”¨çš„clientæœ‰CoreV1Clientã€AppsV1beta1Clientã€ExtensionsV1beta1Clientç­‰ã€‚\n3. client-goå¯¹k8sèµ„æºçš„è°ƒç”¨ åˆ›å»ºclientset\n//è·å–kubeconfig kubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\") //åˆ›å»ºconfig\tconfig, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) //åˆ›å»ºclientset clientset, err := kubernetes.NewForConfig(config) //å…·ä½“çš„èµ„æºè°ƒç”¨è§ä»¥ä¸‹ä¾‹å­ 3.1 deployment //å£°æ˜deploymentå¯¹è±¡ var deployment *v1beta1.Deployment //æ„é€ deploymentå¯¹è±¡ //åˆ›å»ºdeployment deployment, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Create(\u003cdeployment\u003e) //æ›´æ–°deployment deployment, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Update(\u003cdeployment\u003e) //åˆ é™¤deployment err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Delete(\u003cdeployment.Name\u003e, \u0026meta_v1.DeleteOptions{}) //æŸ¥è¯¢deployment deployment, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Get(\u003cdeployment.Name\u003e, meta_v1.GetOptions{}) //åˆ—å‡ºdeployment deploymentList, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch deployment watchInterface, err := clientset.AppsV1beta1().Deployments(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.2 service //å£°æ˜serviceå¯¹è±¡ var service *v1.Service //æ„é€ serviceå¯¹è±¡ //åˆ›å»ºservice service, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Create(\u003cservice\u003e) //æ›´æ–°service service, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Update(\u003cservice\u003e) //åˆ é™¤service err := clientset.CoreV1().Services(\u003cnamespace\u003e).Delete(\u003cservice.Name\u003e, \u0026meta_v1.DeleteOptions{}) //æŸ¥è¯¢service service, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Get(\u003cservice.Name\u003e, meta_v1.GetOptions{}) //åˆ—å‡ºservice serviceList, err := clientset.CoreV1().Services(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch service watchInterface, err := clientset.CoreV1().Services(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.3 ingress //å£°æ˜ingresså¯¹è±¡ var ingress *v1beta1.Ingress //æ„é€ ingresså¯¹è±¡ //åˆ›å»ºingress ingress, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Create(\u003cingress\u003e) //æ›´æ–°ingress ingress, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Update(\u003cingress\u003e) //åˆ é™¤ingress err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Delete(\u003cingress.Name\u003e, \u0026meta_v1.DeleteOptions{}) //æŸ¥è¯¢ingress ingress, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Get(\u003cingress.Name\u003e, meta_v1.GetOptions{}) //åˆ—å‡ºingress ingressList, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch ingress watchInterface, err := clientset.ExtensionsV1beta1().Ingresses(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.4 replicaSet //å£°æ˜replicaSetå¯¹è±¡ var replicaSet *v1beta1.ReplicaSet //æ„é€ replicaSetå¯¹è±¡ //åˆ›å»ºreplicaSet replicaSet, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Create(\u003creplicaSet\u003e) //æ›´æ–°replicaSet replicaSet, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Update(\u003creplicaSet\u003e) //åˆ é™¤replicaSet err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Delete(\u003creplicaSet.Name\u003e, \u0026meta_v1.DeleteOptions{}) //æŸ¥è¯¢replicaSet replicaSet, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Get(\u003creplicaSet.Name\u003e, meta_v1.GetOptions{}) //åˆ—å‡ºreplicaSet replicaSetList, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch replicaSet watchInterface, err := clientset.ExtensionsV1beta1().ReplicaSets(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) æ–°ç‰ˆçš„kubernetesä¸­ä¸€èˆ¬é€šè¿‡deploymentæ¥åˆ›å»ºreplicaSetï¼Œå†é€šè¿‡replicaSetæ¥æ§åˆ¶podã€‚\n3.5 pod //å£°æ˜podå¯¹è±¡ var pod *v1.Pod //åˆ›å»ºpod pod, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Create(\u003cpod\u003e) //æ›´æ–°pod pod, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Update(\u003cpod\u003e) //åˆ é™¤pod err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Delete(\u003cpod.Name\u003e, \u0026meta_v1.DeleteOptions{}) //æŸ¥è¯¢pod pod, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Get(\u003cpod.Name\u003e, meta_v1.GetOptions{}) //åˆ—å‡ºpod podList, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch pod watchInterface, err := clientset.CoreV1().Pods(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) 3.6 statefulset //å£°æ˜statefulsetå¯¹è±¡ var statefulset *v1.StatefulSet //åˆ›å»ºstatefulset statefulset, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Create(\u003cstatefulset\u003e) //æ›´æ–°statefulset statefulset, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Update(\u003cstatefulset\u003e) //åˆ é™¤statefulset err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Delete(\u003cstatefulset.Name\u003e, \u0026meta_v1.DeleteOptions{}) //æŸ¥è¯¢statefulset statefulset, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Get(\u003cstatefulset.Name\u003e, meta_v1.GetOptions{}) //åˆ—å‡ºstatefulset statefulsetList, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).List(\u0026meta_v1.ListOptions{}) //watch statefulset watchInterface, err := clientset.AppsV1().StatefulSets(\u003cnamespace\u003e).Watch(\u0026meta_v1.ListOptions{}) â€‹\té€šè¿‡ä»¥ä¸Šå¯¹kubernetesçš„èµ„æºå¯¹è±¡çš„æ“ä½œå‡½æ•°å¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸ªèµ„æºå¯¹è±¡éƒ½æœ‰å¢åˆ æ”¹æŸ¥ç­‰æ–¹æ³•ï¼ŒåŸºæœ¬è°ƒç”¨é€»è¾‘ç±»ä¼¼ã€‚ä¸€èˆ¬äºŒæ¬¡å¼€å‘åªéœ€è¦åˆ›å»ºdeploymentã€serviceã€ingressä¸‰ä¸ªèµ„æºå¯¹è±¡å³å¯ï¼Œpodå¯¹è±¡ç”±deploymentåŒ…å«çš„replicaSetæ¥æ§åˆ¶åˆ›å»ºå’Œåˆ é™¤ã€‚å‡½æ•°è°ƒç”¨çš„å…¥å‚ä¸€èˆ¬åªæœ‰NAMESPACEå’ŒkubernetesObjectä¸¤ä¸ªå‚æ•°ï¼Œéƒ¨åˆ†æ“ä½œæœ‰Optionsçš„å‚æ•°ã€‚åœ¨åˆ›å»ºå‰ï¼Œéœ€è¦å¯¹èµ„æºå¯¹è±¡æ„é€ æ•°æ®ï¼Œå¯ä»¥ç†è§£ä¸ºç¼–è¾‘ä¸€ä¸ªèµ„æºå¯¹è±¡çš„yamlæ–‡ä»¶ï¼Œç„¶åé€šè¿‡kubectl create -f xxx.yamlæ¥åˆ›å»ºå¯¹è±¡ã€‚\nå‚è€ƒæ–‡æ¡£:\nhttps://github.com/kubernetes/client-go ","categories":"","description":"","excerpt":"1. client-goç®€ä»‹ 1.1 client-goè¯´æ˜ â€‹\tclient-goæ˜¯ä¸€ä¸ªè°ƒç”¨kubernetesé›†ç¾¤èµ„æºå¯¹è±¡APIçš„å®¢æˆ· â€¦","ref":"/kubernetes-notes/develop/client-go/","tags":["æºç åˆ†æ","Kubernetes"],"title":"client-goçš„ä½¿ç”¨åŠæºç åˆ†æ"},{"body":"1. æŸ¥çœ‹ç³»ç»ŸEventäº‹ä»¶ kubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e è¯¥å‘½ä»¤å¯ä»¥æ˜¾ç¤ºPodåˆ›å»ºæ—¶çš„é…ç½®å®šä¹‰ã€çŠ¶æ€ç­‰ä¿¡æ¯å’Œæœ€è¿‘çš„Eventäº‹ä»¶ï¼Œäº‹ä»¶ä¿¡æ¯å¯ç”¨äºæ’é”™ã€‚ä¾‹å¦‚å½“PodçŠ¶æ€ä¸ºPendingï¼Œå¯é€šè¿‡æŸ¥çœ‹Eventäº‹ä»¶ç¡®è®¤åŸå› ï¼Œä¸€èˆ¬åŸå› æœ‰å‡ ç§ï¼š\næ²¡æœ‰å¯ç”¨çš„Nodeå¯è°ƒåº¦ å¼€å¯äº†èµ„æºé…é¢ç®¡ç†å¹¶ä¸”å½“å‰Podçš„ç›®æ ‡èŠ‚ç‚¹ä¸Šæ°å¥½æ²¡æœ‰å¯ç”¨çš„èµ„æº æ­£åœ¨ä¸‹è½½é•œåƒï¼ˆé•œåƒæ‹‰å–è€—æ—¶å¤ªä¹…ï¼‰æˆ–é•œåƒä¸‹è½½å¤±è´¥ã€‚ kubectl describeè¿˜å¯ä»¥æŸ¥çœ‹å…¶å®ƒk8så¯¹è±¡ï¼šNODE,RC,Service,Namespace,Secretsã€‚\n1.1. Pod kubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e ä»¥ä¸‹æ˜¯å®¹å™¨çš„å¯åŠ¨å‘½ä»¤éé˜»å¡å¼å¯¼è‡´å®¹å™¨æŒ‚æ‰ï¼Œè¢«k8sé¢‘ç¹é‡å¯æ‰€äº§ç”Ÿçš„äº‹ä»¶ã€‚\nkubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e Events: FirstSeen LastSeen Count From SubobjectPath Reason Message â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€ â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€ 7m 7m 1 {scheduler } Scheduled Successfully assigned yangsc-1-0-0-index0 to 10.8.216.19 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Pulled Container image \"gcr.io/kube-system/pause:0.8.0\" already present on machine 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Created Created with docker id 84f133c324d0 7m 7m 1 {kubelet 10.8.216.19} containers{infra} Started Started with docker id 84f133c324d0 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 3f9f82abb145 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 3f9f82abb145 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id fb112e4002f4 7m 7m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id fb112e4002f4 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 613b119d4474 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 613b119d4474 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 25cb68d1fd3d 6m 6m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 25cb68d1fd3d 5m 5m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 7d9ee8610b28 5m 5m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 7d9ee8610b28 3m 3m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 88b9e8d582dd 3m 3m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 88b9e8d582dd 7m 1m 7 {kubelet 10.8.216.19} containers{yangsc0} Pulling Pulling image \"gcr.io/test/tcp-hello:1.0.0\" 1m 1m 1 {kubelet 10.8.216.19} containers{yangsc0} Started Started with docker id 089abff050e7 1m 1m 1 {kubelet 10.8.216.19} containers{yangsc0} Created Created with docker id 089abff050e7 7m 1m 7 {kubelet 10.8.216.19} containers{yangsc0} Pulled Successfully pulled image \"gcr.io/test/tcp-hello:1.0.0\" 6m 7s 34 {kubelet 10.8.216.19} containers{yangsc0} Backoff Back-off restarting failed docker container 1.2. NODE kubectl describe node 10.8.216.20 [root@FC-43745A-10 ~]# kubectl describe node 10.8.216.20 Name: 10.8.216.20 Labels: kubernetes.io/hostname=10.8.216.20,namespace/bcs-cc=true,namespace/myview=true CreationTimestamp: Mon, 17 Apr 2017 11:32:52 +0800 Phase: Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€ Ready True Fri, 18 Aug 2017 09:38:33 +0800 Tue, 02 May 2017 17:40:58 +0800 KubeletReady kubelet is posting ready status OutOfDisk False Fri, 18 Aug 2017 09:38:33 +0800 Mon, 17 Apr 2017 11:31:27 +0800 KubeletHasSufficientDisk kubelet has sufficient disk space available Addresses: 10.8.216.20,10.8.216.20 Capacity: cpu: 32 memory: 67323039744 pods: 40 System Info: Machine ID: 723bafc7f6764022972b3eae1ce6b198 System UUID: 4C4C4544-0042-4210-8044-C3C04F595631 Boot ID: da01f2e3-987a-425a-9ca7-1caaec35d1e5 Kernel Version: 3.10.0-327.28.3.el7.x86_64 OS Image: CentOS Linux 7 (Core) Container Runtime Version: docker://1.13.1 Kubelet Version: v1.1.1-xxx2-13.1+79c90c68bfb72f-dirty Kube-Proxy Version: v1.1.1-xxx2-13.1+79c90c68bfb72f-dirty ExternalID: 10.8.216.20 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ bcs-cc bcs-cc-api-0-0-1364-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) bcs-cc bcs-cc-api-0-0-1444-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) fw fw-demo2-0-0-1519-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) myview myview-api-0-0-1362-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) myview myview-api-0-0-1442-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) qa-ts-dna ts-dna-console3-0-0-1434-index0 1 (3%) 1 (3%) 4294967296 (6%) 4294967296 (6%) Allocated resources: (Total limits may be over 100%, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md) CPU Requests CPU Limits Memory Requests Memory Limits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6 (18%) 6 (18%) 25769803776 (38%) 25769803776 (38%) No events. 1.3. RC kubectl describe rc mytest-1-0-0 --namespace=test [root@FC-43745A-10 ~]# kubectl describe rc mytest-1-0-0 --namespace=test Name: mytest-1-0-0 Namespace: test Image(s): gcr.io/test/mywebcalculator:1.0.1 Selector: app=mytest,appVersion=1.0.0 Labels: app=mytest,appVersion=1.0.0,env=ts,zone=inner Replicas: 1 current / 1 desired Pods Status: 1 Running / 0 Waiting / 0 Succeeded / 0 Failed No volumes. Events: FirstSeen LastSeen Count From SubobjectPath Reason Message â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€ â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€ 20h 19h 9 {replication-controller } FailedCreate Error creating: Pod \"mytest-1-0-0-index0\" is forbidden: limited to 10 pods 20h 17h 7 {replication-controller } FailedCreate Error creating: pods \"mytest-1-0-0-index0\" already exists 20h 17h 4 {replication-controller } SuccessfulCreate Created pod: mytest-1-0-0-index0 1.4. NAMESPACE kubectl describe namespace test [root@FC-43745A-10 ~]# kubectl describe namespace test Name: test Labels: \u003cnone\u003e Status: Active Resource Quotas Resource Used Hard --- --- --- cpu 5 20 memory 1342177280 53687091200 persistentvolumeclaims 0 10 pods 4 10 replicationcontrollers 8 20 resourcequotas 1 1 secrets 3 10 services 8 20 No resource limits. 1.5. Service kubectl describe service xxx-containers-1-1-0 --namespace=test [root@FC-43745A-10 ~]# kubectl describe service xxx-containers-1-1-0 --namespace=test Name: xxx-containers-1-1-0 Namespace: test Labels: app=xxx-containers,appVersion=1.1.0,env=ts,zone=inner Selector: app=xxx-containers,appVersion=1.1.0 Type: ClusterIP IP: 10.254.46.42 Port: port-dna-tcp-35913 35913/TCP Endpoints: 10.0.92.17:35913 Port: port-l7-tcp-8080 8080/TCP Endpoints: 10.0.92.17:8080 Session Affinity: None No events. 2. æŸ¥çœ‹å®¹å™¨æ—¥å¿— 1ã€æŸ¥çœ‹æŒ‡å®špodçš„æ—¥å¿—\nkubectl logs \u003cpod_name\u003e kubectl logs -f \u003cpod_name\u003e #ç±»ä¼¼tail -fçš„æ–¹å¼æŸ¥çœ‹ 2ã€æŸ¥çœ‹ä¸Šä¸€ä¸ªpodçš„æ—¥å¿—\nkubectl logs -p \u003cpod_name\u003e 3ã€æŸ¥çœ‹æŒ‡å®špodä¸­æŒ‡å®šå®¹å™¨çš„æ—¥å¿—\nkubectl logs \u003cpod_name\u003e -c \u003ccontainer_name\u003e 4ã€kubectl logs --help\n[root@node5 ~]# kubectl logs --help Print the logs for a container in a pod. If the pod has only one container, the container name is optional. Usage: kubectl logs [-f] [-p] POD [-c CONTAINER] [flags] Aliases: logs, log Examples: # Return snapshot logs from pod nginx with only one container $ kubectl logs nginx # Return snapshot of previous terminated ruby container logs from pod web-1 $ kubectl logs -p -c ruby web-1 # Begin streaming the logs of the ruby container in pod web-1 $ kubectl logs -f -c ruby web-1 # Display only the most recent 20 lines of output in pod nginx $ kubectl logs --tail=20 nginx # Show all logs from pod nginx written in the last hour $ kubectl logs --since=1h nginx 3. æŸ¥çœ‹k8sæœåŠ¡æ—¥å¿— 3.1. journalctl åœ¨Linuxç³»ç»Ÿä¸Šsystemdç³»ç»Ÿæ¥ç®¡ç†kubernetesæœåŠ¡ï¼Œå¹¶ä¸”journalç³»ç»Ÿä¼šæ¥ç®¡æœåŠ¡ç¨‹åºçš„è¾“å‡ºæ—¥å¿—ï¼Œå¯ä»¥é€šè¿‡systemctl status æˆ–journalctl -u -fæ¥æŸ¥çœ‹kubernetesæœåŠ¡çš„æ—¥å¿—ã€‚\nå…¶ä¸­kubernetesç»„ä»¶åŒ…æ‹¬ï¼š\nk8sç»„ä»¶ æ¶‰åŠæ—¥å¿—å†…å®¹ å¤‡æ³¨ kube-apiserver kube-controller-manager Podæ‰©å®¹ç›¸å…³æˆ–RCç›¸å…³ kube-scheduler Podæ‰©å®¹ç›¸å…³æˆ–RCç›¸å…³ kubelet Podç”Ÿå‘½å‘¨æœŸç›¸å…³ï¼šåˆ›å»ºã€åœæ­¢ç­‰ etcd 3.2. æ—¥å¿—æ–‡ä»¶ ä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šæ—¥å¿—å­˜æ”¾ç›®å½•æ¥ä¿å­˜å’ŒæŸ¥çœ‹æ—¥å¿—\n--logtostderr=falseï¼šä¸è¾“å‡ºåˆ°stderr --log-dir=/var/log/kubernetes:æ—¥å¿—çš„å­˜æ”¾ç›®å½• --alsologtostderr=false:è®¾ç½®ä¸ºtrueè¡¨ç¤ºæ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶ä¹Ÿè¾“å‡ºåˆ°stderr --v=0:glogçš„æ—¥å¿—çº§åˆ« --vmodule=gfs*=2,test*=4ï¼šglogåŸºäºæ¨¡å—çš„è¯¦ç»†æ—¥å¿—çº§åˆ« 4. å¸¸è§é—®é¢˜ 4.1. PodçŠ¶æ€ä¸€ç›´ä¸ºPending kubectl describe \u003cpod_name\u003e --namespace=\u003cNAMESPACE\u003e æŸ¥çœ‹è¯¥PODçš„äº‹ä»¶ã€‚\næ­£åœ¨ä¸‹è½½é•œåƒä½†æ‹‰å–ä¸ä¸‹æ¥ï¼ˆé•œåƒæ‹‰å–è€—æ—¶å¤ªä¹…ï¼‰[ä¸€èˆ¬éƒ½æ˜¯è¯¥åŸå› ] æ²¡æœ‰å¯ç”¨çš„Nodeå¯è°ƒåº¦ å¼€å¯äº†èµ„æºé…é¢ç®¡ç†å¹¶ä¸”å½“å‰Podçš„ç›®æ ‡èŠ‚ç‚¹ä¸Šæ°å¥½æ²¡æœ‰å¯ç”¨çš„èµ„æº è§£å†³æ–¹æ³•ï¼š\næŸ¥çœ‹è¯¥PODæ‰€åœ¨å®¿ä¸»æœºä¸é•œåƒä»“åº“ä¹‹é—´çš„ç½‘ç»œæ˜¯å¦æœ‰é—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨æ‹‰å–é•œåƒ åˆ é™¤PODå®ä¾‹ï¼Œè®©PODè°ƒåº¦åˆ°åˆ«çš„å®¿ä¸»æœºä¸Š 4.2. Podåˆ›å»ºåä¸æ–­é‡å¯ kubectl get podsä¸­PodçŠ¶æ€ä¸€ä¼šrunningï¼Œä¸€ä¼šä¸æ˜¯ï¼Œä¸”RESTARTSæ¬¡æ•°ä¸æ–­å¢åŠ ã€‚\nä¸€èˆ¬åŸå› ä¸ºå®¹å™¨å¯åŠ¨å‘½ä»¤ä¸æ˜¯é˜»å¡å¼å‘½ä»¤ï¼Œå¯¼è‡´å®¹å™¨è¿è¡Œåé©¬ä¸Šé€€å‡ºã€‚\néé˜»å¡å¼å‘½ä»¤ï¼š\næœ¬èº«CMDæŒ‡å®šçš„å‘½ä»¤å°±æ˜¯éé˜»å¡å¼å‘½ä»¤ å°†æœåŠ¡å¯åŠ¨æ–¹å¼è®¾ç½®ä¸ºåå°è¿è¡Œ è§£å†³æ–¹æ³•ï¼š\n1ã€å°†å‘½ä»¤æ”¹ä¸ºé˜»å¡å¼å‘½ä»¤ï¼ˆå‰å°è¿è¡Œï¼‰ï¼Œä¾‹å¦‚ï¼šzkServer.sh start-foreground\n2ã€javaè¿è¡Œç¨‹åºçš„å¯åŠ¨è„šæœ¬å°† nohup xxx \u0026çš„nobupå’Œ\u0026å»æ‰ï¼Œä¾‹å¦‚ï¼š\nnohup JAVA_HOME/bin/java JAVA_OPTS -cp $CLASSPATH com.cnc.open.processor.Main \u0026 æ”¹ä¸ºï¼š\nJAVA_HOME/bin/java JAVA_OPTS -cp $CLASSPATH com.cnc.open.processor.Main æ–‡ç« å‚è€ƒã€ŠKubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. æŸ¥çœ‹ç³»ç»ŸEventäº‹ä»¶ kubectl describe pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e  â€¦","ref":"/kubernetes-notes/operation/kubernetes-troubleshooting/","tags":["Kubernetes"],"title":"Kubernetesé›†ç¾¤é—®é¢˜æ’æŸ¥"},{"body":"1. Kubernetesçš„æ€»æ¶æ„å›¾ 2. Kuberneteså„ä¸ªç»„ä»¶ä»‹ç» 2.1 kube-master[æ§åˆ¶èŠ‚ç‚¹] masterçš„å·¥ä½œæµç¨‹å›¾\nKubecfgå°†ç‰¹å®šçš„è¯·æ±‚ï¼Œæ¯”å¦‚åˆ›å»ºPodï¼Œå‘é€ç»™Kubernetes Clientã€‚ Kubernetes Clientå°†è¯·æ±‚å‘é€ç»™API serverã€‚ API Serveræ ¹æ®è¯·æ±‚çš„ç±»å‹ï¼Œæ¯”å¦‚åˆ›å»ºPodæ—¶storageç±»å‹æ˜¯podsï¼Œç„¶åä¾æ­¤é€‰æ‹©ä½•ç§REST Storage APIå¯¹è¯·æ±‚ä½œå‡ºå¤„ç†ã€‚ REST Storage APIå¯¹çš„è¯·æ±‚ä½œç›¸åº”çš„å¤„ç†ã€‚ å°†å¤„ç†çš„ç»“æœå­˜å…¥é«˜å¯ç”¨é”®å€¼å­˜å‚¨ç³»ç»ŸEtcdä¸­ã€‚ åœ¨API Serverå“åº”Kubecfgçš„è¯·æ±‚åï¼ŒSchedulerä¼šæ ¹æ®Kubernetes Clientè·å–é›†ç¾¤ä¸­è¿è¡ŒPodåŠMinion/Nodeä¿¡æ¯ã€‚ ä¾æ®ä»Kubernetes Clientè·å–çš„ä¿¡æ¯ï¼ŒSchedulerå°†æœªåˆ†å‘çš„Podåˆ†å‘åˆ°å¯ç”¨çš„Minion/NodeèŠ‚ç‚¹ä¸Šã€‚ 2.1.1 API Server[èµ„æºæ“ä½œå…¥å£] æä¾›äº†èµ„æºå¯¹è±¡çš„å”¯ä¸€æ“ä½œå…¥å£ï¼Œå…¶ä»–æ‰€æœ‰ç»„ä»¶éƒ½å¿…é¡»é€šè¿‡å®ƒæä¾›çš„APIæ¥æ“ä½œèµ„æºæ•°æ®ï¼Œåªæœ‰API Serverä¸å­˜å‚¨é€šä¿¡ï¼Œå…¶ä»–æ¨¡å—é€šè¿‡API Serverè®¿é—®é›†ç¾¤çŠ¶æ€ã€‚\nç¬¬ä¸€ï¼Œæ˜¯ä¸ºäº†ä¿è¯é›†ç¾¤çŠ¶æ€è®¿é—®çš„å®‰å…¨ã€‚\nç¬¬äºŒï¼Œæ˜¯ä¸ºäº†éš”ç¦»é›†ç¾¤çŠ¶æ€è®¿é—®çš„æ–¹å¼å’Œåç«¯å­˜å‚¨å®ç°çš„æ–¹å¼ï¼šAPI Serveræ˜¯çŠ¶æ€è®¿é—®çš„æ–¹å¼ï¼Œä¸ä¼šå› ä¸ºåç«¯å­˜å‚¨æŠ€æœ¯etcdçš„æ”¹å˜è€Œæ”¹å˜ã€‚\nä½œä¸ºkubernetesç³»ç»Ÿçš„å…¥å£ï¼Œå°è£…äº†æ ¸å¿ƒå¯¹è±¡çš„å¢åˆ æ”¹æŸ¥æ“ä½œï¼Œä»¥RESTFulæ¥å£æ–¹å¼æä¾›ç»™å¤–éƒ¨å®¢æˆ·å’Œå†…éƒ¨ç»„ä»¶è°ƒç”¨ã€‚å¯¹ç›¸å…³çš„èµ„æºæ•°æ®â€œå…¨é‡æŸ¥è¯¢â€+â€œå˜åŒ–ç›‘å¬â€ï¼Œå®æ—¶å®Œæˆç›¸å…³çš„ä¸šåŠ¡åŠŸèƒ½ã€‚\næ›´å¤šAPI Serverä¿¡æ¯è¯·å‚è€ƒï¼šKubernetesæ ¸å¿ƒåŸç†ï¼ˆä¸€ï¼‰ä¹‹API Server\n2.1.2 Controller Manager[å†…éƒ¨ç®¡ç†æ§åˆ¶ä¸­å¿ƒ] å®ç°é›†ç¾¤æ•…éšœæ£€æµ‹å’Œæ¢å¤çš„è‡ªåŠ¨åŒ–å·¥ä½œï¼Œè´Ÿè´£æ‰§è¡Œå„ç§æ§åˆ¶å™¨ï¼Œä¸»è¦æœ‰ï¼š endpoint-controllerï¼šå®šæœŸå…³è”serviceå’Œpod(å…³è”ä¿¡æ¯ç”±endpointå¯¹è±¡ç»´æŠ¤)ï¼Œä¿è¯serviceåˆ°podçš„æ˜ å°„æ€»æ˜¯æœ€æ–°çš„ã€‚ replication-controllerï¼šå®šæœŸå…³è”replicationControllerå’Œpodï¼Œä¿è¯replicationControllerå®šä¹‰çš„å¤åˆ¶æ•°é‡ä¸å®é™…è¿è¡Œpodçš„æ•°é‡æ€»æ˜¯ä¸€è‡´çš„ã€‚ æ›´å¤šController Managerä¿¡æ¯è¯·å‚è€ƒï¼šKubernetesæ ¸å¿ƒåŸç†ï¼ˆäºŒï¼‰ä¹‹Controller Manager\n2.1.3 Scheduler[é›†ç¾¤åˆ†å‘è°ƒåº¦å™¨] Scheduleræ”¶é›†å’Œåˆ†æå½“å‰Kubernetesé›†ç¾¤ä¸­æ‰€æœ‰Minion/NodeèŠ‚ç‚¹çš„èµ„æº(å†…å­˜ã€CPU)è´Ÿè½½æƒ…å†µï¼Œç„¶åä¾æ­¤åˆ†å‘æ–°å»ºçš„Podåˆ°Kubernetesé›†ç¾¤ä¸­å¯ç”¨çš„èŠ‚ç‚¹ã€‚ å®æ—¶ç›‘æµ‹Kubernetesé›†ç¾¤ä¸­æœªåˆ†å‘å’Œå·²åˆ†å‘çš„æ‰€æœ‰è¿è¡Œçš„Podã€‚ Schedulerä¹Ÿç›‘æµ‹Minion/NodeèŠ‚ç‚¹ä¿¡æ¯ï¼Œç”±äºä¼šé¢‘ç¹æŸ¥æ‰¾Minion/NodeèŠ‚ç‚¹ï¼ŒSchedulerä¼šç¼“å­˜ä¸€ä»½æœ€æ–°çš„ä¿¡æ¯åœ¨æœ¬åœ°ã€‚ æœ€åï¼ŒScheduleråœ¨åˆ†å‘Podåˆ°æŒ‡å®šçš„Minion/NodeèŠ‚ç‚¹åï¼Œä¼šæŠŠPodç›¸å…³çš„ä¿¡æ¯Bindingå†™å›API Serverã€‚ æ›´å¤šSchedulerä¿¡æ¯è¯·å‚è€ƒï¼šKubernetesæ ¸å¿ƒåŸç†ï¼ˆä¸‰ï¼‰ä¹‹Scheduler\n2.2 kube-node[æœåŠ¡èŠ‚ç‚¹] kubeletç»“æ„å›¾\n2.2.1 Kubelet[èŠ‚ç‚¹ä¸Šçš„Podç®¡å®¶] è´Ÿè´£NodeèŠ‚ç‚¹ä¸Špodçš„åˆ›å»ºã€ä¿®æ”¹ã€ç›‘æ§ã€åˆ é™¤ç­‰å…¨ç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†\nå®šæ—¶ä¸ŠæŠ¥æœ¬Nodeçš„çŠ¶æ€ä¿¡æ¯ç»™API Serverã€‚\nkubeletæ˜¯Master API Serverå’ŒMinion/Nodeä¹‹é—´çš„æ¡¥æ¢ï¼Œæ¥æ”¶Master API Serveråˆ†é…ç»™å®ƒçš„commandså’Œworkï¼Œé€šè¿‡kube-apiserveré—´æ¥ä¸Etcdé›†ç¾¤äº¤äº’ï¼Œè¯»å–é…ç½®ä¿¡æ¯ã€‚\nå…·ä½“çš„å·¥ä½œå¦‚ä¸‹ï¼š\nè®¾ç½®å®¹å™¨çš„ç¯å¢ƒå˜é‡ã€ç»™å®¹å™¨ç»‘å®šVolumeã€ç»™å®¹å™¨ç»‘å®šPortã€æ ¹æ®æŒ‡å®šçš„Podè¿è¡Œä¸€ä¸ªå•ä¸€å®¹å™¨ã€ç»™æŒ‡å®šçš„Podåˆ›å»ºnetwork å®¹å™¨ã€‚\nåŒæ­¥Podçš„çŠ¶æ€ã€åŒæ­¥Podçš„çŠ¶æ€ã€ä»cAdvisorè·å–container infoã€ pod infoã€ root infoã€ machine infoã€‚\nåœ¨å®¹å™¨ä¸­è¿è¡Œå‘½ä»¤ã€æ€æ­»å®¹å™¨ã€åˆ é™¤Podçš„æ‰€æœ‰å®¹å™¨ã€‚\næ›´å¤šKubeletä¿¡æ¯è¯·å‚è€ƒï¼šKubernetesæ ¸å¿ƒåŸç†ï¼ˆå››ï¼‰ä¹‹Kubelet\n2.2.2 Proxy[è´Ÿè½½å‡è¡¡ã€è·¯ç”±è½¬å‘] Proxyæ˜¯ä¸ºäº†è§£å†³å¤–éƒ¨ç½‘ç»œèƒ½å¤Ÿè®¿é—®è·¨æœºå™¨é›†ç¾¤ä¸­å®¹å™¨æä¾›çš„åº”ç”¨æœåŠ¡è€Œè®¾è®¡çš„ï¼Œè¿è¡Œåœ¨æ¯ä¸ªMinion/Nodeä¸Šã€‚Proxyæä¾›TCP/UDP socketsçš„proxyï¼Œæ¯åˆ›å»ºä¸€ç§Serviceï¼ŒProxyä¸»è¦ä»etcdè·å–Serviceså’ŒEndpointsçš„é…ç½®ä¿¡æ¯ï¼ˆä¹Ÿå¯ä»¥ä»fileè·å–ï¼‰ï¼Œç„¶åæ ¹æ®é…ç½®ä¿¡æ¯åœ¨Minion/Nodeä¸Šå¯åŠ¨ä¸€ä¸ªProxyçš„è¿›ç¨‹å¹¶ç›‘å¬ç›¸åº”çš„æœåŠ¡ç«¯å£ï¼Œå½“å¤–éƒ¨è¯·æ±‚å‘ç”Ÿæ—¶ï¼ŒProxyä¼šæ ¹æ®Load Balancerå°†è¯·æ±‚åˆ†å‘åˆ°åç«¯æ­£ç¡®çš„å®¹å™¨å¤„ç†ã€‚ Proxyä¸ä½†è§£å†³äº†åŒä¸€ä¸»å®¿æœºç›¸åŒæœåŠ¡ç«¯å£å†²çªçš„é—®é¢˜ï¼Œè¿˜æä¾›äº†Serviceè½¬å‘æœåŠ¡ç«¯å£å¯¹å¤–æä¾›æœåŠ¡çš„èƒ½åŠ›ï¼ŒProxyåç«¯ä½¿ç”¨äº†éšæœºã€è½®å¾ªè´Ÿè½½å‡è¡¡ç®—æ³•ã€‚ 2.2.3 kubectl[é›†ç¾¤ç®¡ç†å‘½ä»¤è¡Œå·¥å…·é›†] é€šè¿‡å®¢æˆ·ç«¯çš„kubectlå‘½ä»¤é›†æ“ä½œï¼ŒAPI Serverå“åº”å¯¹åº”çš„å‘½ä»¤ç»“æœï¼Œä»è€Œè¾¾åˆ°å¯¹kubernetesé›†ç¾¤çš„ç®¡ç†ã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttps://yq.aliyun.com/articles/47308?spm=5176.100240.searchblog.19.jF7FFa\n","categories":"","description":"","excerpt":"1. Kubernetesçš„æ€»æ¶æ„å›¾ 2. Kuberneteså„ä¸ªç»„ä»¶ä»‹ç» 2.1 kube-master[æ§åˆ¶èŠ‚ç‚¹] masterçš„å·¥ä½œ â€¦","ref":"/kubernetes-notes/concepts/architecture/kubernetes-architecture/","tags":["Kubernetes"],"title":"Kubernetesæ€»æ¶æ„å›¾"},{"body":"1. API Serverç®€ä»‹ k8s API Serveræä¾›äº†k8så„ç±»èµ„æºå¯¹è±¡ï¼ˆpod,RC,Serviceç­‰ï¼‰çš„å¢åˆ æ”¹æŸ¥åŠwatchç­‰HTTP Restæ¥å£ï¼Œæ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ•°æ®æ€»çº¿å’Œæ•°æ®ä¸­å¿ƒã€‚\nkubernetes API Serverçš„åŠŸèƒ½ï¼š\næä¾›äº†é›†ç¾¤ç®¡ç†çš„REST APIæ¥å£(åŒ…æ‹¬è®¤è¯æˆæƒã€æ•°æ®æ ¡éªŒä»¥åŠé›†ç¾¤çŠ¶æ€å˜æ›´)ï¼› æä¾›å…¶ä»–æ¨¡å—ä¹‹é—´çš„æ•°æ®äº¤äº’å’Œé€šä¿¡çš„æ¢çº½ï¼ˆå…¶ä»–æ¨¡å—é€šè¿‡API ServeræŸ¥è¯¢æˆ–ä¿®æ”¹æ•°æ®ï¼Œåªæœ‰API Serveræ‰ç›´æ¥æ“ä½œetcdï¼‰; æ˜¯èµ„æºé…é¢æ§åˆ¶çš„å…¥å£ï¼› æ‹¥æœ‰å®Œå¤‡çš„é›†ç¾¤å®‰å…¨æœºåˆ¶. kube-apiserverå·¥ä½œåŸç†å›¾\n2. å¦‚ä½•è®¿é—®kubernetes API k8sé€šè¿‡kube-apiserverè¿™ä¸ªè¿›ç¨‹æä¾›æœåŠ¡ï¼Œè¯¥è¿›ç¨‹è¿è¡Œåœ¨å•ä¸ªk8s-masterèŠ‚ç‚¹ä¸Šã€‚é»˜è®¤æœ‰ä¸¤ä¸ªç«¯å£ã€‚\n2.1. æœ¬åœ°ç«¯å£ è¯¥ç«¯å£ç”¨äºæ¥æ”¶HTTPè¯·æ±‚ï¼› è¯¥ç«¯å£é»˜è®¤å€¼ä¸º8080ï¼Œå¯ä»¥é€šè¿‡API Serverçš„å¯åŠ¨å‚æ•°â€œ--insecure-portâ€çš„å€¼æ¥ä¿®æ”¹é»˜è®¤å€¼ï¼› é»˜è®¤çš„IPåœ°å€ä¸ºâ€œlocalhostâ€ï¼Œå¯ä»¥é€šè¿‡å¯åŠ¨å‚æ•°â€œ--insecure-bind-addressâ€çš„å€¼æ¥ä¿®æ”¹è¯¥IPåœ°å€ï¼› éè®¤è¯æˆ–æˆæƒçš„HTTPè¯·æ±‚é€šè¿‡è¯¥ç«¯å£è®¿é—®API Serverã€‚ 2.2. å®‰å…¨ç«¯å£ è¯¥ç«¯å£é»˜è®¤å€¼ä¸º6443ï¼Œå¯é€šè¿‡å¯åŠ¨å‚æ•°â€œ--secure-portâ€çš„å€¼æ¥ä¿®æ”¹é»˜è®¤å€¼ï¼› é»˜è®¤IPåœ°å€ä¸ºéæœ¬åœ°ï¼ˆNon-Localhostï¼‰ç½‘ç»œç«¯å£ï¼Œé€šè¿‡å¯åŠ¨å‚æ•°â€œ--bind-addressâ€è®¾ç½®è¯¥å€¼ï¼› è¯¥ç«¯å£ç”¨äºæ¥æ”¶HTTPSè¯·æ±‚ï¼› ç”¨äºåŸºäºTockenæ–‡ä»¶æˆ–å®¢æˆ·ç«¯è¯ä¹¦åŠHTTP Baseçš„è®¤è¯ï¼› ç”¨äºåŸºäºç­–ç•¥çš„æˆæƒï¼› é»˜è®¤ä¸å¯åŠ¨HTTPSå®‰å…¨è®¿é—®æ§åˆ¶ã€‚ 2.3. è®¿é—®æ–¹å¼ Kubernetes REST APIå¯å‚è€ƒhttps://kubernetes.io/docs/api-reference/v1.6/\n2.3.1. curl curl localhost:8080/api curl localhost:8080/api/v1/pods curl localhost:8080/api/v1/services curl localhost:8080/api/v1/replicationcontrollers 2.3.2. Kubectl Proxy Kubectl Proxyä»£ç†ç¨‹åºæ—¢èƒ½ä½œä¸ºAPI Serverçš„åå‘ä»£ç†ï¼Œä¹Ÿèƒ½ä½œä¸ºæ™®é€šå®¢æˆ·ç«¯è®¿é—®API Serverçš„ä»£ç†ã€‚é€šè¿‡masterèŠ‚ç‚¹çš„8080ç«¯å£æ¥å¯åŠ¨è¯¥ä»£ç†ç¨‹åºã€‚\nkubectl proxy --port=8080 \u0026\nå…·ä½“è§kubectl proxy --help\n[root@node5 ~]# kubectl proxy --help To proxy all of the kubernetes api and nothing else, use: kubectl proxy --api-prefix=/ To proxy only part of the kubernetes api and also some static files: kubectl proxy --www=/my/files --www-prefix=/static/ --api-prefix=/api/ The above lets you 'curl localhost:8001/api/v1/pods'. To proxy the entire kubernetes api at a different root, use: kubectl proxy --api-prefix=/custom/ The above lets you 'curl localhost:8001/custom/api/v1/pods' Usage: kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags] Examples: # Run a proxy to kubernetes apiserver on port 8011, serving static content from ./local/www/ $ kubectl proxy --port=8011 --www=./local/www/ # Run a proxy to kubernetes apiserver on an arbitrary local port. # The chosen port for the server will be output to stdout. $ kubectl proxy --port=0 # Run a proxy to kubernetes apiserver, changing the api prefix to k8s-api # This makes e.g. the pods api available at localhost:8011/k8s-api/v1/pods/ $ kubectl proxy --api-prefix=/k8s-api Flags: --accept-hosts=\"^localhost$,^127//.0//.0//.1$,^//[::1//]$\": Regular expression for hosts that the proxy should accept. --accept-paths=\"^/.*\": Regular expression for paths that the proxy should accept. --api-prefix=\"/\": Prefix to serve the proxied API under. --disable-filter[=false]: If true, disable request filtering in the proxy. This is dangerous, and can leave you vulnerable to XSRF attacks, when used with an accessible port. -p, --port=8001: The port on which to run the proxy. Set to 0 to pick a random port. --reject-methods=\"POST,PUT,PATCH\": Regular expression for HTTP methods that the proxy should reject. --reject-paths=\"^/api/.*/exec,^/api/.*/run\": Regular expression for paths that the proxy should reject. -u, --unix-socket=\"\": Unix socket on which to run the proxy. -w, --www=\"\": Also serve static files from the given directory under the specified prefix. -P, --www-prefix=\"/static/\": Prefix to serve static files under, if static file directory is specified. Global Flags: --alsologtostderr[=false]: log to standard error as well as files --api-version=\"\": The API version to use when talking to the server --certificate-authority=\"\": Path to a cert. file for the certificate authority. --client-certificate=\"\": Path to a client key file for TLS. --client-key=\"\": Path to a client key file for TLS. --cluster=\"\": The name of the kubeconfig cluster to use --context=\"\": The name of the kubeconfig context to use --insecure-skip-tls-verify[=false]: If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure. --kubeconfig=\"\": Path to the kubeconfig file to use for CLI requests. --log-backtrace-at=:0: when logging hits line file:N, emit a stack trace --log-dir=\"\": If non-empty, write log files in this directory --log-flush-frequency=5s: Maximum number of seconds between log flushes --logtostderr[=true]: log to standard error instead of files --match-server-version[=false]: Require server version to match client version --namespace=\"\": If present, the namespace scope for this CLI request. --password=\"\": Password for basic authentication to the API server. -s, --server=\"\": The address and port of the Kubernetes API server --stderrthreshold=2: logs at or above this threshold go to stderr --token=\"\": Bearer token for authentication to the API server. --user=\"\": The name of the kubeconfig user to use --username=\"\": Username for basic authentication to the API server. --v=0: log level for V logs --vmodule=: comma-separated list of pattern=N settings for file-filtered logging 2.3.3. kubectlå®¢æˆ·ç«¯ å‘½ä»¤è¡Œå·¥å…·kubectlå®¢æˆ·ç«¯ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°è½¬æ¢ä¸ºå¯¹API Serverçš„REST APIè°ƒç”¨ï¼Œå¹¶å°†è°ƒç”¨ç»“æœè¾“å‡ºã€‚\nå‘½ä»¤æ ¼å¼ï¼škubectl [command] [options]\nå…·ä½“å¯å‚è€ƒk8så¸¸ç”¨å‘½ä»¤\n2.3.4. ç¼–ç¨‹æ–¹å¼è°ƒç”¨ ä½¿ç”¨åœºæ™¯ï¼š\n1ã€è¿è¡Œåœ¨Podé‡Œçš„ç”¨æˆ·è¿›ç¨‹è°ƒç”¨kubernetes API,é€šå¸¸ç”¨æ¥å®ç°åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºçš„ç›®æ ‡ã€‚\n2ã€å¼€å‘åŸºäºkubernetesçš„ç®¡ç†å¹³å°ï¼Œæ¯”å¦‚è°ƒç”¨kubernetes APIæ¥å®ŒæˆPodã€Serviceã€RCç­‰èµ„æºå¯¹è±¡çš„å›¾å½¢åŒ–åˆ›å»ºå’Œç®¡ç†ç•Œé¢ã€‚å¯ä»¥ä½¿ç”¨kubernetesæä¾›çš„Client Libraryã€‚\nå…·ä½“å¯å‚è€ƒhttps://github.com/kubernetes/client-goã€‚\n3. é€šè¿‡API Serverè®¿é—®Nodeã€Podå’ŒService k8s API Serveræœ€ä¸»è¦çš„RESTæ¥å£æ˜¯èµ„æºå¯¹è±¡çš„å¢åˆ æ”¹æŸ¥ï¼Œå¦å¤–è¿˜æœ‰ä¸€ç±»ç‰¹æ®Šçš„RESTæ¥å£â€”k8s Proxy APIæ¥å£ï¼Œè¿™ç±»æ¥å£çš„ä½œç”¨æ˜¯ä»£ç†RESTè¯·æ±‚ï¼Œå³kubernetes API ServeræŠŠæ”¶åˆ°çš„RESTè¯·æ±‚è½¬å‘åˆ°æŸä¸ªNodeä¸Šçš„kubeletå®ˆæŠ¤è¿›ç¨‹çš„RESTç«¯å£ä¸Šï¼Œç”±è¯¥kubeletè¿›ç¨‹è´Ÿè´£å“åº”ã€‚\n3.1. Nodeç›¸å…³æ¥å£ å…³äºNodeç›¸å…³çš„æ¥å£çš„RESTè·¯å¾„ä¸ºï¼š/api/v1/proxy/nodes/{name}ï¼Œå…¶ä¸­{name}ä¸ºèŠ‚ç‚¹çš„åç§°æˆ–IPåœ°å€ã€‚\n/api/v1/proxy/nodes/{name}/pods/ #åˆ—å‡ºæŒ‡å®šèŠ‚ç‚¹å†…æ‰€æœ‰Podçš„ä¿¡æ¯ /api/v1/proxy/nodes/{name}/stats/ #åˆ—å‡ºæŒ‡å®šèŠ‚ç‚¹å†…ç‰©ç†èµ„æºçš„ç»Ÿè®¡ä¿¡æ¯ /api/v1/prxoy/nodes/{name}/spec/ #åˆ—å‡ºæŒ‡å®šèŠ‚ç‚¹çš„æ¦‚è¦ä¿¡æ¯ è¿™é‡Œè·å–çš„Podä¿¡æ¯æ¥è‡ªNodeè€Œéetcdæ•°æ®åº“ï¼Œä¸¤è€…æ—¶é—´ç‚¹å¯èƒ½å­˜åœ¨åå·®ã€‚å¦‚æœåœ¨kubeletè¿›ç¨‹å¯åŠ¨æ—¶åŠ --enable-debugging-handles=trueå‚æ•°ï¼Œé‚£ä¹ˆkubernetes Proxy APIè¿˜ä¼šå¢åŠ ä»¥ä¸‹æ¥å£ï¼š\n/api/v1/proxy/nodes/{name}/run #åœ¨èŠ‚ç‚¹ä¸Šè¿è¡ŒæŸä¸ªå®¹å™¨ /api/v1/proxy/nodes/{name}/exec #åœ¨èŠ‚ç‚¹ä¸Šçš„æŸä¸ªå®¹å™¨ä¸­è¿è¡ŒæŸæ¡å‘½ä»¤ /api/v1/proxy/nodes/{name}/attach #åœ¨èŠ‚ç‚¹ä¸ŠattachæŸä¸ªå®¹å™¨ /api/v1/proxy/nodes/{name}/portForward #å®ç°èŠ‚ç‚¹ä¸Šçš„Podç«¯å£è½¬å‘ /api/v1/proxy/nodes/{name}/logs #åˆ—å‡ºèŠ‚ç‚¹çš„å„ç±»æ—¥å¿—ä¿¡æ¯ /api/v1/proxy/nodes/{name}/metrics #åˆ—å‡ºå’Œè¯¥èŠ‚ç‚¹ç›¸å…³çš„Metricsä¿¡æ¯ /api/v1/proxy/nodes/{name}/runningpods #åˆ—å‡ºèŠ‚ç‚¹å†…è¿è¡Œä¸­çš„Podä¿¡æ¯ /api/v1/proxy/nodes/{name}/debug/pprof #åˆ—å‡ºèŠ‚ç‚¹å†…å½“å‰webæœåŠ¡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬CPUå’Œå†…å­˜çš„ä½¿ç”¨æƒ…å†µ 3.2. Podç›¸å…³æ¥å£ /api/v1/proxy/namespaces/{namespace}/pods/{name}/{path:*} #è®¿é—®podçš„æŸä¸ªæœåŠ¡æ¥å£ /api/v1/proxy/namespaces/{namespace}/pods/{name} #è®¿é—®Pod #ä»¥ä¸‹å†™æ³•ä¸åŒï¼ŒåŠŸèƒ½ä¸€æ · /api/v1/namespaces/{namespace}/pods/{name}/proxy/{path:*} #è®¿é—®podçš„æŸä¸ªæœåŠ¡æ¥å£ /api/v1/namespaces/{namespace}/pods/{name}/proxy #è®¿é—®Pod 3.3. Serviceç›¸å…³æ¥å£ /api/v1/proxy/namespaces/{namespace}/services/{name} Podçš„proxyæ¥å£çš„ä½œç”¨ï¼šåœ¨kubernetesé›†ç¾¤ä¹‹å¤–è®¿é—®æŸä¸ªpodå®¹å™¨çš„æœåŠ¡ï¼ˆHTTPæœåŠ¡ï¼‰ï¼Œå¯ä»¥ç”¨Proxy APIå®ç°ï¼Œè¿™ç§åœºæ™¯å¤šç”¨äºç®¡ç†ç›®çš„ï¼Œæ¯”å¦‚é€ä¸€æ’æŸ¥Serviceçš„Podå‰¯æœ¬ï¼Œæ£€æŸ¥å“ªäº›Podçš„æœåŠ¡å­˜åœ¨å¼‚å¸¸é—®é¢˜ã€‚\n4. é›†ç¾¤åŠŸèƒ½æ¨¡å—ä¹‹é—´çš„é€šä¿¡ kubernetes API Serverä½œä¸ºé›†ç¾¤çš„æ ¸å¿ƒï¼Œè´Ÿè´£é›†ç¾¤å„åŠŸèƒ½æ¨¡å—ä¹‹é—´çš„é€šä¿¡ï¼Œé›†ç¾¤å†…å„ä¸ªåŠŸèƒ½æ¨¡å—é€šè¿‡API Serverå°†ä¿¡æ¯å­˜å…¥etcdï¼Œå½“éœ€è¦è·å–å’Œæ“ä½œè¿™äº›æ•°æ®æ—¶ï¼Œé€šè¿‡API Serveræä¾›çš„RESTæ¥å£ï¼ˆGET/LIST/WATCHæ–¹æ³•ï¼‰æ¥å®ç°ï¼Œä»è€Œå®ç°å„æ¨¡å—ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚\n4.1. kubeletä¸API Serveräº¤äº’ æ¯ä¸ªNodeèŠ‚ç‚¹ä¸Šçš„kubeletå®šæœŸå°±ä¼šè°ƒç”¨API Serverçš„RESTæ¥å£æŠ¥å‘Šè‡ªèº«çŠ¶æ€ï¼ŒAPI Serveræ¥æ”¶è¿™äº›ä¿¡æ¯åï¼Œå°†èŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯æ›´æ–°åˆ°etcdä¸­ã€‚kubeletä¹Ÿé€šè¿‡API Serverçš„Watchæ¥å£ç›‘å¬Podä¿¡æ¯ï¼Œä»è€Œå¯¹Nodeæœºå™¨ä¸Šçš„PODè¿›è¡Œç®¡ç†ã€‚\nç›‘å¬ä¿¡æ¯ kubeletåŠ¨ä½œ æ–°çš„PODå‰¯æœ¬è¢«è°ƒåº¦ç»‘å®šåˆ°æœ¬èŠ‚ç‚¹ æ‰§è¡ŒPODå¯¹åº”çš„å®¹å™¨çš„åˆ›å»ºå’Œå¯åŠ¨é€»è¾‘ PODå¯¹è±¡è¢«åˆ é™¤ åˆ é™¤æœ¬èŠ‚ç‚¹ä¸Šç›¸åº”çš„PODå®¹å™¨ ä¿®æ”¹PODä¿¡æ¯ ä¿®æ”¹æœ¬èŠ‚ç‚¹çš„PODå®¹å™¨ 4.2. kube-controller-managerä¸API Serveräº¤äº’ kube-controller-managerä¸­çš„Node Controlleræ¨¡å—é€šè¿‡API Serveræä¾›çš„Watchæ¥å£ï¼Œå®æ—¶ç›‘æ§Nodeçš„ä¿¡æ¯ï¼Œå¹¶åšç›¸åº”å¤„ç†ã€‚\n4.3. kube-schedulerä¸API Serveräº¤äº’ Scheduleré€šè¿‡API Serverçš„Watchæ¥å£ç›‘å¬åˆ°æ–°å»ºPodå‰¯æœ¬çš„ä¿¡æ¯åï¼Œå®ƒä¼šæ£€ç´¢æ‰€æœ‰ç¬¦åˆè¯¥Podè¦æ±‚çš„Nodeåˆ—è¡¨ï¼Œå¼€å§‹æ‰§è¡ŒPodè°ƒåº¦é€»è¾‘ã€‚è°ƒåº¦æˆåŠŸåå°†Podç»‘å®šåˆ°ç›®æ ‡èŠ‚ç‚¹ä¸Šã€‚\n4.4. ç‰¹åˆ«è¯´æ˜ ä¸ºäº†ç¼“è§£å„æ¨¡å—å¯¹API Serverçš„è®¿é—®å‹åŠ›ï¼Œå„åŠŸèƒ½æ¨¡å—éƒ½é‡‡ç”¨ç¼“å­˜æœºåˆ¶æ¥ç¼“å­˜æ•°æ®ï¼Œå„åŠŸèƒ½æ¨¡å—å®šæ—¶ä»API Serverè·å–æŒ‡å®šçš„èµ„æºå¯¹è±¡ä¿¡æ¯ï¼ˆLIST/WATCHæ–¹æ³•ï¼‰ï¼Œç„¶åå°†ä¿¡æ¯ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜ï¼ŒåŠŸèƒ½æ¨¡å—åœ¨æŸäº›æƒ…å†µä¸‹ä¸ç›´æ¥è®¿é—®API Serverï¼Œè€Œæ˜¯é€šè¿‡è®¿é—®ç¼“å­˜æ•°æ®æ¥é—´æ¥è®¿é—®API Serverã€‚\nå‚è€ƒã€Škubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. API Serverç®€ä»‹ k8s API Serveræä¾›äº†k8så„ç±»èµ„æºå¯¹è±¡ï¼ˆpod,RC,Serviceç­‰ï¼‰çš„å¢åˆ æ”¹æŸ¥åŠwatch â€¦","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-api-server/","tags":["Kubernetes"],"title":"Kubernetesæ ¸å¿ƒåŸç†ï¼ˆä¸€ï¼‰ä¹‹API Server"},{"body":"1. Podæ˜¯ä»€ä¹ˆï¼ˆwhatï¼‰ 1.1. Podæ¦‚å¿µ Podæ˜¯kubernetesé›†ç¾¤ä¸­æœ€å°çš„éƒ¨ç½²å’Œç®¡ç†çš„åŸºæœ¬å•å…ƒï¼ŒååŒå¯»å€ï¼ŒååŒè°ƒåº¦ã€‚ Podæ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨çš„é›†åˆï¼Œæ˜¯ä¸€ä¸ªæˆ–ä¸€ç»„æœåŠ¡ï¼ˆè¿›ç¨‹ï¼‰çš„æŠ½è±¡é›†åˆã€‚ Podä¸­å¯ä»¥å…±äº«ç½‘ç»œå’Œå­˜å‚¨ï¼ˆå¯ä»¥ç®€å•ç†è§£ä¸ºä¸€ä¸ªé€»è¾‘ä¸Šçš„è™šæ‹Ÿæœºï¼Œä½†å¹¶ä¸æ˜¯è™šæ‹Ÿæœºï¼‰ã€‚ Podè¢«åˆ›å»ºåç”¨ä¸€ä¸ªUIDæ¥å”¯ä¸€æ ‡è¯†ï¼Œå½“Podç”Ÿå‘½å‘¨æœŸç»“æŸï¼Œè¢«ä¸€ä¸ªç­‰ä»·Podæ›¿ä»£ï¼ŒUIDå°†é‡æ–°ç”Ÿæˆã€‚ 1.1.1. Podä¸Docker Dockeræ˜¯ç›®å‰Podæœ€å¸¸ç”¨çš„å®¹å™¨ç¯å¢ƒï¼Œä½†ä»æ”¯æŒå…¶ä»–å®¹å™¨ç¯å¢ƒã€‚ Podæ˜¯ä¸€ç»„è¢«æ¨¡å—åŒ–çš„æ‹¥æœ‰å…±äº«å‘½åç©ºé—´å’Œå…±äº«å­˜å‚¨å·çš„å®¹å™¨ï¼Œä½†å¹¶æ²¡æœ‰å…±äº«PID å‘½åç©ºé—´ï¼ˆå³åŒä¸ªPodçš„ä¸åŒå®¹å™¨ä¸­è¿›ç¨‹çš„PIDæ˜¯ç‹¬ç«‹çš„ï¼Œäº’ç›¸çœ‹ä¸åˆ°éè‡ªå·±å®¹å™¨çš„è¿›ç¨‹ï¼‰ã€‚ 1.1.2. Podä¸­å®¹å™¨çš„è¿è¡Œæ–¹å¼ åªè¿è¡Œä¸€ä¸ªå•ç‹¬çš„å®¹å™¨ å³one-container-per-Podæ¨¡å¼ï¼Œæ˜¯æœ€å¸¸ç”¨çš„æ¨¡å¼ï¼Œå¯ä»¥æŠŠè¿™æ ·çš„Podçœ‹æˆå•ç‹¬çš„ä¸€ä¸ªå®¹å™¨å»ç®¡ç†ã€‚\nè¿è¡Œå¤šä¸ªå¼ºå…³è”çš„å®¹å™¨ å³sidecaræ¨¡å¼ï¼ŒPod å°è£…äº†ä¸€ç»„ç´§è€¦åˆã€å…±äº«èµ„æºã€ååŒå¯»å€çš„å®¹å™¨ï¼Œå°†è¿™ç»„å®¹å™¨ä½œä¸ºä¸€ä¸ªç®¡ç†å•å…ƒã€‚\n1.2. Podç®¡ç†å¤šä¸ªå®¹å™¨ Podæ˜¯ä¸€ç»„ç´§è€¦åˆçš„å®¹å™¨çš„é›†åˆï¼ŒPodå†…çš„å®¹å™¨ä½œä¸ºä¸€ä¸ªæ•´ä½“ä»¥Podå½¢å¼è¿›è¡ŒååŒå¯»å€ï¼ŒååŒè°ƒåº¦ã€ååŒç®¡ç†ã€‚ç›¸åŒPodå†…çš„å®¹å™¨å…±äº«ç½‘ç»œå’Œå­˜å‚¨ã€‚\n1.2.1. ç½‘ç»œ æ¯ä¸ªPodè¢«åˆ†é…äº†å”¯ä¸€çš„IPåœ°å€ï¼Œè¯¥Podå†…çš„æ‰€æœ‰å®¹å™¨å…±äº«ä¸€ä¸ªç½‘ç»œç©ºé—´ï¼ŒåŒ…æ‹¬IPå’Œç«¯å£ã€‚ åŒä¸ªPodä¸åŒå®¹å™¨ä¹‹é—´é€šè¿‡localhosté€šä¿¡ï¼ŒPodå†…ç«¯å£ä¸èƒ½å†²çªã€‚ ä¸åŒPodä¹‹é—´çš„é€šä¿¡åˆ™é€šè¿‡IP+ç«¯å£çš„å½¢å¼æ¥è®¿é—®åˆ°Podå†…çš„å…·ä½“æœåŠ¡ï¼ˆå®¹å™¨ï¼‰ã€‚ 1.2.2. å­˜å‚¨ å¯ä»¥åœ¨Podä¸­åˆ›å»ºå…±äº«å­˜å‚¨å·çš„æ–¹å¼æ¥å®ç°ä¸åŒå®¹å™¨ä¹‹é—´æ•°æ®å…±äº«ã€‚ 2. ä¸ºä»€ä¹ˆéœ€è¦Pod(why) 2.1. ç®¡ç†éœ€æ±‚ Pod æ˜¯ä¸€ç§æ¨¡å¼çš„æŠ½è±¡ï¼šäº’ç›¸åä½œçš„å¤šä¸ªè¿›ç¨‹ï¼ˆå®¹å™¨ï¼‰å…±åŒå½¢æˆä¸€ä¸ªå®Œæ•´çš„æœåŠ¡ã€‚ä»¥ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨çš„æ–¹å¼ç»„åˆæˆä¸€ä¸ªæ•´ä½“ï¼Œä½œä¸ºç®¡ç†çš„åŸºæœ¬å•å…ƒï¼Œé€šè¿‡Podå¯ä»¥æ–¹ä¾¿éƒ¨ç½²ã€æ°´å¹³æ‰©å±•ï¼ŒååŒè°ƒåº¦ç­‰ã€‚\n2.2. èµ„æºå…±äº«å’Œé€šä¿¡ Podä½œä¸ºå¤šä¸ªç´§è€¦åˆçš„å®¹å™¨çš„é›†åˆï¼Œé€šè¿‡å…±äº«ç½‘ç»œå’Œå­˜å‚¨çš„æ–¹å¼æ¥ç®€åŒ–ç´§è€¦åˆå®¹å™¨ä¹‹é—´çš„é€šä¿¡ï¼Œä»è¿™ä¸ªè§’åº¦ï¼Œå¯ä»¥å°†Podç®€å•ç†è§£ä¸ºä¸€ä¸ªé€»è¾‘ä¸Šçš„â€œè™šæ‹Ÿæœºâ€ã€‚è€Œä¸åŒçš„Podä¹‹é—´çš„é€šä¿¡åˆ™é€šè¿‡Podçš„IPå’Œç«¯å£çš„æ–¹å¼ã€‚\n2.3. Podè®¾è®¡çš„ä¼˜åŠ¿ è°ƒåº¦å™¨å’Œæ§åˆ¶å™¨çš„å¯æ‹”æ’æ€§ã€‚ å°†Pod çš„ç”Ÿå­˜æœŸä» controller ä¸­å‰¥ç¦»å‡ºæ¥ï¼Œä»è€Œå‡å°‘ç›¸äº’å½±å“ã€‚ é«˜å¯ç”¨--åœ¨ç»ˆæ­¢å’Œåˆ é™¤ Pod å‰ï¼Œéœ€è¦æå‰ç”Ÿæˆæ›¿ä»£ Podã€‚ é›†ç¾¤çº§åˆ«çš„åŠŸèƒ½å’Œ Kubeletï¼ˆPod Controllerï¼‰ çº§åˆ«çš„åŠŸèƒ½ç»„åˆæ›´åŠ æ¸…æ™°ã€‚ 3. Podçš„ä½¿ç”¨(how) Podä¸€èˆ¬æ˜¯é€šè¿‡å„ç§ä¸åŒç±»å‹çš„Controllerå¯¹Podè¿›è¡Œç®¡ç†å’Œæ§åˆ¶ï¼ŒåŒ…æ‹¬è‡ªæˆ‘æ¢å¤ï¼ˆä¾‹å¦‚Podå› å¼‚å¸¸é€€å‡ºï¼Œåˆ™ä¼šå†èµ·ä¸€ä¸ªç›¸åŒçš„Podæ›¿ä»£è¯¥Podï¼Œè€Œè¯¥Podåˆ™ä¼šè¢«æ¸…é™¤ï¼‰ã€‚ä¹Ÿå¯ä»¥ä¸é€šè¿‡Controllerå•ç‹¬åˆ›å»ºä¸€ä¸ªPodï¼Œä½†ä¸€èˆ¬å¾ˆå°‘è¿™ä¹ˆæ“ä½œï¼Œå› ä¸ºè¿™ä¸ªPodæ˜¯ä¸€ä¸ªå­¤ç«‹çš„å®ä½“ï¼Œå¹¶ä¸ä¼šè¢«Controllerç®¡ç†ã€‚\n3.1. Controller Controlleræ˜¯kubernetesä¸­ç”¨äºå¯¹Podè¿›è¡Œç®¡ç†çš„æ§åˆ¶å™¨ï¼Œé€šè¿‡è¯¥æ§åˆ¶å™¨è®©Podå§‹ç»ˆç»´æŒåœ¨ä¸€ä¸ªç”¨æˆ·åŸæœ¬è®¾å®šæˆ–æœŸæœ›çš„çŠ¶æ€ã€‚å¦‚æœèŠ‚ç‚¹å®•æœºæˆ–è€…Podå› å…¶ä»–åŸå› æ­»äº¡ï¼Œåˆ™ä¼šåœ¨å…¶ä»–èŠ‚ç‚¹èµ·ä¸€ä¸ªç›¸åŒçš„Podæ¥æ›¿ä»£è¯¥Podã€‚\nå¸¸ç”¨çš„Controlleræœ‰ï¼š\nDeployment StatefulSet DaemonSet Controlleræ˜¯é€šè¿‡ç”¨æˆ·æä¾›çš„Podæ¨¡æ¿æ¥åˆ›å»ºå’Œæ§åˆ¶Podã€‚\n3.2. Podæ¨¡æ¿ Podæ¨¡æ¿ç”¨æ¥å®šä¹‰Podçš„å„ç§å±æ€§ï¼ŒControlleré€šè¿‡Podæ¨¡æ¿æ¥ç”Ÿæˆå¯¹åº”çš„Podã€‚\nPodæ¨¡æ¿ç±»ä¼¼ä¸€ä¸ªé¥¼å¹²æ¨¡å…·ï¼Œé€šè¿‡æ¨¡å…·å·²ç»ç”Ÿæˆçš„é¥¼å¹²ä¸åŸæ¨¡å…·å·²ç»æ²¡æœ‰å…³ç³»ï¼Œå³å¯¹åŸæ¨¡å…·çš„ä¿®æ”¹ä¸ä¼šå½±å“å·²ç»ç”Ÿæˆçš„é¥¼å¹²ï¼Œåªä¼šå¯¹é€šè¿‡ä¿®æ”¹åçš„æ¨¡å…·ç”Ÿæˆçš„é¥¼å¹²æœ‰å½±å“ã€‚è¿™ç§æ–¹å¼å¯ä»¥æ›´åŠ æ–¹ä¾¿åœ°æ§åˆ¶å’Œç®¡ç†Podã€‚\n4. Podçš„ç»ˆæ­¢ ç”¨æˆ·å‘èµ·ä¸€ä¸ªåˆ é™¤Podçš„è¯·æ±‚ï¼Œç³»ç»Ÿä¼šå…ˆå‘é€TERMä¿¡å·ç»™æ¯ä¸ªå®¹å™¨çš„ä¸»è¿›ç¨‹ï¼Œå¦‚æœåœ¨å®½é™æœŸï¼ˆé»˜è®¤30ç§’ï¼‰ä¸»è¿›ç¨‹æ²¡æœ‰è‡ªä¸»ç»ˆæ­¢è¿è¡Œï¼Œåˆ™ç³»ç»Ÿä¼šå‘é€KILLä¿¡å·ç»™è¯¥è¿›ç¨‹ï¼Œæ¥ç€Podå°†è¢«åˆ é™¤ã€‚\n4.1. Podç»ˆæ­¢çš„æµç¨‹ ç”¨æˆ·å‘é€ä¸€ä¸ªåˆ é™¤ Pod çš„å‘½ä»¤ï¼Œ å¹¶ä½¿ç”¨é»˜è®¤çš„å®½é™æœŸï¼ˆ30s)ã€‚ æŠŠ API server ä¸Šçš„ pod çš„æ—¶é—´æ›´æ–°æˆ Pod ä¸å®½é™æœŸä¸€èµ·è¢«è®¤ä¸º â€œdeadâ€ ä¹‹å¤–çš„æ—¶é—´ç‚¹ã€‚ ä½¿ç”¨å®¢æˆ·ç«¯çš„å‘½ä»¤ï¼Œæ˜¾ç¤ºå‡ºçš„Podçš„çŠ¶æ€ä¸º terminatingã€‚ ï¼ˆä¸ç¬¬3æ­¥åŒæ—¶å‘ç”Ÿï¼‰Kubelet å‘ç°æŸä¸€ä¸ª Pod ç”±äºæ—¶é—´è¶…è¿‡ç¬¬2æ­¥çš„è®¾ç½®è€Œè¢«æ ‡å¿—æˆ terminating çŠ¶æ€æ—¶ï¼Œ Kubelet å°†å¯åŠ¨ä¸€ä¸ªåœæ­¢è¿›ç¨‹ã€‚ å¦‚æœ pod å·²ç»è¢«å®šä¹‰æˆä¸€ä¸ª preStop hookï¼Œè¿™ä¼šåœ¨ pod å†…éƒ¨è¿›è¡Œè°ƒç”¨ã€‚å¦‚æœå®½é™æœŸå·²ç»è¿‡æœŸä½† preStop é”šä¾ç„¶è¿˜åœ¨è¿è¡Œï¼Œå°†è°ƒç”¨ç¬¬2æ­¥å¹¶åœ¨åŸæ¥çš„å®½é™æœŸä¸ŠåŠ ä¸€ä¸ªå°çš„æ—¶é—´çª—å£ï¼ˆ2 ç§’é’Ÿï¼‰ã€‚ æŠŠ Pod é‡Œçš„è¿›ç¨‹å‘é€åˆ° TERM ä¿¡å·ã€‚ ï¼ˆä¸ç¬¬3æ­¥åŒæ—¶å‘ç”Ÿï¼‰ï¼ŒPod è¢«ä»ç»ˆç«¯çš„æœåŠ¡åˆ—è¡¨é‡Œç§»é™¤ï¼ŒåŒæ—¶ä¹Ÿä¸å†è¢« replication controllers çœ‹åšæ—¶ä¸€ç»„è¿è¡Œä¸­çš„ podsã€‚ åœ¨è´Ÿè½½å‡è¡¡ï¼ˆæ¯”å¦‚è¯´ service proxyï¼‰ä¼šå°†å®ƒä»¬ä»è½®è¯¢ä¸­ç§»é™¤å‰ï¼Œ Pods è¿™ç§æ…¢å…³é—­çš„æ–¹å¼å¯ä»¥ç»§ç»­ä¸ºæµé‡æä¾›æœåŠ¡ã€‚ å½“å®½æœŸé™è¿‡æœŸæ—¶ï¼Œ ä»»ä½•è¿˜åœ¨ Pod é‡Œè¿è¡Œçš„è¿›ç¨‹éƒ½ä¼šè¢« SIGKILL æ€æ‰ã€‚ Kubelet é€šè¿‡åœ¨ API server æŠŠå®½æœŸé™è®¾ç½®æˆ0(ç«‹åˆ»åˆ é™¤)çš„æ–¹å¼å®Œæˆåˆ é™¤ Podçš„è¿‡ç¨‹ã€‚ è¿™æ—¶ Pod åœ¨ API é‡Œæ¶ˆå¤±ï¼Œä¹Ÿä¸å†èƒ½è¢«ç”¨æˆ·çœ‹åˆ°ã€‚ 4.2. å¼ºåˆ¶åˆ é™¤Pod å¼ºåˆ¶åˆ é™¤Podæ˜¯æŒ‡ä»k8sé›†ç¾¤çŠ¶æ€å’ŒEtcdä¸­ç«‹åˆ»åˆ é™¤å¯¹åº”çš„Podæ•°æ®ï¼ŒAPI Serverä¸ä¼šç­‰å¾…kubeletçš„ç¡®è®¤ä¿¡æ¯ã€‚è¢«å¼ºåˆ¶åˆ é™¤åï¼Œå³å¯é‡æ–°åˆ›å»ºä¸€ä¸ªç›¸åŒåå­—çš„Podã€‚\nåˆ é™¤é»˜è®¤çš„å®½é™æœŸæ˜¯30ç§’ï¼Œé€šè¿‡å°†å®½é™æœŸè®¾ç½®ä¸º0çš„æ–¹å¼å¯ä»¥å¼ºåˆ¶åˆ é™¤Podã€‚\né€šè¿‡kubectl delete å‘½ä»¤ååŠ --forceå’Œ--grace-period=0çš„å‚æ•°å¼ºåˆ¶åˆ é™¤Podã€‚\nkubectl delete pod \u003cpod_name\u003e --namespace=\u003cnamespace\u003e --force --grace-period=0 4.3. Podç‰¹æƒæ¨¡å¼ ç‰¹æƒæ¨¡å¼æ˜¯æŒ‡è®©Podä¸­çš„è¿›ç¨‹å…·æœ‰è®¿é—®å®¿ä¸»æœºç³»ç»Ÿè®¾å¤‡æˆ–ä½¿ç”¨ç½‘ç»œæ ˆæ“ä½œç­‰çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ç¼–å†™ç½‘ç»œæ’ä»¶å’Œå·æ’ä»¶ã€‚\né€šè¿‡å°†container specä¸­çš„SecurityContextè®¾ç½®ä¸ºprivilegedå³å°†è¯¥å®¹å™¨èµ‹äºˆäº†ç‰¹æƒæ¨¡å¼ã€‚ç‰¹æƒæ¨¡å¼çš„ä½¿ç”¨è¦æ±‚k8sç‰ˆæœ¬é«˜äºv1.1ã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-overview/ https://kubernetes.io/docs/concepts/workloads/pods/pod/ ","categories":"","description":"","excerpt":"1. Podæ˜¯ä»€ä¹ˆï¼ˆwhatï¼‰ 1.1. Podæ¦‚å¿µ Podæ˜¯kubernetesé›†ç¾¤ä¸­æœ€å°çš„éƒ¨ç½²å’Œç®¡ç†çš„åŸºæœ¬å•å…ƒï¼ŒååŒå¯»å€ï¼ŒååŒè°ƒåº¦ã€‚ â€¦","ref":"/kubernetes-notes/concepts/pod/pod/","tags":["Kubernetes"],"title":"Podä»‹ç»"},{"body":"åˆè¯†Goè¯­è¨€ 1. æ¦‚è¿° ä¸€ä¸ªåœ¨è¯­è¨€å±‚é¢å®ç°äº†å¹¶å‘æœºåˆ¶çš„ç±»Cé€šç”¨å‹ç¼–ç¨‹è¯­è¨€ã€‚\n2. Goå…³é”®å­—ï¼ˆ25ä¸ªï¼‰ ç±»åˆ« å…³é”®å­— è¯´æ˜ ç¨‹åºå£°æ˜ packageï¼Œimport åŒ…çš„å£°æ˜å’Œå¯¼å…¥ å£°æ˜ä¸å®šä¹‰ varï¼Œconst å˜é‡å’Œå¸¸é‡çš„å£°æ˜ type ç”¨äºå®šä¹‰ç±»å‹ å¤åˆæ•°æ®ç±»å‹ struct å®šä¹‰ç»“æ„ä½“ï¼Œç±»ä¼¼javaä¸­çš„class interface å®šä¹‰æ¥å£ map å®šä¹‰é”®å€¼å¯¹ func å®šä¹‰å‡½æ•°å’Œæ–¹æ³• chan å®šä¹‰ç®¡é“ï¼Œå¹¶å‘ä¸­channelé€šä¿¡ å¹¶å‘ç¼–ç¨‹ go å¹¶å‘ç¼–ç¨‹ select ç”¨äºé€‰æ‹©ä¸åŒç±»å‹é€šä¿¡ æµç¨‹è¯­å¥ forï¼›ifï¼Œelseï¼›switchï¼Œcase å¾ªç¯è¯­å¥ï¼›æ¡ä»¶è¯­å¥ï¼›é€‰æ‹©è¯­å¥ breakï¼Œcontinueï¼Œfallthroughï¼Œdefaultï¼Œgoto è·³è½¬è¯­å¥ç­‰ return å‡½æ•°è¿”å›å€¼ defer å»¶è¿Ÿå‡½æ•°ï¼Œç”¨äºreturnå‰é‡Šæ”¾èµ„æº range ç”¨äºè¯»å–sliceï¼Œmapï¼Œchannelå®¹å™¨ç±»æ•°æ® 3. Goè¯­è¨€å‘½ä»¤ Usageï¼šgo command [arguments]\nåˆ†ç±» å‘½ä»¤ è¯´æ˜ build compile packages and dependencies clean remove object files doc show documentation for package or symbol env print Go environment information fix run go tool fix on packages fmt run gofmt on package sources generate generate Go files by processing source get download and install packages and dependencies install compile and install packages and dependencies list list packages run compile and run Go program test test packages tool run specified go tool version print Go version vet run go tool vet on packages ","categories":"","description":"","excerpt":"åˆè¯†Goè¯­è¨€ 1. æ¦‚è¿° ä¸€ä¸ªåœ¨è¯­è¨€å±‚é¢å®ç°äº†å¹¶å‘æœºåˆ¶çš„ç±»Cé€šç”¨å‹ç¼–ç¨‹è¯­è¨€ã€‚\n2. Goå…³é”®å­—ï¼ˆ25ä¸ªï¼‰ â€¦","ref":"/golang-notes/introduction/golang/","tags":["Golang"],"title":"Golangä»‹ç»"},{"body":"1.å˜é‡ 1.1å˜é‡å£°æ˜ //1ã€å•å˜é‡å£°æ˜,ç±»å‹æ”¾åœ¨å˜é‡åä¹‹åï¼Œå¯ä»¥ä¸ºä»»æ„ç±»å‹ var å˜é‡å ç±»å‹ var v1,v2,v3 string //å¤šå˜é‡åŒç±»å‹å£°æ˜ //2ã€å¤šå˜é‡å£°æ˜ var { v1 int v2 []int } 1.2å˜é‡åˆå§‹åŒ– //1ã€ä½¿ç”¨å…³é”®å­—varï¼Œå£°æ˜å˜é‡ç±»å‹å¹¶èµ‹å€¼ var v1 int=10 //2ã€ä½¿ç”¨å…³é”®å­—varï¼Œç›´æ¥å¯¹å˜é‡èµ‹å€¼ï¼Œgoå¯ä»¥è‡ªåŠ¨æ¨å¯¼å‡ºå˜é‡ç±»å‹ var v2=10 //3ã€ç›´æ¥ä½¿ç”¨â€œï¼š=â€å¯¹å˜é‡èµ‹å€¼ï¼Œä¸ä½¿ç”¨varï¼Œä¸¤è€…åŒæ—¶ä½¿ç”¨ä¼šè¯­æ³•å†²çªï¼Œæ¨èä½¿ç”¨ v3:=10 1.3å˜é‡èµ‹å€¼ //1ã€å£°æ˜åå†å˜é‡èµ‹å€¼ var v int v=10 //2ã€å¤šé‡èµ‹å€¼ï¼Œç»å¸¸ä½¿ç”¨åœ¨å‡½æ•°çš„å¤šè¿”å›å€¼ä¸­ï¼Œerr,v=func(arg) iï¼Œj=j,i //ä¸¤è€…äº’æ¢ï¼Œå¹¶ä¸éœ€è¦å¼•å…¥ä¸­é—´å˜é‡ 1.4åŒ¿åå˜é‡ //Goä¸­æ‰€æœ‰å£°æ˜åçš„å˜é‡éƒ½éœ€è¦è°ƒç”¨åˆ°ï¼Œå½“å‡ºç°å‡½æ•°å¤šè¿”å›å€¼ï¼Œå¹¶ä¸”éƒ¨åˆ†è¿”å›å€¼ä¸éœ€è¦ä½¿ç”¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨åŒ¿åå˜é‡ä¸¢å¼ƒè¯¥è¿”å›å€¼ func GetName()(firstName,lastName,nickName string){ return \"May\",\"Chan\",\"Make\" } _,_,nickName:=GetName() //ä½¿ç”¨åŒ¿åå˜é‡ä¸¢å¼ƒéƒ¨åˆ†è¿”å›å€¼ 2.å¸¸é‡ â€‹\tGoè¯­è¨€ä¸­ï¼Œå¸¸é‡æ˜¯ç¼–è¯‘æ—¶æœŸå°±å·²çŸ¥ä¸”ä¸å¯å˜çš„å€¼ï¼Œå¸¸é‡å¯ä»¥æ˜¯æ•°å€¼ç±»å‹ï¼ˆæ•´å‹ã€æµ®ç‚¹å‹ã€å¤æ•°ç±»å‹ï¼‰ã€å¸ƒå°”ç±»å‹ã€å­—ç¬¦ä¸²ç±»å‹ã€‚\n2.1å­—é¢å¸¸é‡ //å­—é¢å¸¸é‡(literal)æŒ‡ç¨‹åºä¸­ç¡¬ç¼–ç çš„å¸¸é‡ 3.14 â€œfooâ€ true 2.2å¸¸é‡å®šä¹‰ //1ã€å¯ä»¥é™å®šå¸¸é‡ç±»å‹ï¼Œä½†éå¿…éœ€ const Pi float64 = 3.14 //2ã€æ— ç±»å‹å¸¸é‡å’Œå­—é¢å¸¸é‡ä¸€æ · const zero=0.0 //3ã€å¤šå¸¸é‡èµ‹å€¼ const( size int64=1024 eof=-1 ) //4ã€å¸¸é‡çš„å¤šé‡èµ‹å€¼ï¼Œç±»ä¼¼å˜é‡çš„å¤šé‡èµ‹å€¼ const u,v float32=0,3 const a,b,c=3,4,\"foo\" //æ— ç±»å‹å¸¸é‡çš„å¤šé‡èµ‹å€¼ //5ã€å¸¸é‡èµ‹å€¼æ˜¯ç¼–è¯‘æœŸè¡Œä¸ºï¼Œå¯ä»¥èµ‹å€¼ä¸ºä¸€ä¸ªç¼–è¯‘æœŸè¿ç®—çš„å¸¸é‡è¡¨è¾¾å¼ const mask=1\u003c\u003c3 2.3é¢„å®šä¹‰å¸¸é‡ //é¢„å®šä¹‰å¸¸é‡ï¼štrueã€falseã€iota //iotaï¼šå¯ä¿®æ”¹å¸¸é‡ï¼Œåœ¨æ¯æ¬¡constå‡ºç°æ—¶è¢«é‡ç½®ä¸º0ï¼Œåœ¨ä¸‹ä¸€ä¸ªconstå‡ºç°å‰ï¼Œæ¯å‡ºç°ä¸€æ¬¡iotaï¼Œå…¶ä»£è¡¨çš„å€¼è‡ªåŠ¨å¢1ã€‚ const( //iotaé‡ç½®ä¸º0 c0=iota //c0==0 c1=iota //c1==1 c2=iota //c2==2 ) //ä¸¤ä¸ªconstèµ‹å€¼è¯­å¥ä¸€æ ·å¯ä»¥çœç•¥åä¸€ä¸ª const( //iotaé‡ç½®ä¸º0 c0=iota //c0==0 c1 //c1==1 c2 //c2==2 ) 2.4æšä¸¾ æšä¸¾æŒ‡ä¸€ç³»åˆ—ç›¸å…³å¸¸é‡ã€‚\nconst( Sunday=iota //Sunday==0,ä»¥æ­¤ç±»æ¨ Monday Tuesday Wednesday Thursday Friday Saturday //å¤§å†™å­—æ¯å¼€å¤´è¡¨ç¤ºåŒ…å¤–å¯è§ numberOfDays //å°å†™å­—æ¯å¼€å¤´è¡¨ç¤ºåŒ…å†…ç§æœ‰ ) ","categories":"","description":"","excerpt":"1.å˜é‡ 1.1å˜é‡å£°æ˜ //1ã€å•å˜é‡å£°æ˜,ç±»å‹æ”¾åœ¨å˜é‡åä¹‹åï¼Œå¯ä»¥ä¸ºä»»æ„ç±»å‹ var å˜é‡å ç±»å‹ var v1,v2,v3 â€¦","ref":"/golang-notes/basis/var-const/","tags":["Golang"],"title":"å˜é‡ä¸å¸¸é‡"},{"body":" é¢å‘å¯¹è±¡ç¼–ç¨‹ â€‹ æŠŠä¸€ç»„æ•°æ®ç»“æ„å’Œå¤„ç†å®ƒä»¬çš„æ–¹æ³•ç»„æˆå¯¹è±¡ï¼ˆobjectï¼‰ï¼ŒæŠŠç›¸åŒè¡Œä¸ºçš„å¯¹è±¡å½’çº³ä¸ºç±»ï¼ˆclassï¼‰ï¼Œé€šè¿‡ç±»çš„å°è£…ï¼ˆencapsulationï¼‰éšè—å†…éƒ¨ç»†èŠ‚ï¼Œé€šè¿‡ç»§æ‰¿ï¼ˆinheritanceï¼‰å®ç°ç±»çš„ç‰¹åŒ–ï¼ˆspecializationï¼‰[æ–¹æ³•çš„é‡å†™ï¼Œå­ç±»ä¸åŒäºçˆ¶ç±»çš„ç‰¹æ€§]ï¼æ³›åŒ–ï¼ˆgeneralizationï¼‰[å…±æ€§ï¼Œå­ç±»éƒ½æ‹¥æœ‰çˆ¶ç±»çš„ç‰¹æ€§]ï¼Œé€šè¿‡å¤šæ€ï¼ˆpolymorphismï¼‰å®ç°åŸºäºå¯¹è±¡ç±»å‹çš„åŠ¨æ€åˆ†æ´¾ï¼ˆdynamic dispatchï¼‰ã€‚\né¢å¯¹å¯¹è±¡æ€æƒ³ â€‹ é¢å‘å¯¹è±¡æ€æƒ³æ˜¯å¯¹ç°å®ä¸–ç•Œäº‹ç‰©çš„æŠ½è±¡ï¼Œç³»ç»Ÿä¸­ä¸€åˆ‡äº‹ç‰©çš†ä¸ºå¯¹è±¡ï¼›å¯¹è±¡æ˜¯å±æ€§åŠå…¶æ“ä½œçš„å°è£…ä½“ï¼›å¯¹è±¡å¯æŒ‰å…¶æ€§è´¨åˆ’åˆ†ä¸ºç±»ï¼Œå¯¹è±¡æˆä¸ºç±»çš„å®ä¾‹ï¼›å®ä¾‹å…³ç³»å’Œç»§æ‰¿å…³ç³»æ˜¯å¯¹è±¡ä¹‹é—´çš„é™æ€å…³ç³»ï¼›æ¶ˆæ¯ä¼ é€’æ˜¯å¯¹è±¡ä¹‹é—´åŠ¨æ€è”ç³»çš„å”¯ä¸€å½¢å¼ï¼Œä¹Ÿæ˜¯è®¡ç®—çš„å”¯ä¸€å½¢å¼ï¼›æ–¹æ³•æ˜¯æ¶ˆæ¯çš„åºåˆ—ã€‚\nï¼ˆä¸€ï¼‰ç±»å‹ç³»ç»Ÿ[ç±»çš„å£°æ˜] ç±»å‹ç³»ç»Ÿï¼š\nä¸€ç»„åŸºæœ¬ç±»å‹æ„æˆçš„â€œåŸºæœ¬ç±»å‹é›†åˆâ€ï¼› â€œåŸºæœ¬ç±»å‹é›†åˆâ€ä¸Šå®šä¹‰çš„ä¸€ç³»åˆ—ç»„åˆã€è¿ç®—ã€è½¬æ¢æ–¹æ³•ã€‚ ç±»å‹ç³»ç»ŸåŒ…æ‹¬åŸºç¡€ç±»å‹ï¼ˆbyteã€intã€boolã€floatç­‰ï¼‰ï¼›å¤åˆç±»å‹ï¼ˆæ•°ç»„ã€ç»“æ„ä½“ã€æŒ‡é’ˆç­‰ï¼‰ï¼›å¯ä»¥æŒ‡å‘ä»»ä½•å¯¹è±¡çš„ç±»å‹ï¼ˆAnyç±»å‹ï¼Œç±»ä¼¼Javaçš„Objectç±»å‹ï¼‰ï¼›å€¼è¯­ä¹‰å’Œå¼•ç”¨è¯­ä¹‰ï¼›é¢å‘å¯¹è±¡ç±»å‹ï¼›æ¥å£ã€‚Goå¤§å¤šæ•°ç±»å‹ä¸ºå€¼è¯­ä¹‰ï¼Œå¯ä»¥ç»™ä»»ä½•ç±»å‹æ·»åŠ æ–¹æ³•ï¼ˆåŒ…æ‹¬å†…ç½®ç±»å‹ï¼Œä¸åŒ…æ‹¬æŒ‡é’ˆç±»å‹ï¼‰ã€‚Anyç±»å‹æ˜¯ç©ºæ¥å£å³interface{}ã€‚\n1.æ–¹æ³• 1ã€ä¸ºç±»å‹æ·»åŠ æ–¹æ³•[ç±»æ–¹æ³•å£°æ˜]ï¼Œæ–¹æ³•å³ä¸ºæœ‰æ¥æ”¶è€…çš„å‡½æ•° func (å¯¹è±¡å å¯¹è±¡ç±»å‹) å‡½æ•°å(å‚æ•°åˆ—è¡¨) (è¿”å›å€¼åˆ—è¡¨) å¯éšæ—¶ä¸ºæŸä¸ªå¯¹è±¡æ·»åŠ æ–¹æ³•å³ä¸ºæŸä¸ªæ–¹æ³•æ·»åŠ å½’å±å¯¹è±¡ï¼ˆreceiverï¼‰ï¼Œä»¥æ–¹æ³•ä¸ºä¸­å¿ƒ åœ¨Goè¯­è¨€ä¸­æ²¡æœ‰éšè—çš„thisæŒ‡é’ˆï¼Œå³æ˜¾ç¤ºä¼ é€’ï¼Œå½¢å‚å³ä¸ºthisï¼Œä¾‹å¦‚ä»¥ä¸‹çš„å½¢å‚ä¸ºaã€‚\ntype Integer int func (a Integer) Less(b Integer) bool{ //è¡¨ç¤ºaè¿™ä¸ªå¯¹è±¡å®šä¹‰äº†Lessè¿™ä¸ªæ–¹æ³•ï¼Œaå¯ä»¥ä¸ºä»»æ„ç±»å‹ return a\u003cb } //ç±»å‹åŸºäºå€¼ä¼ é€’ï¼Œå¦‚æœè¦ä¿®æ”¹å€¼éœ€è¦ä¼ é€’æŒ‡é’ˆ func (a *Integer) Add(b Integer){ *a+=b //é€šè¿‡æŒ‡é’ˆä¼ é€’æ¥æ”¹å˜å€¼ } 2.å€¼è¯­ä¹‰å’Œå¼•ç”¨è¯­ä¹‰ å€¼ç±»å‹ï¼šbçš„ä¿®æ”¹å¹¶ä¸ä¼šå½±å“açš„å€¼\nå¼•ç”¨ç±»å‹ï¼šbçš„ä¿®æ”¹ä¼šå½±å“açš„å€¼\nGoå¤§å¤šç±»å‹ä¸ºå€¼è¯­ä¹‰ï¼ŒåŒ…æ‹¬åŸºæœ¬ç±»å‹ï¼šbyteï¼Œintï¼Œstringç­‰ï¼›å¤åˆç±»å‹ï¼šæ•°ç»„ï¼Œç»“æ„ä½“(struct)ï¼ŒæŒ‡é’ˆç­‰\n//2ã€å€¼è¯­ä¹‰å’Œå¼•ç”¨è¯­ä¹‰ b=a b.Modify() //å€¼ç±»å‹ var a=[3]int{1,2,3} b:=a b[1]++ fmt.Println(a,b) //a=[1,2,3] b=[1,3,3] //å¼•ç”¨ç±»å‹ a:=[3]int{1,2,3} b:=\u0026a //bæŒ‡å‘a,å³ä¸ºaçš„åœ°å€ï¼Œå¯¹bæŒ‡å‘çš„å€¼æ”¹å˜å®é™…ä¸Šå°±æ˜¯å¯¹açš„æ”¹å˜ï¼ˆæ•°ç»„æœ¬èº«å°±æ˜¯ä¸€ç§åœ°å€æŒ‡å‘ï¼‰ b[1]++ fmt.Println(a,*b) //a=[1,3,3] b=[1,3,3] //*b,å–åœ°å€æŒ‡å‘çš„å€¼ 3.ç»“æ„ä½“ 3ã€ç»“æ„ä½“[ç±»å±æ€§çš„å£°æ˜] structçš„åŠŸèƒ½ç±»ä¼¼Javaçš„classï¼Œå¯å®ç°åµŒå¥—ç»„åˆ(ç±»ä¼¼ç»§æ‰¿çš„åŠŸèƒ½) structå®é™…ä¸Šå°±æ˜¯ä¸€ç§å¤åˆç±»å‹ï¼Œåªæ˜¯å¯¹ç±»ä¸­çš„å±æ€§è¿›è¡Œå®šä¹‰èµ‹å€¼ï¼Œå¹¶æ²¡æœ‰å¯¹æ–¹æ³•è¿›è¡Œå®šä¹‰ï¼Œæ–¹æ³•å¯ä»¥éšæ—¶å®šä¹‰ç»‘å®šåˆ°è¯¥ç±»çš„å¯¹è±¡ä¸Šï¼Œæ›´å…·çµæ´»æ€§ã€‚å¯åˆ©ç”¨åµŒå¥—ç»„åˆæ¥å®ç°ç±»ä¼¼ç»§æ‰¿çš„åŠŸèƒ½é¿å…ä»£ç é‡å¤ã€‚\ntype Rect struct{ //å®šä¹‰çŸ©å½¢ç±» x,y float64 //ç±»å‹åªåŒ…å«å±æ€§ï¼Œå¹¶æ²¡æœ‰æ–¹æ³• width,height float64 } func (r *Rect) Area() float64{ //ä¸ºRectç±»å‹ç»‘å®šAreaçš„æ–¹æ³•ï¼Œ*Rectä¸ºæŒ‡é’ˆå¼•ç”¨å¯ä»¥ä¿®æ”¹ä¼ å…¥å‚æ•°çš„å€¼ return r.width*r.height //æ–¹æ³•å½’å±äºç±»å‹ï¼Œä¸å½’å±äºå…·ä½“çš„å¯¹è±¡ï¼Œå£°æ˜è¯¥ç±»å‹çš„å¯¹è±¡å³å¯è°ƒç”¨è¯¥ç±»å‹çš„æ–¹æ³• } ï¼ˆäºŒï¼‰åˆå§‹åŒ–[å®ä¾‹åŒ–å¯¹è±¡] æ•°æ®åˆå§‹åŒ–çš„å†…å»ºå‡½æ•°new()ä¸make()ï¼ŒäºŒè€…éƒ½æ˜¯ç”¨æ¥åˆ†é…ç©ºé—´ã€‚åŒºåˆ«å¦‚ä¸‹:\nnew() func new(Type) *Type å†…ç½®å‡½æ•° new åˆ†é…ç©ºé—´ã€‚ä¼ é€’ç»™new å‡½æ•°çš„æ˜¯ä¸€ä¸ªç±»å‹ï¼Œä¸æ˜¯ä¸€ä¸ªå€¼ã€‚è¿”å›å€¼æ˜¯æŒ‡å‘è¿™ä¸ªæ–°åˆ†é…çš„é›¶å€¼çš„æŒ‡é’ˆ make() func make(Type, size IntegerType) Type å†…å»ºå‡½æ•° make åˆ†é…å¹¶ä¸”åˆå§‹åŒ– ä¸€ä¸ª slice, æˆ–è€… map æˆ–è€… chan å¯¹è±¡ã€‚ å¹¶ä¸”åªèƒ½æ˜¯è¿™ä¸‰ç§å¯¹è±¡ã€‚ å’Œ new ä¸€æ ·ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ ç±»å‹ï¼Œä¸æ˜¯ä¸€ä¸ªå€¼ã€‚ ä½†æ˜¯make çš„è¿”å›å€¼å°±æ˜¯è¿™ä¸ªç±»å‹ï¼ˆå³ä½¿ä¸€ä¸ªå¼•ç”¨ç±»å‹ï¼‰ï¼Œè€Œä¸æ˜¯æŒ‡é’ˆã€‚ å…·ä½“çš„è¿”å›å€¼ï¼Œä¾èµ–å…·ä½“ä¼ å…¥çš„ç±»å‹ã€‚ //åˆ›å»ºå®ä¾‹ rect1:=new(Rect) //newä¸€ä¸ªå¯¹è±¡ rect2:=\u0026Rect{} //ä¸ºèµ‹å€¼é»˜è®¤å€¼ï¼Œboolé»˜è®¤å€¼ä¸ºfalseï¼Œinté»˜è®¤ä¸ºé›¶å€¼0ï¼Œstringé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸² rect3:=\u0026Rect{0,0,100,200} //å–åœ°å€å¹¶èµ‹å€¼,æŒ‰å£°æ˜çš„å˜é‡é¡ºåºä¾æ¬¡èµ‹å€¼ rect4:=\u0026Rect{width:100,height:200} //æŒ‰å˜é‡åèµ‹å€¼ä¸æŒ‰é¡ºåºèµ‹å€¼ //æ„é€ å‡½æ•°ï¼šæ²¡æœ‰æ„é€ å‚æ•°çš„æ¦‚å¿µï¼Œé€šå¸¸ç”±å…¨å±€çš„åˆ›å»ºå‡½æ•°NewXXXæ¥å®ç°æ„é€ å‡½æ•°çš„åŠŸèƒ½ func NewRect(x,y,width,height float64) *Rect{ return \u0026Rect{x,y,width,height} //åˆ©ç”¨æŒ‡é’ˆæ¥æ”¹å˜ä¼ å…¥å‚æ•°çš„å€¼è¾¾åˆ°ç±»ä¼¼æ„é€ å‚æ•°çš„æ•ˆæœ } //æ–¹æ³•çš„é‡è½½,Goä¸æ”¯æŒæ–¹æ³•çš„é‡è½½ï¼ˆå‡½æ•°åŒåï¼Œå‚æ•°ä¸åŒï¼‰ //v â€¦interface{}è¡¨ç¤ºå‚æ•°ä¸å®šçš„æ„æ€ï¼Œå…¶ä¸­væ˜¯sliceç±»å‹ï¼ŒåŠå£°æ˜ä¸å®šå‚æ•°ï¼Œå¯ä»¥ä¼ å…¥ä»»æ„å‚æ•°ï¼Œå®ç°ç±»ä¼¼æ–¹æ³•çš„é‡è½½ func (poem *Poem) recite(v ...interface{}) { fmt.Println(v) } ï¼ˆä¸‰ï¼‰åŒ¿åç»„åˆ[ç»§æ‰¿] â€‹ ç»„åˆï¼Œå³æ–¹æ³•ä»£ç†ï¼Œä¾‹å¦‚AåŒ…å«Bï¼Œå³Aé€šè¿‡æ¶ˆæ¯ä¼ é€’çš„å½¢å¼ä»£ç†äº†Bçš„æ–¹æ³•ï¼Œè€Œä¸éœ€è¦é‡å¤å†™Bçš„æ–¹æ³•ã€‚\nâ€‹ ç»§æ‰¿æ˜¯æŒ‡è¿™æ ·ä¸€ç§èƒ½åŠ›ï¼šå®ƒå¯ä»¥ä½¿ç”¨ç°æœ‰ç±»çš„æ‰€æœ‰åŠŸèƒ½ï¼Œå¹¶åœ¨æ— éœ€é‡æ–°ç¼–å†™åŸæ¥çš„ç±»çš„æƒ…å†µä¸‹å¯¹è¿™äº›åŠŸèƒ½è¿›è¡Œæ‰©å±•ã€‚ç»§æ‰¿ä¸»è¦ä¸ºäº†ä»£ç å¤ç”¨ï¼Œç»§æ‰¿ä¹Ÿå¯ä»¥æ‰©å±•å·²å­˜åœ¨çš„ä»£ç æ¨¡å—ï¼ˆç±»ï¼‰ã€‚\nâ€‹ ä¸¥æ ¼æ¥è®²ï¼Œç»§æ‰¿æ˜¯â€œa kind of â€ï¼Œå³å­ç±»æ˜¯çˆ¶ç±»çš„ä¸€ç§ï¼Œä¾‹å¦‚studentæ˜¯personçš„ä¸€ç§ï¼›ç»„åˆæ˜¯â€œa part ofâ€ï¼Œå³çˆ¶ç±»æ˜¯å­ç±»ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œä¾‹å¦‚çœ¼ç›æ˜¯å¤´éƒ¨çš„ä¸€éƒ¨åˆ†ã€‚\n//1ã€åŒ¿åç»„åˆçš„æ–¹å¼å®ç°äº†ç±»ä¼¼Javaç»§æ‰¿çš„åŠŸèƒ½ï¼Œå¯ä»¥å®ç°å¤šç»§æ‰¿ type Base struct{ Name string } func (base *Base) Foo(){...} //Baseçš„Foo()æ–¹æ³• func (base *Base) Bar(){...} //Baseçš„Bar()æ–¹æ³• type Foo struct{ Base //é€šè¿‡ç»„åˆçš„æ–¹å¼å£°æ˜äº†åŸºç±»ï¼Œå³ç»§æ‰¿äº†åŸºç±» ... } func (foo *Foo) Bar(){ foo.Base.Bar() //å¹¶æ”¹å†™äº†åŸºç±»çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®ç°æ—¶å…ˆè°ƒç”¨åŸºç±»çš„Bar()æ–¹æ³• ... //å¦‚æœæ²¡æœ‰æ”¹å†™å³ä¸ºç»§æ‰¿ï¼Œè°ƒç”¨foo.Foo()å’Œè°ƒç”¨foo.Base.Foo()çš„ä½œç”¨çš„ä¸€æ ·çš„ } //ä¿®æ”¹å†…å­˜å¸ƒå±€ type Foo struct{ ... //å…¶ä»–æˆå‘˜ä¿¡æ¯ Base } //ä»¥æŒ‡é’ˆæ–¹å¼ç»„åˆ type Foo struct{ *Base //ä»¥æŒ‡é’ˆæ–¹å¼æ´¾ç”Ÿï¼Œåˆ›å»ºFooå®ä¾‹æ—¶ï¼Œéœ€è¦å¤–éƒ¨æä¾›ä¸€ä¸ªBaseç±»å®ä¾‹çš„æŒ‡é’ˆ ... } //åå­—å†²çªé—®é¢˜,ç»„åˆå†…å¤–å¦‚æœå‡ºç°åå­—é‡å¤é—®é¢˜ï¼Œåªä¼šè®¿é—®åˆ°æœ€å¤–å±‚ï¼Œå†…å±‚ä¼šè¢«éšè—ï¼Œä¸ä¼šæŠ¥é”™ï¼Œå³ç±»ä¼¼javaä¸­æ–¹æ³•è¦†ç›–/é‡å†™ã€‚ type X struct{ Name string } type Y struct{ X //Y.X.Nameä¼šè¢«éšè—ï¼Œå†…å±‚ä¼šè¢«éšè— Name string //åªä¼šè®¿é—®åˆ°Y.Nameï¼Œåªä¼šè°ƒç”¨å¤–å±‚å±æ€§ } ï¼ˆå››ï¼‰å¯è§æ€§[å°è£…] â€‹ å°è£…ï¼Œä¹Ÿå°±æ˜¯æŠŠå®¢è§‚äº‹ç‰©å°è£…æˆæŠ½è±¡çš„ç±»ï¼Œå¹¶ä¸”ç±»å¯ä»¥æŠŠè‡ªå·±çš„æ•°æ®å’Œæ–¹æ³•åªè®©å¯ä¿¡çš„ç±»æˆ–è€…å¯¹è±¡æ“ä½œï¼Œå¯¹ä¸å¯ä¿¡çš„è¿›è¡Œä¿¡æ¯éšè—ã€‚\nâ€‹ å°è£…çš„æœ¬è´¨æˆ–ç›®çš„å…¶å®ç¨‹åºå¯¹ä¿¡æ¯(æ•°æ®)çš„æ§åˆ¶åŠ›ã€‚å°è£…åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šè¯¥éšè—çš„éšè—ï¼Œè¯¥æš´éœ²çš„æš´éœ²ã€‚å°è£…å¯ä»¥éšè—å®ç°ç»†èŠ‚ï¼Œä½¿å¾—ä»£ç æ¨¡å—åŒ–ã€‚\nâ€‹ Goä¸­ç”¨å¤§å†™å­—æ¯å¼€å¤´æ¥è¡¨ç¤ºpublicï¼Œå¯ä»¥åŒ…å¤–è®¿é—®ï¼›å°å†™å­—æ¯å¼€å¤´æ¥è¡¨ç¤ºprivateï¼Œåªèƒ½åŒ…å†…è®¿é—®ï¼›è®¿é—®æ€§æ˜¯åŒ…çº§åˆ«éç±»å‹çº§åˆ« â€‹ å¦‚æœå¯è®¿é—®æ€§æ˜¯ç±»å‹ä¸€è‡´çš„ï¼Œå¯ä»¥åŠ friendå…³é”®å­—è¡¨ç¤ºæœ‹å‹å…³ç³»å¯äº’ç›¸è®¿é—®å½¼æ­¤çš„ç§æœ‰æˆå‘˜(å±æ€§å’Œæ–¹æ³•)\ntype Rect struct{ X,Y float64 Width,Height float64 //å­—æ¯å¤§å†™å¼€å¤´è¡¨ç¤ºè¯¥å±æ€§å¯ä»¥ç”±åŒ…å¤–è®¿é—®åˆ° } func (r *Rect) area() float64{ //å­—æ¯å°å†™å¼€å¤´è¡¨ç¤ºè¯¥æ–¹æ³•åªèƒ½åŒ…å†…è°ƒç”¨ return r.Width*r.Height } ï¼ˆäº”ï¼‰æ¥å£[å¤šæ€] â€‹ å¤šæ€æ€§ï¼ˆpolymorphisnï¼‰æ˜¯å…è®¸ä½ å°†çˆ¶å¯¹è±¡è®¾ç½®æˆä¸ºå’Œä¸€ä¸ªæˆ–æ›´å¤šçš„ä»–çš„å­å¯¹è±¡ç›¸ç­‰çš„æŠ€æœ¯ï¼Œèµ‹å€¼ä¹‹åï¼Œçˆ¶å¯¹è±¡å°±å¯ä»¥æ ¹æ®å½“å‰èµ‹å€¼ç»™å®ƒçš„å­å¯¹è±¡çš„ç‰¹æ€§ä»¥ä¸åŒçš„æ–¹å¼è¿ä½œã€‚\nâ€‹ ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯å…è®¸å°†å­ç±»ç±»å‹çš„æŒ‡é’ˆèµ‹å€¼ç»™çˆ¶ç±»ç±»å‹çš„æŒ‡é’ˆã€‚\nâ€‹ å³ä¸€ä¸ªå¼•ç”¨å˜é‡å€’åº•ä¼šæŒ‡å‘å“ªä¸ªç±»çš„å®ä¾‹å¯¹è±¡ï¼Œè¯¥å¼•ç”¨å˜é‡å‘å‡ºçš„æ–¹æ³•è°ƒç”¨åˆ°åº•æ˜¯å“ªä¸ªç±»ä¸­å®ç°çš„æ–¹æ³•ï¼Œå¿…é¡»åœ¨ç”±ç¨‹åºè¿è¡ŒæœŸé—´æ‰èƒ½å†³å®šã€‚ä¸ä¿®æ”¹ç¨‹åºä»£ç å°±å¯ä»¥æ”¹å˜ç¨‹åºè¿è¡Œæ—¶æ‰€ç»‘å®šçš„å…·ä½“ä»£ç ï¼Œè®©ç¨‹åºå¯ä»¥é€‰æ‹©å¤šä¸ªè¿è¡ŒçŠ¶æ€ï¼Œè¿™å°±æ˜¯å¤šæ€æ€§ã€‚å¤šæ€åˆ†ä¸ºç¼–è¯‘æ—¶å¤šæ€ï¼ˆé™æ€å¤šæ€ï¼‰å’Œè¿è¡Œæ—¶å¤šæ€ï¼ˆåŠ¨æ€å¤šæ€ï¼‰ï¼Œç¼–è¯‘æ—¶å¤šæ€ä¸€èˆ¬é€šè¿‡æ–¹æ³•é‡è½½å®ç°ï¼Œè¿è¡Œæ—¶å¤šæ€ä¸€èˆ¬é€šè¿‡æ–¹æ³•é‡å†™å®ç°ã€‚\n5.1æ¥å£æ¦‚å¿µ â€‹ æ¥å£å³ä¸€ç»„æ–¹æ³•çš„é›†åˆï¼Œå®šä¹‰äº†å¯¹è±¡çš„ä¸€ç»„è¡Œä¸ºï¼Œæ–¹æ³•åŒ…å«å®é™…çš„ä»£ç ã€‚æ¢å¥è¯è¯´ï¼Œä¸€ä¸ªæ¥å£å°±æ˜¯å®šä¹‰ï¼ˆè§„èŒƒæˆ–çº¦æŸï¼‰ï¼Œè€Œæ–¹æ³•å°±æ˜¯å®ç°ï¼Œæ¥å£çš„ä½œç”¨åº”è¯¥æ˜¯å°†å®šä¹‰ä¸å®ç°åˆ†ç¦»ï¼Œé™ä½è€¦åˆåº¦ã€‚ä¹ æƒ¯ç”¨â€œerâ€ç»“å°¾æ¥å‘½åï¼Œä¾‹å¦‚â€œReaderâ€ã€‚æ¥å£ä¸å¯¹è±¡çš„å…³ç³»æ˜¯å¤šå¯¹å¤šï¼Œå³ä¸€ä¸ªå¯¹è±¡å¯ä»¥å®ç°å¤šä¸ªæ¥å£ï¼Œä¸€ä¸ªæ¥å£ä¹Ÿå¯ä»¥è¢«å¤šä¸ªå¯¹è±¡å®ç°ã€‚\nâ€‹ æ¥å£æ˜¯Goè¯­è¨€æ•´ä¸ªç±»å‹ç³»ç»Ÿçš„åŸºçŸ³ï¼Œå…¶ä»–è¯­è¨€çš„æ¥å£æ˜¯ä¸åŒç»„ä»¶ä¹‹é—´çš„å¥‘çº¦çš„å­˜åœ¨ï¼Œå¯¹å¥‘çº¦çš„å®ç°æ˜¯å¼ºåˆ¶æ€§çš„ï¼Œå¿…é¡»æ˜¾å¼å£°æ˜å®ç°äº†è¯¥æ¥å£ï¼Œè¿™ç±»æ¥å£ç§°ä¹‹ä¸ºâ€œä¾µå…¥å¼æ¥å£â€ã€‚è€ŒGoè¯­è¨€çš„æ¥å£æ˜¯éšå¼å­˜åœ¨ï¼Œåªè¦å®ç°äº†è¯¥æ¥å£çš„æ‰€æœ‰å‡½æ•°åˆ™ä»£è¡¨å·²ç»å®ç°äº†è¯¥æ¥å£ï¼Œå¹¶ä¸éœ€è¦æ˜¾å¼çš„æ¥å£å£°æ˜ã€‚\næ¥å£çš„æ¯”å–» â€‹ ä½ çš„ç”µè„‘ä¸Šåªæœ‰ä¸€ä¸ªUSBæ¥å£ã€‚è¿™ä¸ªUSBæ¥å£å¯ä»¥æ¥MP3ï¼Œæ•°ç ç›¸æœºï¼Œæ‘„åƒå¤´ï¼Œé¼ æ ‡ï¼Œé”®ç›˜ç­‰ã€‚ã€‚ã€‚æ‰€æœ‰çš„ä¸Šè¿°ç¡¬ä»¶éƒ½å¯ä»¥å…¬ç”¨è¿™ä¸ªæ¥å£ï¼Œæœ‰å¾ˆå¥½çš„æ‰©å±•æ€§ï¼Œè¯¥USBæ¥å£å®šä¹‰äº†ä¸€ç§è§„èŒƒï¼Œåªè¦å®ç°äº†è¯¥è§„èŒƒï¼Œå°±å¯ä»¥å°†ä¸åŒçš„è®¾å¤‡æ¥å…¥ç”µè„‘ï¼Œè€Œè®¾å¤‡çš„æ”¹å˜å¹¶ä¸ä¼šå¯¹ç”µè„‘æœ¬èº«æœ‰ä»€ä¹ˆå½±å“ï¼ˆä½è€¦åˆï¼‰ã€‚\né¢å‘æ¥å£ç¼–ç¨‹ â€‹ æ¥å£è¡¨ç¤ºè°ƒç”¨è€…å’Œè®¾è®¡è€…çš„ä¸€ç§çº¦å®šï¼Œåœ¨å¤šäººåˆä½œå¼€å‘åŒä¸€ä¸ªé¡¹ç›®æ—¶ï¼Œäº‹å…ˆå®šä¹‰å¥½ç›¸äº’è°ƒç”¨çš„æ¥å£å¯ä»¥å¤§å¤§æé«˜å¼€å‘çš„æ•ˆç‡ã€‚æ¥å£æ˜¯ç”¨ç±»æ¥å®ç°çš„ï¼Œå®ç°æ¥å£çš„ç±»å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ¥å£çš„å£°æ˜æ¥å®ç°æ¥å£æä¾›çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ‰äº†æ¥å£ï¼Œå°±å¯ä»¥åœ¨ä¸å½±å“ç°æœ‰æ¥å£å£°æ˜çš„æƒ…å†µä¸‹ï¼Œä¿®æ”¹æ¥å£çš„å†…éƒ¨å®ç°ï¼Œä»è€Œä½¿å…¼å®¹æ€§é—®é¢˜æœ€å°åŒ–ã€‚ â€‹ å½“å…¶ä»–è®¾è®¡è€…è°ƒç”¨äº†æ¥å£åï¼Œå°±ä¸èƒ½å†éšæ„æ›´æ”¹æ¥å£çš„å®šä¹‰ï¼Œå¦åˆ™é¡¹ç›®å¼€å‘è€…äº‹å…ˆçš„çº¦å®šå°±å¤±å»äº†æ„ä¹‰ã€‚ä½†æ˜¯å¯ä»¥åœ¨ç±»ä¸­ä¿®æ”¹ç›¸åº”çš„ä»£ç ï¼Œå®Œæˆéœ€è¦æ”¹åŠ¨çš„å†…å®¹ã€‚\n5.2éä¾µå…¥å¼æ¥å£ éä¾µå…¥å¼æ¥å£ï¼šä¸€ä¸ªç±»åªéœ€è¦å®ç°äº†æ¥å£è¦æ±‚çš„æ‰€æœ‰å‡½æ•°å°±è¡¨ç¤ºå®ç°äº†è¯¥æ¥å£ï¼Œå¹¶ä¸éœ€è¦æ˜¾å¼å£°æ˜\ntype File struct{ //ç±»çš„å±æ€§ } //Fileç±»çš„æ–¹æ³• func (f *File) Read(buf []byte) (n int,err error) func (f *File) Write(buf []byte) (n int,err error) func (f *File) Seek(off int64,whence int) (pos int64,err error) func (f *File) Close() error //æ¥å£1ï¼šIFile type IFile interface{ Read(buf []byte) (n int,err error) Write(buf []byte) (n int,err error) Seek(off int64,whence int) (pos int64,err error) Close() error } //æ¥å£2ï¼šIReader type IReader interface{ Read(buf []byte) (n int,err error) } //æ¥å£èµ‹å€¼,Fileç±»å®ç°äº†IFileå’ŒIReaderæ¥å£ï¼Œå³æ¥å£æ‰€åŒ…å«çš„æ‰€æœ‰æ–¹æ³• var file1 IFile = new(File) var file2 IReader = new(File) 5.3æ¥å£èµ‹å€¼ åªè¦ç±»å®ç°äº†è¯¥æ¥å£çš„æ‰€æœ‰æ–¹æ³•ï¼Œå³å¯å°†è¯¥ç±»èµ‹å€¼ç»™è¿™ä¸ªæ¥å£ï¼Œæ¥å£ä¸»è¦ç”¨äºå¤šæ€åŒ–æ–¹æ³•ã€‚å³å¯¹æ¥å£å®šä¹‰çš„æ–¹æ³•ï¼Œä¸åŒçš„å®ç°æ–¹å¼ã€‚\næ¥å£èµ‹å€¼ï¼š 1ï¼‰å°†å¯¹è±¡å®ä¾‹èµ‹å€¼ç»™æ¥å£\ntype IUSB interface{ //å®šä¹‰IUSBçš„æ¥å£æ–¹æ³• } //æ–¹æ³•å®šä¹‰åœ¨ç±»å¤–ï¼Œç»‘å®šè¯¥ç±»ï¼Œä»¥ä¸‹ä¸ºæ–¹ä¾¿ï¼Œå¤‡æ³¨å†™åœ¨ç±»ä¸­ type MP3 struct{ //å®ç°IUSBçš„æ¥å£ï¼Œå…·ä½“å®ç°æ–¹å¼æ˜¯MP3çš„æ–¹æ³• } type Mouse struct{ //å®ç°IUSBçš„æ¥å£ï¼Œå…·ä½“å®ç°æ–¹å¼æ˜¯Mouseçš„æ–¹æ³• } //æ¥å£èµ‹å€¼ç»™å…·ä½“çš„å¯¹è±¡å®ä¾‹MP3 var usb IUSB =new(MP3) usb.Connect() usb.Close() //æ¥å£èµ‹å€¼ç»™å…·ä½“çš„å¯¹è±¡å®ä¾‹Mouse var usb IUSB =new(Mouse) usb.Connect() usb.Close() 2ï¼‰å°†æ¥å£èµ‹å€¼ç»™å¦ä¸€ä¸ªæ¥å£\nåªè¦ä¸¤ä¸ªæ¥å£æ‹¥æœ‰ç›¸åŒçš„æ–¹æ³•åˆ—è¡¨ï¼ˆä¸æ¬¡åºæ— å…³ï¼‰ï¼Œå³æ˜¯ä¸¤ä¸ªç›¸åŒçš„æ¥å£ï¼Œå¯ä»¥ç›¸äº’èµ‹å€¼ æ¥å£èµ‹å€¼åªéœ€è¦æ¥å£Açš„æ–¹æ³•åˆ—è¡¨æ˜¯æ¥å£Bçš„å­é›†ï¼ˆå³å‡è®¾æ¥å£Aä¸­å®šä¹‰çš„æ‰€æœ‰æ–¹æ³•ï¼Œéƒ½åœ¨æ¥å£Bä¸­æœ‰å®šä¹‰ï¼‰ï¼Œé‚£ä¹ˆBæ¥å£çš„å®ä¾‹å¯ä»¥èµ‹å€¼ç»™Açš„å¯¹è±¡ã€‚åä¹‹ä¸æˆç«‹ï¼Œå³å­æ¥å£BåŒ…å«äº†çˆ¶æ¥å£Aï¼Œå› æ­¤å¯ä»¥å°†å­æ¥å£çš„å®ä¾‹èµ‹å€¼ç»™çˆ¶æ¥å£ã€‚ å³å­æ¥å£å®ä¾‹å®ç°äº†å­æ¥å£çš„æ‰€æœ‰æ–¹æ³•ï¼Œè€Œçˆ¶æ¥å£çš„æ–¹æ³•åˆ—è¡¨æ˜¯å­æ¥å£çš„å­é›†ï¼Œåˆ™å­æ¥å£å®ä¾‹è‡ªç„¶å®ç°äº†çˆ¶æ¥å£çš„æ‰€æœ‰æ–¹æ³•ï¼Œå› æ­¤å¯ä»¥å°†å­æ¥å£å®ä¾‹èµ‹å€¼ç»™çˆ¶æ¥å£ã€‚ type Writer interface{ //çˆ¶æ¥å£ Write(buf []byte) (n int,err error) } type ReadWriter interface{ //å­æ¥å£ Read(buf []byte) (n int,err error) Write(buf []byte) (n int,err error) } var file1 ReadWriter=new(File) //å­æ¥å£å®ä¾‹ var file2 Writer=file1 //å­æ¥å£å®ä¾‹èµ‹å€¼ç»™çˆ¶æ¥å£ 5.4æ¥å£æŸ¥è¯¢ è‹¥è¦åœ¨ switch å¤–åˆ¤æ–­ä¸€ä¸ªæ¥å£ç±»å‹æ˜¯å¦å®ç°äº†æŸä¸ªæ¥å£ï¼Œå¯ä»¥ä½¿ç”¨â€œé€—å· ok â€ã€‚\nvalue, ok := Interfacevariable.(implementType)\nå…¶ä¸­ Interfacevariable æ˜¯æ¥å£å˜é‡ï¼ˆæ¥å£å€¼ï¼‰ï¼ŒimplementType ä¸ºå®ç°æ­¤æ¥å£çš„ç±»å‹ï¼Œvalue è¿”å›æ¥å£å˜é‡å®é™…ç±»å‹å˜é‡çš„å€¼ï¼Œå¦‚æœè¯¥ç±»å‹å®ç°äº†æ­¤æ¥å£è¿”å› trueã€‚\n//åˆ¤æ–­file1æ¥å£æŒ‡å‘çš„å¯¹è±¡å®ä¾‹æ˜¯å¦æ˜¯Fileç±»å‹ var file1 Writer=... if file5,ok:=file1.(File);ok{ ... } 5.5æ¥å£ç±»å‹æŸ¥è¯¢ åœ¨ Go ä¸­ï¼Œè¦åˆ¤æ–­ä¼ é€’ç»™æ¥å£å€¼çš„å˜é‡ç±»å‹ï¼Œå¯ä»¥åœ¨ä½¿ç”¨ type switch å¾—åˆ°ã€‚(type)åªèƒ½åœ¨ switch ä¸­ä½¿ç”¨ã€‚\n// å¦ä¸€ä¸ªå®ç°äº† I æ¥å£çš„ R ç±»å‹ type R struct { i int } func (p *R) Get() int { return p.i } func (p *R) Put(v int) { p.i = v } func f(p I) { switch t := p.(type) { // åˆ¤æ–­ä¼ é€’ç»™ p çš„å®é™…ç±»å‹ case *S: // æŒ‡å‘ S çš„æŒ‡é’ˆç±»å‹ case *R: // æŒ‡å‘ R çš„æŒ‡é’ˆç±»å‹ case S: // S ç±»å‹ case R: // R ç±»å‹ default: //å®ç°äº† I æ¥å£çš„å…¶ä»–ç±»å‹ } } 5.6æ¥å£ç»„åˆ //æ¥å£ç»„åˆç±»ä¼¼ç±»å‹ç»„åˆï¼Œåªä¸è¿‡åªåŒ…å«æ–¹æ³•ï¼Œä¸åŒ…å«æˆå‘˜å˜é‡ type ReadWriter interface{ //æ¥å£ç»„åˆï¼Œé¿å…ä»£ç é‡å¤ Reader //æ¥å£Reader Writer //æ¥å£Writer } 5.7Anyç±»å‹[ç©ºæ¥å£] æ¯ç§ç±»å‹éƒ½èƒ½åŒ¹é…åˆ°ç©ºæ¥å£ï¼šinterface{}ã€‚ç©ºæ¥å£ç±»å‹å¯¹æ–¹æ³•æ²¡æœ‰ä»»ä½•çº¦æŸï¼ˆå› ä¸ºæ²¡æœ‰æ–¹æ³•ï¼‰ï¼Œå®ƒèƒ½åŒ…å«ä»»æ„ç±»å‹ï¼Œä¹Ÿå¯ä»¥å®ç°åˆ°å…¶ä»–æ¥å£ç±»å‹çš„è½¬æ¢ã€‚å¦‚æœä¼ é€’ç»™è¯¥æ¥å£çš„ç±»å‹å˜é‡å®ç°äº†è½¬æ¢åçš„æ¥å£åˆ™å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œå¦åˆ™å‡ºç°è¿è¡Œæ—¶é”™è¯¯ã€‚\n//interface{}å³ä¸ºå¯ä»¥æŒ‡å‘ä»»ä½•å¯¹è±¡çš„Anyç±»å‹ï¼Œç±»ä¼¼Javaä¸­çš„Objectç±» var v1 interface{}=struct{X int}{1} var v2 interface{}=\"abc\" func DoSomething(v interface{}) { //è¯¥å‡½æ•°å¯ä»¥æ¥æ”¶ä»»ä½•ç±»å‹çš„å‚æ•°ï¼Œå› ä¸ºä»»ä½•ç±»å‹éƒ½å®ç°äº†ç©ºæ¥å£ // ... } 5.8æ¥å£çš„ä»£ç ç¤ºä¾‹ //æ¥å£animal type Animal interface { Speak() string } //Dogç±»å®ç°animalæ¥å£ type Dog struct { } func (d Dog) Speak() string { return \"Woof!\" } //Catç±»å®ç°animalæ¥å£ type Cat struct { } func (c Cat) Speak() string { return \"Meow!\" } //Llamaå®ç°animalæ¥å£ type Llama struct { } func (l Llama) Speak() string { return \"?????\" } //JavaProgrammerå®ç°animalæ¥å£ type JavaProgrammer struct { } func (j JavaProgrammer) Speak() string { return \"Design patterns!\" } //ä¸»å‡½æ•° func main() { animals := []Animal{Dog{}, Cat{}, Llama{}, JavaProgrammer{}} //åˆ©ç”¨æ¥å£å®ç°å¤šæ€ for _, animal := range animals { fmt.Println(animal.Speak()) //æ‰“å°ä¸åŒå®ç°è¯¥æ¥å£çš„ç±»çš„æ–¹æ³•è¿”å›å€¼ } } ","categories":"","description":"","excerpt":" é¢å‘å¯¹è±¡ç¼–ç¨‹ â€‹ æŠŠä¸€ç»„æ•°æ®ç»“æ„å’Œå¤„ç†å®ƒä»¬çš„æ–¹æ³•ç»„æˆå¯¹è±¡ï¼ˆobjectï¼‰ï¼ŒæŠŠç›¸åŒè¡Œä¸ºçš„å¯¹è±¡å½’çº³ä¸ºç±»ï¼ˆclassï¼‰ï¼Œé€šè¿‡ç±»çš„å° â€¦","ref":"/golang-notes/oop/golang-object-oriented-programming/","tags":["Golang"],"title":"Golangç³»åˆ—ï¼ˆäºŒï¼‰ä¹‹é¢å‘å¯¹è±¡ç¼–ç¨‹"},{"body":"ï¼ˆä¸€ï¼‰å¹¶å‘åŸºç¡€ 1.æ¦‚å¿µ å¹¶å‘æ„å‘³ç€ç¨‹åºåœ¨è¿è¡Œæ—¶æœ‰å¤šä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå¯¹åº”å¤šä¸ªè°ƒç”¨æ ˆã€‚\nå¹¶å‘ä¸å¹¶è¡Œçš„åŒºåˆ«ï¼š\nå¹¶å‘çš„ä¸»æµå®ç°æ¨¡å‹ï¼š\nå®ç°æ¨¡å‹ è¯´æ˜ ç‰¹ç‚¹ å¤šè¿›ç¨‹ æ“ä½œç³»ç»Ÿå±‚é¢çš„å¹¶å‘æ¨¡å¼ å¤„ç†ç®€å•ï¼Œäº’ä¸å½±å“ï¼Œä½†å¼€é”€å¤§ å¤šçº¿ç¨‹ ç³»ç»Ÿå±‚é¢çš„å¹¶å‘æ¨¡å¼ æœ‰æ•ˆï¼Œå¼€é”€è¾ƒå¤§ï¼Œé«˜å¹¶å‘æ—¶å½±å“æ•ˆç‡ åŸºäºå›è°ƒçš„éé˜»å¡/å¼‚æ­¥IO å¤šç”¨äºé«˜å¹¶å‘æœåŠ¡å™¨å¼€å‘ä¸­ ç¼–ç¨‹å¤æ‚ï¼Œå¼€é”€å° åç¨‹ ç”¨æˆ·æ€çº¿ç¨‹ï¼Œä¸éœ€è¦æ“ä½œç³»ç»ŸæŠ¢å è°ƒåº¦ï¼Œå¯„å­˜äºçº¿ç¨‹ä¸­ ç¼–ç¨‹ç®€å•ï¼Œç»“æ„ç®€å•ï¼Œå¼€é”€æå°ï¼Œä½†éœ€è¦è¯­è¨€çš„æ”¯æŒ å…±äº«å†…å­˜ç³»ç»Ÿï¼šçº¿ç¨‹ä¹‹é—´é‡‡ç”¨å…±äº«å†…å­˜çš„æ–¹å¼é€šä¿¡ï¼Œé€šè¿‡åŠ é”æ¥é¿å…æ­»é”æˆ–èµ„æºç«äº‰ã€‚\næ¶ˆæ¯ä¼ é€’ç³»ç»Ÿï¼šå°†çº¿ç¨‹é—´å…±äº«çŠ¶æ€å°è£…åœ¨æ¶ˆæ¯ä¸­ï¼Œé€šè¿‡å‘é€æ¶ˆæ¯æ¥å…±äº«å†…å­˜ï¼Œè€Œéé€šè¿‡å…±äº«å†…å­˜æ¥é€šä¿¡ã€‚\n2.åç¨‹ æ‰§è¡Œä½“æ˜¯ä¸ªæŠ½è±¡çš„æ¦‚å¿µï¼Œåœ¨æ“ä½œç³»ç»Ÿä¸­åˆ†ä¸ºä¸‰ä¸ªçº§åˆ«ï¼šè¿›ç¨‹ï¼ˆprocessï¼‰ï¼Œè¿›ç¨‹å†…çš„çº¿ç¨‹ï¼ˆthreadï¼‰ï¼Œè¿›ç¨‹å†…çš„åç¨‹ï¼ˆcoroutineï¼Œè½»é‡çº§çº¿ç¨‹ï¼‰ã€‚åç¨‹çš„æ•°é‡çº§å¯è¾¾åˆ°ä¸Šç™¾ä¸‡ä¸ªï¼Œè¿›ç¨‹å’Œçº¿ç¨‹çš„æ•°é‡çº§æœ€å¤šä¸è¶…è¿‡ä¸€ä¸‡ä¸ªã€‚Goè¯­è¨€ä¸­çš„åç¨‹å«goroutineï¼ŒGoæ ‡å‡†åº“æä¾›çš„è°ƒç”¨æ“ä½œï¼ŒIOæ“ä½œéƒ½ä¼šå‡ºè®©CPUç»™å…¶ä»–goroutineï¼Œè®©åç¨‹é—´çš„åˆ‡æ¢ç®¡ç†ä¸ä¾èµ–ç³»ç»Ÿçš„çº¿ç¨‹å’Œè¿›ç¨‹ï¼Œä¸ä¾èµ–CPUçš„æ ¸å¿ƒæ•°é‡ã€‚\n3.å¹¶å‘é€šä¿¡ å¹¶å‘ç¼–ç¨‹çš„éš¾åº¦åœ¨äºåè°ƒï¼Œåè°ƒéœ€è¦é€šè¿‡é€šä¿¡ï¼Œå¹¶å‘é€šä¿¡æ¨¡å‹åˆ†ä¸ºå…±äº«æ•°æ®å’Œæ¶ˆæ¯ã€‚å…±äº«æ•°æ®å³å¤šä¸ªå¹¶å‘å•å…ƒä¿æŒå¯¹åŒä¸€ä¸ªæ•°æ®çš„å¼•ç”¨ï¼Œæ•°æ®å¯ä»¥æ˜¯å†…å­˜æ•°æ®å—ï¼Œç£ç›˜æ–‡ä»¶ï¼Œç½‘ç»œæ•°æ®ç­‰ã€‚æ•°æ®å…±äº«é€šè¿‡åŠ é”çš„æ–¹å¼æ¥é¿å…æ­»é”å’Œèµ„æºç«äº‰ã€‚Goè¯­è¨€åˆ™é‡‡å–æ¶ˆæ¯æœºåˆ¶æ¥é€šä¿¡ï¼Œæ¯ä¸ªå¹¶å‘å•å…ƒæ˜¯ç‹¬ç«‹çš„ä¸ªä½“ï¼Œæœ‰ç‹¬ç«‹çš„å˜é‡ï¼Œä¸åŒå¹¶å‘å•å…ƒé—´è¿™äº›å˜é‡ä¸å…±äº«ï¼Œæ¯ä¸ªå¹¶å‘å•å…ƒçš„è¾“å…¥è¾“å‡ºåªé€šè¿‡æ¶ˆæ¯çš„æ–¹å¼ã€‚\nï¼ˆäºŒï¼‰goroutine 1. goå…³é”®å­— //å®šä¹‰è°ƒç”¨ä½“ func Add(x,y int){ z:=x+y fmt.Println(z) } //goå…³é”®å­—æ‰§è¡Œè°ƒç”¨ï¼Œå³ä¼šäº§ç”Ÿä¸€ä¸ªgoroutineå¹¶å‘æ‰§è¡Œ //å½“å‡½æ•°è¿”å›æ—¶ï¼Œgoroutineè‡ªåŠ¨ç»“æŸï¼Œå¦‚æœæœ‰è¿”å›å€¼,è¿”å›å€¼ä¼šè‡ªåŠ¨è¢«ä¸¢å¼ƒ go Add(1,1) //å¹¶å‘æ‰§è¡Œ func main(){ for i:=0;i\u003c10;i++{//ä¸»å‡½æ•°å¯åŠ¨äº†10ä¸ªgoroutineï¼Œç„¶åè¿”å›ï¼Œç¨‹åºé€€å‡ºï¼Œå¹¶ä¸ä¼šç­‰å¾…å…¶ä»–goroutineç»“æŸ go Add(i,i) //æ‰€ä»¥éœ€è¦é€šè¿‡channelé€šä¿¡æ¥ä¿è¯å…¶ä»–goroutineå¯ä»¥é¡ºåˆ©æ‰§è¡Œ } } 2. sync.WaitGroup sync.WaitGroupç”¨æ¥å®ç°å¯åŠ¨ä¸€ç»„goroutineï¼Œå¹¶ç­‰å¾…ä»»åŠ¡åšå®Œå†ç»“æŸgoroutineã€‚\nä½¿ç”¨æ–¹æ³•æ˜¯ï¼š\nwg.Add()ï¼šmainåç¨‹é€šè¿‡è°ƒç”¨ wg.Add(delta int) è®¾ç½®workeråç¨‹çš„ä¸ªæ•°ï¼Œç„¶ååˆ›å»ºworkeråç¨‹ï¼› wg.Done()ï¼šworkeråç¨‹æ‰§è¡Œç»“æŸä»¥åï¼Œéƒ½è¦è°ƒç”¨ wg.Done()ï¼Œè¡¨ç¤ºåšå®Œä»»åŠ¡ï¼Œgoroutineå‡1ï¼› wg.Wait() ï¼šmainåç¨‹è°ƒç”¨ wg.Wait() ä¸”è¢«blockï¼Œç›´åˆ°æ‰€æœ‰workeråç¨‹å…¨éƒ¨æ‰§è¡Œç»“æŸåè¿”å›ã€‚ é’ˆå¯¹å¯èƒ½panicçš„goroutineï¼Œå¯ä»¥ä½¿ç”¨defer wg.Done()æ¥ç»“æŸgoroutineã€‚ ç¤ºä¾‹ï¼š\npackage main import ( \"fmt\" \"sync\" ) func main() { var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { fmt.Println(i) wg.Done() }(i) } wg.Wait() } è¾“å‡ºå¦‚ä¸‹ï¼Œéšæœºè¾“å‡º0åˆ°9çš„æ•°å­—\n9 5 6 7 8 1 0 3 4 2 3. sync.Map Go è¯­è¨€åŸç”Ÿ map å¹¶ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯¹å®ƒè¿›è¡Œå¹¶å‘è¯»å†™æ“ä½œçš„æ—¶å€™ï¼Œéœ€è¦åŠ é”ã€‚sync.map åˆ™æ˜¯ä¸€ç§å¹¶å‘å®‰å…¨çš„ mapï¼Œå¯ä»¥ä½¿ç”¨åœ¨å¹¶å‘è¯»å†™mapçš„åœºæ™¯ä¸­ã€‚\nsync.Mapå¸¸è§æ“ä½œï¼š\nå†™å…¥ï¼šm.Store(\"1\", 18) è¯»å–ï¼šage, ok := m.Load(\"1\") åˆ é™¤ï¼šm.Delete(\"1\") éå†ï¼šm.Range(func(key, value interface{}) bool{} å­˜åœ¨åˆ™è¯»å–å¦åˆ™å†™å…¥ï¼š\tm.LoadOrStore(\"2\", 100) package main import ( \"fmt\" \"sync\" ) func main() { var m sync.Map // 1. å†™å…¥ m.Store(\"1\", 18) m.Store(\"2\", 20) // 2. è¯»å– age, ok := m.Load(\"1\") fmt.Println(age, ok) // 3. éå† m.Range(func(key, value interface{}) bool { name := key.(string) age := value.(int) fmt.Println(name, age) return true }) // 4. åˆ é™¤ m.Delete(\"1\") age, ok = m.Load(\"1\") fmt.Println(age, ok) // 5. å¦‚æœkeyå­˜åœ¨åˆ™è¯»å–ï¼Œå¦åˆ™å†™å…¥ç»™å®šçš„å€¼ m.LoadOrStore(\"2\", 100) age, _ = m.Load(\"2\") fmt.Println(age) } ç¤ºä¾‹ï¼š\n3.1. ä¸åŠ é”çš„mapå¹¶å‘è¯»å†™ ä»¥ä¸‹ä½¿ç”¨çº¿ç¨‹ä¸å®‰å…¨çš„mapï¼Œè¿›è¡Œå¹¶å‘å†™å…¥ï¼Œå°±ä¼šå‡ºç°å¹¶å‘æŠ¥é”™ã€‚å¯ä»¥é€šè¿‡åŠ é”æ¥è§£å†³å¹¶å‘é—®é¢˜ï¼Œä½†æ›´æ¨èä½¿ç”¨sync.Mapæ¥å®ç°ã€‚\nç¤ºä¾‹1ï¼š\npackage main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { // ç”Ÿæˆéšæœºç§å­ rand.Seed(time.Now().Unix()) sm := make(map[int]int) var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { for j := 0; j \u003c= 9; j++ { r := rand.Intn(100) // ç”Ÿæˆ0-99çš„éšæœºæ•° sm[j] = r // åŒæ—¶å¯¹mapè¿›è¡Œå¹¶å‘å†™å…¥ } wg.Done() }(i) } wg.Wait() // æ‰“å°mapä¸­çš„å€¼ fmt.Println(sm) } è¾“å‡ºå¼‚å¸¸ï¼š\nmapä¸èƒ½å¹¶å‘å†™å…¥ã€‚\nfatal error: concurrent map writes 3.2. åŠ é”çš„å¹¶å‘è¯»å†™ ç¤ºä¾‹2ï¼š\npackage main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { // ç”Ÿæˆéšæœºç§å­ rand.Seed(time.Now().Unix()) sm := \u0026SafeMap{ Map: make(map[int]int), } var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { for j := 0; j \u003c= 9; j++ { r := rand.Intn(100) // ç”Ÿæˆ0-99çš„éšæœºæ•° sm.Set(i, r) // åŒæ—¶å¯¹mapè¿›è¡Œå¹¶å‘å†™å…¥ } wg.Done() }(i) } wg.Wait() // æ‰“å°mapä¸­çš„å€¼ fmt.Println(sm.Map) } // SafeMap type SafeMap struct { Map map[int]int lock sync.RWMutex // åŠ é” } // Set func (m *SafeMap) Set(key, value int) { m.lock.Lock() defer m.lock.Unlock() m.Map[key] = value } // Get func (m *SafeMap) Get(key int) int { return m.Map[key] } æ­£å¸¸è¾“å‡º\nmap[0:52 1:16 2:86 3:50 4:97 5:38 6:54 7:75 8:26 9:32] 3.3. ä½¿ç”¨sync.Mapå¹¶å‘è¯»å†™ ç¤ºä¾‹3ï¼š\nä»¥ä¸‹ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„sync.Mapæ¥å®ç°å¯¹mapçš„å€¼è¿›è¡Œå¹¶å‘çš„è¯»å†™ã€‚\npackage main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { // ç”Ÿæˆéšæœºç§å­ rand.Seed(time.Now().Unix()) // ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„sync.Map var sm sync.Map var wg sync.WaitGroup for i := 0; i \u003c= 9; i++ { wg.Add(1) go func(i int) { for j := 0; j \u003c= 9; j++ { r := rand.Intn(100) // ç”Ÿæˆ0-99çš„éšæœºæ•° sm.Store(j, r) } wg.Done() }(i) } wg.Wait() // æ‰“å°mapä¸­çš„å€¼ sm.Range(func(k, v interface{}) bool { fmt.Println(k, v) return true }) } æ­£å¸¸è¾“å‡ºï¼š\n2 92 4 5 8 48 9 6 0 50 1 64 6 27 7 86 3 59 5 57 ï¼ˆä¸‰ï¼‰channel â€‹ channelå°±åƒç®¡é“çš„å½¢å¼ï¼Œæ˜¯goroutineä¹‹é—´çš„é€šä¿¡æ–¹å¼ï¼Œæ˜¯è¿›ç¨‹å†…çš„é€šä¿¡æ–¹å¼ï¼Œè·¨è¿›ç¨‹é€šä¿¡å»ºè®®ç”¨åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ–¹æ³•æ¥è§£å†³ï¼Œä¾‹å¦‚Socketæˆ–httpç­‰é€šä¿¡åè®®ã€‚channelæ˜¯ç±»å‹ç›¸å…³ï¼Œå³ä¸€ä¸ªchannelåªèƒ½ä¼ é€’ä¸€ç§ç±»å‹çš„å€¼ï¼Œåœ¨å£°æ˜æ—¶æŒ‡å®šã€‚\n1ã€åŸºæœ¬è¯­æ³• 1ï¼‰channelçš„å£°æ˜ //1ã€channelå£°æ˜ï¼Œå£°æ˜ä¸€ä¸ªç®¡é“chanNameï¼Œè¯¥ç®¡é“å¯ä»¥ä¼ é€’çš„ç±»å‹æ˜¯ElementType //ç®¡é“æ˜¯ä¸€ç§å¤åˆç±»å‹ï¼Œ[chan ElementType],è¡¨ç¤ºå¯ä»¥ä¼ é€’ElementTypeç±»å‹çš„ç®¡é“[ç±»ä¼¼å®šè¯­ä»å¥çš„ä¿®é¥°æ–¹æ³•] var chanName chan ElementType var ch chan int //å£°æ˜ä¸€ä¸ªå¯ä»¥ä¼ é€’intç±»å‹çš„ç®¡é“ var m map[string] chan bool //å£°æ˜ä¸€ä¸ªmapï¼Œå€¼çš„ç±»å‹ä¸ºå¯ä»¥ä¼ é€’boolç±»å‹çš„ç®¡é“ 2ï¼‰åˆå§‹åŒ– //2ã€åˆå§‹åŒ–ch:=make(chan int) //makeä¸€èˆ¬ç”¨æ¥å£°æ˜ä¸€ä¸ªå¤åˆç±»å‹ï¼Œå‚æ•°ä¸ºå¤åˆç±»å‹çš„å±æ€§ 3ï¼‰ç®¡é“è¯»å†™ //3ã€ç®¡é“å†™å…¥,æŠŠå€¼æƒ³è±¡æˆä¸€ä¸ªçƒï¼Œ\"\u003c-\"çš„æ–¹å‘ï¼Œè¡¨ç¤ºçƒçš„æµå‘ï¼Œchå³ä¸ºç®¡é“ //å†™å…¥æ—¶ï¼Œå½“ç®¡é“å·²æ»¡ï¼ˆç®¡é“æœ‰ç¼“å†²é•¿åº¦ï¼‰åˆ™ä¼šå¯¼è‡´ç¨‹åºå µå¡ï¼Œç›´åˆ°æœ‰goroutineä»ä¸­è¯»å–å‡ºå€¼ ch \u003c- value //ç®¡é“è¯»å–ï¼Œ\"\u003c-\"è¡¨ç¤ºä»ç®¡é“æŠŠçƒå€’å‡ºæ¥èµ‹å€¼ç»™ä¸€ä¸ªå˜é‡ //å½“ç®¡é“ä¸ºç©ºï¼Œè¯»å–æ•°æ®ä¼šå¯¼è‡´ç¨‹åºé˜»å¡ï¼Œç›´åˆ°æœ‰goroutineå†™å…¥å€¼ value:= \u003c-ch 4ï¼‰select //4ã€æ¯ä¸ªcaseå¿…é¡»æ˜¯ä¸€ä¸ªIOæ“ä½œï¼Œé¢å‘channelçš„æ“ä½œï¼Œåªæ‰§è¡Œå…¶ä¸­çš„ä¸€ä¸ªcaseæ“ä½œï¼Œä¸€æ—¦æ»¡è¶³åˆ™ç»“æŸselectè¿‡ç¨‹ //é¢å‘channelçš„æ“ä½œæ— éä¸‰ç§æƒ…å†µï¼šæˆåŠŸè¯»å‡ºï¼›æˆåŠŸå†™å…¥ï¼›å³æ²¡æœ‰è¯»å‡ºä¹Ÿæ²¡æœ‰å†™å…¥ select{ case \u003c-chan1: //å¦‚æœchan1è¯»åˆ°æ•°æ®ï¼Œåˆ™è¿›è¡Œè¯¥caseå¤„ç†è¯­å¥ case chan2\u003c-1: //å¦‚æœæˆåŠŸå‘chan2å†™å…¥æ•°æ®ï¼Œåˆ™è¿›å…¥è¯¥caseå¤„ç†è¯­å¥ default: //å¦‚æœä¸Šé¢éƒ½æ²¡æœ‰æˆåŠŸï¼Œåˆ™è¿›å…¥defaultå¤„ç†æµç¨‹ } 2ã€ç¼“å†²å’Œè¶…æ—¶æœºåˆ¶ 1ï¼‰ç¼“å†²æœºåˆ¶ //1ã€ç¼“å†²æœºåˆ¶ï¼šä¸ºç®¡é“æŒ‡å®šç©ºé—´é•¿åº¦ï¼Œè¾¾åˆ°ç±»ä¼¼æ¶ˆæ¯é˜Ÿåˆ—çš„æ•ˆæœ c:=make(chan int,1024) //ç¬¬äºŒä¸ªå‚æ•°ä¸ºç¼“å†²åŒºå¤§å°ï¼Œä¸åˆ‡ç‰‡çš„ç©ºé—´å¤§å°ç±»ä¼¼ //é€šè¿‡rangeå…³é”®å­—æ¥å®ç°ä¾æ¬¡è¯»å–ç®¡é“çš„æ•°æ®ï¼Œä¸æ•°ç»„æˆ–åˆ‡ç‰‡çš„rangeä½¿ç”¨æ–¹æ³•ç±»ä¼¼ for i :=range c{ fmt.Println(\"Received:\",i) } 2ï¼‰è¶…æ—¶æœºåˆ¶ //2ã€è¶…æ—¶æœºåˆ¶ï¼šåˆ©ç”¨selectåªè¦ä¸€ä¸ªcaseæ»¡è¶³ï¼Œç¨‹åºå°±ç»§ç»­æ‰§è¡Œè€Œä¸è€ƒè™‘å…¶ä»–caseçš„æƒ…å†µçš„ç‰¹æ€§å®ç°è¶…æ—¶æœºåˆ¶ timeout:=make(chan bool,1) //è®¾ç½®ä¸€ä¸ªè¶…æ—¶ç®¡é“ go func(){ time.Sleep(1e9) //è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œç­‰å¾…ä¸€åˆ†é’Ÿ timeout\u003c-true //ä¸€åˆ†é’Ÿåå¾€ç®¡é“æ”¾ä¸€ä¸ªtrueçš„å€¼ }() // select { case \u003c-ch: //å¦‚æœè¯»åˆ°æ•°æ®ï¼Œåˆ™ä¼šç»“æŸselectè¿‡ç¨‹ //ä»chä¸­è¯»å–æ•°æ® case \u003c-timeout: //å¦‚æœå‰é¢çš„caseæ²¡æœ‰è°ƒç”¨åˆ°ï¼Œå¿…å®šä¼šè¯»åˆ°trueå€¼ï¼Œç»“æŸselectï¼Œé¿å…æ°¸ä¹…ç­‰å¾… //ä¸€ç›´æ²¡æœ‰ä»chä¸­è¯»å–åˆ°æ•°æ®ï¼Œä½†ä»timeoutä¸­è¯»å–åˆ°äº†æ•°æ® } 3ã€channelçš„ä¼ é€’ //1ã€channelçš„ä¼ é€’ï¼Œæ¥å®ç°Linuxç³»ç»Ÿä¸­ç®¡é“çš„åŠŸèƒ½ï¼Œä»¥æ’ä»¶çš„æ–¹å¼å¢åŠ æ•°æ®å¤„ç†çš„æµç¨‹ type PipeData struct{ value int handler func(int) int //handleræ˜¯å±æ€§ï¼Ÿ next chan int //å¯ä»¥æŠŠ[chan int]çœ‹æˆä¸€ä¸ªæ•´ä½“ï¼Œè¡¨ç¤ºæ”¾intç±»å‹çš„ç®¡é“ } func handler(queue chan *PipeData){ //queueæ˜¯ä¸€ä¸ªå­˜æ”¾*PipeDateç±»å‹çš„ç®¡é“ï¼Œå¯æ”¹å˜ç®¡é“é‡Œçš„æ•°æ®å—å†…å®¹ for data:=range queue{ //dataçš„ç±»å‹å°±æ˜¯ç®¡é“å­˜æ”¾å®šä¹‰çš„ç±»å‹ï¼Œå³PipeData data.next \u003c- data.handler(data.value) //è¯¥æ–¹æ³•å®ç°å°†PipeDataçš„valueå€¼å­˜æ”¾åˆ°nextçš„ç®¡é“ä¸­ } } 4ã€å•å‘channel //2ã€å•å‘channelï¼šåªèƒ½ç”¨äºæ¥æ”¶æˆ–å‘é€æ•°æ®ï¼Œæ˜¯å¯¹channelçš„ä¸€ç§ä½¿ç”¨é™åˆ¶ //å•å‘channelçš„å£°æ˜ var ch1 chan int //æ­£å¸¸channelï¼Œå¯è¯»å†™ var ch2 chan\u003c- int //å•å‘åªå†™channel [chan\u003c- int]çœ‹æˆä¸€ä¸ªæ•´ä½“ï¼Œè¡¨ç¤ºæµå…¥ç®¡é“ var ch3 \u003c-chan int //å•å‘åªè¯»channel [\u003c-chan int]çœ‹æˆä¸€ä¸ªæ•´ä½“ï¼Œè¡¨ç¤ºæµå‡ºç®¡é“ //ç®¡é“ç±»å‹å¼ºåˆ¶è½¬æ¢ ch4:=make(chan int) //ch4ä¸ºåŒå‘ç®¡é“ ch5:=\u003c-chan int(ch4) //æŠŠ[\u003c-chan int]çœ‹æˆå•å‘åªè¯»ç®¡é“ç±»å‹ï¼Œå¯¹ch4è¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢ ch6:=chan\u003c- int(ch4) //æŠŠ[chan\u003c- int]çœ‹æˆå•å‘åªå†™ç®¡é“ç±»å‹ï¼Œå¯¹ch4è¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢ func Parse(ch \u003c-chan int){ //æœ€å°æƒé™åŸåˆ™ for value:=range ch{ fmt.Println(\"Parsing value\",value) } } 5ã€å…³é—­channel //3ã€å…³é—­channelï¼Œä½¿ç”¨å†…ç½®å‡½æ•°close()å‡½æ•°å³å¯ close(ch) //åˆ¤æ–­channelæ˜¯å¦å…³é—­ x,ok:=\u003c-ch //ok==falseè¡¨ç¤ºchannelå·²ç»å…³é—­ if !ok { //å¦‚æœchannelå…³é—­ï¼Œok==falseï¼Œ!ok==true //æ‰§è¡Œä½“ } ï¼ˆå››ï¼‰å¤šæ ¸å¹¶è¡ŒåŒ–ä¸åŒæ­¥é” 1ã€å¤šæ ¸å¹¶è¡ŒåŒ– //å¤šæ ¸å¹¶è¡ŒåŒ– runtime.GOMAXPROCS(16) //è®¾ç½®ç¯å¢ƒå˜é‡GOMAXPROCSçš„å€¼æ¥æ§åˆ¶ä½¿ç”¨å¤šå°‘ä¸ªCPUæ ¸å¿ƒ runtime.NumCPU() //æ¥è·å–æ ¸å¿ƒæ•° //å‡ºè®©æ—¶é—´ç‰‡ runtime.Gosched() //åœ¨æ¯ä¸ªgoroutineä¸­æ§åˆ¶ä½•æ—¶å‡ºè®©æ—¶é—´ç‰‡ç»™å…¶ä»–goroutine 2ã€åŒæ­¥é” //åŒæ­¥é” sync.Mutex //å•è¯»å•å†™ï¼šå ç”¨Mutexåï¼Œå…¶ä»–goroutineåªèƒ½ç­‰åˆ°å…¶é‡Šæ”¾è¯¥Mutex sync.RWMutex //å•å†™å¤šè¯»ï¼šä¼šé˜»æ­¢å†™ï¼Œä¸ä¼šé˜»æ­¢è¯» RLock() //è¯»é” Lock() //å†™é” RUnlock() //è§£é”ï¼ˆè¯»é”ï¼‰ Unlock() //è§£é”ï¼ˆå†™é”ï¼‰ //å…¨å±€å”¯ä¸€æ€§æ“ä½œ //onceçš„Doæ–¹æ³•ä¿è¯å…¨å±€åªè°ƒç”¨æŒ‡å®šå‡½æ•°(setup)ä¸€æ¬¡ï¼Œå…¶ä»–goroutineåœ¨è°ƒç”¨åˆ°æ­¤å‡½æ•°æ˜¯ä¼šé˜»å¡ï¼Œç›´åˆ°onceè°ƒç”¨ç»“æŸæ‰ç»§ç»­ once.Do(setup) ","categories":"","description":"","excerpt":"ï¼ˆä¸€ï¼‰å¹¶å‘åŸºç¡€ 1.æ¦‚å¿µ å¹¶å‘æ„å‘³ç€ç¨‹åºåœ¨è¿è¡Œæ—¶æœ‰å¤šä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå¯¹åº”å¤šä¸ªè°ƒç”¨æ ˆã€‚\nå¹¶å‘ä¸å¹¶è¡Œçš„åŒºåˆ«ï¼š\nå¹¶å‘çš„ä¸»æµå®ç°æ¨¡å‹ï¼š â€¦","ref":"/golang-notes/concurrency/golang-concurrent-programming/","tags":["Golang"],"title":"Golangç³»åˆ—ï¼ˆä¸‰ï¼‰ä¹‹å¹¶å‘ç¼–ç¨‹"},{"body":"1. éƒ¨ç½² 1.1. ä½¿ç”¨å®‰è£…åŒ…çš„æ–¹å¼ rpm -ivh nginx-xxx.rpm\n1.2. ä½¿ç”¨æºä»£ç å®‰è£… 1.2.1. ä¸‹è½½æºç åŒ… wget http://blob.wae.haplat.net/nginx/nginx-1.9.13.tar.gz 1.2.2. åˆ›å»ºä¸´æ—¶ç›®å½•å¹¶è§£å‹æºç åŒ… mkdir $HOME/build cd $HOME/build \u0026\u0026 tar zxvf nginx-\u003cversion-number\u003e.tar.gz 1.2.3. ç¼–è¯‘å¹¶å®‰è£… cd $HOME/build/nginx-\u003cversion-number\u003e ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ ... #\u003cæ›´å¤šé…ç½®é¡¹è§ä»¥ä¸‹è¯´æ˜\u003e make \u0026\u0026 make install 1.2.4. é…ç½®é¡¹ 1.2.4.1. é€šç”¨é…ç½®é¡¹ é…ç½®é€‰é¡¹ è¯´æ˜ --prefix= nginxå®‰è£…çš„æ ¹è·¯å¾„ï¼Œæ‰€æœ‰å…¶ä»–çš„è·¯å¾„éƒ½è¦ä¾èµ–ä¸è¯¥é€‰é¡¹ --sbin-path= nginxäºŒè¿›åˆ¶æ–‡ä»¶çš„è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šåˆ™ä¼šä¾èµ–äº--prefix --conf-path= å¦‚æœåœ¨å‘½ä»¤è¡Œä¸­æ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œåˆ™é€šè¿‡è¯¥é…ç½®é¡¹å»æŸ¥æ‰¾é…ç½®æ–‡ä»¶ --error-log-path= æŒ‡å®šé”™è¯¯æ–‡ä»¶çš„è·¯å¾„ --pid-path= æŒ‡å®šçš„æ–‡ä»¶å°†ä¼šå†™å…¥nginx masterè¿›ç¨‹çš„pidï¼Œé€šå¸¸åœ¨/var/runä¸‹ --lock-path= å…±äº«å­˜å‚¨å™¨äº’æ–¥é”æ–‡ä»¶çš„è·¯å¾„ --user= workerè¿›ç¨‹è¿è¡Œçš„ç”¨æˆ· --group= workerè¿›ç¨‹è¿è¡Œçš„ç»„ --with-file-aio å¯åŠ¨å¼‚æ­¥I/O --with-debug å¯ç”¨è°ƒè¯•æ—¥å¿—ï¼Œç”Ÿäº§ç¯å¢ƒä¸æ¨èé…ç½® 1.2.4.2. ä¼˜åŒ–é…ç½®é¡¹ é…ç½®é€‰é¡¹ è¯´æ˜ --with-cc= å¦‚æœæƒ³è®¾ç½®ä¸€ä¸ªä¸åœ¨é»˜è®¤PATHä¸‹çš„Cç¼–è¯‘å™¨ --with-cpp= è®¾ç½®Cé¢„å¤„ç†å™¨çš„ç›¸åº”è·¯å¾„ --with-cc-opt= æŒ‡å®šå¿…è¦çš„includeæ–‡ä»¶è·¯å¾„ --with-ld-opt= åŒ…å«è¿æ¥å™¨åº“çš„è·¯å¾„å’Œè¿è¡Œè·¯å¾„ --with-cpu-opt= é€šè¿‡è¯¥é€‰é¡¹ä¸ºç‰¹å®šçš„CPUæ„å»ºnginx 1.2.4.3. httpæ¨¡å—çš„é…ç½®é¡¹ é…ç½®é€‰é¡¹ è¯´æ˜ --without-http-cache åœ¨ä½¿ç”¨upstreamæ¨¡å—æ—¶ï¼Œnginxèƒ½å¤Ÿé…ç½®æœ¬åœ°ç¼“å­˜å†…å®¹ï¼Œè¯¥é€‰é¡¹å¯ä»¥ç¦ç”¨ç¼“å­˜ --with-http_perl_module nginxé…ç½®èƒ½å¤Ÿæ‰©å±•ä½¿ç”¨perlä»£ç ã€‚è¯¥é¡¹å¯ç”¨è¿™ä¸ªæ¨¡å—ï¼Œä½†ä¼šé™ä½æ€§èƒ½ --with-perl_modules_path= å¯¹äºé¢å¤–åµŒå…¥çš„perlæ¨¡å—ï¼Œè¯¥é€‰é¡¹æŒ‡å®šè¯¥perlè§£æå™¨çš„è·¯å¾„ --with-perl= å¦‚æœåœ¨é»˜è®¤çš„è·¯å¾„ä¸­æ‰¾ä¸åˆ°perlåˆ™æŒ‡å®šperlï¼ˆ5.6ç‰ˆæœ¬ä»¥ä¸Šï¼‰çš„è·¯å¾„ --http-log-path= httpè®¿é—®æ—¥å¿—çš„é»˜è®¤è·¯å¾„ --http-client-body-temp-path= ä»å®¢æˆ·ç«¯æ”¶åˆ°è¯·æ±‚åï¼Œè¯¥é¡¹ç”¨äºä½œä¸ºè¯·æ±‚ä½“ä¸´æ—¶å­˜æ”¾çš„ç›®å½• --http-proxy-temp-path= åœ¨ä½¿ç”¨ä»£ç†åï¼Œé€šè¿‡è¯¥é¡¹è®¾ç½®å­˜æ”¾ä¸´æ—¶æ–‡ä»¶è·¯å¾„ --http-fastcgi-temp-path= è®¾ç½®FastCGIä¸´æ—¶æ–‡ä»¶çš„ç›®å½• --http-uwsgi-temp-path= è®¾ç½®uWSGIä¸´æ—¶æ–‡ä»¶çš„ç›®å½• --http-scgi-temp-path= è®¾ç½®SCGIä¸´æ—¶æ–‡ä»¶çš„ç›®å½• 1.2.4.4. å…¶ä»–æ¨¡å—é¢å¤–é…ç½®é¡¹ é»˜è®¤æ²¡æœ‰å®‰è£…è¿™äº›æ¨¡å—ï¼Œå¯ä»¥é€šè¿‡--with-_moduleæ¥å¯ç”¨ç›¸åº”çš„æ¨¡å—åŠŸèƒ½ã€‚\né…ç½®é€‰é¡¹ è¯´æ˜ --with-http_ssl_module å¦‚æœéœ€è¦å¯¹æµé‡è¿›è¡ŒåŠ å¯†ï¼Œå¯ä»¥ä½¿ç”¨è¯¥é€‰é¡¹ï¼Œå†URLsä¸­å¼€å§‹éƒ¨åˆ†å°†ä¼šæ˜¯https(éœ€è¦OpenSSLåº“) --with-http_realip_module å¦‚æœnginxåœ¨ä¸ƒå±‚è´Ÿè½½å‡è¡¡å™¨æˆ–è€…å…¶ä»–è®¾å¤‡ä¹‹åï¼Œå®ƒä»¬å°†Httpå¤´ä¸­çš„å®¢æˆ·ç«¯IPåœ°å€ä¼ é€’ï¼Œåˆ™éœ€è¦å¯ç”¨è¯¥æ¨¡å—ï¼Œå†å¤šä¸ªå®¢æˆ·å¤„äºä¸€ä¸ªIPåœ°å€çš„æƒ…å†µä¸‹ä½¿ç”¨ --with-http_addition_module è¯¥æ¨¡å—ä½œä¸ºè¾“å‡ºè¿‡æ»¤å™¨ï¼Œä½¿èƒ½å¤Ÿåœ¨è¯·æ±‚ç»è¿‡ä¸€ä¸ªlocationå‰æˆ–åæ—¶åœ¨è¯¥locationæœ¬èº«æ·»åŠ å†…å®¹ --with-http_xslt_module è¯¥æ¨¡å—ç”¨äºå¤„ç†XMLå“åº”è½¬æ¢ï¼ŒåŸºäºä¸€ä¸ªæˆ–å¤šä¸ªXSLTæ ¼å¼ --with-http_image_filter_module è¯¥æ¨¡å—è¢«ä½œä¸ºå›¾åƒè¿‡æ»¤å™¨ä½¿ç”¨ï¼Œåœ¨å°†å›¾åƒæŠ•é€’åˆ°å®¢æˆ·ä¹‹å‰è¿›è¡Œå¤„ç†ï¼ˆéœ€è¦libgdåº“ï¼‰ --with-http_geoip_module ä½¿ç”¨è¯¥æ¨¡å—ï¼Œèƒ½å¤Ÿè®¾ç½®å„ç§å˜é‡ä»¥ä¾¿åœ¨é…ç½®æ–‡ä»¶ä¸­çš„åŒºæ®µä½¿ç”¨ï¼ŒåŸºäºåœ°ç†ä½ç½®æŸ¥æ‰¾å®¢æˆ·ç«¯IPåœ°å€ --with-http_sub_module è¯¥æ¨¡å—å®ç°æ›¿ä»£è¿‡æ»¤ï¼Œåœ¨å“åº”ä¸­ç”¨ä¸€ä¸ªå­—ç¬¦ä¸²æ›¿ä»£å¦ä¸€ä¸ªå­—ç¬¦ä¸² --with-heep_dav_module å¯ç”¨è¿™ä¸ªæ¨¡å—å°†æ¿€æ´»ä½¿ç”¨WebDAVçš„é…ç½®æŒ‡ä»¤ã€‚ --with-http_flv_module å¦‚æœéœ€è¦æä¾›Flashæµåª’ä½“è§†é¢‘æ–‡ä»¶ï¼Œé‚£ä¹ˆè¯¥æ¨¡å—å°†ä¼šæä¾›ä¼ªæµåª’ä½“ --with-http_mp4_module è¿™ä¸ªæ¨¡å—æ”¯æŒH.264/AACæ–‡ä»¶ä¼ªæµåª’ä½“ --with-http_gzip_static_module å½“è¢«è°ƒç”¨çš„èµ„æºæ²¡æœ‰.gzç»“å°¾æ ¼å¼çš„æ–‡ä»¶æ—¶ï¼Œå¦‚æœæƒ³æ”¯æŒå‘é€é¢„å‹ç¼©ç‰ˆæœ¬çš„é™æ€æ–‡ä»¶ï¼Œé‚£ä¹ˆä½¿ç”¨è¯¥æ¨¡å— --with-http_gunzip_module å¯¹äºä¸æ”¯æŒgzipç¼–ç çš„å®¢æˆ·ï¼Œè¯¥æ¨¡å—ç”¨äºä¸ºå®¢æˆ·è§£å‹ç¼©é¢„å‹ç¼©å†…å®¹ --with-http_random_index_module å¦‚æœä½ æƒ³æä¾›ä»ä¸€ä¸ªç›®å½•ä¸­éšæœºé€‰æ‹©æ–‡ä»¶çš„ç´¢å¼•æ–‡ä»¶ï¼Œé‚£ä¹ˆè¯¥æ¨¡å—éœ€è¦æ¿€æ´» --with-http_secure_link_module è¯¥æ¨¡å—æä¾›ä¸€ç§æœºåˆ¶ï¼Œå®ƒä¼šå°†ä¸€ä¸ªå“ˆå¸Œå€¼é“¾æ¥åˆ°ä¸€ä¸ªURLä¸­ï¼Œå› æ­¤åªæœ‰é‚£äº›ä½¿ç”¨æ­£ç¡®å¯†ç èƒ½å¤Ÿè®¡ç®—é“¾æ¥ --with-http_stub_status_module å¯ç”¨è¿™ä¸ªæ¨¡å—åä¼šæ”¶é›†Nginxè‡ªèº«çš„çŠ¶æ€ä¿¡æ¯ã€‚è¾“å‡ºçš„çŠ¶æ€ä¿¡æ¯å¯ä»¥ä½¿ç”¨RRDtoolæˆ–ç±»ä¼¼çš„ä¸œè¥¿ç»˜åˆ¶æˆå›¾ 2. é…ç½® é…ç½®æ–‡ä»¶ä¸€èˆ¬ä¸º/etc/nginx/nginx.confæˆ–/usr/local/nginx/conf/nginx.confã€‚\n2.1. åŸºæœ¬é…ç½®æ ¼å¼ \u003csection\u003e{ \u003cdirective\u003e \u003cparameters\u003e; } æ¯ä¸€ä¸ªæŒ‡ä»¤è¡Œç”±åˆ†å·ç»“æŸï¼Œå¤§æ‹¬å·{}è¡¨ç¤ºä¸€ä¸ªæ–°çš„ä¸Šä¸‹æ–‡ã€‚\n2.2. Nginxå…¨å±€é…ç½®å‚æ•° å…¨å±€é…ç½®æŒ‡ä»¤\næ¨¡å— é…ç½®é¡¹ è¯´æ˜ mainæ¨¡å— user é…ç½®workerè¿›ç¨‹çš„ç”¨æˆ·å’Œç»„ï¼Œå¦‚æœå¿½ç•¥groupï¼Œåˆ™groupç­‰äºæŒ‡å®šçš„ç”¨æˆ·çš„æ‰€å±ç»„ worker_processes æŒ‡å®šworkerè¿›ç¨‹çš„å¯åŠ¨æ•°é‡ï¼Œå¯å°†å…¶è®¾ç½®ä¸ºå¯ç”¨çš„CPUå†…æ ¸æ•°ï¼Œè‹¥ä¸ºautoä¸ºè‡ªåŠ¨æ£€æµ‹ error_log æ‰€æœ‰é”™è¯¯çš„å†™å…¥æ–‡ä»¶ï¼Œç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šé”™è¯¯çš„çº§åˆ«ï¼ˆdebugï¼Œinfoï¼Œnoticeï¼Œwarnï¼Œerrorï¼Œcritï¼Œalertï¼Œemergï¼‰ pid è®¾ç½®ä¸»è¿›ç¨‹IPçš„æ–‡ä»¶ eventsæ¨¡å— use ç”¨äºè®¾ç½®ä½¿ç”¨ä»€ä¹ˆæ ·çš„è¿æ¥æ–¹æ³• worker_connections ç”¨äºé…ç½®ä¸€ä¸ªå·¥ä½œè¿›ç¨‹èƒ½å¤Ÿæ¥å—çš„å¹¶å‘è¿æ¥æœ€å¤§æ•°ã€‚åŒ…æ‹¬å®¢æˆ·è¿æ¥å’Œå‘ä¸Šæ¸¸æœåŠ¡å™¨çš„è¿æ¥ã€‚ 2.3. ä½¿ç”¨includeæ–‡ä»¶ includeæ–‡ä»¶å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹ä»¥å¢å¼ºé…ç½®æ–‡ä»¶çš„å¯è¯»æ€§ï¼Œä½¿ç”¨includeæ–‡ä»¶è¦ç¡®ä¿è¢«åŒ…å«æ–‡ä»¶è‡ªèº«æ­£ç¡®çš„nginxè¯­æ³•ï¼Œå³é…ç½®æŒ‡ä»¤å’Œå—ï¼Œç„¶åæŒ‡å®šè¿™äº›æ–‡ä»¶çš„è·¯å¾„ã€‚\ninclude /etc/nginx/mime.types;\nè‹¥ä½¿ç”¨é€šé…ç¬¦åˆ™è¡¨ç¤ºé€šé…çš„å¤šä¸ªæ–‡ä»¶ï¼Œè‹¥æ²¡æœ‰ç»™å®šå…¨è·¯å¾„åˆ™ä¾æ®ä¸»é…ç½®æ–‡ä»¶è·¯å¾„è¿›è¡Œæœç´¢ã€‚\ninclude /etc/nginx/conf.d/*.conf\næµ‹è¯•é…ç½®æ–‡ä»¶(åŒ…æ‹¬includeçš„é…ç½®æ–‡ä»¶)è¯­æ³•ï¼š\nnginx -t -c {path-to-nginx.conf}\n2.4. é…ç½®è¯´æ˜ 2.4.1. mainæ¨¡å— #mainæ¨¡å—ç±»ä¼¼mainå‡½æ•°åŒ…å«å…¶ä»–å­æ¨¡å—ï¼Œéæ¨¡å—é…ç½®é¡¹(åŒ…æ‹¬æ¨¡å—å†…)åˆ†å·ç»“å°¾ï¼Œå­æ¨¡å—é…ç½®èŠ±æ‹¬å·ç»“å°¾ user nobady; #ä¸€èˆ¬æŒ‰é»˜è®¤è®¾ç½® pid /var/run/nginx.pid; #è¿›ç¨‹æ ‡è¯†ç¬¦å­˜æ”¾è·¯å¾„ï¼Œä¸€èˆ¬æŒ‰é»˜è®¤è®¾ç½® worker_processes auto; #nginxå¯¹å¤–æä¾›webæœåŠ¡æ—¶çš„worderè¿›ç¨‹æ•°ï¼Œå¯å°†å…¶è®¾ç½®ä¸ºå¯ç”¨çš„CPUå†…æ ¸æ•°ï¼Œautoä¸ºè‡ªåŠ¨æ£€æµ‹ worker_rlimit_nofile 100000; # æ›´æ”¹workerè¿›ç¨‹çš„æœ€å¤§æ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶ error_log logs/error.log info; #é”™è¯¯æ—¥å¿—å­˜æ”¾è·¯å¾„ keepalive_timeout 60; #keepalive_timeout 60; events{ #è§eventsæ¨¡å— } http{ #è§httpæ¨¡å— server{ ... location /{ } } } mail{ #è§mailæ¨¡å— } 2.4.2. eventsæ¨¡å— events { worker_connections 2048; #è®¾ç½®å¯ç”±ä¸€ä¸ªworkerè¿›ç¨‹åŒæ—¶æ‰“å¼€çš„æœ€å¤§è¿æ¥æ•° multi_accept on; #å‘Šè¯‰nginxæ”¶åˆ°ä¸€ä¸ªæ–°è¿æ¥é€šçŸ¥åæ¥å—å°½å¯èƒ½å¤šçš„è¿æ¥ use epoll; #è®¾ç½®ç”¨äºå¤ç”¨å®¢æˆ·ç«¯çº¿ç¨‹çš„è½®è¯¢æ–¹æ³•ã€‚Linux 2.6+ï¼šä½¿ç”¨epollï¼›*BSDï¼šä½¿ç”¨kqueueã€‚ } 2.4.3. httpæ¨¡å— http { #httpæ¨¡å— server { #serveræ¨¡å—ï¼ŒhttpæœåŠ¡ä¸Šçš„è™šæ‹Ÿä¸»æœºï¼Œ server å½“åšå¯¹åº”ä¸€ä¸ªåŸŸåè¿›è¡Œçš„é…ç½® listen 80; #é…ç½®ç›‘å¬ç«¯å£ server_name www.linuxidc.com; #é…ç½®è®¿é—®åŸŸå access_log logs/linuxidc.access.log main; #æŒ‡å®šæ—¥å¿—æ–‡ä»¶çš„å­˜æ”¾è·¯å¾„ index index.html; #é»˜è®¤è®¿é—®é¡µé¢ root /var/www/androidj.com/htdocs; # root æ˜¯æŒ‡å°†æœ¬åœ°çš„ä¸€ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºæ‰€æœ‰ url è¯·æ±‚çš„æ ¹è·¯å¾„ upstream backend { #åå‘ä»£ç†çš„åç«¯æœºå™¨ï¼Œå®ç°è´Ÿè½½å‡è¡¡ ip_hash; #æŒ‡æ˜äº†æˆ‘ä»¬å‡è¡¡çš„æ–¹å¼æ˜¯æŒ‰ç…§ç”¨æˆ·çš„ ip åœ°å€è¿›è¡Œåˆ†é… server backend1.example.com; server backend2.example.com; server backend3.example.com; server backend4.example.com; } location / { #location æ˜¯åœ¨ä¸€ä¸ªåŸŸåä¸‹å¯¹æ›´ç²¾ç»†çš„è·¯å¾„è¿›è¡Œé…ç½® proxy_pass http://backend; #åå‘ä»£ç†åˆ°åç«¯æœºå™¨ } } server { listen 80; server_name www.Androidj.com; access_log logs/androidj.access.log main; location / { index index.html; root /var/www/androidj.com/htdocs; } } } 2.4.4. mailæ¨¡å— mail { auth_http 127.0.0.1:80/auth.php; pop3_capabilities \"TOP\" \"USER\"; imap_capabilities \"IMAP4rev1\" \"UIDPLUS\"; server { listen 110; protocol pop3; proxy on; } server { listen 25; protocol smtp; proxy on; smtp_auth login plain; xclient off; } } ","categories":"","description":"","excerpt":"1. éƒ¨ç½² 1.1. ä½¿ç”¨å®‰è£…åŒ…çš„æ–¹å¼ rpm -ivh nginx-xxx.rpm\n1.2. ä½¿ç”¨æºä»£ç å®‰è£… 1.2.1. â€¦","ref":"/linux-notes/nginx/install-nginx/","tags":["Nginx"],"title":"Nginxçš„éƒ¨ç½²ä¸é…ç½®"},{"body":"1. Goä¸­çš„æµ‹è¯•æ¡†æ¶ Goè¯­è¨€ä¸­è‡ªå¸¦æœ‰ä¸€ä¸ªè½»é‡çº§çš„æµ‹è¯•æ¡†æ¶testingå’Œè‡ªå¸¦çš„go testå‘½ä»¤æ¥å®ç°å•å…ƒæµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•ï¼Œtestingæ¡†æ¶å’Œå…¶ä»–è¯­è¨€ä¸­çš„æµ‹è¯•æ¡†æ¶ç±»ä¼¼ï¼Œä½ å¯ä»¥åŸºäºè¿™ä¸ªæ¡†æ¶å†™é’ˆå¯¹ç›¸åº”å‡½æ•°çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¹Ÿå¯ä»¥åŸºäºè¯¥æ¡†æ¶å†™ç›¸åº”çš„å‹åŠ›æµ‹è¯•ç”¨ä¾‹ã€‚\n2. å•å…ƒæµ‹è¯•åŸåˆ™ æ–‡ä»¶åå¿…é¡»æ˜¯_test.goç»“å°¾çš„ï¼Œè¿™æ ·åœ¨æ‰§è¡Œgo testçš„æ—¶å€™æ‰ä¼šæ‰§è¡Œåˆ°ç›¸åº”çš„ä»£ç  ä½ å¿…é¡»import testingè¿™ä¸ªåŒ… æ‰€æœ‰çš„æµ‹è¯•ç”¨ä¾‹å‡½æ•°å¿…é¡»æ˜¯Testå¼€å¤´ æµ‹è¯•ç”¨ä¾‹ä¼šæŒ‰ç…§æºä»£ç ä¸­å†™çš„é¡ºåºä¾æ¬¡æ‰§è¡Œ æµ‹è¯•å‡½æ•°TestXxx()çš„å‚æ•°æ˜¯testing.Tï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥ç±»å‹æ¥è®°å½•é”™è¯¯æˆ–è€…æ˜¯æµ‹è¯•çŠ¶æ€ æµ‹è¯•æ ¼å¼ï¼šfunc TestXxx (t *testing.T),Xxxéƒ¨åˆ†å¯ä»¥ä¸ºä»»æ„çš„å­—æ¯æ•°å­—çš„ç»„åˆï¼Œä½†æ˜¯é¦–å­—æ¯ä¸èƒ½æ˜¯å°å†™å­—æ¯[a-z]ï¼Œä¾‹å¦‚Testintdivæ˜¯é”™è¯¯çš„å‡½æ•°åã€‚ å‡½æ•°ä¸­é€šè¿‡è°ƒç”¨testing.Tçš„Error, Errorf, FailNow, Fatal, FatalIfæ–¹æ³•ï¼Œè¯´æ˜æµ‹è¯•ä¸é€šè¿‡ï¼Œè°ƒç”¨Logæ–¹æ³•ç”¨æ¥è®°å½•æµ‹è¯•çš„ä¿¡æ¯ã€‚ 3 æµ‹è¯•å¸¸ç”¨å‘½ä»¤ # æµ‹è¯•æ•´ä¸ªç›®å½• go test -v ./pkg/... ./cmd/... -coverprofile cover.out # æµ‹è¯•æŸä¸ªæ–‡ä»¶ go test -v file_test.go file.go # æµ‹è¯•æŸä¸ªå‡½æ•° go test -v -test.run TestFunction 4. ç¤ºä¾‹ 4.1. æºæ–‡ä»¶getest.go package gotest import ( \"errors\" ) func Division(a, b float64) (float64, error) { if b == 0 { return 0, errors.New(\"é™¤æ•°ä¸èƒ½ä¸º0\") } return a / b, nil } 4.2. æµ‹è¯•æ–‡ä»¶gotest_test.go func Test_Division_2(t *testing.T) { if _, e := Division(6, 0); e == nil { //try a unit test on function t.Error(\"Division did not work as expected.\") // å¦‚æœä¸æ˜¯å¦‚é¢„æœŸçš„é‚£ä¹ˆå°±æŠ¥é”™ } else { t.Log(\"one test passed.\", e) //è®°å½•ä¸€äº›ä½ æœŸæœ›è®°å½•çš„ä¿¡æ¯ } } 5. å‹åŠ›æµ‹è¯• å‹åŠ›æµ‹è¯•ç”¨æ¥æ£€æµ‹å‡½æ•°(æ–¹æ³•ï¼‰çš„æ€§èƒ½ï¼Œå’Œç¼–å†™å•å…ƒåŠŸèƒ½æµ‹è¯•çš„æ–¹æ³•ç±»ä¼¼ã€‚\nå‹åŠ›æµ‹è¯•ç”¨ä¾‹å¿…é¡»éµå¾ªå¦‚ä¸‹æ ¼å¼ï¼Œå…¶ä¸­XXXå¯ä»¥æ˜¯ä»»æ„å­—æ¯æ•°å­—çš„ç»„åˆï¼Œä½†æ˜¯é¦–å­—æ¯ä¸èƒ½æ˜¯å°å†™å­—æ¯ func BenchmarkXXX(b *testing.B) { ... } go testä¸ä¼šé»˜è®¤æ‰§è¡Œå‹åŠ›æµ‹è¯•çš„å‡½æ•°ï¼Œå¦‚æœè¦æ‰§è¡Œå‹åŠ›æµ‹è¯•éœ€è¦å¸¦ä¸Šå‚æ•°-test.benchï¼Œè¯­æ³•:-test.bench=\"test_name_regex\",ä¾‹å¦‚go test -test.bench=\".*\"è¡¨ç¤ºæµ‹è¯•å…¨éƒ¨çš„å‹åŠ›æµ‹è¯•å‡½æ•° åœ¨å‹åŠ›æµ‹è¯•ç”¨ä¾‹ä¸­,è¯·è®°å¾—åœ¨å¾ªç¯ä½“å†…ä½¿ç”¨testing.B.N,ä»¥ä½¿æµ‹è¯•å¯ä»¥æ­£å¸¸çš„è¿è¡Œ æ–‡ä»¶åä¹Ÿå¿…é¡»ä»¥_test.goç»“å°¾ 5.1. ç¤ºä¾‹ package gotest import ( \"testing\" ) func Benchmark_Division(b *testing.B) { for i := 0; i \u003c b.N; i++ { //use b.N for looping Division(4, 5) } } func Benchmark_TimeConsumingFunction(b *testing.B) { b.StopTimer() //è°ƒç”¨è¯¥å‡½æ•°åœæ­¢å‹åŠ›æµ‹è¯•çš„æ—¶é—´è®¡æ•° //åšä¸€äº›åˆå§‹åŒ–çš„å·¥ä½œ,ä¾‹å¦‚è¯»å–æ–‡ä»¶æ•°æ®,æ•°æ®åº“è¿æ¥ä¹‹ç±»çš„, //è¿™æ ·è¿™äº›æ—¶é—´ä¸å½±å“æˆ‘ä»¬æµ‹è¯•å‡½æ•°æœ¬èº«çš„æ€§èƒ½ b.StartTimer() //é‡æ–°å¼€å§‹æ—¶é—´ for i := 0; i \u003c b.N; i++ { Division(4, 5) } } æ‰§è¡Œæµ‹è¯•å‘½ä»¤\ngo test -file webbench_test.go -test.bench=\".*\" ","categories":"","description":"","excerpt":"1. Goä¸­çš„æµ‹è¯•æ¡†æ¶ Goè¯­è¨€ä¸­è‡ªå¸¦æœ‰ä¸€ä¸ªè½»é‡çº§çš„æµ‹è¯•æ¡†æ¶testingå’Œè‡ªå¸¦çš„go testå‘½ä»¤æ¥å®ç°å•å…ƒæµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•ï¼Œtesting â€¦","ref":"/golang-notes/test/test/","tags":["Golang"],"title":"å•å…ƒæµ‹è¯•"},{"body":"1. beegoçš„ä½¿ç”¨ 1.1. beegoçš„å®‰è£… go get github.com/astaxie/beego 1.2. beegoçš„å‡çº§ 1ã€ç›´æ¥å‡çº§\ngo get -u github.com/astaxie/beego 2ã€æºç ä¸‹è½½å‡çº§\nç”¨æˆ·è®¿é—® https://github.com/astaxie/beego ,ä¸‹è½½æºç ï¼Œç„¶åè¦†ç›–åˆ° $GOPATH/src/github.com/astaxie/beego ç›®å½•ï¼Œç„¶åé€šè¿‡æœ¬åœ°æ‰§è¡Œå®‰è£…å°±å¯ä»¥å‡çº§äº†ï¼š\ngo install github.com/astaxie/beego 2. beegoçš„æ¶æ„ beego æ˜¯ä¸€ä¸ªå¿«é€Ÿå¼€å‘ Go åº”ç”¨çš„ HTTP æ¡†æ¶ï¼Œä»–å¯ä»¥ç”¨æ¥å¿«é€Ÿå¼€å‘ APIã€Web åŠåç«¯æœåŠ¡ç­‰å„ç§åº”ç”¨ï¼Œæ˜¯ä¸€ä¸ª RESTful çš„æ¡†æ¶ã€‚\n2.1. beegoæ¶æ„å›¾ beego æ˜¯åŸºäºå…«å¤§ç‹¬ç«‹çš„æ¨¡å—æ„å»ºçš„ï¼Œæ˜¯ä¸€ä¸ªé«˜åº¦è§£è€¦çš„æ¡†æ¶ã€‚\nå¯ä»¥ä½¿ç”¨ cache æ¨¡å—æ¥åšä½ çš„ç¼“å­˜é€»è¾‘ï¼›ä½¿ç”¨æ—¥å¿—æ¨¡å—æ¥è®°å½•ä½ çš„æ“ä½œä¿¡æ¯ï¼›ä½¿ç”¨ config æ¨¡å—æ¥è§£æä½ å„ç§æ ¼å¼çš„æ–‡ä»¶ã€‚\n2.2. beegoæ‰§è¡Œé€»è¾‘ å‚è€ƒï¼š\nhttps://beego.me/docs/intro/ https://beego.me/docs/install/ ","categories":"","description":"","excerpt":"1. beegoçš„ä½¿ç”¨ 1.1. beegoçš„å®‰è£… go get github.com/astaxie/beego 1.2. beegoçš„å‡ â€¦","ref":"/golang-notes/web/beego/beego-introduction/","tags":["Golang"],"title":"Beego ä»‹ç»"},{"body":"1. httpåŒ…å»ºç«‹webæœåŠ¡å™¨ package main import ( \"fmt\" \"log\" \"net/http\" \"strings\" ) func sayhelloName(w http.ResponseWriter, r *http.Request) { r.ParseForm() fmt.Println(r.Form) fmt.Println(\"path\", r.URL.Path) fmt.Println(\"scheme\", r.URL.Scheme) fmt.Println(r.Form[\"url_long\"]) for k, v := range r.Form { fmt.Println(\"key:\", k) fmt.Println(\"val:\", strings.Join((v), \"\")) } fmt.Println(w, \"hello world\") } func main() { http.HandleFunc(\"/\", sayhelloName) err := http.ListenAndServe(\":9090\", nil) if err != nil { log.Fatal(\"ListenAndServe:\", err) } } 2. httpåŒ…çš„è¿è¡Œæœºåˆ¶ ç›¸å…³æºç ä½äºï¼š/src/net/http/server.go\næœåŠ¡ç«¯çš„å‡ ä¸ªæ¦‚å¿µ\nRequestï¼šç”¨æˆ·è¯·æ±‚çš„ä¿¡æ¯ï¼Œç”¨æ¥è§£æç”¨æˆ·çš„è¯·æ±‚ä¿¡æ¯ï¼ŒåŒ…æ‹¬postï¼Œgetï¼ŒCookieï¼Œurlç­‰ä¿¡æ¯ã€‚ Response:æœåŠ¡å™¨éœ€è¦åé¦ˆç»™å®¢æˆ·ç«¯çš„ä¿¡æ¯ã€‚ Connï¼šç”¨æˆ·çš„æ¯æ¬¡è¯·æ±‚é“¾æ¥ã€‚ Handle:å¤„ç†è¯·æ±‚å’Œç”Ÿæˆè¿”å›ä¿¡æ¯çš„å¤„ç†é€»è¾‘ã€‚ Goå®ç°webæœåŠ¡çš„æµç¨‹\nåˆ›å»ºListen Socketï¼Œç›‘å¬æŒ‡å®šçš„ç«¯å£ï¼Œç­‰å¾…å®¢æˆ·ç«¯è¯·æ±‚åˆ°æ¥ã€‚ Listen Socketæ¥å—å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼Œå¾—åˆ°Client Socketï¼Œæ¥ä¸‹æ¥é€šè¿‡Client Socketä¸å®¢æˆ·ç«¯é€šä¿¡ã€‚ å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚ï¼Œé¦–å…ˆä»Client Socketè¯»å–HTTPè¯·æ±‚çš„åè®®å¤´ï¼Œå¦‚æœæ˜¯POSTæ–¹æ³•ï¼Œè¿˜å¯èƒ½è¦è¯»å–å®¢æˆ·ç«¯æäº¤çš„æ•°æ®ï¼Œç„¶åäº¤ç»™ç›¸åº”çš„handlerå¤„ç†è¯·æ±‚ï¼Œhandlerå¤„ç†å®Œï¼Œå°†æ•°æ®é€šè¿‡Client Socketè¿”å›ç»™å®¢æˆ·ç«¯ã€‚ 2.1. httpåŒ…æ‰§è¡Œæµç¨‹å›¾ 2.2. æ³¨å†Œè·¯ç”±[HandleFunc] http.HandlerFuncç±»å‹é»˜è®¤å®ç°äº†ServeHTTPçš„æ¥å£ã€‚\n// The HandlerFunc type is an adapter to allow the use of // ordinary functions as HTTP handlers. If f is a function // with the appropriate signature, HandlerFunc(f) is a // Handler that calls f. type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } // HandleFunc registers the handler function for the given pattern // in the DefaultServeMux. // The documentation for ServeMux explains how patterns are matched. func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } ... // HandleFunc registers the handler function for the given pattern. func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { mux.Handle(pattern, HandlerFunc(handler)) } Handle\n// Handle registers the handler for the given pattern. // If a handler already exists for pattern, Handle panics. func (mux *ServeMux) Handle(pattern string, handler Handler) { mux.mu.Lock() defer mux.mu.Unlock() if pattern == \"\" { panic(\"http: invalid pattern \" + pattern) } if handler == nil { panic(\"http: nil handler\") } if mux.m[pattern].explicit { panic(\"http: multiple registrations for \" + pattern) } mux.m[pattern] = muxEntry{explicit: true, h: handler, pattern: pattern} if pattern[0] != '/' { mux.hosts = true } // Helpful behavior: // If pattern is /tree/, insert an implicit permanent redirect for /tree. // It can be overridden by an explicit registration. n := len(pattern) if n \u003e 0 \u0026\u0026 pattern[n-1] == '/' \u0026\u0026 !mux.m[pattern[0:n-1]].explicit { // If pattern contains a host name, strip it and use remaining // path for redirect. path := pattern if pattern[0] != '/' { // In pattern, at least the last character is a '/', so // strings.Index can't be -1. path = pattern[strings.Index(pattern, \"/\"):] } url := \u0026url.URL{Path: path} mux.m[pattern[0:n-1]] = muxEntry{h: RedirectHandler(url.String(), StatusMovedPermanently), pattern: pattern} } } 2.3. å¦‚ä½•ç›‘å¬ç«¯å£ é€šè¿‡ListenAndServeæ¥ç›‘å¬ï¼Œåº•å±‚å®ç°ï¼šåˆå§‹åŒ–ä¸€ä¸ªserverå¯¹è±¡ï¼Œè°ƒç”¨net.Listen(\"tcp\",addr)ï¼Œä¹Ÿå°±æ˜¯åº•å±‚ç”¨TCPåè®®æ­å»ºäº†ä¸€ä¸ªæœåŠ¡ï¼Œç›‘å¬è®¾ç½®çš„ç«¯å£ã€‚ç„¶åè°ƒç”¨srv.Serve(net.Listener)å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å¤„ç†æ¥æ”¶å®¢æˆ·ç«¯çš„è¯·æ±‚ä¿¡æ¯ã€‚è¿™ä¸ªå‡½æ•°é‡Œèµ·äº†ä¸€ä¸ªforå¾ªç¯ï¼Œé€šè¿‡Listeneræ¥æ”¶è¯·æ±‚ï¼Œåˆ›å»ºconnï¼Œå¼€ä¸€ä¸ªgoroutineï¼ŒæŠŠè¯·æ±‚çš„æ•°æ®å½“ä½œå‚æ•°ç»™connå»æœåŠ¡ï¼šgo c.serve()ï¼Œå³æ¯æ¬¡è¯·æ±‚éƒ½æ˜¯åœ¨æ–°çš„goroutineä¸­å»æœåŠ¡ï¼Œåˆ©äºé«˜å¹¶å‘ã€‚\nsrc/net/http/server.go\n// ListenAndServe always returns a non-nil error. func ListenAndServe(addr string, handler Handler) error { server := \u0026Server{Addr: addr, Handler: handler} return server.ListenAndServe() } ... // ListenAndServe listens on the TCP network address srv.Addr and then // calls Serve to handle requests on incoming connections. // Accepted connections are configured to enable TCP keep-alives. // If srv.Addr is blank, \":http\" is used. // ListenAndServe always returns a non-nil error. func (srv *Server) ListenAndServe() error { addr := srv.Addr if addr == \"\" { addr = \":http\" } ln, err := net.Listen(\"tcp\", addr) if err != nil { return err } return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)}) } 2.4. å¦‚ä½•æ¥æ”¶å®¢æˆ·ç«¯çš„è¯·æ±‚ srv.Serve\n// Serve accepts incoming connections on the Listener l, creating a // new service goroutine for each. The service goroutines read requests and // then call srv.Handler to reply to them. // Serve always returns a non-nil error. func (srv *Server) Serve(l net.Listener) error { defer l.Close() if fn := testHookServerServe; fn != nil { fn(srv, l) } var tempDelay time.Duration // how long to sleep on accept failure if err := srv.setupHTTP2(); err != nil { return err } for { rw, e := l.Accept() if e != nil { if ne, ok := e.(net.Error); ok \u0026\u0026 ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u003e max { tempDelay = max } srv.logf(\"http: Accept error: %v; retrying in %v\", e, tempDelay) time.Sleep(tempDelay) continue } return e } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve() } } å…³é”®ä»£ç ï¼š\nc := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve() newConn\n// Create new connection from rwc. func (srv *Server) newConn(rwc net.Conn) *conn { c := \u0026conn{ server: srv, rwc: rwc, } if debugServerConnections { c.rwc = newLoggingConn(\"server\", c.rwc) } return c } 2.5. å¦‚ä½•åˆ†é…handler connå…ˆè§£ærequestï¼šc.readRequest()ï¼Œè·å–ç›¸åº”çš„handler:handler:=c.server.Handlerï¼Œå³ListenAndServeçš„ç¬¬äºŒä¸ªå‚æ•°ï¼Œå› ä¸ºå€¼ä¸ºnilï¼Œæ‰€ä»¥é»˜è®¤handler=DefaultServeMuxã€‚è¯¥å˜é‡æ˜¯ä¸€ä¸ªè·¯ç”±å™¨ï¼Œç”¨æ¥åŒ¹é…urlè·³è½¬åˆ°å…¶ç›¸åº”çš„handleå‡½æ•°ã€‚å…¶ä¸­http.HandleFunc(\"/\",sayhelloName)å³æ³¨å†Œäº†è¯·æ±‚â€œ/â€çš„è·¯ç”±è§„åˆ™ï¼Œå½“uriä¸ºâ€œ/â€æ—¶ï¼Œè·¯ç”±è·³è½¬åˆ°å‡½æ•°sayhelloNameã€‚DefaultServeMuxä¼šè°ƒç”¨ServeHTTPæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•å†…éƒ¨è°ƒç”¨sayhelloNameæœ¬èº«ï¼Œæœ€åå†™å…¥responseçš„ä¿¡æ¯åé¦ˆç»™å®¢æˆ·ç«¯ã€‚\n2.5.1. c.serve() // Serve a new connection. func (c *conn) serve() { ... for { w, err := c.readRequest() ... serverHandler{c.server}.ServeHTTP(w, w.req) .. } } 2.5.2. c.readRequest() // Read next request from connection. func (c *conn) readRequest() (w *response, err error) { if c.hijacked() { return nil, ErrHijacked } if d := c.server.ReadTimeout; d != 0 { c.rwc.SetReadDeadline(time.Now().Add(d)) } if d := c.server.WriteTimeout; d != 0 { defer func() { c.rwc.SetWriteDeadline(time.Now().Add(d)) }() } c.r.setReadLimit(c.server.initialReadLimitSize()) c.mu.Lock() // while using bufr if c.lastMethod == \"POST\" { // RFC 2616 section 4.1 tolerance for old buggy clients. peek, _ := c.bufr.Peek(4) // ReadRequest will get err below c.bufr.Discard(numLeadingCRorLF(peek)) } req, err := readRequest(c.bufr, keepHostHeader) c.mu.Unlock() if err != nil { if c.r.hitReadLimit() { return nil, errTooLarge } return nil, err } c.lastMethod = req.Method c.r.setInfiniteReadLimit() hosts, haveHost := req.Header[\"Host\"] if req.ProtoAtLeast(1, 1) \u0026\u0026 (!haveHost || len(hosts) == 0) { return nil, badRequestError(\"missing required Host header\") } if len(hosts) \u003e 1 { return nil, badRequestError(\"too many Host headers\") } if len(hosts) == 1 \u0026\u0026 !validHostHeader(hosts[0]) { return nil, badRequestError(\"malformed Host header\") } for k, vv := range req.Header { if !validHeaderName(k) { return nil, badRequestError(\"invalid header name\") } for _, v := range vv { if !validHeaderValue(v) { return nil, badRequestError(\"invalid header value\") } } } delete(req.Header, \"Host\") req.RemoteAddr = c.remoteAddr req.TLS = c.tlsState if body, ok := req.Body.(*body); ok { body.doEarlyClose = true } w = \u0026response{ conn: c, req: req, reqBody: req.Body, handlerHeader: make(Header), contentLength: -1, } w.cw.res = w w.w = newBufioWriterSize(\u0026w.cw, bufferBeforeChunkingSize) return w, nil } 2.5.3. ServeHTTP(w, w.req) func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == \"*\" \u0026\u0026 req.Method == \"OPTIONS\" { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } 2.5.4. DefaultServeMux type ServeMux struct { mu sync.RWMutex m map[string]muxEntry hosts bool // whether any patterns contain hostnames } type muxEntry struct { explicit bool h Handler pattern string } // NewServeMux allocates and returns a new ServeMux. func NewServeMux() *ServeMux { return \u0026ServeMux{m: make(map[string]muxEntry)} } // DefaultServeMux is the default ServeMux used by Serve. var DefaultServeMux = NewServeMux() handleræ¥å£çš„å®šä¹‰\ntype Handler interface { ServeHTTP(ResponseWriter, *Request) } 2.5.5. ServeMux.ServeHTTP // ServeHTTP dispatches the request to the handler whose // pattern most closely matches the request URL. func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \"*\" { if r.ProtoAtLeast(1, 1) { w.Header().Set(\"Connection\", \"close\") } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) h.ServeHTTP(w, r) } mux.Handler(r)\n// Handler returns the handler to use for the given request, // consulting r.Method, r.Host, and r.URL.Path. It always returns // a non-nil handler. If the path is not in its canonical form, the // handler will be an internally-generated handler that redirects // to the canonical path. // // Handler also returns the registered pattern that matches the // request or, in the case of internally-generated redirects, // the pattern that will match after following the redirect. // // If there is no registered handler that applies to the request, // Handler returns a ``page not found'' handler and an empty pattern. func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) { if r.Method != \"CONNECT\" { if p := cleanPath(r.URL.Path); p != r.URL.Path { _, pattern = mux.handler(r.Host, p) url := *r.URL url.Path = p return RedirectHandler(url.String(), StatusMovedPermanently), pattern } } return mux.handler(r.Host, r.URL.Path) } // handler is the main implementation of Handler. // The path is known to be in canonical form, except for CONNECT methods. func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), \"\" } return } 2.6. httpè¿æ¥å¤„ç†æµç¨‹å›¾ 3. httpçš„æ‰§è¡Œæµç¨‹æ€»ç»“ 1ã€é¦–å…ˆè°ƒç”¨Http.HandleFuncï¼ŒæŒ‰å¦‚ä¸‹é¡ºåºæ‰§è¡Œï¼š\nè°ƒç”¨äº†DefaultServerMuxçš„HandleFuncã€‚ è°ƒç”¨äº†DefaultServerMuxçš„Handleã€‚ å¾€DefaultServerMuxçš„map[string] muxEntryä¸­å¢åŠ å¯¹åº”çš„handlerå’Œè·¯ç”±è§„åˆ™ã€‚ 2ã€è°ƒç”¨http.ListenAndServe(\":9090\",nil)ï¼ŒæŒ‰å¦‚ä¸‹é¡ºåºæ‰§è¡Œï¼š\nå®ä¾‹åŒ–Serverã€‚ è°ƒç”¨Serverçš„ListenAndServe()ã€‚ è°ƒç”¨net.Listen(\"tcp\",addr)ç›‘å¬ç«¯å£ã€‚ å¯åŠ¨ä¸€ä¸ªforå¾ªç¯ï¼Œåœ¨å¾ªç¯ä½“ä¸­Acceptè¯·æ±‚ã€‚ å¯¹æ¯ä¸ªè¯·æ±‚å®ä¾‹åŒ–ä¸€ä¸ªConnï¼Œå¹¶ä¸”å¼€å¯ä¸€ä¸ªgoroutineä¸ºè¿™ä¸ªè¯·æ±‚è¿›è¡ŒæœåŠ¡go c.serve()ã€‚ è¯»å–æ¯ä¸ªè¯·æ±‚çš„å†…å®¹w,err:=c.readRequest()ã€‚ åˆ¤æ–­handleræ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®handlerï¼Œhandleré»˜è®¤è®¾ç½®ä¸ºDefaultServeMuxã€‚ è°ƒç”¨handlerçš„ServeHttpã€‚ æ ¹æ®requesté€‰æ‹©handlerï¼Œå¹¶ä¸”è¿›å…¥åˆ°è¿™ä¸ªhandlerçš„ServeHTTP, mux.handler(r).ServeHTTP(w,r) é€‰æ‹©handler åˆ¤æ–­æ˜¯å¦æœ‰è·¯ç”±èƒ½æ»¡è¶³è¿™ä¸ªrequestï¼ˆå¾ªç¯éå†ServeMuxçš„muxEntryï¼‰ã€‚ å¦‚æœæœ‰è·¯ç”±æ»¡è¶³ï¼Œè°ƒç”¨è¿™ä¸ªè·¯ç”±handlerçš„ServeHttpã€‚ å¦‚æœæ²¡æœ‰è·¯ç”±æ»¡è¶³ï¼Œè°ƒç”¨NotFoundHandlerçš„ServeHttpã€‚ 4. è‡ªå®šä¹‰è·¯ç”± Goæ”¯æŒå¤–éƒ¨å®ç°è·¯ç”±å™¨ï¼ŒListenAndServeçš„ç¬¬äºŒä¸ªå‚æ•°å°±æ˜¯é…ç½®å¤–éƒ¨è·¯ç”±å™¨ï¼Œå®ƒæ˜¯ä¸€ä¸ªHandleræ¥å£ã€‚å³å¤–éƒ¨è·¯ç”±å™¨å®ç°Hanlderæ¥å£ã€‚\nHandleræ¥å£ï¼š\ntype Handler interface { ServeHTTP(ResponseWriter, *Request) } è‡ªå®šä¹‰è·¯ç”±\npackage main import ( \"fmt\" \"net/http\" ) type MyMux struct{ } func (p *MyMux) ServeHTTP(w http.ResponseWriter,r *http.Request){ if r.URL.Path==\"/\"{ sayhelloName(w,r) return } http.NotFound(w,r) return } func sayhelloName(w http.ResponseWriter,r *http.Request){ fmt.Fprintln(w,\"Hello myroute\") } func main() { mux:=\u0026MyMux{} http.ListenAndServe(\":9090\",mux) } æ–‡ç« å‚è€ƒï¼š\nã€ŠGo webç¼–ç¨‹ã€‹\n","categories":"","description":"","excerpt":"1. httpåŒ…å»ºç«‹webæœåŠ¡å™¨ package main import ( \"fmt\" \"log\" \"net/http\" â€¦","ref":"/golang-notes/web/golang-http-execution-flow/","tags":["Golang"],"title":"HttpåŒ…æºç åˆ†æ"},{"body":"JSONå¤„ç† JSONæ˜¯ä¸€ç§è½»é‡çº§çš„æ•°æ®äº¤æ¢è¯­è¨€ã€‚\n1. è§£æJSON[Unmarshal(data []byte, v interface{})] 1.1. Unmarshalæºç  /src/encoding/json/decode.go\nfunc Unmarshal(data []byte, v interface{}) error { // Check for well-formedness. // Avoids filling out half a data structure // before discovering a JSON syntax error. var d decodeState err := checkValid(data, \u0026d.scan) if err != nil { return err } d.init(data) return d.unmarshal(v) } ... func (d *decodeState) unmarshal(v interface{}) (err error) { defer func() { if r := recover(); r != nil { if _, ok := r.(runtime.Error); ok { panic(r) } err = r.(error) } }() rv := reflect.ValueOf(v) if rv.Kind() != reflect.Ptr || rv.IsNil() { return \u0026InvalidUnmarshalError{reflect.TypeOf(v)} } d.scan.reset() // We decode rv not rv.Elem because the Unmarshaler interface // test must be applied at the top level of the value. d.value(rv) return d.savedError } 1.2. è§£æåˆ°ç»“æ„ä½“ package main import ( \"encoding/json\" \"fmt\" ) type Server struct { ServerName string ServerIP string } type Serverslice struct { Servers []Server } func main() { var s Serverslice str := `{\"servers\": [{\"serverName\":\"Shanghai_VPN\",\"serverIP\":\"127.0.0.1\"}, {\"serverName\":\"Beijing_VPN\",\"serverIP\":\"127.0.0.2\"}]}` err:=json.Unmarshal([]byte(str), \u0026s) if err!=nil{ fmt.Println(err) } fmt.Println(s) } è¯´æ˜\nJSONæ ¼å¼ä¸ç»“æ„ä½“ä¸€ä¸€å¯¹åº”ï¼ŒUnmarshalæ–¹æ³•å³å°†JSONæ–‡æœ¬è½¬æ¢æˆç»“æ„ä½“ã€‚åªä¼šåŒ¹é…ç»“æ„ä½“ä¸­çš„å¯å¯¼å‡ºå­—æ®µï¼Œå³é¦–å­—æ¯å¤§å†™å­—æ®µï¼ˆç±»ä¼¼javaçš„publicï¼‰ï¼ŒåŒ¹é…è§„åˆ™å¦‚ä¸‹ï¼šjsonçš„keyä¸ºFooä¸ºä¾‹\nå…ˆæŸ¥æ‰¾struct tagä¸­å«æœ‰Fooçš„å¯å¯¼å‡ºçš„structå­—æ®µï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰ å…¶æ¬¡æŸ¥æ‰¾å­—æ®µåä¸ºFooçš„å¯å¯¼å‡ºå­—æ®µã€‚ æœ€åæŸ¥æ‰¾ç±»ä¼¼FOOæˆ–è€…FoOè¿™ç±»é™¤é¦–å­—æ¯å¤–ï¼Œå…¶ä»–å¤§å°å†™ä¸æ•æ„Ÿçš„å¯å¯¼å‡ºå­—æ®µã€‚ 1.3. è§£æåˆ°interface 2. ç”ŸæˆJSON[Marshal(v interface{})] 2.1. Marshalæºç  /src/encoding/json/encode.go\nfunc Marshal(v interface{}) ([]byte, error) { e := \u0026encodeState{} err := e.marshal(v) if err != nil { return nil, err } return e.Bytes(), nil } ... func (e *encodeState) marshal(v interface{}) (err error) { defer func() { if r := recover(); r != nil { if _, ok := r.(runtime.Error); ok { panic(r) } if s, ok := r.(string); ok { panic(s) } err = r.(error) } }() e.reflectValue(reflect.ValueOf(v)) return nil } 2.2. ä½¿ç”¨æ–¹æ³• package main import ( \"encoding/json\" \"fmt\" ) type Server struct { ServerName string `json:\"serverName,string\"` ServerIP string `json:\"serverIP,omitempty\"` } type Serverslice struct { Servers []Server `json:\"servers\"` } func main() { var s Serverslice s.Servers = append(s.Servers, Server{ServerName: \"Shanghai_VPN\", ServerIP: \"127.0.0.1\"}) s.Servers = append(s.Servers, Server{ServerName: \"Beijing_VPN\", ServerIP: \"127.0.02\"}) b, err := json.Marshal(s) if err != nil { fmt.Println(\"JSON ERR:\", err) } fmt.Println(string(b)) } 2.3. è¯´æ˜ Marshalæ–¹æ³•å°†ç»“æ„ä½“è½¬æ¢æˆjsonæ–‡æœ¬ï¼ŒåŒ¹é…è§„åˆ™å¦‚ä¸‹ï¼š\nå¦‚æœå­—æ®µçš„tagæ˜¯â€œ-â€ï¼Œé‚£ä¹ˆè¯¥å­—æ®µä¸ä¼šè¾“å‡ºåˆ°JSONã€‚ å¦‚æœtagä¸­å¸¦æœ‰è‡ªå®šä¹‰åç§°ï¼Œé‚£ä¹ˆè¯¥è‡ªå®šä¹‰åç§°ä¼šå‡ºç°åœ¨JSONå­—æ®µåä¸­ã€‚ä¾‹å¦‚ä¾‹å­ä¸­çš„â€œserverNameâ€ å¦‚æœtagä¸­å¸¦æœ‰â€œomitemptyâ€é€‰é¡¹ï¼Œé‚£ä¹ˆå¦‚æœè¯¥å­—æ®µå€¼ä¸ºç©ºï¼Œå°±ä¸ä¼šè¾“å‡ºåˆ°JSONä¸­ã€‚ å¦‚æœå­—æ®µç±»å‹æ˜¯bool,string,int,int64ç­‰ï¼Œè€Œtagä¸­å¸¦æœ‰â€œï¼Œstringâ€é€‰é¡¹ï¼Œé‚£ä¹ˆè¿™ä¸ªå­—æ®µåœ¨è¾“å‡ºåˆ°JSONçš„æ—¶å€™ä¼šæŠŠè¯¥å­—æ®µå¯¹åº”çš„å€¼è½¬æ¢æˆJSONå­—ç¬¦ä¸²ã€‚ æ³¨æ„äº‹é¡¹ï¼š\nMarshalåªæœ‰åœ¨è½¬æ¢æˆåŠŸçš„æ—¶å€™æ‰ä¼šè¿”å›æ•°æ®ï¼ŒJSONå¯¹è±¡åªæ”¯æŒstringä½œä¸ºkeyï¼Œå¦‚æœè¦ç¼–ç ä¸€ä¸ªmap,é‚£ä¹ˆå¿…é¡»æ˜¯map[string]Tè¿™ç§ç±»å‹ã€‚ï¼ˆTä¸ºä»»æ„ç±»å‹ï¼‰ Channel,complexå’Œfunctionä¸èƒ½è¢«ç¼–ç æˆJSONã€‚ åµŒå¥—çš„æ•°æ®ä¸èƒ½ç¼–ç ï¼Œä¼šè¿›å…¥æ­»å¾ªç¯ã€‚ æŒ‡é’ˆåœ¨ç¼–ç æ—¶ä¼šè¾“å‡ºæŒ‡é’ˆæŒ‡å‘çš„å†…å®¹ï¼Œè€Œç©ºæŒ‡é’ˆä¼šè¾“å‡ºnullã€‚ ","categories":"","description":"","excerpt":"JSONå¤„ç† JSONæ˜¯ä¸€ç§è½»é‡çº§çš„æ•°æ®äº¤æ¢è¯­è¨€ã€‚\n1. è§£æJSON[Unmarshal(data []byte, v â€¦","ref":"/golang-notes/text/json/","tags":["Golang"],"title":"Jsonå¤„ç†"},{"body":"Podçš„é…ç½®ç®¡ç† Kubernetes v1.2çš„ç‰ˆæœ¬æä¾›ç»Ÿä¸€çš„é›†ç¾¤é…ç½®ç®¡ç†æ–¹æ¡ˆâ€“ConfigMapã€‚\n1. ConfigMapï¼šå®¹å™¨åº”ç”¨çš„é…ç½®ç®¡ç† ä½¿ç”¨åœºæ™¯ï¼š\nç”Ÿæˆä¸ºå®¹å™¨å†…çš„ç¯å¢ƒå˜é‡ã€‚ è®¾ç½®å®¹å™¨å¯åŠ¨å‘½ä»¤çš„å¯åŠ¨å‚æ•°ï¼ˆéœ€è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼‰ã€‚ ä»¥Volumeçš„å½¢å¼æŒ‚è½½ä¸ºå®¹å™¨å†…éƒ¨çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚ ConfigMapä»¥ä¸€ä¸ªæˆ–å¤šä¸ªkey:valueçš„å½¢å¼ä¿å­˜åœ¨kubernetesç³»ç»Ÿä¸­ä¾›åº”ç”¨ä½¿ç”¨ï¼Œæ—¢å¯ä»¥è¡¨ç¤ºä¸€ä¸ªå˜é‡çš„å€¼ï¼ˆä¾‹å¦‚ï¼šapploglevel=infoï¼‰ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºå®Œæ•´é…ç½®æ–‡ä»¶çš„å†…å®¹ï¼ˆä¾‹å¦‚ï¼šserver.xml=\u003c?xml...\u003e...ï¼‰ã€‚\nå¯ä»¥é€šè¿‡yamlé…ç½®æ–‡ä»¶æˆ–è€…ä½¿ç”¨kubectl create configmapå‘½ä»¤çš„æ–¹å¼åˆ›å»ºConfigMapã€‚\n2. åˆ›å»ºConfigMap 2.1. é€šè¿‡yamlæ–‡ä»¶æ–¹å¼ cm-appvars.yaml\napiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: info appdatadir: /var/data å¸¸ç”¨å‘½ä»¤\nkubectl create -f cm-appvars.yaml\nkubectl get configmap\nkubectl describe configmap cm-appvars\nkubectl get configmap cm-appvars -o yaml\n2.2. é€šè¿‡kubectlå‘½ä»¤è¡Œæ–¹å¼ é€šè¿‡kubectl create configmapåˆ›å»ºï¼Œä½¿ç”¨å‚æ•°--from-fileæˆ–--from-literalæŒ‡å®šå†…å®¹ï¼Œå¯ä»¥åœ¨ä¸€è¡Œä¸­æŒ‡å®šå¤šä¸ªå‚æ•°ã€‚\n1ï¼‰é€šè¿‡--from-fileå‚æ•°ä»æ–‡ä»¶ä¸­è¿›è¡Œåˆ›å»ºï¼Œå¯ä»¥æŒ‡å®škeyçš„åç§°ï¼Œä¹Ÿå¯ä»¥åœ¨ä¸€ä¸ªå‘½ä»¤è¡Œä¸­åˆ›å»ºåŒ…å«å¤šä¸ªkeyçš„ConfigMapã€‚\nkubectl create configmap NAME --from-file=[key=]source --from-file=[key=]source\n2ï¼‰é€šè¿‡--from-fileå‚æ•°ä»ç›®å½•ä¸­è¿›è¡Œåˆ›å»ºï¼Œè¯¥ç›®å½•ä¸‹çš„æ¯ä¸ªé…ç½®æ–‡ä»¶åè¢«è®¾ç½®ä¸ºkeyï¼Œæ–‡ä»¶å†…å®¹è¢«è®¾ç½®ä¸ºvalueã€‚\nkubectl create configmap NAME --from-file=config-files-dir\n3ï¼‰é€šè¿‡--from-literalä»æ–‡æœ¬ä¸­è¿›è¡Œåˆ›å»ºï¼Œç›´æ¥å°†æŒ‡å®šçš„key=valueåˆ›å»ºä¸ºConfigMapçš„å†…å®¹ã€‚\nkubectl create configmap NAME --from-literal=key1=value1 --from-literal=key2=value2\nå®¹å™¨åº”ç”¨å¯¹ConfigMapçš„ä½¿ç”¨æœ‰ä¸¤ç§æ–¹æ³•ï¼š\né€šè¿‡ç¯å¢ƒå˜é‡è·å–ConfigMapä¸­çš„å†…å®¹ã€‚ é€šè¿‡VolumeæŒ‚è½½çš„æ–¹å¼å°†ConfigMapä¸­çš„å†…å®¹æŒ‚è½½ä¸ºå®¹å™¨å†…éƒ¨çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚ 2.3. é€šè¿‡ç¯å¢ƒå˜é‡çš„æ–¹å¼ ConfigMapçš„yamlæ–‡ä»¶:cm-appvars.yaml\napiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: info appdatadir: /var/data Podçš„yamlæ–‡ä»¶ï¼šcm-test-pod.yaml\napiVersion: v1 kind: Pod metadata: name: cm-test-pod spec: containers: - name: cm-test image: busybox command: [\"/bin/sh\",\"-c\",\"env|grep APP\"] env: - name: APPLOGLEVEL valueFrom: configMapKeyRef: name: cm-appvars key: apploglevel - name: APPDATADIR valueFrom: configMapKeyRef: name: cm-appvars key: appdatadir åˆ›å»ºå‘½ä»¤ï¼š\nkubectl create -f cm-test-pod.yaml\nkubectl get pods --show-all\nkubectl logs cm-test-pod\n3. ä½¿ç”¨ConfigMapçš„é™åˆ¶æ¡ä»¶ ConfigMapå¿…é¡»åœ¨Podä¹‹å‰åˆ›å»º ConfigMapä¹Ÿå¯ä»¥å®šä¹‰ä¸ºå±äºæŸä¸ªNamespaceã€‚åªæœ‰å¤„äºç›¸åŒNamespaceä¸­çš„Podå¯ä»¥å¼•ç”¨å®ƒã€‚ kubeletåªæ”¯æŒå¯ä»¥è¢«API Serverç®¡ç†çš„Podä½¿ç”¨ConfigMapã€‚é™æ€Podæ— æ³•å¼•ç”¨ã€‚ åœ¨Podå¯¹ConfigMapè¿›è¡ŒæŒ‚è½½æ“ä½œæ—¶ï¼Œå®¹å™¨å†…åªèƒ½æŒ‚è½½ä¸ºâ€œç›®å½•â€ï¼Œæ— æ³•æŒ‚è½½ä¸ºæ–‡ä»¶ã€‚ å‚è€ƒæ–‡ç« \nã€ŠKubernetesæƒå¨æŒ‡å—ã€‹ ","categories":"","description":"","excerpt":"Podçš„é…ç½®ç®¡ç† Kubernetes v1.2çš„ç‰ˆæœ¬æä¾›ç»Ÿä¸€çš„é›†ç¾¤é…ç½®ç®¡ç†æ–¹æ¡ˆâ€“ConfigMapã€‚\n1. ConfigMapï¼šå®¹å™¨åº”ç”¨çš„ â€¦","ref":"/kubernetes-notes/concepts/configmap/pod-configmap/","tags":["Kubernetes"],"title":"ConfigMap"},{"body":"1. Dockerçš„ç½‘ç»œåŸºç¡€ 1.1. Network Namespace ä¸åŒçš„ç½‘ç»œå‘½åç©ºé—´ä¸­ï¼Œåè®®æ ˆæ˜¯ç‹¬ç«‹çš„ï¼Œå®Œå…¨éš”ç¦»ï¼Œå½¼æ­¤ä¹‹é—´æ— æ³•é€šä¿¡ã€‚åŒä¸€ä¸ªç½‘ç»œå‘½åç©ºé—´æœ‰ç‹¬ç«‹çš„è·¯ç”±è¡¨å’Œç‹¬ç«‹çš„Iptables/Netfilteræ¥æä¾›åŒ…çš„è½¬å‘ã€NATã€IPåŒ…è¿‡æ»¤ç­‰åŠŸèƒ½ã€‚\n1.1.1. ç½‘ç»œå‘½åç©ºé—´çš„å®ç° å°†ä¸ç½‘ç»œåè®®æ ˆç›¸å…³çš„å…¨å±€å˜é‡å˜æˆä¸€ä¸ªNet Namespaceå˜é‡çš„æˆå‘˜ï¼Œç„¶ååœ¨è°ƒç”¨åè®®æ ˆå‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªNamepaceå‚æ•°ã€‚\n1.1.2. ç½‘ç»œå‘½åç©ºé—´çš„æ“ä½œ 1ã€åˆ›å»ºç½‘ç»œå‘½åç©ºé—´\nip netns add name\n2ã€åœ¨å‘½åç©ºé—´å†…æ‰§è¡Œå‘½ä»¤\nip netns exec name command\n3ã€è¿›å…¥å‘½åç©ºé—´\nip netns exec name bash\n2. Dockerçš„ç½‘ç»œå®ç° 2.1. å®¹å™¨ç½‘ç»œ Dockerä½¿ç”¨Linuxæ¡¥æ¥ï¼Œåœ¨å®¿ä¸»æœºè™šæ‹Ÿä¸€ä¸ªDockerå®¹å™¨ç½‘æ¡¥(docker0)ï¼ŒDockerå¯åŠ¨ä¸€ä¸ªå®¹å™¨æ—¶ä¼šæ ¹æ®Dockerç½‘æ¡¥çš„ç½‘æ®µåˆ†é…ç»™å®¹å™¨ä¸€ä¸ªIPåœ°å€ï¼Œç§°ä¸ºContainer-IPï¼ŒåŒæ—¶Dockerç½‘æ¡¥æ˜¯æ¯ä¸ªå®¹å™¨çš„é»˜è®¤ç½‘å…³ã€‚å› ä¸ºåœ¨åŒä¸€å®¿ä¸»æœºå†…çš„å®¹å™¨éƒ½æ¥å…¥åŒä¸€ä¸ªç½‘æ¡¥ï¼Œè¿™æ ·å®¹å™¨ä¹‹é—´å°±èƒ½å¤Ÿé€šè¿‡å®¹å™¨çš„Container-IPç›´æ¥é€šä¿¡ã€‚\nDockerç½‘æ¡¥æ˜¯å®¿ä¸»æœºè™šæ‹Ÿå‡ºæ¥çš„ï¼Œå¹¶ä¸æ˜¯çœŸå®å­˜åœ¨çš„ç½‘ç»œè®¾å¤‡ï¼Œå¤–éƒ¨ç½‘ç»œæ˜¯æ— æ³•å¯»å€åˆ°çš„ï¼Œè¿™ä¹Ÿæ„å‘³ç€å¤–éƒ¨ç½‘ç»œæ— æ³•é€šè¿‡ç›´æ¥Container-IPè®¿é—®åˆ°å®¹å™¨ã€‚å¦‚æœå®¹å™¨å¸Œæœ›å¤–éƒ¨è®¿é—®èƒ½å¤Ÿè®¿é—®åˆ°ï¼Œå¯ä»¥é€šè¿‡æ˜ å°„å®¹å™¨ç«¯å£åˆ°å®¿ä¸»ä¸»æœºï¼ˆç«¯å£æ˜ å°„ï¼‰ï¼Œå³docker runåˆ›å»ºå®¹å™¨æ—¶å€™é€šè¿‡ -p æˆ– -P å‚æ•°æ¥å¯ç”¨ï¼Œè®¿é—®å®¹å™¨çš„æ—¶å€™å°±é€šè¿‡[å®¿ä¸»æœºIP]:[å®¹å™¨ç«¯å£]è®¿é—®å®¹å™¨ã€‚\n2.2. 4ç±»ç½‘ç»œæ¨¡å¼ Dockerç½‘ç»œæ¨¡å¼ é…ç½® è¯´æ˜ hostæ¨¡å¼ --net=host å®¹å™¨å’Œå®¿ä¸»æœºå…±äº«Network namespaceã€‚ containeræ¨¡å¼ --net=container:NAME_or_ID å®¹å™¨å’Œå¦å¤–ä¸€ä¸ªå®¹å™¨å…±äº«Network namespaceã€‚ kubernetesä¸­çš„podå°±æ˜¯å¤šä¸ªå®¹å™¨å…±äº«ä¸€ä¸ªNetwork namespaceã€‚ noneæ¨¡å¼ --net=none å®¹å™¨æœ‰ç‹¬ç«‹çš„Network namespaceï¼Œä½†å¹¶æ²¡æœ‰å¯¹å…¶è¿›è¡Œä»»ä½•ç½‘ç»œè®¾ç½®ï¼Œå¦‚åˆ†é…veth pair å’Œç½‘æ¡¥è¿æ¥ï¼Œé…ç½®IPç­‰ã€‚ bridgeæ¨¡å¼ --net=bridgeï¼ˆé»˜è®¤ä¸ºè¯¥æ¨¡å¼ï¼‰ æ¡¥æ¥æ¨¡å¼ 3. Dockerç½‘ç»œæ¨¡å¼ 3.1. bridgeæ¡¥æ¥æ¨¡å¼ åœ¨bridgeæ¨¡å¼ä¸‹ï¼ŒDockerå¯ä»¥ä½¿ç”¨ç‹¬ç«‹çš„ç½‘ç»œæ ˆã€‚å®ç°æ–¹å¼æ˜¯çˆ¶è¿›ç¨‹åœ¨åˆ›å»ºå­è¿›ç¨‹çš„æ—¶å€™é€šè¿‡ä¼ å…¥CLONE_NEWNETçš„å‚æ•°åˆ›å»ºå‡ºä¸€ä¸ªç½‘ç»œå‘½åç©ºé—´ã€‚\nå®ç°æ­¥éª¤ï¼š\nDocker Daemoné¦–æ¬¡å¯åŠ¨æ—¶ä¼šåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç½‘æ¡¥docker0ï¼Œåœ°å€é€šå¸¸ä¸º172.x.x.xå¼€å¤´ï¼Œåœ¨ç§æœ‰çš„ç½‘ç»œç©ºé—´ä¸­ç»™è¿™ä¸ªç½‘ç»œåˆ†é…ä¸€ä¸ªå­ç½‘ã€‚ ç”±Dockeråˆ›å»ºå¤„ç†çš„æ¯ä¸ªå®¹å™¨ï¼Œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿä»¥å¤ªè®¾å¤‡å¯¹ï¼ˆveth pairï¼‰ï¼Œä¸€ç«¯å…³è”åˆ°ç½‘æ¡¥ï¼Œå¦ä¸€ç«¯ä½¿ç”¨NamespaceæŠ€æœ¯æ˜ å°„åˆ°å®¹å™¨å†…çš„eth0è®¾å¤‡ï¼Œç„¶åä»ç½‘æ¡¥çš„åœ°å€æ®µå†…ç»™eth0æ¥å£åˆ†é…ä¸€ä¸ªIPåœ°å€ã€‚ ä¸€èˆ¬æƒ…å†µï¼Œå®¿ä¸»æœºIPä¸docker0 IPã€å®¹å™¨IPæ˜¯ä¸åŒçš„IPæ®µï¼Œé»˜è®¤æƒ…å†µï¼Œå¤–éƒ¨çœ‹ä¸åˆ°docker0å’Œå®¹å™¨IPï¼Œå¯¹äºå¤–éƒ¨æ¥è¯´ç›¸å½“äºdocker0å’Œå®¹å™¨çš„IPä¸ºå†…ç½‘IPã€‚\n3.1.1. å¤–éƒ¨ç½‘ç»œè®¿é—®Dockerå®¹å™¨ å¤–éƒ¨è®¿é—®dockerå®¹å™¨å¯ä»¥é€šè¿‡ç«¯å£æ˜ å°„(NAT)çš„æ–¹å¼ï¼ŒDockerä½¿ç”¨NATçš„æ–¹å¼å°†å®¹å™¨å†…éƒ¨çš„æœåŠ¡ä¸å®¿ä¸»æœºçš„æŸä¸ªç«¯å£port_1ç»‘å®šã€‚\nå¤–éƒ¨è®¿é—®å®¹å™¨çš„æµç¨‹å¦‚ä¸‹ï¼š\nå¤–ç•Œç½‘ç»œé€šè¿‡å®¿ä¸»æœºçš„IPå’Œæ˜ å°„çš„ç«¯å£port_1è®¿é—®ã€‚ å½“å®¿ä¸»æœºæ”¶åˆ°æ­¤ç±»è¯·æ±‚ï¼Œä¼šé€šè¿‡DNATå°†è¯·æ±‚çš„ç›®æ ‡IPå³å®¿ä¸»æœºIPå’Œç›®æ ‡ç«¯å£å³æ˜ å°„ç«¯å£port_1æ›¿æ¢æˆå®¹å™¨çš„IPå’Œå®¹å™¨çš„ç«¯å£port_0ã€‚ ç”±äºå®¿ä¸»æœºä¸Šå¯ä»¥è¯†åˆ«å®¹å™¨IPï¼Œæ‰€ä»¥å®¿ä¸»æœºå°†è¯·æ±‚å‘ç»™veth pairã€‚ veth pairå°†è¯·æ±‚å‘é€ç»™å®¹å™¨å†…éƒ¨çš„eth0ï¼Œç”±å®¹å™¨å†…éƒ¨çš„æœåŠ¡è¿›è¡Œå¤„ç†ã€‚ 3.1.2. Dockerå®¹å™¨è®¿é—®å¤–éƒ¨ç½‘ç»œ dockerå®¹å™¨è®¿é—®å¤–éƒ¨ç½‘ç»œçš„æµç¨‹ï¼š\ndockerå®¹å™¨å‘å¤–éƒ¨ç›®æ ‡IPå’Œç›®æ ‡ç«¯å£port_2å‘èµ·è¯·æ±‚ï¼Œè¯·æ±‚æŠ¥æ–‡ä¸­çš„æºIPä¸ºå®¹å™¨IPã€‚\nè¯·æ±‚é€šè¿‡å®¹å™¨å†…éƒ¨çš„eth0åˆ°veth pairçš„å¦ä¸€ç«¯docker0ç½‘æ¡¥ã€‚\ndocker0ç½‘æ¡¥é€šè¿‡æ•°æ®æŠ¥è½¬å‘åŠŸèƒ½å°†è¯·æ±‚è½¬å‘åˆ°å®¿ä¸»æœºçš„eth0ã€‚\nå®¿ä¸»æœºå¤„ç†è¯·æ±‚æ—¶é€šè¿‡SNATå°†è¯·æ±‚ä¸­çš„æºIPæ¢æˆå®¿ä¸»æœºeth0çš„IPã€‚\nå¤„ç†åçš„æŠ¥æ–‡é€šè¿‡è¯·æ±‚çš„ç›®æ ‡IPå‘é€åˆ°å¤–éƒ¨ç½‘ç»œã€‚\n3.1.3. ç¼ºç‚¹ ä½¿ç”¨NATçš„æ–¹å¼å¯èƒ½ä¼šå¸¦æ¥æ€§èƒ½çš„é—®é¢˜ï¼Œå½±å“ç½‘ç»œä¼ è¾“æ•ˆç‡ã€‚\n3.2. hostæ¨¡å¼ hostæ¨¡å¼å¹¶æ²¡æœ‰ç»™å®¹å™¨åˆ›å»ºä¸€ä¸ªéš”ç¦»çš„ç½‘ç»œç¯å¢ƒï¼Œè€Œæ˜¯å’Œå®¿ä¸»æœºå…±ç”¨ä¸€ä¸ªç½‘ç»œå‘½åç©ºé—´ï¼Œå®¹å™¨ä½¿ç”¨å®¿ä¸»æœºçš„eth0å’Œå¤–ç•Œè¿›è¡Œé€šä¿¡ï¼ŒåŒæ ·å®¹å™¨ä¹Ÿå…±ç”¨å®¿ä¸»æœºçš„ç«¯å£èµ„æºï¼Œå³åˆ†é…ç«¯å£å¯èƒ½å­˜åœ¨ä¸å®¿ä¸»æœºå·²åˆ†é…çš„ç«¯å£å†²çªçš„é—®é¢˜ã€‚\nå®ç°çš„æ–¹å¼å³çˆ¶è¿›ç¨‹åœ¨åˆ›å»ºå­è¿›ç¨‹çš„æ—¶å€™ä¸ä¼ å…¥CLONE_NEWNETçš„å‚æ•°ï¼Œä»è€Œå’Œå®¿ä¸»æœºå…±äº«ä¸€ä¸ªç½‘ç»œç©ºé—´ã€‚\nhostæ¨¡å¼æ²¡æœ‰é€šè¿‡NATçš„æ–¹å¼è¿›è¡Œè½¬å‘å› æ­¤æ€§èƒ½ä¸Šç›¸å¯¹è¾ƒå¥½ï¼Œä½†æ˜¯ä¸å­˜åœ¨ç½‘ç»œéš”ç¦»æ€§ï¼Œå¯èƒ½äº§ç”Ÿç«¯å£å†²çªçš„é—®é¢˜ã€‚\n3.3. containeræ¨¡å¼ containeræ¨¡å¼å³dockerå®¹å™¨å¯ä»¥ä½¿ç”¨å…¶ä»–å®¹å™¨çš„ç½‘ç»œå‘½åç©ºé—´ï¼Œå³å’Œå…¶ä»–å®¹å™¨å¤„äºåŒä¸€ä¸ªç½‘ç»œå‘½åç©ºé—´ã€‚\næ­¥éª¤ï¼š\næŸ¥æ‰¾å…¶ä»–å®¹å™¨çš„ç½‘ç»œå‘½åç©ºé—´ã€‚ æ–°åˆ›å»ºçš„å®¹å™¨çš„ç½‘ç»œå‘½åç©ºé—´ä½¿ç”¨å…¶ä»–å®¹å™¨çš„ç½‘ç»œå‘½åç©ºé—´ã€‚ é€šè¿‡å’Œå…¶ä»–å®¹å™¨å…±äº«ç½‘ç»œå‘½åç©ºé—´çš„æ–¹å¼ï¼Œå¯ä»¥è®©ä¸åŒçš„å®¹å™¨ä¹‹é—´å¤„äºç›¸åŒçš„ç½‘ç»œå‘½åç©ºé—´ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡localhostçš„æ–¹å¼è¿›è¡Œé€šä¿¡ï¼Œç®€åŒ–äº†å¼ºå…³è”çš„å¤šä¸ªå®¹å™¨ä¹‹é—´çš„é€šä¿¡é—®é¢˜ã€‚\nk8sä¸­çš„podçš„æ¦‚å¿µå°±æ˜¯é€šè¿‡ä¸€ç»„å®¹å™¨å…±äº«ä¸€ä¸ªç½‘ç»œå‘½åç©ºé—´æ¥è¾¾åˆ°podå†…éƒ¨çš„ä¸åŒå®¹å™¨å¯ä»¥ç›´æ¥é€šè¿‡localhostçš„æ–¹å¼è¿›è¡Œé€šä¿¡ã€‚\n3.4. noneæ¨¡å¼ noneæ¨¡å¼å³ä¸ä¸ºå®¹å™¨åˆ›å»ºä»»ä½•çš„ç½‘ç»œç¯å¢ƒï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦æ‰‹åŠ¨å»åˆ›å»ºä¸åŒçš„ç½‘ç»œå®šåˆ¶é…ç½®ã€‚\nå‚è€ƒï¼š\nã€ŠDockeræºç åˆ†æã€‹ ","categories":"","description":"","excerpt":"1. Dockerçš„ç½‘ç»œåŸºç¡€ 1.1. Network Namespace ä¸åŒçš„ç½‘ç»œå‘½åç©ºé—´ä¸­ï¼Œåè®®æ ˆæ˜¯ç‹¬ç«‹çš„ï¼Œå®Œå…¨éš”ç¦»ï¼Œå½¼æ­¤ä¹‹é—´æ— æ³•é€š â€¦","ref":"/kubernetes-notes/network/docker-network/","tags":["Kubernetes"],"title":"Dockerç½‘ç»œ"},{"body":"1. Master é›†ç¾¤çš„æ§åˆ¶èŠ‚ç‚¹ï¼Œè´Ÿè´£æ•´ä¸ªé›†ç¾¤çš„ç®¡ç†å’Œæ§åˆ¶ï¼Œkubernetesçš„æ‰€æœ‰çš„å‘½ä»¤åŸºæœ¬éƒ½æ˜¯å‘ç»™Masterï¼Œç”±å®ƒæ¥è´Ÿè´£å…·ä½“çš„æ‰§è¡Œè¿‡ç¨‹ã€‚\n1.1. Masterçš„ç»„ä»¶ kube-apiserverï¼šèµ„æºå¢åˆ æ”¹æŸ¥çš„å…¥å£ kube-controller-managerï¼šèµ„æºå¯¹è±¡çš„å¤§æ€»ç®¡ kube-schedulerï¼šè´Ÿè´£èµ„æºè°ƒåº¦ï¼ˆPodè°ƒåº¦ï¼‰ etcd Server:kubernetesçš„æ‰€æœ‰çš„èµ„æºå¯¹è±¡çš„æ•°æ®ä¿å­˜åœ¨etcdä¸­ã€‚ 2. Node Nodeæ˜¯é›†ç¾¤çš„å·¥ä½œè´Ÿè½½èŠ‚ç‚¹ï¼Œé»˜è®¤æƒ…å†µkubeletä¼šå‘Masteræ³¨å†Œè‡ªå·±ï¼Œä¸€æ—¦Nodeè¢«çº³å…¥é›†ç¾¤ç®¡ç†èŒƒå›´ï¼Œkubeletä¼šå®šæ—¶å‘Masteræ±‡æŠ¥è‡ªèº«çš„æƒ…æŠ¥ï¼ŒåŒ…æ‹¬æ“ä½œç³»ç»Ÿï¼ŒDockerç‰ˆæœ¬ï¼Œæœºå™¨èµ„æºæƒ…å†µç­‰ã€‚\nå¦‚æœNodeè¶…è¿‡æŒ‡å®šæ—¶é—´ä¸ä¸ŠæŠ¥ä¿¡æ¯ï¼Œä¼šè¢«Masteråˆ¤æ–­ä¸ºâ€œå¤±è”â€ï¼Œæ ‡è®°ä¸ºNot Readyï¼ŒéšåMasterä¼šè§¦å‘Podè½¬ç§»ã€‚\n2.1. Nodeçš„ç»„ä»¶ kubelet:Podçš„ç®¡å®¶ï¼Œä¸Masteré€šä¿¡ kube-proxyï¼šå®ç°kubernetes Serviceçš„é€šä¿¡ä¸è´Ÿè½½å‡è¡¡æœºåˆ¶çš„é‡è¦ç»„ä»¶ Dockerï¼šå®¹å™¨çš„åˆ›å»ºå’Œç®¡ç† 2.2. Nodeç›¸å…³å‘½ä»¤ kubectl get nodes\nkuebctl describe node {node_name}\n2.3. describeå‘½ä»¤çš„Nodeä¿¡æ¯ NodeåŸºæœ¬ä¿¡æ¯ï¼šåç§°ã€æ ‡ç­¾ã€åˆ›å»ºæ—¶é—´ç­‰ Nodeå½“å‰çš„çŠ¶æ€ï¼ŒNodeå¯åŠ¨åä¼šè¿›è¡Œè‡ªæ£€å·¥ä½œï¼Œç£ç›˜æ˜¯å¦æ»¡ï¼Œå†…å­˜æ˜¯å¦ä¸è¶³ï¼Œè‹¥éƒ½æ­£å¸¸åˆ™åˆ‡æ¢ä¸ºReadyçŠ¶æ€ã€‚ Nodeçš„ä¸»æœºåœ°å€ä¸ä¸»æœºå Nodeä¸Šçš„èµ„æºæ€»é‡ï¼šCPU,å†…å­˜ï¼Œæœ€å¤§å¯è°ƒåº¦Podæ•°é‡ç­‰ Nodeå¯åˆ†é…èµ„æºé‡ï¼šå½“å‰Nodeå¯ç”¨äºåˆ†é…çš„èµ„æºé‡ ä¸»æœºç³»ç»Ÿä¿¡æ¯ï¼šä¸»æœºå”¯ä¸€æ ‡è¯†ç¬¦UUIDï¼ŒLinux kernelç‰ˆæœ¬å·ï¼Œæ“ä½œç³»ç»Ÿï¼Œkubernetesç‰ˆæœ¬ï¼Œkubeletä¸kube-proxyç‰ˆæœ¬ å½“å‰æ­£åœ¨è¿è¡Œçš„Podåˆ—è¡¨åŠæ¦‚è¦ä¿¡æ¯ å·²åˆ†é…çš„èµ„æºä½¿ç”¨æ¦‚è¦ï¼Œä¾‹å¦‚èµ„æºç”³è¯·çš„æœ€ä½ã€æœ€å¤§å…è®¸ä½¿ç”¨é‡å ç³»ç»Ÿæ€»é‡çš„ç™¾åˆ†æ¯” Nodeç›¸å…³çš„Eventä¿¡æ¯ã€‚ 3. Pod Podæ˜¯Kubernetesä¸­æ“ä½œçš„åŸºæœ¬å•å…ƒã€‚æ¯ä¸ªPodä¸­æœ‰ä¸ªæ ¹å®¹å™¨(Pauseå®¹å™¨)ï¼ŒPauseå®¹å™¨çš„çŠ¶æ€ä»£è¡¨æ•´ä¸ªå®¹å™¨ç»„çš„çŠ¶æ€ï¼Œå…¶ä»–ä¸šåŠ¡å®¹å™¨å…±äº«Pauseçš„IPï¼Œå³Pod IPï¼Œå…±äº«PauseæŒ‚è½½çš„Volumeï¼Œè¿™æ ·ç®€åŒ–äº†åŒä¸ªPodä¸­ä¸åŒå®¹å™¨ä¹‹é—´çš„ç½‘ç»œé—®é¢˜å’Œæ–‡ä»¶å…±äº«é—®é¢˜ã€‚\nKubernetesé›†ç¾¤ä¸­ï¼ŒåŒå®¿ä¸»æœºçš„æˆ–ä¸åŒå®¿ä¸»æœºçš„Podä¹‹é—´è¦æ±‚èƒ½å¤ŸTCP/IPç›´æ¥é€šä¿¡ï¼Œå› æ­¤é‡‡ç”¨è™šæ‹ŸäºŒå±‚ç½‘ç»œæŠ€æœ¯æ¥å®ç°ï¼Œä¾‹å¦‚Flannelï¼ŒOpenvswitch(OVS)ç­‰ï¼Œè¿™æ ·åœ¨åŒä¸ªé›†ç¾¤ä¸­ï¼Œä¸åŒçš„å®¿ä¸»æœºçš„Pod IPä¸ºä¸åŒIPæ®µçš„IPï¼Œé›†ç¾¤ä¸­çš„æ‰€æœ‰Pod IPéƒ½æ˜¯å”¯ä¸€çš„ï¼Œä¸åŒPodä¹‹é—´å¯ä»¥ç›´æ¥é€šä¿¡ã€‚ Podæœ‰ä¸¤ç§ç±»å‹ï¼šæ™®é€šPodå’Œé™æ€Podã€‚é™æ€Podå³ä¸é€šè¿‡K8Sè°ƒåº¦å’Œåˆ›å»ºï¼Œç›´æ¥åœ¨æŸä¸ªå…·ä½“çš„Nodeæœºå™¨ä¸Šé€šè¿‡å…·ä½“çš„æ–‡ä»¶æ¥å¯åŠ¨ã€‚æ™®é€šPodåˆ™æ˜¯ç”±K8Såˆ›å»ºã€è°ƒåº¦ï¼ŒåŒæ—¶æ•°æ®å­˜æ”¾åœ¨ETCDä¸­ã€‚ Pod IPå’Œå…·ä½“çš„å®¹å™¨ç«¯å£ï¼ˆContainnerPortï¼‰ç»„æˆä¸€ä¸ªå…·ä½“çš„é€šä¿¡åœ°å€ï¼Œå³Endpointã€‚ä¸€ä¸ªPodä¸­å¯ä»¥å­˜åœ¨å¤šä¸ªå®¹å™¨ï¼Œå¯ä»¥æœ‰å¤šä¸ªç«¯å£ï¼ŒPod IPä¸€æ ·ï¼Œå³æœ‰å¤šä¸ªEndpointã€‚ Pod Volumeæ˜¯å®šä¹‰åœ¨Podä¹‹ä¸Šï¼Œè¢«å„ä¸ªå®¹å™¨æŒ‚è½½åˆ°è‡ªå·±çš„æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå¯ä»¥ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå®ç°åç«¯å­˜å‚¨åŠŸèƒ½ã€‚ Podä¸­çš„Eventäº‹ä»¶å¯ä»¥ç”¨æ¥æ’æŸ¥é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡kubectl describe pod xxx æ¥æŸ¥çœ‹å¯¹åº”çš„äº‹ä»¶ã€‚ æ¯ä¸ªPodå¯ä»¥å¯¹å…¶èƒ½ä½¿ç”¨çš„æœåŠ¡å™¨ä¸Šçš„è®¡ç®—èµ„æºè®¾ç½®é™é¢ï¼Œä¸€èˆ¬ä¸ºCPUå’ŒMemoryã€‚K8Sä¸­ä¸€èˆ¬å°†åƒåˆ†ä¹‹ä¸€ä¸ªçš„CPUé…ç½®ä½œä¸ºæœ€å°å•ä½ï¼Œç”¨mè¡¨ç¤ºï¼Œæ˜¯ä¸€ä¸ªç»å¯¹å€¼ï¼Œå³100må¯¹äºä¸€ä¸ªCoreçš„æœºå™¨è¿˜æ˜¯48ä¸ªCoreçš„æœºå™¨éƒ½æ˜¯ä¸€æ ·çš„å¤§å°ã€‚Memoryé…é¢ä¹Ÿæ˜¯ä¸ªç»å¯¹å€¼ï¼Œå•ä½ä¸ºå†…å­˜å­—èŠ‚æ•°ã€‚ èµ„æºé…é¢çš„ä¸¤ä¸ªå‚æ•° Requests:è¯¥èµ„æºçš„æœ€å°ç”³è¯·é‡ï¼Œç³»ç»Ÿå¿…é¡»æ»¡è¶³è¦æ±‚ã€‚ Limits:è¯¥èµ„æºæœ€å¤§å…è®¸ä½¿ç”¨é‡ï¼Œå½“è¶…è¿‡è¯¥é‡ï¼ŒK8Sä¼škillå¹¶é‡å¯Podã€‚ 4. Label Labelæ˜¯ä¸€ä¸ªé”®å€¼å¯¹ï¼Œå¯ä»¥é™„åŠ åœ¨ä»»ä½•å¯¹è±¡ä¸Šï¼Œæ¯”å¦‚Node,Pod,Service,RCç­‰ã€‚Labelå’Œèµ„æºå¯¹è±¡æ˜¯å¤šå¯¹å¤šçš„å…³ç³»ï¼Œå³ä¸€ä¸ªLabelå¯ä»¥è¢«æ·»åŠ åˆ°å¤šä¸ªå¯¹è±¡ä¸Šï¼Œä¸€ä¸ªå¯¹è±¡ä¹Ÿå¯ä»¥å®šä¹‰å¤šä¸ªLabelã€‚ Labelçš„ä½œç”¨ä¸»è¦ç”¨æ¥å®ç°ç²¾ç»†çš„ã€å¤šç»´åº¦çš„èµ„æºåˆ†ç»„ç®¡ç†ï¼Œä»¥ä¾¿è¿›è¡Œèµ„æºåˆ†é…ï¼Œè°ƒåº¦ï¼Œé…ç½®ï¼Œéƒ¨ç½²ç­‰å·¥ä½œã€‚ Labelé€šä¿—ç†è§£å°±æ˜¯â€œæ ‡ç­¾â€ï¼Œé€šè¿‡æ ‡ç­¾æ¥è¿‡æ»¤ç­›é€‰æŒ‡å®šçš„å¯¹è±¡ï¼Œè¿›è¡Œå…·ä½“çš„æ“ä½œã€‚k8sé€šè¿‡Label Selector(æ ‡ç­¾é€‰æ‹©å™¨)æ¥ç­›é€‰æŒ‡å®šLabelçš„èµ„æºå¯¹è±¡ï¼Œç±»ä¼¼SQLè¯­å¥ä¸­çš„æ¡ä»¶æŸ¥è¯¢ï¼ˆWHEREè¯­å¥ï¼‰ã€‚ Label Selectoræœ‰åŸºäºç­‰å¼å’ŒåŸºäºé›†åˆçš„ä¸¤ç§è¡¨è¾¾æ–¹å¼ï¼Œå¯ä»¥å¤šä¸ªæ¡ä»¶è¿›è¡Œç»„åˆä½¿ç”¨ã€‚ åŸºäºç­‰å¼ï¼šname=redis-slaveï¼ˆåŒ¹é…name=redis-slaveçš„èµ„æºå¯¹è±¡ï¼‰;env!=product(åŒ¹é…æ‰€æœ‰ä¸å…·æœ‰æ ‡ç­¾env=productçš„èµ„æºå¯¹è±¡) åŸºäºé›†åˆï¼šname in (redis-slave,redis-master);name not in (php-frontend)ï¼ˆåŒ¹é…æ‰€æœ‰ä¸å…·æœ‰æ ‡ç­¾name=php-frontendçš„èµ„æºå¯¹è±¡ï¼‰ ä½¿ç”¨åœºæ™¯\nkube-controllerè¿›ç¨‹é€šè¿‡èµ„æºå¯¹è±¡RCä¸Šå®šä¹‰çš„Label Selectoræ¥ç­›é€‰è¦ç›‘æ§çš„Podå‰¯æœ¬æ•°ï¼Œä»è€Œå®ç°å‰¯æœ¬æ•°å§‹ç»ˆä¿æŒé¢„æœŸæ•°ç›®ã€‚ kube-proxyè¿›ç¨‹é€šè¿‡Serviceçš„Label Selectoræ¥é€‰æ‹©å¯¹åº”Podï¼Œè‡ªåŠ¨å»ºç«‹æ¯ä¸ªServiceåˆ°å¯¹åº”Podçš„è¯·æ±‚è½¬å‘è·¯ç”±è¡¨ï¼Œä»è€Œå®ç°Serviceçš„æ™ºèƒ½è´Ÿè½½å‡è¡¡æœºåˆ¶ã€‚ kube-schedulerå®ç°Podå®šå‘è°ƒåº¦ï¼šå¯¹Nodeå®šä¹‰ç‰¹å®šçš„Labelï¼Œå¹¶ä¸”åœ¨Podå®šä¹‰æ–‡ä»¶ä¸­ä½¿ç”¨NodeSelectoræ ‡ç­¾è°ƒåº¦ç­–ç•¥ã€‚ 5. Replication Controller(RC) RCæ˜¯k8sç³»ç»Ÿä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå®šä¹‰äº†ä¸€ä¸ªæœŸæœ›çš„åœºæ™¯ã€‚\nä¸»è¦åŒ…æ‹¬ï¼š\nPodæœŸæœ›çš„å‰¯æœ¬æ•°ï¼ˆreplicasï¼‰ ç”¨äºç­›é€‰ç›®æ ‡Podçš„Label Selector ç”¨äºåˆ›å»ºPodçš„æ¨¡æ¿ï¼ˆtemplateï¼‰ RCç‰¹æ€§è¯´æ˜ï¼š\nPodçš„ç¼©æ”¾å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®ç°ï¼škubectl scale rc redis-slave --replicas=3 åˆ é™¤RCå¹¶ä¸ä¼šåˆ é™¤è¯¥RCåˆ›å»ºçš„Podï¼Œå¯ä»¥å°†å‰¯æœ¬æ•°è®¾ç½®ä¸º0ï¼Œå³å¯åˆ é™¤å¯¹åº”Podã€‚æˆ–è€…é€šè¿‡kubectl stop /deleteå‘½ä»¤æ¥ä¸€æ¬¡æ€§åˆ é™¤RCå’Œå…¶åˆ›å»ºçš„Podã€‚ æ”¹å˜RCä¸­Podæ¨¡æ¿çš„é•œåƒç‰ˆæœ¬å¯ä»¥å®ç°æ»šåŠ¨å‡çº§ï¼ˆRolling Updateï¼‰ã€‚å…·ä½“æ“ä½œè§https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/ Kubernetes1.2ä»¥ä¸Šç‰ˆæœ¬å°†RCå‡çº§ä¸ºReplica Setï¼Œå®ƒä¸å½“å‰RCçš„å”¯ä¸€åŒºåˆ«åœ¨äºReplica Setæ”¯æŒåŸºäºé›†åˆçš„Label Selector(Set-based selector)ï¼Œè€Œæ—§ç‰ˆæœ¬RCåªæ”¯æŒåŸºäºç­‰å¼çš„Label Selector(equality-based selector)ã€‚ Kubernetes1.2ä»¥ä¸Šç‰ˆæœ¬é€šè¿‡Deploymentæ¥ç»´æŠ¤Replica Setè€Œä¸æ˜¯å•ç‹¬ä½¿ç”¨Replica Setã€‚å³æ§åˆ¶æµä¸ºï¼šDelpoymentâ†’Replica Setâ†’Podã€‚å³æ–°ç‰ˆæœ¬çš„Deployment+Replica Setæ›¿ä»£äº†RCçš„ä½œç”¨ã€‚ 6. Deployment Deploymentæ˜¯kubernetes 1.2å¼•å…¥çš„æ¦‚å¿µï¼Œç”¨æ¥è§£å†³Podçš„ç¼–æ’é—®é¢˜ã€‚Deploymentå¯ä»¥ç†è§£ä¸ºRCçš„å‡çº§ç‰ˆï¼ˆRC+Reolicat Setï¼‰ã€‚ç‰¹ç‚¹åœ¨äºå¯ä»¥éšæ—¶çŸ¥é“Podçš„éƒ¨ç½²è¿›åº¦ï¼Œå³å¯¹Podçš„åˆ›å»ºã€è°ƒåº¦ã€ç»‘å®šèŠ‚ç‚¹ã€å¯åŠ¨å®¹å™¨å®Œæ•´è¿‡ç¨‹çš„è¿›åº¦å±•ç¤ºã€‚\nä½¿ç”¨åœºæ™¯\nåˆ›å»ºä¸€ä¸ªDeploymentå¯¹è±¡æ¥ç”Ÿæˆå¯¹åº”çš„Replica Setå¹¶å®ŒæˆPodå‰¯æœ¬çš„åˆ›å»ºè¿‡ç¨‹ã€‚ æ£€æŸ¥Deploymentçš„çŠ¶æ€æ¥ç¡®è®¤éƒ¨ç½²åŠ¨ä½œæ˜¯å¦å®Œæˆï¼ˆPodå‰¯æœ¬çš„æ•°é‡æ˜¯å¦è¾¾åˆ°é¢„æœŸå€¼ï¼‰ã€‚ æ›´æ–°Deploymentä»¥åˆ›å»ºæ–°çš„Pod(ä¾‹å¦‚é•œåƒå‡çº§çš„åœºæ™¯)ã€‚ å¦‚æœå½“å‰Deploymentä¸ç¨³å®šï¼Œå›é€€åˆ°ä¸Šä¸€ä¸ªDeploymentç‰ˆæœ¬ã€‚ æŒ‚èµ·æˆ–æ¢å¤ä¸€ä¸ªDeploymentã€‚ å¯ä»¥é€šè¿‡kubectl describe deploymentæ¥æŸ¥çœ‹Deploymentæ§åˆ¶çš„Podçš„æ°´å¹³æ‹“å±•è¿‡ç¨‹ã€‚\n7. Horizontal Pod Autoscaler(HPA) Horizontal Pod Autoscaler(HPA)å³Podæ¨ªå‘è‡ªåŠ¨æ‰©å®¹ï¼Œä¸RCä¸€æ ·ä¹Ÿå±äºk8sçš„èµ„æºå¯¹è±¡ã€‚\nHPAåŸç†ï¼šé€šè¿‡è¿½è¸ªåˆ†æRCæ§åˆ¶çš„æ‰€æœ‰ç›®æ ‡Podçš„è´Ÿè½½å˜åŒ–æƒ…å†µï¼Œæ¥ç¡®å®šæ˜¯å¦é’ˆå¯¹æ€§è°ƒæ•´Podçš„å‰¯æœ¬æ•°ã€‚\nPodè´Ÿè½½åº¦é‡æŒ‡æ ‡ï¼š\nCPUUtilizationPercentageï¼šPodæ‰€æœ‰å‰¯æœ¬è‡ªèº«çš„CPUåˆ©ç”¨ç‡çš„å¹³å‡å€¼ã€‚å³å½“å‰Podçš„CPUä½¿ç”¨é‡é™¤ä»¥Pod Requestçš„å€¼ã€‚ åº”ç”¨è‡ªå®šä¹‰çš„åº¦é‡æŒ‡æ ‡ï¼Œæ¯”å¦‚æœåŠ¡æ¯ç§’å†…å“åº”çš„è¯·æ±‚æ•°ï¼ˆTPS/QPSï¼‰ã€‚ 8. Service(æœåŠ¡) 8.1. Serviceæ¦‚è¿° Serviceå®šä¹‰äº†ä¸€ä¸ªæœåŠ¡çš„è®¿é—®å…¥å£åœ°å€ï¼Œå‰ç«¯åº”ç”¨é€šè¿‡è¿™ä¸ªå…¥å£åœ°å€è®¿é—®å…¶èƒŒåçš„ä¸€ç»„ç”±Podå‰¯æœ¬ç»„æˆçš„é›†ç¾¤å®ä¾‹ï¼ŒServiceä¸å…¶åç«¯çš„Podå‰¯æœ¬é›†ç¾¤ä¹‹é—´æ˜¯é€šè¿‡Label Selectoræ¥å®ç°â€œæ— ç¼å¯¹æ¥â€ã€‚RCä¿è¯Serviceçš„Podå‰¯æœ¬å®ä¾‹æ•°ç›®ä¿æŒé¢„æœŸæ°´å¹³ã€‚\n8.2. kubernetesçš„æœåŠ¡å‘ç°æœºåˆ¶ ä¸»è¦é€šè¿‡kube-dnsè¿™ä¸ªç»„ä»¶æ¥è¿›è¡ŒDNSæ–¹å¼çš„æœåŠ¡å‘ç°ã€‚\n8.3. å¤–éƒ¨ç³»ç»Ÿè®¿é—®Serviceçš„é—®é¢˜ IPç±»å‹ è¯´æ˜ Node IP NodeèŠ‚ç‚¹çš„IPåœ°å€ Pod IP Podçš„IPåœ°å€ Cluster IP Serviceçš„IPåœ°å€ 8.3.1. Node IP NodeIPæ˜¯é›†ç¾¤ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„ç‰©ç†ç½‘å¡IPåœ°å€ï¼Œæ˜¯çœŸå®å­˜åœ¨çš„ç‰©ç†ç½‘ç»œï¼Œkubernetesé›†ç¾¤ä¹‹å¤–çš„èŠ‚ç‚¹è®¿é—®kuberneteså†…çš„æŸä¸ªèŠ‚ç‚¹æˆ–TCP/IPæœåŠ¡çš„æ—¶å€™ï¼Œéœ€è¦é€šè¿‡NodeIPè¿›è¡Œé€šä¿¡ã€‚\n8.3.2. Pod IP Pod IPæ˜¯æ¯ä¸ªPodçš„IPåœ°å€ï¼Œæ˜¯Docker Engineæ ¹æ®docker0ç½‘æ¡¥çš„IPæ®µåœ°å€è¿›è¡Œåˆ†é…çš„ï¼Œæ˜¯ä¸€ä¸ªè™šæ‹ŸäºŒå±‚ç½‘ç»œï¼Œé›†ç¾¤ä¸­ä¸€ä¸ªPodçš„å®¹å™¨è®¿é—®å¦ä¸€ä¸ªPodä¸­çš„å®¹å™¨ï¼Œæ˜¯é€šè¿‡Pod IPè¿›è¡Œé€šä¿¡çš„ï¼Œè€ŒçœŸå®çš„TCP/IPæµé‡æ˜¯é€šè¿‡Node IPæ‰€åœ¨çš„ç½‘å¡æµå‡ºçš„ã€‚\n8.3.3. Cluster IP Serviceçš„Cluster IPæ˜¯ä¸€ä¸ªè™šæ‹ŸIPï¼Œåªä½œç”¨äºServiceè¿™ä¸ªå¯¹è±¡ï¼Œç”±kubernetesç®¡ç†å’Œåˆ†é…IPåœ°å€ï¼ˆæ¥æºäºCluster IPåœ°å€æ± ï¼‰ã€‚ Cluster IPæ— æ³•è¢«pingé€šï¼Œå› ä¸ºæ²¡æœ‰ä¸€ä¸ªå®ä½“ç½‘ç»œå¯¹è±¡æ¥å“åº”ã€‚ Cluster IPç»“åˆService Portç»„æˆçš„å…·ä½“é€šä¿¡ç«¯å£æ‰å…·å¤‡TCP/IPé€šä¿¡åŸºç¡€ï¼Œå±äºkubernetesé›†ç¾¤å†…ï¼Œé›†ç¾¤å¤–è®¿é—®è¯¥IPå’Œç«¯å£éœ€è¦é¢å¤–å¤„ç†ã€‚ k8sé›†ç¾¤å†…Node IP ã€Pod IPã€Cluster IPä¹‹é—´çš„é€šä¿¡é‡‡å–k8sè‡ªå·±çš„ç‰¹æ®Šçš„è·¯ç”±è§„åˆ™ï¼Œä¸ä¼ ç»ŸIPè·¯ç”±ä¸åŒã€‚ 8.3.4. å¤–éƒ¨è®¿é—®Kubernetesé›†ç¾¤ é€šè¿‡å®¿ä¸»æœºä¸å®¹å™¨ç«¯å£æ˜ å°„çš„æ–¹å¼è¿›è¡Œè®¿é—®ï¼Œä¾‹å¦‚ï¼šServiceå®šä½æ–‡ä»¶å¦‚ä¸‹ï¼š\nå¯ä»¥é€šè¿‡ä»»æ„Nodeçš„IP åŠ ç«¯å£è®¿é—®è¯¥æœåŠ¡ã€‚ä¹Ÿå¯ä»¥é€šè¿‡Nginxæˆ–HAProxyæ¥è®¾ç½®è´Ÿè½½å‡è¡¡ã€‚\n9. Volume(å­˜å‚¨å·) 9.1. Volumeçš„åŠŸèƒ½ Volumeæ˜¯Podä¸­èƒ½å¤Ÿè¢«å¤šä¸ªå®¹å™¨è®¿é—®çš„å…±äº«ç›®å½•ï¼Œå¯ä»¥è®©å®¹å™¨çš„æ•°æ®å†™åˆ°å®¿ä¸»æœºä¸Šæˆ–è€…å†™æ–‡ä»¶åˆ°ç½‘ç»œå­˜å‚¨ä¸­ å¯ä»¥å®ç°å®¹å™¨é…ç½®æ–‡ä»¶é›†ä¸­åŒ–å®šä¹‰ä¸ç®¡ç†ï¼Œé€šè¿‡ConfigMapèµ„æºå¯¹è±¡æ¥å®ç°ã€‚ 9.2. Volumeçš„ç‰¹ç‚¹ k8sä¸­çš„Volumeä¸Dockerçš„Volumeç›¸ä¼¼ï¼Œä½†ä¸å®Œå…¨ç›¸åŒã€‚\nk8sä¸ŠVolumeå®šä¹‰åœ¨Podä¸Šï¼Œç„¶åè¢«ä¸€ä¸ªPodä¸­çš„å¤šä¸ªå®¹å™¨æŒ‚è½½åˆ°å…·ä½“çš„æ–‡ä»¶ç›®å½•ä¸‹ã€‚ k8sçš„Volumeä¸Podç”Ÿå‘½å‘¨æœŸç›¸å…³è€Œä¸æ˜¯å®¹å™¨æ˜¯ç”Ÿå‘½å‘¨æœŸï¼Œå³å®¹å™¨æŒ‚æ‰ï¼Œæ•°æ®ä¸ä¼šä¸¢å¤±ä½†æ˜¯PodæŒ‚æ‰ï¼Œæ•°æ®åˆ™ä¼šä¸¢å¤±ã€‚ k8sä¸­çš„Volumeæ”¯æŒå¤šç§ç±»å‹çš„Volumeï¼šCephã€GlusterFSç­‰åˆ†å¸ƒå¼ç³»ç»Ÿã€‚ 9.3. Volumeçš„ä½¿ç”¨æ–¹å¼ å…ˆåœ¨Podä¸Šå£°æ˜ä¸€ä¸ªVolumeï¼Œç„¶åå®¹å™¨å¼•ç”¨è¯¥Volumeå¹¶Mountåˆ°å®¹å™¨çš„æŸä¸ªç›®å½•ã€‚\n9.4. Volumeç±»å‹ 9.4.1. emptyDir emptyDir Volumeæ˜¯åœ¨Podåˆ†é…åˆ°Nodeæ—¶åˆ›å»ºçš„ï¼Œåˆå§‹å†…å®¹ä¸ºç©ºï¼Œæ— é¡»æŒ‡å®šå®¿ä¸»æœºä¸Šå¯¹åº”çš„ç›®å½•æ–‡ä»¶ï¼Œç”±K8Sè‡ªåŠ¨åˆ†é…ä¸€ä¸ªç›®å½•ï¼Œå½“Podè¢«åˆ é™¤æ—¶ï¼Œå¯¹åº”çš„emptyDiræ•°æ®ä¹Ÿä¼šæ°¸ä¹…åˆ é™¤ã€‚\nä½œç”¨ï¼š\nä¸´æ—¶ç©ºé—´ï¼Œä¾‹å¦‚ç¨‹åºçš„ä¸´æ—¶æ–‡ä»¶ï¼Œæ— é¡»æ°¸ä¹…ä¿ç•™ é•¿æ—¶é—´ä»»åŠ¡çš„ä¸­é—´è¿‡ç¨‹CheckPointçš„ä¸´æ—¶ä¿å­˜ç›®å½• ä¸€ä¸ªå®¹å™¨éœ€è¦ä»å¦ä¸€ä¸ªå®¹å™¨ä¸­è·å–æ•°æ®çš„ç›®å½•ï¼ˆå³å¤šå®¹å™¨å…±äº«ç›®å½•ï¼‰ è¯´æ˜ï¼š\nç›®å‰ç”¨æˆ·æ— æ³•è®¾ç½®emptyVolumeçš„ä½¿ç”¨ä»‹è´¨ï¼Œå¦‚æœkubeletçš„é…ç½®ä½¿ç”¨ç¡¬ç›˜åˆ™emptyDirå°†åˆ›å»ºåœ¨è¯¥ç¡¬ç›˜ä¸Šã€‚\n9.4.2. hostPath hostPathæ˜¯åœ¨Podä¸ŠæŒ‚è½½å®¿ä¸»æœºä¸Šçš„æ–‡ä»¶æˆ–ç›®å½•ã€‚\nä½œç”¨ï¼š\nå®¹å™¨åº”ç”¨æ—¥å¿—éœ€è¦æŒä¹…åŒ–æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å®¿ä¸»æœºçš„é«˜é€Ÿæ–‡ä»¶ç³»ç»Ÿè¿›è¡Œå­˜å‚¨ éœ€è¦è®¿é—®å®¿ä¸»æœºä¸ŠDockerå¼•æ“å†…éƒ¨æ•°æ®ç»“æ„çš„å®¹å™¨åº”ç”¨æ—¶ï¼Œå¯ä»¥é€šè¿‡å®šä¹‰hostPathä¸ºå®¿ä¸»æœº/var/lib/dockerç›®å½•ï¼Œä½¿å®¹å™¨å†…éƒ¨åº”ç”¨å¯ä»¥ç›´æ¥è®¿é—®Dockerçš„æ–‡ä»¶ç³»ç»Ÿã€‚ æ³¨æ„ç‚¹ï¼š\nåœ¨ä¸åŒçš„Nodeä¸Šå…·æœ‰ç›¸åŒé…ç½®çš„Podå¯èƒ½ä¼šå› ä¸ºå®¿ä¸»æœºä¸Šçš„ç›®å½•æˆ–æ–‡ä»¶ä¸åŒå¯¼è‡´å¯¹Volumeä¸Šç›®å½•æˆ–æ–‡ä»¶çš„è®¿é—®ç»“æœä¸ä¸€è‡´ã€‚ å¦‚æœä½¿ç”¨äº†èµ„æºé…é¢ç®¡ç†ï¼Œåˆ™kubernetesæ— æ³•å°†hostPathåœ¨å®¿ä¸»æœºä¸Šä½¿ç”¨çš„èµ„æºçº³å…¥ç®¡ç†ã€‚ 9.4.3. gcePersistentDisk è¡¨ç¤ºä½¿ç”¨è°·æ­Œå…¬æœ‰äº‘æä¾›çš„æ°¸ä¹…ç£ç›˜ï¼ˆPersistent Disk ,PDï¼‰å­˜æ”¾Volumeçš„æ•°æ®ï¼Œå®ƒä¸EmptyDirä¸åŒï¼ŒPDä¸Šçš„å†…å®¹ä¼šè¢«æ°¸ä¹…ä¿å­˜ã€‚å½“Podè¢«åˆ é™¤æ—¶ï¼ŒPDåªæ˜¯è¢«å¸è½½æ—¶ï¼Œä½†ä¸ä¼šè¢«åˆ é™¤ã€‚éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªæ°¸ä¹…ç£ç›˜ï¼Œæ‰èƒ½ä½¿ç”¨gcePersistentDiskã€‚\nä½¿ç”¨gcePersistentDiskçš„é™åˆ¶æ¡ä»¶ï¼š\nNode(è¿è¡Œkubeletçš„èŠ‚ç‚¹)éœ€è¦æ˜¯GCEè™šæ‹Ÿæœºã€‚ è™šæ‹Ÿæœºéœ€è¦ä¸PDå­˜åœ¨äºç›¸åŒçš„GCEé¡¹ç›®ä¸­å’ŒZoneä¸­ã€‚ 10. Persistent Volume Volumeå®šä¹‰åœ¨Podä¸Šï¼Œå±äºâ€œè®¡ç®—èµ„æºâ€çš„ä¸€éƒ¨åˆ†ï¼Œè€ŒPersistent Volumeå’ŒPersistent Volume Claimæ˜¯ç½‘ç»œå­˜å‚¨ï¼Œç®€ç§°PVå’ŒPVCï¼Œå¯ä»¥ç†è§£ä¸ºk8sé›†ç¾¤ä¸­æŸä¸ªç½‘ç»œå­˜å‚¨ä¸­å¯¹åº”çš„ä¸€å—å­˜å‚¨ã€‚\nPVæ˜¯ç½‘ç»œå­˜å‚¨ï¼Œä¸å±äºä»»ä½•Nodeï¼Œä½†å¯ä»¥åœ¨æ¯ä¸ªNodeä¸Šè®¿é—®ã€‚ PVä¸æ˜¯å®šä¹‰åœ¨Podä¸Šï¼Œè€Œæ˜¯ç‹¬ç«‹äºPodä¹‹å¤–å®šä¹‰ã€‚ PVå¸¸è§ç±»å‹ï¼šGCE Persistent Disksã€NFSã€RBDç­‰ã€‚ PVæ˜¯æœ‰çŠ¶æ€çš„å¯¹è±¡ï¼ŒçŠ¶æ€ç±»å‹å¦‚ä¸‹ï¼š\nAvailable:ç©ºé—²çŠ¶æ€ Bound:å·²ç»ç»‘å®šåˆ°æŸä¸ªPVCä¸Š Released:å¯¹åº”çš„PVCå·²ç»åˆ é™¤ï¼Œä½†èµ„æºè¿˜æ²¡æœ‰å›æ”¶ Failed:PVè‡ªåŠ¨å›æ”¶å¤±è´¥ 11. Namespace Namespaceå³å‘½åç©ºé—´ï¼Œä¸»è¦ç”¨äºå¤šç§Ÿæˆ·çš„èµ„æºéš”ç¦»ï¼Œé€šè¿‡å°†èµ„æºå¯¹è±¡åˆ†é…åˆ°ä¸åŒçš„Namespaceä¸Šï¼Œä¾¿äºä¸åŒçš„åˆ†ç»„åœ¨å…±äº«èµ„æºçš„åŒæ—¶å¯ä»¥è¢«åˆ†åˆ«ç®¡ç†ã€‚\nk8sé›†ç¾¤å¯åŠ¨åä¼šé»˜è®¤åˆ›å»ºä¸€ä¸ªâ€œdefaultâ€çš„Namespaceã€‚å¯ä»¥é€šè¿‡kubectl get namespaecsæŸ¥çœ‹ã€‚\nå¯ä»¥é€šè¿‡kubectl config use-context namespaceé…ç½®å½“å‰k8så®¢æˆ·ç«¯çš„ç¯å¢ƒï¼Œé€šè¿‡kubectl get podsè·å–å½“å‰namespaceçš„Podã€‚æˆ–è€…é€šè¿‡kubectl get pods --namespace=NAMESPACEæ¥è·å–æŒ‡å®šnamespaceçš„Podã€‚\nNamespace yamlæ–‡ä»¶çš„å®šä¹‰\n12. Annotation(æ³¨è§£) Annotationä¸Labelç±»ä¼¼ï¼Œä¹Ÿä½¿ç”¨key/valueçš„å½¢å¼è¿›è¡Œå®šä¹‰ï¼ŒLabelå®šä¹‰å…ƒæ•°æ®ï¼ˆMetadataï¼‰,Annotationå®šä¹‰â€œé™„åŠ â€ä¿¡æ¯ã€‚\né€šå¸¸Annotationè®°å½•ä¿¡æ¯å¦‚ä¸‹ï¼š\nbuildä¿¡æ¯ï¼Œreleaseä¿¡æ¯ï¼ŒDockeré•œåƒä¿¡æ¯ç­‰ã€‚ æ—¥å¿—åº“ã€ç›‘æ§åº“ç­‰ã€‚ å‚è€ƒã€ŠKubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. Master é›†ç¾¤çš„æ§åˆ¶èŠ‚ç‚¹ï¼Œè´Ÿè´£æ•´ä¸ªé›†ç¾¤çš„ç®¡ç†å’Œæ§åˆ¶ï¼Œkubernetesçš„æ‰€æœ‰çš„å‘½ä»¤åŸºæœ¬éƒ½æ˜¯å‘ç»™Masterï¼Œç”±å®ƒæ¥è´Ÿè´£å…·ä½“çš„æ‰§è¡Œ â€¦","ref":"/kubernetes-notes/concepts/object/kubernetes-basic-concepts/","tags":["Kubernetes"],"title":"KubernetesåŸºæœ¬æ¦‚å¿µ"},{"body":"1. redisæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆwhatï¼‰ Redisæ˜¯ä¸€ä¸ªå¼€æºï¼ˆBSDè®¸å¯ï¼‰ï¼Œå†…å­˜å­˜å‚¨çš„æ•°æ®ç»“æ„æœåŠ¡å™¨ï¼Œå¯ç”¨ä½œæ•°æ®åº“ï¼Œé«˜é€Ÿç¼“å­˜å’Œæ¶ˆæ¯é˜Ÿåˆ—ä»£ç†ã€‚å®ƒæ”¯æŒå­—ç¬¦ä¸²ã€å“ˆå¸Œè¡¨ã€åˆ—è¡¨ã€é›†åˆã€æœ‰åºé›†åˆï¼Œä½å›¾ï¼Œhyperloglogsç­‰æ•°æ®ç±»å‹ã€‚å†…ç½®å¤åˆ¶ã€Luaè„šæœ¬ã€LRUæ”¶å›ã€äº‹åŠ¡ä»¥åŠä¸åŒçº§åˆ«ç£ç›˜æŒä¹…åŒ–åŠŸèƒ½ï¼ŒåŒæ—¶é€šè¿‡Redis Sentinelæä¾›é«˜å¯ç”¨ï¼Œé€šè¿‡Redis Clusteræä¾›è‡ªåŠ¨åˆ†åŒºã€‚\nRedisæ˜¯ä¸€ä¸ªå¼€æºçš„ä½¿ç”¨ANSI Cè¯­è¨€ç¼–å†™ã€éµå®ˆBSDåè®®ã€æ”¯æŒç½‘ç»œã€å¯åŸºäºå†…å­˜äº¦å¯æŒä¹…åŒ–çš„æ—¥å¿—å‹ã€Key-Valueæ•°æ®åº“ï¼Œå¹¶æä¾›å¤šç§è¯­è¨€çš„APIã€‚\n2. ä¸ºä»€ä¹ˆä½¿ç”¨redisï¼Ÿï¼ˆwhyï¼‰ 2.1. redisçš„ç‰¹ç‚¹ Redisæ”¯æŒæ•°æ®çš„æŒä¹…åŒ–ï¼Œå¯ä»¥å°†å†…å­˜ä¸­çš„æ•°æ®ä¿æŒåœ¨ç£ç›˜ä¸­ï¼Œé‡å¯çš„æ—¶å€™å¯ä»¥å†æ¬¡åŠ è½½è¿›è¡Œä½¿ç”¨ã€‚ Redisä¸ä»…ä»…æ”¯æŒç®€å•çš„key-valueç±»å‹çš„æ•°æ®ï¼ŒåŒæ—¶è¿˜æä¾›listï¼Œsetï¼Œzsetï¼Œhashç­‰æ•°æ®ç»“æ„çš„å­˜å‚¨ã€‚ Redisæ”¯æŒæ•°æ®çš„å¤‡ä»½ï¼Œå³master-slaveæ¨¡å¼çš„æ•°æ®å¤‡ä»½ã€‚ 2.2. redisçš„ä¼˜åŠ¿ æ€§èƒ½æé«˜ â€“ Redisèƒ½è¯»çš„é€Ÿåº¦æ˜¯110000æ¬¡/s,å†™çš„é€Ÿåº¦æ˜¯81000æ¬¡/s ã€‚ ä¸°å¯Œçš„æ•°æ®ç±»å‹ â€“ Redisæ”¯æŒäºŒè¿›åˆ¶æ¡ˆä¾‹çš„ Strings, Lists, Hashes, Sets åŠ Ordered Sets æ•°æ®ç±»å‹æ“ä½œã€‚ åŸå­ â€“ Redisçš„æ‰€æœ‰æ“ä½œéƒ½æ˜¯åŸå­æ€§çš„ï¼ŒåŒæ—¶Redisè¿˜æ”¯æŒå¯¹å‡ ä¸ªæ“ä½œå…¨å¹¶åçš„åŸå­æ€§æ‰§è¡Œã€‚ ä¸°å¯Œçš„ç‰¹æ€§ â€“ Redisè¿˜æ”¯æŒ publish/subscribe, é€šçŸ¥, key è¿‡æœŸç­‰ç­‰ç‰¹æ€§ã€‚ 2.3. redisä¸å…¶ä»–key-valueå­˜å‚¨æœ‰ä»€ä¹ˆä¸åŒ Redisæœ‰ç€æ›´ä¸ºå¤æ‚çš„æ•°æ®ç»“æ„å¹¶ä¸”æä¾›å¯¹ä»–ä»¬çš„åŸå­æ€§æ“ä½œï¼ŒRedisçš„æ•°æ®ç±»å‹éƒ½æ˜¯åŸºäºåŸºæœ¬æ•°æ®ç»“æ„çš„åŒæ—¶å¯¹ç¨‹åºå‘˜é€æ˜ï¼Œæ— éœ€è¿›è¡Œé¢å¤–çš„æŠ½è±¡ã€‚ Redisè¿è¡Œåœ¨å†…å­˜ä¸­ä½†æ˜¯å¯ä»¥æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œæ‰€ä»¥åœ¨å¯¹ä¸åŒæ•°æ®é›†è¿›è¡Œé«˜é€Ÿè¯»å†™æ—¶éœ€è¦æƒè¡¡å†…å­˜ï¼Œåº”ä¸ºæ•°æ®é‡ä¸èƒ½å¤§äºç¡¬ä»¶å†…å­˜ã€‚ ç›¸æ¯”åœ¨ç£ç›˜ä¸Šç›¸åŒçš„å¤æ‚çš„æ•°æ®ç»“æ„ï¼Œåœ¨å†…å­˜ä¸­æ“ä½œèµ·æ¥éå¸¸ç®€å•ï¼Œè¿™æ ·Rediså¯ä»¥åšå¾ˆå¤šå†…éƒ¨å¤æ‚æ€§å¾ˆå¼ºçš„äº‹æƒ…ã€‚ åœ¨ç£ç›˜æ ¼å¼æ–¹é¢ä»–ä»¬æ˜¯ç´§å‡‘çš„ä»¥è¿½åŠ çš„æ–¹å¼äº§ç”Ÿçš„ï¼Œå› ä¸ºä»–ä»¬å¹¶ä¸éœ€è¦è¿›è¡Œéšæœºè®¿é—®ã€‚ 3. å¦‚ä½•ä½¿ç”¨redisï¼Ÿï¼ˆhowï¼‰ 3.1. redisçš„æ•°æ®ç±»å‹ æ•°æ®ç±»å‹ æ¦‚å¿µ å¸¸ç”¨å‘½ä»¤ String(å­—ç¬¦ä¸²) key-valueå‹ SET ï¼ŒGET Hash(å“ˆå¸Œ) field-value,é€‚ç”¨äºå­˜å‚¨å¯¹è±¡ç±»å‹ï¼ˆå¯¹è±¡å-å¯¹è±¡å±æ€§å€¼ï¼‰ HMSETï¼ŒHEGTALL List(åˆ—è¡¨) stringç±»å‹çš„æœ‰åºåˆ—è¡¨ï¼ŒæŒ‰ç…§æ’å…¥é¡ºåºæ’åº lpushï¼Œlrange Set(é›†åˆ) stringç±»å‹çš„æ— åºé›†åˆ saddï¼Œsmembers zset(sorted setï¼šæœ‰åºé›†åˆ) stringç±»å‹å…ƒç´ çš„é›†åˆ,ä¸”ä¸å…è®¸é‡å¤çš„æˆå‘˜ã€‚æ¯ä¸ªå…ƒç´ å…³è”ä¸€ä¸ªdoubleå€¼æ¥è¿›è¡Œæ’åºï¼Œdoubleå€¼å¯ä»¥é‡å¤ä½†å…ƒç´ ä¸èƒ½é‡å¤ã€‚ zaddï¼ŒZRANGEBYSCORE 3.2. rediså¸¸ç”¨å‘½ä»¤ ","categories":"","description":"","excerpt":"1. redisæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆwhatï¼‰ Redisæ˜¯ä¸€ä¸ªå¼€æºï¼ˆBSDè®¸å¯ï¼‰ï¼Œå†…å­˜å­˜å‚¨çš„æ•°æ®ç»“æ„æœåŠ¡å™¨ï¼Œå¯ç”¨ä½œæ•°æ®åº“ï¼Œé«˜é€Ÿç¼“å­˜å’Œæ¶ˆæ¯é˜Ÿåˆ—ä»£ç†ã€‚å®ƒ â€¦","ref":"/linux-notes/redis/redis-introduction/","tags":["Redis"],"title":"Redisä»‹ç»"},{"body":" etcdctlçš„v3ç‰ˆæœ¬ä¸v2ç‰ˆæœ¬ä½¿ç”¨å‘½ä»¤æœ‰æ‰€ä¸åŒï¼Œæœ¬æ–‡ä»‹ç»etcdctl v3ç‰ˆæœ¬çš„å‘½ä»¤å·¥å…·çš„ä½¿ç”¨æ–¹å¼ã€‚\n1. etcdctlçš„å®‰è£… etcdctlçš„äºŒè¿›åˆ¶æ–‡ä»¶å¯ä»¥åœ¨ github.com/coreos/etcd/releases é€‰æ‹©å¯¹åº”çš„ç‰ˆæœ¬ä¸‹è½½ï¼Œä¾‹å¦‚å¯ä»¥æ‰§è¡Œä»¥ä¸‹install_etcdctl.shçš„è„šæœ¬ï¼Œä¿®æ”¹å…¶ä¸­çš„ç‰ˆæœ¬ä¿¡æ¯ã€‚\n#!/bin/bash ETCD_VER=v3.3.4 ETCD_DIR=etcd-download DOWNLOAD_URL=https://github.com/coreos/etcd/releases/download # Download mkdir ${ETCD_DIR} cd ${ETCD_DIR} wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz tar -xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz # install cd etcd-${ETCD_VER}-linux-amd64 cp etcdctl /usr/local/bin/ 2. etcdctl V3 ä½¿ç”¨etcdctlv3çš„ç‰ˆæœ¬æ—¶ï¼Œéœ€è®¾ç½®ç¯å¢ƒå˜é‡ETCDCTL_API=3ã€‚\nexport ETCDCTL_API=3 # æˆ–è€…åœ¨`/etc/profile`æ–‡ä»¶ä¸­æ·»åŠ ç¯å¢ƒå˜é‡ vi /etc/profile ... export ETCDCTL_API=3 ... source /etc/profile # æˆ–è€…åœ¨å‘½ä»¤æ‰§è¡Œå‰åŠ  ETCDCTL_API=3 ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS member list æŸ¥çœ‹å½“å‰etcdctlçš„ç‰ˆæœ¬ä¿¡æ¯etcdctl versionã€‚\n[root@k8s-dbg-master-1 etcd]# etcdctl version etcdctl version: 3.3.4 API version: 3.3 æ›´å¤šå‘½ä»¤å¸®åŠ©å¯ä»¥æŸ¥è¯¢etcdctl â€”helpã€‚\n[root@k8s-dbg-master-1 etcd]# etcdctl --help NAME: etcdctl - A simple command line client for etcd3. USAGE: etcdctl VERSION: 3.3.4 API VERSION: 3.3 COMMANDS: get\tGets the key or a range of keys put\tPuts the given key into the store del\tRemoves the specified key or range of keys [key, range_end) txn\tTxn processes all the requests in one transaction compaction\tCompacts the event history in etcd alarm disarm\tDisarms all alarms alarm list\tLists all alarms defrag\tDefragments the storage of the etcd members with given endpoints endpoint health\tChecks the healthiness of endpoints specified in `--endpoints` flag endpoint status\tPrints out the status of endpoints specified in `--endpoints` flag endpoint hashkv\tPrints the KV history hash for each endpoint in --endpoints move-leader\tTransfers leadership to another etcd cluster member. watch\tWatches events stream on keys or prefixes version\tPrints the version of etcdctl lease grant\tCreates leases lease revoke\tRevokes leases lease timetolive\tGet lease information lease list\tList all active leases lease keep-alive\tKeeps leases alive (renew) member add\tAdds a member into the cluster member remove\tRemoves a member from the cluster member update\tUpdates a member in the cluster member list\tLists all members in the cluster snapshot save\tStores an etcd node backend snapshot to a given file snapshot restore\tRestores an etcd member snapshot to an etcd directory snapshot status\tGets backend snapshot status of a given file make-mirror\tMakes a mirror at the destination etcd cluster migrate\tMigrates keys in a v2 store to a mvcc store lock\tAcquires a named lock elect\tObserves and participates in leader election auth enable\tEnables authentication auth disable\tDisables authentication user add\tAdds a new user user delete\tDeletes a user user get\tGets detailed information of a user user list\tLists all users user passwd\tChanges password of user user grant-role\tGrants a role to a user user revoke-role\tRevokes a role from a user role add\tAdds a new role role delete\tDeletes a role role get\tGets detailed information of a role role list\tLists all roles role grant-permission\tGrants a key to a role role revoke-permission\tRevokes a key from a role check perf\tCheck the performance of the etcd cluster help\tHelp about any command OPTIONS: --cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --command-timeout=5s\ttimeout for short running command (excluding dial timeout) --debug[=false]\tenable client-side debug logging --dial-timeout=2s\tdial timeout for client connections -d, --discovery-srv=\"\"\tdomain name to query for SRV records describing cluster endpoints --endpoints=[127.0.0.1:2379]\tgRPC endpoints --hex[=false]\tprint byte strings as hex encoded strings --insecure-discovery[=true]\taccept insecure SRV records describing cluster endpoints --insecure-skip-tls-verify[=false]\tskip server certificate verification --insecure-transport[=true]\tdisable transport security for client connections --keepalive-time=2s\tkeepalive time for client connections --keepalive-timeout=6s\tkeepalive timeout for client connections --key=\"\"\tidentify secure client using this TLS key file --user=\"\"\tusername[:password] for authentication (prompt if password is not supplied) -w, --write-out=\"simple\"\tset the output format (fields, json, protobuf, simple, table) 3. etcdctl å¸¸ç”¨å‘½ä»¤ 3.1. æŒ‡å®šetcdé›†ç¾¤ HOST_1=10.240.0.17 HOST_2=10.240.0.18 HOST_3=10.240.0.19 ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379 etcdctl --endpoints=$ENDPOINTS member list å¦‚æœetcdè®¾ç½®äº†è¯ä¹¦è®¿é—®ï¼Œåˆ™éœ€è¦æ·»åŠ è¯ä¹¦ç›¸å…³å‚æ•°ï¼š\nETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS --cacert=\u003cca-file\u003e --cert=\u003ccert-file\u003e --key=\u003ckey-file\u003e \u003ccommand\u003e å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\n--cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --key=\"\"\tidentify secure client using this TLS key file --endpoints=[127.0.0.1:2379]\tgRPC endpoints å¯ä»¥è‡ªå®šä¹‰aliaså‘½ä»¤\n# alias å‘½ä»¤ï¼Œé¿å…æ¯æ¬¡éœ€è¦è¾“å…¥è¯ä¹¦å‚æ•° alias ectl='ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS --cacert=\u003cca-file\u003e --cert=\u003ccert-file\u003e --key=\u003ckey-file\u003e' # ç›´æ¥ä½¿ç”¨åˆ«åæ‰§è¡Œå‘½ä»¤ ectl \u003ccommand\u003e 3.2. å¢åˆ æ”¹æŸ¥ 1ã€å¢\netcdctl --endpoints=$ENDPOINTS put foo \"Hello World!\" 2ã€æŸ¥\netcdctl --endpoints=$ENDPOINTS get foo etcdctl --endpoints=$ENDPOINTS --write-out=\"json\" get foo åŸºäºç›¸åŒå‰ç¼€æŸ¥æ‰¾\netcdctl --endpoints=$ENDPOINTS put web1 value1 etcdctl --endpoints=$ENDPOINTS put web2 value2 etcdctl --endpoints=$ENDPOINTS put web3 value3 etcdctl --endpoints=$ENDPOINTS get web --prefix åˆ—å‡ºæ‰€æœ‰çš„key\netcdctl --endpoints=$ENDPOINTS get / --prefix --keys-only 3ã€åˆ \netcdctl --endpoints=$ENDPOINTS put key myvalue etcdctl --endpoints=$ENDPOINTS del key etcdctl --endpoints=$ENDPOINTS put k1 value1 etcdctl --endpoints=$ENDPOINTS put k2 value2 etcdctl --endpoints=$ENDPOINTS del k --prefix 3.3. é›†ç¾¤çŠ¶æ€ é›†ç¾¤çŠ¶æ€ä¸»è¦æ˜¯etcdctl endpoint status å’Œetcdctl endpoint healthä¸¤æ¡å‘½ä»¤ã€‚\netcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status +------------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +------------------+------------------+---------+---------+-----------+-----------+------------+ | 10.240.0.17:2379 | 4917a7ab173fabe7 | 3.0.0 | 45 kB | true | 4 | 16726 | | 10.240.0.18:2379 | 59796ba9cd1bcd72 | 3.0.0 | 45 kB | false | 4 | 16726 | | 10.240.0.19:2379 | 94df724b66343e6c | 3.0.0 | 45 kB | false | 4 | 16726 | +------------------+------------------+---------+---------+-----------+-----------+------------+ etcdctl --endpoints=$ENDPOINTS endpoint health 10.240.0.17:2379 is healthy: successfully committed proposal: took = 3.345431ms 10.240.0.19:2379 is healthy: successfully committed proposal: took = 3.767967ms 10.240.0.18:2379 is healthy: successfully committed proposal: took = 4.025451ms 3.4. é›†ç¾¤æˆå‘˜ è·Ÿé›†ç¾¤æˆå‘˜ç›¸å…³çš„å‘½ä»¤å¦‚ä¸‹ï¼š\nmember add\tAdds a member into the cluster member remove\tRemoves a member from the cluster member update\tUpdates a member in the cluster member list\tLists all members in the cluster ä¾‹å¦‚ etcdctl member liståˆ—å‡ºé›†ç¾¤æˆå‘˜çš„å‘½ä»¤ã€‚\netcdctl --endpoints=http://172.16.5.4:12379 member list -w table +-----------------+---------+-------+------------------------+-----------------------------------------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | +-----------------+---------+-------+------------------------+-----------------------------------------------+ | c856d92a82ba66a | started | etcd0 | http://172.16.5.4:2380 | http://172.16.5.4:2379,http://172.16.5.4:4001 | +-----------------+---------+-------+------------------------+-----------------------------------------------+ 4. etcdctl get ä½¿ç”¨etcdctl {command} --helpå¯ä»¥æŸ¥çœ‹å…·ä½“å‘½ä»¤çš„å¸®åŠ©ä¿¡æ¯ã€‚\n# etcdctl get --help NAME: get - Gets the key or a range of keys USAGE: etcdctl get [options] \u003ckey\u003e [range_end] OPTIONS: --consistency=\"l\"\tLinearizable(l) or Serializable(s) --from-key[=false]\tGet keys that are greater than or equal to the given key using byte compare --keys-only[=false]\tGet only the keys --limit=0\tMaximum number of results --order=\"\"\tOrder of results; ASCEND or DESCEND (ASCEND by default) --prefix[=false]\tGet keys with matching prefix --print-value-only[=false]\tOnly write values when using the \"simple\" output format --rev=0\tSpecify the kv revision --sort-by=\"\"\tSort target; CREATE, KEY, MODIFY, VALUE, or VERSION GLOBAL OPTIONS: --cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --command-timeout=5s\ttimeout for short running command (excluding dial timeout) --debug[=false]\tenable client-side debug logging --dial-timeout=2s\tdial timeout for client connections --endpoints=[127.0.0.1:2379]\tgRPC endpoints --hex[=false]\tprint byte strings as hex encoded strings --insecure-skip-tls-verify[=false]\tskip server certificate verification --insecure-transport[=true]\tdisable transport security for client connections --key=\"\"\tidentify secure client using this TLS key file --user=\"\"\tusername[:password] for authentication (prompt if password is not supplied) -w, --write-out=\"simple\"\tset the output format (fields, json, protobuf, simple, table) æ–‡ç« å‚è€ƒï¼š\nhttps://coreos.com/etcd/docs/latest/demo.html\n","categories":"","description":"","excerpt":" etcdctlçš„v3ç‰ˆæœ¬ä¸v2ç‰ˆæœ¬ä½¿ç”¨å‘½ä»¤æœ‰æ‰€ä¸åŒï¼Œæœ¬æ–‡ä»‹ç»etcdctl v3ç‰ˆæœ¬çš„å‘½ä»¤å·¥å…·çš„ä½¿ç”¨æ–¹å¼ã€‚\n1. etcdctl â€¦","ref":"/kubernetes-notes/etcd/etcdctl/etcdctl-v3/","tags":["Etcd"],"title":"etcdctl-V3"},{"body":"1. Etcdæ˜¯ä»€ä¹ˆï¼ˆwhatï¼‰ etcd is a distributed, consistent key-value store for shared configuration and service discovery, with a focus on being:\nSecure: automatic TLS with optional client cert authentication[å¯é€‰çš„SSLå®¢æˆ·ç«¯è¯ä¹¦è®¤è¯ï¼šæ”¯æŒhttpsè®¿é—® ] Fast: benchmarked 10,000 writes/sec[å•å®ä¾‹æ¯ç§’ 1000 æ¬¡å†™æ“ä½œ] Reliable: properly distributed using Raft[ä½¿ç”¨Raftä¿è¯ä¸€è‡´æ€§] etcdæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ã€ä¸€è‡´æ€§çš„é”®å€¼å­˜å‚¨ç³»ç»Ÿï¼Œä¸»è¦ç”¨äºé…ç½®å…±äº«å’ŒæœåŠ¡å‘ç°ã€‚[ä»¥ä¸Šå†…å®¹æ¥è‡ªetcdå®˜ç½‘]\n2. ä¸ºä»€ä¹ˆä½¿ç”¨Etcdï¼ˆwhyï¼‰ 2.1. Etcdçš„ä¼˜åŠ¿ ç®€å•ã€‚ä½¿ç”¨Goè¯­è¨€ç¼–å†™éƒ¨ç½²ç®€å•ï¼›ä½¿ç”¨HTTPä½œä¸ºæ¥å£ä½¿ç”¨ç®€å•ï¼›ä½¿ç”¨Raftç®—æ³•ä¿è¯å¼ºä¸€è‡´æ€§è®©ç”¨æˆ·æ˜“äºç†è§£ã€‚ æ•°æ®æŒä¹…åŒ–ã€‚etcdé»˜è®¤æ•°æ®ä¸€æ›´æ–°å°±è¿›è¡ŒæŒä¹…åŒ–ã€‚ å®‰å…¨ã€‚etcdæ”¯æŒSSLå®¢æˆ·ç«¯å®‰å…¨è®¤è¯ã€‚ 3. å¦‚ä½•å®ç°Etcdæ¶æ„ï¼ˆhowï¼‰ 3.1. Etcdçš„ç›¸å…³åè¯è§£é‡Š Raftï¼šetcdæ‰€é‡‡ç”¨çš„ä¿è¯åˆ†å¸ƒå¼ç³»ç»Ÿå¼ºä¸€è‡´æ€§çš„ç®—æ³•ã€‚ Nodeï¼šä¸€ä¸ªRaftçŠ¶æ€æœºå®ä¾‹ã€‚ Memberï¼š ä¸€ä¸ªetcdå®ä¾‹ã€‚å®ƒç®¡ç†ç€ä¸€ä¸ªNodeï¼Œå¹¶ä¸”å¯ä»¥ä¸ºå®¢æˆ·ç«¯è¯·æ±‚æä¾›æœåŠ¡ã€‚ Clusterï¼šç”±å¤šä¸ªMemberæ„æˆå¯ä»¥ååŒå·¥ä½œçš„etcdé›†ç¾¤ã€‚ Peerï¼šå¯¹åŒä¸€ä¸ªetcdé›†ç¾¤ä¸­å¦å¤–ä¸€ä¸ªMemberçš„ç§°å‘¼ã€‚ Clientï¼š å‘etcdé›†ç¾¤å‘é€HTTPè¯·æ±‚çš„å®¢æˆ·ç«¯ã€‚ WALï¼šé¢„å†™å¼æ—¥å¿—ï¼Œetcdç”¨äºæŒä¹…åŒ–å­˜å‚¨çš„æ—¥å¿—æ ¼å¼ã€‚ snapshotï¼šetcdé˜²æ­¢WALæ–‡ä»¶è¿‡å¤šè€Œè®¾ç½®çš„å¿«ç…§ï¼Œå­˜å‚¨etcdæ•°æ®çŠ¶æ€ã€‚ Proxyï¼šetcdçš„ä¸€ç§æ¨¡å¼ï¼Œä¸ºetcdé›†ç¾¤æä¾›åå‘ä»£ç†æœåŠ¡ã€‚ Leaderï¼šRaftç®—æ³•ä¸­é€šè¿‡ç«é€‰è€Œäº§ç”Ÿçš„å¤„ç†æ‰€æœ‰æ•°æ®æäº¤çš„èŠ‚ç‚¹ã€‚ Followerï¼šç«é€‰å¤±è´¥çš„èŠ‚ç‚¹ä½œä¸ºRaftä¸­çš„ä»å±èŠ‚ç‚¹ï¼Œä¸ºç®—æ³•æä¾›å¼ºä¸€è‡´æ€§ä¿è¯ã€‚ Candidateï¼šå½“Followerè¶…è¿‡ä¸€å®šæ—¶é—´æ¥æ”¶ä¸åˆ°Leaderçš„å¿ƒè·³æ—¶è½¬å˜ä¸ºCandidateå¼€å§‹ç«é€‰ã€‚ã€å€™é€‰äººã€‘ Termï¼šæŸä¸ªèŠ‚ç‚¹æˆä¸ºLeaderåˆ°ä¸‹ä¸€æ¬¡ç«é€‰æ—¶é—´ï¼Œç§°ä¸ºä¸€ä¸ªTermã€‚ã€ä»»æœŸã€‘ Indexï¼šæ•°æ®é¡¹ç¼–å·ã€‚Raftä¸­é€šè¿‡Termå’ŒIndexæ¥å®šä½æ•°æ®ã€‚ 3.2. Etcdçš„æ¶æ„å›¾ ä¸€ä¸ªç”¨æˆ·çš„è¯·æ±‚å‘é€è¿‡æ¥ï¼Œä¼šç»ç”±HTTP Serverè½¬å‘ç»™Storeè¿›è¡Œå…·ä½“çš„äº‹åŠ¡å¤„ç†ï¼Œå¦‚æœæ¶‰åŠåˆ°èŠ‚ç‚¹çš„ä¿®æ”¹ï¼Œåˆ™äº¤ç»™Raftæ¨¡å—è¿›è¡ŒçŠ¶æ€çš„å˜æ›´ã€æ—¥å¿—çš„è®°å½•ï¼Œç„¶åå†åŒæ­¥ç»™åˆ«çš„etcdèŠ‚ç‚¹ä»¥ç¡®è®¤æ•°æ®æäº¤ï¼Œæœ€åè¿›è¡Œæ•°æ®çš„æäº¤ï¼Œå†æ¬¡åŒæ­¥ã€‚\n1ã€HTTP Server: ç”¨äºå¤„ç†ç”¨æˆ·å‘é€çš„APIè¯·æ±‚ä»¥åŠå…¶å®ƒetcdèŠ‚ç‚¹çš„åŒæ­¥ä¸å¿ƒè·³ä¿¡æ¯è¯·æ±‚ã€‚\n2ã€Raft: Raftå¼ºä¸€è‡´æ€§ç®—æ³•çš„å…·ä½“å®ç°ï¼Œæ˜¯etcdçš„æ ¸å¿ƒã€‚\n3ã€WAL: Write Ahead Logï¼ˆé¢„å†™å¼æ—¥å¿—ï¼‰ï¼Œæ˜¯etcdçš„æ•°æ®å­˜å‚¨æ–¹å¼ï¼Œç”¨äºç³»ç»Ÿæä¾›åŸå­æ€§å’ŒæŒä¹…æ€§çš„ä¸€ç³»åˆ—æŠ€æœ¯ã€‚é™¤äº†åœ¨å†…å­˜ä¸­å­˜æœ‰æ‰€æœ‰æ•°æ®çš„çŠ¶æ€ä»¥åŠèŠ‚ç‚¹çš„ç´¢å¼•ä»¥å¤–ï¼Œetcdå°±é€šè¿‡WALè¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨ã€‚WALä¸­ï¼Œæ‰€æœ‰çš„æ•°æ®æäº¤å‰éƒ½ä¼šäº‹å…ˆè®°å½•æ—¥å¿—ã€‚\nEntry[æ—¥å¿—å†…å®¹]:\nè´Ÿè´£å­˜å‚¨å…·ä½“æ—¥å¿—çš„å†…å®¹ã€‚\nSnapshot[å¿«ç…§å†…å®¹]:\nSnapshotæ˜¯ä¸ºäº†é˜²æ­¢æ•°æ®è¿‡å¤šè€Œè¿›è¡Œçš„çŠ¶æ€å¿«ç…§ï¼Œæ—¥å¿—å†…å®¹å‘ç”Ÿå˜åŒ–æ—¶ä¿å­˜Raftçš„çŠ¶æ€ã€‚\n4ã€Store: ç”¨äºå¤„ç†etcdæ”¯æŒçš„å„ç±»åŠŸèƒ½çš„äº‹åŠ¡ï¼ŒåŒ…æ‹¬æ•°æ®ç´¢å¼•ã€èŠ‚ç‚¹çŠ¶æ€å˜æ›´ã€ç›‘æ§ä¸åé¦ˆã€äº‹ä»¶å¤„ç†ä¸æ‰§è¡Œç­‰ç­‰ï¼Œæ˜¯etcdå¯¹ç”¨æˆ·æä¾›çš„å¤§å¤šæ•°APIåŠŸèƒ½çš„å…·ä½“å®ç°ã€‚\n","categories":"","description":"","excerpt":"1. Etcdæ˜¯ä»€ä¹ˆï¼ˆwhatï¼‰ etcd is a distributed, consistent key-value store for â€¦","ref":"/kubernetes-notes/etcd/etcd-introduction/","tags":["Etcd"],"title":"Etcdä»‹ç»"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/framework/cobra/","tags":"","title":"Cobraå‘½ä»¤æ¡†æ¶"},{"body":" confdçš„æºç å‚è€ƒï¼šhttps://github.com/kelseyhightower/confd\næœ¬æ–‡åˆ†æçš„confdçš„ç‰ˆæœ¬æ˜¯v0.16.0ï¼Œä»£ç å‚è€ƒï¼šhttps://github.com/kelseyhightower/confd/tree/v0.16.0ã€‚\n1. Main confdçš„å…¥å£å‡½æ•° Main å‡½æ•°ï¼Œå…ˆè§£æå‚æ•°ï¼Œå¦‚æœæ˜¯æ‰“å°ç‰ˆæœ¬ä¿¡æ¯çš„å‚æ•°ï¼Œåˆ™æ‰§è¡Œæ‰“å°ç‰ˆæœ¬çš„å‘½ä»¤ã€‚\nfunc main() { flag.Parse() if config.PrintVersion { fmt.Printf(\"confd %s (Git SHA: %s, Go Version: %s)\\n\", Version, GitSHA, runtime.Version()) os.Exit(0) } ... } å…¶ä¸­ç‰ˆæœ¬ä¿¡æ¯è®°å½•åœ¨https://github.com/kelseyhightower/confd/blob/v0.16.0/version.go#L3\nconst Version = \"0.16.0\" 1.1. initConfig åˆå§‹åŒ–é…ç½®æ–‡ä»¶ã€‚\nif err := initConfig(); err != nil { log.Fatal(err.Error()) } initConfigå‡½æ•°å¯¹åŸºæœ¬çš„é…ç½®å†…å®¹åšåˆå§‹åŒ–ï¼Œå½“æ²¡æœ‰æŒ‡å®šåç«¯å­˜å‚¨çš„æ—¶å€™ï¼Œè®¾ç½®é»˜è®¤å­˜å‚¨ã€‚\n// initConfig initializes the confd configuration by first setting defaults, // then overriding settings from the confd config file, then overriding // settings from environment variables, and finally overriding // settings from flags set on the command line. // It returns an error if any. func initConfig() error { _, err := os.Stat(config.ConfigFile) if os.IsNotExist(err) { log.Debug(\"Skipping confd config file.\") } else { log.Debug(\"Loading \" + config.ConfigFile) configBytes, err := ioutil.ReadFile(config.ConfigFile) if err != nil { return err } _, err = toml.Decode(string(configBytes), \u0026config) if err != nil { return err } } // Update config from environment variables. processEnv() if config.SecretKeyring != \"\" { kr, err := os.Open(config.SecretKeyring) if err != nil { log.Fatal(err.Error()) } defer kr.Close() config.PGPPrivateKey, err = ioutil.ReadAll(kr) if err != nil { log.Fatal(err.Error()) } } if config.LogLevel != \"\" { log.SetLevel(config.LogLevel) } if config.SRVDomain != \"\" \u0026\u0026 config.SRVRecord == \"\" { config.SRVRecord = fmt.Sprintf(\"_%s._tcp.%s.\", config.Backend, config.SRVDomain) } // Update BackendNodes from SRV records. if config.Backend != \"env\" \u0026\u0026 config.SRVRecord != \"\" { log.Info(\"SRV record set to \" + config.SRVRecord) srvNodes, err := getBackendNodesFromSRV(config.SRVRecord) if err != nil { return errors.New(\"Cannot get nodes from SRV records \" + err.Error()) } switch config.Backend { case \"etcd\": vsm := make([]string, len(srvNodes)) for i, v := range srvNodes { vsm[i] = config.Scheme + \"://\" + v } srvNodes = vsm } config.BackendNodes = srvNodes } if len(config.BackendNodes) == 0 { switch config.Backend { case \"consul\": config.BackendNodes = []string{\"127.0.0.1:8500\"} case \"etcd\": peerstr := os.Getenv(\"ETCDCTL_PEERS\") if len(peerstr) \u003e 0 { config.BackendNodes = strings.Split(peerstr, \",\") } else { config.BackendNodes = []string{\"http://127.0.0.1:4001\"} } case \"etcdv3\": config.BackendNodes = []string{\"127.0.0.1:2379\"} case \"redis\": config.BackendNodes = []string{\"127.0.0.1:6379\"} case \"vault\": config.BackendNodes = []string{\"http://127.0.0.1:8200\"} case \"zookeeper\": config.BackendNodes = []string{\"127.0.0.1:2181\"} } } // Initialize the storage client log.Info(\"Backend set to \" + config.Backend) if config.Watch { unsupportedBackends := map[string]bool{ \"dynamodb\": true, \"ssm\": true, } if unsupportedBackends[config.Backend] { log.Info(fmt.Sprintf(\"Watch is not supported for backend %s. Exiting...\", config.Backend)) os.Exit(1) } } if config.Backend == \"dynamodb\" \u0026\u0026 config.Table == \"\" { return errors.New(\"No DynamoDB table configured\") } config.ConfigDir = filepath.Join(config.ConfDir, \"conf.d\") config.TemplateDir = filepath.Join(config.ConfDir, \"templates\") return nil } 1.2. storeClient log.Info(\"Starting confd\") storeClient, err := backends.New(config.BackendsConfig) if err != nil { log.Fatal(err.Error()) } æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„å­˜å‚¨åç«¯ç±»å‹æ„é€ ä¸€ä¸ªå­˜å‚¨åç«¯çš„clientï¼Œå…¶ä¸­ä¸»è¦è°ƒç”¨çš„å‡½æ•°ä¸ºbackends.New(config.BackendsConfig)ã€‚\nå½“æ²¡æœ‰è®¾ç½®å­˜å‚¨åç«¯æ—¶ï¼Œé»˜è®¤ä¸ºetcdã€‚\nif config.Backend == \"\" { config.Backend = \"etcd\" } backendNodes := config.BackendNodes å½“å­˜å‚¨åç«¯ä¸ºfileç±»å‹çš„å¤„ç†ã€‚\nif config.Backend == \"file\" { log.Info(\"Backend source(s) set to \" + strings.Join(config.YAMLFile, \", \")) } else { log.Info(\"Backend source(s) set to \" + strings.Join(backendNodes, \", \")) } æœ€åå†æ ¹æ®ä¸åŒç±»å‹çš„å­˜å‚¨åç«¯ï¼Œè°ƒç”¨ä¸åŒçš„å­˜å‚¨åç«¯æ„å»ºå‡½æ•°ï¼Œæœ¬æ–‡åªåˆ†æredisç±»å‹çš„å­˜å‚¨åç«¯ã€‚\nswitch config.Backend { case \"consul\": return consul.New(config.BackendNodes, config.Scheme, config.ClientCert, config.ClientKey, config.ClientCaKeys, config.BasicAuth, config.Username, config.Password, ) case \"etcd\": // Create the etcd client upfront and use it for the life of the process. // The etcdClient is an http.Client and designed to be reused. return etcd.NewEtcdClient(backendNodes, config.ClientCert, config.ClientKey, config.ClientCaKeys, config.BasicAuth, config.Username, config.Password) case \"etcdv3\": return etcdv3.NewEtcdClient(backendNodes, config.ClientCert, config.ClientKey, config.ClientCaKeys, config.BasicAuth, config.Username, config.Password) case \"zookeeper\": return zookeeper.NewZookeeperClient(backendNodes) case \"rancher\": return rancher.NewRancherClient(backendNodes) case \"redis\": return redis.NewRedisClient(backendNodes, config.ClientKey, config.Separator) case \"env\": return env.NewEnvClient() case \"file\": return file.NewFileClient(config.YAMLFile, config.Filter) case \"vault\": vaultConfig := map[string]string{ \"app-id\": config.AppID, \"user-id\": config.UserID, \"role-id\": config.RoleID, \"secret-id\": config.SecretID, \"username\": config.Username, \"password\": config.Password, \"token\": config.AuthToken, \"cert\": config.ClientCert, \"key\": config.ClientKey, \"caCert\": config.ClientCaKeys, \"path\": config.Path, } return vault.New(backendNodes[0], config.AuthType, vaultConfig) case \"dynamodb\": table := config.Table log.Info(\"DynamoDB table set to \" + table) return dynamodb.NewDynamoDBClient(table) case \"ssm\": return ssm.New() } return nil, errors.New(\"Invalid backend\") å…¶ä¸­redisç±»å‹çš„å­˜å‚¨åç«¯è°ƒç”¨äº†NewRedisClientæ–¹æ³•æ¥æ„é€ redisçš„clientã€‚\ncase \"redis\": return redis.NewRedisClient(backendNodes, config.ClientKey, config.Separator) å…¶ä¸­æ¶‰åŠä¸‰ä¸ªå‚æ•°ï¼š\nbackendNodesï¼šredisçš„èŠ‚ç‚¹åœ°å€ã€‚ ClientKeyï¼šredisçš„å¯†ç ã€‚ Separatorï¼šæŸ¥æ‰¾redisé”®çš„åˆ†éš”ç¬¦ï¼Œè¯¥å‚æ•°åªç”¨åœ¨redisç±»å‹ã€‚ NewRedisClientå‡½æ•°æ–¹æ³•å¦‚ä¸‹ï¼š\n// NewRedisClient returns an *redis.Client with a connection to named machines. // It returns an error if a connection to the cluster cannot be made. func NewRedisClient(machines []string, password string, separator string) (*Client, error) { if separator == \"\" { separator = \"/\" } log.Debug(fmt.Sprintf(\"Redis Separator: %#v\", separator)) var err error clientWrapper := \u0026Client{machines: machines, password: password, separator: separator, client: nil, pscChan: make(chan watchResponse), psc: redis.PubSubConn{Conn: nil} } clientWrapper.client, _, err = tryConnect(machines, password, true) return clientWrapper, err } 1.3. processor stopChan := make(chan bool) doneChan := make(chan bool) errChan := make(chan error, 10) var processor template.Processor switch { case config.Watch: processor = template.WatchProcessor(config.TemplateConfig, stopChan, doneChan, errChan) default: processor = template.IntervalProcessor(config.TemplateConfig, stopChan, doneChan, errChan, config.Interval) } go processor.Process() å½“å¼€å¯watchå‚æ•°çš„æ—¶å€™ï¼Œåˆ™æ„é€ WatchProcessorï¼Œå¦åˆ™æ„é€ IntervalProcessorï¼Œæœ€åèµ·ä¸€ä¸ªgoroutineã€‚\ngo processor.Process() è¿™å—çš„é€»è¾‘åœ¨æœ¬æ–‡ç¬¬äºŒéƒ¨åˆ†æã€‚\n1.4. signalChan signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM) for { select { case err := \u003c-errChan: log.Error(err.Error()) case s := \u003c-signalChan: log.Info(fmt.Sprintf(\"Captured %v. Exiting...\", s)) close(doneChan) case \u003c-doneChan: os.Exit(0) } } 2. Process type Processor interface { Process() } Processoræ˜¯ä¸€ä¸ªæ¥å£ç±»å‹ï¼Œä¸»è¦çš„å®ç°ä½“æœ‰ï¼š\nintervalProcessorï¼šé»˜è®¤çš„å®ç°ä½“ï¼Œå³æ²¡æœ‰æ·»åŠ watchå‚æ•°ã€‚ watchProcessorï¼šæ·»åŠ watchå‚æ•°çš„å®ç°ä½“ã€‚ 2.1. intervalProcessor type intervalProcessor struct { config Config stopChan chan bool doneChan chan bool errChan chan error interval int } intervalProcessoræ ¹æ®configå†…å®¹å’Œå‡ ä¸ªchannelæ„é€ ä¸€ä¸ªintervalProcessorã€‚\nfunc IntervalProcessor(config Config, stopChan, doneChan chan bool, errChan chan error, interval int) Processor { return \u0026intervalProcessor{config, stopChan, doneChan, errChan, interval} } 2.1.1. intervalProcessor.Process func (p *intervalProcessor) Process() { defer close(p.doneChan) for { ts, err := getTemplateResources(p.config) if err != nil { log.Fatal(err.Error()) break } process(ts) select { case \u003c-p.stopChan: break case \u003c-time.After(time.Duration(p.interval) * time.Second): continue } } } é€šè¿‡è§£æconfigå†…å®¹è·å–TemplateResourcesï¼Œå…¶ä¸­æ ¸å¿ƒå‡½æ•°ä¸ºprocess(ts)ï¼Œç„¶åæ‰§è¡Œt.process()ï¼Œè¯¥å‡½æ•°ä¸­ä¼šè°ƒç”¨t.sync()ã€‚t.process()çš„å…·ä½“é€»è¾‘åæ–‡åˆ†æã€‚\nfunc process(ts []*TemplateResource) error { var lastErr error for _, t := range ts { if err := t.process(); err != nil { log.Error(err.Error()) lastErr = err } } return lastErr } 2.2. watchProcessor type watchProcessor struct { config Config stopChan chan bool doneChan chan bool errChan chan error wg sync.WaitGroup } watchProcessoræ ¹æ®configå†…å®¹å’Œå‡ ä¸ªchannelæ„é€ ä¸€ä¸ªwatchProcessorã€‚\nfunc WatchProcessor(config Config, stopChan, doneChan chan bool, errChan chan error) Processor { var wg sync.WaitGroup return \u0026watchProcessor{config, stopChan, doneChan, errChan, wg} } 2.2.1. watchProcessor.Process func (p *watchProcessor) Process() { defer close(p.doneChan) ts, err := getTemplateResources(p.config) if err != nil { log.Fatal(err.Error()) return } for _, t := range ts { t := t p.wg.Add(1) go p.monitorPrefix(t) } p.wg.Wait() } watchProcessor.Processæ–¹æ³•å®ç°äº†Processoræ¥å£ä¸­å®šä¹‰çš„æ–¹æ³•ï¼Œé€šè¿‡è§£æconfigå†…å®¹è·å–TemplateResourcesï¼Œå†éå†TemplateResourcesæ‰§è¡ŒmonitorPrefixï¼Œæœ‰å¤šå°‘ä¸ªTemplateResourceså°±è¿è¡Œå¤šå°‘ä¸ªmonitorPrefixçš„goroutineã€‚\n2.2.2. monitorPrefix func (p *watchProcessor) monitorPrefix(t *TemplateResource) { defer p.wg.Done() keys := util.AppendPrefix(t.Prefix, t.Keys) for { index, err := t.storeClient.WatchPrefix(t.Prefix, keys, t.lastIndex, p.stopChan) if err != nil { p.errChan \u003c- err // Prevent backend errors from consuming all resources. time.Sleep(time.Second * 2) continue } t.lastIndex = index if err := t.process(); err != nil { p.errChan \u003c- err } } } å…ˆå¯¹é…ç½®æ–‡ä»¶ä¸­çš„prefixå’Œkeyså‚æ•°è¿›è¡Œæ‹¼æ¥ã€‚\nkeys := util.AppendPrefix(t.Prefix, t.Keys) AppendPrefixå‡½æ•°å¦‚ä¸‹ï¼š\nfunc AppendPrefix(prefix string, keys []string) []string { s := make([]string, len(keys)) for i, k := range keys { s[i] = path.Join(prefix, k) } return s } æ¥ç€å†æ‰§è¡ŒstoreClientçš„WatchPrefixæ–¹æ³•ï¼Œå› ä¸ºstoreClientæ˜¯ä¸€ä¸ªæ¥å£ï¼Œå¯¹åº”ä¸åŒç±»å‹çš„å­˜å‚¨åç«¯ï¼ŒWatchPrefixçš„å®ç°é€»è¾‘ä¹Ÿä¸åŒï¼Œæœ¬æ–‡åˆ†æçš„å­˜å‚¨ç±»å‹ä¸ºredisã€‚\nindex, err := t.storeClient.WatchPrefix(t.Prefix, keys, t.lastIndex, p.stopChan) if err != nil { p.errChan \u003c- err // Prevent backend errors from consuming all resources. time.Sleep(time.Second * 2) continue } storeClient.WatchPrefixä¸»è¦æ˜¯è·å–lastIndexçš„å€¼ï¼Œè¿™ä¸ªå€¼åœ¨t.process()ä¸­ä½¿ç”¨ã€‚\nt.lastIndex = index if err := t.process(); err != nil { p.errChan \u003c- err } 2.3. TemplateResource.process æ— è®ºæ˜¯å¦åŠ watchå‚æ•°ï¼Œå³intervalProcessorå’ŒwatchProcessoræœ€ç»ˆéƒ½ä¼šè°ƒç”¨åˆ°TemplateResource.processè¿™ä¸ªå‡½æ•°ï¼Œè€Œè¿™ä¸ªå‡½æ•°ä¸­çš„æ ¸å¿ƒå‡½æ•°ä¸ºt.sync()ã€‚\n// process is a convenience function that wraps calls to the three main tasks // required to keep local configuration files in sync. First we gather vars // from the store, then we stage a candidate configuration file, and finally sync // things up. // It returns an error if any. func (t *TemplateResource) process() error { if err := t.setFileMode(); err != nil { return err } if err := t.setVars(); err != nil { return err } if err := t.createStageFile(); err != nil { return err } if err := t.sync(); err != nil { return err } return nil } 2.3.1. setFileMode setFileModeè®¾ç½®æ–‡ä»¶çš„æƒé™ï¼Œå¦‚æœæ²¡æœ‰åœ¨é…ç½®æ–‡ä»¶æŒ‡å®šmodeå‚æ•°åˆ™é»˜è®¤ä¸º0644ï¼Œå¦åˆ™æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„modeæ¥è®¾ç½®æ–‡ä»¶æƒé™ã€‚\n// setFileMode sets the FileMode. func (t *TemplateResource) setFileMode() error { if t.Mode == \"\" { if !util.IsFileExist(t.Dest) { t.FileMode = 0644 } else { fi, err := os.Stat(t.Dest) if err != nil { return err } t.FileMode = fi.Mode() } } else { mode, err := strconv.ParseUint(t.Mode, 0, 32) if err != nil { return err } t.FileMode = os.FileMode(mode) } return nil } 2.3.2. setVars setVarså°†åç«¯å­˜å‚¨ä¸­æœ€æ–°çš„å€¼æ‹¿å‡ºæ¥æš‚å­˜åˆ°å†…å­˜ä¸­ä¾›åç»­è¿›ç¨‹ä½¿ç”¨ã€‚å…¶ä¸­æ ¹æ®ä¸åŒçš„åç«¯ï¼ŒstoreClient.GetValuesçš„é€»è¾‘å¯èƒ½ä¸åŒï¼Œä½†é€šè¿‡æ¥å£çš„æ–¹å¼å¯ä»¥è®©ä¸åŒçš„å­˜å‚¨åç«¯å®ç°ä¸åŒçš„è·å–å€¼çš„æ–¹æ³•ã€‚\n// setVars sets the Vars for template resource. func (t *TemplateResource) setVars() error { var err error log.Debug(\"Retrieving keys from store\") log.Debug(\"Key prefix set to \" + t.Prefix) result, err := t.storeClient.GetValues(util.AppendPrefix(t.Prefix, t.Keys)) if err != nil { return err } log.Debug(\"Got the following map from store: %v\", result) t.store.Purge() for k, v := range result { t.store.Set(path.Join(\"/\", strings.TrimPrefix(k, t.Prefix)), v) } return nil } 2.3.3. createStageFile createStageFileé€šè¿‡srcçš„templateæ–‡ä»¶å’Œæœ€æ–°å†…å­˜ä¸­çš„å˜é‡æ•°æ®ç”ŸæˆStageFileï¼Œè¯¥æ–‡ä»¶åœ¨syncä¸­å’Œç›®æ ‡æ–‡ä»¶è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹æ˜¯å¦æœ‰ä¿®æ”¹ã€‚å³StageFileå®é™…ä¸Šæ˜¯æ ¹æ®åç«¯å­˜å‚¨ç”Ÿæˆçš„æœ€æ–°çš„é…ç½®æ–‡ä»¶ï¼Œå¦‚æœè¿™ä»½é…ç½®æ–‡ä»¶è·Ÿå½“å‰çš„é…ç½®æ–‡ä»¶ä¸åŒï¼Œè¡¨æ˜åç«¯å­˜å‚¨çš„æ•°æ®è¢«æ›´æ–°äº†éœ€è¦é‡æ–°ç”Ÿæˆä¸€ä»½æ–°çš„é…ç½®æ–‡ä»¶ã€‚\n// createStageFile stages the src configuration file by processing the src // template and setting the desired owner, group, and mode. It also sets the // StageFile for the template resource. // It returns an error if any. func (t *TemplateResource) createStageFile() error { log.Debug(\"Using source template \" + t.Src) if !util.IsFileExist(t.Src) { return errors.New(\"Missing template: \" + t.Src) } log.Debug(\"Compiling source template \" + t.Src) tmpl, err := template.New(filepath.Base(t.Src)).Funcs(t.funcMap).ParseFiles(t.Src) if err != nil { return fmt.Errorf(\"Unable to process template %s, %s\", t.Src, err) } // create TempFile in Dest directory to avoid cross-filesystem issues temp, err := ioutil.TempFile(filepath.Dir(t.Dest), \".\"+filepath.Base(t.Dest)) if err != nil { return err } if err = tmpl.Execute(temp, nil); err != nil { temp.Close() os.Remove(temp.Name()) return err } defer temp.Close() // Set the owner, group, and mode on the stage file now to make it easier to // compare against the destination configuration file later. os.Chmod(temp.Name(), t.FileMode) os.Chown(temp.Name(), t.Uid, t.Gid) t.StageFile = temp return nil } 2.3.4. sync if err := t.sync(); err != nil { return err } t.sync()æ˜¯æ‰§è¡Œconfdæ ¸å¿ƒåŠŸèƒ½çš„å‡½æ•°ï¼Œå°†é…ç½®æ–‡ä»¶é€šè¿‡æ¨¡æ¿çš„æ–¹å¼è‡ªåŠ¨ç”Ÿæˆï¼Œå¹¶æ‰§è¡Œæ£€æŸ¥å‘½ä»¤å’Œreloadå‘½ä»¤ã€‚è¯¥éƒ¨åˆ†é€»è¾‘åœ¨æœ¬æ–‡ç¬¬ä¸‰éƒ¨åˆ†åˆ†æã€‚\n3. sync syncé€šè¿‡æ¯”è¾ƒæºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„å·®åˆ«ï¼Œå¦‚æœä¸åŒåˆ™é‡æ–°ç”Ÿæˆæ–°çš„é…ç½®ï¼Œå½“è®¾ç½®äº†check_cmdå’Œreload_cmdçš„æ—¶å€™ï¼Œä¼šæ‰§è¡Œcheck_cmdæŒ‡å®šçš„æ£€æŸ¥å‘½ä»¤ï¼Œå¦‚æœéƒ½æ²¡æœ‰é—®é¢˜åˆ™æ‰§è¡Œreload_cmdä¸­æŒ‡å®šçš„reloadå‘½ä»¤ã€‚\n3.1. IsConfigChanged IsConfigChangedæ¯”è¾ƒæºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶æ˜¯å¦ç›¸ç­‰ï¼Œå…¶ä¸­æ¯”è¾ƒå†…å®¹åŒ…æ‹¬ï¼šUidã€Gidã€Modeã€Md5ã€‚åªè¦å…¶ä¸­ä»»æ„å€¼ä¸åŒåˆ™è®¤ä¸ºä¸¤ä¸ªæ–‡ä»¶ä¸åŒã€‚\n// IsConfigChanged reports whether src and dest config files are equal. // Two config files are equal when they have the same file contents and // Unix permissions. The owner, group, and mode must match. // It return false in other cases. func IsConfigChanged(src, dest string) (bool, error) { if !IsFileExist(dest) { return true, nil } d, err := FileStat(dest) if err != nil { return true, err } s, err := FileStat(src) if err != nil { return true, err } if d.Uid != s.Uid { log.Info(fmt.Sprintf(\"%s has UID %d should be %d\", dest, d.Uid, s.Uid)) } if d.Gid != s.Gid { log.Info(fmt.Sprintf(\"%s has GID %d should be %d\", dest, d.Gid, s.Gid)) } if d.Mode != s.Mode { log.Info(fmt.Sprintf(\"%s has mode %s should be %s\", dest, os.FileMode(d.Mode), os.FileMode(s.Mode))) } if d.Md5 != s.Md5 { log.Info(fmt.Sprintf(\"%s has md5sum %s should be %s\", dest, d.Md5, s.Md5)) } if d.Uid != s.Uid || d.Gid != s.Gid || d.Mode != s.Mode || d.Md5 != s.Md5 { return true, nil } return false, nil } å¦‚æœæ–‡ä»¶å‘ç”Ÿæ”¹å˜åˆ™æ‰§è¡Œcheck_cmdå‘½ä»¤ï¼ˆæœ‰é…ç½®çš„æƒ…å†µä¸‹ï¼‰ï¼Œé‡æ–°ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼Œå¹¶æ‰§è¡Œreload_cmdå‘½ä»¤ï¼ˆæœ‰é…ç½®çš„æƒ…å†µä¸‹ï¼‰ã€‚\nif ok { log.Info(\"Target config \" + t.Dest + \" out of sync\") if !t.syncOnly \u0026\u0026 t.CheckCmd != \"\" { if err := t.check(); err != nil { return errors.New(\"Config check failed: \" + err.Error()) } } log.Debug(\"Overwriting target config \" + t.Dest) err := os.Rename(staged, t.Dest) if err != nil { if strings.Contains(err.Error(), \"device or resource busy\") { log.Debug(\"Rename failed - target is likely a mount. Trying to write instead\") // try to open the file and write to it var contents []byte var rerr error contents, rerr = ioutil.ReadFile(staged) if rerr != nil { return rerr } err := ioutil.WriteFile(t.Dest, contents, t.FileMode) // make sure owner and group match the temp file, in case the file was created with WriteFile os.Chown(t.Dest, t.Uid, t.Gid) if err != nil { return err } } else { return err } } if !t.syncOnly \u0026\u0026 t.ReloadCmd != \"\" { if err := t.reload(); err != nil { return err } } log.Info(\"Target config \" + t.Dest + \" has been updated\") } else { log.Debug(\"Target config \" + t.Dest + \" in sync\") } 3.2. check checkæ£€æŸ¥æš‚å­˜çš„é…ç½®æ–‡ä»¶å³stageFileï¼Œè¯¥æ–‡ä»¶æ˜¯ç”±æœ€æ–°çš„åç«¯å­˜å‚¨ä¸­çš„æ•°æ®ç”Ÿæˆçš„ã€‚\nif !t.syncOnly \u0026\u0026 t.CheckCmd != \"\" { if err := t.check(); err != nil { return errors.New(\"Config check failed: \" + err.Error()) } } t.check()åªæ˜¯æ‰§è¡Œé…ç½®æ–‡ä»¶ä¸­checkcmdå‚æ•°æŒ‡å®šçš„å‘½ä»¤è€Œå·²ï¼Œæ ¹æ®æ˜¯å¦æ‰§è¡ŒæˆåŠŸæ¥è¿”å›æŠ¥é”™ã€‚å½“checkå‘½ä»¤äº§ç”Ÿé”™è¯¯çš„æ˜¯ï¼Œåˆ™ç›´æ¥returnæŠ¥é”™ï¼Œä¸å†æ‰§è¡Œé‡æ–°ç”Ÿæˆé…ç½®æ–‡ä»¶å’Œ``reload`çš„æ“ä½œäº†ã€‚\n// check executes the check command to validate the staged config file. The // command is modified so that any references to src template are substituted // with a string representing the full path of the staged file. This allows the // check to be run on the staged file before overwriting the destination config // file. // It returns nil if the check command returns 0 and there are no other errors. func (t *TemplateResource) check() error { var cmdBuffer bytes.Buffer data := make(map[string]string) data[\"src\"] = t.StageFile.Name() tmpl, err := template.New(\"checkcmd\").Parse(t.CheckCmd) if err != nil { return err } if err := tmpl.Execute(\u0026cmdBuffer, data); err != nil { return err } return runCommand(cmdBuffer.String()) } checkä¼šé€šè¿‡æ¨¡æ¿è§£æçš„æ–¹å¼è§£æå‡ºcheckcmdä¸­çš„{{.src}}éƒ¨åˆ†ï¼Œå¹¶ç”¨stageFileæ¥æ›¿ä»£ã€‚å³checkçš„å‘½ä»¤æ˜¯æ‹‰å–æœ€æ–°åç«¯å­˜å‚¨çš„æ•°æ®å½¢æˆä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆstageFileï¼‰ï¼Œå¹¶é€šè¿‡æŒ‡å®šçš„checkcmdæ¥æ£€æŸ¥æœ€æ–°çš„ä¸´æ—¶é…ç½®æ–‡ä»¶æ˜¯å¦åˆæ³•ï¼Œå¦‚æœåˆæ³•åˆ™æ›¿æ¢ä¼šæ–°çš„é…ç½®æ–‡ä»¶ï¼Œå¦åˆ™è¿”å›é”™è¯¯ã€‚\n3.3. Overwriting å°†stagedæ–‡ä»¶å‘½åä¸ºDestæ–‡ä»¶çš„åå­—ï¼Œè¯»å–stagedæ–‡ä»¶ä¸­çš„å†…å®¹å¹¶å°†å®ƒå†™å…¥åˆ°Destæ–‡ä»¶ä¸­ï¼Œè¯¥è¿‡ç¨‹å®é™…ä¸Šå°±æ˜¯é‡æ–°ç”Ÿæˆä¸€ä»½æ–°çš„é…ç½®æ–‡ä»¶ã€‚stagedæ–‡ä»¶çš„ç”Ÿæˆé€»è¾‘åœ¨å‡½æ•°createStageFileä¸­ã€‚\nlog.Debug(\"Overwriting target config \" + t.Dest) err := os.Rename(staged, t.Dest) if err != nil { if strings.Contains(err.Error(), \"device or resource busy\") { log.Debug(\"Rename failed - target is likely a mount. Trying to write instead\") // try to open the file and write to it var contents []byte var rerr error contents, rerr = ioutil.ReadFile(staged) if rerr != nil { return rerr } err := ioutil.WriteFile(t.Dest, contents, t.FileMode) // make sure owner and group match the temp file, in case the file was created with WriteFile os.Chown(t.Dest, t.Uid, t.Gid) if err != nil { return err } } else { return err } } 3.4. reload å¦‚æœæ²¡æœ‰æŒ‡å®šsyncOnlyå‚æ•°å¹¶ä¸”æŒ‡å®šäº†ReloadCmdåˆ™æ‰§è¡Œreloadæ“ä½œã€‚\nif !t.syncOnly \u0026\u0026 t.ReloadCmd != \"\" { if err := t.reload(); err != nil { return err } } å…¶ä¸­t.reload()å®ç°å¦‚ä¸‹ï¼š\n// reload executes the reload command. // It returns nil if the reload command returns 0. func (t *TemplateResource) reload() error { return runCommand(t.ReloadCmd) } t.reload()å’Œt.check()éƒ½è°ƒç”¨äº†runCommandå‡½æ•°ï¼š\n// runCommand is a shared function used by check and reload // to run the given command and log its output. // It returns nil if the given cmd returns 0. // The command can be run on unix and windows. func runCommand(cmd string) error { log.Debug(\"Running \" + cmd) var c *exec.Cmd if runtime.GOOS == \"windows\" { c = exec.Command(\"cmd\", \"/C\", cmd) } else { c = exec.Command(\"/bin/sh\", \"-c\", cmd) } output, err := c.CombinedOutput() if err != nil { log.Error(fmt.Sprintf(\"%q\", string(output))) return err } log.Debug(fmt.Sprintf(\"%q\", string(output))) return nil } 4. redisClient.WatchPrefix redisClient.WatchPrefixæ˜¯å½“ç”¨æˆ·è®¾ç½®äº†watchå‚æ•°çš„æ—¶å€™ï¼Œå¹¶ä¸”å­˜å‚¨åç«¯ä¸ºredisï¼Œåˆ™ä¼šè°ƒç”¨åˆ°redisçš„watchæœºåˆ¶ã€‚å…¶ä¸­redisClient.WatchPrefixæ˜¯rediså­˜å‚¨ç±»å‹çš„æ—¶å€™å®ç°äº†StoreClientæ¥å£çš„WatchPrefixæ–¹æ³•ã€‚\n// The StoreClient interface is implemented by objects that can retrieve // key/value pairs from a backend store. type StoreClient interface { GetValues(keys []string) (map[string]string, error) WatchPrefix(prefix string, keys []string, waitIndex uint64, stopChan chan bool) (uint64, error) } StoreClientæ˜¯å¯¹åç«¯å­˜å‚¨ç±»å‹çš„æŠ½è±¡ï¼Œå¸¸ç”¨çš„åç«¯å­˜å‚¨ç±»å‹æœ‰Etcdå’ŒRedisç­‰ï¼Œä¸åŒçš„åç«¯å­˜å‚¨ç±»å‹GetValueså’ŒWatchPrefixçš„å…·ä½“å®ç°ä¸åŒï¼Œæœ¬æ–‡ä¸»è¦åˆ†æRedisç±»å‹çš„watchæœºåˆ¶ã€‚\n4.1. WatchPrefix WatchPrefixçš„è°ƒç”¨å‡½æ•°åœ¨monitorPrefixçš„éƒ¨åˆ†ï¼Œå…·ä½“å‚è€ƒï¼š\nfunc (p *watchProcessor) monitorPrefix(t *TemplateResource) { defer p.wg.Done() keys := util.AppendPrefix(t.Prefix, t.Keys) for { index, err := t.storeClient.WatchPrefix(t.Prefix, keys, t.lastIndex, p.stopChan) if err != nil { p.errChan \u003c- err // Prevent backend errors from consuming all resources. time.Sleep(time.Second * 2) continue } t.lastIndex = index if err := t.process(); err != nil { p.errChan \u003c- err } } } redisçš„watchä¸»è¦é€šè¿‡pub-subçš„æœºåˆ¶ï¼Œå³WatchPrefixä¼šæ ¹æ®ä¼ å…¥çš„prefixèµ·ä¸€ä¸ªsubçš„ç›‘å¬æœºåˆ¶ï¼Œè€Œåœ¨å†™å…¥redisçš„æ•°æ®çš„åŒæ—¶éœ€è¦æ‰§è¡Œredisçš„publishæ“ä½œï¼Œchannelä¸ºç¬¦åˆprefixçš„å€¼ï¼Œvalueä¸ºç»™å®šå‘½ä»¤ä¹‹ä¸€ï¼Œå®é™…ä¸Šæ˜¯ç»™å®šå‘½ä»¤ä¹‹ä¸€ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå‘½ä»¤å¹¶æ²¡æœ‰å…³ç³»ï¼Œåˆ™ä¼šè§¦å‘watchæœºåˆ¶ï¼Œä»è€Œè‡ªåŠ¨æ›´æ–°é…ç½®ï¼Œç»™å®šçš„å‘½ä»¤åˆ—è¡¨å¦‚ä¸‹ï¼š\n\"del\", \"append\", \"rename_from\", \"rename_to\", \"expire\", \"set\", \"incrby\", \"incrbyfloat\", \"hset\", \"hincrby\", \"hincrbyfloat\", \"hdel\" subç›‘å¬çš„keyçš„æ ¼å¼å¦‚ä¸‹ï¼š\n__keyspace@0__:{prefix}/* å¦‚æœåªæ˜¯å†™å…¥redisæ•°æ®è€Œæ²¡æœ‰è‡ªåŠ¨æ‰§è¡Œpublishçš„æ“ä½œï¼Œå¹¶ä¸ä¼šè§¦å‘redisçš„watchæœºåˆ¶æ¥è‡ªåŠ¨æ›´æ–°é…ç½®ã€‚ä½†æ˜¯å¦‚æœä½¿ç”¨etcdï¼Œåˆ™etcdçš„watchæœºåˆ¶ï¼Œåªéœ€è¦ç”¨æˆ·å†™å…¥æˆ–æ›´æ–°æ•°æ®å°±å¯ä»¥è‡ªåŠ¨è§¦å‘æ›´æ–°é…ç½®ã€‚\nWatchPrefixæºç å¦‚ä¸‹ï¼š\nfunc (c *Client) WatchPrefix(prefix string, keys []string, waitIndex uint64, stopChan chan bool) (uint64, error) { if waitIndex == 0 { return 1, nil } if len(c.pscChan) \u003e 0 { var respChan watchResponse for len(c.pscChan) \u003e 0 { respChan = \u003c-c.pscChan } return respChan.waitIndex, respChan.err } go func() { if c.psc.Conn == nil { rClient, db, err := tryConnect(c.machines, c.password, false); if err != nil { c.psc = redis.PubSubConn{Conn: nil} c.pscChan \u003c- watchResponse{0, err} return } c.psc = redis.PubSubConn{Conn: rClient}\tgo func() { defer func() { c.psc.Close() c.psc = redis.PubSubConn{Conn: nil} }() for { switch n := c.psc.Receive().(type) { case redis.PMessage: log.Debug(fmt.Sprintf(\"Redis Message: %s %s\\n\", n.Channel, n.Data)) data := string(n.Data) commands := [12]string{\"del\", \"append\", \"rename_from\", \"rename_to\", \"expire\", \"set\", \"incrby\", \"incrbyfloat\", \"hset\", \"hincrby\", \"hincrbyfloat\", \"hdel\"} for _, command := range commands { if command == data { c.pscChan \u003c- watchResponse{1, nil} break } } case redis.Subscription: log.Debug(fmt.Sprintf(\"Redis Subscription: %s %s %d\\n\", n.Kind, n.Channel, n.Count)) if n.Count == 0 { c.pscChan \u003c- watchResponse{0, nil} return } case error: log.Debug(fmt.Sprintf(\"Redis error: %v\\n\", n)) c.pscChan \u003c- watchResponse{0, n} return } } }() c.psc.PSubscribe(\"__keyspace@\" + strconv.Itoa(db) + \"__:\" + c.transform(prefix) + \"*\") } }() select { case \u003c-stopChan: c.psc.PUnsubscribe() return waitIndex, nil case r := \u003c- c.pscChan: return r.waitIndex, r.err } } 5. æ€»ç»“ confdçš„ä½œç”¨æ˜¯é€šè¿‡å°†é…ç½®å­˜æ”¾åˆ°å­˜å‚¨åç«¯ï¼Œæ¥è‡ªåŠ¨è§¦å‘æ›´æ–°é…ç½®çš„åŠŸèƒ½ï¼Œå…¶ä¸­å¸¸ç”¨çš„åç«¯æœ‰Etcdå’ŒRedisç­‰ã€‚ ä¸åŒçš„å­˜å‚¨åç«¯ï¼Œwatchæœºåˆ¶ä¸åŒï¼Œä¾‹å¦‚Etcdåªéœ€è¦æ›´æ–°keyä¾¿å¯ä»¥è§¦å‘è‡ªåŠ¨æ›´æ–°é…ç½®çš„æ“ä½œï¼Œè€Œredisé™¤äº†æ›´æ–°keyè¿˜éœ€è¦æ‰§è¡Œpublishçš„æ“ä½œã€‚ å¯ä»¥é€šè¿‡é…ç½®check_cmdæ¥æ ¡éªŒé…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®ï¼Œå¦‚æœé…ç½®æ–‡ä»¶éæ³•åˆ™ä¸ä¼šæ‰§è¡Œè‡ªåŠ¨æ›´æ–°é…ç½®å’Œreloadçš„æ“ä½œï¼Œä½†æ˜¯å½“å­˜å‚¨åç«¯å­˜å…¥çš„éæ³•æ•°æ®ï¼Œä¼šå¯¼è‡´æ¯æ¬¡æ ¡éªŒéƒ½æ˜¯å¤±è´¥çš„ï¼Œå³ä½¿åé¢æ–°å¢çš„é…ç½®éƒ¨åˆ†æ˜¯åˆæ³•çš„ï¼Œæ‰€ä»¥éœ€è¦æœ‰æœºåˆ¶æ¥æ§åˆ¶å­˜å…¥å­˜å‚¨åç«¯çš„æ•°æ®å§‹ç»ˆæ˜¯åˆæ³•çš„ã€‚ å‚è€ƒï¼š\nhttps://github.com/kelseyhightower/confd/tree/v0.16.0 ","categories":"","description":"","excerpt":" confdçš„æºç å‚è€ƒï¼šhttps://github.com/kelseyhightower/confd\næœ¬æ–‡åˆ†æçš„confdçš„ç‰ˆæœ¬ â€¦","ref":"/golang-notes/code/confd-code-analysis/","tags":["Golang"],"title":"confdæºç åˆ†æ"},{"body":"1. ç¼–è¯‘CSI CephFS plugin CSI CephFS pluginç”¨æ¥æä¾›CephFSå­˜å‚¨å·å’ŒæŒ‚è½½å­˜å‚¨å·ï¼Œæºç å‚è€ƒï¼šhttps://github.com/ceph/ceph-csi ã€‚\n1.1. ç¼–è¯‘äºŒè¿›åˆ¶ $ make cephfsplugin 1.2. ç¼–è¯‘Dockeré•œåƒ $ make image-cephfsplugin 2. é…ç½®é¡¹ 2.1. å‘½ä»¤è¡Œå‚æ•° Option Default value Description --endpoint unix://tmp/csi.sock CSI endpoint, must be a UNIX socket --drivername csi-cephfsplugin name of the driver (Kubernetes: provisioner field in StorageClass must correspond to this value) --nodeid empty This nodeâ€™s ID --volumemounter empty default volume mounter. Available options are kernel and fuse. This is the mount method used if volume parameters donâ€™t specify otherwise. If left unspecified, the driver will first probe for ceph-fuse in systemâ€™s path and will choose Ceph kernel client if probing failed. 2.2. volumeå‚æ•° Parameter Required Description monitors yes Comma separated list of Ceph monitors (e.g. 192.168.100.1:6789,192.168.100.2:6789,192.168.100.3:6789) mounter no Mount method to be used for this volume. Available options are kernel for Ceph kernel client and fuse for Ceph FUSE driver. Defaults to â€œdefault mounterâ€, see command line arguments. provisionVolume yes Mode of operation. BOOL value. If true, a new CephFS volume will be provisioned. If false, an existing CephFS will be used. pool for provisionVolume=true Ceph pool into which the volume shall be created rootPath for provisionVolume=false Root path of an existing CephFS volume csiProvisionerSecretName, csiNodeStageSecretName for Kubernetes name of the Kubernetes Secret object containing Ceph client credentials. Both parameters should have the same value csiProvisionerSecretNamespace, csiNodeStageSecretNamespace for Kubernetes namespaces of the above Secret objects 2.3. provisionVolume 2.3.1. ç®¡ç†å‘˜å¯†é’¥è®¤è¯ å½“provisionVolume=trueæ—¶ï¼Œå¿…è¦çš„ç®¡ç†å‘˜è®¤è¯å‚æ•°å¦‚ä¸‹ï¼š\nadminID: ID of an admin client adminKey: key of the admin client 2.3.2. æ™®é€šç”¨æˆ·å¯†é’¥è®¤è¯ å½“provisionVolume=falseæ—¶ï¼Œå¿…è¦çš„ç”¨æˆ·è®¤è¯å‚æ•°å¦‚ä¸‹ï¼š\nuserID: ID of a user client userKey: key of a user client å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/ceph/ceph-csi/blob/master/docs/deploy-cephfs.md ","categories":"","description":"","excerpt":"1. ç¼–è¯‘CSI CephFS plugin CSI CephFS pluginç”¨æ¥æä¾›CephFSå­˜å‚¨å·å’ŒæŒ‚è½½å­˜å‚¨å·ï¼Œæºç å‚ â€¦","ref":"/kubernetes-notes/storage/csi/ceph/csi-cephfs-plugin/","tags":["CSI"],"title":"csi-cephfs-plugin"},{"body":" æœ¬æ–‡ä¸»è¦åˆ†æcsi-provisionerçš„æºç ï¼Œå…³äºå¼€å‘ä¸€ä¸ªDynamic Provisionerï¼Œå…·ä½“å¯å‚è€ƒnfs-client-provisionerçš„æºç åˆ†æ\n1. Dynamic Provisioner 1.1. Provisioner Interface å¼€å‘Dynamic Provisioneréœ€è¦å®ç°Provisioneræ¥å£ï¼Œè¯¥æ¥å£æœ‰ä¸¤ä¸ªæ–¹æ³•ï¼Œåˆ†åˆ«æ˜¯ï¼š\nProvisionï¼šåˆ›å»ºå­˜å‚¨èµ„æºï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªPVå¯¹è±¡ã€‚ Deleteï¼šç§»é™¤å¯¹åº”çš„å­˜å‚¨èµ„æºï¼Œä½†å¹¶æ²¡æœ‰åˆ é™¤PVå¯¹è±¡ã€‚ 1.2. å¼€å‘provisionerçš„æ­¥éª¤ å†™ä¸€ä¸ªprovisionerå®ç°Provisioneræ¥å£ï¼ˆåŒ…å«Provisionå’ŒDeleteçš„æ–¹æ³•ï¼‰ã€‚ é€šè¿‡è¯¥provisioneræ„å»ºProvisionControllerã€‚ æ‰§è¡ŒProvisionControllerçš„Runæ–¹æ³•ã€‚ 2. CSI Provisioner CSI Provisionerçš„æºç å¯å‚è€ƒï¼šhttps://github.com/kubernetes-csi/external-provisionerã€‚\n2.1. Main å‡½æ•° 2.1.1. è¯»å–ç¯å¢ƒå˜é‡ æºç å¦‚ä¸‹ï¼š\nvar ( provisioner = flag.String(\"provisioner\", \"\", \"Name of the provisioner. The provisioner will only provision volumes for claims that request a StorageClass with a provisioner field set equal to this name.\") master = flag.String(\"master\", \"\", \"Master URL to build a client config from. Either this or kubeconfig needs to be set if the provisioner is being run out of cluster.\") kubeconfig = flag.String(\"kubeconfig\", \"\", \"Absolute path to the kubeconfig file. Either this or master needs to be set if the provisioner is being run out of cluster.\") csiEndpoint = flag.String(\"csi-address\", \"/run/csi/socket\", \"The gRPC endpoint for Target CSI Volume\") connectionTimeout = flag.Duration(\"connection-timeout\", 10*time.Second, \"Timeout for waiting for CSI driver socket.\") volumeNamePrefix = flag.String(\"volume-name-prefix\", \"pvc\", \"Prefix to apply to the name of a created volume\") volumeNameUUIDLength = flag.Int(\"volume-name-uuid-length\", -1, \"Truncates generated UUID of a created volume to this length. Defaults behavior is to NOT truncate.\") showVersion = flag.Bool(\"version\", false, \"Show version.\") provisionController *controller.ProvisionController version = \"unknown\" ) func init() { var config *rest.Config var err error flag.Parse() flag.Set(\"logtostderr\", \"true\") if *showVersion { fmt.Println(os.Args[0], version) os.Exit(0) } glog.Infof(\"Version: %s\", version) ...\t}\té€šè¿‡initå‡½æ•°è§£æç›¸å…³å‚æ•°ï¼Œå…¶å®provisioneræŒ‡æ˜ä¸ºPVCæä¾›PVçš„provisionerçš„åå­—ï¼Œéœ€è¦å’ŒStorageClasså¯¹è±¡ä¸­çš„provisionerå­—æ®µä¸€è‡´ã€‚\n2.1.2. è·å–clientsetå¯¹è±¡ æºç å¦‚ä¸‹ï¼š\n// get the KUBECONFIG from env if specified (useful for local/debug cluster) kubeconfigEnv := os.Getenv(\"KUBECONFIG\") if kubeconfigEnv != \"\" { glog.Infof(\"Found KUBECONFIG environment variable set, using that..\") kubeconfig = \u0026kubeconfigEnv } if *master != \"\" || *kubeconfig != \"\" { glog.Infof(\"Either master or kubeconfig specified. building kube config from that..\") config, err = clientcmd.BuildConfigFromFlags(*master, *kubeconfig) } else { glog.Infof(\"Building kube configs for running in cluster...\") config, err = rest.InClusterConfig() } if err != nil { glog.Fatalf(\"Failed to create config: %v\", err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create client: %v\", err) } // snapclientset.NewForConfig creates a new Clientset for VolumesnapshotV1alpha1Client snapClient, err := snapclientset.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create snapshot client: %v\", err) } csiAPIClient, err := csiclientset.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create CSI API client: %v\", err) } é€šè¿‡è¯»å–å¯¹åº”çš„k8sçš„é…ç½®ï¼Œåˆ›å»ºclientsetå¯¹è±¡ï¼Œç”¨æ¥æ‰§è¡Œk8så¯¹åº”çš„APIï¼Œå…¶ä¸­ä¸»è¦åŒ…æ‹¬å¯¹PVå’ŒPVCç­‰å¯¹è±¡çš„åˆ›å»ºåˆ é™¤ç­‰æ“ä½œã€‚\n2.1.3. k8sç‰ˆæœ¬æ ¡éªŒ // The controller needs to know what the server version is because out-of-tree // provisioners aren't officially supported until 1.5 serverVersion, err := clientset.Discovery().ServerVersion() if err != nil { glog.Fatalf(\"Error getting server version: %v\", err) } è·å–äº†k8sçš„ç‰ˆæœ¬ä¿¡æ¯ï¼Œå› ä¸ºprovisionersçš„åŠŸèƒ½åœ¨k8s 1.5åŠä»¥ä¸Šç‰ˆæœ¬æ‰æ”¯æŒã€‚\n2.1.4. è¿æ¥ csi socket // Generate a unique ID for this provisioner timeStamp := time.Now().UnixNano() / int64(time.Millisecond) identity := strconv.FormatInt(timeStamp, 10) + \"-\" + strconv.Itoa(rand.Intn(10000)) + \"-\" + *provisioner // Provisioner will stay in Init until driver opens csi socket, once it's done // controller will exit this loop and proceed normally. socketDown := true grpcClient := \u0026grpc.ClientConn{} for socketDown { grpcClient, err = ctrl.Connect(*csiEndpoint, *connectionTimeout) if err == nil { socketDown = false continue } time.Sleep(10 * time.Second) } åœ¨Provisionerä¼šåœç•™åœ¨åˆå§‹åŒ–çŠ¶æ€ï¼Œç›´åˆ°csi socketè¿æ¥æˆåŠŸæ‰æ­£å¸¸è¿è¡Œã€‚å¦‚æœè¿æ¥å¤±è´¥ï¼Œä¼šæš‚åœ10ç§’åé‡è¯•ï¼Œå…¶ä¸­æ¶‰åŠä»¥ä¸‹2ä¸ªå‚æ•°ï¼š\ncsiEndpointï¼šCSI Volumeçš„gRPCåœ°å€ï¼Œé»˜è®¤é€šè¿‡ä¸º/run/csi/socketã€‚ connectionTimeoutï¼šè¿æ¥CSI driver socketçš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º10ç§’ã€‚ 2.1.5. æ„é€ csi-Provisionerå¯¹è±¡ // Create the provisioner: it implements the Provisioner interface expected by // the controller csiProvisioner := ctrl.NewCSIProvisioner(clientset, csiAPIClient, *csiEndpoint, *connectionTimeout, identity, *volumeNamePrefix, *volumeNameUUIDLength, grpcClient, snapClient) provisionController = controller.NewProvisionController( clientset, *provisioner, csiProvisioner, serverVersion.GitVersion, ) é€šè¿‡å‚æ•°clientset, csiAPIClient, csiEndpoint, connectionTimeout, identity, volumeNamePrefix, volumeNameUUIDLength, grpcClient, snapClientæ„é€ csi-Provisionerå¯¹è±¡ã€‚\né€šè¿‡csiProvisioneræ„é€ ProvisionControllerå¯¹è±¡ã€‚\n2.1.6. è¿è¡ŒProvisionController func main() { provisionController.Run(wait.NeverStop) } ProvisionControllerå®ç°äº†å…·ä½“çš„PVå’ŒPVCçš„ç›¸å…³é€»è¾‘ï¼ŒRunæ–¹æ³•ä»¥å¸¸é©»è¿›ç¨‹çš„æ–¹å¼è¿è¡Œã€‚\n2.2. Provisionå’ŒDeleteæ–¹æ³• 2.2.1. Provisionæ–¹æ³• csiProvisionerçš„Provisionæ–¹æ³•å…·ä½“æºç å‚è€ƒï¼šhttps://github.com/kubernetes-csi/external-provisioner/blob/master/pkg/controller/controller.go#L336\nProvisionæ–¹æ³•ç”¨æ¥åˆ›å»ºå­˜å‚¨èµ„æºï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªPVå¯¹è±¡ã€‚å…¶ä¸­å…¥å‚æ˜¯VolumeOptionsï¼Œç”¨æ¥æŒ‡å®šPVå¯¹è±¡çš„ç›¸å…³å±æ€§ã€‚\n1ã€æ„é€ PVç›¸å…³å±æ€§\npvName, err := makeVolumeName(p.volumeNamePrefix, fmt.Sprintf(\"%s\", options.PVC.ObjectMeta.UID), p.volumeNameUUIDLength) if err != nil { return nil, err } 2ã€æ„é€ CSIPersistentVolumeSourceç›¸å…³å±æ€§\ndriverState, err := checkDriverState(p.grpcClient, p.timeout, needSnapshotSupport) if err != nil { return nil, err } ... // Resolve controller publish, node stage, node publish secret references controllerPublishSecretRef, err := getSecretReference(controllerPublishSecretNameKey, controllerPublishSecretNamespaceKey, options.Parameters, pvName, options.PVC) if err != nil { return nil, err } nodeStageSecretRef, err := getSecretReference(nodeStageSecretNameKey, nodeStageSecretNamespaceKey, options.Parameters, pvName, options.PVC) if err != nil { return nil, err } nodePublishSecretRef, err := getSecretReference(nodePublishSecretNameKey, nodePublishSecretNamespaceKey, options.Parameters, pvName, options.PVC) if err != nil { return nil, err } ... volumeAttributes := map[string]string{provisionerIDKey: p.identity} for k, v := range rep.Volume.Attributes { volumeAttributes[k] = v } ... fsType := \"\" for k, v := range options.Parameters { switch strings.ToLower(k) { case \"fstype\": fsType = v } } if len(fsType) == 0 { fsType = defaultFSType } 3ã€åˆ›å»ºCSI CreateVolumeRequest\n// Create a CSI CreateVolumeRequest and Response req := csi.CreateVolumeRequest{ Name: pvName, Parameters: options.Parameters, VolumeCapabilities: volumeCaps, CapacityRange: \u0026csi.CapacityRange{ RequiredBytes: int64(volSizeBytes), }, } ... glog.V(5).Infof(\"CreateVolumeRequest %+v\", req) rep := \u0026csi.CreateVolumeResponse{} ... opts := wait.Backoff{Duration: backoffDuration, Factor: backoffFactor, Steps: backoffSteps} err = wait.ExponentialBackoff(opts, func() (bool, error) { ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() rep, err = p.csiClient.CreateVolume(ctx, \u0026req) if err == nil { // CreateVolume has finished successfully return true, nil } if status, ok := status.FromError(err); ok { if status.Code() == codes.DeadlineExceeded { // CreateVolume timed out, give it another chance to complete glog.Warningf(\"CreateVolume timeout: %s has expired, operation will be retried\", p.timeout.String()) return false, nil } } // CreateVolume failed , no reason to retry, bailing from ExponentialBackoff return false, err }) if err != nil { return nil, err } if rep.Volume != nil { glog.V(3).Infof(\"create volume rep: %+v\", *rep.Volume) } respCap := rep.GetVolume().GetCapacityBytes() if respCap \u003c volSizeBytes { capErr := fmt.Errorf(\"created volume capacity %v less than requested capacity %v\", respCap, volSizeBytes) delReq := \u0026csi.DeleteVolumeRequest{ VolumeId: rep.GetVolume().GetId(), } delReq.ControllerDeleteSecrets = provisionerCredentials ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() _, err := p.csiClient.DeleteVolume(ctx, delReq) if err != nil { capErr = fmt.Errorf(\"%v. Cleanup of volume %s failed, volume is orphaned: %v\", capErr, pvName, err) } return nil, capErr } Provisonæ–¹æ³•æ ¸å¿ƒåŠŸèƒ½æ˜¯è°ƒç”¨p.csiClient.CreateVolume(ctx, \u0026req)ã€‚\n4ã€æ„é€ PVå¯¹è±¡\npv := \u0026v1.PersistentVolume{ ObjectMeta: metav1.ObjectMeta{ Name: pvName, }, Spec: v1.PersistentVolumeSpec{ PersistentVolumeReclaimPolicy: options.PersistentVolumeReclaimPolicy, AccessModes: options.PVC.Spec.AccessModes, Capacity: v1.ResourceList{ v1.ResourceName(v1.ResourceStorage): bytesToGiQuantity(respCap), }, // TODO wait for CSI VolumeSource API PersistentVolumeSource: v1.PersistentVolumeSource{ CSI: \u0026v1.CSIPersistentVolumeSource{ Driver: driverState.driverName, VolumeHandle: p.volumeIdToHandle(rep.Volume.Id), FSType: fsType, VolumeAttributes: volumeAttributes, ControllerPublishSecretRef: controllerPublishSecretRef, NodeStageSecretRef: nodeStageSecretRef, NodePublishSecretRef: nodePublishSecretRef, }, }, }, } if driverState.capabilities.Has(PluginCapability_ACCESSIBILITY_CONSTRAINTS) { pv.Spec.NodeAffinity = GenerateVolumeNodeAffinity(rep.Volume.AccessibleTopology) } glog.Infof(\"successfully created PV %+v\", pv.Spec.PersistentVolumeSource) return pv, nil Provisionæ–¹æ³•åªæ˜¯é€šè¿‡VolumeOptionså‚æ•°æ¥æ„å»ºPVå¯¹è±¡ï¼Œå¹¶æ²¡æœ‰æ‰§è¡Œå…·ä½“PVçš„åˆ›å»ºæˆ–åˆ é™¤çš„æ“ä½œã€‚\nä¸åŒç±»å‹çš„Provisionerçš„ï¼Œä¸€èˆ¬æ˜¯PersistentVolumeSourceç±»å‹å’Œå‚æ•°ä¸åŒï¼Œä¾‹å¦‚csi-provisionerå¯¹åº”çš„PersistentVolumeSourceä¸ºCSIï¼Œå¹¶ä¸”éœ€è¦ä¼ å…¥CSIç›¸å…³çš„å‚æ•°ï¼š\nDriver VolumeHandle FSType VolumeAttributes ControllerPublishSecretRef NodeStageSecretRef NodePublishSecretRef 2.2.2. Deleteæ–¹æ³• csiProvisionerçš„deleteæ–¹æ³•å…·ä½“æºç å‚è€ƒï¼šhttps://github.com/kubernetes-csi/external-provisioner/blob/master/pkg/controller/controller.go#L606\nfunc (p *csiProvisioner) Delete(volume *v1.PersistentVolume) error { if volume == nil || volume.Spec.CSI == nil { return fmt.Errorf(\"invalid CSI PV\") } volumeId := p.volumeHandleToId(volume.Spec.CSI.VolumeHandle) _, err := checkDriverState(p.grpcClient, p.timeout, false) if err != nil { return err } req := csi.DeleteVolumeRequest{ VolumeId: volumeId, } // get secrets if StorageClass specifies it storageClassName := volume.Spec.StorageClassName if len(storageClassName) != 0 { if storageClass, err := p.client.StorageV1().StorageClasses().Get(storageClassName, metav1.GetOptions{}); err == nil { // Resolve provision secret credentials. // No PVC is provided when resolving provision/delete secret names, since the PVC may or may not exist at delete time. provisionerSecretRef, err := getSecretReference(provisionerSecretNameKey, provisionerSecretNamespaceKey, storageClass.Parameters, volume.Name, nil) if err != nil { return err } credentials, err := getCredentials(p.client, provisionerSecretRef) if err != nil { return err } req.ControllerDeleteSecrets = credentials } } ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() _, err = p.csiClient.DeleteVolume(ctx, \u0026req) return err } Deleteæ–¹æ³•ä¸»è¦æ˜¯è°ƒç”¨äº†p.csiClient.DeleteVolume(ctx, \u0026req)æ–¹æ³•ã€‚\n2.3. æ€»ç»“ csi provisionerå®ç°äº†Provisioneræ¥å£ï¼Œå…¶ä¸­åŒ…å«Provisonå’ŒDeleteä¸¤ä¸ªæ–¹æ³•:\nProvisionï¼šè°ƒç”¨csiClient.CreateVolumeæ–¹æ³•ï¼ŒåŒæ—¶æ„é€ å¹¶è¿”å›PVå¯¹è±¡ã€‚ Deleteï¼šè°ƒç”¨csiClient.DeleteVolumeæ–¹æ³•ã€‚ csi provisionerçš„æ ¸å¿ƒæ–¹æ³•éƒ½è°ƒç”¨äº†csi-clientç›¸å…³æ–¹æ³•ã€‚\n3. csi-client csi clientçš„ç›¸å…³ä»£ç å‚è€ƒï¼šhttps://github.com/container-storage-interface/spec/blob/master/lib/go/csi/v0/csi.pb.go\n3.1. æ„é€ csi-client 3.1.1. æ„é€ grpcClient // Provisioner will stay in Init until driver opens csi socket, once it's done // controller will exit this loop and proceed normally. socketDown := true grpcClient := \u0026grpc.ClientConn{} for socketDown { grpcClient, err = ctrl.Connect(*csiEndpoint, *connectionTimeout) if err == nil { socketDown = false continue } time.Sleep(10 * time.Second) } é€šè¿‡è¿æ¥csi socketï¼Œè¿æ¥æˆåŠŸæ‰æ„é€ å¯ç”¨çš„grpcClientã€‚\n3.1.2. æ„é€ csi-client é€šè¿‡grpcClientæ„é€ csi-clientã€‚\n// Create the provisioner: it implements the Provisioner interface expected by // the controller csiProvisioner := ctrl.NewCSIProvisioner(clientset, csiAPIClient, *csiEndpoint, *connectionTimeout, identity, *volumeNamePrefix, *volumeNameUUIDLength, grpcClient, snapClient) NewCSIProvisioner\n// NewCSIProvisioner creates new CSI provisioner func NewCSIProvisioner(client kubernetes.Interface, csiAPIClient csiclientset.Interface, csiEndpoint string, connectionTimeout time.Duration, identity string, volumeNamePrefix string, volumeNameUUIDLength int, grpcClient *grpc.ClientConn, snapshotClient snapclientset.Interface) controller.Provisioner { csiClient := csi.NewControllerClient(grpcClient) provisioner := \u0026csiProvisioner{ client: client, grpcClient: grpcClient, csiClient: csiClient, csiAPIClient: csiAPIClient, snapshotClient: snapshotClient, timeout: connectionTimeout, identity: identity, volumeNamePrefix: volumeNamePrefix, volumeNameUUIDLength: volumeNameUUIDLength, } return provisioner } NewControllerClient\ncsiClient := csi.NewControllerClient(grpcClient) ... type controllerClient struct { cc *grpc.ClientConn } func NewControllerClient(cc *grpc.ClientConn) ControllerClient { return \u0026controllerClient{cc} } 3.2. csiClient.CreateVolume csi provisonerä¸­è°ƒç”¨csiClient.CreateVolumeä»£ç å¦‚ä¸‹ï¼š\nopts := wait.Backoff{Duration: backoffDuration, Factor: backoffFactor, Steps: backoffSteps} err = wait.ExponentialBackoff(opts, func() (bool, error) { ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() rep, err = p.csiClient.CreateVolume(ctx, \u0026req) if err == nil { // CreateVolume has finished successfully return true, nil } if status, ok := status.FromError(err); ok { if status.Code() == codes.DeadlineExceeded { // CreateVolume timed out, give it another chance to complete glog.Warningf(\"CreateVolume timeout: %s has expired, operation will be retried\", p.timeout.String()) return false, nil } } // CreateVolume failed , no reason to retry, bailing from ExponentialBackoff return false, err }) CreateVolumeRequestçš„æ„é€ ï¼š\n// Create a CSI CreateVolumeRequest and Response req := csi.CreateVolumeRequest{ Name: pvName, Parameters: options.Parameters, VolumeCapabilities: volumeCaps, CapacityRange: \u0026csi.CapacityRange{ RequiredBytes: int64(volSizeBytes), }, } ... req.VolumeContentSource = volumeContentSource ... req.AccessibilityRequirements = requirements ... req.ControllerCreateSecrets = provisionerCredentials å…·ä½“çš„Createå®ç°æ–¹æ³•å¦‚ä¸‹ï¼š\nå…¶ä¸­csiClientæ˜¯ä¸ªæ¥å£ç±»å‹\nå…·ä½“ä»£ç å‚è€ƒcontrollerClient.CreateVolume\nfunc (c *controllerClient) CreateVolume(ctx context.Context, in *CreateVolumeRequest, opts ...grpc.CallOption) (*CreateVolumeResponse, error) { out := new(CreateVolumeResponse) err := grpc.Invoke(ctx, \"/csi.v0.Controller/CreateVolume\", in, out, c.cc, opts...) if err != nil { return nil, err } return out, nil } 3.3. csiClient.DeleteVolume csi provisonerä¸­è°ƒç”¨csiClient.DeleteVolumeä»£ç å¦‚ä¸‹ï¼š\nfunc (p *csiProvisioner) Delete(volume *v1.PersistentVolume) error { ... req := csi.DeleteVolumeRequest{ VolumeId: volumeId, } // get secrets if StorageClass specifies it ... ctx, cancel := context.WithTimeout(context.Background(), p.timeout) defer cancel() _, err = p.csiClient.DeleteVolume(ctx, \u0026req) return err } DeleteVolumeRequestçš„æ„é€ ï¼š\nreq := csi.DeleteVolumeRequest{ VolumeId: volumeId, } ... req.ControllerDeleteSecrets = credentials å°†æ„é€ çš„DeleteVolumeRequestä¼ ç»™DeleteVolumeæ–¹æ³•ã€‚\nå…·ä½“çš„Deleteå®ç°æ–¹æ³•å¦‚ä¸‹ï¼š\nå…·ä½“ä»£ç å‚è€ƒï¼šcontrollerClient.DeleteVolume\nfunc (c *controllerClient) DeleteVolume(ctx context.Context, in *DeleteVolumeRequest, opts ...grpc.CallOption) (*DeleteVolumeResponse, error) { out := new(DeleteVolumeResponse) err := grpc.Invoke(ctx, \"/csi.v0.Controller/DeleteVolume\", in, out, c.cc, opts...) if err != nil { return nil, err } return out, nil } 4. ProvisionController.Run è‡ªå®šä¹‰çš„provisionerå®ç°äº†Provisoneræ¥å£çš„Provisionå’ŒDeleteæ–¹æ³•ï¼Œè¿™ä¸¤ä¸ªæ–¹æ³•ä¸»è¦å¯¹åç«¯å­˜å‚¨åšåˆ›å»ºå’Œåˆ é™¤æ“ä½œï¼Œå¹¶æ²¡æœ‰å¯¹PVå¯¹è±¡è¿›è¡Œåˆ›å»ºå’Œåˆ é™¤æ“ä½œã€‚\nPVå¯¹è±¡çš„ç›¸å…³æ“ä½œå…·ä½“ç”±ProvisionControllerä¸­çš„provisionClaimOperationå’ŒdeleteVolumeOperationå…·ä½“æ‰§è¡Œï¼ŒåŒæ—¶è°ƒç”¨äº†å…·ä½“provisionerçš„Provisionå’ŒDeleteä¸¤ä¸ªæ–¹æ³•æ¥å¯¹å­˜å‚¨æ•°æ®åšå¤„ç†ã€‚\nfunc main() { provisionController.Run(wait.NeverStop) } è¿™å—ä»£ç é€»è¾‘å¯å‚è€ƒï¼šnfs-client-provisioner æºç åˆ†æ\nå‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes-csi/external-provisioner https://github.com/container-storage-interface/spec https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md https://github.com/container-storage-interface/spec/blob/master/spec.md ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦åˆ†æcsi-provisionerçš„æºç ï¼Œå…³äºå¼€å‘ä¸€ä¸ªDynamic Provisionerï¼Œå…·ä½“å¯å‚ â€¦","ref":"/kubernetes-notes/develop/csi/csi-provisioner/","tags":["æºç åˆ†æ"],"title":"csi-provisioneræºç åˆ†æ"},{"body":"Dockerå­¦ä¹ ç¬”è®° è¯¦è§ï¼šDockerå­¦ä¹ ç¬”è®°\n","categories":"","description":"","excerpt":"Dockerå­¦ä¹ ç¬”è®° è¯¦è§ï¼šDockerå­¦ä¹ ç¬”è®°\n","ref":"/kubernetes-notes/runtime/docker/docker-notes/","tags":"","title":"Dockerå­¦ä¹ ç¬”è®°"},{"body":" æœ¬æ–‡ä¸»è¦è®°å½•ä¸€äº›Golangç›¸å…³çš„èµ„æºé“¾æ¥å’Œä¹¦ç±\n1. å®˜æ–¹æ–‡æ¡£ 1.1. å®˜ç½‘ https://golang.org/\nhttps://golang.org/doc/\n1.2. åŸºç¡€ A Tour of Go\nEffective Go\nFrequently Asked Questions (FAQ)\nCodeReviewComments\nUber Go Style Guide | ä¸­æ–‡ç‰ˆ\n1.3. è¡¥å…… Diagnostics\nThe Go Wiki\nLanguage Specification\nThe Go Memory Model\nGo Playground\nawesome-go.com\n1.4. The Go Blog Share Memory by Communicating Defer, Panic, and Recover Go Slices: usage and internals Profiling Go Programs 2. ä¹¦ç± ã€ŠThe Go Programming Languageã€‹\nã€ŠMastering Goã€‹\nã€ŠGoè¯­è¨€å®æˆ˜ã€‹\n3. GitHubä¸Šä¼˜ç§€çš„Goé¡¹ç›® ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦è®°å½•ä¸€äº›Golangç›¸å…³çš„èµ„æºé“¾æ¥å’Œä¹¦ç±\n1. å®˜æ–¹æ–‡æ¡£ 1.1. å®˜ç½‘ https://golang.org/ â€¦","ref":"/golang-notes/summary/go-resource/","tags":["Golang"],"title":"Golangèµ„æº"},{"body":" æœ¬æ–‡åŸºäºã€ŠKubernetesæºç å‰–æã€‹æ•´ç†ï¼Œç»“åˆk8s v1.22.0ä»£ç åˆ†æ\næ¦‚è¿° k8så£°æ˜å¼APIçš„æ€æƒ³ï¼Œä»¥èµ„æºæè¿°å¯¹è±¡ä¸ºä¸­å¿ƒï¼Œå£°æ˜å¯¹è±¡çš„specï¼Œé€šè¿‡ç³»ç»Ÿç»´æŒstatusçŠ¶æ€å§‹ç»ˆæ˜¯ç”¨æˆ·å£°æ˜çš„èµ„æºæè¿°specï¼Œå…·ä½“å¯ä»¥å‚è€ƒç†è§£k8sèµ„æºå¯¹è±¡ã€‚k8sæ˜¯ä¸€ä¸ªå®Œå…¨ä»¥èµ„æºä¸ºä¸­å¿ƒçš„ç³»ç»Ÿï¼Œæœ¬è´¨æ˜¯ä¸€ä¸ªèµ„æºæ§åˆ¶ç³»ç»Ÿ--æ³¨å†Œã€ç®¡ç†ã€è°ƒåº¦èµ„æºå¹¶ç»´æŠ¤èµ„æºçš„çŠ¶æ€ã€‚k8så°†èµ„æºè¿›è¡Œå†æ¬¡åˆ†ç»„å’Œç‰ˆæœ¬åŒ–ï¼Œå½¢æˆGroup(èµ„æºç»„)ã€Version(èµ„æºç‰ˆæœ¬)ã€Resource(èµ„æº)ã€å…¶ä¸­kindè¡¨ç¤ºèµ„æºç§ç±»ã€‘\nèµ„æºæè¿°å¯¹è±¡ èµ„æºæè¿°å¯¹è±¡å¿…é¡»çš„å£°æ˜å…ƒç´ å¦‚ä¸‹ï¼š\napiVersionï¼škubernetes APIçš„ç‰ˆæœ¬ï¼ŒåŒ…å«\u003cgroup\u003e/\u003cversion\u003e kindï¼škuberneteså¯¹è±¡çš„ç±»å‹ metadataï¼šå”¯ä¸€æ ‡è¯†è¯¥å¯¹è±¡çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬nameï¼ŒUIDï¼Œå¯é€‰çš„namespace specï¼šæ ‡è¯†å¯¹è±¡çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸åŒå¯¹è±¡çš„specçš„æ ¼å¼ä¸åŒï¼Œå¯ä»¥åµŒå¥—å…¶ä»–å¯¹è±¡çš„å­—æ®µã€‚ è¯´æ˜ï¼š\n\u003cgroup\u003e/\u003cversion\u003e/\u003cresource\u003eè¡¨ç¤ºå”¯ä¸€ä¸€ç§èµ„æºå¯¹è±¡ï¼Œä¾‹å¦‚apps/v1/deploymentã€‚\næ¯ç§èµ„æºå¯¹è±¡éƒ½æœ‰å¢åˆ æ”¹æŸ¥çš„æ“ä½œæ–¹æ³•ã€‚å…·ä½“å¯åŒ…å«8ç±»æ“ä½œæ¥å£ï¼š\nå¢ï¼šcreate\nåˆ ï¼šdeleteï¼Œdeletecollection\næ”¹ï¼šupdateï¼Œpatch\næŸ¥ï¼šgetï¼Œlistï¼Œwatch\né™¤äº†å†…ç½®çš„k8sèµ„æºå¯¹è±¡ï¼Œå¯ç”¨CRDè‡ªå®šä¹‰èµ„æºå¯¹è±¡ï¼Œåœ¨k8sç³»ç»Ÿä¸­ä½¿ç”¨ã€‚ ç¤ºä¾‹ï¼š\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 èµ„æºå¯¹è±¡åˆ†ç±»åŠä»£ç ç›®å½• èµ„æºå¯¹è±¡é€šç”¨ç»“æ„ ","categories":"","description":"","excerpt":" æœ¬æ–‡åŸºäºã€ŠKubernetesæºç å‰–æã€‹æ•´ç†ï¼Œç»“åˆk8s v1.22.0ä»£ç åˆ†æ\næ¦‚è¿° k8så£°æ˜å¼APIçš„æ€æƒ³ï¼Œä»¥èµ„æºæè¿°å¯¹è±¡ä¸ºä¸­å¿ƒï¼Œå£° â€¦","ref":"/k8s-source-code-analysis/summary/resource-object/","tags":["æºç åˆ†æ"],"title":"k8sæ ¸å¿ƒæ•°æ®ç»“æ„åˆ†æ"},{"body":"1. k8sçŸ¥è¯†ä½“ç³» ä»¥ä¸‹æ•´ç†äº†k8sæ¶‰åŠçš„ç›¸å…³çŸ¥è¯†ä½“ç³»ã€‚\næ€ç»´å¯¼å›¾ï¼šhttps://www.processon.com/view/link/5d7f7d08e4b03461a3a937e2\n2. k8sé‡ç‚¹å¼€æºé¡¹ç›® TODO\n","categories":"","description":"","excerpt":"1. k8sçŸ¥è¯†ä½“ç³» ä»¥ä¸‹æ•´ç†äº†k8sæ¶‰åŠçš„ç›¸å…³çŸ¥è¯†ä½“ç³»ã€‚\næ€ç»´å¯¼ â€¦","ref":"/kubernetes-notes/paas/k8s/","tags":["Kubernetes"],"title":"k8sçŸ¥è¯†ä½“ç³»"},{"body":"é—®é¢˜æè¿° write /proc/self/attr/keycreate: permission denied å…·ä½“æŠ¥é”™ï¼š\nkuberuntime_manager.go:758] createPodSandbox for pod \"ecc-hostpath-provisioner-8jbhf_kube-system(b8050fd3-4ffe-11eb-a82e-c6090b53405b)\" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"ecc-hostpath-provisioner-8jbhf\": Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"write /proc/self/attr/keycreate: permission denied\\\"\": unknown è§£å†³åŠæ³• SELINUXæœªè®¾ç½®æˆdisabled\n# å°†SELINUXè®¾ç½®æˆdisabled setenforce 0 # ä¸´æ—¶ç”Ÿæ•ˆ # æ°¸ä¹…ç”Ÿæ•ˆï¼Œä½†éœ€é‡å¯ï¼Œé…åˆä¸Šè¿°å‘½ä»¤å¯ä»¥ä¸ç”¨ç«‹å³é‡å¯ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config # æŸ¥çœ‹SELinuxçŠ¶æ€ $ /usr/sbin/sestatus -v SELinux status: disabled $ getenforce Disabled ","categories":"","description":"","excerpt":"é—®é¢˜æè¿° write /proc/self/attr/keycreate: permission denied å…·ä½“æŠ¥é”™ï¼š â€¦","ref":"/kubernetes-notes/trouble-shooting/node/keycreate-permission-denied/","tags":["é—®é¢˜æ’æŸ¥"],"title":"keycreate permission denied"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/kubeedge/","tags":"","title":"KubeEdge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/architecture/","tags":"","title":"kubernetesæ¶æ„"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/","tags":"","title":"Kuberneteså­¦ä¹ ç¬”è®°"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/","tags":"","title":"Kubernetesæºç åˆ†æ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/file/","tags":"","title":"Linux æ–‡ä»¶ç³»ç»Ÿ"},{"body":"1. é€šè¿‡å®¹å™¨çš„æ–¹å¼éƒ¨ç½² mkdir -p ~/data/mysql docker run --name my-mysql -v ~/data/mysql:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 2. é€šè¿‡k8s deploymentçš„æ–¹å¼éƒ¨ç½² éƒ¨ç½²çš„æ³¨æ„äº‹é¡¹ï¼š\nmysqlæ•°æ®çš„æŒä¹…åŒ–ï¼šæ•°æ®å·æ˜ å°„å’Œå›ºå®šå®¿ä¸»æœº\nè®¾ç½®è´¦å·å¯†ç åŠæœåŠ¡ç«¯å£\napiVersion: apps/v1 kind: Deployment metadata: name: mysql namespace: default spec: replicas: 1 strategy: type: Recreate # å…ˆåˆ é™¤æ—§ Podï¼Œå†å¯åŠ¨æ–° Pod selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:8.0 env: - name: MYSQL_ROOT_PASSWORD value: \"rootpass\" - name: MYSQL_USER value: \"user\" - name: MYSQL_PASSWORD value: \"userpass\" ports: - containerPort: 3306 # MySQL é»˜è®¤ç«¯å£ hostPort: 13306 # ç›´æ¥æ˜ å°„åˆ°å®¿ä¸»æœºç«¯å£ volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql # MySQL æ•°æ®ç›®å½• volumes: - name: mysql-storage hostPath: path: /data/mysql # å®¿ä¸»æœºä¸Šçš„ MySQL å­˜å‚¨ç›®å½• type: DirectoryOrCreate nodeName: mysql-node ","categories":"","description":"","excerpt":"1. é€šè¿‡å®¹å™¨çš„æ–¹å¼éƒ¨ç½² mkdir -p ~/data/mysql docker run --name my-mysql -v â€¦","ref":"/linux-notes/mysql/deploy-mysql/","tags":["Mysql"],"title":"MysqlæœåŠ¡éƒ¨ç½²"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækube-apiserverä¸­cmdéƒ¨åˆ†çš„ä»£ç ï¼Œå³NewAPIServerCommandç›¸å…³çš„ä»£ç ã€‚æ›´å¤šå…·ä½“çš„é€»è¾‘å¾…åç»­æ–‡ç« åˆ†æã€‚\nkube-apiserverçš„cmdéƒ¨åˆ†ç›®å½•ä»£ç ç»“æ„å¦‚ä¸‹ï¼š\nkube-apiserver â”œâ”€â”€ apiserver.go # kube-apiserverçš„mainå…¥å£ â””â”€â”€ app â”œâ”€â”€ aggregator.go â”œâ”€â”€ apiextensions.go â”œâ”€â”€ options # åˆå§‹åŒ–kube-apiserverä½¿ç”¨åˆ°çš„option â”‚Â â”œâ”€â”€ options.go # åŒ…æ‹¬ï¼šNewServerRunOptionsã€Flagsç­‰ â”‚Â â”œâ”€â”€ options_test.go â”‚Â â””â”€â”€ validation.go â”œâ”€â”€ server.go # åŒ…æ‹¬ï¼šNewAPIServerCommandã€Runã€CreateServerChainã€Completeç­‰ 1. Main æ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kube-apiserver/apiserver.go\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewAPIServerCommand(server.SetupSignalHandler()) // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"error: %v\\n\", err) os.Exit(1) } } æ ¸å¿ƒä»£ç ï¼š\n// åˆå§‹åŒ–APIServerCommand command := app.NewAPIServerCommand(server.SetupSignalHandler()) // æ‰§è¡ŒExecute err := command.Execute() 2. NewAPIServerCommand æ­¤éƒ¨åˆ†çš„ä»£ç ä½äº/cmd/kube-apiserver/app/server.go\nNewAPIServerCommandå³Cobraå‘½ä»¤è¡Œæ¡†æ¶çš„æ„é€ å‡½æ•°ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼š\næ„é€ option æ·»åŠ Flags æ‰§è¡ŒRunå‡½æ•° å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kube-apiserver/app/server.go\n// NewAPIServerCommand creates a *cobra.Command object with default parameters func NewAPIServerCommand(stopCh \u003c-chan struct{}) *cobra.Command { s := options.NewServerRunOptions() cmd := \u0026cobra.Command{ Use: \"kube-apiserver\", Long: `The Kubernetes API server validates and configures data for the api objects which include pods, services, replicationcontrollers, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.`, RunE: func(cmd *cobra.Command, args []string) error { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) // set default options completedOptions, err := Complete(s) if err != nil { return err } // validate options if errs := completedOptions.Validate(); len(errs) != 0 { return utilerrors.NewAggregate(errs) } return Run(completedOptions, stopCh) }, } fs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := apiserverflag.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStderr(), namedFlagSets, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStdout(), namedFlagSets, cols) }) return cmd } æ ¸å¿ƒä»£ç ï¼š\n// æ„é€ option s := options.NewServerRunOptions() // æ·»åŠ flags fs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } // set default options completedOptions, err := Complete(s) // Run Run(completedOptions, stopCh) 3. NewServerRunOptions NewServerRunOptionsåŸºäºé»˜è®¤çš„å‚æ•°æ„é€ ServerRunOptionsç»“æ„ä½“ã€‚ServerRunOptionsæ˜¯apiserverè¿è¡Œçš„é…ç½®ä¿¡æ¯ã€‚å…·ä½“ç»“æ„ä½“å®šä¹‰å¦‚ä¸‹ã€‚\n3.1. ServerRunOptions å…¶ä¸­ä¸»è¦çš„é…ç½®å¦‚ä¸‹ï¼š\nGenericServerRunOptions Etcd SecureServing KubeletConfig ... // ServerRunOptions runs a kubernetes api server. type ServerRunOptions struct { GenericServerRunOptions *genericoptions.ServerRunOptions Etcd *genericoptions.EtcdOptions SecureServing *genericoptions.SecureServingOptionsWithLoopback InsecureServing *genericoptions.DeprecatedInsecureServingOptionsWithLoopback Audit *genericoptions.AuditOptions Features *genericoptions.FeatureOptions Admission *kubeoptions.AdmissionOptions Authentication *kubeoptions.BuiltInAuthenticationOptions Authorization *kubeoptions.BuiltInAuthorizationOptions CloudProvider *kubeoptions.CloudProviderOptions StorageSerialization *kubeoptions.StorageSerializationOptions APIEnablement *genericoptions.APIEnablementOptions AllowPrivileged bool EnableLogsHandler bool EventTTL time.Duration KubeletConfig kubeletclient.KubeletClientConfig KubernetesServiceNodePort int MaxConnectionBytesPerSec int64 ServiceClusterIPRange net.IPNet // TODO: make this a list ServiceNodePortRange utilnet.PortRange SSHKeyfile string SSHUser string ProxyClientCertFile string ProxyClientKeyFile string EnableAggregatorRouting bool MasterCount int EndpointReconcilerType string ServiceAccountSigningKeyFile string } 3.2. NewServerRunOptions NewServerRunOptionsåˆå§‹åŒ–é…ç½®ç»“æ„ä½“ã€‚\n// NewServerRunOptions creates a new ServerRunOptions object with default parameters func NewServerRunOptions() *ServerRunOptions { s := ServerRunOptions{ GenericServerRunOptions: genericoptions.NewServerRunOptions(), Etcd: genericoptions.NewEtcdOptions(storagebackend.NewDefaultConfig(kubeoptions.DefaultEtcdPathPrefix, nil)), SecureServing: kubeoptions.NewSecureServingOptions(), InsecureServing: kubeoptions.NewInsecureServingOptions(), Audit: genericoptions.NewAuditOptions(), Features: genericoptions.NewFeatureOptions(), Admission: kubeoptions.NewAdmissionOptions(), Authentication: kubeoptions.NewBuiltInAuthenticationOptions().WithAll(), Authorization: kubeoptions.NewBuiltInAuthorizationOptions(), CloudProvider: kubeoptions.NewCloudProviderOptions(), StorageSerialization: kubeoptions.NewStorageSerializationOptions(), APIEnablement: genericoptions.NewAPIEnablementOptions(), EnableLogsHandler: true, EventTTL: 1 * time.Hour, MasterCount: 1, EndpointReconcilerType: string(reconcilers.LeaseEndpointReconcilerType), KubeletConfig: kubeletclient.KubeletClientConfig{ Port: ports.KubeletPort, ReadOnlyPort: ports.KubeletReadOnlyPort, PreferredAddressTypes: []string{ // --override-hostname string(api.NodeHostName), // internal, preferring DNS if reported string(api.NodeInternalDNS), string(api.NodeInternalIP), // external, preferring DNS if reported string(api.NodeExternalDNS), string(api.NodeExternalIP), }, EnableHttps: true, HTTPTimeout: time.Duration(5) * time.Second, }, ServiceNodePortRange: kubeoptions.DefaultServiceNodePortRange, } s.ServiceClusterIPRange = kubeoptions.DefaultServiceIPCIDR // Overwrite the default for storage data format. s.Etcd.DefaultStorageMediaType = \"application/vnd.kubernetes.protobuf\" return \u0026s } 3.3. Complete å½“kube-apiserverçš„flagsè¢«è§£æåï¼Œè°ƒç”¨Completeå®Œæˆé»˜è®¤é…ç½®ã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kube-apiserver/app/server.go\n// Should be called after kube-apiserver flags parsed. func Complete(s *options.ServerRunOptions) (completedServerRunOptions, error) { var options completedServerRunOptions // set defaults if err := s.GenericServerRunOptions.DefaultAdvertiseAddress(s.SecureServing.SecureServingOptions); err != nil { return options, err } if err := kubeoptions.DefaultAdvertiseAddress(s.GenericServerRunOptions, s.InsecureServing.DeprecatedInsecureServingOptions); err != nil { return options, err } serviceIPRange, apiServerServiceIP, err := master.DefaultServiceIPRange(s.ServiceClusterIPRange) if err != nil { return options, fmt.Errorf(\"error determining service IP ranges: %v\", err) } s.ServiceClusterIPRange = serviceIPRange if err := s.SecureServing.MaybeDefaultWithSelfSignedCerts(s.GenericServerRunOptions.AdvertiseAddress.String(), []string{\"kubernetes.default.svc\", \"kubernetes.default\", \"kubernetes\"}, []net.IP{apiServerServiceIP}); err != nil { return options, fmt.Errorf(\"error creating self-signed certificates: %v\", err) } if len(s.GenericServerRunOptions.ExternalHost) == 0 { if len(s.GenericServerRunOptions.AdvertiseAddress) \u003e 0 { s.GenericServerRunOptions.ExternalHost = s.GenericServerRunOptions.AdvertiseAddress.String() } else { if hostname, err := os.Hostname(); err == nil { s.GenericServerRunOptions.ExternalHost = hostname } else { return options, fmt.Errorf(\"error finding host name: %v\", err) } } glog.Infof(\"external host was not specified, using %v\", s.GenericServerRunOptions.ExternalHost) } s.Authentication.ApplyAuthorization(s.Authorization) // Use (ServiceAccountSigningKeyFile != \"\") as a proxy to the user enabling // TokenRequest functionality. This defaulting was convenient, but messed up // a lot of people when they rotated their serving cert with no idea it was // connected to their service account keys. We are taking this oppurtunity to // remove this problematic defaulting. if s.ServiceAccountSigningKeyFile == \"\" { // Default to the private server key for service account token signing if len(s.Authentication.ServiceAccounts.KeyFiles) == 0 \u0026\u0026 s.SecureServing.ServerCert.CertKey.KeyFile != \"\" { if kubeauthenticator.IsValidServiceAccountKeyFile(s.SecureServing.ServerCert.CertKey.KeyFile) { s.Authentication.ServiceAccounts.KeyFiles = []string{s.SecureServing.ServerCert.CertKey.KeyFile} } else { glog.Warning(\"No TLS key provided, service account token authentication disabled\") } } } if s.Etcd.StorageConfig.DeserializationCacheSize == 0 { // When size of cache is not explicitly set, estimate its size based on // target memory usage. glog.V(2).Infof(\"Initializing deserialization cache size based on %dMB limit\", s.GenericServerRunOptions.TargetRAMMB) // This is the heuristics that from memory capacity is trying to infer // the maximum number of nodes in the cluster and set cache sizes based // on that value. // From our documentation, we officially recommend 120GB machines for // 2000 nodes, and we scale from that point. Thus we assume ~60MB of // capacity per node. // TODO: We may consider deciding that some percentage of memory will // be used for the deserialization cache and divide it by the max object // size to compute its size. We may even go further and measure // collective sizes of the objects in the cache. clusterSize := s.GenericServerRunOptions.TargetRAMMB / 60 s.Etcd.StorageConfig.DeserializationCacheSize = 25 * clusterSize if s.Etcd.StorageConfig.DeserializationCacheSize \u003c 1000 { s.Etcd.StorageConfig.DeserializationCacheSize = 1000 } } if s.Etcd.EnableWatchCache { glog.V(2).Infof(\"Initializing cache sizes based on %dMB limit\", s.GenericServerRunOptions.TargetRAMMB) sizes := cachesize.NewHeuristicWatchCacheSizes(s.GenericServerRunOptions.TargetRAMMB) if userSpecified, err := serveroptions.ParseWatchCacheSizes(s.Etcd.WatchCacheSizes); err == nil { for resource, size := range userSpecified { sizes[resource] = size } } s.Etcd.WatchCacheSizes, err = serveroptions.WriteWatchCacheSizes(sizes) if err != nil { return options, err } } // TODO: remove when we stop supporting the legacy group version. if s.APIEnablement.RuntimeConfig != nil { for key, value := range s.APIEnablement.RuntimeConfig { if key == \"v1\" || strings.HasPrefix(key, \"v1/\") || key == \"api/v1\" || strings.HasPrefix(key, \"api/v1/\") { delete(s.APIEnablement.RuntimeConfig, key) s.APIEnablement.RuntimeConfig[\"/v1\"] = value } if key == \"api/legacy\" { delete(s.APIEnablement.RuntimeConfig, key) } } } options.ServerRunOptions = s return options, nil } 3. AddFlagSet AddFlagSetä¸»è¦çš„ä½œç”¨æ˜¯é€šè¿‡å¤–éƒ¨ä¼ å…¥çš„flagçš„å…·ä½“å€¼ï¼Œè§£æçš„æ—¶å€™ä¼ é€’ç»™optionçš„ç»“æ„ä½“ï¼Œæœ€ç»ˆç»™apiserverä½¿ç”¨ã€‚\nå…¶ä¸­NewAPIServerCommandå…³äºAddFlagSetçš„ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š\nfs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } 3.1. Flags Flagså®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kube-apiserver/app/options/options.go\n// Flags returns flags for a specific APIServer by section name func (s *ServerRunOptions) Flags() (fss apiserverflag.NamedFlagSets) { // Add the generic flags. s.GenericServerRunOptions.AddUniversalFlags(fss.FlagSet(\"generic\")) s.Etcd.AddFlags(fss.FlagSet(\"etcd\")) s.SecureServing.AddFlags(fss.FlagSet(\"secure serving\")) s.InsecureServing.AddFlags(fss.FlagSet(\"insecure serving\")) s.InsecureServing.AddUnqualifiedFlags(fss.FlagSet(\"insecure serving\")) // TODO: remove it until kops stops using `--address` s.Audit.AddFlags(fss.FlagSet(\"auditing\")) s.Features.AddFlags(fss.FlagSet(\"features\")) s.Authentication.AddFlags(fss.FlagSet(\"authentication\")) s.Authorization.AddFlags(fss.FlagSet(\"authorization\")) s.CloudProvider.AddFlags(fss.FlagSet(\"cloud provider\")) s.StorageSerialization.AddFlags(fss.FlagSet(\"storage\")) s.APIEnablement.AddFlags(fss.FlagSet(\"api enablement\")) s.Admission.AddFlags(fss.FlagSet(\"admission\")) // Note: the weird \"\"+ in below lines seems to be the only way to get gofmt to // arrange these text blocks sensibly. Grrr. fs := fss.FlagSet(\"misc\") fs.DurationVar(\u0026s.EventTTL, \"event-ttl\", s.EventTTL, \"Amount of time to retain events.\") fs.BoolVar(\u0026s.AllowPrivileged, \"allow-privileged\", s.AllowPrivileged, \"If true, allow privileged containers. [default=false]\") fs.BoolVar(\u0026s.EnableLogsHandler, \"enable-logs-handler\", s.EnableLogsHandler, \"If true, install a /logs handler for the apiserver logs.\") // Deprecated in release 1.9 fs.StringVar(\u0026s.SSHUser, \"ssh-user\", s.SSHUser, \"If non-empty, use secure SSH proxy to the nodes, using this user name\") fs.MarkDeprecated(\"ssh-user\", \"This flag will be removed in a future version.\") // Deprecated in release 1.9 fs.StringVar(\u0026s.SSHKeyfile, \"ssh-keyfile\", s.SSHKeyfile, \"If non-empty, use secure SSH proxy to the nodes, using this user keyfile\") fs.MarkDeprecated(\"ssh-keyfile\", \"This flag will be removed in a future version.\") fs.Int64Var(\u0026s.MaxConnectionBytesPerSec, \"max-connection-bytes-per-sec\", s.MaxConnectionBytesPerSec, \"\"+ \"If non-zero, throttle each user connection to this number of bytes/sec. \"+ \"Currently only applies to long-running requests.\") fs.IntVar(\u0026s.MasterCount, \"apiserver-count\", s.MasterCount, \"The number of apiservers running in the cluster, must be a positive number. (In use when --endpoint-reconciler-type=master-count is enabled.)\") fs.StringVar(\u0026s.EndpointReconcilerType, \"endpoint-reconciler-type\", string(s.EndpointReconcilerType), \"Use an endpoint reconciler (\"+strings.Join(reconcilers.AllTypes.Names(), \", \")+\")\") // See #14282 for details on how to test/try this option out. // TODO: remove this comment once this option is tested in CI. fs.IntVar(\u0026s.KubernetesServiceNodePort, \"kubernetes-service-node-port\", s.KubernetesServiceNodePort, \"\"+ \"If non-zero, the Kubernetes master service (which apiserver creates/maintains) will be \"+ \"of type NodePort, using this as the value of the port. If zero, the Kubernetes master \"+ \"service will be of type ClusterIP.\") fs.IPNetVar(\u0026s.ServiceClusterIPRange, \"service-cluster-ip-range\", s.ServiceClusterIPRange, \"\"+ \"A CIDR notation IP range from which to assign service cluster IPs. This must not \"+ \"overlap with any IP ranges assigned to nodes for pods.\") fs.Var(\u0026s.ServiceNodePortRange, \"service-node-port-range\", \"\"+ \"A port range to reserve for services with NodePort visibility. \"+ \"Example: '30000-32767'. Inclusive at both ends of the range.\") // Kubelet related flags: fs.BoolVar(\u0026s.KubeletConfig.EnableHttps, \"kubelet-https\", s.KubeletConfig.EnableHttps, \"Use https for kubelet connections.\") fs.StringSliceVar(\u0026s.KubeletConfig.PreferredAddressTypes, \"kubelet-preferred-address-types\", s.KubeletConfig.PreferredAddressTypes, \"List of the preferred NodeAddressTypes to use for kubelet connections.\") fs.UintVar(\u0026s.KubeletConfig.Port, \"kubelet-port\", s.KubeletConfig.Port, \"DEPRECATED: kubelet port.\") fs.MarkDeprecated(\"kubelet-port\", \"kubelet-port is deprecated and will be removed.\") fs.UintVar(\u0026s.KubeletConfig.ReadOnlyPort, \"kubelet-read-only-port\", s.KubeletConfig.ReadOnlyPort, \"DEPRECATED: kubelet port.\") fs.DurationVar(\u0026s.KubeletConfig.HTTPTimeout, \"kubelet-timeout\", s.KubeletConfig.HTTPTimeout, \"Timeout for kubelet operations.\") fs.StringVar(\u0026s.KubeletConfig.CertFile, \"kubelet-client-certificate\", s.KubeletConfig.CertFile, \"Path to a client cert file for TLS.\") fs.StringVar(\u0026s.KubeletConfig.KeyFile, \"kubelet-client-key\", s.KubeletConfig.KeyFile, \"Path to a client key file for TLS.\") fs.StringVar(\u0026s.KubeletConfig.CAFile, \"kubelet-certificate-authority\", s.KubeletConfig.CAFile, \"Path to a cert file for the certificate authority.\") // TODO: delete this flag in 1.13 repair := false fs.BoolVar(\u0026repair, \"repair-malformed-updates\", false, \"deprecated\") fs.MarkDeprecated(\"repair-malformed-updates\", \"This flag will be removed in a future version\") fs.StringVar(\u0026s.ProxyClientCertFile, \"proxy-client-cert-file\", s.ProxyClientCertFile, \"\"+ \"Client certificate used to prove the identity of the aggregator or kube-apiserver \"+ \"when it must call out during a request. This includes proxying requests to a user \"+ \"api-server and calling out to webhook admission plugins. It is expected that this \"+ \"cert includes a signature from the CA in the --requestheader-client-ca-file flag. \"+ \"That CA is published in the 'extension-apiserver-authentication' configmap in \"+ \"the kube-system namespace. Components receiving calls from kube-aggregator should \"+ \"use that CA to perform their half of the mutual TLS verification.\") fs.StringVar(\u0026s.ProxyClientKeyFile, \"proxy-client-key-file\", s.ProxyClientKeyFile, \"\"+ \"Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver \"+ \"when it must call out during a request. This includes proxying requests to a user \"+ \"api-server and calling out to webhook admission plugins.\") fs.BoolVar(\u0026s.EnableAggregatorRouting, \"enable-aggregator-routing\", s.EnableAggregatorRouting, \"Turns on aggregator routing requests to endpoints IP rather than cluster IP.\") fs.StringVar(\u0026s.ServiceAccountSigningKeyFile, \"service-account-signing-key-file\", s.ServiceAccountSigningKeyFile, \"\"+ \"Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.)\") return fss } 4. Run Runä»¥å¸¸é©»çš„æ–¹å¼è¿è¡Œapiserverã€‚\nä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\næ„é€ ä¸€ä¸ªèšåˆçš„serverç»“æ„ä½“ã€‚ æ‰§è¡ŒPrepareRunã€‚ æœ€ç»ˆæ‰§è¡ŒRunã€‚ æ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kube-apiserver/app/server.go\n// Run runs the specified APIServer. This should never exit. func Run(completeOptions completedServerRunOptions, stopCh \u003c-chan struct{}) error { // To help debugging, immediately log version glog.Infof(\"Version: %+v\", version.Get()) server, err := CreateServerChain(completeOptions, stopCh) if err != nil { return err } return server.PrepareRun().Run(stopCh) } 4.1. CreateServerChain æ„é€ èšåˆçš„Serverã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\né¦–å…ˆç”Ÿæˆconfigå¯¹è±¡ï¼ŒåŒ…æ‹¬kubeAPIServerConfigã€apiExtensionsConfigã€‚ å†é€šè¿‡configç”Ÿæˆserverå¯¹è±¡ï¼ŒåŒ…æ‹¬apiExtensionsServerã€kubeAPIServerã€‚ æ‰§è¡ŒapiExtensionsServerã€kubeAPIServerçš„PrepareRunéƒ¨åˆ†ã€‚ ç”Ÿæˆèšåˆçš„configå¯¹è±¡aggregatorConfigã€‚ åŸºäºaggregatorConfigã€kubeAPIServerã€apiExtensionsServerç”Ÿæˆèšåˆçš„serveraggregatorServerã€‚ æ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kube-apiserver/app/server.go\n// CreateServerChain creates the apiservers connected via delegation. func CreateServerChain(completedOptions completedServerRunOptions, stopCh \u003c-chan struct{}) (*genericapiserver.GenericAPIServer, error) { nodeTunneler, proxyTransport, err := CreateNodeDialer(completedOptions) if err != nil { return nil, err } kubeAPIServerConfig, insecureServingInfo, serviceResolver, pluginInitializer, admissionPostStartHook, err := CreateKubeAPIServerConfig(completedOptions, nodeTunneler, proxyTransport) if err != nil { return nil, err } // If additional API servers are added, they should be gated. apiExtensionsConfig, err := createAPIExtensionsConfig(*kubeAPIServerConfig.GenericConfig, kubeAPIServerConfig.ExtraConfig.VersionedInformers, pluginInitializer, completedOptions.ServerRunOptions, completedOptions.MasterCount) if err != nil { return nil, err } apiExtensionsServer, err := createAPIExtensionsServer(apiExtensionsConfig, genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, admissionPostStartHook) if err != nil { return nil, err } // otherwise go down the normal path of standing the aggregator up in front of the API server // this wires up openapi kubeAPIServer.GenericAPIServer.PrepareRun() // This will wire up openapi for extension api server apiExtensionsServer.GenericAPIServer.PrepareRun() // aggregator comes last in the chain aggregatorConfig, err := createAggregatorConfig(*kubeAPIServerConfig.GenericConfig, completedOptions.ServerRunOptions, kubeAPIServerConfig.ExtraConfig.VersionedInformers, serviceResolver, proxyTransport, pluginInitializer) if err != nil { return nil, err } aggregatorServer, err := createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers) if err != nil { // we don't need special handling for innerStopCh because the aggregator server doesn't create any go routines return nil, err } if insecureServingInfo != nil { insecureHandlerChain := kubeserver.BuildInsecureHandlerChain(aggregatorServer.GenericAPIServer.UnprotectedHandler(), kubeAPIServerConfig.GenericConfig) if err := insecureServingInfo.Serve(insecureHandlerChain, kubeAPIServerConfig.GenericConfig.RequestTimeout, stopCh); err != nil { return nil, err } } return aggregatorServer.GenericAPIServer, nil } 4.2. PrepareRun PrepareRunä¸»è¦æ‰§è¡Œä¸€äº›APIå®‰è£…æ“ä½œã€‚\næ­¤éƒ¨åˆ†çš„ä»£ç ä½äºvendor/k8s.io/apiserver/pkg/server/genericapiserver.go\n// PrepareRun does post API installation setup steps. func (s *GenericAPIServer) PrepareRun() preparedGenericAPIServer { if s.swaggerConfig != nil { routes.Swagger{Config: s.swaggerConfig}.Install(s.Handler.GoRestfulContainer) } if s.openAPIConfig != nil { routes.OpenAPI{ Config: s.openAPIConfig, }.Install(s.Handler.GoRestfulContainer, s.Handler.NonGoRestfulMux) } s.installHealthz() // Register audit backend preShutdownHook. if s.AuditBackend != nil { s.AddPreShutdownHook(\"audit-backend\", func() error { s.AuditBackend.Shutdown() return nil }) } return preparedGenericAPIServer{s} } 4.3. preparedGenericAPIServer.Run preparedGenericAPIServer.Runè¿è¡Œä¸€ä¸ªå®‰å…¨çš„http serverã€‚å…·ä½“çš„å®ç°é€»è¾‘å¾…åç»­æ–‡ç« åˆ†æã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äºvendor/k8s.io/apiserver/pkg/server/genericapiserver.go\n// Run spawns the secure http server. It only returns if stopCh is closed // or the secure port cannot be listened on initially. func (s preparedGenericAPIServer) Run(stopCh \u003c-chan struct{}) error { err := s.NonBlockingRun(stopCh) if err != nil { return err } \u003c-stopCh err = s.RunPreShutdownHooks() if err != nil { return err } // Wait for all requests to finish, which are bounded by the RequestTimeout variable. s.HandlerChainWaitGroup.Wait() return nil } æ ¸å¿ƒå‡½æ•°ï¼š\nerr := s.NonBlockingRun(stopCh) preparedGenericAPIServer.Runä¸»è¦æ˜¯è°ƒç”¨NonBlockingRunå‡½æ•°ï¼Œæœ€ç»ˆè¿è¡Œä¸€ä¸ªhttp serverã€‚è¯¥éƒ¨åˆ†é€»è¾‘å¾…åç»­æ–‡ç« åˆ†æã€‚\n5. æ€»ç»“ NewAPIServerCommandé‡‡ç”¨äº†Cobraå‘½ä»¤è¡Œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ä¸»è¦åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\næ„é€ optionå‚æ•°ï¼Œæä¾›ç»™æ‰§è¡Œä¸»ä½“(ä¾‹å¦‚ æœ¬æ–‡çš„server)ä½œä¸ºé…ç½®å‚æ•°ä½¿ç”¨ã€‚ æ·»åŠ Flagsï¼Œä¸»è¦ç”¨æ¥é€šè¿‡ä¼ å…¥çš„flagså‚æ•°æœ€ç»ˆè§£ææˆoptionä¸­ä½¿ç”¨çš„ç»“æ„ä½“å±æ€§ã€‚ æ‰§è¡ŒRunå‡½æ•°ï¼Œæ‰§è¡Œä¸»ä½“çš„è¿è¡Œé€»è¾‘éƒ¨åˆ†ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰ã€‚ å…¶ä¸­Runå‡½æ•°çš„ä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\næ„é€ ä¸€ä¸ªèšåˆçš„serverç»“æ„ä½“ã€‚ æ‰§è¡ŒPrepareRunã€‚ æœ€ç»ˆæ‰§è¡ŒpreparedGenericAPIServer.Runã€‚ preparedGenericAPIServer.Runä¸»è¦æ˜¯è°ƒç”¨NonBlockingRunå‡½æ•°ï¼Œæœ€ç»ˆè¿è¡Œä¸€ä¸ªhttp serverã€‚NonBlockingRunçš„å…·ä½“é€»è¾‘å¾…åç»­æ–‡ç« å†å•ç‹¬åˆ†æã€‚\nå‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-apiserver https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/server.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/aggregator.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/options/options.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækube-apiserverä¸­cmdéƒ¨åˆ†çš„ä»£ç ï¼Œ â€¦","ref":"/k8s-source-code-analysis/kube-apiserver/newapiservercommand/","tags":["æºç åˆ†æ"],"title":"kube-apiserveræºç åˆ†æï¼ˆä¸€ï¼‰ä¹‹ NewAPIServerCommand"},{"body":"1. ç®€ä»‹ NVIDIA device plugin é€šè¿‡k8s daemonsetçš„æ–¹å¼éƒ¨ç½²åˆ°æ¯ä¸ªk8sçš„nodeèŠ‚ç‚¹ä¸Šï¼Œå®ç°äº†Kubernetes device pluginçš„æ¥å£ã€‚\næä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š\næš´éœ²æ¯ä¸ªèŠ‚ç‚¹çš„GPUæ•°é‡ç»™é›†ç¾¤ è·Ÿè¸ªGPUçš„å¥åº·æƒ…å†µ ä½¿åœ¨k8sçš„èŠ‚ç‚¹å¯ä»¥è¿è¡ŒGPUå®¹å™¨ 2. è¦æ±‚ NVIDIA drivers ~= 384.81 nvidia-docker version \u003e 2.0 (see how to install and it's prerequisites) docker configured with nvidia as the default runtime. Kubernetes version \u003e= 1.10 3. ä½¿ç”¨ 3.1. å®‰è£…NVIDIA driverså’Œnvidia-docker æä¾›GPUèŠ‚ç‚¹çš„æœºå™¨ï¼Œå‡†å¤‡å·¥ä½œå¦‚ä¸‹\nå®‰è£…NVIDIA drivers ~= 384.81 å®‰è£…nvidia-docker version \u003e 2.0 3.2. é…ç½®docker runtime é…ç½®nvidia runtimeä½œä¸ºGPUèŠ‚ç‚¹çš„é»˜è®¤runtimeã€‚\nä¿®æ”¹æ–‡ä»¶/etc/docker/daemon.jsonï¼Œå¢åŠ ä»¥ä¸‹runtimeå†…å®¹ã€‚\n{ \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } 3.3. éƒ¨ç½²nvidia-device-plugin $ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml nvidia-device-pluginçš„daemonset yamlæ–‡ä»¶å¦‚ä¸‹ï¼š\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-device-plugin-daemonset namespace: kube-system spec: selector: matchLabels: name: nvidia-device-plugin-ds updateStrategy: type: RollingUpdate template: metadata: # This annotation is deprecated. Kept here for backward compatibility # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: name: nvidia-device-plugin-ds spec: tolerations: # This toleration is deprecated. Kept here for backward compatibility # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ - key: CriticalAddonsOnly operator: Exists - key: nvidia.com/gpu operator: Exists effect: NoSchedule # Mark this pod as a critical add-on; when enabled, the critical add-on # scheduler reserves resources for critical add-on pods so that they can # be rescheduled after a failure. # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: \"system-node-critical\" containers: - image: nvidia/k8s-device-plugin:1.0.0-beta4 name: nvidia-device-plugin-ctr securityContext: allowPrivilegeEscalation: false capabilities: drop: [\"ALL\"] volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins 3.4. è¿è¡ŒGPUä»»åŠ¡ åˆ›å»ºä¸€ä¸ªGPUçš„podï¼Œpodçš„èµ„æºç±»å‹æŒ‡å®šä¸ºnvidia.com/gpuã€‚\napiVersion: v1 kind: Pod metadata: name: gpu-pod spec: containers: - name: cuda-container image: nvidia/cuda:9.0-devel resources: limits: nvidia.com/gpu: 2 # requesting 2 GPUs - name: digits-container image: nvidia/digits:6.0 resources: limits: nvidia.com/gpu: 2 # requesting 2 GPUs 4. æ„å»ºå’Œè¿è¡Œnvidia-device-plugin 4.1. dockeræ–¹å¼ 4.1.1. ç¼–è¯‘ ç›´æ¥æ‹‰å–dockerhubçš„é•œåƒ $ docker pull nvidia/k8s-device-plugin:1.0.0-beta4 æ‹‰å–ä»£ç æ„å»ºé•œåƒ $ docker build -t nvidia/k8s-device-plugin:1.0.0-beta4 https://github.com/NVIDIA/k8s-device-plugin.git#1.0.0-beta4 ä¿®æ”¹nvidia-device-pluginåæ„å»ºé•œåƒ $ git clone https://github.com/NVIDIA/k8s-device-plugin.git \u0026\u0026 cd k8s-device-plugin $ git checkout 1.0.0-beta4 $ docker build -t nvidia/k8s-device-plugin:1.0.0-beta4 . 4.1.2. è¿è¡Œ dockeræœ¬åœ°è¿è¡Œ $ docker run --security-opt=no-new-privileges --cap-drop=ALL --network=none -it -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins nvidia/k8s-device-plugin:1.0.0-beta4 daemonsetè¿è¡Œ $ kubectl create -f nvidia-device-plugin.yml 4.2. édockeræ–¹å¼ 4.2.1. ç¼–è¯‘ $ C_INCLUDE_PATH=/usr/local/cuda/include LIBRARY_PATH=/usr/local/cuda/lib64 go build 4.2.2. æœ¬åœ°è¿è¡Œ $ ./k8s-device-plugin å‚è€ƒï¼š\nhttps://github.com/NVIDIA/k8s-device-plugin k8s-device-plugin gpu-support ","categories":"","description":"","excerpt":"1. ç®€ä»‹ NVIDIA device plugin é€šè¿‡k8s daemonsetçš„æ–¹å¼éƒ¨ç½²åˆ°æ¯ä¸ªk8sçš„nodeèŠ‚ç‚¹ä¸Šï¼Œå®ç° â€¦","ref":"/kubernetes-notes/runtime/gpu/nvidia-device-plugin/","tags":["Kubernetes"],"title":"nvidia-device-pluginä»‹ç»"},{"body":"Podåˆ›å»ºåŸºæœ¬æµç¨‹å›¾ Podåˆ›å»ºå®Œæ•´æµç¨‹å›¾ å›¾ç‰‡æ¥æºï¼šhttps://fuckcloudnative.io/posts/what-happens-when-k8s/\nå‚è€ƒ:\nhttps://fuckcloudnative.io/posts/what-happens-when-k8s/ ","categories":"","description":"","excerpt":"Podåˆ›å»ºåŸºæœ¬æµç¨‹å›¾ Podåˆ›å»ºå®Œæ•´æµç¨‹å›¾ å›¾ç‰‡æ¥ â€¦","ref":"/kubernetes-notes/principle/flow/pod-flow/","tags":["Kubernetes"],"title":"Podåˆ›å»ºæµç¨‹"},{"body":"1. shellç®€ä»‹ shellæ˜¯ç”¨æˆ·å’ŒLinuxå†…æ ¸ä¹‹é—´çš„ä¸€å±‚ä»£ç†ï¼Œè§£é‡Šç”¨æˆ·è¾“å…¥çš„å‘½ä»¤ï¼Œä¼ é€’ç»™å†…æ ¸ã€‚\nshellæ˜¯ä¸€ç§è„šæœ¬è¯­è¨€ï¼ˆè§£é‡Šæ€§è¯­è¨€ï¼‰ã€‚\nShellæ—¢æ˜¯ä¸€ç§å‘½ä»¤è¯­è¨€ï¼Œåˆæ˜¯ä¸€ç§ç¨‹åºè®¾è®¡è¯­è¨€ã€‚ä½œä¸ºå‘½ä»¤è¯­è¨€ï¼Œå®ƒäº¤äº’å¼åœ°è§£é‡Šå’Œæ‰§è¡Œç”¨æˆ·è¾“å…¥çš„å‘½ä»¤ï¼›ä½œä¸ºç¨‹åºè®¾è®¡è¯­è¨€ï¼Œå®ƒå®šä¹‰äº†å„ç§å˜é‡å’Œå‚æ•°ï¼Œå¹¶æä¾›äº†è®¸å¤šåœ¨é«˜çº§è¯­è¨€ä¸­æ‰å…·æœ‰çš„æ§åˆ¶ç»“æ„ï¼ŒåŒ…æ‹¬å¾ªç¯å’Œåˆ†æ”¯ã€‚\nShellæœ‰ä¸¤ç§æ‰§è¡Œå‘½ä»¤çš„æ–¹å¼ï¼š\näº¤äº’å¼ï¼ˆInteractiveï¼‰ï¼šè§£é‡Šæ‰§è¡Œç”¨æˆ·çš„å‘½ä»¤ï¼Œç”¨æˆ·è¾“å…¥ä¸€æ¡å‘½ä»¤ï¼ŒShellå°±è§£é‡Šæ‰§è¡Œä¸€æ¡ã€‚ æ‰¹å¤„ç†ï¼ˆBatchï¼‰ï¼šç”¨æˆ·äº‹å…ˆå†™ä¸€ä¸ªShellè„šæœ¬(Script)ï¼Œå…¶ä¸­æœ‰å¾ˆå¤šæ¡å‘½ä»¤ï¼Œè®©Shellä¸€æ¬¡æŠŠè¿™äº›å‘½ä»¤æ‰§è¡Œå®Œï¼Œè€Œä¸å¿…ä¸€æ¡ä¸€æ¡åœ°æ•²å‘½ä»¤ã€‚ Shellè„šæœ¬å’Œç¼–ç¨‹è¯­è¨€å¾ˆç›¸ä¼¼ï¼Œä¹Ÿæœ‰å˜é‡å’Œæµç¨‹æ§åˆ¶è¯­å¥ï¼Œä½†Shellè„šæœ¬æ˜¯è§£é‡Šæ‰§è¡Œçš„ï¼Œä¸éœ€è¦ç¼–è¯‘ï¼ŒShellç¨‹åºä»è„šæœ¬ä¸­ä¸€è¡Œä¸€è¡Œè¯»å–å¹¶æ‰§è¡Œè¿™äº›å‘½ä»¤ï¼Œç›¸å½“äºä¸€ä¸ªç”¨æˆ·æŠŠè„šæœ¬ä¸­çš„å‘½ä»¤ä¸€è¡Œä¸€è¡Œæ•²åˆ°Shellæç¤ºç¬¦ä¸‹æ‰§è¡Œã€‚\nUnix/Linuxä¸Šå¸¸è§çš„Shellè„šæœ¬è§£é‡Šå™¨æœ‰bashã€shã€cshã€kshç­‰ï¼Œä¹ æƒ¯ä¸ŠæŠŠå®ƒä»¬ç§°ä½œä¸€ç§Shellã€‚\nç¨‹åºè®¾è®¡è¯­è¨€å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šç¼–è¯‘å‹è¯­è¨€å’Œè§£é‡Šå‹è¯­è¨€ã€‚\n1.1. ç¼–è¯‘å‹è¯­è¨€ å¾ˆå¤šä¼ ç»Ÿçš„ç¨‹åºè®¾è®¡è¯­è¨€ï¼Œä¾‹å¦‚Fortranã€Adaã€Pascalã€Cã€C++å’ŒJavaï¼Œéƒ½æ˜¯ç¼–è¯‘å‹è¯­è¨€ã€‚è¿™ç±»è¯­è¨€éœ€è¦é¢„å…ˆå°†æˆ‘ä»¬å†™å¥½çš„æºä»£ç (source code)è½¬æ¢æˆç›®æ ‡ä»£ç (object code)ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä½œâ€œç¼–è¯‘â€ã€‚è¿è¡Œç¨‹åºæ—¶ï¼Œç›´æ¥è¯»å–ç›®æ ‡ä»£ç (object code)ã€‚ç”±äºç¼–è¯‘åçš„ç›®æ ‡ä»£ç (object code)éå¸¸æ¥è¿‘è®¡ç®—æœºåº•å±‚ï¼Œå› æ­¤æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œè¿™æ˜¯ç¼–è¯‘å‹è¯­è¨€çš„ä¼˜ç‚¹ã€‚\nç¼–è¯‘å‹è¯­è¨€å¤šåŠè¿ä½œäºåº•å±‚ï¼Œæ‰€å¤„ç†çš„æ˜¯å­—èŠ‚ã€æ•´æ•°ã€æµ®ç‚¹æ•°æˆ–æ˜¯å…¶ä»–æœºå™¨å±‚çº§çš„å¯¹è±¡ï¼Œå¾€å¾€å®ç°ä¸€ä¸ªç®€å•çš„åŠŸèƒ½éœ€è¦å¤§é‡å¤æ‚çš„ä»£ç ã€‚\n1.2. è§£é‡Šå‹è¯­è¨€ æœ‰çš„è¯­è¨€ï¼ˆä¾‹å¦‚ï¼š Shellã€JavaScriptã€Pythonã€PHPç­‰ï¼‰éœ€è¦ä¸€è¾¹æ‰§è¡Œä¸€è¾¹ç¿»è¯‘ï¼Œä¸ä¼šäº§ç”Ÿä»»ä½•å¯æ‰§è¡Œæ–‡ä»¶ï¼Œç”¨æˆ·éœ€è¦æ‹¿åˆ°æºç æ‰èƒ½è¿è¡Œç¨‹åºã€‚ç¨‹åºè¿è¡Œåä¼šå³æ—¶ç¿»è¯‘ï¼Œç¿»è¯‘ä¸€éƒ¨åˆ†æ‰§è¡Œä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸ç”¨ç­‰æ‰€æœ‰ä»£ç ç¿»è¯‘å®Œã€‚\nè¿™ä¸ªè¿‡ç¨‹å«è§£é‡Šï¼Œè¿™ç±»è¯­è¨€å«è§£é‡Šå‹è¯­è¨€æˆ–è„šæœ¬è¯­è¨€ï¼Œå®Œæˆè§£é‡Šè¿‡ç¨‹çš„è½¯ä»¶å«è§£é‡Šå™¨ã€‚\nè§£é‡Šå‹è¯­è¨€ä¹Ÿè¢«ç§°ä½œâ€œè„šæœ¬è¯­è¨€â€ã€‚å› ä¸ºæ¯æ¬¡æ‰§è¡Œç¨‹åºéƒ½å¤šäº†ç¼–è¯‘çš„è¿‡ç¨‹ï¼Œå› æ­¤æ•ˆç‡æœ‰æ‰€ä¸‹é™ã€‚\nä½¿ç”¨è„šæœ¬ç¼–ç¨‹è¯­è¨€çš„å¥½å¤„æ˜¯ï¼Œå®ƒä»¬å¤šåŠè¿è¡Œåœ¨æ¯”ç¼–è¯‘å‹è¯­è¨€è¿˜é«˜çš„å±‚çº§ï¼Œèƒ½å¤Ÿè½»æ˜“å¤„ç†æ–‡ä»¶ä¸ç›®å½•ä¹‹ç±»çš„å¯¹è±¡ï¼›ç¼ºç‚¹æ˜¯å®ƒä»¬çš„æ•ˆç‡é€šå¸¸ä¸å¦‚ç¼–è¯‘å‹è¯­è¨€ã€‚\nè„šæœ¬ç¼–ç¨‹è¯­è¨€çš„ä¾‹å­æœ‰awkã€Perlã€Pythonã€Rubyä¸Shellã€‚\n2. å¸¸è§çš„Shellç±»å‹ shellç±»å‹ è¯´æ˜ sh sh æ˜¯ UNIX ä¸Šçš„æ ‡å‡† shellï¼Œå¾ˆå¤š UNIX ç‰ˆæœ¬éƒ½é…æœ‰ shã€‚ bash bash shell æ˜¯ Linux çš„é»˜è®¤ shellï¼Œbash å…¼å®¹ shï¼Œä½†å¹¶ä¸å®Œå…¨ä¸€è‡´ã€‚ csh è¯­æ³•æœ‰ç‚¹ç±»ä¼¼Cè¯­è¨€ã€‚ ... 2.1. æŸ¥çœ‹shell $ cat /etc/shells /bin/sh /bin/bash /sbin/nologin /usr/bin/sh /usr/bin/bash /usr/sbin/nologin /bin/tcsh /bin/csh æŸ¥çœ‹é»˜è®¤shell\n$ echo $SHELL /bin/bash sh ä¸€èˆ¬è¢« bash ä»£æ›¿ï¼Œ/bin/shå¾€å¾€æ˜¯æŒ‡å‘/bin/bashçš„ç¬¦å·é“¾æ¥ã€‚\n$ ls -l /bin/sh lrwxrwxrwx. 1 root root 4 Mar 8 2018 /bin/sh -\u003e bash 3. ä½¿ç”¨shellåœºæ™¯ ä¹‹æ‰€ä»¥è¦ä½¿ç”¨Shellè„šæœ¬æ˜¯åŸºäºï¼š\nç®€å•æ€§ï¼šShellæ˜¯ä¸€ä¸ªé«˜çº§è¯­è¨€ï¼›é€šè¿‡å®ƒï¼Œä½ å¯ä»¥ç®€æ´åœ°è¡¨è¾¾å¤æ‚çš„æ“ä½œã€‚ å¯ç§»æ¤æ€§ï¼šä½¿ç”¨POSIXæ‰€å®šä¹‰çš„åŠŸèƒ½ï¼Œå¯ä»¥åšåˆ°è„šæœ¬æ— é¡»ä¿®æ”¹å°±å¯åœ¨ä¸åŒçš„ç³»ç»Ÿä¸Šæ‰§è¡Œã€‚ å¼€å‘å®¹æ˜“ï¼šå¯ä»¥åœ¨çŸ­æ—¶é—´å†…å®Œæˆä¸€ä¸ªåŠŸèƒ½å¼ºå¤§åˆå¦¤ç”¨çš„è„šæœ¬ã€‚ ä½†æ˜¯ï¼Œè€ƒè™‘åˆ°Shellè„šæœ¬çš„å‘½ä»¤é™åˆ¶å’Œæ•ˆç‡é—®é¢˜ï¼Œä¸‹åˆ—æƒ…å†µä¸€èˆ¬ä¸ä½¿ç”¨Shellï¼š\nèµ„æºå¯†é›†å‹çš„ä»»åŠ¡ï¼Œå°¤å…¶åœ¨éœ€è¦è€ƒè™‘æ•ˆç‡æ—¶ï¼ˆæ¯”å¦‚ï¼Œæ’åºï¼Œhashç­‰ç­‰ï¼‰ã€‚ éœ€è¦å¤„ç†å¤§ä»»åŠ¡çš„æ•°å­¦æ“ä½œï¼Œå°¤å…¶æ˜¯æµ®ç‚¹è¿ç®—ï¼Œç²¾ç¡®è¿ç®—ï¼Œæˆ–è€…å¤æ‚çš„ç®—æœ¯è¿ç®—ï¼ˆè¿™ç§æƒ…å†µä¸€èˆ¬ä½¿ç”¨C++æˆ–FORTRAN æ¥å¤„ç†ï¼‰ã€‚ æœ‰è·¨å¹³å°ï¼ˆæ“ä½œç³»ç»Ÿï¼‰ç§»æ¤éœ€æ±‚ï¼ˆä¸€èˆ¬ä½¿ç”¨C æˆ–Javaï¼‰ã€‚ å¤æ‚çš„åº”ç”¨ï¼Œåœ¨å¿…é¡»ä½¿ç”¨ç»“æ„åŒ–ç¼–ç¨‹çš„æ—¶å€™ï¼ˆéœ€è¦å˜é‡çš„ç±»å‹æ£€æŸ¥ï¼Œå‡½æ•°åŸå‹ï¼Œç­‰ç­‰ï¼‰ã€‚ å¯¹äºå½±å“ç³»ç»Ÿå…¨å±€æ€§çš„å…³é”®ä»»åŠ¡åº”ç”¨ã€‚ å¯¹äºå®‰å…¨æœ‰å¾ˆé«˜è¦æ±‚çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ä½ éœ€è¦ä¸€ä¸ªå¥å£®çš„ç³»ç»Ÿæ¥é˜²æ­¢å…¥ä¾µã€ç ´è§£ã€æ¶æ„ç ´åç­‰ç­‰ã€‚ é¡¹ç›®ç”±è¿ä¸²çš„ä¾èµ–çš„å„ä¸ªéƒ¨åˆ†ç»„æˆã€‚ éœ€è¦å¤§è§„æ¨¡çš„æ–‡ä»¶æ“ä½œã€‚ éœ€è¦å¤šç»´æ•°ç»„çš„æ”¯æŒã€‚ éœ€è¦æ•°æ®ç»“æ„çš„æ”¯æŒï¼Œæ¯”å¦‚é“¾è¡¨æˆ–æ•°ç­‰æ•°æ®ç»“æ„ã€‚ éœ€è¦äº§ç”Ÿæˆ–æ“ä½œå›¾å½¢åŒ–ç•Œé¢ GUIã€‚ éœ€è¦ç›´æ¥æ“ä½œç³»ç»Ÿç¡¬ä»¶ã€‚ éœ€è¦ I/O æˆ–socket æ¥å£ã€‚ éœ€è¦ä½¿ç”¨åº“æˆ–è€…é—ç•™ä¸‹æ¥çš„è€ä»£ç çš„æ¥å£ã€‚ ç§äººçš„ã€é—­æºçš„åº”ç”¨ï¼ˆshell è„šæœ¬æŠŠä»£ç å°±æ”¾åœ¨æ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œå…¨ä¸–ç•Œéƒ½èƒ½çœ‹åˆ°ï¼‰ã€‚ 4. shellè„šæœ¬ æ‰“å¼€æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œæ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œæ‰©å±•åä¸ºshï¼ˆshä»£è¡¨shellï¼‰ï¼Œæ‰©å±•åå¹¶ä¸å½±å“è„šæœ¬æ‰§è¡Œï¼Œè§åçŸ¥æ„å°±å¥½ï¼Œå¦‚æœä½ ç”¨phpå†™shell è„šæœ¬ï¼Œæ‰©å±•åå°±ç”¨phpå¥½äº†ã€‚\nè¾“å…¥ä¸€äº›ä»£ç ï¼š\n#!/bin/bash echo \"Hello World !\" â€œ#!â€ æ˜¯ä¸€ä¸ªçº¦å®šçš„æ ‡è®°ï¼Œå®ƒå‘Šè¯‰ç³»ç»Ÿè¿™ä¸ªè„šæœ¬éœ€è¦ä»€ä¹ˆè§£é‡Šå™¨æ¥æ‰§è¡Œï¼Œå³ä½¿ç”¨å“ªä¸€ç§Shellã€‚echoå‘½ä»¤ç”¨äºå‘çª—å£è¾“å‡ºæ–‡æœ¬ã€‚\n5. è¿è¡Œshell è¿è¡ŒShellè„šæœ¬æœ‰ä¸¤ç§æ–¹æ³•ã€‚\n5.1. ä½œä¸ºå¯æ‰§è¡Œç¨‹åº å°†ä¸Šé¢çš„ä»£ç ä¿å­˜ä¸ºtest.shï¼Œå¹¶ cd åˆ°ç›¸åº”ç›®å½•ï¼š\nchmod +x ./test.sh #ä½¿è„šæœ¬å…·æœ‰æ‰§è¡Œæƒé™ ./test.sh #æ‰§è¡Œè„šæœ¬ æ³¨æ„ï¼Œä¸€å®šè¦å†™æˆ./test.shï¼Œè€Œä¸æ˜¯test.shã€‚è¿è¡Œå…¶å®ƒäºŒè¿›åˆ¶çš„ç¨‹åºä¹Ÿä¸€æ ·ï¼Œç›´æ¥å†™test.shï¼Œlinuxç³»ç»Ÿä¼šå»PATHé‡Œå¯»æ‰¾æœ‰æ²¡æœ‰å«test.shçš„ï¼Œè€Œåªæœ‰/bin, /sbin, /usr/binï¼Œ/usr/sbinç­‰åœ¨PATHé‡Œï¼Œä½ çš„å½“å‰ç›®å½•é€šå¸¸ä¸åœ¨PATHé‡Œï¼Œæ‰€ä»¥å†™æˆtest.shæ˜¯ä¼šæ‰¾ä¸åˆ°å‘½ä»¤çš„ï¼Œè¦ç”¨./test.shå‘Šè¯‰ç³»ç»Ÿè¯´ï¼Œå°±åœ¨å½“å‰ç›®å½•æ‰¾ã€‚\n5.2. ä½œä¸ºè§£é‡Šå™¨å‚æ•° è¿™ç§è¿è¡Œæ–¹å¼æ˜¯ï¼Œç›´æ¥è¿è¡Œè§£é‡Šå™¨\n# ä½¿ç”¨ sh è§£é‡Šå™¨ sh test.sh # ä½¿ç”¨ bash è§£é‡Šå™¨ bash test.sh è¿™ç§æ–¹å¼è¿è¡Œçš„è„šæœ¬ï¼Œä¸éœ€è¦åœ¨ç¬¬ä¸€è¡ŒæŒ‡å®šè§£é‡Šå™¨ä¿¡æ¯ï¼Œå†™äº†ä¹Ÿæ²¡ç”¨ã€‚\nå‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. shellç®€ä»‹ shellæ˜¯ç”¨æˆ·å’ŒLinuxå†…æ ¸ä¹‹é—´çš„ä¸€å±‚ä»£ç†ï¼Œè§£é‡Šç”¨æˆ·è¾“å…¥çš„å‘½ä»¤ï¼Œä¼ é€’ç»™å†…æ ¸ã€‚\nshellæ˜¯ä¸€ç§è„šæœ¬è¯­è¨€ï¼ˆè§£é‡Šæ€§è¯­ â€¦","ref":"/linux-notes/shell/shell-introduction/","tags":["Shell"],"title":"Shellç®€ä»‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/summary/","tags":"","title":"summary"},{"body":"1. volumeæ¦‚è¿° å®¹å™¨ä¸Šçš„æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸåŒå®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸä¸€è‡´ï¼Œå³å®¹å™¨æŒ‚æ‰ä¹‹åï¼Œå®¹å™¨å°†ä¼šä»¥æœ€åˆé•œåƒä¸­çš„æ–‡ä»¶ç³»ç»Ÿå†…å®¹å¯åŠ¨ï¼Œä¹‹å‰å®¹å™¨è¿è¡Œæ—¶äº§ç”Ÿçš„æ–‡ä»¶å°†ä¼šä¸¢å¤±ã€‚ Podçš„volumeçš„ç”Ÿå‘½å‘¨æœŸåŒPodçš„ç”Ÿå‘½å‘¨æœŸä¸€è‡´ï¼Œå½“Podè¢«åˆ é™¤çš„æ—¶å€™ï¼Œå¯¹åº”çš„volumeæ‰ä¼šè¢«åˆ é™¤ã€‚å³Podä¸­çš„å®¹å™¨é‡å¯æ—¶ï¼Œä¹‹å‰çš„æ–‡ä»¶ä»å¯ä»¥ä¿å­˜ã€‚ å®¹å™¨ä¸­çš„è¿›ç¨‹çœ‹åˆ°çš„æ˜¯ç”±å…¶ Docker é•œåƒå’Œå·ç»„æˆçš„æ–‡ä»¶ç³»ç»Ÿè§†å›¾ã€‚\nPod volumeçš„ä½¿ç”¨æ–¹å¼\nPod ä¸­çš„æ¯ä¸ªå®¹å™¨éƒ½å¿…é¡»ç‹¬ç«‹æŒ‡å®šæ¯ä¸ªå·çš„æŒ‚è½½ä½ç½®ï¼Œéœ€è¦ç»™Podé…ç½®volumeç›¸å…³å‚æ•°ã€‚\nPodçš„volumeå…³é”®å­—æ®µå¦‚ä¸‹ï¼š\nspec.volumesï¼šæä¾›æ€æ ·çš„æ•°æ®å· spec.containers.volumeMountsï¼šæŒ‚è½½åˆ°å®¹å™¨çš„ä»€ä¹ˆè·¯å¾„ 2. volumeç±»å‹ 2.1. emptyDir 1ã€ç‰¹ç‚¹\nä¼šåˆ›å»ºemptyDirå¯¹åº”çš„ç›®å½•ï¼Œé»˜è®¤ä¸ºç©ºï¼ˆå¦‚æœè¯¥ç›®å½•åŸæ¥æœ‰æ–‡ä»¶ä¹Ÿä¼šè¢«é‡ç½®ä¸ºç©ºï¼‰ Podä¸­çš„ä¸åŒå®¹å™¨å¯ä»¥åœ¨ç›®å½•ä¸­è¯»å†™ç›¸åŒæ–‡ä»¶ï¼ˆå³Podä¸­çš„ä¸åŒå®¹å™¨å¯ä»¥é€šè¿‡è¯¥æ–¹å¼æ¥å…±äº«æ–‡ä»¶ï¼‰ å½“Podè¢«åˆ é™¤ï¼ŒemptyDirÂ ä¸­çš„æ•°æ®å°†è¢«æ°¸ä¹…åˆ é™¤ï¼Œå¦‚æœåªæ˜¯PodæŒ‚æ‰è¯¥æ•°æ®è¿˜ä¼šä¿ç•™ 2ã€ä½¿ç”¨åœºæ™¯\nä¸åŒå®¹å™¨ä¹‹é—´å…±äº«æ–‡ä»¶ï¼ˆä¾‹å¦‚æ—¥å¿—é‡‡é›†ç­‰ï¼‰\næš‚å­˜ç©ºé—´ï¼Œä¾‹å¦‚ç”¨äºåŸºäºç£ç›˜çš„åˆå¹¶æ’åº\nç”¨ä½œé•¿æ—¶é—´è®¡ç®—å´©æºƒæ¢å¤æ—¶çš„æ£€æŸ¥ç‚¹\n3ã€ç¤ºä¾‹\napiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {} 2.2. hostPath 1ã€ç‰¹ç‚¹\nä¼šå°†å®¿ä¸»æœºçš„ç›®å½•æˆ–æ–‡ä»¶æŒ‚è½½åˆ°Podä¸­ 2ã€ä½¿ç”¨åœºæ™¯\nè¿è¡Œéœ€è¦è®¿é—® Docker å†…éƒ¨çš„å®¹å™¨ï¼›ä½¿ç”¨ /var/lib/docker çš„ hostPath\nåœ¨å®¹å™¨ä¸­è¿è¡Œ cAdvisorï¼›ä½¿ç”¨ /dev/cgroups çš„ hostPath\nå…¶ä»–ä½¿ç”¨åˆ°å®¿ä¸»æœºæ–‡ä»¶çš„åœºæ™¯\nhostPathçš„typeå­—æ®µ\nå€¼ è¡Œä¸º ç©ºå­—ç¬¦ä¸²ï¼ˆé»˜è®¤ï¼‰ç”¨äºå‘åå…¼å®¹ï¼Œè¿™æ„å‘³ç€åœ¨æŒ‚è½½ hostPath å·ä¹‹å‰ä¸ä¼šæ‰§è¡Œä»»ä½•æ£€æŸ¥ã€‚ DirectoryOrCreate å¦‚æœåœ¨ç»™å®šçš„è·¯å¾„ä¸Šæ²¡æœ‰ä»»ä½•ä¸œè¥¿å­˜åœ¨ï¼Œé‚£ä¹ˆå°†æ ¹æ®éœ€è¦åœ¨é‚£é‡Œåˆ›å»ºä¸€ä¸ªç©ºç›®å½•ï¼Œæƒé™è®¾ç½®ä¸º 0755ï¼Œä¸ Kubelet å…·æœ‰ç›¸åŒçš„ç»„å’Œæ‰€æœ‰æƒã€‚ Directory ç»™å®šçš„è·¯å¾„ä¸‹å¿…é¡»å­˜åœ¨ç›®å½• FileOrCreate å¦‚æœåœ¨ç»™å®šçš„è·¯å¾„ä¸Šæ²¡æœ‰ä»»ä½•ä¸œè¥¿å­˜åœ¨ï¼Œé‚£ä¹ˆä¼šæ ¹æ®éœ€è¦åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ï¼Œæƒé™è®¾ç½®ä¸º 0644ï¼Œä¸ Kubelet å…·æœ‰ç›¸åŒçš„ç»„å’Œæ‰€æœ‰æƒã€‚ File ç»™å®šçš„è·¯å¾„ä¸‹å¿…é¡»å­˜åœ¨æ–‡ä»¶ Socket ç»™å®šçš„è·¯å¾„ä¸‹å¿…é¡»å­˜åœ¨ UNIX å¥—æ¥å­— CharDevice ç»™å®šçš„è·¯å¾„ä¸‹å¿…é¡»å­˜åœ¨å­—ç¬¦è®¾å¤‡ BlockDevice ç»™å®šçš„è·¯å¾„ä¸‹å¿…é¡»å­˜åœ¨å—è®¾å¤‡ æ³¨æ„äº‹é¡¹\nç”±äºæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„æ–‡ä»¶éƒ½ä¸åŒï¼Œå…·æœ‰ç›¸åŒé…ç½®çš„ pod åœ¨ä¸åŒèŠ‚ç‚¹ä¸Šçš„è¡Œä¸ºå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ å½“ Kubernetes æŒ‰ç…§è®¡åˆ’æ·»åŠ èµ„æºæ„ŸçŸ¥è°ƒåº¦æ—¶ï¼Œå°†æ— æ³•è€ƒè™‘ hostPath ä½¿ç”¨çš„èµ„æº åœ¨åº•å±‚ä¸»æœºä¸Šåˆ›å»ºçš„æ–‡ä»¶æˆ–ç›®å½•åªèƒ½ç”± root å†™å…¥ã€‚æ‚¨éœ€è¦åœ¨ç‰¹æƒå®¹å™¨ä¸­ä»¥ root èº«ä»½è¿è¡Œè¿›ç¨‹ï¼Œæˆ–ä¿®æ”¹ä¸»æœºä¸Šçš„æ–‡ä»¶æƒé™ä»¥ä¾¿å†™å…¥ hostPath å· 3ã€ç¤ºä¾‹\napiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: Directory 2.2.1. åŒæ­¥æ–‡ä»¶å˜åŒ–åˆ°å®¹å™¨å†… hostPathçš„æŒ‚è½½æ–¹å¼å¯ä»¥æŒ‚è½½ç›®å½•å’Œæ–‡ä»¶ä¸¤ç§æ ¼å¼ï¼Œå¦‚æœä½¿ç”¨æ–‡ä»¶æŒ‚è½½çš„æ–¹å¼ï¼Œé€šè¿‡ç®€å•çš„viç­‰å‘½ä»¤ä¿®æ”¹å®¿ä¸»æœºçš„æ–‡ä»¶ï¼Œå¹¶ä¸ä¼šå®æ—¶åŒæ­¥åˆ°å®¹å™¨å†…çš„æ˜ å°„æ–‡ä»¶ã€‚è€Œéœ€è¦å¯¹å®¹å™¨è¿›è¡Œé‡å¯çš„æ“ä½œæ‰å¯ä»¥æŠŠæ–‡ä»¶çš„ä¿®æ”¹å†…å®¹åŒæ­¥åˆ°æ–‡ä»¶ä¸­ï¼Œä½†ç”Ÿäº§çš„å®¹å™¨ä¸€èˆ¬ä¸å»ºè®®æ‰§è¡Œé‡å¯çš„æ“ä½œã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹çš„æ–¹å¼æ¥é¿å…è¿™ä¸ªé—®é¢˜çš„å‘ç”Ÿã€‚\nä»¥ä¸Šæ–‡ä»¶ä¸åŒæ­¥çš„æœ¬è´¨åŸå› æ˜¯å®¹å™¨åœ¨åˆæ¬¡æŒ‚è½½çš„æ—¶å€™ä½¿ç”¨äº†å®¿ä¸»æœºçš„æ–‡ä»¶çš„inode numberè¿›è¡Œæ ‡è¯†ï¼Œè€Œviç­‰æ“ä½œä¼šå¯¼è‡´æ–‡ä»¶çš„inode numberå‘ç”Ÿå˜åŒ–ï¼Œæ‰€ä»¥å½“å®¿ä¸»æœºæ–‡ä»¶çš„inode numberå‘ç”Ÿå˜åŒ–ï¼Œå®¹å™¨å†…å¹¶ä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä¸ºäº†ä¿æŒæ–‡ä»¶å†…å®¹ä¸€è‡´ï¼Œåˆ™éœ€è¦ä¿æŒä¿®æ”¹æ–‡ä»¶çš„åŒæ—¶æ–‡ä»¶çš„inode numberä¸å˜ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ cat æˆ–echo å‘½ä»¤è¦†ç›–æ–‡ä»¶çš„å†…å®¹åˆ™inode numberä¸ä¼šå‘ç”Ÿå˜åŒ–ã€‚\nç¤ºä¾‹ï¼š\ncontainers: volumeMounts: - mountPath: /etc/hosts name: hosts readOnly: true volumes: - hostPath: path: /etc/hosts type: FileOrCreate name: hosts ä¾‹å¦‚ï¼Œä»¥ä¸Šçš„æ¡ˆä¾‹æ˜¯é€šè¿‡æŒ‚è½½å®¿ä¸»æœºçš„/etc/hostsæ–‡ä»¶æ¥æ˜ å°„åˆ°å®¹å™¨ï¼Œå¦‚æœæƒ³ä¿®æ”¹å®¿ä¸»æœºçš„hostsæ–‡ä»¶æ¥åŒæ­¥å®¹å™¨å†…çš„hostsæ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹çš„æ–¹å¼:\n# æŸ¥çœ‹æ–‡ä»¶çš„inode ls -i /etc/hosts 39324780 /etc/hosts # è¿½åŠ è®°å½• echo \"1.1.1.1 xxx.com\" \u003e\u003e /etc/hosts # æ›¿æ¢å†…å®¹ sed 's/1.1.1.1/2.2.2.2/g' /etc/hosts \u003e temp.txt cat temp.txt \u003e /etc/hosts # æŸ¥çœ‹å®¿ä¸»æœºå’Œå®¹å™¨å†…çš„inodeå·éƒ½æ²¡æœ‰å‘ç”Ÿå˜åŒ– # crictl exec -it 20891de31a4a6 sh /var/www/html # ls -i /etc/hosts 39324780 /etc/hosts 2.3. configMap configMapæä¾›äº†ä¸€ç§ç»™Podæ³¨å…¥é…ç½®æ–‡ä»¶çš„æ–¹å¼ï¼Œé…ç½®æ–‡ä»¶å†…å®¹å­˜å‚¨åœ¨configMapå¯¹è±¡ä¸­ï¼Œå¦‚æœPodä½¿ç”¨configMapä½œä¸ºvolumeçš„ç±»å‹ï¼Œéœ€è¦å…ˆåˆ›å»ºconfigMapçš„å¯¹è±¡ã€‚\nç¤ºä¾‹\napiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: test image: busybox volumeMounts: - name: config-vol mountPath: /etc/config volumes: - name: config-vol configMap: name: log-config items: - key: log_level path: log_level 2.4. cephfs cephfsçš„æ–¹å¼å°†Podçš„å­˜å‚¨æŒ‚è½½åˆ°cephé›†ç¾¤ä¸­ï¼Œé€šè¿‡å¤–éƒ¨å­˜å‚¨çš„æ–¹å¼æŒä¹…åŒ–Podçš„æ•°æ®ï¼ˆå³å½“Podè¢«åˆ é™¤æ•°æ®ä»å¯ä»¥å­˜å‚¨åœ¨cephé›†ç¾¤ä¸­ï¼‰ï¼Œå‰ææ˜¯å…ˆéƒ¨ç½²å’Œç»´æŠ¤å¥½ä¸€ä¸ªcephé›†ç¾¤ã€‚\nç¤ºä¾‹\napiVersion: v1 kind: Pod metadata: name: cephfs spec: containers: - name: cephfs-rw image: kubernetes/pause volumeMounts: - mountPath: \"/mnt/cephfs\" name: cephfs volumes: - name: cephfs cephfs: monitors: - 10.16.154.78:6789 - 10.16.154.82:6789 - 10.16.154.83:6789 # by default the path is /, but you can override and mount a specific path of the filesystem by using the path attribute # path: /some/path/in/side/cephfs user: admin secretFile: \"/etc/ceph/admin.secret\" readOnly: true æ›´å¤šå¯å‚è€ƒ CephFS ç¤ºä¾‹ã€‚\n2.5. nfs nfsçš„æ–¹å¼ç±»ä¼¼cephfsï¼Œå³å°†Podæ•°æ®å­˜å‚¨åˆ°NFSé›†ç¾¤ä¸­ï¼Œå…·ä½“å¯å‚è€ƒNFSç¤ºä¾‹ã€‚\n2.6. persistentVolumeClaim persistentVolumeClaim å·ç”¨äºå°†PersistentVolumeæŒ‚è½½åˆ°å®¹å™¨ä¸­ã€‚PersistentVolumes æ˜¯åœ¨ç”¨æˆ·ä¸çŸ¥é“ç‰¹å®šäº‘ç¯å¢ƒçš„ç»†èŠ‚çš„æƒ…å†µä¸‹â€œå£°æ˜â€æŒä¹…åŒ–å­˜å‚¨ï¼ˆä¾‹å¦‚ GCE PersistentDisk æˆ– iSCSI å·ï¼‰çš„ä¸€ç§æ–¹å¼ã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/storage/volumes/ ","categories":"","description":"","excerpt":"1. volumeæ¦‚è¿° å®¹å™¨ä¸Šçš„æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸåŒå®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸä¸€è‡´ï¼Œå³å®¹å™¨æŒ‚æ‰ä¹‹åï¼Œå®¹å™¨å°†ä¼šä»¥æœ€åˆé•œåƒä¸­çš„æ–‡ä»¶ç³»ç»Ÿå†…å®¹å¯åŠ¨ï¼Œä¹‹å‰å®¹å™¨è¿è¡Œæ—¶äº§ç”Ÿ â€¦","ref":"/kubernetes-notes/storage/volume/volume/","tags":["Kubernetes"],"title":"Volumeä»‹ç»"},{"body":" æœ¬æ–‡ä¸»è¦æè¿°ä¸ªäººä½¿ç”¨vscodeä¸­çš„å¸¸ç”¨æ’ä»¶å’Œé…ç½®\n1. å¸¸ç”¨æ’ä»¶ æ’ä»¶åç§° è¯´æ˜ Atom One Dark Theme ä»£ç é£æ ¼ä¸»é¢˜ JetBrains Icon Theme Iconä¸»é¢˜ GitLens æ˜¾ç¤ºæŸè¡Œæäº¤è®°å½• Partial Diff é€šè¿‡å‰ªåˆ‡æ¿diffå¯¹æ¯” Go/Python ç¼–ç¨‹è¯­è¨€æ’ä»¶ 2. å¸¸ç”¨å¿«æ·é”® è¯¦ç»†å¿«æ·é”®å‚è€ƒï¼švscodeå¿«æ·é”®\nåŠŸèƒ½ å¿«æ·é”® å‡½æ•°è·³è½¬å’Œå¼•ç”¨ ï¼ˆcommand+å•å‡»ï¼‰æˆ–è€…F12 å›åˆ°ä¸Šæ¬¡è·³å›åŸå¤„ï¼ˆè·³è½¬å‰ä½ç½®ï¼‰ ctrl + - æŸ¥çœ‹æ¥å£çš„å®ç°æ–¹æ³• æ‰“å¼€terminalç»ˆç«¯ Ctrl + ` å¤šè¡Œ å—é€‰æ‹©ç¼–è¾‘ Shift + option +é¼ æ ‡é€‰æ‹©å— 3. é…ç½®å¤šçª—å£æ˜¾ç¤º vscodeé»˜è®¤æ‰“å¼€ä¸€ä¸ªæ–°é¡¹ç›®å°±éœ€è¦æ‰“å¼€æ–°çš„çª—å£ï¼Œä¸ºäº†é¿å…å¾ˆå¤šé¡¹ç›®åŒæ—¶æµè§ˆè€Œæ‰“å¼€å¤ªå¤šçª—å£ï¼Œå› æ­¤è®¾ç½®åœ¨åŒä¸€ä¸ªçª—å£ä¸­å¯ä»¥æ˜¾ç¤ºå’Œå¿«é€Ÿåˆ‡æ¢å¤šä¸ªé¡¹ç›®ã€‚\n1ã€å·¦ä¸‹è§’ç‚¹å‡»é…ç½®æŒ‰é’®ï¼Œé€‰ä¸­ settingsã€‚\n2ã€æœç´¢Â window.nativeÂ é€‰ä¸­æ­¤é€‰é¡¹ã€‚\n3ã€é‡å¯å¹¶åˆå¹¶çª—å£ã€‚\n4. é…ç½®è¿œç¨‹å¼€å‘ 4.1. æœ¬åœ°æ–‡ä»¶ä¼ è¿œç¨‹å¼€å‘ å®‰è£…sftpæ’ä»¶\nåˆ›å»ºsftpé…ç½®æ–‡ä»¶\nåˆ›å»º.vscodeç›®å½•ï¼Œåœ¨ç›®å½•ä¸‹åˆ›å»ºsftp.jsonæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š\n{ \"name\": \"ip\", \"host\": \"ip\", \"protocol\": \"sftp\", \"port\": 22, \"username\": \"root\", \"privateKeyPath\": \"~/.ssh/id_rsa\", \"remotePath\": \"/home/go/src/projectname\", \"uploadOnSave\": true, \"ignore\": [ \".git\", \".vscode\", \".idea\", \".DS_Store\", \"node_modules\" ], \"watcher\": { \"files\": \"/home/go/src/github.com/projectname/*\", \"autoUpload\": true, \"autoDelete\": true } } 4.2. è¿œç¨‹æ–‡ä»¶ä¼ æœ¬åœ°å¼€å‘ åœ¨è¿œç¨‹è®¾å¤‡ä¸Šgit cloneä»£ç ï¼Œç„¶ååœ¨vscodeä¸Šå®‰è£…remoteçš„æ’ä»¶ï¼Œé€šè¿‡remoteæ’ä»¶è¿æ¥è¿œç¨‹å¼€å‘æœºå¹¶æ‰“å¼€ä»£ç ç›®å½•ã€‚å³å¯è¿›è¡Œè¿œç¨‹å¼€å‘ã€‚\né€šè¿‡vscodeåœ¨è¿œç¨‹æœºå™¨ä¸Šé¢å®‰è£…ç›¸å…³çš„ä»£ç æ’ä»¶å³å¯å®ç°ä»£ç è·³è½¬ç­‰æ“ä½œã€‚\n","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦æè¿°ä¸ªäººä½¿ç”¨vscodeä¸­çš„å¸¸ç”¨æ’ä»¶å’Œé…ç½®\n1. å¸¸ç”¨æ’ä»¶ æ’ä»¶åç§° è¯´æ˜ Atom One Dark Theme â€¦","ref":"/linux-notes/ide/vscode/","tags":["IDE"],"title":"vscodeä½¿ç”¨é…ç½®"},{"body":"1. Ubuntuå®‰è£…containerd ä»¥ä¸‹ä»¥Ubuntuä¸ºä¾‹\nè¯´æ˜ï¼šå®‰è£…containerdä¸å®‰è£…dockeræµç¨‹åŸºæœ¬ä¸€è‡´ï¼Œå·®åˆ«åœ¨äºä¸éœ€è¦å®‰è£…docker-ce\ncontainerd: apt-get install -y containerd.io docker: apt-get install docker-ce docker-ce-cli containerd.io 1. å¸è½½æ—§ç‰ˆæœ¬ sudo apt-get remove docker docker-engine docker.io containerd runc å¦‚æœéœ€è¦åˆ é™¤é•œåƒåŠå®¹å™¨æ•°æ®åˆ™æ‰§è¡Œä»¥ä¸‹å‘½ä»¤\nsudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd 2. å‡†å¤‡åŒ…ç¯å¢ƒ 1ã€æ›´æ–°aptï¼Œå…è®¸ä½¿ç”¨httpsã€‚\nsudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 2ã€æ·»åŠ dockerå®˜æ–¹GPG keyã€‚\nsudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg 3ã€è®¾ç½®è½¯ä»¶ä»“åº“æº\necho \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null 3. å®‰è£…containerd # å®‰è£…containerd sudo apt-get update sudo apt-get install -y containerd.io # å¦‚æœæ˜¯å®‰è£…dockeråˆ™æ‰§è¡Œï¼š sudo apt-get install docker-ce docker-ce-cli containerd.io # æŸ¥çœ‹è¿è¡ŒçŠ¶æ€ systemctl enable containerd systemctl status containerd å®‰è£…æŒ‡å®šç‰ˆæœ¬\n# æŸ¥çœ‹ç‰ˆæœ¬ apt-cache madison containerd # sudo apt-get install containerd=\u003cVERSION\u003e 4. ä¿®æ”¹é…ç½® åœ¨ Linux ä¸Šï¼Œcontainerd çš„é»˜è®¤ CRI å¥—æ¥å­—æ˜¯ /run/containerd/containerd.sockã€‚\n1ã€ç”Ÿæˆé»˜è®¤é…ç½®\ncontainerd config default \u003e /etc/containerd/config.toml 2ã€ä¿®æ”¹CgroupDriverä¸ºsystemd\nk8så®˜æ–¹æ¨èä½¿ç”¨systemdç±»å‹çš„CgroupDriverã€‚\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true 3ã€é‡å¯containerd\nsystemctl restart containerd 2. ç¦»çº¿äºŒè¿›åˆ¶å®‰è£…containerd æŠŠcontainerdã€runcã€cni-pluginsã€nerdctläºŒè¿›åˆ¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå†ä¸Šä¼ åˆ°å¯¹åº”æœåŠ¡å™¨ï¼Œè§£å‹æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•ï¼Œä¿®æ”¹containerdé…ç½®æ–‡ä»¶ï¼Œå¯åŠ¨containerdã€‚\n#!/bin/bash set -e ContainerdVersion=$1 ContainerdVersion=${ContainerdVersion:-1.6.6} RuncVersion=$2 RuncVersion=${RuncVersion:-1.1.3} CniVersion=$3 CniVersion=${CniVersion:-1.1.1} NerdctlVersion=$4 NerdctlVersion=${NerdctlVersion:-0.21.0} CrictlVersion=$5 CrictlVersion=${CrictlVersion:-1.24.2} echo \"--------------install containerd--------------\" wget https://github.com/containerd/containerd/releases/download/v${ContainerdVersion}/containerd-${ContainerdVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-${ContainerdVersion}-linux-amd64.tar.gz echo \"--------------install containerd service--------------\" wget https://raw.githubusercontent.com/containerd/containerd/681aaf68b7dcbe08a51c3372cbb8f813fb4466e0/containerd.service mv containerd.service /lib/systemd/system/ mkdir -p /etc/containerd/ containerd config default \u003e /etc/containerd/config.toml sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml echo \"--------------install runc--------------\" wget https://github.com/opencontainers/runc/releases/download/v${RuncVersion}/runc.amd64 chmod +x runc.amd64 mv runc.amd64 /usr/local/bin/runc echo \"--------------install cni plugins--------------\" wget https://github.com/containernetworking/plugins/releases/download/v${CniVersion}/cni-plugins-linux-amd64-v${CniVersion}.tgz rm -fr /opt/cni/bin mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v${CniVersion}.tgz echo \"--------------install nerdctl--------------\" wget https://github.com/containerd/nerdctl/releases/download/v${NerdctlVersion}/nerdctl-${NerdctlVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local/bin nerdctl-${NerdctlVersion}-linux-amd64.tar.gz echo \"--------------install crictl--------------\" wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v${CrictlVersion}/crictl-v${CrictlVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local/bin crictl-v${CrictlVersion}-linux-amd64.tar.gz cat \u003e /etc/crictl.yaml \u003c\u003c \\EOF runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 2 debug: false pull-image-on-create: false EOF # å¯åŠ¨containerdæœåŠ¡ systemctl daemon-reload systemctl enable contaienrd systemctl restart contaienrd 3. Containerdé…ç½®ä»£ç† ç”±äºèŠ‚ç‚¹åˆ°k8så®˜æ–¹ä»“åº“ç½‘ç»œä¸é€šï¼Œæˆ–è€…è®¾å¤‡å¤„äºå†…ç½‘ï¼Œå¯ä»¥é€šè¿‡é…ç½®http_proxyä»£ç†çš„æ–¹å¼æ¥æ‹‰å–é•œåƒã€‚\nvi /lib/systemd/system/containerd.service # æ·»åŠ ä»£ç†ç¯å¢ƒå˜é‡ [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] Environment=\"HTTP_PROXY=http://squid:3128/\" # æ·»åŠ ç¯å¢ƒå˜é‡ä»£ç† Environment=\"HTTPS_PROXY=http://squid:3128/\" # æ·»åŠ ç¯å¢ƒå˜é‡ä»£ç† ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target # é‡å¯æœåŠ¡ systemctl daemon-reload systemctl restart containerd å‚è€ƒï¼š\nhttps://github.com/containerd/containerd\nhttps://github.com/containerd/containerd/blob/main/docs/getting-started.md\nhttps://docs.docker.com/engine/install/ubuntu/\nhttps://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd\ncontainerd/containerd.service at main Â· containerd/containerd Â· GitHub\nGitHub - containerd/nerdctl: containerd ctl GitHub - kubernetes-sigs/cri-tools: CLI and validation tools for Kubelet Container Runtime Interface (CRI) .\n","categories":"","description":"","excerpt":"1. Ubuntuå®‰è£…containerd ä»¥ä¸‹ä»¥Ubuntuä¸ºä¾‹\nè¯´æ˜ï¼šå®‰è£…containerdä¸å®‰è£…dockeræµç¨‹åŸºæœ¬ä¸€è‡´ï¼Œå·®åˆ«åœ¨äºä¸ â€¦","ref":"/kubernetes-notes/runtime/containerd/install-containerd/","tags":["Containerd"],"title":"å®‰è£…Containerd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/etcd/install/","tags":"","title":"éƒ¨ç½²Etcdé›†ç¾¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/setup/installer/","tags":"","title":"éƒ¨ç½²k8sé›†ç¾¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/storage/volume/","tags":"","title":"å­˜å‚¨å·æ¦‚å¿µ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/principle/component/","tags":"","title":"æ ¸å¿ƒç»„ä»¶"},{"body":"æœ¬æ–‡ä¸»è¦ä»‹ç»å¦‚ä½•é€šè¿‡Ollamaå’ŒOpenWebUIæ¥æ­å»ºä¸€ä¸ªæœ¬åœ°ç§æœ‰åŒ–è¿è¡Œçš„å¤§æ¨¡å‹å·¥å…·ã€‚ç§æœ‰åŒ–å¤§æ¨¡å‹çš„æ„å»ºä¸»è¦ç”¨äºè§£å†³æ•°æ®çš„å®‰å…¨æ€§é—®é¢˜ï¼Œå¯¹äºå¤§éƒ¨åˆ†ç§æœ‰æ•°æ®ä¸é€‚åˆé€šè¿‡å¤–éƒ¨çš„å¤§æ¨¡å‹ç½‘ç«™æ¥ä¸Šä¼ å’Œåˆ†æã€‚\n1. Ollama ä¸ OpenWebUI ä»‹ç» 1.1. Ollamaç®€ä»‹ Ollama æ˜¯ä¸€ä¸ª æœ¬åœ°è¿è¡Œçš„ AI å¤§æ¨¡å‹ç®¡ç†å·¥å…·ï¼Œå¯ä»¥è®©ä½ åœ¨æœ¬åœ° å¿«é€Ÿæ‹‰å–ã€ç®¡ç†å’Œè¿è¡Œ å„ç§å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLaMAã€Mistralã€deepseek ç­‰ï¼‰ï¼Œè€Œæ— éœ€ä¾èµ–äº‘ç«¯ APIã€‚å®ƒçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š\nç®€æ˜“å®‰è£…ï¼šæ”¯æŒ macOSã€Linux å’Œ Windowsï¼ˆWSLï¼‰ã€‚ æœ¬åœ°æ¨ç†ï¼šåœ¨æœ¬åœ°è®¾å¤‡ä¸Šç›´æ¥è¿è¡Œ LLMï¼Œä¿æŠ¤æ•°æ®éšç§ã€‚ æ¨¡å‹ç®¡ç†ï¼šå¯ä»¥åƒä½¿ç”¨ Docker ä¸€æ · ollama run llama2 è½»æ¾æ‹‰å–å’Œè¿è¡Œæ¨¡å‹ã€‚ è‡ªå®šä¹‰æ¨¡å‹ï¼šæ”¯æŒé€šè¿‡ Modelfile è¿›è¡Œå¾®è°ƒå’Œå®šåˆ¶ã€‚ æ”¯æŒ APIï¼šå¯ä»¥é€šè¿‡ Pythonã€Node.js ç­‰è¯­è¨€è°ƒç”¨ Ollama æä¾›çš„æœ¬åœ° REST APIã€‚ Ollama é€‚ç”¨äºæœ¬åœ° AI ä»£ç†ã€åµŒå…¥å¼ AI åº”ç”¨ã€éšç§ä¿æŠ¤çš„æ™ºèƒ½åŠ©æ‰‹ç­‰åœºæ™¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è¿è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼Œè€Œæ— éœ€è‡ªå·±æ­å»ºå¤æ‚çš„æ¨ç†ç¯å¢ƒã€‚\n1.2. OpenWebUIç®€ä»‹ Open-WebUI æ˜¯ä¸€ä¸ª å¼€æºçš„ Web ç”¨æˆ·ç•Œé¢ï¼Œç”¨äºç®¡ç†å’Œä½¿ç”¨æœ¬åœ°æˆ–è¿œç¨‹çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ¯”å¦‚ Ollamaã€OpenAIã€Gemini ç­‰ã€‚å®ƒçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š\nå‹å¥½çš„ Web ç•Œé¢ï¼šæä¾› ChatGPT ç±»ä¼¼çš„å¯¹è¯ UIï¼Œæ–¹ä¾¿äº¤äº’ã€‚ æ”¯æŒå¤šç§åç«¯ï¼šå¯ä»¥è¿æ¥ Ollamaã€OpenAI APIã€æœ¬åœ° LLM ç­‰ã€‚ å¤šç”¨æˆ·æ”¯æŒï¼šé€‚ç”¨äºå›¢é˜Ÿåä½œã€‚ å¯¹è¯å†å²ç®¡ç†ï¼šå¯ä¿å­˜å’Œç®¡ç†èŠå¤©è®°å½•ã€‚ æ’ä»¶å’Œè‡ªå®šä¹‰åŠŸèƒ½ï¼šæ”¯æŒæ‰©å±•ï¼Œé€‚ç”¨äºä¸åŒåº”ç”¨åœºæ™¯ã€‚ å®ƒå¯ä»¥è®©æœ¬åœ° LLM å˜å¾—æ›´åŠ æ˜“ç”¨ï¼Œé€‚åˆä¸ªäººã€ä¼ä¸šéƒ¨ç½²æœ¬åœ° AI åŠ©æ‰‹ã€‚\n2. éƒ¨ç½²ollama 2.1. è„šæœ¬å®‰è£…ollama curl -fsSL https://ollama.com/install.sh | sh è¾“å‡º\n\u003e\u003e\u003e Installing ollama to /usr/local \u003e\u003e\u003e Downloading Linux amd64 bundle ######################################################################## 100.0% \u003e\u003e\u003e Creating ollama user... \u003e\u003e\u003e Adding ollama user to render group... \u003e\u003e\u003e Adding ollama user to video group... \u003e\u003e\u003e Adding current user to ollama group... \u003e\u003e\u003e Creating ollama systemd service... \u003e\u003e\u003e Enabling and starting ollama service... Created symlink /etc/systemd/system/default.target.wants/ollama.service -\u003e /etc/systemd/system/ollama.service. \u003e\u003e\u003e The Ollama API is now available at 127.0.0.1:11434. \u003e\u003e\u003e Install complete. Run \"ollama\" from the command line. WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode. é»˜è®¤æœåŠ¡ç›‘å¬çš„åœ°å€ä¸ºï¼š127.0.0.1:11434\n2.2. æŸ¥çœ‹ollamaæœåŠ¡çŠ¶æ€ systemctl status ollama * ollama.service - Ollama Service Loaded: loaded (/etc/systemd/system/ollama.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2025-02-07 17:21:55 +08; 23s ago Main PID: 53472 (ollama) Tasks: 10 Memory: 10.3M CGroup: /system.slice/ollama.service `-53472 /usr/local/bin/ollama serve æŸ¥çœ‹ollamaå‘½ä»¤\n# ollama --help Large language model runner Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \"ollama [command] --help\" for more information about a command. 2.3. æ‹‰å–ä¸€ä¸ªå¤§æ¨¡å‹ å¯ä»¥åœ¨ https://ollama.com/search ç½‘ç«™ä¸Šï¼Œé€‰æ‹©ä¸€ä¸ªæ‰€éœ€è¦çš„å¤§æ¨¡å‹ï¼Œä¾‹å¦‚deepseek-r1:7bã€‚\n# ä¸‹è½½æŒ‡å®šçš„å¤§æ¨¡å‹ï¼Œä¾‹å¦‚deepseek # ollama pull deepseek-r1:7b pulling manifest pulling 96c415656d37... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.7 GB pulling 369ca498f347... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 387 B pulling 6e4c38e1172f... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 1.1 KB pulling f4d24e9138dd... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 148 B pulling 40fb844194b2... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 487 B verifying sha256 digest writing manifest success # ollama list NAME ID SIZE MODIFIED deepseek-r1:7b 0a8c26691023 4.7 GB 12 minutes ago 2.4. è¿è¡Œå¤§æ¨¡å‹ # ollama run deepseek-r1:7b \u003e\u003e\u003e ä½ æ˜¯è° \u003cthink\u003e \u003c/think\u003e æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚ \u003e\u003e\u003e /bye # æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„æ¨¡å‹ # ollama ps NAME ID SIZE PROCESSOR UNTIL deepseek-r1:7b 0a8c26691023 5.5 GB 100% CPU 3 minutes from now 2.5. ä¿®æ”¹ollamaæœåŠ¡åœ°å€å’Œç›®å½• 2.5.1. ä¿®æ”¹ollamaæœåŠ¡åœ°å€ ollamaæœåŠ¡é»˜è®¤ç›‘å¬127.0.0.1, å¦‚æœè¦ä¿®æ”¹ç›‘å¬åœ°å€ï¼Œåˆ™å¯ä»¥æ·»åŠ Environment=\"OLLAMA_HOST=0.0.0.0:11434\"ã€‚\nvi /etc/systemd/system/ollama.service [Unit] Description=Ollama Service After=network-online.target [Service] Environment=\"OLLAMA_HOST=0.0.0.0:11434\" # å¢åŠ ç¯å¢ƒå˜é‡ ExecStart=/usr/local/bin/ollama serve User=ollama Group=ollama Restart=always RestartSec=3 Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\" [Install] WantedBy=default.target # é‡å¯æœåŠ¡ systemctl daemon-reload systemctl restart ollama systemctl status ollama 2.5.2. ä¿®æ”¹ollamaæ•°æ®ç›®å½• å‚è€ƒï¼šollama/docs/faq.md\né»˜è®¤å­˜å‚¨ç›®å½•\nmacOS:Â ~/.ollama/models Linux:Â /usr/share/ollama/.ollama/models Windows:Â C:\\Users\\%username%\\.ollama\\models ä»¥linuxç³»ç»Ÿä¸ºä¾‹ï¼Œä¿®æ”¹é»˜è®¤çš„å­˜å‚¨ç›®å½•ï¼š\ndir=\"/data/ollama/models\" # åˆ›å»ºç›®å½•å¹¶åˆ†é…æƒé™ mkdir -p /data/ollama/models sudo chown -R ollama:ollama /data/ollama/models # æ·»åŠ ç¯å¢ƒå˜é‡OLLAMA_MODELS vi /etc/systemd/system/ollama.service [Service] Environment=\"OLLAMA_MODELS=/data/ollama/models\" # é‡å¯æœåŠ¡ systemctl daemon-reload systemctl restart ollama systemctl status ollama # è¿ç§»æ•°æ® cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models sudo chown -R ollama:ollama /data/ollama/models 3. éƒ¨ç½²open-webui 3.1. å•ç‹¬éƒ¨ç½²open-webui å¦‚æœå·²ç»éƒ¨ç½²äº†ollamaæœåŠ¡ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å•ç‹¬éƒ¨ç½²open-webuiï¼Œä¿®æ”¹OLLAMA_BASE_URLä¸ºollamaçš„æœåŠ¡åœ°å€ã€‚å¦‚æœä½¿ç”¨host-networkï¼Œé»˜è®¤æœåŠ¡ç›‘å¬ç«¯å£ä¸º8080ã€‚\ndocker run -d --network=host -e OLLAMA_BASE_URL=http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main ç¯å¢ƒå˜é‡\nOLLAMA_BASE_URL:http://OLLAMA_HOST:11434 : è®¾ç½®ollamaæœåŠ¡çš„åœ°å€ HF_HUB_OFFLINE: \"1\"ï¼šè®¾ç½®æ¨¡å‹ä¸ºç¦»çº¿çš„ç¯å¢ƒ ENABLE_OPENAI_API: \"false\"ï¼šè®¾ç½®å…³é—­openaiçš„æ¥å£ è®¿é—®open-webuiæœåŠ¡ï¼š\nè®¿é—®http://æœåŠ¡å™¨IP:8080ï¼Œæ³¨å†Œç”¨æˆ·åå¯†ç ç„¶åç™»å½•ã€‚å°±å¯ä»¥ä½¿ç”¨æœ¬åœ°çš„å¤§æ¨¡å‹æœåŠ¡ã€‚\n3.2. éƒ¨ç½²open-webuiå’ŒollamaæœåŠ¡ å¦‚æœä¸æƒ³å•ç‹¬éƒ¨ç½²ollamaï¼Œå¯ä»¥é€šè¿‡open-webui:ollamaé•œåƒï¼ŒåŒæ—¶éƒ¨ç½²open-webuiå’Œollamaï¼Œä¸¤ä¸ªæœåŠ¡é›†æˆåœ¨åŒä¸€ä¸ªé•œåƒä¸­ã€‚\n# ä¸‹è½½é•œåƒ docker pull ghcr.io/open-webui/open-webui:ollama # è¿è¡Œopen-webui:ollama docker run -d -p 3000:8080 -v ollama:/root/.ollama -v ollama-open-webui:/app/backend/data --name ollama-open-webui --restart always ghcr.io/open-webui/open-webui:ollama æŸ¥çœ‹æœåŠ¡\n# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ghcr.io/open-webui/open-webui ollama 29d60b4958c8 4 days ago 8.02GB # docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3175fc20c608 ghcr.io/open-webui/open-webui:ollama \"bash start.sh\" 16 minutes ago Up 16 minutes (healthy) 0.0.0.0:3000-\u003e8080/tcp ollama-open-webui ç™»å½•å®¹å™¨ä¸‹è½½å¤§æ¨¡å‹æ–‡ä»¶\n# ç™»å½•å®¹å™¨ # docker exec -it 3175fc20c608 bash # ä¸‹è½½æŒ‡å®šçš„å¤§æ¨¡å‹ # ollama pull deepseek-r1:7b pulling manifest pulling 96c415656d37... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.7 GB pulling 369ca498f347... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 387 B pulling 6e4c38e1172f... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 1.1 KB pulling f4d24e9138dd... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 148 B pulling 40fb844194b2... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 487 B verifying sha256 digest writing manifest success # ollama list NAME ID SIZE MODIFIED deepseek-r1:7b 0a8c26691023 4.7 GB 12 minutes ago åˆ™å¯ä»¥è®¿é—®æ‰€éƒ¨å±æœåŠ¡å™¨çš„åœ°å€å’Œç«¯å£æ¥è®¿é—®open-webuiçš„æœåŠ¡ã€‚\n3.3. æ„å»ºæœ¬åœ°çŸ¥è¯†åº“ 3.3.1. è‡ªå®šä¹‰æ–‡ä»¶åˆ†æ å¯ä»¥é€šè¿‡é¡µé¢ä¸Šä¼ æœ¬åœ°çš„çŸ¥è¯†åº“æ–‡ä»¶ï¼Œè®©AIå›ç­”å…³äºè‡ªå®šä¹‰æ–‡ä»¶ä¸­çš„å†…å®¹ã€‚\nä¾‹å¦‚ï¼šæˆ‘é€šè¿‡æ–‡ä»¶è‡ªå®šä¹‰äº†å†…å®¹ï¼Œæé—®å¼ é£çš„ç”µè¯å·ç ï¼Œåˆ™å¯ä»¥é€šè¿‡æ–‡ç« ä¸­çš„å†…å®¹æ¥å›ç­”ã€‚\nå…¶ä¸­è‡ªå®šä¹‰æ–‡æ¡£çš„å†…å®¹å¦‚ä¸‹ï¼š\nåŒæ ·å¯ä»¥ä¸Šä¼ å…¶ä»–æ–‡ä»¶æ¥æ„å»ºä¸€ä¸ªæœ¬åœ°å¤§æ¨¡å‹çŸ¥è¯†åº“ã€‚ç„¶åå€ŸåŠ©å¤§æ¨¡å‹æ¥æŸ¥è¯¢å’Œåˆ†ææ•°æ®å†…å®¹ã€‚\n3.3.2. æœ¬åœ°åŒ–æ•°æ®å­˜å‚¨ å…¶ä¸­open-webuiçš„æœ¬åœ°åŒ–æ•°æ®å­˜å‚¨åœ¨å®¹å™¨å†…çš„/app/backend/data/ç›®å½•ä¸‹ã€‚\n/app/backend/data# ls -l total 236 drwxr-xr-x 7 root root 4096 Feb 11 10:48 cache drwxr-xr-x 2 root root 4096 Feb 18 06:11 uploads drwxr-xr-x 3 root root 4096 Feb 18 06:11 vector_db -rw-r--r-- 1 root root 229376 Feb 18 06:38 webui.db # å¯ä»¥ä»uploadsç›®å½•çœ‹åˆ°ä¸Šä¼ çš„æœ¬åœ°æ–‡ä»¶ /app/backend/data/uploads# cat 117e6f99-0657-40d1-ab6f-1bea81e78053_ollama-docs.md å¼ é£çš„ç”µè¯å·ç æ˜¯u987438274 æ›¹æ“çš„ç”µè¯å·ç æ˜¯123456 å…³ç¾½çš„ç”µè¯å·ç æ˜¯5352345 3.4. FAQ 1ï¼‰open-webuié¡µé¢æ— æ³•é€‰æ‹©æ¨¡å‹ é—®é¢˜ï¼š\nå½“å•ç‹¬éƒ¨ç½²open-webuiï¼Œå¯èƒ½ä¼šé‡åˆ°open-webuié¡µé¢æ— æ³•é€‰æ‹©æ¨¡å‹å…·ä½“çš„ç°è±¡å¦‚ä¸‹ï¼š\nopen-webuiæ—¥å¿—æŠ¥é”™ï¼š\nINFO [open_webui.routers.ollama] get_all_models() ERROR [open_webui.routers.ollama] Connection error: Cannot connect to host 1.1.1.1:11434 ssl:default [Connect call failed ('1.1.1.1', 11434)] åŸå› ï¼š\næŒ‰å®˜ç½‘å‘½ä»¤ä½¿ç”¨ç«¯å£æ˜ å°„çš„ç½‘ç»œæ¨¡å¼ï¼Œå¦‚æœOLLAMA_BASE_URLé…ç½®ä¸º127.0.0.1åˆ™è®¿é—®ä¸åˆ°å•ç‹¬éƒ¨ç½²çš„ollamaæœåŠ¡ï¼Œå¦‚æœæ”¹ç”¨å…·ä½“çš„ollamaçš„IPä¹Ÿå¯èƒ½å­˜åœ¨è®¿é—®å¤±è´¥çš„é—®é¢˜ã€‚\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main è§£å†³æ–¹æ¡ˆï¼š\ndockerç½‘ç»œæ¨¡å¼æ”¹ä¸ºhost-networkçš„ç½‘ç»œæ¨¡å¼\ndocker run -d --network=host -e OLLAMA_BASE_URL=http://OLLAMA_HOST:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 2ï¼‰æ•°æ®ç›®å½•æ²¡æƒé™permission denied å¦‚æœç”¨æˆ·ä¿®æ”¹äº†ollamaçš„modelsçš„å­˜å‚¨ç›®å½•ï¼Œå‡ºç°ollamaæœåŠ¡é‡å¯å¤±è´¥ï¼Œæˆ–è€…pull modelæ•°æ®æŠ¥é”™\n# ä¿®æ”¹ollamaçš„modelç›®å½•åollamaæœåŠ¡é‡å¯æŠ¥é”™ Error: mkdir /data/ollama: permission denied # è¿ç§»modelæ•°æ®åå‡ºç°æ²¡æƒé™ï¼Œå› ä¸ºä½¿ç”¨äº†rootå‘½ä»¤æ‰§è¡Œ cp -fr /usr/share/ollama/.ollama/models/* /data/ollama/models # ollama pull deepseek-r1:70b writing manifest Error: open /data/ollama/models/manifests/registry.ollama.ai/library/deepseek-r1/70b: permission denied åŸå› ï¼š\nollamaé»˜è®¤ä½¿ç”¨çš„ç”¨æˆ·åæ˜¯ ollamaï¼Œå› æ­¤éœ€è¦ç»™ç›®å½•æ·»åŠ ç”¨æˆ·çš„æƒé™ï¼Œä¾‹å¦‚ï¼šç›®å½•åˆ›å»ºå’Œmodelæ–‡ä»¶è¿ç§»æ˜¯é€šè¿‡rootæˆ–å…¶ä»–ç”¨æˆ·æ‰§è¡Œçš„ã€‚\nsudo chown -R ollama:ollama /data/ollama/models 4. æ€»ç»“ æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ollamaå’Œopen-webuiçš„éƒ¨ç½²ï¼Œä»è€Œæ­å»ºä¸€ä¸ªæœ¬åœ°åŒ–ç§æœ‰çš„å¤§æ¨¡å‹å·¥å…·ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½å­˜å‚¨åœ¨æœ¬åœ°ã€‚å¯ä»¥é€šè¿‡ä¸Šä¼ æ–‡ä»¶æ¥åˆ†ææœ¬åœ°çš„æ•°æ®ï¼Œç±»ä¼¼æ„å»ºæœ¬åœ°å¤§æ¨¡å‹çŸ¥è¯†åº“ã€‚\nä¸è¿‡æœ¬åœ°å¤§æ¨¡å‹çš„å“åº”é€Ÿåº¦ä¾èµ–äºå¤§æ¨¡å‹æœ¬èº«å’Œæœ¬åœ°çš„èµ„æºï¼ŒåŒ…æ‹¬cpuå’Œgpuï¼Œæ²¡æœ‰gpuèµ„æºä¹Ÿå¯ä»¥è¿è¡Œã€‚åœ¨èµ„æºè¾ƒå°çš„æƒ…å†µä¸‹ï¼Œå¤§æ¨¡å‹å›ç­”é—®é¢˜çš„é€Ÿåº¦æ¯”è¾ƒæ…¢ã€‚å¦‚æœå®Œå…¨éœ€è¦ç¦»çº¿çš„å¤§æ¨¡å‹åˆ†ææ•°æ®ï¼Œåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹éœ€è¦å†è¿›ä¸€æ­¥åšä¼˜åŒ–æ‰èƒ½å¾—åˆ°æ¯”è¾ƒå¥½çš„ä½“éªŒã€‚\nå‚è€ƒï¼š\nhttps://ollama.com/download/linux https://github.com/ollama/ollama/blob/main/docs/faq.md https://docs.openwebui.com/getting-started/quick-start https://github.com/open-webui/open-webui#troubleshooting ","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦ä»‹ç»å¦‚ä½•é€šè¿‡Ollamaå’ŒOpenWebUIæ¥æ­å»ºä¸€ä¸ªæœ¬åœ°ç§æœ‰åŒ–è¿è¡Œçš„å¤§æ¨¡å‹å·¥å…·ã€‚ç§æœ‰åŒ–å¤§æ¨¡å‹çš„æ„å»ºä¸»è¦ç”¨äºè§£å†³æ•°æ®çš„å®‰å…¨æ€§é—®é¢˜ï¼Œå¯¹ â€¦","ref":"/linux-notes/llm/build-ollama-openwebui/","tags":["å¤§æ¨¡å‹"],"title":"åŸºäºOllamaæ„å»ºæœ¬åœ°å¤§æ¨¡å‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/trouble-shooting/node/","tags":"","title":"èŠ‚ç‚¹é—®é¢˜"},{"body":"1. é•œåƒä»“åº“çš„åŸºæœ¬æ“ä½œ 1.1. ç™»å½•é•œåƒä»“åº“ docker login -u \u003cusername\u003e -p \u003cpassword\u003e \u003cregistry-addr\u003e 1.2. æ‹‰å–é•œåƒ docker pull https://registry.xxx.com/dev/nginx:latest 1.3. æ¨é€é•œåƒ docker push https://registry.xxx.com/dev/nginx:latest 1.4. é‡å‘½åé•œåƒ docker tag \u003cold-image\u003e \u003cnew-image\u003e 2. docker.xxx.comé•œåƒä»“åº“ ä½¿ç”¨docker.xxx.comé•œåƒä»“åº“ã€‚\n2.1. æ‰€æœ‰èŠ‚ç‚¹é…ç½®insecure-registries #cat /etc/docker/daemon.json { \"data-root\": \"/data/docker\", \"debug\": false, \"insecure-registries\": [ ... \"docker.xxx.com:8080\" ], ... } 2.2. æ‰€æœ‰èŠ‚ç‚¹é…ç½®/var/lib/kubelet/config.json å…·ä½“å‚è€ƒï¼šconfiguring-nodes-to-authenticate-to-a-private-registry\nåœ¨æŸä¸ªèŠ‚ç‚¹ç™»å½•docker.xxx.com:8080é•œåƒä»“åº“ï¼Œä¼šæ›´æ–° $HOME/.docker/config.json æ£€æŸ¥$HOME/.docker/config.jsonæ˜¯å¦æœ‰è¯¥é•œåƒä»“åº“çš„authä¿¡æ¯ã€‚ #cat ~/.docker/config.json { \"auths\": { \"docker.xxx.com:8080\": { \"auth\": \"\u003cæ­¤å¤„ä¸ºå‡­è¯ä¿¡æ¯\u003e\" } }, \"HttpHeaders\": { \"User-Agent\": \"Docker-Client/18.09.9 (linux)\" } } å°†$HOME/.docker/config.jsonæ‹·è´åˆ°æ‰€æœ‰çš„NodeèŠ‚ç‚¹ä¸Šçš„/var/lib/kubelet/config.jsonã€‚ # è·å–æ‰€æœ‰èŠ‚ç‚¹çš„IP nodes=$(kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type==\"ExternalIP\")]}{.address} {end}') # æ‹·è´åˆ°æ‰€æœ‰èŠ‚ç‚¹ for n in $nodes; do scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json; done 2.3. åˆ›å»ºdocker.xxx.comé•œåƒçš„pod æŒ‡å®šé•œåƒä¸ºï¼šdocker.xxx.com:8080/public/2048:latest\nå®Œæ•´pod.yaml\napiVersion: apps/v1beta2 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" generation: 1 labels: k8s-app: dockeroa-hub qcloud-app: dockeroa-hub name: dockeroa-hub namespace: test spec: progressDeadlineSeconds: 600 replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dockeroa-hub qcloud-app: dockeroa-hub strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: k8s-app: dockeroa-hub qcloud-app: dockeroa-hub spec: containers: - image: docker.xxx.com:8080/public/2048:latest imagePullPolicy: Always name: game resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always nodeName: 192.168.1.1 schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 æŸ¥çœ‹podçŠ¶æ€\n#kgpoowide -n game NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES docker-oa-757bbbddb5-h6j7m 1/1 Running 0 14m 192.168.2.51 192.168.1.1 \u003cnone\u003e \u003cnone\u003e docker-oa-757bbbddb5-jp5dw 1/1 Running 0 14m 192.168.1.32 192.168.1.2 \u003cnone\u003e \u003cnone\u003e docker-oa-757bbbddb5-nlw9f 1/1 Running 0 14m 192.168.0.43 192.168.1.3 \u003cnone\u003e \u003cnone\u003e å‚è€ƒï¼š\nhttps://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-registry ","categories":"","description":"","excerpt":"1. é•œåƒä»“åº“çš„åŸºæœ¬æ“ä½œ 1.1. ç™»å½•é•œåƒä»“åº“ docker login -u \u003cusername\u003e -p \u003cpassword\u003e â€¦","ref":"/kubernetes-notes/operation/registry/config-private-registry/","tags":["Kubernetes"],"title":"é…ç½®ç§æœ‰é•œåƒä»“åº“"},{"body":"1. å®‰è£…etcdadm åœ¨Releases Â· kubernetes-sigs/etcdadm Â· GitHubä¸­é€‰æ‹©éœ€è¦éƒ¨ç½²çš„ç‰ˆæœ¬ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\nwget https://github.com/kubernetes-sigs/etcdadm/releases/download/v0.1.5/etcdadm-linux-amd64 mv etcdadm-linux-amd64 /usr/bin/etcdadm chmod +x /usr/bin/etcdadm 2. éƒ¨ç½²etcdé›†ç¾¤ 2.1. init etcdçš„ç‰ˆæœ¬å¯ä»¥åœ¨ Releases Â· etcd-io/etcd Â· GitHub ä¸­æŸ¥è¯¢ã€‚\netcdadm init --name \u003cnode1\u003e --version=3.5.4 2.2. ä¸Šä¼ è¯ä¹¦åˆ°å…¶ä»–æœºå™¨ # ç™»å½•node2 node3 mkdir -p /etc/etcd/pki # å°†node1çš„/etc/etcd/pki/ca.* æ‹·è´åˆ°node2 node3 /etc/etcd/pki/ scp /etc/etcd/pki/ca.* node2:/etc/etcd/pki/ 2.3. join etcdadm join https://\u003cnode1\u003e:2379 --name=\u003cnode2\u003e --version=3.5.4 etcdadm join https://\u003cnode1\u003e:2379 --name=\u003cnode3\u003e --version=3.5.4 3. æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ è®¾ç½®etcdctlç¯å¢ƒå˜é‡\n# æ·»åŠ endpoint cat \u003e\u003e /etc/etcd/etcdctl.env \u003c\u003c EOF export ETCDCTL_ENDPOINTS=node1:2379,node2:2379,node2:2379 EOF # æ‹·è´å‘½ä»¤è„šæœ¬åˆ°/usr/bin/ cp /opt/bin/etcdctl /opt/bin/etcdctl.sh /usr/bin/ æŸ¥çœ‹é›†ç¾¤çŠ¶æ€\n$ etcdctl.sh endpoint status -w table +--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | node1:2379 | 5fe84cb4a0ef4e69 | 3.5.4 | 20 kB | true | false | 3 | 13 | 13 | | | node2:2379 | cb8d48da0ea9b8c0 | 3.5.4 | 20 kB | false | false | 3 | 13 | 13 | | | node3:2379 | fafa80c55eebeffa | 3.5.4 | 20 kB | false | false | 3 | 13 | 13 | | +--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 4. Etcdå¯åŠ¨é…ç½®æ–‡ä»¶ systemd service\n# cat /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos/etcd Conflicts=etcd-member.service Conflicts=etcd2.service [Service] EnvironmentFile=/etc/etcd/etcd.env ExecStart=/opt/bin/etcd Type=notify TimeoutStartSec=0 Restart=on-failure RestartSec=5s LimitNOFILE=65536 Nice=-10 IOSchedulingClass=best-effort IOSchedulingPriority=2 MemoryLow=200M [Install] WantedBy=multi-user.target /etc/etcd/etcd.env\nETCD_NAME=etcd01 # Initial cluster configuration ETCD_INITIAL_CLUSTER=etcd01=https://node1:2380 ETCD_INITIAL_CLUSTER_TOKEN=88ad6def ETCD_INITIAL_CLUSTER_STATE=new # Peer configuration ETCD_INITIAL_ADVERTISE_PEER_URLS=https://node1:2380 ETCD_LISTEN_PEER_URLS=https://node1:2380 ETCD_CLIENT_CERT_AUTH=true ETCD_PEER_CERT_FILE=/etc/etcd/pki/peer.crt ETCD_PEER_KEY_FILE=/etc/etcd/pki/peer.key ETCD_PEER_TRUSTED_CA_FILE=/etc/etcd/pki/ca.crt # Client/server configuration ETCD_ADVERTISE_CLIENT_URLS=https://node1:2379 ETCD_LISTEN_CLIENT_URLS=https://node1:2379,https://127.0.0.1:2379 ETCD_PEER_CLIENT_CERT_AUTH=true ETCD_CERT_FILE=/etc/etcd/pki/server.crt ETCD_KEY_FILE=/etc/etcd/pki/server.key ETCD_TRUSTED_CA_FILE=/etc/etcd/pki/ca.crt # Other ETCD_DATA_DIR=/var/lib/etcd ETCD_STRICT_RECONFIG_CHECK=true GOMAXPROCS=48 å‚è€ƒï¼š\nGitHub - kubernetes-sigs/etcdadm\nClustering Guide | etcd\n","categories":"","description":"","excerpt":"1. å®‰è£…etcdadm åœ¨Releases Â· kubernetes-sigs/etcdadm Â· GitHubä¸­é€‰æ‹©éœ€è¦éƒ¨ç½²çš„ç‰ˆæœ¬ï¼Œç¤ºä¾‹ â€¦","ref":"/kubernetes-notes/etcd/install/install-etcd-by-etcdadm/","tags":"","title":"ä½¿ç”¨etcdadméƒ¨ç½²Etcdé›†ç¾¤"},{"body":" æœ¬æ–‡ä¸ºåŸºäºkubeadmæ­å»ºç”Ÿäº§ç¯å¢ƒçº§åˆ«é«˜å¯ç”¨çš„k8sé›†ç¾¤ã€‚\n1. ç¯å¢ƒå‡†å¤‡ 1.0. masterç¡¬ä»¶é…ç½® å‚è€ƒï¼š\nMasterèŠ‚ç‚¹è§„æ ¼\né«˜å¯é æ¨èé…ç½® - å®¹å™¨æœåŠ¡ ACK - é˜¿é‡Œäº‘\nKubernetesé›†ç¾¤MasterèŠ‚ç‚¹ä¸Šè¿è¡Œç€etcdã€kube-apiserverã€kube-controllerç­‰æ ¸å¿ƒç»„ä»¶ï¼Œå¯¹äºKubernetesé›†ç¾¤çš„ç¨³å®šæ€§æœ‰ç€è‡³å…³é‡è¦çš„å½±å“ï¼Œå¯¹äºç”Ÿäº§ç¯å¢ƒçš„é›†ç¾¤ï¼Œå¿…é¡»æ…é‡é€‰æ‹©Masterè§„æ ¼ã€‚Masterè§„æ ¼è·Ÿé›†ç¾¤è§„æ¨¡æœ‰å…³ï¼Œé›†ç¾¤è§„æ¨¡è¶Šå¤§ï¼Œæ‰€éœ€è¦çš„Masterè§„æ ¼ä¹Ÿè¶Šé«˜ã€‚\nè¯´æ˜Â ï¼šå¯ä»å¤šä¸ªè§’åº¦è¡¡é‡é›†ç¾¤è§„æ¨¡ï¼Œä¾‹å¦‚èŠ‚ç‚¹æ•°é‡ã€Podæ•°é‡ã€éƒ¨ç½²é¢‘ç‡ã€è®¿é—®é‡ã€‚è¿™é‡Œç®€å•çš„è®¤ä¸ºé›†ç¾¤è§„æ¨¡å°±æ˜¯é›†ç¾¤é‡Œçš„èŠ‚ç‚¹æ•°é‡ã€‚\nå¯¹äºå¸¸è§çš„é›†ç¾¤è§„æ¨¡ï¼Œå¯ä»¥å‚è§å¦‚ä¸‹çš„æ–¹å¼é€‰æ‹©MasterèŠ‚ç‚¹çš„è§„æ ¼ï¼ˆå¯¹äºæµ‹è¯•ç¯å¢ƒï¼Œè§„æ ¼å¯ä»¥å°ä¸€äº›ã€‚ä¸‹é¢çš„é€‰æ‹©èƒ½å°½é‡ä¿è¯Masterè´Ÿè½½ç»´æŒåœ¨ä¸€ä¸ªè¾ƒä½çš„æ°´å¹³ä¸Šï¼‰ã€‚\nèŠ‚ç‚¹è§„æ¨¡ Masterè§„æ ¼ ç£ç›˜ 1~5ä¸ªèŠ‚ç‚¹ 4æ ¸8 GBï¼ˆä¸å»ºè®®2æ ¸4 GBï¼‰ 6~20ä¸ªèŠ‚ç‚¹ 4æ ¸16 GB 21~100ä¸ªèŠ‚ç‚¹ 8æ ¸32 GB 100~200ä¸ªèŠ‚ç‚¹ 16æ ¸64 GB 1000ä¸ªèŠ‚ç‚¹ 32æ ¸128GB 1TÂ SSD æ³¨æ„äº‹é¡¹ï¼š\nç”±äºEtcdçš„æ€§èƒ½ç“¶é¢ˆï¼ŒEtcdçš„æ•°æ®å­˜å‚¨ç›˜å°½é‡é€‰æ‹©SSDç£ç›˜ã€‚\nä¸ºäº†å®ç°å¤šæœºæˆ¿å®¹ç¾ï¼Œå¯å°†ä¸‰å°masteråˆ†å¸ƒåœ¨ä¸€ä¸ªå¯ç”¨åŒºä¸‹ä¸‰ä¸ªä¸åŒæœºæˆ¿ã€‚ï¼ˆæœºæˆ¿ä¹‹é—´çš„ç½‘ç»œå»¶è¿Ÿåœ¨10æ¯«ç§’åŠä»¥ä¸‹çº§åˆ«ï¼‰\nç”³è¯·LBæ¥åšmasterèŠ‚ç‚¹çš„è´Ÿè½½å‡è¡¡å®ç°é«˜å¯ç”¨ï¼ŒLBä½œä¸ºapiserverçš„è®¿é—®åœ°å€ã€‚\n1.1. è®¾ç½®é˜²ç«å¢™ç«¯å£ç­–ç•¥ ç”Ÿäº§ç¯å¢ƒè®¾ç½®k8sèŠ‚ç‚¹çš„iptablesç«¯å£è®¿é—®è§„åˆ™ã€‚\n1.1.1. masterèŠ‚ç‚¹ç«¯å£é…ç½® åè®® æ–¹å‘ ç«¯å£èŒƒå›´ ç›®çš„ ä½¿ç”¨è€… TCP å…¥ç«™ 6443 Kubernetes API server æ‰€æœ‰ TCP å…¥ç«™ 2379-2380 etcd server client API kube-apiserver, etcd TCP å…¥ç«™ 10250 Kubelet API è‡ªèº«, æ§åˆ¶é¢ TCP å…¥ç«™ 10259 kube-scheduler è‡ªèº« TCP å…¥ç«™ 10257 kube-controller-manager è‡ªèº« 1.1.2. workerèŠ‚ç‚¹ç«¯å£é…ç½® åè®® æ–¹å‘ ç«¯å£èŒƒå›´ ç›®çš„ ä½¿ç”¨è€… TCP å…¥ç«™ 10250 Kubelet API è‡ªèº«, æ§åˆ¶é¢ TCP å…¥ç«™ 30000-32767 NodePort Services æ‰€æœ‰ æ·»åŠ é˜²ç«å¢™iptablesè§„åˆ™\nmasterèŠ‚ç‚¹å¼€æ”¾6443ã€2379ã€2380ç«¯å£ã€‚\niptables -A INPUT -p tcp -m multiport --dports 6443,2379,2380,10250 -j ACCEPT 1.2. å…³é—­â€‹â€‹swapâ€‹â€‹åˆ†åŒº [root@master ~]#swapoff -a [root@master ~]# [root@master ~]# free -m total used free shared buff/cache available Mem: 976 366 135 6 474 393 Swap: 0 0 0 # swap ä¸€æ ä¸º0ï¼Œè¡¨ç¤ºå·²ç»å…³é—­äº†swap 1.3. å¼€å¯br_netfilterå’Œbridge-nf-call-iptables å‚è€ƒï¼šhttps://imroc.cc/post/202105/why-enable-bridge-nf-call-iptables/\n# è®¾ç½®åŠ è½½br_netfilteræ¨¡å— cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # å¼€å¯bridge-nf-call-iptables ï¼Œè®¾ç½®æ‰€éœ€çš„ sysctl å‚æ•°ï¼Œå‚æ•°åœ¨é‡æ–°å¯åŠ¨åä¿æŒä¸å˜ cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # åº”ç”¨ sysctl å‚æ•°è€Œä¸é‡æ–°å¯åŠ¨ sudo sysctl --system 2. å®‰è£…å®¹å™¨è¿è¡Œæ—¶ åœ¨æ‰€æœ‰ä¸»æœºä¸Šå®‰è£…å®¹å™¨è¿è¡Œæ—¶ï¼Œæ¨èä½¿ç”¨containerdä¸ºruntimeã€‚ä»¥ä¸‹åˆ†åˆ«æ˜¯containerdä¸dockerçš„å®‰è£…å‘½ä»¤ã€‚\n2.1. Containerd 1ã€å‚è€ƒï¼šå®‰è£…containerd\n# for ubuntu apt install -y containerd.io 2ã€ç”Ÿæˆé»˜è®¤é…ç½®\ncontainerd config default \u003e /etc/containerd/config.toml 3ã€ä¿®æ”¹CgroupDriverä¸ºsystemd\nk8så®˜æ–¹æ¨èä½¿ç”¨systemdç±»å‹çš„CgroupDriverã€‚\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true 4ã€é‡å¯containerd\nsystemctl restart containerd 2.2. Docker # for ubuntu apt install -y docker.io å®˜æ–¹å»ºè®®é…ç½®cgroupdriverä¸ºsystemdã€‚\n# ä¿®æ”¹dockerè¿›ç¨‹ç®¡ç†å™¨ vi /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } systemctl daemon-reload \u0026\u0026 systemctl restart docker docker info | grep -i cgroup 2.3. Container Socket è¿è¡Œæ—¶ Unix åŸŸå¥—æ¥å­— Containerd unix:///var/run/containerd/containerd.sock CRI-O unix:///var/run/crio/crio.sock Docker Engine (ä½¿ç”¨ cri-dockerd) unix:///var/run/cri-dockerd.sock 3. å®‰è£…kubeadm,kubelet,kubectl å®‰è£…è„šæœ¬å¯å‚è€ƒä»“åº“ï¼šhttps://github.com/huweihuang/kubeadm-scripts.git\nåœ¨æ‰€æœ‰ä¸»æœºä¸Šå®‰è£…kubeadmï¼Œkubeletï¼Œkubectlã€‚æœ€å¥½ç‰ˆæœ¬ä¸éœ€è¦å®‰è£…çš„k8sçš„ç‰ˆæœ¬ä¸€è‡´ã€‚\n# ä»¥Ubuntuç³»ç»Ÿä¸ºä¾‹ # å®‰è£…ä»“åº“ä¾èµ– sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl # use google registry sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list # or use aliyun registry curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - tee /etc/apt/sources.list.d/kubernetes.list \u003c\u003cEOF deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF # å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„kubeadm, kubelet, kubectl apt-get update apt-get install -y kubelet=1.24.2-00 kubeadm=1.24.2-00 kubectl=1.24.2-00 # æŸ¥è¯¢æœ‰å“ªäº›ç‰ˆæœ¬ apt-cache madison kubeadm ç¦»çº¿ä¸‹è½½å®‰è£…\n#!/bin/bash Version=${Version:-1.24.2} wget https://dl.k8s.io/release/v${Version}/bin/linux/amd64/kubeadm wget https://dl.k8s.io/release/v${Version}/bin/linux/amd64/kubelet wget https://dl.k8s.io/release/v${Version}/bin/linux/amd64/kubectl chmod +x kubeadm kubelet kubectl cp kubeadm kubelet kubectl /usr/bin/ # add kubelet serivce cat \u003e /lib/systemd/system/kubelet.service \u003c\u003c EOF [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/home/ Wants=network-online.target After=network-online.target [Service] ExecStart=/usr/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target EOF mkdir -p /etc/systemd/system/kubelet.service.d cat \u003e /etc/systemd/system/kubelet.service.d/10-kubeadm.conf \u003c\u003c \\EOF # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS EOF systemctl daemon-reload systemctl enable kubelet systemctl restart kubelet 4. é…ç½®kubeadm config å‚è€ƒï¼š\nkubeadm Configuration (v1beta3) | Kubernetes kubeadm Configuration (v1beta2) | Kubernetes 4.1. é…ç½®é¡¹è¯´æ˜ 4.1.1. é…ç½®ç±»å‹ kubeadm configæ”¯æŒä»¥ä¸‹å‡ ç±»é…ç½®ã€‚\napiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰“å°initå’Œjoinçš„é»˜è®¤é…ç½®ã€‚\nkubeadm config print init-defaults kubeadm config print join-defaults 4.1.2. Inité…ç½® kubeadm inité…ç½®ä¸­åªæœ‰InitConfiguration å’Œ ClusterConfiguration æ˜¯å¿…é¡»çš„ã€‚\nInitConfiguration:\napiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration bootstrapTokens: ... nodeRegistration: ... bootstrapTokens nodeRegistration criSocketï¼šruntimeçš„socket nameï¼šèŠ‚ç‚¹åç§° localAPIEndpoint advertiseAddressï¼šapiserverçš„å¹¿æ’­IP bindPortï¼šk8sæ§åˆ¶é¢å®‰å…¨ç«¯å£ ClusterConfiguration:\napiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration networking: ... etcd: ... apiServer: extraArgs: ... extraVolumes: ... ... networking:\npodSubnetï¼šPod CIDRèŒƒå›´ serviceSubnetï¼š service CIDRèŒƒå›´ dnsDomain etcd:\ndataDirï¼šEtcdçš„æ•°æ®å­˜å‚¨ç›®å½• apiserver\ncertSANsï¼šè®¾ç½®é¢å¤–çš„apiserverçš„åŸŸåç­¾åè¯ä¹¦ imageRepositoryï¼šé•œåƒä»“åº“\ncontrolPlaneEndpointï¼šæ§åˆ¶é¢LBçš„åŸŸå\nkubernetesVersionï¼šk8sç‰ˆæœ¬\n4.2. Inité…ç½®ç¤ºä¾‹ åœ¨masterèŠ‚ç‚¹ç”Ÿæˆé»˜è®¤é…ç½®ï¼Œå¹¶ä¿®æ”¹é…ç½®å‚æ•°ã€‚\nkubeadm config print init-defaults \u003e kubeadm-config.yaml ä¿®æ”¹é…ç½®å†…å®¹\napiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 # ä¿®æ”¹ä¸ºapiserverçš„IP æˆ–è€…å»æ‰localAPIEndpointåˆ™ä¼šè¯»å–é»˜è®¤IPã€‚ bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: node taints: null --- apiServer: certSANs: - lb.k8s.domain # æ·»åŠ é¢å¤–çš„apiserverçš„åŸŸå - \u003cvip/lb_ip\u003e timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} # é»˜è®¤ä¸ºcoredns etcd: local: dataDir: /data/etcd # ä¿®æ”¹etcdçš„å­˜å‚¨ç›˜ç›®å½• imageRepository: k8s.gcr.io # ä¿®æ”¹é•œåƒä»“åº“åœ°å€ controlPlaneEndpoint: lb.k8s.domain # ä¿®æ”¹æ§åˆ¶é¢åŸŸå kind: ClusterConfiguration kubernetesVersion: 1.24.0 # k8s ç‰ˆæœ¬ networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # è®¾ç½®podçš„IPèŒƒå›´ scheduler: {} --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd # è®¾ç½®ä¸ºsystemd å®‰è£…å®Œæˆåå¯ä»¥æŸ¥çœ‹kubeadm config\nkubectl get cm -n kube-system kubeadm-config -oyaml 5. å®‰è£…Masteræ§åˆ¶é¢ æå‰æ‹‰å–é•œåƒï¼š\nkubeadm config images pull 5.1. å®‰è£…master sudo kubeadm init --config kubeadm-config.yaml --upload-certs --node-name \u003cnodename\u003e éƒ¨ç½²å‚æ•°è¯´æ˜ï¼š\n--control-plane-endpointï¼šæŒ‡å®šæ§åˆ¶é¢(kube-apiserver)çš„IPæˆ–DNSåŸŸååœ°å€ã€‚\n--apiserver-advertise-addressï¼škube-apiserverçš„IPåœ°å€ã€‚\n--pod-network-cidrï¼špod networkèŒƒå›´ï¼Œæ§åˆ¶é¢ä¼šè‡ªåŠ¨ç»™æ¯ä¸ªèŠ‚ç‚¹åˆ†é…CIDRã€‚\n--service-cidrï¼šserviceçš„IPèŒƒå›´ï¼Œdefault \"10.96.0.0/12\"ã€‚\n--kubernetes-versionï¼šæŒ‡å®šk8sçš„ç‰ˆæœ¬ã€‚\n--image-repositoryï¼šæŒ‡å®šk8sé•œåƒä»“åº“åœ°å€ã€‚\n--upload-certs ï¼šæ ‡å¿—ç”¨æ¥å°†åœ¨æ‰€æœ‰æ§åˆ¶å¹³é¢å®ä¾‹ä¹‹é—´çš„å…±äº«è¯ä¹¦ä¸Šä¼ åˆ°é›†ç¾¤ã€‚\n--node-nameï¼šhostname-overrideï¼Œä½œä¸ºèŠ‚ç‚¹åç§°ã€‚\næ‰§è¡Œå®Œæ¯•ä¼šè¾“å‡ºæ·»åŠ masterå’Œæ·»åŠ workerçš„å‘½ä»¤å¦‚ä¸‹ï¼š\n... You can now join any number of control-plane node by running the following command on each as a root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 5.2. æ·»åŠ å…¶ä»–master æ·»åŠ masterå’Œæ·»åŠ workerçš„å·®åˆ«åœ¨äºæ·»åŠ masterå¤šäº†--control-plane å‚æ•°æ¥è¡¨ç¤ºæ·»åŠ ç±»å‹ä¸ºmasterã€‚\nkubeadm join \u003ccontrol-plane-endpoint\u003e:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e \\ --control-plane --certificate-key \u003ccertificate-key\u003e \\ --node-name \u003cnodename\u003e 6. æ·»åŠ NodeèŠ‚ç‚¹ kubeadm join \u003ccontrol-plane-endpoint\u003e:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e \\ --cri-socket /run/containerd/containerd.sock \\ --node-name \u003cnodename\u003e 7. å®‰è£…ç½‘ç»œæ’ä»¶ ## å¦‚æœå®‰è£…ä¹‹ånodeçš„çŠ¶æ€éƒ½æ”¹ä¸ºreadyï¼Œå³ä¸ºæˆåŠŸ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kubectl apply -f ./kube-flannel.yml kubectl get nodes å¦‚æœPod CIDRçš„ç½‘æ®µä¸æ˜¯10.244.0.0/16ï¼Œåˆ™éœ€è¦åŠ flannelé…ç½®ä¸­çš„ç½‘æ®µæ›´æ”¹ä¸ºä¸Pod CIDRçš„ç½‘æ®µä¸€è‡´ã€‚\n7.1. é—®é¢˜ Warning FailedCreatePodSandBox 4m6s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"300d9b570cc1e23b6335c407b8e7d0ef2c74dc2fe5d7a110678c2dc919c62edf\": plugin type=\"flannel\" failed (add): failed to delegate add: failed to set bridge addr: \"cni0\" already has an IP address different from 10.244.3.1/24 åŸå› ï¼š\nå®¿ä¸»æœºèŠ‚ç‚¹æœ‰cni0ç½‘å¡ï¼Œä¸”ç½‘å¡çš„IPæ®µä¸flannelçš„CIDRç½‘æ®µä¸åŒï¼Œå› æ­¤éœ€è¦åˆ é™¤è¯¥ç½‘å¡ï¼Œè®©å…¶é‡å»ºã€‚\næŸ¥çœ‹cni0ç½‘å¡\n# ifconfig cni0 |grep -w inet inet 10.244.5.1 netmask 255.255.255.0 broadcast 10.244.116.255 æŸ¥çœ‹flannelé…ç½®\n# cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.116.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true å‘ç°cni0 IPä¸FLANNEL_SUBNETç½‘æ®µä¸ä¸€è‡´ï¼Œå› æ­¤åˆ é™¤cni0é‡å»ºã€‚\nè§£å†³ï¼š\nifconfig cni0 down ip link delete cni0 8. éƒ¨ç½²dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml é•œåƒï¼š kubernetesui/dashboard:v2.5.0\né»˜è®¤ç«¯å£ï¼š8443\nç™»å½•é¡µé¢éœ€è¦å¡«å…¥tokenæˆ–kubeconfig\nå‚è€ƒï¼šdashboard/creating-sample-user\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard åˆ›å»ºç”¨æˆ·\nkubectl -n kubernetes-dashboard create token admin-user 9. é‡ç½®éƒ¨ç½² # kubeadmé‡ç½® kubeadm reset # æ¸…ç©ºæ•°æ®ç›®å½• rm -fr /data/etcd rm -fr /etc/kubernetes rm -fr ~/.kube/ åˆ é™¤flannel\nifconfig cni0 down ip link delete cni0 ifconfig flannel.1 down ip link delete flannel.1 rm -rf /var/lib/cni/ rm -f /etc/cni/net.d/* 10. é—®é¢˜æ’æŸ¥ 10.1. kubeadm tokenè¿‡æœŸ é—®é¢˜æè¿°:\næ·»åŠ èŠ‚ç‚¹æ—¶æŠ¥ä»¥ä¸‹é”™è¯¯ï¼š\n[discovery] The cluster-info ConfigMap does not yet contain a JWS signature for token ID \"abcdef\", will try again åŸå› ï¼štokenè¿‡æœŸï¼Œåˆå§‹åŒ–tokenåä¼šåœ¨24å°æ—¶å€™ä¼šè¢«masteråˆ é™¤ã€‚\nè§£å†³åŠæ³•ï¼š\n# é‡æ–°ç”Ÿæˆtoken kubeadm token create --print-join-command kubeadm token list # kubeadm token create oumnnc.aqlxuvdbntlvzoiv # é‡æ–°ç”Ÿæˆhash openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' åŸºäºæ–°ç”Ÿæˆçš„tokené‡æ–°æ·»åŠ èŠ‚ç‚¹ã€‚\n10.2. ä¿®æ”¹kubeadm joinçš„master IPæˆ–ç«¯å£ kubeadm joinå‘½ä»¤ä¼šå»kube-publicå‘½åç©ºé—´è·å–åä¸ºcluster-infoçš„ConfigMapã€‚å¦‚æœéœ€è¦ä¿®æ”¹kubeadm joinä½¿ç”¨çš„masterçš„IPæˆ–ç«¯å£ï¼Œåˆ™éœ€è¦ä¿®æ”¹cluster-infoçš„configmapã€‚\n# æŸ¥çœ‹cluster-info kubectl -n kube-public get configmaps cluster-info -o yaml # ä¿®æ”¹cluster-info kubectl -n kube-public edit configmaps cluster-info ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„serverå­—æ®µ\nclusters: - cluster: certificate-authority-data: xxx server: https://lb.k8s.domain:36443 name: \"\" æ‰§è¡Œkubeadm joinçš„å‘½ä»¤æ—¶æŒ‡å®šæ–°ä¿®æ”¹çš„masteråœ°å€ã€‚\n10.3. conntrack not found [preflight] Some fatal errors occurred: [ERROR FileExisting-conntrack]: conntrack not found in system path è§£å†³æ–¹æ³•ï¼š\napt -y install conntrack 10.4. Kubelet: unable to determine runtime API version Error: failed to run Kubelet: unable to determine runtime API version: rpc error:code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix: missing address\" è§£å†³æ–¹æ³•ï¼š\næ£€æŸ¥kubeletçš„å¯åŠ¨å‚æ•°ï¼Œå¯ä»¥ç”¨äºŒè¿›åˆ¶ç›´æ¥æ·»åŠ å‚æ•°debug\n# æŸ¥çœ‹å¯åŠ¨å‚æ•°æ˜¯å¦é—æ¼ï¼Œæ¯”å¦‚10-kubeadm.conf æ–‡ä»¶å‚æ•°ç¼ºå¤± systemctl cat --no-pager kubelet cat /lib/systemd/system/kubelet.service cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf å‚è€ƒï¼š\nåˆ©ç”¨ kubeadm åˆ›å»ºé«˜å¯ç”¨é›†ç¾¤ | Kubernetes ä½¿ç”¨ kubeadm åˆ›å»ºé›†ç¾¤ | Kubernetes é«˜å¯ç”¨æ‹“æ‰‘é€‰é¡¹ | Kubernetes kubeadm init | Kubernetes v1.24.2|kubeadm|v1beta3 Installing kubeadm | Kubernetes Ports and Protocols | Kubernetes å®¹å™¨è¿è¡Œæ—¶ | Kubernetes https://github.com/Mirantis/cri-dockerd é…ç½® cgroup é©±åŠ¨|Kubernetes GitHub: flannel is a network fabric for containers éƒ¨ç½²å’Œè®¿é—® Kubernetes ä»ªè¡¨æ¿ï¼ˆDashboardï¼‰ | Kubernetes ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸ºåŸºäºkubeadmæ­å»ºç”Ÿäº§ç¯å¢ƒçº§åˆ«é«˜å¯ç”¨çš„k8sé›†ç¾¤ã€‚\n1. ç¯å¢ƒå‡†å¤‡ 1.0. masterç¡¬ä»¶é…ç½® å‚è€ƒï¼š\nMaster â€¦","ref":"/kubernetes-notes/setup/installer/install-k8s-by-kubeadm/","tags":["Kubernetes"],"title":"ä½¿ç”¨kubeadméƒ¨ç½²ç”Ÿäº§ç¯å¢ƒkubernetesé›†ç¾¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/summary/","tags":"","title":"è¯­è¨€æ¦‚è¿°"},{"body":"æºç æ•´ä½“ç»“æ„å›¾ ","categories":"","description":"","excerpt":"æºç æ•´ä½“ç»“æ„å›¾ ","ref":"/k8s-source-code-analysis/kube-controller-manager/controller-manager-xmind/","tags":"","title":"controller-manager æºç æ€ç»´å¯¼å›¾"},{"body":"æºç æ•´ä½“ç»“æ„å›¾ ","categories":"","description":"","excerpt":"æºç æ•´ä½“ç»“æ„å›¾ ","ref":"/k8s-source-code-analysis/kube-scheduler/scheduler-xmind/","tags":"","title":"kube-scheduler æºç æ€ç»´å¯¼å›¾"},{"body":"æºç æ•´ä½“ç»“æ„å›¾ ","categories":"","description":"","excerpt":"æºç æ•´ä½“ç»“æ„å›¾ ","ref":"/k8s-source-code-analysis/kubelet/kubelet-xmind/","tags":"","title":"kubelet æºç æ€ç»´å¯¼å›¾"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/paas/","tags":"","title":"äº‘åŸç”Ÿä½“ç³»"},{"body":"1. NodeSelector 1.1. æ¦‚å¿µ å¦‚æœéœ€è¦é™åˆ¶Podåˆ°æŒ‡å®šçš„Nodeä¸Šè¿è¡Œï¼Œåˆ™å¯ä»¥ç»™Nodeæ‰“æ ‡ç­¾å¹¶ç»™Podé…ç½®NodeSelectorã€‚\n1.2. ä½¿ç”¨æ–¹å¼ 1.2.1. ç»™Nodeæ‰“æ ‡ç­¾ # get nodeçš„name kubectl get nodes # è®¾ç½®Label kubectl label nodes \u003cnode-name\u003e \u003clabel-key\u003e=\u003clabel-value\u003e # ä¾‹å¦‚ kubectl label nodes node-1 disktype=ssd # æŸ¥çœ‹Nodeçš„Label kubectl get nodes --show-labels # åˆ é™¤Nodeçš„label kubectl label node \u003cnode-name\u003e \u003clabel-key\u003e- 1.2.2. ç»™Podè®¾ç½®NodeSelector apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd # å¯¹åº”Nodeçš„Label 1.3. äº²å’Œæ€§ï¼ˆAffinityï¼‰å’Œåäº²å’Œæ€§ï¼ˆAnti-affinityï¼‰ å¾…è¡¥å……\n2. Taint å’Œ Toleration 2.1. æ¦‚å¿µ nodeSelectorå¯ä»¥é€šè¿‡æ‰“æ ‡ç­¾çš„å½¢å¼è®©Podè¢«è°ƒåº¦åˆ°æŒ‡å®šçš„Nodeä¸Šï¼ŒTaint åˆ™ç›¸åï¼Œå®ƒä½¿èŠ‚ç‚¹èƒ½å¤Ÿæ’æ–¥ä¸€ç±»ç‰¹å®šçš„Podï¼Œé™¤éPodè¢«æŒ‡å®šäº†tolerationçš„æ ‡ç­¾ã€‚ï¼ˆtaintå³æ±¡ç‚¹ï¼ŒNodeè¢«æ‰“ä¸Šæ±¡ç‚¹ï¼›åªæœ‰å®¹å¿[toleration]è¿™äº›æ±¡ç‚¹çš„Podæ‰å¯èƒ½è¢«è°ƒåº¦åˆ°è¯¥Nodeï¼‰ã€‚\n2.2. ä½¿ç”¨æ–¹å¼ 2.2.1. kubectl taint # ç»™èŠ‚ç‚¹å¢åŠ ä¸€ä¸ªtaintï¼Œå®ƒçš„keyæ˜¯\u003ckey\u003eï¼Œvalueæ˜¯\u003cvalue\u003eï¼Œeffectæ˜¯NoScheduleã€‚ kubectl taint nodes \u003cnode_name\u003e \u003ckey\u003e=\u003cvalue\u003e:NoSchedule åªæœ‰æ‹¥æœ‰å’Œè¿™ä¸ªtaintç›¸åŒ¹é…çš„tolerationçš„podæ‰èƒ½å¤Ÿè¢«åˆ†é…åˆ°Â node_nameÂ è¿™ä¸ªèŠ‚ç‚¹ã€‚\nä¾‹å¦‚ï¼Œåœ¨ PodSpec ä¸­å®šä¹‰ pod çš„ tolerationï¼š\ntolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\" tolerations: - key: \"key\" operator: \"Exists\" effect: \"NoSchedule\" 2.2.2. åŒ¹é…è§„åˆ™ï¼š ä¸€ä¸ª toleration å’Œä¸€ä¸ª taint ç›¸â€œåŒ¹é…â€æ˜¯æŒ‡å®ƒä»¬æœ‰ä¸€æ ·çš„ key å’Œ effect ï¼Œå¹¶ä¸”ï¼š\nå¦‚æœ operator æ˜¯ Exists ï¼ˆæ­¤æ—¶ toleration ä¸èƒ½æŒ‡å®š valueï¼‰ å¦‚æœ operator æ˜¯ Equal ï¼Œåˆ™å®ƒä»¬çš„ value åº”è¯¥ç›¸ç­‰ ç‰¹æ®Šæƒ…å†µï¼š\nå¦‚æœä¸€ä¸ª toleration çš„ key ä¸ºç©ºä¸” operator ä¸º Exists ï¼Œè¡¨ç¤ºè¿™ä¸ª toleration ä¸ä»»æ„çš„ key ã€ value å’Œ effect éƒ½åŒ¹é…ï¼Œå³è¿™ä¸ª toleration èƒ½å®¹å¿ä»»æ„ taintã€‚\ntolerations: - operator: \"Exists\" å¦‚æœä¸€ä¸ª toleration çš„ effect ä¸ºç©ºï¼Œåˆ™ key å€¼ä¸ä¹‹ç›¸åŒçš„ç›¸åŒ¹é… taint çš„ effect å¯ä»¥æ˜¯ä»»æ„å€¼ã€‚\ntolerations: - key: \"key\" operator: \"Exists\" ä¸€ä¸ªèŠ‚ç‚¹å¯ä»¥è®¾ç½®å¤šä¸ªtaintï¼Œä¸€ä¸ªpodä¹Ÿå¯ä»¥è®¾ç½®å¤šä¸ªtolerationã€‚Kubernetes å¤„ç†å¤šä¸ª taint å’Œ toleration çš„è¿‡ç¨‹å°±åƒä¸€ä¸ªè¿‡æ»¤å™¨ï¼šä»ä¸€ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰ taint å¼€å§‹éå†ï¼Œè¿‡æ»¤æ‰é‚£äº› pod ä¸­å­˜åœ¨ä¸ä¹‹ç›¸åŒ¹é…çš„ toleration çš„ taintã€‚ä½™ä¸‹æœªè¢«è¿‡æ»¤çš„ taint çš„ effect å€¼å†³å®šäº† pod æ˜¯å¦ä¼šè¢«åˆ†é…åˆ°è¯¥èŠ‚ç‚¹ï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹æƒ…å†µï¼š\nå¦‚æœæœªè¢«è¿‡æ»¤çš„ taint ä¸­å­˜åœ¨ä¸€ä¸ªä»¥ä¸Š effect å€¼ä¸º NoSchedule çš„ taintï¼Œåˆ™ Kubernetes ä¸ä¼šå°† pod åˆ†é…åˆ°è¯¥èŠ‚ç‚¹ã€‚ å¦‚æœæœªè¢«è¿‡æ»¤çš„ taint ä¸­ä¸å­˜åœ¨ effect å€¼ä¸º NoSchedule çš„ taintï¼Œä½†æ˜¯å­˜åœ¨ effect å€¼ä¸º PreferNoSchedule çš„ taintï¼Œåˆ™ Kubernetes ä¼šå°è¯•å°† pod åˆ†é…åˆ°è¯¥èŠ‚ç‚¹ã€‚ å¦‚æœæœªè¢«è¿‡æ»¤çš„ taint ä¸­å­˜åœ¨ä¸€ä¸ªä»¥ä¸Š effect å€¼ä¸º NoExecute çš„ taintï¼Œåˆ™ Kubernetes ä¸ä¼šå°† pod åˆ†é…åˆ°è¯¥èŠ‚ç‚¹ï¼ˆå¦‚æœ pod è¿˜æœªåœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œï¼‰ï¼Œæˆ–è€…å°† pod ä»è¯¥èŠ‚ç‚¹é©±é€ï¼ˆå¦‚æœ pod å·²ç»åœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œï¼‰ã€‚ 2.2.3. effectçš„ç±»å‹ NoScheduleï¼šåªæœ‰æ‹¥æœ‰å’Œè¿™ä¸ª taint ç›¸åŒ¹é…çš„ toleration çš„ pod æ‰èƒ½å¤Ÿè¢«åˆ†é…åˆ°è¿™ä¸ªèŠ‚ç‚¹ã€‚\nPreferNoScheduleï¼šç³»ç»Ÿä¼šå°½é‡é¿å…å°† pod è°ƒåº¦åˆ°å­˜åœ¨å…¶ä¸èƒ½å®¹å¿ taint çš„èŠ‚ç‚¹ä¸Šï¼Œä½†è¿™ä¸æ˜¯å¼ºåˆ¶çš„ã€‚\nNoExecuteÂ ï¼šä»»ä½•ä¸èƒ½å¿å—è¿™ä¸ª taint çš„ pod éƒ½ä¼šé©¬ä¸Šè¢«é©±é€ï¼Œä»»ä½•å¯ä»¥å¿å—è¿™ä¸ª taint çš„ pod éƒ½ä¸ä¼šè¢«é©±é€ã€‚Podå¯æŒ‡å®šå±æ€§Â tolerationSecondsÂ çš„å€¼ï¼Œè¡¨ç¤ºpod è¿˜èƒ½ç»§ç»­åœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œçš„æ—¶é—´ã€‚\ntolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" tolerationSeconds: 3600 2.3. ä½¿ç”¨åœºæ™¯ 2.3.1. ä¸“ç”¨èŠ‚ç‚¹ kubectl taint nodes \u003cnodename\u003e dedicated=\u003cgroupName\u003e:NoSchedule å…ˆç»™Nodeæ·»åŠ taintï¼Œç„¶åç»™Podæ·»åŠ ç›¸å¯¹åº”çš„ tolerationï¼Œåˆ™è¯¥Podå¯è°ƒåº¦åˆ°taintçš„Nodeï¼Œä¹Ÿå¯è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ã€‚\nå¦‚æœæƒ³è®©Podåªè°ƒåº¦æŸäº›èŠ‚ç‚¹ä¸”æŸäº›èŠ‚ç‚¹åªæ¥å—å¯¹åº”çš„Podï¼Œåˆ™éœ€è¦åœ¨Nodeä¸Šæ·»åŠ Labelï¼ˆä¾‹å¦‚ï¼šdedicated=groupNameï¼‰ï¼ŒåŒæ—¶ç»™Podçš„nodeSelectoræ·»åŠ å¯¹åº”çš„Labelã€‚\n2.3.2. ç‰¹æ®Šç¡¬ä»¶èŠ‚ç‚¹ å¦‚æœæŸäº›èŠ‚ç‚¹é…ç½®äº†ç‰¹æ®Šç¡¬ä»¶ï¼ˆä¾‹å¦‚CPUï¼‰ï¼Œå¸Œæœ›ä¸ä½¿ç”¨è¿™äº›ç‰¹æ®Šç¡¬ä»¶çš„Podä¸è¢«è°ƒåº¦è¯¥Nodeï¼Œä»¥ä¾¿ä¿ç•™å¿…è¦èµ„æºã€‚å³å¯ç»™Nodeè®¾ç½®taintå’Œlabelï¼ŒåŒæ—¶ç»™Podè®¾ç½®tolerationå’Œlabelæ¥ä½¿å¾—è¿™äº›Nodeä¸“é—¨è¢«æŒ‡å®šPodä½¿ç”¨ã€‚\n# kubectl taint kubectl taint nodes nodename special=true:NoSchedule # æˆ–è€… kubectl taint nodes nodename special=true:PreferNoSchedule 2.3.3. åŸºäºtainté©±é€ effect å€¼ NoExecute ï¼Œå®ƒä¼šå½±å“å·²ç»åœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œçš„ podï¼Œå³æ ¹æ®ç­–ç•¥å¯¹Podè¿›è¡Œé©±é€ã€‚\nå¦‚æœ pod ä¸èƒ½å¿å—effect å€¼ä¸º NoExecute çš„ taintï¼Œé‚£ä¹ˆ pod å°†é©¬ä¸Šè¢«é©±é€ å¦‚æœ pod èƒ½å¤Ÿå¿å—effect å€¼ä¸º NoExecute çš„ taintï¼Œä½†æ˜¯åœ¨ toleration å®šä¹‰ä¸­æ²¡æœ‰æŒ‡å®š tolerationSecondsï¼Œåˆ™ pod è¿˜ä¼šä¸€ç›´åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œã€‚ å¦‚æœ pod èƒ½å¤Ÿå¿å—effect å€¼ä¸º NoExecute çš„ taintï¼Œè€Œä¸”æŒ‡å®šäº† tolerationSecondsï¼Œåˆ™ pod è¿˜èƒ½åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œè¿™ä¸ªæŒ‡å®šçš„æ—¶é—´é•¿åº¦ã€‚ å‚è€ƒï¼š\nhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/\nhttps://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n","categories":"","description":"","excerpt":"1. NodeSelector 1.1. æ¦‚å¿µ å¦‚æœéœ€è¦é™åˆ¶Podåˆ°æŒ‡å®šçš„Nodeä¸Šè¿è¡Œï¼Œåˆ™å¯ä»¥ç»™Nodeæ‰“æ ‡ç­¾å¹¶ç»™Podé… â€¦","ref":"/kubernetes-notes/operation/node/nodeselector-and-taint/","tags":["Kubernetes"],"title":"æŒ‡å®šèŠ‚ç‚¹è°ƒåº¦ä¸éš”ç¦»"},{"body":"èµ„æºé…é¢ï¼ˆResourceQuotaï¼‰ ResourceQuotaå¯¹è±¡ç”¨æ¥å®šä¹‰æŸä¸ªå‘½åç©ºé—´ä¸‹æ‰€æœ‰èµ„æºçš„ä½¿ç”¨é™é¢ï¼Œå…¶å®åŒ…æ‹¬ï¼š\nè®¡ç®—èµ„æºçš„é…é¢ å­˜å‚¨èµ„æºçš„é…é¢ å¯¹è±¡æ•°é‡çš„é…é¢ å¦‚æœé›†ç¾¤çš„æ€»å®¹é‡å°äºå‘½åç©ºé—´çš„é…é¢æ€»é¢ï¼Œå¯èƒ½ä¼šäº§ç”Ÿèµ„æºç«äº‰ã€‚è¿™æ—¶ä¼šæŒ‰ç…§å…ˆåˆ°å…ˆå¾—æ¥å¤„ç†ã€‚ èµ„æºç«äº‰å’Œé…é¢çš„æ›´æ–°éƒ½ä¸ä¼šå½±å“å·²ç»åˆ›å»ºå¥½çš„èµ„æºã€‚\n1. å¯åŠ¨èµ„æºé…é¢ Kubernetes çš„ä¼—å¤šå‘è¡Œç‰ˆæœ¬é»˜è®¤å¼€å¯äº†èµ„æºé…é¢çš„æ”¯æŒã€‚å½“åœ¨apiserverçš„--admission-controlé…ç½®ä¸­æ·»åŠ ResourceQuotaå‚æ•°åï¼Œä¾¿å¯ç”¨äº†ã€‚ å½“ä¸€ä¸ªå‘½åç©ºé—´ä¸­å«æœ‰ResourceQuotaå¯¹è±¡æ—¶ï¼Œèµ„æºé…é¢å°†å¼ºåˆ¶æ‰§è¡Œã€‚\n2. è®¡ç®—èµ„æºé…é¢ å¯ä»¥åœ¨ç»™å®šçš„å‘½åç©ºé—´ä¸­é™åˆ¶å¯ä»¥è¯·æ±‚çš„è®¡ç®—èµ„æºï¼ˆcompute resourcesï¼‰çš„æ€»é‡ã€‚\nèµ„æºåç§° æè¿° cpu éç»ˆæ­¢æ€çš„æ‰€æœ‰pod, cpuè¯·æ±‚æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ limits.cpu éç»ˆæ­¢æ€çš„æ‰€æœ‰podï¼Œ cpué™åˆ¶æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ limits.memory éç»ˆæ­¢æ€çš„æ‰€æœ‰pod, å†…å­˜é™åˆ¶æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ memory éç»ˆæ­¢æ€çš„æ‰€æœ‰pod, å†…å­˜è¯·æ±‚æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ requests.cpu éç»ˆæ­¢æ€çš„æ‰€æœ‰pod, cpuè¯·æ±‚æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ requests.memory éç»ˆæ­¢æ€çš„æ‰€æœ‰pod, å†…å­˜è¯·æ±‚æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ 3. å­˜å‚¨èµ„æºé…é¢ å¯ä»¥åœ¨ç»™å®šçš„å‘½åç©ºé—´ä¸­é™åˆ¶å¯ä»¥è¯·æ±‚çš„å­˜å‚¨èµ„æºï¼ˆstorage resourcesï¼‰çš„æ€»é‡ã€‚\nèµ„æºåç§° æè¿° requests.storage æ‰€æœ‰PVC, å­˜å‚¨è¯·æ±‚æ€»é‡ä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ persistentvolumeclaims å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„PVCï¼ˆpersistent volume claimsï¼‰æ€»æ•°ã€‚ .storageclass.storage.k8s.io/requests.storage å’Œè¯¥å­˜å‚¨ç±»å…³è”çš„æ‰€æœ‰PVC, å­˜å‚¨è¯·æ±‚æ€»å’Œä¸èƒ½è¶…å‡ºæ­¤å€¼ã€‚ .storageclass.storage.k8s.io/persistentvolumeclaims å’Œè¯¥å­˜å‚¨ç±»å…³è”çš„æ‰€æœ‰PVCï¼Œå‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„PVCï¼ˆpersistent volume claimsï¼‰æ€»æ•°ã€‚ 4. å¯¹è±¡æ•°é‡çš„é…é¢ èµ„æºåç§° æè¿° congfigmaps å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„é…ç½®æ˜ å°„çš„æ€»æ•°ã€‚ persistentvolumeclaims å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„PVCæ€»æ•°ã€‚ pods å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„éç»ˆæ­¢æ€çš„podæ€»æ•°ã€‚å¦‚æœä¸€ä¸ªpodçš„status.phase æ˜¯ Failed, Succeeded, åˆ™è¯¥podå¤„äºç»ˆæ­¢æ€ã€‚ replicationcontrollers å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„rcæ€»æ•°ã€‚ resourcequotas å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„èµ„æºé…é¢ï¼ˆresource quotasï¼‰æ€»æ•°ã€‚ services å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„æœåŠ¡æ€»æ•°é‡ã€‚ services.loadbalancers å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„æœåŠ¡çš„è´Ÿè½½å‡è¡¡çš„æ€»æ•°é‡ã€‚ services.nodeports å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„æœåŠ¡çš„ä¸»æœºæ¥å£çš„æ€»æ•°é‡ã€‚ secrets å‘½åç©ºé—´ä¸­å¯ä»¥å­˜åœ¨çš„secretsçš„æ€»æ•°é‡ã€‚ ä¾‹å¦‚ï¼šå¯ä»¥å®šä¹‰podçš„é™é¢æ¥é¿å…æŸç”¨æˆ·æ¶ˆè€—è¿‡å¤šçš„Pod IPsã€‚\n5. é™é¢çš„ä½œç”¨åŸŸ ä½œç”¨åŸŸ æè¿° Terminating åŒ¹é… spec.activeDeadlineSeconds \u003e= 0 çš„pod NotTerminating åŒ¹é… spec.activeDeadlineSeconds is nil çš„pod BestEffort åŒ¹é…å…·æœ‰æœ€ä½³æœåŠ¡è´¨é‡çš„pod NotBestEffort åŒ¹é…å…·æœ‰éæœ€ä½³æœåŠ¡è´¨é‡çš„pod 6. requestå’Œlimit å½“åˆ†é…è®¡ç®—èµ„æºæ—¶ï¼Œæ¯ä¸ªå®¹å™¨å¯ä»¥ä¸ºcpuæˆ–è€…å†…å­˜æŒ‡å®šä¸€ä¸ªè¯·æ±‚å€¼å’Œä¸€ä¸ªé™åº¦å€¼ã€‚å¯ä»¥é…ç½®é™é¢å€¼æ¥é™åˆ¶å®ƒä»¬ä¸­çš„ä»»ä½•ä¸€ä¸ªå€¼ã€‚ å¦‚æœæŒ‡å®šäº†requests.cpu æˆ–è€… requests.memoryçš„é™é¢å€¼ï¼Œé‚£ä¹ˆå°±è¦æ±‚ä¼ å…¥çš„æ¯ä¸€ä¸ªå®¹å™¨æ˜¾å¼çš„æŒ‡å®šè¿™äº›èµ„æºçš„è¯·æ±‚ã€‚å¦‚æœæŒ‡å®šäº†limits.cpuæˆ–è€…limits.memoryï¼Œé‚£ä¹ˆå°±è¦æ±‚ä¼ å…¥çš„æ¯ä¸€ä¸ªå®¹å™¨æ˜¾å¼çš„æŒ‡å®šè¿™äº›èµ„æºçš„é™åº¦ã€‚\n7. æŸ¥çœ‹å’Œè®¾ç½®é…é¢ # åˆ›å»ºnamespace $ kubectl create namespace myspace # åˆ›å»ºresourcequota $ cat \u003c\u003cEOF \u003e compute-resources.yaml apiVersion: v1 kind: ResourceQuota metadata: name: compute-resources spec: hard: pods: \"4\" requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi EOF $ kubectl create -f ./compute-resources.yaml --namespace=myspace # æŸ¥è¯¢resourcequota $ kubectl get quota --namespace=myspace NAME AGE compute-resources 30s # æŸ¥è¯¢resourcequotaçš„è¯¦ç»†ä¿¡æ¯ $ kubectl describe quota compute-resources --namespace=myspace Name: compute-resources Namespace: myspace Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi pods 0 4 requests.cpu 0 1 requests.memory 0 1Gi 8. é…é¢å’Œé›†ç¾¤å®¹é‡ èµ„æºé…é¢å¯¹è±¡ä¸é›†ç¾¤å®¹é‡æ— å…³ï¼Œå®ƒä»¬ä»¥ç»å¯¹å•ä½è¡¨ç¤ºã€‚å³å¢åŠ èŠ‚ç‚¹çš„èµ„æºå¹¶ä¸ä¼šå¢åŠ å·²ç»é…ç½®çš„namespaceçš„èµ„æºã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/policy/resource-quotas/ ","categories":"","description":"","excerpt":"èµ„æºé…é¢ï¼ˆResourceQuotaï¼‰ ResourceQuotaå¯¹è±¡ç”¨æ¥å®šä¹‰æŸä¸ªå‘½åç©ºé—´ä¸‹æ‰€æœ‰èµ„æºçš„ä½¿ç”¨é™é¢ï¼Œå…¶å®åŒ…æ‹¬ï¼š\nè®¡ç®—èµ„æºçš„é…é¢  â€¦","ref":"/kubernetes-notes/resource/resource-quota/","tags":["Kubernetes"],"title":"èµ„æºé…é¢"},{"body":"1. VXLANç®€ä»‹ VXLANï¼ˆVirtual Extensible LANï¼‰æ˜¯ä¸€ç§ç½‘ç»œè™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸäºŒå±‚ç½‘ç»œæ‰©å±•çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ä¸­å¿ƒå¤§è§„æ¨¡éƒ¨ç½²ä¸­ã€‚å®ƒé€šè¿‡éš§é“æŠ€æœ¯å°†äºŒå±‚ä»¥å¤ªç½‘å¸§å°è£…åœ¨ä¸‰å±‚UDPåŒ…ä¸­ï¼Œå®ç°äº†è·¨ä¸‰å±‚ç½‘ç»œçš„äºŒå±‚ç½‘ç»œå»¶å±•ã€‚\n1.1. VXLANçš„åŸºæœ¬æ¦‚å¿µ ç›®çš„ï¼šè§£å†³äºŒå±‚ç½‘ç»œæ‰©å±•çš„é—®é¢˜ï¼Œä¾‹å¦‚VLANçš„æ•°é‡é™åˆ¶ï¼ˆä¼ ç»ŸVLAN ID åªèƒ½æ”¯æŒ4096ï¼ˆ2çš„12æ¬¡æ–¹ï¼‰ä¸ªï¼‰ã€‚ å°è£…åè®®ï¼šVXLANå°†äºŒå±‚ä»¥å¤ªç½‘å¸§å°è£…ä¸ºUDPæ•°æ®åŒ…ï¼ˆå³VXLANéš§é“ï¼‰ã€‚ VXLANç½‘ç»œæ ‡è¯†ï¼š VXLANä½¿ç”¨24ä½çš„VXLANç½‘ç»œæ ‡è¯†ï¼ˆVNIï¼ŒVirtual Network Identifierï¼‰ï¼Œæ”¯æŒå¤šè¾¾16,777,216ï¼ˆ2çš„24æ¬¡æ–¹ï¼‰ä¸ªè™šæ‹Ÿç½‘ç»œã€‚ æ¯ä¸ªVNIå¯¹åº”ä¸€ä¸ªè™šæ‹Ÿçš„äºŒå±‚å¹¿æ’­åŸŸï¼ˆç±»ä¼¼äºä¼ ç»Ÿçš„VLANï¼‰ã€‚ 1.2.VXLANåº”ç”¨åœºæ™¯ å¤šç§Ÿæˆ·æ•°æ®ä¸­å¿ƒï¼šä¸ºä¸åŒç§Ÿæˆ·æä¾›é€»è¾‘éš”ç¦»çš„è™šæ‹Ÿç½‘ç»œã€‚ æ··åˆäº‘å’Œè·¨æ•°æ®ä¸­å¿ƒè¿æ¥ï¼šæ‰©å±•äºŒå±‚ç½‘ç»œåˆ°ä¸åŒä½ç½®çš„æ•°æ®ä¸­å¿ƒã€‚ å®¹å™¨ç½‘ç»œï¼šåœ¨Kubernetesç­‰å¹³å°ä¸Šï¼Œç”¨VXLANæ„å»ºè·¨èŠ‚ç‚¹çš„Podç½‘ç»œã€‚ 1.3. VXLANçš„ä¼˜ç‚¹ å¯æ‰©å±•æ€§ï¼š æ”¯æŒå¤§é‡è™šæ‹Ÿç½‘ç»œï¼ˆ16Mï¼‰ã€‚ è·¨ä¸‰å±‚ç½‘ç»œæ‰©å±•äºŒå±‚ç½‘ç»œï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®ä¸­å¿ƒã€‚ ç½‘ç»œéš”ç¦»ï¼šé€šè¿‡VNIå®ç°ç½‘ç»œéš”ç¦»ï¼Œé€‚åˆå¤šç§Ÿæˆ·åœºæ™¯ã€‚ çµæ´»æ€§ï¼šVXLANåœ¨IPç½‘ç»œä¸­è¿è¡Œï¼Œä¸ä¾èµ–åº•å±‚çš„ç‰©ç†æ‹“æ‰‘ã€‚ 1.4. VXLANçš„å±€é™æ€§ æ€§èƒ½å¼€é”€ï¼šå°è£…å’Œè§£å°è£…å¢åŠ äº†CPUè´Ÿè½½ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶å®ç°ä¸­ã€‚ å¤æ‚æ€§ï¼šéœ€è¦é¢å¤–é…ç½®VTEPå’Œä¸‰å±‚ç½‘ç»œï¼Œç»´æŠ¤æˆæœ¬è¾ƒé«˜ã€‚ MTUé—®é¢˜ï¼šVXLANå°è£…å¢åŠ äº†æ•°æ®åŒ…é•¿åº¦ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç½‘ç»œçš„MTUï¼ˆé€šå¸¸ä¸º1600å­—èŠ‚æˆ–æ›´å¤§ï¼‰ã€‚ 2. VXLANçš„åŸç† 2.1. VXLANçš„å…³é”®ç»„æˆ VTEPï¼ˆVXLAN Tunnel Endpointï¼‰ ï¼š VTEPæ˜¯VXLANéš§é“çš„èµ·ç‚¹å’Œç»ˆç‚¹ï¼Œç”¨äºå°è£…å’Œè§£å°è£…VXLANæ•°æ®åŒ…ã€‚ é€šå¸¸è¿è¡Œåœ¨ç‰©ç†äº¤æ¢æœºæˆ–è™šæ‹Ÿæœºä¸»æœºçš„ç½‘å¡ä¸Šã€‚ åŒ…æ‹¬ä¸¤ä¸ªæ¥å£ï¼š æœ¬åœ°æ¥å£ï¼šè¿æ¥åˆ°äºŒå±‚ç½‘ç»œã€‚ éš§é“æ¥å£ï¼šè¿æ¥åˆ°ä¸‰å±‚ç½‘ç»œã€‚ VXLANå¤´ ï¼š VXLANå¤´æ’å…¥åˆ°åŸå§‹ä»¥å¤ªç½‘å¸§å’ŒUDPå¤´ä¹‹é—´ã€‚ VXLANå¤´åŒ…å«VNIç­‰ä¿¡æ¯ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„è™šæ‹Ÿç½‘ç»œã€‚ UDPå¤´ ï¼š VXLANæ•°æ®åŒ…å°è£…åœ¨UDPä¸­ï¼Œä»¥ä¾¿é€šè¿‡ä¸‰å±‚ç½‘ç»œä¼ è¾“ã€‚ é»˜è®¤ä½¿ç”¨UDPç«¯å£å·4789ã€‚ ç¡®ä¿iptablesè§„åˆ™ä¸­è¯¥UDPç«¯å£æ˜¯æ”¾å¼€çš„ã€‚ 2.2. VXLANçš„å·¥ä½œåŸç† VXLANé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°è·¨ä¸‰å±‚ç½‘ç»œçš„äºŒå±‚é€šä¿¡ï¼š\n1. æ•°æ®åŒ…å°è£…\nVTEPæ•è·æœ¬åœ°è™šæ‹Ÿæœºï¼ˆVMï¼‰çš„ä»¥å¤ªç½‘å¸§ã€‚ VTEPåœ¨å¸§ä¸Šå°è£…ï¼š æ·»åŠ VXLANå¤´ï¼Œç”¨äºæ ‡è¯†VNIã€‚ æ·»åŠ UDPå¤´ï¼Œä¾¿äºä¸‰å±‚ç½‘ç»œä¼ è¾“ã€‚ æ·»åŠ å¤–å±‚IPå¤´å’ŒMACå¤´ï¼Œç”¨äºåœ¨ä¸‰å±‚ç½‘ç»œä¸­å¯»å€ã€‚ 2. æ•°æ®åŒ…ä¼ è¾“\nå°è£…åçš„æ•°æ®åŒ…é€šè¿‡ä¸‰å±‚ç½‘ç»œä¼ è¾“åˆ°ç›®æ ‡VTEPã€‚ ä¼ è¾“è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸‰å±‚ç½‘ç»œçš„è·¯ç”±åŠŸèƒ½ï¼Œå¯ä»¥è·¨è¶Šä¸åŒçš„å­ç½‘ã€‚ 3. æ•°æ®åŒ…è§£å°è£…\nç›®æ ‡VTEPæ¥æ”¶åˆ°VXLANæ•°æ®åŒ…åï¼Œè§£æå¤–å±‚IPå¤´ã€‚ æ£€æŸ¥VNIï¼Œå°†æ•°æ®åŒ…è§£å°è£…å›åŸå§‹äºŒå±‚ä»¥å¤ªç½‘å¸§ã€‚ å°†è§£å°è£…åçš„å¸§è½¬å‘åˆ°ç›®æ ‡è™šæ‹Ÿæœºã€‚ 3. VLAN å’Œ VXLAN çš„åŒºåˆ« ç‰¹æ€§ VLAN VXLAN å®šä¹‰ äºŒå±‚ç½‘ç»œåˆ†æ®µæŠ€æœ¯ï¼Œé€šè¿‡ 802.1Q æ ‡å‡†å®ç°ã€‚ äºŒå±‚è¦†ç›–ç½‘ç»œæŠ€æœ¯ï¼Œé€šè¿‡ä¸‰å±‚ç½‘ç»œæ‰©å±•äºŒå±‚ç½‘ç»œã€‚ æ ‡å‡† IEEE 802.1Q IETF RFC 7348 éš”ç¦»æ–¹å¼ é€šè¿‡ 12 ä½ VLAN ID æ ‡è®°å¸§ï¼Œå®ç°äºŒå±‚å¹¿æ’­åŸŸéš”ç¦»ã€‚ é€šè¿‡ 24 ä½ VXLAN IDï¼ˆVNIï¼‰å®ç°éš”ç¦»ã€‚ æ”¯æŒç½‘ç»œæ•°é‡ æœ€å¤š 4096 ä¸ª VLAN è¶…è¿‡ 1600 ä¸‡ä¸ªè™šæ‹Ÿç½‘ç»œ å°è£…æ–¹å¼ åœ¨ä»¥å¤ªç½‘å¸§ä¸­æ·»åŠ  4 å­—èŠ‚ VLAN Tagã€‚ åœ¨ä»¥å¤ªç½‘å¸§å¤–å°è£… UDPï¼Œå¢åŠ  IP å’Œ VXLAN æ ‡å¤´ã€‚ ç½‘ç»œè¾¹ç•Œ é™äºå±€åŸŸç½‘ï¼Œä¾èµ–ç‰©ç†æ‹“æ‰‘ã€‚ åŸºäºä¸‰å±‚ç½‘ç»œï¼Œæ”¯æŒè·¨åœ°åŸŸã€è·¨æ•°æ®ä¸­å¿ƒè¿æ¥ã€‚ æ€§èƒ½ æ€§èƒ½é«˜ï¼Œç¡¬ä»¶äº¤æ¢æœºå¯¹ VLAN æ”¯æŒæˆç†Ÿã€‚ å°è£…å’Œè§£å°è£…å¢åŠ å¼€é”€ï¼Œä½†çµæ´»æ€§æ›´å¼ºã€‚ é€‚ç”¨åœºæ™¯ ä¸­å°å‹ç½‘ç»œï¼Œå±€åŸŸç½‘å†…çš„ç®€å•éš”ç¦»éœ€æ±‚ã€‚ å¤§è§„æ¨¡äº‘ç¯å¢ƒï¼Œå¤šç§Ÿæˆ·æ•°æ®ä¸­å¿ƒï¼Œè·¨åœ°åŸŸç½‘ç»œã€‚ å¤æ‚æ€§ é…ç½®ç®€å•ï¼Œç»´æŠ¤å®¹æ˜“ã€‚ é…ç½®å¤æ‚ï¼Œéœ€è¦æ”¯æŒ VXLAN çš„è®¾å¤‡ã€‚ ç¡¬ä»¶ä¾èµ– å¹¿æ³›æ”¯æŒï¼Œå‡ ä¹æ‰€æœ‰äº¤æ¢æœºéƒ½æ”¯æŒã€‚ éœ€è¦æ”¯æŒ VXLAN çš„è®¾å¤‡æˆ–è½¯ä»¶å®ç°ã€‚ å¹¿æ’­åŸŸæ‰©å±• å¹¿æ’­åŸŸè¾ƒå¤§ï¼Œä¸é€‚åˆå¤§è§„æ¨¡ç½‘ç»œã€‚ é€šè¿‡ä¸‰å±‚ç½‘ç»œæ‰©å±•äºŒå±‚å¹¿æ’­åŸŸã€‚ 4. Flannel VXLAN çš„åŸºæœ¬åŸç† Flannel æ˜¯ Kubernetes ä¸­å¸¸ç”¨çš„ç½‘ç»œæ’ä»¶ä¹‹ä¸€ï¼Œç”¨äºå®ç°å®¹å™¨è·¨èŠ‚ç‚¹çš„ç½‘ç»œé€šä¿¡ã€‚å®ƒæ”¯æŒå¤šç§ç½‘ç»œåç«¯ï¼Œå…¶ä¸­ VXLAN åç«¯ æ˜¯ä¸€ç§å¸¸ç”¨çš„é€‰æ‹©ï¼Œåˆ©ç”¨ VXLAN éš§é“å®ç°ä¸åŒèŠ‚ç‚¹çš„å®¹å™¨ç½‘ç»œäº’é€šã€‚\nFlannel ä½¿ç”¨ VXLAN åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„äºŒå±‚ç½‘ç»œï¼ŒæŠŠä½äºä¸åŒèŠ‚ç‚¹ä¸Šçš„å®¹å™¨å­ç½‘è¿æ¥èµ·æ¥ã€‚è¿™äº›å­ç½‘ç»Ÿä¸€ç»„æˆä¸€ä¸ªé€»è¾‘ä¸Šçš„æ‰å¹³ç½‘ç»œï¼Œä½¿å¾—å®¹å™¨å¯ä»¥ä½¿ç”¨ Pod IP ç›´æ¥äº’é€šã€‚\nåœ¨ VXLAN æ¨¡å¼ä¸‹ï¼š\næ¯ä¸ªèŠ‚ç‚¹åˆ†é…ä¸€ä¸ªç‹¬ç«‹çš„å­ç½‘ï¼ˆä¾‹å¦‚ /24ï¼‰ï¼Œè¯¥å­ç½‘ä¸­çš„ IP åœ°å€åˆ†é…ç»™è¯¥èŠ‚ç‚¹ä¸Šçš„ Podã€‚ VXLAN éš§é“ç”¨äºåœ¨ä¸åŒèŠ‚ç‚¹ä¹‹é—´å°è£…å’Œä¼ è¾“æ•°æ®åŒ…ã€‚ 4.1. Flannel VXLAN çš„å…³é”®ç»„ä»¶ etcd / Kubernetes APIï¼š Flannel ä½¿ç”¨ etcd æˆ– Kubernetes API ä½œä¸ºå­˜å‚¨ï¼Œä¿å­˜æ¯ä¸ªèŠ‚ç‚¹çš„å­ç½‘åˆ†é…ä¿¡æ¯ã€‚ ä¾‹å¦‚ï¼ŒèŠ‚ç‚¹ A çš„å­ç½‘æ˜¯ 10.1.1.0/24ï¼ŒèŠ‚ç‚¹ B çš„å­ç½‘æ˜¯ 10.1.2.0/24ã€‚ flanneld è¿›ç¨‹ï¼š æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ flanneldï¼Œè´Ÿè´£ï¼š ä» etcd è·å–å­ç½‘ä¿¡æ¯ã€‚ é…ç½® VXLAN è®¾å¤‡ã€‚ ç®¡ç†è·¯ç”±è§„åˆ™ã€‚ VXLAN è®¾å¤‡ï¼š Flannel åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºä¸€ä¸ª VXLAN è™šæ‹Ÿç½‘ç»œæ¥å£ï¼ˆå¦‚ flannel.1ï¼‰ã€‚ é€šè¿‡è¿™ä¸ªæ¥å£ï¼Œå°†æ•°æ®åŒ…å°è£…åˆ° VXLAN éš§é“ä¸­ã€‚ 4.2. Flannel VXLAN çš„é€šä¿¡æµç¨‹ 4.2.1. Pod åˆ° Pod é€šä¿¡ç¤ºä¾‹ å‡è®¾ Pod1 åœ¨èŠ‚ç‚¹ Aï¼ŒPod2 åœ¨èŠ‚ç‚¹ Bï¼ŒPod1 çš„ IP ä¸º 10.1.1.2ï¼ŒPod2 çš„ IP ä¸º 10.1.2.3ï¼š\næ•°æ®åŒ…ç”Ÿæˆï¼š Pod1 æƒ³è¦ä¸ Pod2 é€šä¿¡ï¼Œå‘é€ä¸€ä¸ª IP æ•°æ®åŒ…ï¼Œç›®æ ‡åœ°å€æ˜¯ 10.1.2.3ã€‚ èŠ‚ç‚¹è·¯ç”±æŸ¥æ‰¾ï¼š èŠ‚ç‚¹ A çš„è·¯ç”±è¡¨æ ¹æ®ç›®æ ‡ IP (10.1.2.3)ï¼Œå‘ç°å…¶å­ç½‘ 10.1.2.0/24å±äºèŠ‚ç‚¹ Bã€‚ æ•°æ®åŒ…è¢«è½¬å‘åˆ° VXLAN è®¾å¤‡ flannel.1ã€‚ VXLAN å°è£…ï¼š åœ¨ flannel.1ä¸Šï¼Œæ•°æ®åŒ…è¢«å°è£…ï¼š åŸå§‹ IP æ•°æ®åŒ…è¢«ä½œä¸º VXLAN çš„æœ‰æ•ˆè½½è·ã€‚ VXLAN å¤´éƒ¨å’Œå¤–å±‚ UDP/IP å¤´éƒ¨è¢«æ·»åŠ ã€‚ å¤–å±‚ IP å¤´çš„ç›®æ ‡åœ°å€æ˜¯èŠ‚ç‚¹ B çš„ç‰©ç† IP åœ°å€ã€‚ è·¨ç½‘ç»œä¼ è¾“ï¼š å°è£…åçš„æ•°æ®åŒ…é€šè¿‡åº•å±‚ä¸‰å±‚ç½‘ç»œï¼ˆé€šå¸¸æ˜¯å®¿ä¸»æœºçš„ç‰©ç†ç½‘å¡ï¼‰å‘é€åˆ°èŠ‚ç‚¹ Bã€‚ è§£å°è£…å’Œè½¬å‘ï¼š èŠ‚ç‚¹ B çš„ VXLAN è®¾å¤‡æ¥æ”¶åˆ°æ•°æ®åŒ…åï¼Œè§£å°è£…å¤–å±‚å¤´éƒ¨ï¼Œè¿˜åŸå‡ºåŸå§‹ IP æ•°æ®åŒ…ã€‚ èŠ‚ç‚¹ B æ ¹æ®è·¯ç”±è§„åˆ™å°†æ•°æ®åŒ…è½¬å‘ç»™ Pod2ã€‚ 4.2.2. è·¯ç”±å’Œ ARP çš„å¤„ç† è·¯ç”±è¡¨ï¼š\nFlannel ä¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹é…ç½®è·¯ç”±è§„åˆ™ï¼Œå°†ç›®æ ‡å­ç½‘ä¸ç›¸åº”çš„ VXLAN éš§é“è®¾å¤‡å…³è”ã€‚\nä¾‹å¦‚ï¼š\n10.1.2.0/24 via 192.168.1.2 dev flannel.1 ARP å¤„ç†ï¼š\nVXLAN éœ€è¦çŸ¥é“è¿œç¨‹èŠ‚ç‚¹çš„ç‰©ç† IP åœ°å€ã€‚ Flannel åœ¨ VXLAN æ¨¡å¼ä¸‹é€šè¿‡ etcd æˆ– Kubernetes API ç»´æŠ¤èŠ‚ç‚¹çš„ IP æ˜ å°„å…³ç³»ï¼Œè€Œä¸æ˜¯ä¾èµ–ä¼ ç»Ÿçš„ ARPã€‚ 5. Flannel VXLANæŠ¥æ–‡è§£æ é€šè¿‡vxlanéš§é“ä¼ è¾“éœ€è¦å°vxlanåŒ…å’Œè§£vxlanåŒ…ï¼Œä»¥ä¸‹æè¿°vxlanæŠ¥æ–‡å†…å®¹ã€‚\n5.1. VXLAN æŠ¥æ–‡ç»“æ„ Flannel çš„ VXLAN æŠ¥æ–‡å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š\nå­—æ®µ æè¿° å¤–å±‚ä»¥å¤ªç½‘å¤´ ç”¨äºä¼ è¾“ VXLAN æŠ¥æ–‡çš„ç‰©ç†ç½‘ç»œçš„ MAC åœ°å€ã€‚ å¤–å±‚ IP å¤´ ç”¨äºä¸‰å±‚ä¼ è¾“ï¼ŒåŒ…å«æº IPï¼ˆæœ¬åœ°ç‰©ç†æœºï¼‰å’Œç›®çš„ IPï¼ˆç›®æ ‡ç‰©ç†æœºï¼‰ã€‚ UDP å¤´ ç”¨äºå°è£… VXLAN æµé‡ï¼Œé€šå¸¸ä½¿ç”¨ VXLAN çš„é»˜è®¤ç«¯å£ 4789ã€‚ VXLAN æ ‡å¤´ åŒ…å« VXLAN çš„å…³é”®ä¿¡æ¯ï¼Œä¾‹å¦‚ VNIï¼ˆè™šæ‹Ÿç½‘ç»œæ ‡è¯†ï¼‰ã€‚ å†…å±‚ä»¥å¤ªç½‘å¤´ åŸå§‹çš„ä»¥å¤ªç½‘å¸§å¤´ï¼Œç”¨äºå®¹å™¨æˆ– Pod ä¹‹é—´é€šä¿¡ã€‚ å†…å±‚æ•°æ®ï¼ˆPayloadï¼‰ å®é™…çš„åº”ç”¨å±‚æ•°æ®ï¼Œä¾‹å¦‚ HTTPã€TCP æˆ– ICMP æ•°æ®ã€‚ ç¤ºä¾‹ï¼š\n+-----------------------------+ | å¤–å±‚ä»¥å¤ªç½‘å¤´ | MAC æºåœ°å€ | MAC ç›®çš„åœ°å€ | ç±»å‹ | +-----------------------------+ | å¤–å±‚ IP å¤´ | æº IP åœ°å€ | ç›®çš„ IP åœ°å€ | åè®®| +-----------------------------+ | UDP å¤´ | æºç«¯å£ | ç›®çš„ç«¯å£ | é•¿åº¦ | +-----------------------------+ | VXLAN æ ‡å¤´ | Flag | Reserved | VNI | +-----------------------------+ | å†…å±‚ä»¥å¤ªç½‘å¤´ | æº MAC åœ°å€ | ç›®çš„ MAC åœ°å€ | +-----------------------------+ | å†…å±‚æ•°æ®ï¼ˆPayloadï¼‰ | å®é™…çš„æ•°æ®å†…å®¹ | +-----------------------------+ 5.2. æŠ¥æ–‡è¯¦ç»†è§£æ 1. å¤–å±‚ä»¥å¤ªç½‘å¤´\nä½œç”¨ï¼šç”¨äºæ‰¿è½½ VXLAN æŠ¥æ–‡çš„å®é™…ç½‘ç»œä¼ è¾“ã€‚ å†…å®¹ï¼š æº MAC åœ°å€ï¼šå‘é€æŠ¥æ–‡çš„ç‰©ç†ç½‘å¡çš„ MAC åœ°å€ã€‚ ç›®çš„ MAC åœ°å€ï¼šç›®æ ‡ä¸»æœºçš„ç‰©ç†ç½‘å¡çš„ MAC åœ°å€ã€‚ 2. å¤–å±‚ IP å¤´\nä½œç”¨ï¼šåœ¨ä¸‰å±‚ç½‘ç»œä¸­å°† VXLAN æŠ¥æ–‡è·¯ç”±åˆ°ç›®æ ‡ä¸»æœºã€‚ å†…å®¹ï¼š æº IP åœ°å€ï¼šå‘é€æŠ¥æ–‡çš„ç‰©ç†ä¸»æœºçš„ IP åœ°å€ã€‚ ç›®çš„ IP åœ°å€ï¼šç›®æ ‡ä¸»æœºçš„ IP åœ°å€ã€‚ åè®®ç±»å‹ï¼šUDPã€‚ 3. UDP å¤´\nä½œç”¨ï¼šå°è£… VXLAN æ•°æ®ã€‚ å†…å®¹ï¼š æºç«¯å£ï¼šåŠ¨æ€åˆ†é…çš„éšæœºç«¯å£ã€‚ ç›®çš„ç«¯å£ï¼šVXLAN çš„ç«¯å£ï¼Œä¾‹å¦‚ 8472ï¼ˆå¯ä»¥åœ¨ Flannel é…ç½®ä¸­è‡ªå®šä¹‰ï¼‰ã€‚ é•¿åº¦å’Œæ ¡éªŒå’Œï¼šç”¨äºç¡®ä¿ UDP æŠ¥æ–‡çš„å®Œæ•´æ€§ã€‚ 4. VXLAN æ ‡å¤´\nä½œç”¨ï¼šæ ‡è¯†è™šæ‹Ÿç½‘ç»œä»¥åŠå¯¹ VXLAN æµé‡è¿›è¡Œå¿…è¦çš„æ§åˆ¶ã€‚ æ ¼å¼ï¼š Flagï¼š8 ä½ï¼Œæ ‡è¯†æ˜¯å¦å¯ç”¨ VXLAN åŠŸèƒ½ï¼Œé€šå¸¸ä¸º 0x08ã€‚ VNIï¼ˆVirtual Network Identifierï¼‰ï¼š24 ä½ï¼Œæ ‡è¯† VXLAN æ‰€å±çš„è™šæ‹Ÿç½‘ç»œã€‚ Reservedï¼šç”¨äºå¯¹é½å’Œæ‰©å±•ï¼Œé€šå¸¸ä¸º 0ã€‚ 5. å†…å±‚ä»¥å¤ªç½‘å¤´\nä½œç”¨ï¼šå°è£…åŸå§‹çš„ä»¥å¤ªç½‘å¸§ï¼Œç”¨äºå®¹å™¨æˆ– Pod ä¹‹é—´çš„äºŒå±‚é€šä¿¡ã€‚ å†…å®¹ï¼š æº MAC åœ°å€ï¼šå‘é€å®¹å™¨æˆ– Pod çš„ MAC åœ°å€ã€‚ ç›®çš„ MAC åœ°å€ï¼šç›®æ ‡å®¹å™¨æˆ– Pod çš„ MAC åœ°å€ã€‚ 6. å†…å±‚æ•°æ®ï¼ˆPayloadï¼‰\nä½œç”¨ï¼šå®é™…çš„ç”¨æˆ·æ•°æ®ã€‚ å†…å®¹ï¼š æ•°æ®ç±»å‹ï¼šå¯ä»¥æ˜¯ IP æ•°æ®åŒ…ï¼ˆä¾‹å¦‚ TCPã€UDP æˆ– ICMPï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯ ARP ç­‰åè®®ã€‚ 5.3. æŠ¥æ–‡çš„æ•è· å¯ä»¥é€šè¿‡å‘½ä»¤ip -d link show flannel.1æŸ¥çœ‹vxlan idï¼ˆVNIï¼‰ ï¼Œvxlanç«¯å£ï¼ŒvxlanåŸºäºçš„ç‰©ç†ç½‘å¡ã€‚\nä¾‹å¦‚ï¼Œä»¥ä¸‹æŸ¥è¯¢åˆ°vxlan id(VNI)ä¸º1ï¼Œvxlanç«¯å£ä¸º8472ï¼ŒåŸºäºçš„ç‰©ç†ç½‘å¡æ˜¯bond0ã€‚\n# ip -d link show flannel.1 10: flannel.1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether xxxxxx brd xxxxx promiscuity 0 minmtu 68 maxmtu 65535 vxlan id 1 local xxxxxxx dev bond0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 ä½¿ç”¨ tcpdumpï¼Œåœ¨ç‰©ç†ç½‘å¡ä¸Šæ•è· VXLAN æŠ¥æ–‡ï¼Œ åè®®æ˜¯UDPï¼Œæ·»åŠ vxlançš„ç«¯å£ã€‚\ntcpdump -i bond0 udp port 8472 -vv 6. å¦‚ä½•é¿å…VXLANå†²çª åœ¨åŒä¸€å°ç‰©ç†æœºä¸Šå¦‚æœå­˜åœ¨å…¶ä»–è®¾å¤‡ï¼ˆä¾‹å¦‚ï¼šå¼¹æ€§å¤–ç½‘EIPè®¾å¤‡ï¼‰ä½¿ç”¨äº†VXLANçš„éš§é“æŠ€æœ¯ï¼Œå¯èƒ½ä¸Flannelçš„VXLANè®¾å¤‡å­˜åœ¨å†²çªï¼Œä»¥ä¸‹æ˜¯å†²çªçš„å¯èƒ½æ€§ã€åŸå› åŠè§£å†³æ–¹æ³•ï¼š\n6.1. å¯èƒ½å­˜åœ¨çš„å†²çª 1. UDP ç«¯å£å†²çª\nVXLAN é€šå¸¸ä½¿ç”¨ UDP ç«¯å£ 4789 è¿›è¡Œå°è£…ä¼ è¾“ã€‚å¦‚æœå¼¹æ€§å¤–ç½‘ IP çš„ VXLAN å®ç°å’Œ Flannel ï¼ˆé»˜è®¤ç«¯å£8472ï¼‰éƒ½ä½¿ç”¨ç›¸åŒçš„ UDP ç«¯å£ï¼Œé‚£ä¹ˆä¼šå¯¼è‡´ç«¯å£å†²çªï¼Œä½¿å¾—å…¶ä¸­ä¸€ä¸ªåŠŸèƒ½å¤±æ•ˆã€‚ 2. VNI (VXLAN Network Identifier) å†²çª\nVXLAN çš„æ¯ä¸ªè™šæ‹Ÿç½‘ç»œé€šè¿‡ VNIï¼ˆ24 ä½ï¼‰è¿›è¡ŒåŒºåˆ†ã€‚ å¦‚æœå¼¹æ€§å¤–ç½‘ IP å’Œ Flannel çš„ VXLAN ä½¿ç”¨äº†ç›¸åŒçš„ VNIï¼Œåˆ™å¯èƒ½å¯¼è‡´ VXLAN éš§é“ä¹‹é—´çš„éš”ç¦»æ€§å¤±æ•ˆï¼Œé€ æˆæ•°æ®åŒ…æ··ä¹±ã€‚ 3. è·¯ç”±æˆ–è®¾å¤‡åå†²çª\nä¸¤è€…éƒ½ä¼šåœ¨ç‰©ç†æœºä¸Šåˆ›å»º VXLAN è®¾å¤‡ï¼ˆå¦‚ vxlan0 æˆ– flannel.1ï¼‰ï¼Œå¦‚æœè®¾å¤‡åç§°ç›¸åŒï¼Œå¯èƒ½å¯¼è‡´é…ç½®æ··ä¹±ã€‚ è·¯ç”±è¡¨å¯èƒ½åŒæ—¶å­˜åœ¨ä¸ VXLAN éš§é“ç›¸å…³çš„è§„åˆ™ï¼Œå¦‚æœè·¯ç”±ç›®æ ‡ç½‘ç»œæœ‰é‡å ï¼Œå¯èƒ½å¯¼è‡´æµé‡è¢«é”™è¯¯è½¬å‘ã€‚ 6.2. å†²çªçš„è§£å†³æ–¹æ³• 1. é¿å… UDP ç«¯å£å†²çª\nç¡®è®¤å¼¹æ€§å¤–ç½‘ IP çš„ VXLAN ç«¯å£å·ã€‚ è‡ªå®šä¹‰Flannelçš„VXLAN ç«¯å£å·ã€‚ç¡®ä¿ä¸¤è€…ä½¿ç”¨ä¸åŒçš„ç«¯å£å·ã€‚ 2. é¿å… VNI å†²çª\nç¡®è®¤å¼¹æ€§å¤–ç½‘ IP çš„ VXLAN å®ç°æ˜¯å¦å…è®¸æŒ‡å®š VNIã€‚å¦‚æœå…è®¸ï¼Œåˆ™ä¸ºä¸¤è€…è®¾ç½®ä¸åŒçš„ VNI èŒƒå›´ã€‚ Flannel VNI é€šå¸¸ç”±å…¶å†…éƒ¨ç®¡ç†ï¼Œè‡ªåŠ¨åˆ†é…ï¼Œä½†å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šèŒƒå›´æˆ–å›ºå®šå€¼ã€‚ 3. é¿å…è®¾å¤‡åå†²çª\nFlannel é»˜è®¤è®¾å¤‡åæ˜¯ flannel.1ï¼Œä¸€èˆ¬éflannelçš„è®¾å¤‡ä¸ä¼šè®¾ç½®ä¸ºè¯¥åç§°ã€‚ ç¡®ä¿å¼¹æ€§å¤–ç½‘ IP çš„ VXLAN è®¾å¤‡åä¸åŒï¼Œæˆ–æ˜¾å¼æŒ‡å®šè®¾å¤‡åç§°ã€‚ 4. è·¯ç”±éš”ç¦»\nä»”ç»†æ£€æŸ¥è·¯ç”±è¡¨ï¼Œç¡®ä¿ä¸¤è€…çš„ç›®æ ‡å­ç½‘èŒƒå›´æ²¡æœ‰é‡å ã€‚ å¦‚æœè¦ä¿®æ”¹flannelçš„å†²çªå‚æ•°ï¼Œå¯ä»¥ä¿®æ”¹é…ç½®æ–‡ä»¶ã€‚\nkubectl edit cm -n kube-flannel kube-flannel-cfg\n{ \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\", \"VNI\": 1, # é»˜è®¤å€¼ \"Port\": 8472 # é»˜è®¤å€¼ } } 6.3. æŸ¥çœ‹è®¾å¤‡çš„VXLAN å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ—å‡ºæ‰€æœ‰ VXLAN æ¥å£ï¼š\n# ip link show type vxlan # è¾“å‡º 6: vxlan0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 ... 7: flannel.1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 ... å¯ä»¥æŸ¥çœ‹ VXLAN æ¥å£çš„é…ç½®ä¿¡æ¯ï¼š\n# ip -d link show \u003cvxlan_interface\u003e 6: vxlan0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 06:42:ae:ff:fe:90 brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 42 local 192.168.1.10 dev eth0 port 4789 0 ... # vxlan id 42ï¼šè¡¨ç¤º VXLAN çš„ VNIï¼ˆVirtual Network Identifierï¼‰æ˜¯ 42ã€‚ # port 4789ï¼šè¡¨ç¤º VXLAN ä½¿ç”¨çš„ UDP ç«¯å£æ˜¯ 4789ï¼ˆé»˜è®¤ç«¯å£ï¼‰ã€‚ å¦‚æœ VXLAN æ¥å£ç»‘å®šåˆ°äº†æ¡¥æ¥è®¾å¤‡ï¼ˆbridgeï¼‰ï¼Œå¯ä»¥é€šè¿‡ bridge å‘½ä»¤æŸ¥è¯¢è¯¦ç»†ä¿¡æ¯ã€‚\nbridge fdb show dev \u003cvxlan_interface\u003e # è¾“å‡º 00:00:00:00:00:13 dst x.x.x.x self permanent 7. æ€»ç»“ æœ¬æ–‡ä¸»è¦ä»‹ç»äº†VXLANçš„åŸºæœ¬æ¦‚å¿µå’ŒåŸç†ï¼ŒæŠ¥æ–‡è§£æä»¥åŠåœ¨Flannelä¸­çš„ä½¿ç”¨ã€‚é™¤äº†Flannelå¤–ï¼Œåœ¨Calicoå’ŒCiliumçš„ç½‘ç»œæ’ä»¶ä¸­ä¹Ÿæ¶‰åŠåˆ°VXLANçš„ä½¿ç”¨ï¼ŒåŸºæœ¬åŸç†ç±»ä¼¼ï¼Œå¤§åŒå°å¼‚ã€‚é€šè¿‡æœ¬æ–‡çš„ä»‹ç»ï¼Œå¸®åŠ©è¯»è€…å¯¹äºå…¶ä»–åœºæ™¯ä¸‹ä½¿ç”¨VXLANçš„æ–¹å¼ä¹Ÿèƒ½å¤Ÿå¿«é€Ÿç†è§£ï¼Œå¹¶ä¸”å¯ä»¥å¿«é€Ÿæ’æŸ¥VXLANç›¸å…³çš„ç½‘ç»œé—®é¢˜ã€‚\nå‚è€ƒï¼š\nhttps://datatracker.ietf.org/doc/html/rfc7348 https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md ","categories":"","description":"","excerpt":"1. VXLANç®€ä»‹ VXLANï¼ˆVirtual Extensible LANï¼‰æ˜¯ä¸€ç§ç½‘ç»œè™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸäºŒå±‚ç½‘ç»œæ‰©å±•çš„å±€é™æ€§ï¼Œå°¤å…¶ â€¦","ref":"/kubernetes-notes/network/flannel/vxlan/","tags":["Kubernetes","CNI"],"title":"VXLANåŸç†ä»‹ç»"},{"body":"1. kruise-rolloutç®€ä»‹ é‡‘ä¸é›€å‘å¸ƒæ˜¯é€æ­¥å°†æµé‡å¯¼å‘æ–°ç‰ˆæœ¬çš„åº”ç”¨ï¼Œä»¥æœ€å°åŒ–é£é™©ã€‚å…·ä½“è¿‡ç¨‹åŒ…æ‹¬ï¼š\néƒ¨ç½²æ–°ç‰ˆæœ¬çš„ Podã€‚ å°†ä¸€éƒ¨åˆ†æµé‡åˆ†é…åˆ°æ–°ç‰ˆæœ¬ï¼ˆé€šå¸¸æ¯”ä¾‹å¾ˆå°ï¼‰ã€‚ é€æ­¥å¢åŠ æ–°ç‰ˆæœ¬çš„æµé‡æ¯”ä¾‹ï¼Œç›´åˆ°å®Œæˆå…¨é‡å‘å¸ƒã€‚ Kruise Rollouts æ˜¯ä¸€ä¸ª Bypass(æ—è·¯) ç»„ä»¶ï¼Œæä¾› é«˜çº§æ¸è¿›å¼äº¤ä»˜åŠŸèƒ½ ã€‚å¯ä»¥é€šè¿‡Kruise Rolloutsæ’ä»¶æ¥å®ç°é‡‘ä¸é›€å‘å¸ƒçš„èƒ½åŠ›ã€‚\nç»„ä»¶ Kruise Rollouts æ ¸å¿ƒæ¦‚å¿µ å¢å¼ºç°æœ‰çš„å·¥ä½œè´Ÿè½½ æ¶æ„ Bypass æ’æ‹”å’Œçƒ­åˆ‡æ¢ æ˜¯ å‘å¸ƒç±»å‹ å¤šæ‰¹æ¬¡ã€é‡‘ä¸é›€ã€A/Bæµ‹è¯•ã€å…¨é“¾è·¯ç°åº¦ å·¥ä½œè´Ÿè½½ç±»å‹ Deploymentã€StatefulSetã€CloneSetã€Advanced StatefulSetã€Advanced DaemonSet æµé‡ç±»å‹ Ingressã€GatewayAPIã€CRDï¼ˆéœ€è¦ Lua è„šæœ¬ï¼‰ è¿ç§»æˆæœ¬ æ— éœ€è¿ç§»å·¥ä½œè´Ÿè½½å’ŒPods HPA å…¼å®¹æ€§ æ˜¯ 2. å®‰è£…kruise-rollout 2.1. helmå®‰è£…kruise-rollout helm repo add openkruise https://openkruise.github.io/charts/ helm repo update kubectl create ns openkruise helm install kruise-rollout openkruise/kruise-rollout -n openkruise # å‡çº§åˆ°æŒ‡å®šç‰ˆæœ¬ helm upgrade kruise-rollout openkruise/kruise-rollout --version 0.5.0 æŸ¥çœ‹éƒ¨ç½²ç»“æœ\né€šè¿‡helmå’Œkubectlçš„å‘½ä»¤å¯ä»¥çœ‹åˆ°åˆ›å»ºäº†å‡ ä¸ªcrdå’Œkruise-rollout-controller-managerçš„podæ¥æä¾›rolloutçš„åŠŸèƒ½ã€‚\n# helm list -A NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION kruise-rollout\topenkruise\t1 2024-11-24 16:52:36.067327844 +0800 +08\tdeployed\tkruise-rollout-0.5.0\t0.5.0 # kubectl get po -n kruise-rollout NAME READY STATUS RESTARTS AGE kruise-rollout-controller-manager-875654888-rpxds 1/1 Running 0 87s kruise-rollout-controller-manager-875654888-w75xj 1/1 Running 0 87s # kubectl get crd |grep kruise batchreleases.rollouts.kruise.io 2024-11-24T08:52:36Z rollouthistories.rollouts.kruise.io 2024-11-24T08:52:36Z rollouts.rollouts.kruise.io 2024-11-24T08:52:36Z trafficroutings.rollouts.kruise.io 2024-11-24T08:52:36Z 2.2. å®‰è£…kubectl-kruise kubectl-kruiseç”¨äºæ‰§è¡Œrolloutå‘ç‰ˆå’Œå›é€€ç­‰æ“ä½œçš„äºŒè¿›åˆ¶å‘½ä»¤ã€‚\nkubectl-kruise rollout approveï¼šæ‰§è¡Œä¸‹ä¸€æ‰¹ç‰ˆæœ¬å‘å¸ƒ kubectl-kruise rollout undoï¼šå›é€€å…¨éƒ¨å‘ç‰ˆæ‰¹æ¬¡ wget https://github.com/openkruise/kruise-tools/releases/download/v1.1.7/kubectl-kruise-linux-amd64-v1.1.7.tar.gz tar -zvxf kubectl-kruise-linux-amd64-v1.1.7.tar.gz mv linux-amd64/kubectl-kruise /usr/local/bin/ 3. ä½¿ç”¨æŒ‡å— å…ˆéƒ¨ç½²ä¸€ä¸ªk8s deploymentå¯¹è±¡ï¼Œä¾‹å¦‚éƒ¨ç½²10ä¸ªnginx:1.26çš„å®¹å™¨ã€‚\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx namespace: default spec: replicas: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: nginx spec: containers: - image: nginx:1.26 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP 3.1. åˆ›å»ºå‘å¸ƒç­–ç•¥(rolloutå¯¹è±¡) æˆ‘ä»¬ä»¥ä½¿ç”¨å¤šæ‰¹æ¬¡ç°åº¦å‘å¸ƒä¸ºä¾‹ã€‚\nworkloadRefå®šä¹‰ä½œç”¨äºå“ªä¸ªdeploymentå¯¹è±¡ã€‚ strategyå®šä¹‰å‘å¸ƒç­–ç•¥ã€‚ ä»¥ä¸‹æ˜¯å‘å¸ƒç­–ç•¥ç¤ºä¾‹æè¿°ï¼š\nåœ¨ç¬¬ä¸€æ‰¹æ¬¡ï¼šåªå‡çº§ 1ä¸ªPodï¼› åœ¨ç¬¬äºŒæ‰¹æ¬¡ï¼šå‡çº§ 50% çš„ Podsï¼Œå³ 5ä¸ªå·²æ›´æ–°çš„Podï¼› åœ¨ç¬¬ä¸‰æ‰¹æ¬¡ï¼šå‡çº§ 100% çš„ Podsï¼Œå³ 10ä¸ªå·²æ›´æ–°çš„Podã€‚ $ kubectl apply -f - \u003c\u003cEOF apiVersion: rollouts.kruise.io/v1beta1 kind: Rollout metadata: name: rollouts-nginx namespace: default spec: workloadRef: apiVersion: apps/v1 kind: Deployment name: nginx strategy: canary: enableExtraWorkloadForCanary: false steps: - replicas: 1 - replicas: 50% - replicas: 100% EOF 3.2. å‡çº§å‘å¸ƒç‰ˆæœ¬(å‘å¸ƒç¬¬ä¸€æ‰¹æ¬¡) å‡çº§nginx:1.26åˆ°nginx:1.27\nkubectl set image deployment nginx nginx=nginx:1.27 æŸ¥çœ‹ç°åº¦ç»“æœï¼š\n# kubectl get rs -L pod-template-hash -w NAME DESIRED CURRENT READY AGE POD-TEMPLATE-HASH nginx-68556bc579 1 1 1 8s 68556bc579 # nginx:1.27 nginx-d7f5d89c9 9 9 9 24m d7f5d89c9 # nginx:1.26 æŸ¥çœ‹rolloutçŠ¶æ€ï¼š\nrolloutå¯¹è±¡çš„çŠ¶æ€ä¸»è¦æè¿°äº†å½“å‰å¤„äºå“ªä¸ªrollouté˜¶æ®µåŠæ€»çŠ¶æ€å’Œå­é˜¶æ®µçŠ¶æ€çš„ä¿¡æ¯ã€‚\n# kubectl get rollout rollouts-nginx -oyaml apiVersion: rollouts.kruise.io/v1beta1 kind: Rollout metadata: name: rollouts-nginx namespace: default spec: disabled: false strategy: canary: steps: - pause: {} replicas: 1 - pause: {} replicas: 50% - pause: {} replicas: 100% workloadRef: apiVersion: apps/v1 kind: Deployment name: nginx status: canaryStatus: canaryReadyReplicas: 1 canaryReplicas: 1 canaryRevision: 5c9556986d currentStepIndex: 1 currentStepState: StepPaused # å½“å‰æ­¥éª¤çŠ¶æ€ lastUpdateTime: \"2024-11-24T11:52:56Z\" message: BatchRelease is at state Ready, rollout-id , step 1 observedWorkloadGeneration: 36 podTemplateHash: d7f5d89c9 rolloutHash: 77cxd69w47b7bwddwv2w7vxvb4xxdbwcx9x289vw69w788w4w6z4x8dd4vbz2zbw stableRevision: 68556bc579 conditions: - lastTransitionTime: \"2024-11-24T11:52:47Z\" lastUpdateTime: \"2024-11-24T11:52:47Z\" message: Rollout is in Progressing reason: InRolling status: \"True\" type: Progressing message: Rollout is in step(1/3), and you need manually confirm to enter the next step # rolloutä¿¡æ¯ observedGeneration: 2 phase: Progressing # æ•´ä½“çŠ¶æ€ 3.3. å‘å¸ƒç¬¬äºŒæ‰¹æ¬¡ kubectl-kruise rollout approve rollout/rollouts-nginx -n default æŸ¥çœ‹ç°åº¦ç»“æœï¼š\n# kubectl get rs -L pod-template-hash -w NAME DESIRED CURRENT READY AGE POD-TEMPLATE-HASH nginx-68556bc579 5 5 5 7m24s 68556bc579 # nginx:1.27 nginx-d7f5d89c9 5 5 5 32m d7f5d89c9 # nginx:1.26 # kubectl get po NAME READY STATUS RESTARTS AGE nginx-68556bc579-24lpl 1/1 Running 0 7m34s # nginx:1.27 ç¬¬ä¸€æ‰¹æ¬¡ nginx-68556bc579-2dph8 1/1 Running 0 14s # nginx:1.27 ç¬¬äºŒæ‰¹æ¬¡ nginx-68556bc579-57pqt 1/1 Running 0 14s # nginx:1.27 ç¬¬äºŒæ‰¹æ¬¡ nginx-68556bc579-879s9 1/1 Running 0 14s # nginx:1.27 ç¬¬äºŒæ‰¹æ¬¡ nginx-68556bc579-fwt52 1/1 Running 0 14s # nginx:1.27 ç¬¬äºŒæ‰¹æ¬¡ nginx-d7f5d89c9-5fbfp 1/1 Running 0 30m # å…¶ä½™ä¸ºç¬¬ä¸‰æ‰¹æ¬¡ nginx-d7f5d89c9-gkz9p 1/1 Running 0 30m nginx-d7f5d89c9-jhxwl 1/1 Running 0 30m nginx-d7f5d89c9-vrqfz 1/1 Running 0 30m nginx-d7f5d89c9-zk2sj 1/1 Running 0 30m 3.4. å‘å¸ƒç¬¬ä¸‰æ‰¹æ¬¡ kubectl-kruise rollout approve rollout/rollouts-nginx -n default æŸ¥çœ‹ç»“æœï¼š\n# kubectl get rs -L pod-template-hash -w NAME DESIRED CURRENT READY AGE POD-TEMPLATE-HASH nginx-68556bc579 10 10 10 12m 68556bc579 # nginx:1.27 nginx-d7f5d89c9 0 0 0 37m d7f5d89c9 3.5. å‘å¸ƒå›æ»š è¯¥å›æ»šä¼šæŠŠå…¨éƒ¨æ‰¹æ¬¡çš„podéƒ½å›æ»šåˆ°ç¬¬ä¸€æ‰¹å‘ç‰ˆå‰çš„ç‰ˆæœ¬ã€‚\nkubectl-kruise rollout undo rollout/rollouts-nginx -n default 4. k8så¯¹è±¡å˜æ›´ kruise-rolloutçš„åŸç†ä¸»è¦è¿˜æ˜¯åŠ«æŒDeploymentå’ŒReplicaSetçš„æ“ä½œï¼Œä¾‹å¦‚åœ¨å‡çº§deploymentçš„æ—¶å€™ä¼šæŠŠdeploymentåŸå…ˆçš„strategyç­–ç•¥æ”¾åœ¨annotationsä¸­æš‚å­˜ï¼Œå†å¢åŠ paused=trueçš„å‚æ•°æš‚åœdeploymentçš„å‘å¸ƒã€‚åœ¨rolloutå…¨éƒ¨ç»“æŸåå†æ¢å¤åŸå…ˆçš„deploymentå‚æ•°ï¼Œä¸»è¦åŒ…æ‹¬pausedå’Œstrategyã€‚\nä»¥ä¸‹æ˜¯rolloutä¸­é—´é˜¶æ®µçš„deploymentä¿¡æ¯ï¼š\napiVersion: apps/v1 kind: Deployment metadata: annotations: batchrelease.rollouts.kruise.io/control-info: '{\"apiVersion\":\"rollouts.kruise.io/v1beta1\",\"kind\":\"BatchRelease\",\"name\":\"rollouts-nginx\",\"uid\":\"8f29624b-ab77-49d1-abbe-aa92bb3998e2\",\"controller\":true,\"blockOwnerDeletion\":true}' deployment.kubernetes.io/revision: \"5\" rollouts.kruise.io/deployment-extra-status: '{\"updatedReadyReplicas\":1,\"expectedUpdatedReplicas\":1}' rollouts.kruise.io/deployment-strategy: '{\"rollingStyle\":\"Partition\",\"rollingUpdate\":{\"maxUnavailable\":\"25%\",\"maxSurge\":\"25%\"},\"partition\":1}' # deploymentåŸæœ¬çš„strategyç­–ç•¥ rollouts.kruise.io/in-progressing: '{\"rolloutName\":\"rollouts-nginx\"}' generation: 36 labels: app: nginx rollouts.kruise.io/controlled-by-advanced-deployment-controller: \"true\" rollouts.kruise.io/stable-revision: 68556bc579 rollouts.kruise.io/workload-type: deployment name: nginx namespace: default spec: paused: true # å¢åŠ äº†pausedå‚æ•° progressDeadlineSeconds: 600 replicas: 10 revisionHistoryLimit: 10 5. Rolloutæµé‡ç°åº¦ 5.1. æ·»åŠ pod ç‰ˆæœ¬æ ‡ç­¾ ä¸ºäº†é€‚é…Apisixæˆ–Nginxç­‰k8sæµé‡ç½‘å…³çš„ç°åº¦é€»è¾‘ï¼Œæˆ‘ä»¬é€šè¿‡k8s serviceçš„æ–¹å¼æ¥åŠ¨æ€è·å–Podçš„IPï¼ˆå³endpointï¼‰,å…¶ä¸­åŒ…æ‹¬å…¨é‡podï¼Œç°åº¦podå’Œéç°åº¦podã€‚å› æ­¤æˆ‘ä»¬åœ¨æ¯æ¬¡å‘ç‰ˆçš„deploymentç»™podåŠ ä¸Šæ‰€å±çš„ç‰ˆæœ¬æ ‡ç­¾ã€‚\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx namespace: default spec: replicas: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: nginx version: \"1.26\" # å¢åŠ podå¯¹åº”çš„ç‰ˆæœ¬æ ‡ç­¾ï¼Œç”¨äºserviceè”åŠ¨ spec: containers: - image: nginx:1.26 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP 5.2. æ·»åŠ ç°åº¦podçš„service åˆ›å»ºä¸‰ä¸ªserviceæ¥å¯¹åº”å…¨é‡podï¼Œç°åº¦podå’Œéç°åº¦podã€‚\nå…¨é‡podçš„service\napiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: selector: app: nginx # å…¨é‡podçš„æ ‡ç­¾ ports: - name: http protocol: TCP port: 80 targetPort: 80 ç°åº¦podçš„service\napiVersion: v1 kind: Service metadata: name: nginx-canary namespace: default labels: version: canary spec: selector: app: nginx version: \"1.27\" # å¢åŠ ç°åº¦ç‰ˆæœ¬çš„æ ‡ç­¾ï¼Œå¯¹åº”ç°åº¦çš„pod ports: - name: http protocol: TCP port: 80 targetPort: 80 éç°åº¦podçš„service\napiVersion: v1 kind: Service metadata: name: nginx-stable namespace: default spec: selector: app: nginx version: \"1.26\" # å¢åŠ éç°åº¦ç‰ˆæœ¬çš„æ ‡ç­¾ï¼Œå¯¹åº”éç°åº¦çš„pod ports: - name: http protocol: TCP port: 80 targetPort: 80 5.3. åŠ¨æ€è·å–ç°åº¦çš„endpoint é€šè¿‡ä»¥ä¸Šserviceçš„é…ç½®ï¼Œå¯ä»¥åœ¨ç°åº¦çš„è¿‡ç¨‹ä¸­åŠ¨æ€è·å–ç°åº¦podçš„endpointï¼Œç„¶ååŠ¨æ€é›†æˆåˆ°æµé‡ç½‘å…³ä¸­ã€‚\nå…¨é‡podçš„serviceï¼šåœ¨æµé‡ç°åº¦å‰å’Œæµé‡ç°åº¦å®Œæˆåå¯ä»¥æŠŠæµé‡è°ƒåº¦åˆ°è¿™ä¸ªserviceä¸‹çš„podã€‚ ç°åº¦podçš„serviceï¼šå¯ä»¥è®¾ç½®æŠŠç°åº¦æµé‡è°ƒåº¦åˆ°canaryçš„serviceçš„podã€‚ éç°åº¦podçš„serviceï¼šå¯ä»¥æŠŠéç°åº¦æµé‡è°ƒåº¦åˆ°stableçš„serviceçš„podã€‚ ä»¥ä¸‹æ˜¯ç°åº¦å‰åendpointçš„å˜åŒ–ï¼š\n# kubectl get endpoints # ç°åº¦å‰ï¼šcanaryçš„podä¸º0 nginx 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 2d nginx-canary \u003cnone\u003e 2d nginx-stable 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 47h # ç°åº¦ä¸­ï¼šcanaryçš„podå¢åŠ  nginx 10.0.1.202:80,10.0.1.203:80,10.0.1.204:80 + 7 more... 2d \u003cnone\u003e nginx-canary 10.0.1.5:80 2d version=canary nginx-stable 10.0.1.202:80,10.0.1.203:80,10.0.1.204:80 + 6 more... 47h version=stable # ç°åº¦å®Œæˆï¼šstableçš„podä¸º0 NAME ENDPOINTS AGE nginx 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 2d nginx-canary 10.0.1.139:80,10.0.1.202:80,10.0.1.203:80 + 7 more... 2d nginx-stable \u003cnone\u003e 47h 6. æ€»ç»“ k8sçš„deploymenté»˜è®¤æ”¯æŒé‡‘ä¸é›€å‘å¸ƒï¼Œå¯ä»¥é€šè¿‡kubectl rolloutçš„å‘½ä»¤å®ç°ï¼Œé€šè¿‡é…ç½®deploymentä¸­çš„maxSurgeå’ŒmaxUnavailableæ¥æ§åˆ¶å‘ç‰ˆçš„èŠ‚å¥ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥æ§åˆ¶å‘ç‰ˆæ‰¹æ¬¡ï¼š\nkubectl rollout pauseï¼šæš‚åœå‘ç‰ˆ kubectl rollout resumeï¼šç»§ç»­å‘ç‰ˆ kubectl rollout undoï¼šå›æ»šç‰ˆæœ¬ ä½†ç›¸å¯¹æ¥è®²ï¼Œæ§åˆ¶ç²’åº¦æ²¡æœ‰é‚£ä¹ˆç²¾ç¡®ï¼Œè€ŒOpenKruiseçš„Rolloutsæ’ä»¶æä¾›äº†ä¸€ç§æ›´åŠ è½»ä¾¿ï¼Œå¯æ§çš„å‘ç‰ˆå·¥å…·ï¼Œä¸ªäººè®¤ä¸ºæœ€å¤§çš„ä¼˜åŠ¿æ˜¯é…ç½®ç®€å•ï¼Œä¸”æ— éœ€è¿ç§»å·¥ä½œè´Ÿè½½å’ŒPodsï¼Œå¯ä»¥æ›´å¥½çš„é›†æˆåˆ°ç°æœ‰çš„å®¹å™¨å¹³å°ä¸­ã€‚å› æ­¤å¯ä»¥é€šè¿‡è¯¥å·¥å…·æ¥å®ç°é‡‘ä¸é›€å‘ç‰ˆï¼ˆå¤šæ‰¹æ¬¡å‘ç‰ˆï¼‰ï¼Œå®ç°ç°åº¦çš„èƒ½åŠ›ã€‚\nå‚è€ƒï¼š\nhttps://openkruise.io/zh/rollouts/introduction https://openkruise.io/zh/rollouts/installation https://openkruise.io/zh/rollouts/user-manuals/basic-usage ","categories":"","description":"","excerpt":"1. kruise-rolloutç®€ä»‹ é‡‘ä¸é›€å‘å¸ƒæ˜¯é€æ­¥å°†æµé‡å¯¼å‘æ–°ç‰ˆæœ¬çš„åº”ç”¨ï¼Œä»¥æœ€å°åŒ–é£é™©ã€‚å…·ä½“è¿‡ç¨‹åŒ…æ‹¬ï¼š\néƒ¨ç½²æ–°ç‰ˆæœ¬çš„ Podã€‚ å°†ä¸€éƒ¨ â€¦","ref":"/kubernetes-notes/operation/deployment/kruise-rollout/","tags":["Kubernetes"],"title":"Kruise Rolloutå‘å¸ƒ"},{"body":"æœ¬æ–‡ä¸»è¦æè¿°å¸¸ç”¨çš„CNIæ’ä»¶çš„é€‰å‹ï¼Œä¸»è¦åŒ…æ‹¬ciliumï¼Œcalicoï¼Œflannelä¸‰ç§æ’ä»¶çš„å¯¹æ¯”ã€‚\n1. æŠ€æœ¯ç‰¹ç‚¹å¯¹æ¯” ç‰¹æ€§ Cilium Calico Flannel æ•°æ®é¢æŠ€æœ¯ eBPF åŠ é€Ÿ åŸºäº iptablesï¼ˆæ”¯æŒ eBPFï¼‰ vxlanã€host-gwã€ipip ç­‰éš§é“æŠ€æœ¯ è½¬å‘æ•ˆç‡ é«˜ï¼ˆå†…æ ¸çº§åŠ é€Ÿï¼Œç›´é€šæµé‡ï¼‰ é«˜ï¼ˆæ”¯æŒåŸç”Ÿè·¯ç”±ï¼‰ ä¸­ï¼ˆéš§é“æŠ€æœ¯å¢åŠ å¼€é”€ï¼‰ å¯æ‰©å±•æ€§ ä¼˜ï¼ˆæ”¯æŒé«˜çº§ L7 ç­–ç•¥ï¼‰ ä¼˜ï¼ˆæ”¯æŒåŸç”Ÿè·¯ç”±å’Œç®€å•ç½‘ç»œç­–ç•¥ï¼‰ è¾ƒä½ï¼ˆä»¥ L3 ç½‘ç»œä¸ºä¸»ï¼‰ å»¶è¿Ÿ ä½ï¼ˆæ— éœ€é¢å¤–éš§é“æˆ–è§„åˆ™ï¼‰ ä½ï¼ˆæ— éš§é“æˆ– eBPF æ¨¡å¼ï¼‰ è¾ƒé«˜ï¼ˆéš§é“å°è£…å¢åŠ å»¶è¿Ÿï¼‰ ååé‡ é«˜ï¼ˆeBPF é«˜æ•ˆè½¬å‘ï¼‰ ä¸­ï¼ˆä¾èµ– iptables æˆ– eBPFï¼‰ è¾ƒä½ï¼ˆéš§é“å¼€é”€æ˜¾è‘—ï¼‰ 2. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” æ€§èƒ½æŒ‡æ ‡ Cilium Calico Flannel ååé‡ é«˜ï¼ˆeBPF é«˜æ•ˆè½¬å‘ï¼‰ ä¸­-é«˜ï¼ˆå–å†³äºæ¨¡å¼ï¼‰ è¾ƒä½ï¼ˆéš§é“å°è£…æŸè€—è¾ƒå¤§ï¼‰ å»¶è¿Ÿ ä½ï¼ˆç›´æ¥è·¯ç”±æ¨¡å¼æœ€ä½³ï¼‰ è¾ƒä½ï¼ˆééš§é“æ¨¡å¼è¡¨ç°è‰¯å¥½ï¼‰ è¾ƒé«˜ï¼ˆéš§é“å¢åŠ å»¶è¿Ÿï¼‰ CPUä½¿ç”¨ è¾ƒé«˜ï¼ˆeBPF å’Œå¯è§‚æµ‹æ€§åŠŸèƒ½ï¼‰ ä¸­ï¼ˆiptables/eBPF å¼€é”€ï¼‰ ä½ï¼ˆç®€å•æ¶æ„ï¼‰ å†…å­˜ä½¿ç”¨ è¾ƒé«˜ï¼ˆåŠŸèƒ½ä¸°å¯Œï¼‰ ä¸­ ä½ 3. æµ‹è¯•æ•°æ®ç¤ºä¾‹ ä»¥ä¸‹æ˜¯æ ¹æ®å…¸å‹æµ‹è¯•åœºæ™¯æ€»ç»“çš„æŒ‡æ ‡ï¼ˆå•ä½ä¸ºååé‡ Mbps å’Œå»¶è¿Ÿ msï¼‰ï¼š\næµ‹è¯•åœºæ™¯ Cilium Calico Flannel ååé‡ (å•èŠ‚ç‚¹) ~9,000 Mbps ~8,500 Mbps ~6,000 Mbps ååé‡ (è·¨èŠ‚ç‚¹) ~8,000 Mbps ~7,500 Mbps ~5,000 Mbps å»¶è¿Ÿ (å•èŠ‚ç‚¹) ~0.2 ms ~0.3 ms ~1.0 ms å»¶è¿Ÿ (è·¨èŠ‚ç‚¹) ~0.4 ms ~0.5 ms ~2.0 ms 2020å¹´æµ‹è¯•æ•°æ®ï¼š\næ•°æ®æ¥æºï¼šBenchmark results of Kubernetes network plugins (CNI) over 10Gbit/s network (Updated: August 2020)\n[2024å¹´]å•ä½å¸¦å®½æ¶ˆè€—çš„CPUå’Œå†…å­˜æ•°æ®ï¼š\næ•°æ®æ¥æºï¼šBenchmark results of Kubernetes network plugins (CNI) over 40Gbit/s network -2024\nä»¥ä¸Šå¯ä»¥çœ‹å‡ºciliumå•ä½æ¶ˆè€—çš„CPUå’Œå†…å­˜ç›¸æ¯”äºflannelé«˜ã€‚\n4. ç½‘ç»œæ€§èƒ½åˆ†æ 4.1. Cilium ååé‡ï¼šåŸºäº eBPF çš„æ•°æ®é¢æŠ€æœ¯ï¼Œå¯ç›´æ¥åœ¨ Linux å†…æ ¸ä¸­é«˜æ•ˆè½¬å‘æµé‡ï¼Œå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚ä½¿ç”¨ç›´è¿è·¯ç”±æ¨¡å¼ï¼ˆ--tunnel=disabledï¼‰æ—¶ï¼Œè¿›ä¸€æ­¥å‡å°‘å°è£…å¼€é”€ã€‚ å»¶è¿Ÿï¼šæ”¯æŒ Sidecar-less çš„æœåŠ¡ç½‘æ ¼æ¶æ„ï¼Œèƒ½å¤Ÿé™ä½æœåŠ¡é—´é€šä¿¡å»¶è¿Ÿã€‚ èµ„æºæ¶ˆè€—ï¼šç”±äºå…¶é«˜çº§åŠŸèƒ½ï¼ˆå¦‚ Hubble å¯è§‚æµ‹æ€§å’Œ L7 ç­–ç•¥ï¼‰ï¼Œåœ¨ CPU å’Œå†…å­˜ä½¿ç”¨ä¸Šé«˜äºå…¶ä»–æ’ä»¶ã€‚ 4.2. Calico ååé‡ï¼šééš§é“æ¨¡å¼ä¸‹ï¼ŒåŸºäº BGP çš„åŸç”Ÿè·¯ç”±æ€§èƒ½æ¥è¿‘è£¸æœºæ°´å¹³ï¼›å¯ç”¨ eBPF æ¨¡å¼æ—¶ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚ å»¶è¿Ÿï¼šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ç½‘ç»œç­–ç•¥ä¸‹å¯èƒ½å¢åŠ å»¶è¿Ÿã€‚ èµ„æºæ¶ˆè€—ï¼šèµ„æºä½¿ç”¨é€‚ä¸­ï¼Œé€‚åˆå¤§å¤šæ•°ç”Ÿäº§ç¯å¢ƒã€‚ 4.3. Flannel ååé‡ï¼šç”±äºé‡‡ç”¨ VXLANã€IPIP ç­‰éš§é“å°è£…æ–¹å¼ï¼Œå…¶æ€§èƒ½é€šå¸¸ä¸å¦‚ Cilium å’Œ Calicoã€‚ å»¶è¿Ÿï¼šå°è£…å’Œè§£å°è£…çš„é¢å¤–æ“ä½œå¯¼è‡´å»¶è¿Ÿå¢åŠ ã€‚ èµ„æºæ¶ˆè€—ï¼šæ¶æ„ç®€å•ï¼Œèµ„æºä½¿ç”¨æœ€ä½ï¼Œé€‚åˆèµ„æºæœ‰é™çš„å°å‹é›†ç¾¤ã€‚ 5. ä¸šåŠ¡åœºæ™¯é€‰å‹ 1. Ciliumï¼šé€‚åˆé«˜æ€§èƒ½ä¸å®‰å…¨éœ€æ±‚çš„åœºæ™¯\né€‚ç”¨åœºæ™¯ï¼š å¾®æœåŠ¡æ¶æ„ï¼šCilium çš„ eBPF æŠ€æœ¯æ”¯æŒ L7 æ•°æ®åŒ…è¿‡æ»¤ã€æœåŠ¡å¯è§‚æµ‹æ€§å’Œæ—  Sidecar æœåŠ¡ç½‘æ ¼ï¼Œéå¸¸é€‚åˆå¤æ‚å¾®æœåŠ¡ç¯å¢ƒã€‚ è¾¹ç¼˜è®¡ç®—ï¼šåœ¨è¾¹ç¼˜èŠ‚ç‚¹ä¸Šï¼Œéœ€è¦ä½å»¶è¿Ÿå’Œé«˜ååé‡ï¼ŒCilium çš„ç›´è¿è·¯ç”±æ¨¡å¼ï¼ˆ--tunnel=disabledï¼‰éå¸¸é«˜æ•ˆã€‚ å¤šäº‘å’Œæ··åˆäº‘ï¼šæ”¯æŒå¤šç§é«˜çº§ç½‘ç»œåŠŸèƒ½ï¼Œå¦‚ç½‘ç»œç­–ç•¥çš„çµæ´»é…ç½®å’Œé€æ˜çš„åŠ å¯†ã€‚ å±€é™æ€§ï¼š éƒ¨ç½²å¤æ‚åº¦ç›¸å¯¹è¾ƒé«˜ï¼Œå¯¹ Linux å†…æ ¸ç‰ˆæœ¬æœ‰è¦æ±‚ï¼ˆæ¨è 5.3+ï¼‰ã€‚ èµ„æºæ¶ˆè€—æ¯” Flannel é«˜ã€‚ 2. Calicoï¼šé€‚åˆå¤§è§„æ¨¡ã€çµæ´»ç­–ç•¥çš„ä¼ä¸šé›†ç¾¤\né€‚ç”¨åœºæ™¯ï¼š å¤§è§„æ¨¡ Kubernetes é›†ç¾¤ï¼šåŸºäº BGP çš„ç½‘ç»œè·¯ç”±èƒ½å¤Ÿé«˜æ•ˆæ‰©å±•ï¼Œé€‚åˆå…¬æœ‰äº‘å’Œä¼ä¸šçº§å¤§è§„æ¨¡é›†ç¾¤ã€‚ æ³¨é‡å®‰å…¨ç­–ç•¥ï¼šæ”¯æŒä¸°å¯Œçš„ç½‘ç»œå®‰å…¨ç­–ç•¥ï¼Œå¹¶æä¾›å¯¹æ¥ eBPF çš„èƒ½åŠ›ï¼Œå…¼é¡¾æ€§èƒ½å’Œç­–ç•¥çµæ´»æ€§ã€‚ æ··åˆéƒ¨ç½²ï¼šCalico å¯ä»¥åœ¨é Kubernetes å·¥ä½œè´Ÿè½½ä¸­å®ç°ä¸€è‡´çš„ç½‘ç»œç­–ç•¥ã€‚ å±€é™æ€§ï¼š é»˜è®¤åŸºäº iptables çš„å®ç°åœ¨é«˜è´Ÿè½½ä¸‹æ€§èƒ½å¯èƒ½ä¸å¦‚ Cilium çš„ eBPF æ•°æ®é¢ã€‚ ç½‘ç»œç­–ç•¥å¤æ‚åº¦è¾ƒé«˜æ—¶ï¼Œå¯èƒ½å¢åŠ è¿ç»´å·¥ä½œé‡ã€‚ 3. Flannelï¼šé€‚åˆè½»é‡çº§å’Œèµ„æºæœ‰é™çš„é›†ç¾¤\né€‚ç”¨åœºæ™¯ï¼š å°å‹é›†ç¾¤ï¼šFlannel æ¶æ„ç®€å•ï¼Œèµ„æºä½¿ç”¨å°‘ï¼Œé€‚åˆè½»é‡åŒ–çš„ Kubernetes éƒ¨ç½²ã€‚ æµ‹è¯•ç¯å¢ƒï¼šæ€§èƒ½éœ€æ±‚è¾ƒä½çš„å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒä¸­ï¼Œå¯ä»¥å¿«é€Ÿæ­å»ºå’Œè¿è¡Œã€‚ è¾¹ç¼˜è®¡ç®—ï¼ˆéé«˜æ€§èƒ½ï¼‰ï¼šå¯¹ç½‘ç»œæ€§èƒ½è¦æ±‚è¾ƒä½çš„å°å‹è¾¹ç¼˜èŠ‚ç‚¹å¯ä»¥ä½¿ç”¨ã€‚ å±€é™æ€§ï¼š åœ¨ååé‡å’Œå»¶è¿Ÿä¸Šä¸å¦‚ Cilium å’Œ Calicoï¼Œå°¤å…¶æ˜¯éœ€è¦å¤§é‡éš§é“å°è£…æ—¶ã€‚ ç¼ºä¹é«˜çº§ç½‘ç»œåŠŸèƒ½ï¼Œä¾‹å¦‚å¤æ‚çš„ç½‘ç»œç­–ç•¥å’Œè§‚æµ‹èƒ½åŠ›ã€‚ 4. æ¨èé€‰æ‹©æ€»ç»“\nåœºæ™¯ç±»å‹ æ¨èæ’ä»¶ åŸå›  é«˜æ€§èƒ½å¾®æœåŠ¡æ¶æ„ Cilium æä¾› eBPF æŠ€æœ¯ï¼Œæ”¯æŒå¤æ‚ç­–ç•¥å’Œä½å»¶è¿Ÿç½‘ç»œ å¤§è§„æ¨¡ä¼ä¸šé›†ç¾¤ Calico ç¨³å®šã€çµæ´»ï¼Œé€‚åˆå¤šæ ·åŒ–å’Œå¤§è§„æ¨¡ Kubernetes éƒ¨ç½² èµ„æºå—é™ç¯å¢ƒ Flannel ç®€å•æ˜“ç”¨ï¼Œèµ„æºæ¶ˆè€—ä½ è¾¹ç¼˜è®¡ç®— Cilium/Flannel Cilium é€‚åˆé«˜æ€§èƒ½éœ€æ±‚ï¼ŒFlannel é€‚åˆè½»é‡çº§èŠ‚ç‚¹ æ··åˆäº‘/å¤šäº‘ Cilium/Calico Cilium æ”¯æŒé€æ˜åŠ å¯†å’Œç°ä»£æ¶æ„ï¼ŒCalico æä¾›çµæ´»ç½‘ç»œç­–ç•¥æ”¯æŒ å‚è€ƒï¼š\nhttps://docs.cilium.io/en/latest/operations/performance/benchmark/ https://cilium.io/blog/2018/12/03/cni-performance/ https://cilium.io/blog/2021/05/11/cni-benchmark/ Benchmark results of Kubernetes network plugins (CNI) over 40Gbit/s network -2024 Benchmark results of Kubernetes network plugins (CNI) over 10Gbit/s network (Updated: August 2020) ","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦æè¿°å¸¸ç”¨çš„CNIæ’ä»¶çš„é€‰å‹ï¼Œä¸»è¦åŒ…æ‹¬ciliumï¼Œcalicoï¼Œflannelä¸‰ç§æ’ä»¶çš„å¯¹æ¯”ã€‚\n1. â€¦","ref":"/kubernetes-notes/network/cni/cni-plugin-comparison/","tags":["CNI"],"title":"CNIæ’ä»¶é€‰å‹"},{"body":"å¼€å‘ä¸€ä¸ªk8s operatorç»„ä»¶ä¸»è¦ä¼šç”¨åˆ°ä»¥ä¸‹å‡ ä¸ªä»“åº“æˆ–å·¥å…·ï¼š\nkubebuilder/operator-sdkï¼šä¸»è¦ç”¨äºåˆ›å»ºCRDå¯¹è±¡ã€‚\ncontroller-managerï¼šä¸»è¦ç”¨äºå®ç°operatorçš„controlleré€»è¾‘ã€‚\næœ¬æ–‡ä»¥kubebuilderçš„å·¥å…·å’Œcontroller-manager exampleä¸ºä¾‹ã€‚\n1. å‡†å¤‡å·¥å…·ç¯å¢ƒåŠåˆ›å»ºé¡¹ç›® å®‰è£…kubebuilderå·¥å…·\ncurl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder \u0026\u0026 mv kubebuilder /usr/local/bin/ åˆ›å»ºoperatoré¡¹ç›®ç›®å½•\n$ kubebuilder init --domain example.com --license apache2 --owner \"Your Name\" --repo github.com/example/my-operator å°† example.com æ›¿æ¢ä¸ºä½ çš„åŸŸåï¼ŒYour Name æ›¿æ¢ä¸ºä½ çš„åå­—ï¼Œgithub.com/example/my-operator æ›¿æ¢ä¸ºä½ çš„ GitHub ä»“åº“ã€‚\n2. åˆ›å»ºCRDå¯¹è±¡ $ kubebuilder create api --group mygroup --version v1alpha1 --kind MyResource è¿™å°†åœ¨ api/v1alpha1 æ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ªåä¸º myresource_types.go çš„æ–‡ä»¶ï¼Œå…¶ä¸­å®šä¹‰äº† MyResource èµ„æºçš„è§„èŒƒå’ŒçŠ¶æ€ã€‚\nç¼–è¾‘ api/v1alpha1/myresource_types.go æ–‡ä»¶ï¼Œæ·»åŠ è‡ªå®šä¹‰èµ„æºçš„è§„èŒƒå’ŒçŠ¶æ€å­—æ®µã€‚\n3. å®ç°æ§åˆ¶å™¨é€»è¾‘ åœ¨ controllers/myresource_controller.go æ–‡ä»¶ä¸­å®ç°æ§åˆ¶å™¨é€»è¾‘ã€‚è¯¥éƒ¨åˆ†æ˜¯ä¸åŒçš„CRDå®ç°è‡ªå®šä¹‰é€»è¾‘çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚\nä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\nå®šä¹‰ReconcileMyResourceç»“æ„ä½“ã€‚\nå®ç°func (r *ReconcileMyResource) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, error)å‡½æ•°æ¥å£ã€‚\nå®šä¹‰finalizeré€»è¾‘ã€‚\nå®šä¹‰reconcileé€»è¾‘ã€‚\npackage controllers import ( \"context\" \"github.com/go-logr/logr\" \"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\" \"sigs.k8s.io/controller-runtime/pkg/controller/inject\" \"sigs.k8s.io/controller-runtime/pkg/reconcile\" myv1alpha1 \"github.com/example/my-operator/api/v1alpha1\" corev1 \"k8s.io/api/core/v1\" \"k8s.io/apimachinery/pkg/api/errors\" \"k8s.io/apimachinery/pkg/runtime\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"sigs.k8s.io/controller-runtime/pkg/controller\" logf \"sigs.k8s.io/controller-runtime/pkg/log\" \"sigs.k8s.io/controller-runtime/pkg/log/zap\" \"sigs.k8s.io/controller-runtime/pkg/manager\" \"sigs.k8s.io/controller-runtime/pkg/reconcile\" \"sigs.k8s.io/controller-runtime/pkg/source\" ) var log = logf.Log.WithName(\"controller_myresource\") // Add creates a new MyResource Controller and adds it to the Manager. The Manager will set fields on the Controller // and Start it when the Manager is Started. func Add(mgr manager.Manager) error { return add(mgr, newReconciler(mgr)) } // newReconciler returns a new reconcile.Reconciler func newReconciler(mgr manager.Manager) reconcile.Reconciler { return \u0026ReconcileMyResource{client: mgr.GetClient(), scheme: mgr.GetScheme()} } // blank assignment to verify that ReconcileMyResource implements reconcile.Reconciler var _ reconcile.Reconciler = \u0026ReconcileMyResource{} // ReconcileMyResource reconciles a MyResource object type ReconcileMyResource struct { client client.Client scheme *runtime.Scheme } // Reconcile reads that state of the cluster for a MyResource object and makes changes based on the state read // and what is in the MyResource.Spec func (r *ReconcileMyResource) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, error) { reqLogger := log.WithValues(\"Namespace\", request.Namespace, \"Name\", request.Name) reqLogger.Info(\"Reconciling MyResource\") // Fetch the MyResource instance myresource := \u0026myv1alpha1.MyResource{} err := r.client.Get(ctx, request.NamespacedName, myresource) if err != nil { if errors.IsNotFound(err) { // Request object not found, could have been deleted after reconcile request. // Owned objects are automatically garbage collected. For additional cleanup logic use finalizers. // Return and don't requeue return reconcile.Result{}, nil } // Error reading the object - requeue the request. return reconcile.Result{}, err } // Add finalizer logic here // Add reconcile logic here return reconcile.Result{}, nil } 4. æ³¨å†Œæ§åˆ¶å™¨ åœ¨ main.go æ–‡ä»¶ä¸­æ³¨å†Œæ§åˆ¶å™¨ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤\nctrl.SetLogger(zap.New())ï¼šæ³¨å†Œæ—¥å¿—å·¥å…·\nctrl.NewManagerï¼šåˆ›å»ºä¸€ä¸ªmanagerï¼Œå¿…è¦æ—¶å¯è®¾ç½®é€‰ä¸»é€»è¾‘ã€‚\nctrl.NewControllerManagedBy(mgr)ï¼šé¢„è®¡mgråˆ›å»ºmanager controllerå¹¶æ³¨å†ŒReconcilerã€‚\nmgr.Start(ctrl.SetupSignalHandler()): è¿è¡Œmgrã€‚\nfunc main() { // set logger ctrl.SetLogger(zap.New()) // Set up a Manager mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: myv1alpha1.SchemeBuilder.Scheme, MetricsBindAddress: *metricsHost, Port: 9443, LeaderElection: *enableLeaderElection, LeaderElectionID: \"my-operator-lock\", }) if err != nil { setupLog.Error(err, \"unable to start manager\") os.Exit(1) } // in a real controller, we'd create a new scheme for this err = api.AddToScheme(mgr.GetScheme()) if err != nil { setupLog.Error(err, \"unable to add scheme\") os.Exit(1) } // Create a new Reconciler r := \u0026ReconcileMyResource{ client: m.GetClient(), scheme: m.GetScheme(), } // Create a new Controller and register the Reconciler err = ctrl.NewControllerManagedBy(mgr). For(\u0026myv1alpha1.MyResource{}). // è‡ªå®šä¹‰crd Complete(r) // æ³¨å†ŒReconciler if err != nil { setupLog.Error(err, \"unable to create controller\") os.Exit(1) } err = ctrl.NewWebhookManagedBy(mgr). For(\u0026api.ChaosPod{}). Complete() if err != nil { setupLog.Error(err, \"unable to create webhook\") os.Exit(1) } setupLog.Info(\"starting manager\") if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \"problem running manager\") os.Exit(1) } } å‚è€ƒï¼š\nImplementing a controller - The Kubebuilder Book\ncontroller-runtimeæºç åˆ†æ | æä¹¾å¤çš„åšå®¢\nKubebuilder - SDK for building Kubernetes APIs using CRDs\noperator-framework/operator-sdk: SDK for building Kubernetes applications.\nhttps://blog.hdls.me/16500403497126.html\nkubebuilder-cronjob_controller\n","categories":"","description":"","excerpt":"å¼€å‘ä¸€ä¸ªk8s operatorç»„ä»¶ä¸»è¦ä¼šç”¨åˆ°ä»¥ä¸‹å‡ ä¸ªä»“åº“æˆ–å·¥å…·ï¼š\nkubebuilder/operator-sdkï¼šä¸»è¦ç”¨äºåˆ›å»ºCRDå¯¹ â€¦","ref":"/kubernetes-notes/develop/operator/develop-operator/","tags":["Operator"],"title":"å¦‚ä½•å¼€å‘ä¸€ä¸ªOperator"},{"body":" æœ¬æ–‡ä»¥commit idï¼š180282663457080119a1bc6076cce20c922b5c50ï¼Œ å¯¹åº”ç‰ˆæœ¬tag:Â v1.2.1Â çš„æºç åˆ†ætunnel-serverçš„å®ç°é€»è¾‘ã€‚\n1. Tunnel-serverç®€ä»‹ äº‘ä¸è¾¹ä¸€èˆ¬ä½äºä¸åŒç½‘ç»œå¹³é¢ï¼ŒåŒæ—¶è¾¹ç¼˜èŠ‚ç‚¹æ™®éä½äºé˜²ç«å¢™å†…éƒ¨ï¼Œé‡‡ç”¨äº‘(ä¸­å¿ƒ)è¾¹ååŒæ¶æ„ï¼Œå°†å¯¼è‡´åŸç”Ÿ K8s ç³»ç»Ÿçš„è¿ç»´ç›‘æ§èƒ½åŠ›é¢ä¸´å¦‚ä¸‹æŒ‘æˆ˜:\nK8s åŸç”Ÿè¿ç»´èƒ½åŠ›ç¼ºå¤±(å¦‚ kubectl logs/exec ç­‰æ— æ³•æ‰§è¡Œ) ç¤¾åŒºä¸»æµç›‘æ§è¿ç»´ç»„ä»¶æ— æ³•å·¥ä½œ(å¦‚ Prometheus/metrics-server ) åœ¨ OpenYurt ä¸­ï¼Œå¼•å…¥äº†ä¸“é—¨çš„ç»„ä»¶ YurtTunnel è´Ÿè´£è§£å†³äº‘è¾¹é€šä¿¡é—®é¢˜ã€‚åå‘é€šé“æ˜¯è§£å†³è·¨ç½‘ç»œé€šä¿¡çš„ä¸€ç§å¸¸è§æ–¹å¼ï¼Œè€Œ YurtTunnel çš„æœ¬è´¨ä¹Ÿæ˜¯ä¸€ä¸ªåå‘é€šé“ã€‚ å®ƒæ˜¯ä¸€ä¸ªå…¸å‹çš„C/Sç»“æ„çš„ç»„ä»¶ï¼Œç”±éƒ¨ç½²äºäº‘ç«¯çš„ YurtTunnelServer å’Œéƒ¨ç½²äºè¾¹ç¼˜èŠ‚ç‚¹ä¸Šçš„ YurtTunnelAgentç»„æˆã€‚\næœ¬æ–‡ä¸»è¦åˆ†ætunnel-serverçš„ä»£ç†é€»è¾‘ã€‚\nä»¥ä¸‹æ˜¯åŸºæœ¬æ¶æ„å›¾ï¼š\npkgä¸­çš„ç›®å½•ç»“æ„ï¼š\nyurttunnel â”œâ”€â”€ agent # tunnel agentä»£ç  â”œâ”€â”€ constants # å¸¸é‡å€¼ â”œâ”€â”€ handlerwrapper â”œâ”€â”€ informers â”œâ”€â”€ kubernetes # k8s clientsetå·¥å…·åŒ… â”œâ”€â”€ server # æ ¸å¿ƒä»£ç ï¼š tunnel serveré€»è¾‘ â”œâ”€â”€ trafficforward # iptableså’Œdnsæ“ä½œ â””â”€â”€ util 2. NewYurttunnelServerCommand mainå‡½æ•°å…¥å£ï¼š\nfunc main() { cmd := app.NewYurttunnelServerCommand(stop) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { klog.Fatalf(\"%s failed: %s\", projectinfo.GetServerName(), err) } } ä»¥ä¸‹æ˜¯NewYurttunnelServerCommandæ„é€ å‡½æ•°ï¼Œå¸¸è§çš„ä¸‰ä»¶å¥—ï¼Œä¸åšå±•å¼€ï¼š\nè¯»å–å‚æ•°ï¼šserverOptions.AddFlags(cmd.Flags())\nç”Ÿæˆé…ç½®ï¼šcfg.Complete()\næ‰§è¡ŒRunå‡½æ•°ï¼šæ ¸å¿ƒé€»è¾‘\nfunc NewYurttunnelServerCommand(stopCh \u003c-chan struct{}) *cobra.Command { serverOptions := options.NewServerOptions() cmd := \u0026cobra.Command{ Use: \"Launch \" + projectinfo.GetServerName(), Short: projectinfo.GetServerName() + \" sends requests to \" + projectinfo.GetAgentName(), RunE: func(c *cobra.Command, args []string) error { ... cfg, err := serverOptions.Config() if err != nil { return err } if err := Run(cfg.Complete(), stopCh); err != nil { return err } return nil }, Args: cobra.NoArgs, } serverOptions.AddFlags(cmd.Flags()) return cmd } 3. Run(cfg.Complete(), stopCh) Runå‡½æ•°æœ€ç»ˆæ˜¯è¿è¡Œä¸€ä¸ªtunnelserverçš„å¸¸é©»è¿›ç¨‹ã€‚åœ¨ä¹‹å‰ä¼šåšä¸€äº›controlleræˆ–managerçš„å‡†å¤‡å·¥ä½œã€‚\nå…¶ä¸­åŒ…æ‹¬ï¼š\nDNS controller\nIP table manager\ncertificate manager\nRegisterInformersForTunnelServer\né¦–å…ˆæ˜¯æ„å»ºå¹¶è¿è¡Œä¸Šè¿°çš„manageræˆ–controllerï¼Œ æºç ä¸­çš„æ³¨é‡Šä¹Ÿå¤§æ¦‚æè¿°äº†ä¸»è¦æµç¨‹ï¼š\næ³¨å†Œtunnelæ‰€éœ€çš„SharedInformerFactory\nè¿è¡Œdns controller\nè¿è¡Œip table manager\nç»™tunnel serveråˆ›å»ºcertificate manager\nç»™tunnel agent åˆ›å»ºcertificate manager\nåˆ›å»ºhandler wrappers\nç”ŸæˆTLS è¯ä¹¦\nè¿è¡Œtunnel server\nä»¥ä¸‹æ˜¯éƒ¨åˆ†ä»£ç ï¼Œå·²åˆ é™¤æ— å…³ç´§è¦çš„ä»£ç ï¼š\n// run starts the yurttunel-server func Run(cfg *config.CompletedConfig, stopCh \u003c-chan struct{}) error { var wg sync.WaitGroup // register informers that tunnel server need informers.RegisterInformersForTunnelServer(cfg.SharedInformerFactory) // 0. start the DNS controller if cfg.EnableDNSController { dnsController, err := dns.NewCoreDNSRecordController(...) go dnsController.Run(stopCh) } // 1. start the IP table manager if cfg.EnableIptables { iptablesMgr, err := iptables.NewIptablesManagerWithIPFamily(...) wg.Add(1) go iptablesMgr.Run(stopCh, \u0026wg) } // 2. create a certificate manager for the tunnel server certManagerFactory := certfactory.NewCertManagerFactory(cfg.Client) ips, dnsNames, err := getTunnelServerIPsAndDNSNamesBeforeInformerSynced(cfg.Client, stopCh) serverCertMgr, err := certManagerFactory.New(...) serverCertMgr.Start() // 3. create a certificate manager for the tunnel proxy client tunnelProxyCertMgr, err := certManagerFactory.New(...) tunnelProxyCertMgr.Start() // 4. create handler wrappers mInitializer := initializer.NewMiddlewareInitializer(cfg.SharedInformerFactory) wrappers, err := wraphandler.InitHandlerWrappers(mInitializer, cfg.IsIPv6()) // after all of informers are configured completed, start the shared index informer cfg.SharedInformerFactory.Start(stopCh) // 5. waiting for the certificate is generated _ = wait.PollUntil(5*time.Second, func() (bool, error) { // keep polling until the certificate is signed if serverCertMgr.Current() != nil \u0026\u0026 tunnelProxyCertMgr.Current() != nil { return true, nil } klog.Infof(\"waiting for the master to sign the %s certificate\", projectinfo.GetServerName()) return false, nil }, stopCh) // 6. generate the TLS configuration based on the latest certificate tlsCfg, err := certmanager.GenTLSConfigUseCurrentCertAndCertPool(serverCertMgr.Current, cfg.RootCert, \"server\") proxyClientTlsCfg, err := certmanager.GenTLSConfigUseCurrentCertAndCertPool(tunnelProxyCertMgr.Current, cfg.RootCert, \"client\") // 7. start the server ts := server.NewTunnelServer( cfg.EgressSelectorEnabled, cfg.InterceptorServerUDSFile, cfg.ListenAddrForMaster, cfg.ListenInsecureAddrForMaster, cfg.ListenAddrForAgent, cfg.ServerCount, tlsCfg, proxyClientTlsCfg, wrappers, cfg.ProxyStrategy) if err := ts.Run(); err != nil { return err } // 8. start meta server util.RunMetaServer(cfg.ListenMetaAddr) \u003c-stopCh wg.Wait() return nil } 3. TunnelServer anpTunnelServerå®ç°äº†TunnelServerçš„æ¥å£ï¼Œä»¥ä¸‹åˆ†æTunnelServer.Runçš„éƒ¨åˆ†ã€‚\nrunéƒ¨åˆ†ä¸»è¦è¿è¡Œäº†ä¸‰ä¸ªserver\nproxyServerï¼š ä¸»è¦æ˜¯é‡å®šå‘apiserverçš„è¯·æ±‚åˆ°tunnel server\nMasterServerï¼š\nAgentServerï¼šä¸»è¦è¿è¡Œä¸€ä¸ªgrpc serverä¸tunnel agentè¿æ¥ï¼Œå³äº‘è¾¹åå‘éš§é“\nä»£ç å‚è€ƒï¼š/pkg/yurttunnel/server/anpserver.go\n// Run runs the yurttunnel-server func (ats *anpTunnelServer) Run() error { proxyServer := anpserver.NewProxyServer(uuid.New().String(), []anpserver.ProxyStrategy{anpserver.ProxyStrategy(ats.proxyStrategy)}, ats.serverCount, \u0026anpserver.AgentTokenAuthenticationOptions{}) // 1. start the proxier proxierErr := runProxier( \u0026anpserver.Tunnel{Server: proxyServer}, ats.egressSelectorEnabled, ats.interceptorServerUDSFile, ats.tlsCfg) wrappedHandler, err := wh.WrapHandler( NewRequestInterceptor(ats.interceptorServerUDSFile, ats.proxyClientTlsCfg), ats.wrappers, ) // 2. start the master server masterServerErr := runMasterServer( wrappedHandler, ats.egressSelectorEnabled, ats.serverMasterAddr, ats.serverMasterInsecureAddr, ats.tlsCfg) // 3. start the agent server agentServerErr := runAgentServer(ats.tlsCfg, ats.serverAgentAddr, proxyServer) return nil } 4. runAgentServer runAgentServerä¸»è¦è¿è¡Œä¸€ä¸ªgrpc serverä¸edgeç«¯çš„agentå½¢æˆgrpcè¿æ¥ã€‚\n// runAgentServer runs a grpc server that handles connections from the yurttunel-agent // NOTE agent server is responsible for managing grpc connection yurttunel-server // and yurttunnel-agent, and the proxy server is responsible for redirecting requests // to corresponding yurttunel-agent func runAgentServer(tlsCfg *tls.Config, agentServerAddr string, proxyServer *anpserver.ProxyServer) error { serverOption := grpc.Creds(credentials.NewTLS(tlsCfg)) ka := keepalive.ServerParameters{ // Ping the client if it is idle for `Time` seconds to ensure the // connection is still active Time: constants.YurttunnelANPGrpcKeepAliveTimeSec * time.Second, // Wait `Timeout` second for the ping ack before assuming the // connection is dead Timeout: constants.YurttunnelANPGrpcKeepAliveTimeoutSec * time.Second, } grpcServer := grpc.NewServer(serverOption, grpc.KeepaliveParams(ka)) anpagent.RegisterAgentServiceServer(grpcServer, proxyServer) listener, err := net.Listen(\"tcp\", agentServerAddr) klog.Info(\"start handling connection from agents\") if err != nil { return fmt.Errorf(\"fail to listen to agent on %s: %w\", agentServerAddr, err) } go grpcServer.Serve(listener) return nil } 5. Interceptor Interceptorï¼ˆè¯·æ±‚æ‹¦æˆªå™¨ï¼‰æ‹¦æˆªkube-apiserverçš„è¯·æ±‚è½¬å‘ç»™tunnelï¼Œé€šè¿‡tunnelè¯·æ±‚kubeletã€‚\næµç¨‹å›¾ï¼š\nä»£ç åœ°å€ï¼š/pkg/yurttunnel/server/interceptor.go\n// NewRequestInterceptor creates a interceptor object that intercept request from kube-apiserver func NewRequestInterceptor(udsSockFile string, cfg *tls.Config) *RequestInterceptor { if len(udsSockFile) == 0 || cfg == nil { return nil } cfg.InsecureSkipVerify = true contextDialer := func(addr string, header http.Header, isTLS bool) (net.Conn, error) { klog.V(4).Infof(\"Sending request to %q.\", addr) proxyConn, err := net.Dial(\"unix\", udsSockFile) if err != nil { return nil, fmt.Errorf(\"dialing proxy %q failed: %w\", udsSockFile, err) } var connectHeaders string for _, h := range supportedHeaders { if v := header.Get(h); len(v) != 0 { connectHeaders = fmt.Sprintf(\"%s\\r\\n%s: %s\", connectHeaders, h, v) } } fmt.Fprintf(proxyConn, \"CONNECT %s HTTP/1.1\\r\\nHost: localhost%s\\r\\n\\r\\n\", addr, connectHeaders) br := newBufioReader(proxyConn) defer putBufioReader(br) res, err := http.ReadResponse(br, nil) if err != nil { proxyConn.Close() return nil, fmt.Errorf(\"reading HTTP response from CONNECT to %s via proxy %s failed: %w\", addr, udsSockFile, err) } if res.StatusCode != 200 { proxyConn.Close() return nil, fmt.Errorf(\"proxy error from %s while dialing %s, code %d: %v\", udsSockFile, addr, res.StatusCode, res.Status) } // if the request scheme is https, setup a tls connection over the // proxy tunnel (i.e. interceptor \u003c--tls--\u003e kubelet) if isTLS { tlsTunnelConn := tls.Client(proxyConn, cfg) if err := tlsTunnelConn.Handshake(); err != nil { proxyConn.Close() return nil, fmt.Errorf(\"fail to setup TLS handshake through the Tunnel: %w\", err) } klog.V(4).Infof(\"successfully setup TLS connection to %q with headers: %s\", addr, connectHeaders) return tlsTunnelConn, nil } klog.V(2).Infof(\"successfully setup connection to %q with headers: %q\", addr, connectHeaders) return proxyConn, nil } return \u0026RequestInterceptor{ contextDialer: contextDialer, } } å‚è€ƒï¼š\nhttps://openyurt.io/zh/docs/core-concepts/yurttunnel ","categories":"","description":"","excerpt":" æœ¬æ–‡ä»¥commit idï¼š180282663457080119a1bc6076cce20c922b5c50ï¼Œ å¯¹åº”ç‰ˆæœ¬tag: â€¦","ref":"/kubernetes-notes/edge/openyurt/code-analysis/tunnel-server-code-analysis/","tags":["OpenYurt"],"title":"OpenYurtä¹‹TunnelServeræºç åˆ†æ"},{"body":"å®‰è£…openyurtï¼Œä¸ºäº†é€‚é…è¾¹ç¼˜åœºæ™¯ï¼Œéœ€è¦å¯¹k8sç»„ä»¶è¿›è¡Œè°ƒæ•´ã€‚å…¶ä¸­åŒ…æ‹¬ï¼š\nkube-apiserver\nkube-controller-manager\nkube-proxy\nCoreDNS\n1. kube-apiserver ä¸ºäº†å®ç°äº‘è¾¹é€šä¿¡ï¼Œå³ç”¨æˆ·å¯ä»¥æ­£å¸¸ä½¿ç”¨kubectl exec/logsçš„åŠŸèƒ½æ¥ç™»å½•æˆ–æŸ¥çœ‹è¾¹ç¼˜å®¹å™¨çš„ä¿¡æ¯ã€‚éœ€è¦å°†kube-apiserverè®¿é—®kubeletçš„åœ°å€è°ƒæ•´ä¸ºhostnameä¼˜å…ˆã€‚\n$ vi /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 kind: Pod ... spec: dnsPolicy: \"None\" # 1. dnsPolicyä¿®æ”¹ä¸ºNone dnsConfig: # 2. å¢åŠ dnsConfigé…ç½® nameservers: - 1.2.3.4 # ä½¿ç”¨yurt-tunnel-dns serviceçš„clusterIPæ›¿æ¢ searches: - kube-system.svc.cluster.local - svc.cluster.local - cluster.local options: - name: ndots value: \"5\" containers: - command: - kube-apiserver ... - --kubelet-preferred-address-types=Hostname,InternalIP,ExternalIP # 3. æŠŠHostnameæ”¾åœ¨ç¬¬ä¸€ä½ ... 2. kube-controller-manager ç¦ç”¨é»˜è®¤çš„Â nodelifecycleÂ æ§åˆ¶å™¨ï¼Œå½“èŠ‚ç‚¹æ–­è¿æ—¶ä¸é©±é€podã€‚\nnodelifecycleæ§åˆ¶å™¨ä¸»è¦ç”¨æ¥æ ¹æ®nodeçš„statusåŠleaseçš„æ›´æ–°æ—¶é—´æ¥å†³å®šæ˜¯å¦è¦é©±é€èŠ‚ç‚¹ä¸Šçš„podã€‚ä¸ºäº†è®©Â yurt-controller-mamangerÂ èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼Œå› æ­¤éœ€è¦ç¦ç”¨controllerçš„é©±é€åŠŸèƒ½ã€‚\nvim /etc/kubernetes/manifests/kube-controller-manager.yaml # åœ¨--controllers=*,bootstrapsigner,tokencleaneråé¢æ·»åŠ ,-nodelifecycle # å³å‚æ•°ä¸ºï¼š --controllers=*,bootstrapsigner,tokencleaner,-nodelifecycle # å¦‚æœkube-controller-manageræ˜¯ä»¥static podéƒ¨ç½²ï¼Œä¿®æ”¹yamlæ–‡ä»¶åä¼šè‡ªåŠ¨é‡å¯ã€‚ 3. CoreDNS å°†corednsä»deploymentéƒ¨ç½²æ”¹ä¸ºdaemonsetéƒ¨ç½²ã€‚\nå°†deploymentçš„corednså‰¯æœ¬æ•°è°ƒæ•´ä¸º0ã€‚\nkubectl scale --replicas=0 deployment/coredns -n kube-system åˆ›å»ºdaemonsetçš„corednsã€‚\nwget https://raw.githubusercontent.com/huweihuang/kubeadm-scripts/main/openyurt/yurt-tunnel/coredns.ds.yaml kubectl apply -f æ”¯æŒæµé‡æ‹“æ‰‘ï¼š\n# åˆ©ç”¨openyurtå®ç°endpointè¿‡æ»¤ kubectl annotate svc kube-dns -n kube-system openyurt.io/topologyKeys='openyurt.io/nodepool' 4. kube-proxy äº‘è¾¹ç«¯åœºæ™¯ä¸‹ï¼Œè¾¹ç¼˜èŠ‚ç‚¹é—´å¾ˆæœ‰å¯èƒ½æ— æ³•äº’é€šï¼Œå› æ­¤éœ€è¦endpointsåŸºäºnodepoolè¿›è¡Œæ‹“æ‰‘ã€‚ç›´æ¥å°†kube-proxyçš„kubeconfigé…ç½®åˆ é™¤ï¼Œå°†apiserverè¯·æ±‚ç»è¿‡yurthubå³å¯è§£å†³æœåŠ¡æ‹“æ‰‘é—®é¢˜ã€‚\nkubectl edit cm -n kube-system kube-proxy ç¤ºä¾‹ï¼š\napiVersion: v1 data: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 bindAddressHardFail: false clientConnection: acceptContentTypes: \"\" burst: 0 contentType: \"\" #kubeconfig: /var/lib/kube-proxy/kubeconfig.conf \u003c-- åˆ é™¤è¿™ä¸ªé…ç½® qps: 0 clusterCIDR: 100.64.0.0/10 configSyncPeriod: 0s // çœç•¥ å‚è€ƒï¼š\nhttps://openyurt.io/zh/docs/installation/openyurt-prepare/ ","categories":"","description":"","excerpt":"å®‰è£…openyurtï¼Œä¸ºäº†é€‚é…è¾¹ç¼˜åœºæ™¯ï¼Œéœ€è¦å¯¹k8sç»„ä»¶è¿›è¡Œè°ƒæ•´ã€‚å…¶ä¸­åŒ…æ‹¬ï¼š\nkube-apiserver â€¦","ref":"/kubernetes-notes/edge/openyurt/update-k8s-for-openyurt/","tags":["OpenYurt"],"title":"OpenYurt å®‰è£…ç›¸å…³Kubernetesé…ç½®è°ƒæ•´"},{"body":"æœ¬æ–‡ä¸»è¦ä»‹ç»ä¸‰ç§æ–¹å¼æ¥åˆ›å»ºapisixçš„è·¯ç”±è§„åˆ™ã€‚éœ€è¦æå‰åˆ›å»ºå¥½k8s serviceä½œä¸ºè·¯ç”±çš„åç«¯æ ‡è¯†æ¥å…³è”endpointsã€‚\n0. åˆ›å»ºk8s service apiVersion: v1 kind: Service metadata: name: {APP}-service namespace: {NAMESPACE} spec: selector: k8s-app: {APP} ports: - name: http protocol: TCP port: 80 targetPort: 9376 è·¯ç”±è§„åˆ™ä¸»è¦åŒ…æ‹¬ï¼š\nhostsï¼šåŸŸå\npathsï¼šè®¿é—®è·¯å¾„\nbackends:\nserviceName\nservicePort\n1. ä½¿ç”¨ApisixRouteåˆ›å»ºè·¯ç”±è§„åˆ™ ä½¿ç”¨ApisixRouteè‡ªå®šä¹‰CRDåˆ›å»ºè·¯ç”±è§„åˆ™ï¼Œå…·ä½“å‚è€ƒï¼šreferenceã€‚\nç¤ºä¾‹ï¼š\n# httpbin-route.yaml apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: httpserver-route spec: http: - name: rule1 match: hosts: - local.httpbin.org paths: - /* backends: - serviceName: httpbin servicePort: 80 åœ¨k8sä¸­åˆ›å»ºApisixRouteã€‚\nkubectl apply -f httpbin-route.yaml 2. ä½¿ç”¨ingressåˆ›å»ºè·¯ç”±è§„åˆ™ ä½¿ç”¨k8s ingressæ¥åˆ›å»ºè·¯ç”±è§„åˆ™ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\n# httpbin-ingress.yaml # Note use apiVersion is networking.k8s.io/v1, so please make sure your # Kubernetes cluster version is v1.19.0 or higher. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: httpserver-ingress spec: # apisix-ingress-controller is only interested in Ingress # resources with the matched ingressClass name, in our case, # it's apisix. ingressClassName: apisix rules: - host: local.httpbin.org http: paths: - backend: service: name: httpbin port: number: 80 path: / pathType: Prefix åˆ›å»ºk8s ingressã€‚\nkubectl apply -f httpbin-ingress.yaml 3. ä½¿ç”¨Admin APIåˆ›å»ºè·¯ç”±è§„åˆ™ ç›´æ¥è°ƒç”¨Admin APIæˆ–è€…ä½¿ç”¨dashboardåˆ›å»ºè·¯ç”±è§„åˆ™ã€‚\n3.1. ä¸€é”®åˆ›å»ºè·¯ç”±å’Œupstream curl \"http://127.0.0.1:9180/apisix/admin/routes/1\" -H \"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\" -X PUT -d ' { \"methods\": [\"GET\"], \"host\": \"example.com\", \"uri\": \"/anything/*\", \"upstream\": { \"type\": \"roundrobin\", \"nodes\": { \"httpbin.org:80\": 1 } } }' 3.2. åˆ†å¼€åˆ›å»ºè·¯ç”±å’Œupstream æ¨èä½¿ç”¨åˆ†å¼€åˆ›å»ºè·¯ç”±å’Œupstreamã€‚\nåˆ›å»ºupstream\ncurl \"http://127.0.0.1:9180/apisix/admin/upstreams/1\" -H \"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\" -X PUT -d ' { \"type\": \"roundrobin\", \"nodes\": { \"httpbin.org:80\": 1 } }' åˆ›å»ºè·¯ç”±ç»‘å®šupstream\ncurl \"http://127.0.0.1:9180/apisix/admin/routes/1\" -H \"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\" -X PUT -d ' { \"uris\": [\"/get\",\"/list\"], \"host\": \"httpbin.org\", \"upstream_id\": \"1\" }' åˆ é™¤upstream\nDELETE /apisix/admin/upstreams/{id}\nåˆ é™¤route\nDELETE /apisix/admin/routes/{id}\n4. éªŒè¯è·¯ç”±è§„åˆ™ åŸºäºä¸Šè¿°çš„æ–¹å¼ï¼Œapisix-ingress-controllerä¼šè°ƒç”¨apisix adminçš„æ¥å£è‡ªåŠ¨åˆ›å»ºrouteså’Œupstreamsä¸¤ä¸ªä¿¡æ¯å­˜å…¥etcdï¼Œé€šè¿‡ä¸šåŠ¡åŸŸåè®¿é—®apisixå°±å¯ä»¥è®¿é—®åˆ°å…·ä½“çš„podã€‚\næœåŠ¡è°ƒç”¨\nå°†ä¸šåŠ¡åŸŸåè§£æåˆ°apisixçš„IPä¸Šï¼ˆå¦‚æœæ˜¯ç‰©ç†æœºéƒ¨ç½²å¯ä»¥æ˜¯VIPï¼Œæˆ–è€…k8séƒ¨ç½²çš„clusterIPï¼‰ã€‚\nè®¿é—®ä¸šåŠ¡åŸŸåï¼š\ncurl -v http://local.httpbin.org 5. æŸ¥çœ‹etcdä¸­çš„è·¯ç”±è§„åˆ™ etcdctl get /apisix --prefix 5.1. routes /apisix/routes/\né€šè¿‡ingressåˆ›å»ºçš„routes\n{ \"host\": \"local.httpbin.org\", \"create_time\": 1661251916, \"name\": \"ing_default_httpserver-ingress_37a4f3ae\", \"status\": 1, \"uris\": [ \"\\/\", \"\\/*\" ], \"upstream_id\": \"5ce57b8e\", \"labels\": { \"managed-by\": \"apisix-ingress-controller\" }, \"priority\": 0, \"desc\": \"Created by apisix-ingress-controller, DO NOT modify it manually\", \"update_time\": 1661397119, \"id\": \"148730bb\" } é€šè¿‡ApisixRouteåˆ›å»ºçš„routes\n{ \"priority\": 0, \"create_time\": 1661397584, \"name\": \"default_httpserver-route_rule1\", \"status\": 1, \"uris\": [ \"\\/*\" ], \"upstream_id\": \"5ce57b8e\", \"hosts\": [ \"local.httpbin.org\" ], \"labels\": { \"managed-by\": \"apisix-ingress-controller\" }, \"desc\": \"Created by apisix-ingress-controller, DO NOT modify it manually\", \"update_time\": 1661397584, \"id\": \"add8e28c\" } 5.2. upstreams /apisix/upstreams/5ce57b8e\nç›¸åŒçš„backendï¼Œä½¿ç”¨ingressæˆ–ApisixRouteåˆ›å»ºåç”Ÿæˆçš„upstreamsç›¸åŒã€‚\n{ \"scheme\": \"http\", \"pass_host\": \"pass\", \"name\": \"default_httpbin_80\", \"nodes\": [ { \"host\": \"10.244.3.167\", \"priority\": 0, \"port\": 80, \"weight\": 100 } ], \"type\": \"roundrobin\", \"labels\": { \"managed-by\": \"apisix-ingress-controller\" }, \"hash_on\": \"vars\", \"create_time\": 1661251916, \"id\": \"5ce57b8e\", \"update_time\": 1661397584, \"desc\": \"Created by apisix-ingress-controller, DO NOT modify it manually\" } å‚è€ƒï¼š\nhttps://apisix.apache.org/zh/docs/ingress-controller/getting-started/\nhttps://apisix.apache.org/zh/docs/ingress-controller/tutorials/index/\nhttps://apisix.apache.org/zh/docs/ingress-controller/tutorials/proxy-the-httpbin-service/\nhttps://apisix.apache.org/zh/docs/ingress-controller/tutorials/proxy-the-httpbin-service-with-ingress/\n","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦ä»‹ç»ä¸‰ç§æ–¹å¼æ¥åˆ›å»ºapisixçš„è·¯ç”±è§„åˆ™ã€‚éœ€è¦æå‰åˆ›å»ºå¥½k8s serviceä½œä¸ºè·¯ç”±çš„åç«¯æ ‡è¯†æ¥å…³è”endpointsã€‚\n0. åˆ› â€¦","ref":"/kubernetes-notes/network/gateway/apisix-ingress-controller-usage/","tags":["ApiSix"],"title":"åˆ›å»ºè·¯ç”±"},{"body":"1. å®‰è£…kubevirt 1.1. ä¿®æ”¹é•œåƒä»“åº“ é’ˆå¯¹ç§æœ‰ç¯å¢ƒï¼Œéœ€è¦å°†æ‰€éœ€é•œåƒä¸Šä¼ åˆ°è‡ªå·±çš„é•œåƒä»“åº“ä¸­ã€‚\næ¶‰åŠçš„é•œåƒç»„ä»¶æœ‰\nvirt-operator virt-api virt-controller virt-launcher é‡å‘½åé•œåƒè„šæœ¬å¦‚ä¸‹:\n#!/bin/bash # kubevirtç»„ä»¶ç‰ˆæœ¬ version=$1 # ç§æœ‰é•œåƒä»“åº“ registry=$2 # ç§æœ‰é•œåƒä»“åº“çš„namespace namespace=$3 kubevirtRegistry=\"quay.io/kubevirt\" readonly APPLIST=( virt-operator virt-api virt-controller virt-launcher ) for app in \"${APPLIST[@]}\"; do # æ‹‰å–é•œåƒ docker pull ${kubevirtRegistry}/${app}:${version} # é‡å‘½å docker tag ${kubevirtRegistry}/${app}:${version} ${registry}/${namespace}/${app}:${version} # æ¨é€é•œåƒ docker push ${registry}/${namespace}/${app}:${version} done echo \"é‡æ–°å‘½åæˆåŠŸ\" 1.2. éƒ¨ç½²virt-operator é€šè¿‡kubevirt operatorå®‰è£…kubevirtç›¸å…³ç»„ä»¶ï¼Œé€‰æ‹©æŒ‡å®šç‰ˆæœ¬ï¼Œä¸‹è½½kubevirt-operator.yamlå’Œkubevirt-cr.yamlæ–‡ä»¶ï¼Œå¹¶åˆ›å»ºk8sç›¸å…³å¯¹è±¡ã€‚\nå¦‚æœæ˜¯ç§æœ‰é•œåƒä»“åº“ï¼Œåˆ™éœ€è¦å°†kubevirt-operator.yamlæ–‡ä»¶ä¸­é•œåƒçš„åå­—æ›¿æ¢ä¸ºç§æœ‰é•œåƒä»“åº“çš„åœ°å€ï¼Œå¹¶æå‰æŒ‰æ­¥éª¤1æ¨é€æ‰€éœ€é•œåƒåˆ°ç§æœ‰é•œåƒä»“åº“ã€‚\n# Pick an upstream version of KubeVirt to install $ export RELEASE=v0.52.0 # Deploy the KubeVirt operator $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml # Create the KubeVirt CR (instance deployment request) which triggers the actual installation $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml # wait until all KubeVirt components are up $ kubectl -n kubevirt wait kv kubevirt --for condition=Available 1.3. éƒ¨ç½²virtctl virtctlç”¨æ¥å¯åŠ¨å’Œå…³é—­è™šæ‹Ÿæœºã€‚\nVERSION=$(kubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt -o=jsonpath=\"{.status.observedKubeVirtVersion}\") ARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed 's/x86_64/amd64/') || windows-amd64.exe echo ${ARCH} curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-${ARCH} chmod +x virtctl sudo install virtctl /usr/local/bin 2. kubevirtéƒ¨ç½²äº§ç‰© é€šè¿‡æ‰‹åŠ¨éƒ¨ç½²virt-operatorï¼Œä¼šè‡ªåŠ¨éƒ¨ç½²ä»¥ä¸‹ç»„ä»¶\nç»„ä»¶ éƒ¨ç½²æ–¹å¼ å‰¯æœ¬æ•° virt-api deployment 2 virt-controller deployment 2 virt-handler daemonset - å…·ä½“å‚è€ƒ:\n#kg all -n kubevirt NAME READY STATUS RESTARTS AGE pod/virt-api-5fb5cffb7f-hgjjh 1/1 Running 0 23h pod/virt-api-5fb5cffb7f-jcp7x 1/1 Running 0 23h pod/virt-controller-844cd4f58c-h8vsx 1/1 Running 0 23h pod/virt-controller-844cd4f58c-vlxqs 1/1 Running 0 23h pod/virt-handler-lb5ft 1/1 Running 0 23h pod/virt-handler-mtr4d 1/1 Running 0 22h pod/virt-handler-sxd2t 1/1 Running 0 23h pod/virt-operator-8595f577cd-b9txg 1/1 Running 0 23h pod/virt-operator-8595f577cd-p2f69 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubevirt-operator-webhook ClusterIP 10.254.159.81 \u003cnone\u003e 443/TCP 23h service/kubevirt-prometheus-metrics ClusterIP 10.254.7.231 \u003cnone\u003e 443/TCP 23h service/virt-api ClusterIP 10.254.244.139 \u003cnone\u003e 443/TCP 23h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/virt-handler 3 3 3 3 3 kubernetes.io/os=linux 23h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/virt-api 2/2 2 2 23h deployment.apps/virt-controller 2/2 2 2 23h deployment.apps/virt-operator 2/2 2 2 23h NAME DESIRED CURRENT READY AGE replicaset.apps/virt-api-5fb5cffb7f 2 2 2 23h replicaset.apps/virt-controller-844cd4f58c 2 2 2 23h replicaset.apps/virt-operator-8595f577cd 2 2 2 23h NAME AGE PHASE kubevirt.kubevirt.io/kubevirt 23h Deployed 3. åˆ›å»ºè™šæ‹Ÿæœº é€šè¿‡vm.yamlåˆ›å»ºè™šæ‹Ÿæœº\n# ä¸‹è½½vm.yaml wget https://kubevirt.io/labs/manifests/vm.yaml # åˆ›å»ºè™šæ‹Ÿæœº kubectl apply -f https://kubevirt.io/labs/manifests/vm.yaml vm.yamlæ–‡ä»¶\napiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: testvm spec: running: false template: metadata: labels: kubevirt.io/size: small kubevirt.io/domain: testvm spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default masquerade: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userDataBase64: SGkuXG4= æŸ¥çœ‹è™šæ‹Ÿæœº\nkubectl get vms kubectl get vms -o yaml testvm å¯åŠ¨æˆ–æš‚åœè™šæ‹Ÿæœº\n# å¯åŠ¨è™šæ‹Ÿæœº virtctl start testvm # å…³é—­è™šæ‹Ÿæœº virtctl stop testvm # è¿›å…¥è™šæ‹Ÿæœº virtctl console testvm åˆ é™¤è™šæ‹Ÿæœº\nkubectl delete vm testvm å‚è€ƒï¼š\nInstallation - KubeVirt User-Guide KubeVirt quickstart with Minikube | KubeVirt.io Use KubeVirt | KubeVirt.io ","categories":"","description":"","excerpt":"1. å®‰è£…kubevirt 1.1. ä¿®æ”¹é•œåƒä»“åº“ é’ˆå¯¹ç§æœ‰ç¯å¢ƒï¼Œéœ€è¦å°†æ‰€éœ€é•œåƒä¸Šä¼ åˆ°è‡ªå·±çš„é•œåƒä»“åº“ä¸­ã€‚ â€¦","ref":"/kubernetes-notes/kvm/kubevirt/kubevirt-installation/","tags":["KubeVirt"],"title":"KubeVirtçš„ä½¿ç”¨"},{"body":"1. æ–‡ä»¶ç³»ç»Ÿ æ–‡ä»¶ç³»ç»Ÿå°±æ˜¯åˆ†åŒºæˆ–ç£ç›˜ä¸Šçš„æ‰€æœ‰æ–‡ä»¶çš„é€»è¾‘é›†åˆã€‚æ–‡ä»¶ç³»ç»Ÿä¸ä»…åŒ…å«ç€æ–‡ä»¶ä¸­çš„æ•°æ®è€Œä¸”è¿˜æœ‰æ–‡ä»¶ç³»ç»Ÿçš„ç»“æ„ï¼Œæ‰€æœ‰Linux ç”¨æˆ·å’Œç¨‹åºçœ‹åˆ°çš„æ–‡ä»¶ã€ç›®å½•ã€è½¯è¿æ¥åŠæ–‡ä»¶ä¿æŠ¤ä¿¡æ¯ç­‰éƒ½å­˜å‚¨åœ¨å…¶ä¸­ã€‚\nä¸åŒLinuxå‘è¡Œç‰ˆæœ¬ä¹‹é—´çš„æ–‡ä»¶ç³»ç»Ÿå·®åˆ«å¾ˆå°‘ï¼Œä¸»è¦è¡¨ç°åœ¨ç³»ç»Ÿç®¡ç†çš„ç‰¹è‰²å·¥å…·ä»¥åŠè½¯ä»¶åŒ…ç®¡ç†æ–¹å¼çš„ä¸åŒï¼Œæ–‡ä»¶ç›®å½•ç»“æ„åŸºæœ¬ä¸Šéƒ½æ˜¯ä¸€æ ·çš„ã€‚\next2 ï¼š æ—©æœŸlinuxä¸­å¸¸ç”¨çš„æ–‡ä»¶ç³»ç»Ÿï¼› ext3 ï¼š ext2çš„å‡çº§ç‰ˆï¼Œå¸¦æ—¥å¿—åŠŸèƒ½ï¼› RAMFS ï¼š å†…å­˜æ–‡ä»¶ç³»ç»Ÿï¼Œé€Ÿåº¦å¾ˆå¿«ï¼› iso9660ï¼šå…‰ç›˜æˆ–å…‰ç›˜é•œåƒï¼› NFS ï¼š ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿï¼Œç”±SUNå‘æ˜ï¼Œä¸»è¦ç”¨äºè¿œç¨‹æ–‡ä»¶å…±äº«ï¼› MS-DOS ï¼š MS-DOSæ–‡ä»¶ç³»ç»Ÿï¼› FAT ï¼š Windows XP æ“ä½œç³»ç»Ÿé‡‡ç”¨çš„æ–‡ä»¶ç³»ç»Ÿï¼› NTFS ï¼š Windows NT/XP æ“ä½œç³»ç»Ÿé‡‡ç”¨çš„æ–‡ä»¶ç³»ç»Ÿã€‚ 2. åˆ†åŒºä¸ç›®å½• æ–‡ä»¶ç³»ç»Ÿä½äºç£ç›˜åˆ†åŒºä¸­ï¼›ä¸€ä¸ªç¡¬ç›˜å¯ä»¥æœ‰å¤šä¸ªåˆ†åŒºï¼Œä¹Ÿå¯ä»¥åªæœ‰ä¸€ä¸ªåˆ†åŒºï¼›ä¸€ä¸ªåˆ†åŒºåªèƒ½åŒ…å«ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿã€‚\nLinuxæ–‡ä»¶ç³»ç»Ÿä¸Windowsæœ‰è¾ƒå¤§çš„å·®åˆ«ï¼š\nWindowsçš„æ–‡ä»¶ç»“æ„æ˜¯å¤šä¸ªå¹¶åˆ—çš„æ ‘çŠ¶ç»“æ„ï¼Œæœ€é¡¶éƒ¨çš„æ˜¯ä¸åŒçš„ç£ç›˜ï¼ˆåˆ†åŒºï¼‰ï¼Œå¦‚ Cã€Dã€Eã€Fç­‰ã€‚\nLinuxçš„æ–‡ä»¶ç»“æ„æ˜¯å•ä¸ªçš„æ ‘çŠ¶ç»“æ„ï¼Œæ ¹ç›®å½•æ˜¯â€œ/â€ï¼Œå…¶ä»–ç›®å½•éƒ½è¦ä½äºæ ¹ç›®å½•ä¸‹ã€‚\næ¯æ¬¡å®‰è£…ç³»ç»Ÿçš„æ—¶å€™æˆ‘ä»¬éƒ½ä¼šè¿›è¡Œåˆ†åŒºï¼ŒLinuxä¸‹ç£ç›˜åˆ†åŒºå’Œç›®å½•çš„å…³ç³»å¦‚ä¸‹ï¼š\nä»»ä½•ä¸€ä¸ªåˆ†åŒºéƒ½å¿…é¡»å¯¹åº”åˆ°æŸä¸ªç›®å½•ä¸Šï¼Œæ‰èƒ½è¿›è¡Œè¯»å†™æ“ä½œï¼Œç§°ä¸ºâ€œæŒ‚è½½â€ã€‚ è¢«æŒ‚è½½çš„ç›®å½•å¯ä»¥æ˜¯æ ¹ç›®å½•ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–äºŒçº§ã€ä¸‰çº§ç›®å½•ï¼Œä»»ä½•ç›®å½•éƒ½å¯ä»¥æ˜¯æŒ‚è½½ç‚¹ã€‚ ç›®å½•æ˜¯é€»è¾‘ä¸Šçš„åŒºåˆ†ã€‚åˆ†åŒºæ˜¯ç‰©ç†ä¸Šçš„åŒºåˆ†ã€‚ æ ¹ç›®å½•æ˜¯æ‰€æœ‰Linuxçš„æ–‡ä»¶å’Œç›®å½•æ‰€åœ¨çš„åœ°æ–¹ï¼Œéœ€è¦æŒ‚è½½ä¸Šä¸€ä¸ªç£ç›˜åˆ†åŒºã€‚ ä¸‹å›¾æ˜¯å¸¸è§çš„ç›®å½•å’Œåˆ†åŒºçš„å¯¹åº”å…³ç³»ï¼š\næ›´è¯¦ç»†çš„ç›®å½•è·¯å¾„åŠŸèƒ½å¦‚ä¸‹ï¼š\nä¸ºä»€ä¹ˆè¦åˆ†åŒºï¼Œå¦‚ä½•åˆ†åŒºï¼Ÿ\nå¯ä»¥æŠŠä¸åŒèµ„æ–™ï¼Œåˆ†åˆ«æ”¾å…¥ä¸åŒåˆ†åŒºä¸­ç®¡ç†ï¼Œé™ä½é£é™©ã€‚ å¤§ç¡¬ç›˜æœç´¢èŒƒå›´å¤§ï¼Œæ•ˆç‡ä½ã€‚ /homeã€/varã€/usr/local ç»å¸¸æ˜¯å•ç‹¬åˆ†åŒºï¼Œå› ä¸ºç»å¸¸ä¼šæ“ä½œï¼Œå®¹æ˜“äº§ç”Ÿç¢ç‰‡ã€‚ ä¸ºäº†ä¾¿äºå®šä½å’ŒæŸ¥æ‰¾ï¼ŒLinuxä¸­çš„æ¯ä¸ªç›®å½•ä¸€èˆ¬éƒ½å­˜æ”¾ç‰¹å®šç±»å‹çš„æ–‡ä»¶ï¼Œä¸‹è¡¨åˆ—å‡ºäº†å„ç§Linuxå‘è¡Œç‰ˆæœ¬çš„å¸¸è§ç›®å½•ï¼š\nç›®å½• è¯´æ˜ / æ ¹ç›®å½•ï¼Œåªèƒ½åŒ…å«ç›®å½•ï¼Œä¸èƒ½åŒ…å«å…·ä½“æ–‡ä»¶ã€‚ /bin å­˜æ”¾å¯æ‰§è¡Œæ–‡ä»¶ã€‚å¾ˆå¤šå‘½ä»¤å°±å¯¹åº”/binç›®å½•ä¸‹çš„æŸä¸ªç¨‹åºï¼Œä¾‹å¦‚ lsã€cpã€mkdirã€‚/binç›®å½•å¯¹æ‰€æœ‰ç”¨æˆ·æœ‰æ•ˆã€‚ /dev ç¡¬ä»¶é©±åŠ¨ç¨‹åºã€‚ä¾‹å¦‚å£°å¡ã€ç£ç›˜é©±åŠ¨ç­‰ï¼Œè¿˜æœ‰å¦‚ /dev/nullã€/dev/consoleã€/dev/zeroã€/dev/full ç­‰æ–‡ä»¶ã€‚ /etc ä¸»è¦åŒ…å«ç³»ç»Ÿé…ç½®æ–‡ä»¶å’Œç”¨æˆ·ã€ç”¨æˆ·ç»„é…ç½®æ–‡ä»¶ã€‚ /lib ä¸»è¦åŒ…å«å…±äº«åº“æ–‡ä»¶ï¼Œç±»ä¼¼äºWindowsä¸‹çš„DLLï¼›æœ‰æ—¶ä¹Ÿä¼šåŒ…å«å†…æ ¸ç›¸å…³æ–‡ä»¶ã€‚ /boot ç³»ç»Ÿå¯åŠ¨æ–‡ä»¶ï¼Œä¾‹å¦‚Linuxå†…æ ¸ã€å¼•å¯¼ç¨‹åºç­‰ã€‚ /home ç”¨æˆ·å·¥ä½œç›®å½•ï¼ˆä¸»ç›®å½•ï¼‰ï¼Œæ¯ä¸ªç”¨æˆ·éƒ½ä¼šåˆ†é…ä¸€ä¸ªç›®å½•ã€‚ /mnt ä¸´æ—¶æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿã€‚è¿™ä¸ªç›®å½•ä¸€èˆ¬æ˜¯ç”¨äºå­˜æ”¾æŒ‚è½½å‚¨å­˜è®¾å¤‡çš„æŒ‚è½½ç›®å½•çš„ï¼Œä¾‹å¦‚æŒ‚è½½CD-ROMçš„cdromç›®å½•ã€‚ /proc æ“ä½œç³»ç»Ÿè¿è¡Œæ—¶ï¼Œè¿›ç¨‹ï¼ˆæ­£åœ¨è¿è¡Œä¸­çš„ç¨‹åºï¼‰ä¿¡æ¯åŠå†…æ ¸ä¿¡æ¯ï¼ˆæ¯”å¦‚cpuã€ç¡¬ç›˜åˆ†åŒºã€å†…å­˜ä¿¡æ¯ç­‰ï¼‰å­˜æ”¾åœ¨è¿™é‡Œã€‚/procç›®å½•ä¼ªè£…çš„æ–‡ä»¶ç³»ç»Ÿprocçš„æŒ‚è½½ç›®å½•ï¼Œprocå¹¶ä¸æ˜¯çœŸæ­£çš„æ–‡ä»¶ç³»ç»Ÿã€‚ /tmp ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼Œç³»ç»Ÿé‡å¯åä¸ä¼šè¢«ä¿å­˜ã€‚ /usr /userç›®ä¸‹çš„æ–‡ä»¶æ¯”è¾ƒæ··æ‚ï¼ŒåŒ…å«äº†ç®¡ç†å‘½ä»¤ã€å…±äº«æ–‡ä»¶ã€åº“æ–‡ä»¶ç­‰ï¼Œå¯ä»¥è¢«å¾ˆå¤šç”¨æˆ·ä½¿ç”¨ã€‚ /var ä¸»è¦åŒ…å«ä¸€äº›å¯å˜é•¿åº¦çš„æ–‡ä»¶ï¼Œä¼šç»å¸¸å¯¹æ•°æ®è¿›è¡Œè¯»å†™ï¼Œä¾‹å¦‚æ—¥å¿—æ–‡ä»¶å’Œæ‰“å°é˜Ÿåˆ—é‡Œçš„æ–‡ä»¶ã€‚ /sbin å’Œ /bin ç±»ä¼¼ï¼Œä¸»è¦åŒ…å«å¯æ‰§è¡Œæ–‡ä»¶ï¼Œä¸è¿‡ä¸€èˆ¬æ˜¯ç³»ç»Ÿç®¡ç†æ‰€éœ€è¦çš„ï¼Œä¸æ˜¯æ‰€æœ‰ç”¨æˆ·éƒ½éœ€è¦ã€‚ 3. å¸¸ç”¨æ–‡ä»¶ç®¡ç†å‘½ä»¤ ä½ å¯ä»¥é€šè¿‡ä¸‹é¢çš„å‘½ä»¤æ¥ç®¡ç†æ–‡ä»¶ï¼š\nCommand Description cat filename æŸ¥çœ‹æ–‡ä»¶å†…å®¹ã€‚ cd dirname æ”¹å˜æ‰€åœ¨ç›®å½•ã€‚ cp file1 file2 å¤åˆ¶æ–‡ä»¶æˆ–ç›®å½•ã€‚ file filename æŸ¥çœ‹æ–‡ä»¶ç±»å‹(binary, text, etc)ã€‚ find filename dir æœç´¢æ–‡ä»¶æˆ–ç›®å½•ã€‚ head filename æ˜¾ç¤ºæ–‡ä»¶çš„å¼€å¤´ï¼Œä¸tailå‘½ä»¤ç›¸å¯¹ã€‚ less filename æŸ¥çœ‹æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼Œå¯ä»¥åˆ†é¡µæ˜¾ç¤ºï¼Œæ¯”moreå‘½ä»¤è¦å¼ºå¤§ã€‚ ls dirname éå†ç›®å½•ä¸‹çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚ mkdir dirname åˆ›å»ºç›®å½•ã€‚ more filename æŸ¥çœ‹æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼Œå¯ä»¥åˆ†é¡µæ˜¾ç¤ºã€‚ mv file1 file2 ç§»åŠ¨æ–‡ä»¶æˆ–é‡å‘½åã€‚ pwd æ˜¾ç¤ºç”¨æˆ·å½“å‰æ‰€åœ¨ç›®å½•ã€‚ rm filename åˆ é™¤æ–‡ä»¶ã€‚ rmdir dirname åˆ é™¤ç›®å½•ã€‚ tail filename æ˜¾ç¤ºæ–‡ä»¶çš„ç»“å°¾ï¼Œä¸headå‘½ä»¤ç›¸å¯¹ã€‚ touch filename æ–‡ä»¶ä¸å­˜åœ¨æ—¶åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ï¼Œå­˜åœ¨æ—¶ä¿®æ”¹æ–‡ä»¶æ—¶é—´æˆ³ã€‚ whereis filename æŸ¥çœ‹æ–‡ä»¶æ‰€åœ¨ä½ç½®ã€‚ which filename å¦‚æœæ–‡ä»¶åœ¨ç¯å¢ƒå˜é‡PATHä¸­æœ‰å®šä¹‰ï¼Œé‚£ä¹ˆæ˜¾ç¤ºæ–‡ä»¶ä½ç½®ã€‚ 3.1. dfå‘½ä»¤ ç®¡ç†ç£ç›˜åˆ†åŒºæ—¶ç»å¸¸ä¼šä½¿ç”¨ df (disk free) å‘½ä»¤ï¼Œdf -k å‘½ä»¤å¯ä»¥ç”¨æ¥æŸ¥çœ‹ç£ç›˜ç©ºé—´çš„ä½¿ç”¨æƒ…å†µï¼ˆä»¥åƒå­—èŠ‚è®¡ï¼‰ï¼Œä¾‹å¦‚ï¼š\n$df -k Filesystem 1K-blocks Used Available Use% Mounted on /dev/vzfs 10485760 7836644 2649116 75% / /devices 0 0 0 0% /devices æ¯ä¸€åˆ—çš„å«ä¹‰å¦‚ä¸‹ï¼š\nåˆ— è¯´æ˜ Filesystem ä»£è¡¨æ–‡ä»¶ç³»ç»Ÿå¯¹åº”çš„è®¾å¤‡æ–‡ä»¶çš„è·¯å¾„åï¼ˆä¸€èˆ¬æ˜¯ç¡¬ç›˜ä¸Šçš„åˆ†åŒºï¼‰ã€‚ kbytes åˆ†åŒºåŒ…å«çš„æ•°æ®å—ï¼ˆ1024å­—èŠ‚ï¼‰çš„æ•°ç›®ã€‚ used å·²ç”¨ç©ºé—´ã€‚ avail å¯ç”¨ç©ºé—´ã€‚ capacity å·²ç”¨ç©ºé—´çš„ç™¾åˆ†æ¯”ã€‚ Mounted on æ–‡ä»¶ç³»ç»ŸæŒ‚è½½ç‚¹ã€‚ æŸäº›ç›®å½•ï¼ˆä¾‹å¦‚ /devicesï¼‰çš„ kbytesã€usedã€avail åˆ—ä¸º0ï¼Œuseåˆ—ä¸º0%ï¼Œè¿™äº›éƒ½æ˜¯ç‰¹æ®Šï¼ˆæˆ–è™šæ‹Ÿï¼‰æ–‡ä»¶ç³»ç»Ÿï¼Œå³ä½¿ä½äºæ ¹ç›®å½•ä¸‹ï¼Œä¹Ÿä¸å ç”¨ç¡¬ç›˜ç©ºé—´ã€‚\nä½ å¯ä»¥ç»“åˆ -h (human readable) é€‰é¡¹å°†è¾“å‡ºä¿¡æ¯æ ¼å¼åŒ–ï¼Œè®©äººæ›´æ˜“é˜…è¯»ã€‚\n3.2. du å‘½ä»¤ du (disk usage) å‘½ä»¤å¯ä»¥ç”¨æ¥æŸ¥çœ‹ç‰¹å®šç›®å½•çš„ç©ºé—´ä½¿ç”¨æƒ…å†µã€‚\ndu å‘½ä»¤ä¼šæ˜¾ç¤ºæ¯ä¸ªç›®å½•æ‰€å ç”¨æ•°æ®å—ã€‚æ ¹æ®ç³»ç»Ÿçš„ä¸åŒï¼Œä¸€ä¸ªæ•°æ®å—å¯èƒ½æ˜¯ 512 å­—èŠ‚æˆ– 1024 å­—èŠ‚ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š\n$du /etc 10 /etc/cron.d 126 /etc/default 6 /etc/dfs ... ç»“åˆ -h é€‰é¡¹å¯ä»¥è®©ä¿¡æ¯æ˜¾ç¤ºçš„æ›´åŠ æ¸…æ™°ï¼š\n$du -h /etc 5k /etc/cron.d 63k /etc/default 3k /etc/dfs ... 4. æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿ æŒ‚è½½æ˜¯æŒ‡å°†ä¸€ä¸ªç¡¬ä»¶è®¾å¤‡ï¼ˆä¾‹å¦‚ç¡¬ç›˜ã€Uç›˜ã€å…‰ç›˜ç­‰ï¼‰å¯¹åº”åˆ°ä¸€ä¸ªå·²å­˜åœ¨çš„ç›®å½•ä¸Šã€‚ è‹¥è¦è®¿é—®è®¾å¤‡ä¸­çš„æ–‡ä»¶ï¼Œå¿…é¡»å°†æ–‡ä»¶æŒ‚è½½åˆ°ä¸€ä¸ªå·²å­˜åœ¨çš„ç›®å½•ä¸Šï¼Œ ç„¶åé€šè¿‡è®¿é—®è¿™ä¸ªç›®å½•æ¥è®¿é—®å­˜å‚¨è®¾å¤‡ã€‚\nè¿™æ ·å°±ä¸ºç”¨æˆ·æä¾›äº†ç»Ÿä¸€çš„æ¥å£ï¼Œå±è”½äº†ç¡¬ä»¶è®¾å¤‡çš„ç»†èŠ‚ã€‚Linuxå°†æ‰€æœ‰çš„ç¡¬ä»¶è®¾å¤‡çœ‹åšæ–‡ä»¶ï¼Œå¯¹ç¡¬ä»¶è®¾å¤‡çš„æ“ä½œç­‰åŒäºå¯¹æ–‡ä»¶çš„æ“ä½œã€‚\næ³¨æ„ï¼šæŒ‚è½½ç›®å½•å¯ä»¥ä¸ä¸ºç©ºï¼Œä½†æŒ‚è½½åè¿™ä¸ªç›®å½•ä¸‹ä»¥å‰çš„å†…å®¹å°†ä¸å¯ç”¨ã€‚\néœ€è¦çŸ¥é“çš„æ˜¯ï¼Œå…‰ç›˜ã€è½¯ç›˜ã€å…¶ä»–æ“ä½œç³»ç»Ÿä½¿ç”¨çš„æ–‡ä»¶ç³»ç»Ÿçš„æ ¼å¼ä¸linuxä½¿ç”¨çš„æ–‡ä»¶ç³»ç»Ÿæ ¼å¼æ˜¯ä¸ä¸€æ ·çš„ï¼ŒæŒ‚è½½éœ€è¦ç¡®è®¤Linuxæ˜¯å¦æ”¯æŒæ‰€è¦æŒ‚è½½çš„æ–‡ä»¶ç³»ç»Ÿæ ¼å¼ã€‚\næŸ¥çœ‹å½“å‰ç³»ç»Ÿæ‰€æŒ‚è½½çš„ç¡¬ä»¶è®¾å¤‡å¯ä»¥ä½¿ç”¨ mount å‘½ä»¤ï¼š\n$ mount /dev/vzfs on / type reiserfs (rw,usrquota,grpquota) proc on /proc type proc (rw,nodiratime) devpts on /dev/pts type devpts (rw) ä¸€èˆ¬çº¦å®šï¼Œ/mnt ä¸ºä¸´æ—¶æŒ‚è½½ç›®å½•ï¼Œä¾‹å¦‚æŒ‚è½½CD-ROMã€è¿œç¨‹ç½‘ç»œè®¾å¤‡ã€è½¯ç›˜ç­‰ã€‚ ä¹Ÿå¯ä»¥é€šè¿‡mountå‘½ä»¤æ¥æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿï¼Œè¯­æ³•ä¸ºï¼š\nmount -t file_system_type device_to_mount directory_to_mount_to ä¾‹å¦‚ï¼š\nå°† CD-ROM æŒ‚è½½åˆ° /mnt/cdrom ç›®å½•ã€‚\n$ mount -t iso9660 /dev/cdrom /mnt/cdrom æ³¨æ„ï¼šfile_system_typeç”¨æ¥æŒ‡å®šæ–‡ä»¶ç³»ç»Ÿç±»å‹ï¼Œé€šå¸¸å¯ä»¥ä¸æŒ‡å®šï¼ŒLinuxä¼šè‡ªåŠ¨æ­£ç¡®é€‰æ‹©æ–‡ä»¶ç³»ç»Ÿç±»å‹ã€‚\næŒ‚è½½æ–‡ä»¶ç³»ç»Ÿåï¼Œå°±å¯ä»¥é€šè¿‡ cdã€cat ç­‰å‘½ä»¤æ¥æ“ä½œå¯¹åº”æ–‡ä»¶ã€‚\nå¯ä»¥é€šè¿‡ umount å‘½ä»¤æ¥å¸è½½æ–‡ä»¶ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œå¸è½½ cdromï¼š\n$ umount /dev/cdrom ä¸è¿‡ï¼Œå¤§éƒ¨åˆ†ç°ä»£çš„Linuxç³»ç»Ÿéƒ½æœ‰è‡ªåŠ¨æŒ‚è½½å¸è½½åŠŸèƒ½ï¼Œunmount å‘½ä»¤è¾ƒå°‘ç”¨åˆ°ã€‚\n5. ç”¨æˆ·å’Œç¾¤ç»„é…é¢ ç”¨æˆ·å’Œç¾¤ç»„é…é¢å¯ä»¥è®©ç®¡ç†å‘˜ä¸ºæ¯ä¸ªç”¨æˆ·æˆ–ç¾¤ç»„åˆ†é…å›ºå®šçš„ç£ç›˜ç©ºé—´ã€‚ ç®¡ç†å‘˜æœ‰ä¸¤ç§æ–¹å¼æ¥åˆ†é…ç£ç›˜ç©ºé—´ï¼š\nè½¯é™åˆ¶ï¼šå¦‚æœç”¨æˆ·è¶…è¿‡æŒ‡å®šçš„ç©ºé—´ï¼Œä¼šæœ‰ä¸€ä¸ªå®½é™æœŸï¼Œç­‰å¾…ç”¨æˆ·é‡Šæ”¾ç©ºé—´ã€‚ ç¡¬é™åˆ¶ï¼šæ²¡æœ‰å®½é™æœŸï¼Œè¶…å‡ºæŒ‡å®šç©ºé—´ç«‹å³ç¦æ­¢æ“ä½œã€‚ ä¸‹é¢çš„å‘½ä»¤å¯ä»¥ç”¨æ¥ç®¡ç†é…é¢ï¼š\nå‘½ä»¤ è¯´æ˜ quota æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µä»¥åŠæ¯ä¸ªç”¨æˆ·ç»„çš„é…é¢ã€‚ edquota ç¼–è¾‘ç”¨æˆ·å’Œç¾¤ç»„çš„é…é¢ã€‚ quotacheck æŸ¥çœ‹æ–‡ä»¶ç³»ç»Ÿçš„ç£ç›˜ä½¿ç”¨æƒ…å†µï¼Œåˆ›å»ºã€æ£€æŸ¥å¹¶ä¿®å¤é…é¢æ–‡ä»¶ã€‚ setquota è®¾ç½®é…é¢ã€‚ quotaon å¼€å¯ç”¨æˆ·æˆ–ç¾¤ç»„çš„é…é¢åŠŸèƒ½ã€‚ quotaoff å…³é—­ç”¨æˆ·æˆ–ç¾¤ç»„çš„é…é¢åŠŸèƒ½ã€‚ repquota æ‰“å°æŒ‡å®šæ–‡ä»¶ç³»ç»Ÿçš„é…é¢ã€‚ å‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/linux/ ","categories":"","description":"","excerpt":"1. æ–‡ä»¶ç³»ç»Ÿ æ–‡ä»¶ç³»ç»Ÿå°±æ˜¯åˆ†åŒºæˆ–ç£ç›˜ä¸Šçš„æ‰€æœ‰æ–‡ä»¶çš„é€»è¾‘é›†åˆã€‚æ–‡ä»¶ç³»ç»Ÿä¸ä»…åŒ…å«ç€æ–‡ä»¶ä¸­çš„æ•°æ®è€Œä¸”è¿˜æœ‰æ–‡ä»¶ç³»ç»Ÿçš„ç»“æ„ï¼Œæ‰€æœ‰Linux ç”¨æˆ·å’Œç¨‹ â€¦","ref":"/linux-notes/file/linux-file-system/","tags":["Linux"],"title":"Linuxæ–‡ä»¶ç³»ç»Ÿ"},{"body":" æœ¬æ–‡ç”±ç½‘ç»œå†…å®¹æ•´ç†è€Œæˆçš„ç¬”è®°\n1. LVMç®€ä»‹ LVMæ˜¯é€»è¾‘ç›˜å·ç®¡ç†ï¼ˆLogical Volume Managerï¼‰çš„ç®€ç§°ï¼Œå®ƒæ˜¯Linuxç¯å¢ƒä¸‹å¯¹ç£ç›˜åˆ†åŒºè¿›è¡Œç®¡ç†çš„ä¸€ç§æœºåˆ¶ï¼ŒLVMæ˜¯å»ºç«‹åœ¨ç¡¬ç›˜å’Œåˆ†åŒºä¹‹ä¸Šçš„ä¸€ä¸ªé€»è¾‘å±‚ï¼Œæ¥æé«˜ç£ç›˜åˆ†åŒºç®¡ç†çš„çµæ´»æ€§ã€‚\nä¼˜ç‚¹ï¼š\nå¯ä»¥çµæ´»åˆ†é…å’Œç®¡ç†ç£ç›˜ç©ºé—´\nå¯ä»¥å¯¹åˆ†åŒºè¿›è¡ŒåŠ¨æ€çš„æ‰©å®¹\nå¯ä»¥å¢åŠ æ–°çš„ç£ç›˜åˆ°lvmä¸­\n2. LVMæ ¸å¿ƒæ¦‚å¿µ LVMæ¦‚å¿µå›¾ï¼š\nPVï¼ˆPhysical Volumeï¼‰ç‰©ç†å· ç£ç›˜åˆ†åŒºåï¼ˆè¿˜æœªæ ¼å¼åŒ–ä¸ºæ–‡ä»¶ç³»ç»Ÿï¼‰ä½¿ç”¨ pvcreate å‘½ä»¤å¯ä»¥å°†ç¡¬ç›˜åˆ†åŒºåˆ›å»ºä¸º pvï¼Œæ­¤åˆ†åŒºçš„ systemID ä¸º8eï¼Œå³ä¸º LVM æ ¼å¼çš„ç³»ç»Ÿæ ‡è¯†ç¬¦ã€‚ VGï¼ˆVolume Groupï¼‰å·ç»„ å°†å¤šä¸ª PV ç»„åˆèµ·æ¥ï¼Œä½¿ç”¨ vgcreate å‘½ä»¤åˆ›å»ºæˆå·ç»„ã€‚å·ç»„åŒ…å«äº†å¤šä¸ª PVï¼Œç›¸å½“äºé‡æ–°æ•´åˆäº†å¤šä¸ªåˆ†åŒºåå¾—åˆ°çš„ç¡¬ç›˜ã€‚è™½ç„¶ VG æ•´åˆäº†å¤šä¸ª PVï¼Œä½†æ˜¯åˆ›å»º VG æ—¶ä¼šå°†æ‰€æœ‰ç©ºé—´æ ¹æ®æŒ‡å®š PE å¤§å°åˆ’åˆ†ä¸ºå¤šä¸ª PEï¼Œåœ¨ LVM æ¨¡å¼ä¸‹çš„å­˜å‚¨éƒ½æ˜¯ä»¥ PE ä¸ºå•å…ƒï¼Œç±»ä¼¼äºæ–‡ä»¶ç³»ç»Ÿçš„ Blockã€‚ PEï¼ˆPhysical Extendï¼‰ç‰©ç†å­˜å‚¨å•å…ƒ PE æ˜¯ VG ä¸­çš„å­˜å‚¨å•å…ƒã€‚å®é™…å­˜å‚¨çš„æ•°æ®éƒ½æ˜¯åœ¨ PE å­˜å‚¨ã€‚ LVï¼ˆLogical Volumeï¼‰é€»è¾‘å· å¦‚æœè¯´VGæ˜¯æ•´åˆåˆ†åŒºä¸ºç¡¬ç›˜ï¼Œé‚£ä¹ˆ LV å°±æ˜¯æŠŠè¿™ä¸ªç¡¬ç›˜é‡æ–°çš„åˆ†åŒºï¼Œåªä¸è¿‡è¯¥åˆ†åŒºæ˜¯é€šè¿‡ VG æ¥åˆ’åˆ†çš„ã€‚VG ä¸­æœ‰å¾ˆå¤š PE å•å…ƒï¼Œå¯ä»¥æŒ‡å®šå°†å¤šå°‘ PE åˆ’åˆ†ç»™ä¸€ä¸ª LVï¼Œä¹Ÿå¯ä»¥ç›´æ¥æŒ‡å®šå¤§å°æ¥åˆ’åˆ†ã€‚åˆ’åˆ† LV åå°±ç›¸å½“äºåˆ’åˆ†äº†åˆ†åŒºï¼Œåªéœ€è¦å¯¹ LV è¿›è¡Œæ ¼å¼åŒ–å³å¯å˜æˆæ™®é€šçš„æ–‡ä»¶ç³»ç»Ÿã€‚ LEï¼ˆLogical extentï¼‰é€»è¾‘å­˜å‚¨å•å…ƒ LE åˆ™æ˜¯é€»è¾‘å­˜å‚¨å•å…ƒï¼Œå³ LV ä¸­çš„é€»è¾‘å­˜å‚¨å•å…ƒï¼Œå’Œ PE çš„å¤§å°ä¸€æ ·ã€‚ä» VG ä¸­åˆ’åˆ† LVï¼Œå®é™…ä¸Šæ˜¯ä» VG ä¸­åˆ’åˆ† VG ä¸­çš„ PEï¼Œåªä¸è¿‡åˆ’åˆ† LV åå®ƒä¸åœ¨ç§°ä¸º PEï¼Œè€Œæ˜¯ LEã€‚ 3. LVMåŸç† LVM ä¹‹æ‰€ä»¥èƒ½å¤Ÿä¼¸ç¼©å®¹é‡ï¼Œå®ç°çš„æ–¹æ³•å°±æ˜¯è®² LV é‡Œç©ºé—²çš„ PE ç§»å‡ºï¼Œæˆ–å‘ LV ä¸­æ·»åŠ ç©ºé—²çš„ PEã€‚\n4. æ ¼å¼åŒ–ä¸ºLVMç›˜ 4.1. fdiskæ ¼å¼åŒ–2Tä»¥ä¸‹ç£ç›˜ # ä½¿ç”¨fdiskè¿›è¡Œç›˜çš„æ ¼å¼åŒ– fdisk /dev/vdb # ä»¥ä¸‹æ˜¯äº¤äº’è¾“å‡ºç»“æœ Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0xadfbfcb4. Command (m for help): n # æ–°å»ºåˆ†åŒº Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p # å¾…å®šä¸»åˆ†åŒº Partition number (1-4, default 1): 1 # åºå· First sector (2048-1048575999, default 2048): # ç›´æ¥å›è½¦ Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-1048575999, default 1048575999): # ç›´æ¥å›è½¦ Using default value 1048575999 Partition 1 of type Linux and of size 500 GiB is set Command (m for help): p # ç¡®è®¤åˆ†åŒºæƒ…å†µ Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xadfbfcb4 Device Boot Start End Blocks Id System /dev/vdb1 2048 1048575999 524286976 83 Linux Command (m for help): t # é€‰æ‹©ç³»ç»Ÿid Selected partition 1 Hex code (type L to list all codes): 8e # 8eæŒ‡å®šçš„æ˜¯ä½¿ç”¨LVM Changed type of partition 'Linux' to 'Linux LVM' Command (m for help): w # ä¿å­˜ The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. 4.2. partedæ ¼å¼åŒ–2Tä»¥ä¸Šç£ç›˜ # parted /dev/sdk GNU Parted 3.1 ä½¿ç”¨ /dev/sdk Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) mktable æ–°çš„ç£ç›˜æ ‡ç­¾ç±»å‹ï¼Ÿ gpt (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name æ ‡å¿— (parted) mkpart åˆ†åŒºåç§°ï¼Ÿ []? æ–‡ä»¶ç³»ç»Ÿç±»å‹ï¼Ÿ [ext2]? èµ·å§‹ç‚¹ï¼Ÿ 0g ç»“æŸç‚¹ï¼Ÿ 4000G (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name æ ‡å¿— 1 1049kB 4000GB 4000GB (parted) toggle 1 lvm (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name æ ‡å¿— 1 1049kB 4000GB 4000GB lvm (parted) quit ä¿¡æ¯: You may need to update /etc/fstab. 5. LVMæ“ä½œ # pvcreateå¦‚æœæç¤ºå‘½ä»¤ä¸å­˜åœ¨ï¼Œåˆ™éœ€è¦å®‰è£…lvm2 yum install lvm2 -y 5.1. åˆ›å»ºç‰©ç†å·ï¼ˆPVï¼‰ # pvcreate /dev/nvme1n1p1 /dev/nvme2n1p1 Physical volume \"/dev/nvme1n1p1\" successfully created. Physical volume \"/dev/nvme2n1p1\" successfully created. # ä½¿ç”¨pvsæˆ–è€… pvdisplay æŸ¥çœ‹ç»“æœ # pvs PV VG Fmt Attr PSize PFree /dev/nvme1n1p1 lvm2 --- 931.51g 931.51g /dev/nvme2n1p1 lvm2 --- 931.51g 931.51g 5.2. åˆ›å»ºå·ç»„ï¼ˆVGï¼‰ # vgcreate vgdata /dev/nvme1n1p1 /dev/nvme2n1p1 Volume group \"vgdata\" successfully created # ä½¿ç”¨vgs æŸ¥çœ‹vg, vgdisplayçš„ä¿¡æ¯ # lsblkæŸ¥çœ‹ # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 931.5G 0 disk /pcdn_data/storage1_ssd nvme2n1 259:2 0 931.5G 0 disk â””â”€nvme2n1p1 259:5 0 931.5G 0 part â””â”€vgdata-data 251:2 0 1.8T 0 lvm /vgdata nvme1n1 259:1 0 931.5G 0 disk â””â”€nvme1n1p1 259:4 0 931.5G 0 part â””â”€vgdata-data 251:2 0 1.8T 0 lvm /vgdata 5.3. åˆ›å»ºé€»è¾‘å·ï¼ˆLVï¼‰ # lvcreate -L åé¢æ˜¯å¤§å°ï¼Œ -n åé¢æ˜¯é€»è¾‘å·åç§°ï¼Œ vgdataå¯¹åº”ä¸Šé¢çš„å·ç»„ # lvcreate -L 1.8T -n data vgdata Rounding up size to full physical extent 1.80 TiB Logical volume \"data\" created. # ä½¿ç”¨lvdisplay æŸ¥çœ‹ç»“æœ 5.4. æ ¼å¼åŒ–æ–‡ä»¶ç³»ç»ŸåŠæŒ‚è½½ # æŸ¥çœ‹ç£ç›˜ä¿¡æ¯ # fdisk -l ç£ç›˜ /dev/mapper/vgdata-dataï¼š1979.1 GB, 1979124285440 å­—èŠ‚ï¼Œ3865477120 ä¸ªæ‰‡åŒº Units = æ‰‡åŒº of 1 * 512 = 512 bytes æ‰‡åŒºå¤§å°(é€»è¾‘/ç‰©ç†)ï¼š512 å­—èŠ‚ / 512 å­—èŠ‚ I/O å¤§å°(æœ€å°/æœ€ä½³)ï¼š512 å­—èŠ‚ / 512 å­—èŠ‚ # æ ¼å¼åŒ–æˆxfs, /dev/vgdata/dataä¸ºä¸Šé¢ LV Path mkfs.xfs /dev/vgdata/data # mount mkdir -p /data mount /dev/vgdata/data /data 5.5. LVMæ‰©å®¹ LVMæœ€å¤§çš„ä¼˜åŠ¿å°±æ˜¯å…¶å¯ä¼¸ç¼©æ€§ï¼Œä¼¸ç¼©æ€§æœ‰æ›´åŠ åé‡ä¸æ‰©å®¹ã€‚æ‰©å®¹çš„å®è´¨æ˜¯å°† VG ä¸­çš„ç©ºé—² PE æ·»åŠ åˆ° LV ä¸­ï¼Œæ‰€ä»¥åªè¦ VG ä¸­æœ‰ç©ºé—²çš„ PEï¼Œå°±å¯ä»¥è¿›è¡Œæ‰©å®¹ã€‚å³ä½¿æ²¡æœ‰ç©ºé—² PEï¼Œä¹Ÿå¯ä»¥æ·»åŠ PVï¼Œå°†PVåŠ å…¥åˆ°VGä¸­å¢åŠ ç©ºé—²PEã€‚\næ‰©å®¹çš„ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼š\nï¼ˆ1ï¼‰ä½¿ç”¨ lvextend æˆ– lvresize æ·»åŠ æ›´å¤šçš„ PE æˆ–å®¹é‡åˆ° LV\nï¼ˆ2ï¼‰ä½¿ç”¨ resize2fså‘½ä»¤ï¼ˆxfs ä½¿ç”¨ xfs_growfsï¼‰å°† LV å¢åŠ åçš„å®¹é‡æ·»åŠ åˆ°å¯¹åº”çš„æ–‡ä»¶ç³»ç»Ÿä¸­(æ­¤è¿‡ç¨‹æ˜¯ä¿®æ”¹æ–‡ä»¶ç³»ç»Ÿè€ŒéLVMå†…å®¹)\n6. LVMç›¸å…³å‘½ä»¤ 6.1. ç®¡ç† PV åŠŸèƒ½ å‘½ä»¤ åˆ›å»º PV pvcreate æ‰«æå¹¶åˆ—å‡ºæ‰€æœ‰ PV pvscan åˆ—å‡º PV å±æ€§ pvdisplay {name|size} ç§»é™¤ PV pvremove ç§»åŠ¨ PV ä¸­çš„æ•°æ® pvmove 6.2. ç®¡ç† VG åŠŸèƒ½ å‘½ä»¤ åˆ›å»º VG vgcreate æ‰«æå¹¶åˆ—å‡ºæ‰€æœ‰ VG vgscan åˆ—å‡º VG å±æ€§ä¿¡æ¯ vgdisplay ç§»é™¤ï¼ˆåˆ é™¤ï¼‰VG vgremove ä» VG ä¸­ç§»é™¤ PV vgreduce å°† PV æ·»åŠ åˆ° VG ä¸­ vgextend ä¿®æ”¹ VG å±æ€§ vgchange 6.3. ç®¡ç† LV åŠŸèƒ½ å‘½ä»¤ åˆ›å»º LV lvcreate æ‰«æå¹¶åˆ—å‡ºæ‰€æœ‰ LV lvscan åˆ—å‡º LV å±æ€§ä¿¡æ¯ lvdisplay ç§»é™¤ LV lvremove ç¼©å° LV å®¹é‡ lvreduce/lvresize å¢å¤§ LV å®¹é‡ lvextend/lvresize è°ƒæ•´ LV å®¹é‡ lvresize lvcreateå‘½ä»¤\nä¸€èˆ¬ç”¨æ³•ï¼šlvcreate [-L size(M/G) | -l PEnum] -n lv_name vg_name\né€‰é¡¹ï¼š\n-Lï¼šæ ¹æ®å¤§å°åˆ›å»º LVï¼Œå³åˆ†é…å¤šå°‘ç©ºé—´ç»™æ­¤ LV\n-lï¼šæ ¹æ® PE çš„æ•°é‡æ¥åˆ›å»º LVï¼Œå³åˆ†é…å¤šå°‘ä¸ª PE ç»™æ­¤ LV\n-nï¼šæŒ‡å®š LV åç§°\nå‚è€ƒï¼š\nLinuxä¸‹ä½¿ç”¨lvmå°†å¤šå—ç›˜åˆå¹¶ | Z.S.K.'s Records\n100ä¸ªLinuxå‘½ä»¤(5)-LVM - äº‘+ç¤¾åŒº - è…¾è®¯äº‘\nLVMæ•°æ®å· - å®¹å™¨æœåŠ¡ ACK - é˜¿é‡Œäº‘\n","categories":"","description":"","excerpt":" æœ¬æ–‡ç”±ç½‘ç»œå†…å®¹æ•´ç†è€Œæˆçš„ç¬”è®°\n1. LVMç®€ä»‹ LVMæ˜¯é€»è¾‘ç›˜å·ç®¡ç†ï¼ˆLogical Volume Managerï¼‰çš„ç®€ç§°ï¼Œå®ƒæ˜¯Linux â€¦","ref":"/linux-notes/disk/lvm-usage/","tags":["disk"],"title":"LVMçš„ä½¿ç”¨"},{"body":"ç±»å‹ 1. åŸºç¡€ç±»å‹ 1.1. å¸ƒå°”ç±»å‹ //å¸ƒå°”ç±»å‹çš„å…³é”®å­—ä¸ºbool,å€¼ä¸ºtrueæˆ–falseï¼Œä¸å¯å†™ä¸º0æˆ–1 var v1 bool v1=true //æ¥å—è¡¨è¾¾å¼åˆ¤æ–­èµ‹å€¼ï¼Œä¸æ”¯æŒè‡ªåŠ¨æˆ–å¼ºåˆ¶ç±»å‹è½¬æ¢ v2:=(1==2) 1.2. æ•´å‹ //1ã€ç±»å‹è¡¨ç¤º //intå’Œint32ä¸ºä¸åŒç±»å‹ï¼Œä¸ä¼šè‡ªåŠ¨ç±»å‹è½¬æ¢éœ€è¦å¼ºåˆ¶ç±»å‹è½¬æ¢ //å¼ºåˆ¶ç±»å‹è½¬æ¢éœ€æ³¨æ„ç²¾åº¦æŸå¤±ï¼ˆæµ®ç‚¹æ•°â†’æ•´æ•°ï¼‰ï¼Œå€¼æº¢å‡ºï¼ˆå¤§èŒƒå›´â†’å°èŒƒå›´ï¼‰ var v2 int32 v1:=64 v2=int32(v1) //2ã€æ•°å€¼è¿ç®—,æ”¯æŒâ€œ+,-,*,/å’Œ%â€ 5%3 //æ±‚ä½™ //3ã€æ¯”è¾ƒè¿ç®—,â€œ\u003c,\u003e,==,\u003e=,\u003c=,!=â€ //ä¸åŒç±»å‹ä¸èƒ½è¿›è¡Œæ¯”è¾ƒä¾‹å¦‚intå’Œint8ï¼Œä½†å¯ä»¥ä¸å­—é¢å¸¸é‡ï¼ˆliteralï¼‰è¿›è¡Œæ¯”è¾ƒ var i int32 var j int64 i,j=1,2 if i==j //ç¼–è¯‘é”™è¯¯ï¼Œä¸åŒç±»å‹ä¸èƒ½è¿›è¡Œæ¯”è¾ƒ if i==1 || j==2 //ç¼–è¯‘é€šè¿‡ï¼Œå¯ä»¥ä¸å­—é¢å¸¸é‡ï¼ˆliteralï¼‰è¿›è¡Œæ¯”è¾ƒ //4ã€ä½è¿ç®— //Go(^x)å–åä¸Cè¯­è¨€(~x)ä¸åŒï¼Œå…¶ä»–ç±»ä¼¼ï¼Œå…·ä½“è§ä¸‹è¡¨ 1.3. æµ®ç‚¹å‹ //1ã€æµ®ç‚¹å‹åˆ†ä¸ºfloat32(ç±»ä¼¼Cä¸­çš„float)ï¼Œfloat64(ç±»ä¼¼Cä¸­çš„double) var f1 float32 f1=12 //ä¸åŠ å°æ•°ç‚¹ï¼Œè¢«æ¨å¯¼ä¸ºæ•´å‹ f2:=12.0 //åŠ å°æ•°ç‚¹ï¼Œè¢«æ¨å¯¼ä¸ºfloat64 f1=float32(f2) //éœ€è¦æ‰§è¡Œå¼ºåˆ¶è½¬æ¢ //2ã€æµ®ç‚¹æ•°çš„æ¯”è¾ƒ //æµ®ç‚¹æ•°ä¸æ˜¯ç²¾ç¡®çš„è¡¨è¾¾æ–¹å¼ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨â€œ==â€æ¥åˆ¤æ–­æ˜¯å¦ç›¸ç­‰ï¼Œå¯ä»¥å€Ÿç”¨mathçš„åŒ…math.Fdim 1.4. å¤æ•°ç±»å‹ //1ã€å¤æ•°çš„è¡¨ç¤º var v1 complex64 v1=3.2+12i //v1 v2 v3 è¡¨ç¤ºä¸ºåŒä¸€ä¸ªæ•° v2:=3.2+12i v3:=complex(3.2,12) //2ã€å®éƒ¨ä¸è™šéƒ¨ //z=complex(x,y),é€šè¿‡å†…ç½®å‡½æ•°å®éƒ¨x=real(z),è™šéƒ¨y=imag(z) 1.5. å­—ç¬¦ä¸² //å£°æ˜ä¸èµ‹å€¼ var str string str=\"hello world\" 1.6. å­—ç¬¦ç±»å‹ //1ã€byteï¼Œå³uint8çš„åˆ«å //2ã€runeï¼Œå³Unicode 1.7. é”™è¯¯ç±»å‹ï¼ˆerrorï¼‰ 2. å¤åˆç±»å‹ 2.1. æ•°ç»„(array) æ•°ç»„è¡¨ç¤ºåŒä¸€ç±»å‹æ•°æ®ï¼Œæ•°ç»„é•¿åº¦å®šä¹‰åå°±ä¸å¯æ›´æ”¹ï¼Œé•¿åº¦æ˜¯æ•°ç»„å†…çš„ä¸€ä¸ªå†…ç½®å¸¸é‡ï¼Œå¯é€šè¿‡len()æ¥è·å–ã€‚\n//1ã€åˆ›å»ºæ•°ç»„ var array1 [5]int //å£°æ˜ï¼švar å˜é‡å ç±»å‹ var array2 [5]int=[5]int{1,2,3,4,5} //åˆå§‹åŒ– array3ï¼š=[5]int{1,2,3,4,5} //ç›´æ¥ç”¨â€œï¼š=â€èµ‹å€¼ [3][5]int //äºŒç»´æ•°ç»„ [3]*float //æŒ‡é’ˆæ•°ç»„ //2ã€å…ƒç´ è®¿é—® for i,v:=range array{ //ç¬¬ä¸€ä¸ªè¿”å›å€¼ä¸ºæ•°ç»„ä¸‹æ ‡ï¼Œç¬¬äºŒä¸ªä¸ºå…ƒç´ çš„å€¼ } //3ã€å€¼ç±»å‹ //æ•°ç»„åœ¨Goä¸­ä½œä¸ºä¸€ä¸ªå€¼ç±»å‹ï¼Œå€¼ç±»å‹åœ¨èµ‹å€¼å’Œå‡½æ•°å‚æ•°ä¼ é€’æ—¶ï¼Œåªå¤åˆ¶å‰¯æœ¬ï¼Œå› æ­¤åœ¨å‡½æ•°ä½“ä¸­å¹¶ä¸èƒ½æ”¹å˜æ•°ç»„çš„å†…å®¹ï¼Œéœ€ç”¨æŒ‡é’ˆæ¥æ”¹å˜æ•°ç»„çš„å€¼ã€‚ 2.2. åˆ‡ç‰‡(slice) â€‹\tæ•°ç»„åœ¨å®šä¹‰äº†é•¿åº¦åæ— æ³•æ”¹å˜ï¼Œä¸”ä½œä¸ºå€¼ç±»å‹åœ¨ä¼ é€’æ—¶äº§ç”Ÿå‰¯æœ¬ï¼Œå¹¶ä¸èƒ½æ”¹å˜æ•°ç»„å…ƒç´ çš„å€¼ã€‚å› æ­¤åˆ‡ç‰‡çš„åŠŸèƒ½å¼¥è¡¥äº†è¿™ä¸ªä¸è¶³ï¼Œåˆ‡ç‰‡ç±»ä¼¼æŒ‡å‘æ•°ç»„çš„ä¸€ä¸ªæŒ‡é’ˆã€‚å¯ä»¥æŠ½è±¡ä¸ºä¸‰ä¸ªå˜é‡ï¼šæŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆï¼›åˆ‡ç‰‡ä¸­å…ƒç´ çš„ä¸ªæ•°(lenå‡½æ•°)ï¼›å·²åˆ†é…çš„å­˜å‚¨ç©ºé—´(capå‡½æ•°)ã€‚\n//1ã€åˆ›å»ºåˆ‡ç‰‡ //a)åŸºäºæ•°ç»„åˆ›å»º var myArray [5]int=[5]{1,2,3,4,5} var mySlice []int=myArray[first:last] slice1=myArray[:] //åŸºäºæ•°ç»„æ‰€æœ‰å…ƒç´ åˆ›å»º slice2=myArray[:3] //åŸºäºå‰ä¸‰ä¸ªå…ƒç´ åˆ›å»º slice3=myArray[3:] //åŸºäºç¬¬3ä¸ªå…ƒç´ å¼€å§‹åçš„æ‰€æœ‰å…ƒç´ åˆ›å»º //b)ç›´æ¥åˆ›å»º slice1:=make([]int,5) //å…ƒç´ åˆå§‹å€¼ä¸º0ï¼Œåˆå§‹ä¸ªæ•°ä¸º5 slice2:=make([]int,5,10) //å…ƒç´ åˆå§‹å€¼ä¸º0ï¼Œåˆå§‹ä¸ªæ•°ä¸º5ï¼Œé¢„ç•™ä¸ªæ•°ä¸º10 slice3:=[]int{1,2,3,4,5} //åˆå§‹åŒ–èµ‹å€¼ //c)åŸºäºåˆ‡ç‰‡åˆ›å»º oldSlice:=[]int{1,2,3,4,5} newSlice:=oldSlice[:3] //åŸºäºåˆ‡ç‰‡åˆ›å»ºï¼Œä¸èƒ½è¶…è¿‡åŸåˆ‡ç‰‡çš„å­˜å‚¨ç©ºé—´(capå‡½æ•°çš„å€¼) //2ã€å…ƒç´ éå† for i,v:=range slice{ //ä¸æ•°ç»„çš„æ–¹å¼ä¸€è‡´ï¼Œä½¿ç”¨rangeæ¥éå† //ç¬¬ä¸€ä¸ªè¿”å›å€¼(i)ä¸ºç´¢å¼•ï¼Œç¬¬äºŒä¸ªä¸ºå…ƒç´ çš„å€¼(v) } //3ã€åŠ¨æ€å¢å‡å…ƒç´  //åˆ‡ç‰‡åˆ†å­˜å‚¨ç©ºé—´(cap)å’Œå…ƒç´ ä¸ªæ•°(len)ï¼Œå½“å­˜å‚¨ç©ºé—´å°äºå®é™…çš„å…ƒç´ ä¸ªæ•°ï¼Œä¼šé‡æ–°åˆ†é…ä¸€å—åŸç©ºé—´2å€çš„å†…å­˜å—ï¼Œå¹¶å°†åŸæ•°æ®å¤åˆ¶åˆ°è¯¥å†…å­˜å—ä¸­ï¼Œåˆç†çš„åˆ†é…å­˜å‚¨ç©ºé—´å¯ä»¥ä»¥ç©ºé—´æ¢æ—¶é—´ï¼Œé™ä½ç³»ç»Ÿå¼€é”€ã€‚ //æ·»åŠ å…ƒç´  newSlice:=append(oldSlice,1,2,3) //ç›´æ¥å°†å…ƒç´ åŠ è¿›å»ï¼Œè‹¥å­˜å‚¨ç©ºé—´ä¸å¤Ÿä¼šæŒ‰ä¸Šè¿°æ–¹å¼æ‰©å®¹ã€‚ newSlice1:=append(oldSlice1,oldSlice2...) //å°†oldSlice2çš„å…ƒç´ æ‰“æ•£ååŠ åˆ°oldSlice1ä¸­ï¼Œä¸‰ä¸ªç‚¹ä¸å¯çœç•¥ã€‚ //4ã€å†…å®¹å¤åˆ¶ //copy()å‡½æ•°å¯ä»¥å¤åˆ¶åˆ‡ç‰‡ï¼Œå¦‚æœåˆ‡ç‰‡å¤§å°ä¸ä¸€æ ·ï¼ŒæŒ‰è¾ƒå°çš„åˆ‡ç‰‡å…ƒç´ ä¸ªæ•°è¿›è¡Œå¤åˆ¶ slice1:=[]int{1,2,3,4,5} slice2:=[]int{6,7,8} copy(slice2,slice1) //åªä¼šå¤åˆ¶slice1çš„å‰ä¸‰ä¸ªå…ƒç´ åˆ°slice2ä¸­ copy(slice1,slice1) //åªä¼šå¤åˆ¶slice2çš„ä¸‰ä¸ªå…ƒç´ åˆ°slice1ä¸­çš„å‰ä¸‰ä¸ªä½ç½® 2.3. é”®å€¼å¯¹(map) mapæ˜¯ä¸€å †é”®å€¼å¯¹çš„æœªæ’åºé›†åˆã€‚\n//1ã€å…ˆå£°æ˜ååˆ›å»ºå†èµ‹å€¼ var map1 map[é”®ç±»å‹] å€¼ç±»å‹ //åˆ›å»º map1=make(map[é”®ç±»å‹] å€¼ç±»å‹) map1=make(map[é”®ç±»å‹] å€¼ç±»å‹ å­˜å‚¨ç©ºé—´) //èµ‹å€¼ map1[key]=value // ç›´æ¥åˆ›å»º m2 := make(map[string]string) // ç„¶åèµ‹å€¼ m2[\"a\"] = \"aa\" m2[\"b\"] = \"bb\" // åˆå§‹åŒ– + èµ‹å€¼ä¸€ä½“åŒ– m3 := map[string]string{ \"a\": \"aa\", \"b\": \"bb\", } //2ã€å…ƒç´ åˆ é™¤ //delete()å‡½æ•°åˆ é™¤å¯¹åº”keyçš„é”®å€¼å¯¹ï¼Œå¦‚æœkeyä¸å­˜åœ¨ï¼Œä¸ä¼šæŠ¥é”™ï¼›å¦‚æœvalueä¸ºnilï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸(panic)ã€‚ delete(map1,key) //3ã€å…ƒç´ æŸ¥æ‰¾ value,ok:=myMap[key] if ok{//å¦‚æœæ‰¾åˆ° //å¤„ç†æ‰¾åˆ°çš„valueå€¼ } //éå† for key,value:=range myMap{ //å¤„ç†keyæˆ–value } mapå¯ä»¥ç”¨æ¥åˆ¤æ–­ä¸€ä¸ªå€¼æ˜¯å¦åœ¨åˆ‡ç‰‡æˆ–æ•°ç»„ä¸­ã€‚\n// åˆ¤æ–­æŸä¸ªç±»å‹ï¼ˆå‡å¦‚ä¸ºmyTypeï¼‰çš„å€¼æ˜¯å¦åœ¨åˆ‡ç‰‡æˆ–æ•°ç»„ï¼ˆå‡å¦‚ä¸ºmyListï¼‰ä¸­ // æ„é€ ä¸€ä¸ªmap,keyçš„ç±»å‹ä¸ºmyType,valueä¸ºboolå‹ myMap := make(map[myType]bool) myList := []myType{value1, value2} // å°†åˆ‡ç‰‡ä¸­çš„å€¼å­˜ä¸ºmapä¸­çš„keyï¼ˆå› ä¸ºkeyä¸èƒ½é‡å¤ï¼‰,mapçš„valueéƒ½ä¸ºtrue for _, value := range myList { myMap[value] = true } // åˆ¤æ–­valueXæ˜¯å¦åœ¨myListä¸­ï¼Œå³åˆ¤æ–­å…¶æ˜¯å¦åœ¨myMapçš„keyä¸­ if _, ok := myMap[valueX]; ok { // å¦‚æœvalueX åœ¨myListä¸­ï¼Œæ‰§è¡Œåç»­æ“ä½œ } 2.4. æŒ‡é’ˆ(pointer) å…·ä½“å‚è€ƒGoè¯­è¨€æŒ‡é’ˆè¯¦è§£\n2.5. ç»“æ„ä½“(struct) å…·ä½“å‚è€ƒGoé¢å‘å¯¹è±¡ç¼–ç¨‹ä¹‹ç»“æ„ä½“\n2.6. æ¥å£(interface) å…·ä½“å‚è€ƒGoé¢å‘å¯¹è±¡ç¼–ç¨‹ä¹‹æ¥å£\n2.7. é€šé“(chan) å…·ä½“å‚è€ƒGoå¹¶å‘ç¼–ç¨‹ä¹‹channel\n","categories":"","description":"","excerpt":"ç±»å‹ 1. åŸºç¡€ç±»å‹ 1.1. å¸ƒå°”ç±»å‹ //å¸ƒå°”ç±»å‹çš„å…³é”®å­—ä¸ºbool,å€¼ä¸ºtrueæˆ–falseï¼Œä¸å¯å†™ä¸º0æˆ–1 var v1 bool â€¦","ref":"/golang-notes/basis/data-types/","tags":["Golang"],"title":"æ•°æ®ç±»å‹"},{"body":" æœ¬æ–‡ä»¥cobra-demoä¸ºä¾‹ä»‹ç»cobraæ·»åŠ å‘½ä»¤çš„å…·ä½“ä½¿ç”¨æ“ä½œã€‚\n0. cobra-demo cobra-demoç¼–è¯‘äºŒè¿›åˆ¶æ‰§è¡Œçš„ç»“æœå¦‚ä¸‹ã€‚å…·ä½“ä»£ç å‚è€ƒï¼šhttps://github.com/huweihuang/cobra-demo\n$ ./cobra-demo A longer description that spans multiple lines and likely contains examples and usage of using your application. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobra-demo [command] Available Commands: config A brief description of your command help Help about any command server A brief description of your command Flags: --config string config file (default is $HOME/.cobra-demo.yaml) -h, --help help for cobra-demo -t, --toggle Help message for toggle Use \"cobra-demo [command] --help\" for more information about a command. 1. cobra init cobra init çš„å…·ä½“ä½¿ç”¨å‚è€ƒ init\n1.1. cobra init --pkg-name cobra init \u003ccobra-demo\u003e --pkg-name github.com/huweihuang/cobra-demo -a 'author name \u003cemail\u003e' æ‰§è¡Œä»¥ä¸Šå‘½ä»¤ï¼Œåˆ›å»ºçš„æ–‡ä»¶ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n./ â”œâ”€â”€ LICENSE â”œâ”€â”€ cmd â”‚Â â”œâ”€â”€ root.go â””â”€â”€ main.go 1.2. main.go package main import \"github.com/huweihuang/cobra-demo/cmd\" func main() { cmd.Execute() } 1.3. cmd/root.go package cmd import ( \"fmt\" \"os\" \"github.com/spf13/cobra\" homedir \"github.com/mitchellh/go-homedir\" \"github.com/spf13/viper\" ) var cfgFile string // rootCmd represents the base command when called without any subcommands var rootCmd = \u0026cobra.Command{ Use: \"cobra-demo\", Short: \"A brief description of your application\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your application. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, // Uncomment the following line if your bare application // has an action associated with it: //\tRun: func(cmd *cobra.Command, args []string) { }, } // Execute adds all child commands to the root command and sets flags appropriately. // This is called by main.main(). It only needs to happen once to the rootCmd. func Execute() { if err := rootCmd.Execute(); err != nil { fmt.Println(err) os.Exit(1) } } func init() { cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings. // Cobra supports persistent flags, which, if defined here, // will be global for your application. rootCmd.PersistentFlags().StringVar(\u0026cfgFile, \"config\", \"\", \"config file (default is $HOME/.cobra-demo.yaml)\") // Cobra also supports local flags, which will only run // when this action is called directly. rootCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\") } // initConfig reads in config file and ENV variables if set. func initConfig() { if cfgFile != \"\" { // Use config file from the flag. viper.SetConfigFile(cfgFile) } else { // Find home directory. home, err := homedir.Dir() if err != nil { fmt.Println(err) os.Exit(1) } // Search config in home directory with name \".cobra-demo\" (without extension). viper.AddConfigPath(home) viper.SetConfigName(\".cobra-demo\") } viper.AutomaticEnv() // read in environment variables that match // If a config file is found, read it in. if err := viper.ReadInConfig(); err == nil { fmt.Println(\"Using config file:\", viper.ConfigFileUsed()) } } 2. cobra add cobra add çš„å…·ä½“ä½¿ç”¨å‚è€ƒ add\n2.1. cobra add command cobra add serve -a 'author name \u003cemail\u003e' cobra add config -a 'author name \u003cemail\u003e' cobra add create -p 'configCmd' -a 'author name \u003cemail\u003e'# åœ¨çˆ¶å‘½ä»¤configå‘½ä»¤ä¸‹åˆ›å»ºå­å‘½ä»¤create,è‹¥æ²¡æœ‰æŒ‡å®š-p,é»˜è®¤çš„çˆ¶å‘½ä»¤ä¸ºrootCmdã€‚ æ‰§è¡Œä»¥ä¸Šå‘½ä»¤ï¼Œåˆ›å»ºçš„æ–‡ä»¶ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n./ â”œâ”€â”€ LICENSE â”œâ”€â”€ cmd â”‚Â â”œâ”€â”€ config.go # rootCmdçš„å­å‘½ä»¤ â”‚Â â”œâ”€â”€ create.go # configçš„å­å‘½ä»¤ â”‚Â â”œâ”€â”€ root.go # é»˜è®¤çˆ¶å‘½ä»¤ â”‚Â â””â”€â”€ server.go # rootCmdçš„å­å‘½ä»¤ â””â”€â”€ main.go 2.2. cmd/config.go package cmd import ( \"fmt\" \"github.com/spf13/cobra\" ) // configCmd represents the config command var configCmd = \u0026cobra.Command{ Use: \"config\", Short: \"A brief description of your command\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"config called\") }, } func init() { rootCmd.AddCommand(configCmd) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags which will work for this command // and all subcommands, e.g.: // configCmd.PersistentFlags().String(\"foo\", \"\", \"A help for foo\") // Cobra supports local flags which will only run when this command // is called directly, e.g.: // configCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\") } 2.3. cmd/create.go createä¸ºconfigçš„å­å‘½ä»¤ã€‚\npackage cmd import ( \"fmt\" \"github.com/spf13/cobra\" ) // createCmd represents the create command var createCmd = \u0026cobra.Command{ Use: \"create\", Short: \"A brief description of your command\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"create called\") }, } func init() { configCmd.AddCommand(createCmd) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags which will work for this command // and all subcommands, e.g.: // createCmd.PersistentFlags().String(\"foo\", \"\", \"A help for foo\") // Cobra supports local flags which will only run when this command // is called directly, e.g.: // createCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\") } å‚è€ƒï¼š\nhttps://github.com/spf13/cobra https://github.com/spf13/cobra/blob/master/cobra/README.md ","categories":"","description":"","excerpt":" æœ¬æ–‡ä»¥cobra-demoä¸ºä¾‹ä»‹ç»cobraæ·»åŠ å‘½ä»¤çš„å…·ä½“ä½¿ç”¨æ“ä½œã€‚\n0. cobra-demo cobra-demoç¼–è¯‘äºŒè¿›åˆ¶æ‰§è¡Œçš„ç»“æœ â€¦","ref":"/golang-notes/framework/cobra/cobra-command/","tags":["Golang"],"title":"cobra command"},{"body":"1. ç®€ä»‹ iptablesæ˜¯ä¸€ä¸ªè®¾ç½®é˜²ç«å¢™ï¼ˆnetfilterï¼‰è§„åˆ™çš„å‘½ä»¤å·¥å…·ã€‚ç½‘ç»œè§„åˆ™åŒ…æ‹¬æºåœ°å€ã€ç›®çš„åœ°å€ã€ä¼ è¾“åè®®ï¼ˆå¦‚TCPã€UDPã€ICMPï¼‰å’ŒæœåŠ¡ç±»å‹ï¼ˆå¦‚HTTPã€FTPå’ŒSMTPï¼‰ç­‰ï¼Œå½“æ•°æ®åŒ…ä¸è§„åˆ™åŒ¹é…æ—¶ï¼Œiptableså°±æ ¹æ®è§„åˆ™æ‰€å®šä¹‰çš„æ–¹æ³•æ¥å¤„ç†è¿™äº›æ•°æ®åŒ…ï¼Œå¦‚æ”¾è¡Œï¼ˆacceptï¼‰ã€æ‹’ç»ï¼ˆrejectï¼‰å’Œä¸¢å¼ƒï¼ˆdropï¼‰ç­‰ã€‚é…ç½®é˜²ç«å¢™çš„ä¸»è¦å·¥ä½œå°±æ˜¯æ·»åŠ ã€ä¿®æ”¹å’Œåˆ é™¤è¿™äº›è§„åˆ™ã€‚\n2. åŸºæœ¬æ¦‚å¿µ 2.1. é“¾(Chain) ç½‘ç»œè®¾ç½®çš„â€å…³å¡â€œä¸€èˆ¬æœ‰å¤šä¸ªç½‘ç»œè§„åˆ™ï¼Œç§°ä¸ºé“¾ã€‚\nINPUT\nOUTPUT\nFORWORD\nPREROUTING\nPOSTROUTING\n2.2. è¡¨ å…·æœ‰ç›¸åŒåŠŸèƒ½çš„è§„åˆ™çš„é›†åˆå«åšâ€è¡¨â€ã€‚iptableså®šä¹‰äº†å››ç±»è¡¨ã€‚\nfilterè¡¨ï¼šè´Ÿè´£è¿‡æ»¤åŠŸèƒ½ï¼Œé˜²ç«å¢™ï¼›å†…æ ¸æ¨¡å—ï¼šiptables_filter\nnatè¡¨ï¼šnetwork address translationï¼Œç½‘ç»œåœ°å€è½¬æ¢åŠŸèƒ½ï¼›å†…æ ¸æ¨¡å—ï¼šiptable_nat\nmangleè¡¨ï¼šæ‹†è§£æŠ¥æ–‡ï¼Œåšå‡ºä¿®æ”¹ï¼Œå¹¶é‡æ–°å°è£… çš„åŠŸèƒ½ï¼›iptable_mangle\nrawè¡¨ï¼šå…³é—­natè¡¨ä¸Šå¯ç”¨çš„è¿æ¥è¿½è¸ªæœºåˆ¶ï¼›iptable_raw\n2.3. è¡¨å’Œé“¾çš„å…³ç³» PREROUTINGçš„è§„åˆ™å¯ä»¥å­˜åœ¨äºï¼šrawè¡¨ï¼Œmangleè¡¨ï¼Œnatè¡¨ã€‚\nINPUTçš„è§„åˆ™å¯ä»¥å­˜åœ¨äºï¼šmangleè¡¨ï¼Œfilterè¡¨ã€‚\nFORWARDçš„è§„åˆ™å¯ä»¥å­˜åœ¨äºï¼šmangleè¡¨ï¼Œfilterè¡¨ã€‚\nOUTPUTçš„è§„åˆ™å¯ä»¥å­˜åœ¨äºï¼šrawè¡¨mangleè¡¨ï¼Œnatè¡¨ï¼Œfilterè¡¨ã€‚\nPOSTROUTINGçš„è§„åˆ™å¯ä»¥å­˜åœ¨äºï¼šmangleè¡¨ï¼Œnatè¡¨ã€‚\n3. è§„åˆ™åŒ¹é…æ¡ä»¶ åŸºæœ¬åŒ¹é…æ¡ä»¶\næºåœ°å€Source IP\nç›®æ ‡åœ°å€ Destination IP\næ‰©å±•åŒ¹é…æ¡ä»¶\næºç«¯å£Source Port,\nç›®æ ‡ç«¯å£Destination Port\nå¤„ç†æ“ä½œ\nACCEPTï¼šå…è®¸æ•°æ®åŒ…é€šè¿‡ã€‚\nDROPï¼šç›´æ¥ä¸¢å¼ƒæ•°æ®åŒ…ï¼Œä¸ç»™ä»»ä½•å›åº”ä¿¡æ¯ï¼Œè¿™æ—¶å€™å®¢æˆ·ç«¯ä¼šæ„Ÿè§‰è‡ªå·±çš„è¯·æ±‚æ³¥ç‰›å…¥æµ·äº†ï¼Œè¿‡äº†è¶…æ—¶æ—¶é—´æ‰ä¼šæœ‰ååº”ã€‚\nREJECTï¼šæ‹’ç»æ•°æ®åŒ…é€šè¿‡ï¼Œå¿…è¦æ—¶ä¼šç»™æ•°æ®å‘é€ç«¯ä¸€ä¸ªå“åº”çš„ä¿¡æ¯ï¼Œå®¢æˆ·ç«¯åˆšè¯·æ±‚å°±ä¼šæ”¶åˆ°æ‹’ç»çš„ä¿¡æ¯ã€‚\nSNATï¼šæºåœ°å€è½¬æ¢ï¼Œè§£å†³å†…ç½‘ç”¨æˆ·ç”¨åŒä¸€ä¸ªå…¬ç½‘åœ°å€ä¸Šç½‘çš„é—®é¢˜ã€‚\nMASQUERADEï¼šæ˜¯SNATçš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œé€‚ç”¨äºåŠ¨æ€çš„ã€ä¸´æ—¶ä¼šå˜çš„ipä¸Šã€‚\nDNATï¼šç›®æ ‡åœ°å€è½¬æ¢ã€‚\nREDIRECTï¼šåœ¨æœ¬æœºåšç«¯å£æ˜ å°„ã€‚l\n4. æ•°æ®åŒ…ç»è¿‡é˜²ç«å¢™çš„æµç¨‹ å›¾ç‰‡æ¥è‡ªï¼šhttps://www.zsythink.net/archives/1199\nåˆ°æœ¬æœºæŸè¿›ç¨‹çš„æŠ¥æ–‡ï¼šPREROUTING â€“\u003e INPUT\nç”±æœ¬æœºè½¬å‘çš„æŠ¥æ–‡ï¼šPREROUTING â€“\u003e FORWARD â€“\u003e POSTROUTING\nç”±æœ¬æœºçš„æŸè¿›ç¨‹å‘å‡ºæŠ¥æ–‡ï¼ˆé€šå¸¸ä¸ºå“åº”æŠ¥æ–‡ï¼‰ï¼šOUTPUT â€“\u003e POSTROUTING\nå‚è€ƒï¼š\nIPtables-æœ±åŒå°åšå®¢\niptablesè¯¦è§£ï¼ˆ1ï¼‰ï¼šiptablesæ¦‚å¿µ-æœ±åŒå°åšå®¢\n","categories":"","description":"","excerpt":"1. ç®€ä»‹ iptablesæ˜¯ä¸€ä¸ªè®¾ç½®é˜²ç«å¢™ï¼ˆnetfilterï¼‰è§„åˆ™çš„å‘½ä»¤å·¥å…·ã€‚ç½‘ç»œè§„åˆ™åŒ…æ‹¬æºåœ°å€ã€ç›®çš„åœ°å€ã€ä¼ è¾“åè®®ï¼ˆ â€¦","ref":"/linux-notes/network/iptables/","tags":["iptables"],"title":"iptablesä»‹ç»"},{"body":"1. kubectlå‘½ä»¤ä»‹ç» kubectlçš„å‘½ä»¤è¯­æ³•\nkubectl [command] [TYPE] [NAME] [flags] å…¶ä¸­commandï¼ŒTYPEï¼ŒNAMEï¼Œå’Œflagsåˆ†åˆ«æ˜¯ï¼š\ncommand: æŒ‡å®šè¦åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªèµ„æºè¿›è¡Œæ“ä½œï¼Œä¾‹å¦‚createï¼Œgetï¼Œdescribeï¼Œdeleteã€‚\nTYPEï¼šæŒ‡å®šèµ„æºç±»å‹ã€‚èµ„æºç±»å‹åŒºåˆ†å¤§å°å†™ï¼Œæ‚¨å¯ä»¥æŒ‡å®šå•æ•°ï¼Œå¤æ•°æˆ–ç¼©å†™å½¢å¼ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹å‘½ä»¤äº§ç”Ÿç›¸åŒçš„è¾“å‡ºï¼š\nkubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAMEï¼šæŒ‡å®šèµ„æºçš„åç§°ã€‚åç§°åŒºåˆ†å¤§å°å†™ã€‚å¦‚æœçœç•¥åç§°ï¼Œåˆ™ä¼šæ˜¾ç¤ºæ‰€æœ‰èµ„æºçš„è¯¦ç»†ä¿¡æ¯,æ¯”å¦‚$ kubectl get podsã€‚\næŒ‰ç±»å‹å’Œåç§°æŒ‡å®šå¤šç§èµ„æºï¼š\n* è¦åˆ†ç»„èµ„æºï¼Œå¦‚æœå®ƒä»¬éƒ½æ˜¯ç›¸åŒçš„ç±»å‹ï¼š`TYPE1 name1 name2 name\u003c#\u003e`.\u003cbr/\u003e ä¾‹: `$ kubectl get pod example-pod1 example-pod2` * è¦åˆ†åˆ«æŒ‡å®šå¤šç§èµ„æºç±»å‹: `TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE\u003c#\u003e/name\u003c#\u003e`.\u003cbr/\u003e ä¾‹: `$ kubectl get pod/example-pod1 replicationcontroller/example-rc1` flagsï¼šæŒ‡å®šå¯é€‰æ ‡å¿—ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨-sæˆ–--serverflagsæ¥æŒ‡å®šKubernetes APIæœåŠ¡å™¨çš„åœ°å€å’Œç«¯å£ã€‚\næ›´å¤šå‘½ä»¤ä»‹ç»ï¼š\n[root@node5 ~]# kubectl kubectl controls the Kubernetes cluster manager. Find more information at https://github.com/kubernetes/kubernetes. Basic Commands (Beginner): create Create a resource from a file or from stdin. expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service run Run a particular image on the cluster set Set specific features on objects run-container Run a particular image on the cluster. This command is deprecated, use \"run\" instead Basic Commands (Intermediate): get Display one or many resources explain Documentation of resources edit Edit a resource on the server delete Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands: rollout Manage the rollout of a resource rolling-update Perform a rolling update of the given ReplicationController scale Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job autoscale Auto-scale a Deployment, ReplicaSet, or ReplicationController Cluster Management Commands: certificate Modify certificate resources. cluster-info Display cluster info top Display Resource (CPU/Memory/Storage) usage. cordon Mark node as unschedulable uncordon Mark node as schedulable drain Drain node in preparation for maintenance taint Update the taints on one or more nodes Troubleshooting and Debugging Commands: describe Show details of a specific resource or group of resources logs Print the logs for a container in a pod attach Attach to a running container exec Execute a command in a container port-forward Forward one or more local ports to a pod proxy Run a proxy to the Kubernetes API server cp Copy files and directories to and from containers. auth Inspect authorization Advanced Commands: apply Apply a configuration to a resource by filename or stdin patch Update field(s) of a resource using strategic merge patch replace Replace a resource by filename or stdin convert Convert config files between different API versions Settings Commands: label Update the labels on a resource annotate Update the annotations on a resource completion Output shell completion code for the specified shell (bash or zsh) Other Commands: api-versions Print the supported API versions on the server, in the form of \"group/version\" config Modify kubeconfig files help Help about any command plugin Runs a command-line plugin version Print the client and server version information Use \"kubectl \u003ccommand\u003e --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). 2. æ“ä½œçš„å¸¸ç”¨èµ„æºå¯¹è±¡ Node Podes Replication Controllers Services Namespace Deployment StatefulSet å…·ä½“å¯¹è±¡ç±»å‹åŠç¼©å†™ï¼š\n* all * certificatesigningrequests (aka 'csr') * clusterrolebindings * clusterroles * componentstatuses (aka 'cs') * configmaps (aka 'cm') * controllerrevisions * cronjobs * customresourcedefinition (aka 'crd') * daemonsets (aka 'ds') * deployments (aka 'deploy') * endpoints (aka 'ep') * events (aka 'ev') * horizontalpodautoscalers (aka 'hpa') * ingresses (aka 'ing') * jobs * limitranges (aka 'limits') * namespaces (aka 'ns') * networkpolicies (aka 'netpol') * nodes (aka 'no') * persistentvolumeclaims (aka 'pvc') * persistentvolumes (aka 'pv') * poddisruptionbudgets (aka 'pdb') * podpreset * pods (aka 'po') * podsecuritypolicies (aka 'psp') * podtemplates * replicasets (aka 'rs') * replicationcontrollers (aka 'rc') * resourcequotas (aka 'quota') * rolebindings * roles * secrets * serviceaccounts (aka 'sa') * services (aka 'svc') * statefulsets (aka 'sts') * storageclasses (aka 'sc') 3. kubectlå‘½ä»¤åˆ†ç±»[command] 3.1 å¢ 1ï¼‰create:[Create a resource by filename or stdin]\n2ï¼‰run:[ Run a particular image on the cluster]\n3ï¼‰apply:[Apply a configuration to a resource by filename or stdin]\n4ï¼‰proxy:[Run a proxy to the Kubernetes API server ]\n3.2 åˆ  1ï¼‰delete:[Delete resources ]\n3.3 æ”¹ 1ï¼‰scale:[Set a new size for a Replication Controller]\n2ï¼‰exec:[Execute a command in a container]\n3ï¼‰attach:[Attach to a running container]\n4ï¼‰patch:[Update field(s) of a resource by stdin]\n5ï¼‰edit:[Edit a resource on the server]\n6ï¼‰ label:[Update the labels on a resource]\n7ï¼‰annotate:[Auto-scale a replication controller]\n8ï¼‰replace:[Replace a resource by filename or stdin]\n9ï¼‰config:[config modifies kubeconfig files]\n3.4 æŸ¥ 1ï¼‰get:[Display one or many resources]\n2ï¼‰describe:[Show details of a specific resource or group of resources]\n3ï¼‰log:[Print the logs for a container in a pod]\n4ï¼‰cluster-info:[Display cluster info]\n5ï¼‰ version:[Print the client and server version information]\n6ï¼‰api-versions:[Print the supported API versions]\n4. Podç›¸å…³å‘½ä»¤ 4.1 æŸ¥è¯¢Pod kubectl get pod -o wide --namespace=\u003cNAMESPACE\u003e 4.2 è¿›å…¥Pod kubectl exec -it \u003cPodName\u003e /bin/bash --namespace=\u003cNAMESPACE\u003e # è¿›å…¥Podä¸­æŒ‡å®šå®¹å™¨ kubectl exec -it \u003cPodName\u003e -c \u003cContainerName\u003e /bin/bash --namespace=\u003cNAMESPACE\u003e 4.3 åˆ é™¤Pod kubectl delete pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e # å¼ºåˆ¶åˆ é™¤Podï¼Œå½“Podä¸€ç›´å¤„äºTerminatingçŠ¶æ€ kubectl delete pod \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e --force --grace-period=0 # åˆ é™¤æŸä¸ªnamespaceä¸‹æŸä¸ªç±»å‹çš„æ‰€æœ‰å¯¹è±¡ kubectl delete deploy --all --namespace=test 4.4 æ—¥å¿—æŸ¥çœ‹ $ æŸ¥çœ‹è¿è¡Œå®¹å™¨æ—¥å¿— kubectl logs \u003cPodName\u003e --namespace=\u003cNAMESPACE\u003e $ æŸ¥çœ‹ä¸Šä¸€ä¸ªæŒ‚æ‰çš„å®¹å™¨æ—¥å¿— kubectl logs \u003cPodName\u003e -p --namespace=\u003cNAMESPACE\u003e 5. å¸¸ç”¨å‘½ä»¤ 5.1. Nodeéš”ç¦»ä¸æ¢å¤ è¯´æ˜ï¼šNodeè®¾ç½®éš”ç¦»ä¹‹åï¼ŒåŸå…ˆè¿è¡Œåœ¨è¯¥Nodeä¸Šçš„Podä¸å—å½±å“ï¼Œåç»­çš„Podä¸ä¼šè°ƒåº¦åˆ°è¢«éš”ç¦»çš„Nodeä¸Šã€‚\n1. Nodeéš”ç¦»\n# cordonå‘½ä»¤ kubectl cordon \u003cNodeName\u003e # æˆ–è€… kubectl patch node \u003cNodeName\u003e -p '{\"spec\":{\"unschedulable\":true}}' 2. Nodeæ¢å¤\n# uncordon kubectl uncordon \u003cNodeName\u003e # æˆ–è€… kubectl patch node \u003cNodeName\u003e -p '{\"spec\":{\"unschedulable\":false}}' 5.2. kubectl label 1. å›ºå®šPodåˆ°æŒ‡å®šæœºå™¨\nkubectl label node \u003cNodeName\u003e namespace/\u003cNAMESPACE\u003e=true 2. å–æ¶ˆPodå›ºå®šæœºå™¨\nkubectl label node \u003cNodeName\u003e namespace/\u003cNAMESPACE\u003e- 5.3. å‡çº§é•œåƒ # å‡çº§é•œåƒ kubectl set image deployment/nginx nginx=nginx:1.15.12 -n nginx # æŸ¥çœ‹æ»šåŠ¨å‡çº§æƒ…å†µ kubectl rollout status deployment/nginx -n nginx 5.4. è°ƒæ•´èµ„æºå€¼ # è°ƒæ•´æŒ‡å®šå®¹å™¨çš„èµ„æºå€¼ kubectl set resources sts nginx-0 -c=agent --limits=memory=512Mi -n nginx 5.5. è°ƒæ•´readiness probe # æ‰¹é‡æŸ¥çœ‹readiness probe timeoutSeconds kubectl get statefulset -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.template.spec.containers[0].readinessProbe.timeoutSeconds}{\"\\n\"}{end}' # è°ƒæ•´readiness probe timeoutSecondså‚æ•° kubectl patch statefulset nginx-sts --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/readinessProbe/timeoutSeconds\", \"value\":5}]' -n nginx 5.6. è°ƒæ•´tolerationså±æ€§ kubectl patch statefulset nginx-sts --patch '{\"spec\": {\"template\": {\"spec\": {\"tolerations\": [{\"effect\": \"NoSchedule\",\"key\": \"dedicated\",\"operator\": \"Equal\",\"value\": \"nginx\"}]}}}}' -n nginx 5.7. æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹çš„IP kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.addresses[0].address}{\"\\n\"}{end}' 5.8. æŸ¥çœ‹å½“å‰k8sç»„ä»¶leaderèŠ‚ç‚¹ å½“k8sé›†ç¾¤é«˜å¯ç”¨éƒ¨ç½²çš„æ—¶å€™ï¼Œkube-controller-managerå’Œkube-scheduleråªèƒ½ä¸€ä¸ªæœåŠ¡å¤„äºå®é™…é€»è¾‘è¿è¡ŒçŠ¶æ€ï¼Œé€šè¿‡å‚æ•°--leader-elect=trueæ¥å¼€å¯é€‰ä¸¾æ“ä½œã€‚ä»¥ä¸‹æä¾›æŸ¥è¯¢leaderèŠ‚ç‚¹çš„å‘½ä»¤ã€‚\n$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml apiVersion: v1 kind: Endpoints metadata: annotations: control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\"xxx.xxx.xxx.xxx_6537b938-7f5a-11e9-8487-00220d338975\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-05-26T02:03:18Z\",\"renewTime\":\"2019-05-26T02:06:08Z\",\"leaderTransitions\":1}' creationTimestamp: \"2019-05-26T01:52:39Z\" name: kube-controller-manager namespace: kube-system resourceVersion: \"1965\" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: f1755fc5-7f58-11e9-b4c4-00220d338975 ä»¥ä¸Šè¡¨ç¤º\"holderIdentity\":\"xxx.xxx.xxx.xxxä¸ºkube-controller-managerçš„leaderèŠ‚ç‚¹ã€‚\nåŒç†ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹kube-schedulerçš„leaderèŠ‚ç‚¹ã€‚\nkubectl get endpoints kube-scheduler --namespace=kube-system -o yaml 5.9. ä¿®æ”¹å‰¯æœ¬æ•° kubectl scale deployment.v1.apps/nginx-deployment --replicas=10 5.10. æ‰¹é‡åˆ é™¤pod kubectl get po -n default |grep Evicted |awk '{print $1}' |xargs -I {} kubectl delete po {} -n default 5.11. å„ç§æŸ¥çœ‹å‘½ä»¤ # ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·æ¥è¾“å‡ºè§£ç åçš„ Secret kubectl get secret my-secret -o go-template='{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\n\"}}{{$v|base64decode}}{{\"\\n\\n\"}}{{end}}' # åˆ—å‡ºäº‹ä»¶ï¼ˆEventsï¼‰ï¼ŒæŒ‰æ—¶é—´æˆ³æ’åº kubectl get events --sort-by=.metadata.creationTimestamp 5.12. æ‹·è´æ–‡ä»¶ ä»podæ‹·è´åˆ°æœ¬åœ°\næ³¨æ„äº‹é¡¹ï¼š\npodçš„ç›®å½•æ˜¯workdirçš„ç›¸å¯¹è·¯å¾„ï¼Œå¯ä»¥å°†æ–‡ä»¶æ‹·è´åˆ°workdirä¸‹å†æ‹·è´å‡ºæ¥\næ–‡ä»¶ç»å¯¹è·¯å¾„å‰é¢ä¸èƒ½åŠ  /\næ–‡ä»¶ç›®æ ‡ä½ç½®ä¸èƒ½ä¸ºæ–‡ä»¶å¤¹ï¼Œå¿…é¡»ä¸ºæ–‡ä»¶è·¯å¾„\nkubectl cp -n \u003cns\u003e -c \u003ccontainer\u003e \u003cpod_name\u003e:\u003cä¸workdirçš„ç›¸å¯¹è·¯å¾„\u003e \u003cæœ¬åœ°è·¯å¾„æ–‡ä»¶å\u003e # ç¤ºä¾‹ï¼š # å°†pod workdirä¸‹çš„prometheus.env.yamlæ–‡ä»¶æ‹·è´åˆ°æœ¬åœ° kubectl cp -n prometheus -c prometheus prometheus-0:prometheus.env.yaml ./prometheus.env.yaml ä»æœ¬åœ°æ‹·è´åˆ°pod\næ³¨æ„äº‹é¡¹ï¼š\nå¦‚æœæ²¡æœ‰åŠ è·¯å¾„ï¼Œé»˜è®¤æ‹·è´åˆ°podå†…workdirè·¯å¾„ã€‚ kubectl cp \u003cæœ¬åœ°è·¯å¾„æ–‡ä»¶å\u003e -n \u003cns\u003e -c \u003ccontainer\u003e \u003cpod_name\u003e:\u003cä¸workdirçš„ç›¸å¯¹è·¯å¾„\u003e # ç¤ºä¾‹ï¼š kubectl cp ./prometheus.env.yaml -n prometheus -c prometheus prometheus-0:prometheus.env.yaml 5.13. å¼ºåˆ¶åˆ é™¤namespace å¦‚æœå¼ºåˆ¶åˆ é™¤nså¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ é™¤ï¼Œå°†ä»¥ä¸‹çš„calico-systemæ”¹ä¸ºéœ€è¦åˆ é™¤çš„namespaceã€‚\nkubectl get namespaces calico-system -o json \\ | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" \\ | kubectl replace --raw /api/v1/namespaces/calico-system/finalize -f - 5.14. edit status å‚è€ƒï¼š\nhttps://github.com/ulucinar/kubectl-edit-status https://krew.sigs.k8s.io/docs/user-guide/setup/install/ 1ã€é€šè¿‡äºŒè¿›åˆ¶å®‰è£…\nversion=v0.3.0 wget https://github.com/ulucinar/kubectl-edit-status/releases/download/${version}/kubectl-edit-status_${version}_linux_amd64.tar.gz tar -zvxf kubectl-edit-status_${version}_linux_amd64.tar.gz cp kubectl-edit_status /usr/bin/ 2ã€é€šè¿‡krewå®‰è£…edit-status\nå®‰è£…krew\n( set -x; cd \"$(mktemp -d)\" \u0026\u0026 OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" \u0026\u0026 ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" \u0026\u0026 KREW=\"krew-${OS}_${ARCH}\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" \u0026\u0026 tar zxvf \"${KREW}.tar.gz\" \u0026\u0026 ./\"${KREW}\" install krew ) export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" é€šè¿‡krewå®‰è£…edit-status\nè¯´æ˜ï¼šedit-statusä¸€èˆ¬åœ¨è¯¥statusæ˜¯ç»ˆæ­¢çŠ¶æ€æ—¶å¯ä»¥ä¿®æ”¹ï¼Œå¦‚æœéç»ˆæ­¢çŠ¶æ€å¯èƒ½è¢«operator recycleçŠ¶æ€è¦†ç›–ã€‚\nkubectl krew update kubectl krew install edit-status 6. kubectlæ—¥å¿—çº§åˆ« Kubectl æ—¥å¿—è¾“å‡ºè¯¦ç»†ç¨‹åº¦æ˜¯é€šè¿‡ -v æˆ–è€… --v æ¥æ§åˆ¶çš„ï¼Œå‚æ•°åè·Ÿä¸€ä¸ªæ•°å­—è¡¨ç¤ºæ—¥å¿—çš„çº§åˆ«ã€‚ Kubernetes é€šç”¨çš„æ—¥å¿—ä¹ æƒ¯å’Œç›¸å…³çš„æ—¥å¿—çº§åˆ«åœ¨ è¿™é‡Œ æœ‰ç›¸åº”çš„æè¿°ã€‚\nè¯¦ç»†ç¨‹åº¦ æè¿° --v=0 ç”¨äºé‚£äº›åº”è¯¥ å§‹ç»ˆ å¯¹è¿ç»´äººå‘˜å¯è§çš„ä¿¡æ¯ï¼Œå› ä¸ºè¿™äº›ä¿¡æ¯ä¸€èˆ¬å¾ˆæœ‰ç”¨ã€‚ --v=1 å¦‚æœæ‚¨ä¸æƒ³è¦çœ‹åˆ°å†—ä½™ä¿¡æ¯ï¼Œæ­¤å€¼æ˜¯ä¸€ä¸ªåˆç†çš„é»˜è®¤æ—¥å¿—çº§åˆ«ã€‚ --v=2 è¾“å‡ºæœ‰å…³æœåŠ¡çš„ç¨³å®šçŠ¶æ€çš„ä¿¡æ¯ä»¥åŠé‡è¦çš„æ—¥å¿—æ¶ˆæ¯ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½ä¸ç³»ç»Ÿä¸­çš„é‡å¤§å˜åŒ–æœ‰å…³ã€‚è¿™æ˜¯å»ºè®®å¤§å¤šæ•°ç³»ç»Ÿè®¾ç½®çš„é»˜è®¤æ—¥å¿—çº§åˆ«ã€‚ --v=3 åŒ…å«æœ‰å…³ç³»ç»ŸçŠ¶æ€å˜åŒ–çš„æ‰©å±•ä¿¡æ¯ã€‚ --v=4 åŒ…å«è°ƒè¯•çº§åˆ«çš„å†—ä½™ä¿¡æ¯ã€‚ --v=5 è·Ÿè¸ªçº§åˆ«çš„è¯¦ç»†ç¨‹åº¦ã€‚ --v=6 æ˜¾ç¤ºæ‰€è¯·æ±‚çš„èµ„æºã€‚ --v=7 æ˜¾ç¤º HTTP è¯·æ±‚å¤´ã€‚ --v=8 æ˜¾ç¤º HTTP è¯·æ±‚å†…å®¹ã€‚ --v=9 æ˜¾ç¤º HTTP è¯·æ±‚å†…å®¹è€Œä¸”ä¸æˆªæ–­å†…å®¹ã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/reference/kubectl/overview/ https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/ https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/ ","categories":"","description":"","excerpt":"1. kubectlå‘½ä»¤ä»‹ç» kubectlçš„å‘½ä»¤è¯­æ³•\nkubectl [command] [TYPE] [NAME] [flags] å…¶ â€¦","ref":"/kubernetes-notes/operation/kubectl/kubectl-commands/","tags":["Kubernetes"],"title":"kubectlå‘½ä»¤ä½¿ç”¨"},{"body":"virtual-kubelet --help #./virtual-kubelet --help virtual-kubelet implements the Kubelet interface with a pluggable backend implementation allowing users to create kubernetes nodes without running the kubelet. This allows users to schedule kubernetes workloads on nodes that aren't running Kubernetes. Usage: virtual-kubelet [flags] virtual-kubelet [command] Available Commands: help Help about any command providers Show the list of supported providers version Show the version of the program Flags: --cluster-domain string kubernetes cluster-domain (default is 'cluster.local') (default \"cluster.local\") --disable-taint disable the virtual-kubelet node taint --enable-node-lease use node leases (1.13) for node heartbeats --full-resync-period duration how often to perform a full resync of pods between kubernetes and the provider (default 1m0s) -h, --help help for virtual-kubelet --klog.alsologtostderr log to standard error as well as files --klog.log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --klog.log_dir string If non-empty, write log files in this directory --klog.log_file string If non-empty, use this log file --klog.log_file_max_size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --klog.logtostderr log to standard error instead of files (default true) --klog.skip_headers If true, avoid header prefixes in the log messages --klog.skip_log_headers If true, avoid headers when opening log files --klog.stderrthreshold severity logs at or above this threshold go to stderr (default 2) --klog.v Level number for the log level verbosity --klog.vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging --kubeconfig string kube config file to use for connecting to the Kubernetes API server (default \"/root/.kube/config\") --log-level string set the log level, e.g. \"debug\", \"info\", \"warn\", \"error\" (default \"info\") --metrics-addr string address to listen for metrics/stats requests (default \":10255\") --namespace string kubernetes namespace (default is 'all') --nodename string kubernetes node name (default \"virtual-kubelet\") --os string Operating System (Linux/Windows) (default \"Linux\") --pod-sync-workers int set the number of pod synchronization workers (default 10) --provider string cloud provider --provider-config string cloud provider configuration file --startup-timeout duration How long to wait for the virtual-kubelet to start --trace-exporter strings sets the tracing exporter to use, available exporters: [jaeger ocagent] --trace-sample-rate string set probability of tracing samples --trace-service-name string sets the name of the service used to register with the trace exporter (default \"virtual-kubelet\") --trace-tag map add tags to include with traces in key=value form Use \"virtual-kubelet [command] --help\" for more information about a command. ","categories":"","description":"","excerpt":"virtual-kubelet --help #./virtual-kubelet --help virtual-kubelet â€¦","ref":"/kubernetes-notes/multi-cluster/virtual-kubelet/virtual-kubelet-cmd/","tags":["VirtualKubelet"],"title":"Virtual Kubeletå‘½ä»¤"},{"body":"eclipseå¿«æ·é”® 1. å¿«æ·é”® 1.1. ç¼–è¾‘ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ æŸ¥æ‰¾å¹¶æ›¿æ¢ Ctrl+F æ–‡æœ¬ç¼–è¾‘å™¨ æŸ¥æ‰¾ä¸Šä¸€ä¸ª Ctrl+Shift+K æ–‡æœ¬ç¼–è¾‘å™¨ æŸ¥æ‰¾ä¸‹ä¸€ä¸ª Ctrl+K æ–‡æœ¬ç¼–è¾‘å™¨ åˆ é™¤å½“å‰è¡Œ Ctrl+D æ–‡æœ¬ç¼–è¾‘å™¨ å½“å‰è¡Œçš„ä¸‹ä¸€è¡Œæ’å…¥ç©ºè¡Œ Shift+Enter æ–‡æœ¬ç¼–è¾‘å™¨ å½“å‰è¡Œæ’å…¥ç©ºè¡Œ Ctrl+Shift+Enter æ–‡æœ¬ç¼–è¾‘å™¨ å®šä½åˆ°æœ€åç¼–è¾‘çš„ä½ç½® Ctrl+Q å…¨å±€ æ¢å¤ä¸Šä¸€ä¸ªé€‰æ‹© Alt+Shift+â†“ å…¨å±€ å¿«é€Ÿä¿®æ­£ Ctrl+1 å…¨å±€ å†…å®¹è¾…åŠ©ï¼ˆä»£ç æç¤ºï¼‰ Alt+/ å…¨å±€ å…¨éƒ¨é€‰ä¸­ Ctrl+A å…¨å±€ åˆ é™¤ Delete å…¨å±€ ä¸Šä¸‹æ–‡ä¿¡æ¯ Alt+/Alt+Shift+?Ctrl+Shift+Space Javaç¼–è¾‘å™¨ æ˜¾ç¤ºå·¥å…·æç¤ºæè¿° F2 Javaç¼–è¾‘å™¨ é€‰æ‹©å°è£…å…ƒç´  Alt+Shift+â†‘ Javaç¼–è¾‘å™¨ å¢é‡é€‰æ‹©ä¸Šä¸€ä¸ªåŒçº§å…ƒç´  Alt+Shift+â† Javaç¼–è¾‘å™¨ å¢é‡é€‰æ‹©ä¸‹ä¸€ä¸ªåŒçº§å…ƒç´  Alt+Shift+â†’ æ–‡æœ¬ç¼–è¾‘å™¨ å¢é‡æŸ¥æ‰¾ Ctrl+J æ–‡æœ¬ç¼–è¾‘å™¨ å¢é‡é€†å‘æŸ¥æ‰¾ Ctrl+Shift+J javaç¼–è¾‘å™¨ è‡ªåŠ¨ç”Ÿæˆget setæ–¹æ³• Alt+Shift+s å†æŒ‰ r javaç¼–è¾‘å™¨ åˆ—å‡ºæ‰€æœ‰å®ç°æ­¤æ¥å£çš„ç±» ctrl+T 1.2. æŸ¥çœ‹ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ æ”¾å¤§ Ctrl+= å…¨å±€ ç¼©å° Ctrl+- 1.3. çª—å£ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ æ¿€æ´»ç¼–è¾‘å™¨ F12 å…¨å±€ å…³é—­æ‰€æœ‰ç¼–è¾‘å™¨ Ctrl+Shift+W å…¨å±€ ä¸Šä¸€ä¸ªç¼–è¾‘å™¨ Ctrl+Shift+F6 å…¨å±€ ä¸Šä¸€ä¸ªè§†å›¾ Ctrl+Shift+F7 å…¨å±€ ä¸Šä¸€ä¸ªé€è§†å›¾ Ctrl+Shift+F8 å…¨å±€ ä¸‹ä¸€ä¸ªç¼–è¾‘å™¨ Ctrl+F6 å…¨å±€ ä¸‹ä¸€ä¸ªè§†å›¾ Ctrl+F7 å…¨å±€ ä¸‹ä¸€ä¸ªé€è§†å›¾ Ctrl+F8 æ–‡æœ¬ç¼–è¾‘å™¨ å…³é—­å½“å‰çª—å£ Ctrl+W å…¨å±€ æ˜¾ç¤ºè§†å›¾èœå• Ctrl+F10 å…¨å±€ æ˜¾ç¤ºç³»ç»Ÿèœå• Alt+- 1.4. å¯¼èˆª ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® Javaç¼–è¾‘å™¨ æ‰“å¼€ç»“æ„ Ctrl+F3 å…¨å±€ æ‰“å¼€ç±»å‹ Ctrl+Shift+T å…¨å±€ æ‰“å¼€ç±»å‹å±‚æ¬¡ç»“æ„ F4 å…¨å±€ æ‰“å¼€å£°æ˜ F3 å…¨å±€ æ‰“å¼€å¤–éƒ¨javadoc Shift+F2 å…¨å±€ æ‰“å¼€èµ„æº Ctrl+Shift+R å…¨å±€ åé€€å†å²è®°å½• Alt+â† å…¨å±€ å‰è¿›å†å²è®°å½• Alt+â†’ å…¨å±€ ä¸Šä¸€ä¸ª Ctrl+, å…¨å±€ ä¸‹ä¸€ä¸ª Ctrl+. Javaç¼–è¾‘å™¨ æ˜¾ç¤ºå¤§çº² Ctrl+O å…¨å±€ åœ¨å±‚æ¬¡ç»“æ„ä¸­æ‰“å¼€ç±»å‹ Ctrl+Shift+H å…¨å±€ è½¬è‡³åŒ¹é…çš„æ‹¬å· Ctrl+Shift+P å…¨å±€ è½¬è‡³ä¸Šä¸€ä¸ªç¼–è¾‘ä½ç½® Ctrl+Q Javaç¼–è¾‘å™¨ è½¬è‡³ä¸Šä¸€ä¸ªæˆå‘˜ Ctrl+Shift+â†‘ Javaç¼–è¾‘å™¨ è½¬è‡³ä¸‹ä¸€ä¸ªæˆå‘˜ Ctrl+Shift+â†“ æ–‡æœ¬ç¼–è¾‘å™¨ è½¬è‡³è¡Œ Ctrl+L 2. æœç´¢ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ å‡ºç°åœ¨æ–‡ä»¶ä¸­ Ctrl+Shift+U å…¨å±€ æŸ¥æ‰¾ç›®æ ‡æ–‡ä»¶ ctrl+shift+R å…¨å±€ æ‰“å¼€æœç´¢å¯¹è¯æ¡† Ctrl+H å…¨å±€ å·¥ä½œåŒºä¸­çš„å£°æ˜ Ctrl+G å…¨å±€ å·¥ä½œåŒºä¸­çš„å¼•ç”¨ Ctrl+Shift+G å·¥ä½œåŒºåŸŸçš„ç±» æŸ¥çœ‹æŸä¸€ä¸ªç±»çš„ç»§æ‰¿ç±»æˆ–è€…å®ç°ç±» ctrl+T 3. æ–‡æœ¬ç¼–è¾‘ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® æ–‡æœ¬ç¼–è¾‘å™¨ æ”¹å†™åˆ‡æ¢ Insert æ–‡æœ¬ç¼–è¾‘å™¨ ä¸Šæ»šè¡Œ Ctrl+â†‘ æ–‡æœ¬ç¼–è¾‘å™¨ ä¸‹æ»šè¡Œ Ctrl+â†“ 4. æ–‡ä»¶ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ ä¿å­˜ Ctrl+S å…¨å±€ æ‰“å° Ctrl+P å…¨å±€ å…³é—­ Ctrl+F4 å…¨å±€ å…¨éƒ¨ä¿å­˜ Ctrl+Shift+S å…¨å±€ å…¨éƒ¨å…³é—­ Ctrl+Shift+F4 å…¨å±€ å±æ€§ Alt+Enter å…¨å±€ æ–°å»º Ctrl+N 5. é¡¹ç›® ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ å…¨éƒ¨æ„å»º Ctrl+B 5.1. æºä»£ç  ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® Javaç¼–è¾‘å™¨ æ ¼å¼åŒ– Ctrl+Shift+F Javaç¼–è¾‘å™¨ æ·»åŠ /å–æ¶ˆæ³¨é‡Š Ctrl+/ Javaç¼–è¾‘å™¨ æ·»åŠ å¯¼å…¥ Ctrl+Shift+M Javaç¼–è¾‘å™¨ ç»„ç»‡å¯¼å…¥ Ctrl+Shift+O Javaç¼–è¾‘å™¨ ä½¿ç”¨try/catchå—æ¥åŒ…å›´ æœªè®¾ç½®ï¼Œå¤ªå¸¸ç”¨äº†ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œåˆ—å‡ºï¼Œå»ºè®®è‡ªå·±è®¾ç½®ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨Ctrl+1è‡ªåŠ¨ä¿®æ­£ã€‚Alt+Shift+zï¼ˆå°±å¯ä»¥å§ï¼‰ Javaç¼–è¾‘å™¨ å°†æ‰€é€‰åŒºåŸŸå­—æ¯è®¾ç½®ä¸ºå°å†™ Ctrl+Shift+Y Javaç¼–è¾‘å™¨ å°†æ‰€é€‰åŒºåŸŸå­—æ¯è®¾ç½®ä¸ºå¤§å†™ Ctrl+Shift+X Javaç¼–è¾‘å™¨ æ–¹æ³•æ·»åŠ æ³¨é‡Š Alt+Shift+J 5.2. è¿è¡Œ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ å•æ­¥è¿”å› F7 å…¨å±€ å•æ­¥æ‰§è¡Œ F6 å…¨å±€ å•æ­¥è·³å…¥ F5 å…¨å±€ å•æ­¥è·³å…¥é€‰æ‹© Ctrl+F5 å…¨å±€ è°ƒè¯•ä¸Šæ¬¡å¯åŠ¨ F11 å…¨å±€ ç»§ç»­ F8 å…¨å±€ ä½¿ç”¨è¿‡æ»¤å™¨å•æ­¥æ‰§è¡Œ Shift+F5 å…¨å±€ æ·»åŠ /å»é™¤æ–­ç‚¹ Ctrl+Shift+B å…¨å±€ æ˜¾ç¤º Ctrl+D å…¨å±€ è¿è¡Œä¸Šæ¬¡å¯åŠ¨ Ctrl+F11 å…¨å±€ è¿è¡Œè‡³è¡Œ Ctrl+R å…¨å±€ æ‰§è¡Œ Ctrl+U 5.3. é‡æ„ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ æ’¤é”€é‡æ„ Alt+Shift+Z å…¨å±€ æŠ½å–æ–¹æ³• Alt+Shift+M å…¨å±€ æŠ½å–å±€éƒ¨å˜é‡ Alt+Shift+L å…¨å±€ å†…è” Alt+Shift+I å…¨å±€ ç§»åŠ¨ Alt+Shift+V å…¨å±€ é‡å‘½å Alt+Shift+R å…¨å±€ é‡åš Alt+Shift+Y ","categories":"","description":"","excerpt":"eclipseå¿«æ·é”® 1. å¿«æ·é”® 1.1. ç¼–è¾‘ ä½œç”¨åŸŸ åŠŸèƒ½ å¿«æ·é”® å…¨å±€ æŸ¥æ‰¾å¹¶æ›¿æ¢ Ctrl+F â€¦","ref":"/linux-notes/keymap/eclipse-keymap/","tags":["å¿«æ·é”®"],"title":"eclipseå¿«æ·é”®"},{"body":"1. Gitå¸¸ç”¨å‘½ä»¤ åˆ†ç±» å­ç±» git command zsh alias åˆ†æ”¯ æŸ¥çœ‹å½“å‰åˆ†æ”¯ git branch gb åˆ›å»ºæ–°åˆ†æ”¯,ä»åœç•™åœ¨å½“å‰åˆ†æ”¯ git branch åˆ›å»ºå¹¶åˆ‡æ¢åˆ°æ–°åˆ†æ”¯ git checkout -b gcb åˆ‡æ¢åˆ†æ”¯ git checkout åˆå¹¶åˆ†æ”¯ git checkout #åˆ‡æ¢åˆ°è¦åˆå¹¶çš„åˆ†æ”¯git merge â€“no-ff #åˆå¹¶æŒ‡å®šåˆ†æ”¯åˆ°å½“å‰åˆ†æ”¯ æäº¤ æŸ¥çœ‹çŠ¶æ€ git status gst æŸ¥çœ‹ä¿®æ”¹éƒ¨åˆ† git diff --color gd æ·»åŠ æ–‡ä»¶åˆ°æš‚å­˜åŒº git add --all æäº¤æœ¬åœ°ä»“åº“ git commit -m \"\" æ¨é€åˆ°æŒ‡å®šåˆ†æ”¯ git push -u origin æŸ¥çœ‹æäº¤æ—¥å¿— git log - 2. git rebase å¦‚æœä¿¡æ¯ä¿®æ”¹æ— æ³•ç”Ÿæ•ˆï¼Œè®¾ç½®æ°¸ä¹…ç¯å¢ƒå˜é‡ï¼šexport EDITOR=vim\nå¸®åŠ©ä¿¡æ¯ï¼š\n# Rebase 67da308..6ef692b onto 67da308 (1 command) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out 2.1. åˆå¹¶å¤šä½™æäº¤è®°å½• #ä»¥äº¤äº’çš„æ–¹å¼è¿›è¡Œrebase git rebase -i master #åˆå¹¶å¤šä½™æäº¤è®°å½•ï¼šs, squash = use commit, but meld into previous commit pick 6ef692b FIX: Fix parsing docker image version error s 3df667y FIX: the second push s 3fds95t FIX: the third push ä¿å­˜é€€å‡º # è¿›å…¥ä¿®æ”¹äº¤äº’ç•Œé¢ åˆ é™¤éœ€è¦åˆ é™¤çš„æäº¤è®°å½•ï¼Œä¿å­˜é€€å‡º #æŸ¥çœ‹æäº¤è®°å½•æ˜¯å¦å·²è¢«ä¿®æ”¹ git log #æœ€åå¼ºåˆ¶æäº¤åˆ°åˆ†æ”¯ git commit --force -u origin fix/add-unit-test-for-global-role-revoking 2.2. ä¿®æ”¹æäº¤è®°å½• #ä»¥äº¤äº’çš„æ–¹å¼è¿›è¡Œrebase git rebase -i master #ä¿®æ”¹æäº¤è®°å½•ï¼še, edit = use commit, but stop for amending e 6ef692b FIX: Fix parsing docker image version error e 5ty697u FIX: Fix parsing docker image version error #ä¿å­˜é€€å‡º git commit --amend #ä¿®æ”¹æäº¤è®°å½•å†…å®¹ï¼Œä¿å­˜é€€å‡º git rebase --continue git commit --amend #ä¿®æ”¹ä¸‹ä¸€æ¡æäº¤è®°å½•ï¼Œä¿å­˜é€€å‡º git rebase --continue git status # æŸ¥çœ‹çŠ¶æ€æç¤º #æœ€åå¼ºåˆ¶æäº¤åˆ°åˆ†æ”¯ git commit --force -u origin fix/add-unit-test-for-global-role-revoking #æŸ¥çœ‹æäº¤è®°å½•æ˜¯å¦å·²è¢«ä¿®æ”¹ git log 3. gitè®¾ç½®å¿½ç•¥ç‰¹æ®Šæ–‡ä»¶ 3.1. å¿½ç•¥æ–‡ä»¶çš„åŸåˆ™ å¿½ç•¥æ“ä½œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆçš„æ–‡ä»¶ï¼Œæ¯”å¦‚ç¼©ç•¥å›¾ç­‰ï¼› å¿½ç•¥ç¼–è¯‘ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶ã€å¯æ‰§è¡Œæ–‡ä»¶ç­‰ï¼Œä¹Ÿå°±æ˜¯å¦‚æœä¸€ä¸ªæ–‡ä»¶æ˜¯é€šè¿‡å¦ä¸€ä¸ªæ–‡ä»¶è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œé‚£è‡ªåŠ¨ç”Ÿæˆçš„æ–‡ä»¶å°±æ²¡å¿…è¦æ”¾è¿›ç‰ˆæœ¬åº“ï¼Œæ¯”å¦‚Javaç¼–è¯‘äº§ç”Ÿçš„.classæ–‡ä»¶ï¼› å¿½ç•¥ä½ è‡ªå·±çš„å¸¦æœ‰æ•æ„Ÿä¿¡æ¯çš„é…ç½®æ–‡ä»¶ï¼Œæ¯”å¦‚å­˜æ”¾å£ä»¤çš„é…ç½®æ–‡ä»¶ã€‚ 3.2. è®¾ç½®çš„æ–¹æ³• åœ¨é¡¹ç›®çš„workdir ä¸‹ç¼–è¾‘ .gitignore æ–‡ä»¶ï¼Œæ–‡ä»¶çš„è·¯å¾„å¡«å†™ä¸ºworkdirçš„ç›¸å¯¹è·¯å¾„ã€‚\n.idea/ #IDEçš„é…ç½®æ–‡ä»¶ _build/ server/server #äºŒè¿›åˆ¶æ–‡ä»¶ 3.3. gitignore ä¸ç”Ÿæ•ˆè§£å†³æ–¹æ³• åŸå› æ˜¯.gitignoreåªèƒ½å¿½ç•¥é‚£äº›åŸæ¥æ²¡æœ‰è¢«trackçš„æ–‡ä»¶ï¼Œå¦‚æœæŸäº›æ–‡ä»¶å·²ç»è¢«çº³å…¥äº†ç‰ˆæœ¬ç®¡ç†ä¸­ï¼Œåˆ™ä¿®æ”¹.gitignoreæ˜¯æ— æ•ˆçš„ã€‚é‚£ä¹ˆè§£å†³æ–¹æ³•å°±æ˜¯å…ˆæŠŠæœ¬åœ°ç¼“å­˜åˆ é™¤ï¼ˆæ”¹å˜æˆæœªtrackçŠ¶æ€ï¼‰ï¼Œç„¶åå†æäº¤ï¼š\ngit rm -r --cached . git add . git commit -m 'update .gitignore' 4. Gitåˆ†æ”¯é‡å‘½å å‡è®¾åˆ†æ”¯åç§°ä¸ºoldName æƒ³è¦ä¿®æ”¹ä¸º newName\n1. æœ¬åœ°åˆ†æ”¯é‡å‘½å(è¿˜æ²¡æœ‰æ¨é€åˆ°è¿œç¨‹)\ngit branch -m oldName newName 2. è¿œç¨‹åˆ†æ”¯é‡å‘½å (å·²ç»æ¨é€è¿œç¨‹-å‡è®¾æœ¬åœ°åˆ†æ”¯å’Œè¿œç¨‹å¯¹åº”åˆ†æ”¯åç§°ç›¸åŒ) a. é‡å‘½åè¿œç¨‹åˆ†æ”¯å¯¹åº”çš„æœ¬åœ°åˆ†æ”¯\ngit branch -m oldName newName b. åˆ é™¤è¿œç¨‹åˆ†æ”¯\ngit push --delete origin oldName c. ä¸Šä¼ æ–°å‘½åçš„æœ¬åœ°åˆ†æ”¯\ngit push origin newName d.æŠŠä¿®æ”¹åçš„æœ¬åœ°åˆ†æ”¯ä¸è¿œç¨‹åˆ†æ”¯å…³è”\ngit branch --set-upstream-to origin/newName 5. ä»£ç å†²çª git checkout master git pull git checkout \u003cbranch\u003e git rebase -i master fix conflict git rebase --continue git push --force -u origin \u003cbranch\u003e 6. ä¿®æ”¹å†å²æäº¤çš„ç”¨æˆ·ä¿¡æ¯ 1ã€å…‹éš†å¹¶è¿›å…¥ä½ çš„ä»“åº“\ngit clone --bare https://github.com/user/repo.git cd repo.git 2ã€åˆ›å»ºä»¥ä¸‹è„šæœ¬ï¼Œä¾‹å¦‚å‘½åä¸ºrename.sh\n#!/bin/sh git filter-branch --env-filter ' OLD_EMAIL=\"your-old-email@example.com\" #ä¿®æ”¹å‚æ•°ä¸ºä½ çš„æ—§æäº¤é‚®ç®± CORRECT_NAME=\"Your Correct Name\" #ä¿®æ”¹å‚æ•°ä¸ºä½ æ–°çš„ç”¨æˆ·å CORRECT_EMAIL=\"your-correct-email@example.com\" #ä¿®æ”¹å‚æ•°ä¸ºä½ æ–°çš„é‚®ç®±å if [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_COMMITTER_NAME=\"$CORRECT_NAME\" export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\" fi if [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_AUTHOR_NAME=\"$CORRECT_NAME\" export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\" fi ' --tag-name-filter cat -- --branches --tags 3ã€æ‰§è¡Œè„šæœ¬\nchmod +x rename.sh sh rename.sh 4ã€æŸ¥çœ‹æ–° Git å†å²æœ‰æ²¡æœ‰é”™è¯¯ã€‚\n#å¯ä»¥çœ‹åˆ°æäº¤è®°å½•çš„ç”¨æˆ·ä¿¡æ¯å·²ç»ä¿®æ”¹ä¸ºæ–°çš„ç”¨æˆ·ä¿¡æ¯ git log 5ã€ç¡®è®¤æäº¤å†…å®¹ï¼Œé‡æ–°æäº¤ï¼ˆå¯ä»¥å…ˆæŠŠrename.shç§»é™¤æ‰ï¼‰\ngit push --force --tags origin 'refs/heads/*' 7. æ’¤é”€å·²ç»pushçš„æäº¤ # æœ¬åœ°ä»“åº“å›é€€åˆ°æŸä¸€ç‰ˆæœ¬ git reset -hard \u003ccommit-id\u003e # å¼ºåˆ¶ PUSHï¼Œæ­¤æ—¶è¿œç¨‹åˆ†æ”¯å·²ç»æ¢å¤æˆæŒ‡å®šçš„ commit äº† git push origin master --force ","categories":"","description":"","excerpt":"1. Gitå¸¸ç”¨å‘½ä»¤ åˆ†ç±» å­ç±» git command zsh alias åˆ†æ”¯ æŸ¥çœ‹å½“å‰åˆ†æ”¯ git branch gb åˆ›å»ºæ–°åˆ†æ”¯,ä» â€¦","ref":"/linux-notes/git/git-common-cmd/","tags":["Git"],"title":"Gitå¸¸ç”¨å‘½ä»¤"},{"body":"1. Keepalivedçš„å®‰è£… 1.1. yum installæ–¹å¼ yum install -y keepalived 1.2. å®‰è£…åŒ…ç¼–è¯‘æ–¹å¼ æ›´å¤šå®‰è£…åŒ…å‚è€ƒï¼šhttp://www.keepalived.org/download.html\nwget http://www.keepalived.org/software/keepalived-2.0.7.tar.gz tar zxvf keepalived-2.0.7.tar.gz cd keepalived-2.0.7 ./configure --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc --mandir=/usr/share make \u0026\u0026 make install 2. å¸¸ç”¨é…ç½® keepalivedé…ç½®æ–‡ä»¶è·¯å¾„ï¼š/etc/keepalived/keepalivedã€‚\n2.1. MASTERï¼ˆä¸»æœºé…ç½®ï¼‰ global_defs { router_id proxy-keepalived } vrrp_script check_nginx { script \"/etc/keepalived/scripts/check_nginx.sh\" interval 3 weight 2 } vrrp_instance VI_1 { state BACKUP interface eth2 virtual_router_id 15 priority 100 advert_int 1 authentication { auth_type PASS auth_pass xxx } track_script { check_nginx } virtual_ipaddress { 180.101.115.139 218.98.38.29 } nopreempt notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" } 2.2. BACKUPï¼ˆå¤‡æœºé…ç½®ï¼‰ global_defs { router_id proxy-keepalived } vrrp_script check_nginx { script \"/etc/keepalived/scripts/check_nginx.sh\" interval 3 weight 2 } vrrp_instance VI_1 { state BACKUP interface eth2 virtual_router_id 15 priority 99 advert_int 1 authentication { auth_type PASS auth_pass xxx } track_script { check_nginx } virtual_ipaddress { 180.101.115.139 218.98.38.29 } nopreempt notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" } 3. æ³¨æ„äº‹é¡¹ 1ã€æŒ‡å®šNginxå¥åº·æ£€æµ‹è„šæœ¬ï¼š/etc/keepalived/scripts/check_nginx.sh\n2ã€ä¸»å¤‡é…ç½®å·®åˆ«ä¸»è¦ä¸ºï¼ˆå»ºè®®è¿™ä¹ˆé…ç½®ï¼‰ï¼š\nä»¥ä¸‹ä¸¤ç§æ–¹å¼çš„é…ç½®ï¼Œå½“å…¶ä¸­ä¸€å°æœºå™¨keepalivedæŒ‚æ‰åä¼šè‡ªåŠ¨VIPåˆ‡åˆ°å¦ä¸€å°æœºå™¨ï¼Œå½“æŒ‚æ‰æœºå™¨keepalivedæ¢å¤åä¸ä¼šæŠ¢å VIPï¼Œè¯¥æ–¹å¼å¯ä»¥é¿å…æœºå™¨æ¢å¤å†æ¬¡åˆ‡VIPæ‰€å¸¦æ¥çš„å½±å“ã€‚\nä¸»æœº:(state BACKUP;priority 100)\nå¤‡æœºï¼š(state BACKUP;priority 99)\néæŠ¢å ï¼šnopreempt\næˆ–è€…ï¼š\nä¸»æœº:(state MASTER;priority 100)\nå¤‡æœºï¼š(state BACKUP;priority 100)\né»˜è®¤æŠ¢å \n3ã€æŒ‡å®šVIP\nvirtual_ipaddress { 180.101.115.139 218.98.38.29 } 4ã€å¯ä»¥æŒ‡å®šä¸ºéæŠ¢å ï¼šnopreemptï¼Œå³priorityé«˜ä¸ä¼šæŠ¢å å·²ç»ç»‘å®šVIPçš„æœºå™¨ã€‚\n5ã€åˆ¶å®šç»‘å®šIPçš„ç½‘å¡ï¼š interface eth2\n6ã€å¯ä»¥æŒ‡å®škeepalivedçŠ¶æ€å˜åŒ–é€šçŸ¥\nnotify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" 7ã€virtual_router_id 15å€¼ï¼Œä¸»å¤‡å€¼ä¸€è‡´ï¼Œä½†å»ºè®®ä¸åº”ä¸é›†ç¾¤ä¸­å…¶ä»–Nginxæœºå™¨ä¸Šçš„ç›¸åŒï¼Œå¦‚æœåŒä¸€ä¸ªç½‘æ®µé…ç½®çš„virtual_router_id é‡å¤åˆ™ä¼šæŠ¥é”™ï¼Œé€‰æ‹©ä¸€ä¸ªä¸é‡å¤çš„0~255ä¹‹é—´çš„å€¼ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å·²å­˜åœ¨çš„vridã€‚\ntcpdump -nn -i any net 224.0.0.0/8 4. å¸¸ç”¨è„šæœ¬ 4.1. Nginxå¥åº·æ£€æµ‹è„šæœ¬ åœ¨Nginxé…ç½®ç›®å½•ä¸‹ï¼ˆ/etc/nginx/conf.d/ï¼‰å¢åŠ health.confçš„é…ç½®æ–‡ä»¶,è¯¥é…ç½®æ–‡ä»¶ç”¨äºé…ç½®Nginx healthçš„æ¥å£ã€‚\nserver { listen 80 default_server; server_name localhost; default_type text/html; return 200 'Health'; } Nginxå¥åº·æ£€æµ‹è„šæœ¬ï¼š/etc/keepalived/scripts/check_nginx.sh\n4.1.1. æ£€æŸ¥æ¥å£è°ƒç”¨æ˜¯å¦ä¸º200 #!/bin/sh set -x timeout=30 #æŒ‡å®šé»˜è®¤30ç§’æ²¡è¿”å›200åˆ™ä¸ºéå¥åº·ï¼Œè¯¥å€¼å¯æ ¹æ®å®é™…è°ƒæ•´ if [ -n ${timeout} ];then httpcode=`curl -sL -w %{http_code} -m ${timeout} http://localhost -o /dev/null` else httpcode=`curl -sL -w %{http_code} http://localhost -o /dev/null` fi if [ ${httpcode} -ne 200 ];then echo `date`': nginx is not healthy, return http_code is '${httpcode} \u003e\u003e /etc/keeperalived/keepalived.log killall keepalived exit 1 else exit 0 fi 4.1.2. æ£€æŸ¥Nginxè¿›ç¨‹æ˜¯å¦è¿è¡Œ #!/bin/sh if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then echo \"$(date) nginx pid not found\"\u003e\u003e/etc/keepalived/keepalived.log killall keepalived fi 4.2. KeepalivedçŠ¶æ€é€šçŸ¥è„šæœ¬ #!/bin/bash set -x warn_receiver=$1 ip=$(ifconfig bond0|grep inet |awk '{print $2}') warningInfo=\"${ip}_keepalived_changed_status_to_$1\" warn-report --user admin --key=xxxx --target=${warn_receiver} ${warningInfo} echo $(date) $1 \u003e\u003e /etc/keepalived/status è¯´æ˜ï¼š\nipè·å–æœ¬æœºIPï¼Œæœ¬ä¾‹ä¸­IPè·å–æ˜¯bond0çš„IPï¼Œä¸åŒæœºå™¨ç½‘å¡åç§°ä¸åŒéœ€è¦ä¿®æ”¹ä¸ºå¯¹åº”ç½‘å¡åç§°ã€‚ å‘Šè­¦å·¥å…·æ ¹æ®è‡ªå·±æŒ‡å®šã€‚ ","categories":"","description":"","excerpt":"1. Keepalivedçš„å®‰è£… 1.1. yum installæ–¹å¼ yum install -y keepalived 1.2. å®‰è£…åŒ… â€¦","ref":"/linux-notes/keepalived/install-keepalived/","tags":["Keepalived"],"title":"Keepalivedå®‰è£…ä¸é…ç½®"},{"body":"1. Raftåè®®[åˆ†å¸ƒå¼ä¸€è‡´æ€§ç®—æ³•] raftç®—æ³•ä¸­æ¶‰åŠä¸‰ç§è§’è‰²ï¼Œåˆ†åˆ«æ˜¯ï¼š\nfollower: è·Ÿéšè€… candidate: å€™é€‰è€…ï¼Œé€‰ä¸¾è¿‡ç¨‹ä¸­çš„ä¸­é—´çŠ¶æ€è§’è‰² leader: é¢†å¯¼è€… 2. è¿‡ç¨‹ 2.1. é€‰ä¸¾ æœ‰ä¸¤ä¸ªtimeoutæ¥æ§åˆ¶é€‰ä¸¾ï¼Œç¬¬ä¸€ä¸ªæ˜¯election timeoutï¼Œè¯¥æ—¶é—´æ˜¯èŠ‚ç‚¹ä»followeråˆ°æˆä¸ºcandidateçš„æ—¶é—´ï¼Œè¯¥æ—¶é—´æ˜¯150åˆ°300æ¯«ç§’ä¹‹é—´çš„éšæœºå€¼ã€‚å¦ä¸€ä¸ªæ˜¯heartbeat timeoutã€‚\nå½“æŸä¸ªèŠ‚ç‚¹ç»å†å®Œelection timeoutæˆä¸ºcandidateåï¼Œå¼€å¯æ–°çš„ä¸€ä¸ªé€‰ä¸¾å‘¨æœŸï¼Œä»–å‘å…¶ä»–èŠ‚ç‚¹å‘èµ·æŠ•ç¥¨è¯·æ±‚ï¼ˆRequest Voteï¼‰ï¼Œå¦‚æœæ¥æ”¶åˆ°æ¶ˆæ¯çš„èŠ‚ç‚¹åœ¨è¯¥å‘¨æœŸå†…è¿˜æ²¡æŠ•è¿‡ç¥¨åˆ™ç»™è¿™ä¸ªcandidateæŠ•ç¥¨ï¼Œç„¶åèŠ‚ç‚¹é‡ç½®ä»–çš„election timeoutã€‚ å½“è¯¥candidateè·å¾—å¤§éƒ¨åˆ†çš„é€‰ç¥¨ï¼Œåˆ™å¯ä»¥å½“é€‰ä¸ºleaderã€‚ leaderå°±å¼€å§‹å‘é€append entriesç»™å…¶ä»–followerèŠ‚ç‚¹ï¼Œè¿™ä¸ªæ¶ˆæ¯ä¼šåœ¨å†…éƒ¨æŒ‡å®šçš„heartbeat timeoutæ—¶é—´å†…å‘å‡ºï¼Œfolloweræ”¶åˆ°è¯¥ä¿¡æ¯åˆ™å“åº”ç»™leaderã€‚ è¿™ä¸ªé€‰ä¸¾å‘¨æœŸä¼šç»§ç»­ï¼Œç›´åˆ°æŸä¸ªfolloweræ²¡æœ‰æ”¶åˆ°å¿ƒè·³ï¼Œå¹¶æˆä¸ºcandidateã€‚ å¦‚æœæŸä¸ªé€‰ä¸¾å‘¨æœŸå†…ï¼Œæœ‰ä¸¤ä¸ªcandidateåŒæ—¶è·å¾—ç›¸åŒå¤šçš„é€‰ç¥¨ï¼Œåˆ™ä¼šç­‰å¾…ä¸€ä¸ªæ–°çš„å‘¨æœŸé‡æ–°é€‰ä¸¾ã€‚ 2.2. åŒæ­¥ å½“é€‰ä¸¾è¿‡ç¨‹ç»“æŸï¼Œé€‰å‡ºäº†leaderï¼Œåˆ™leaderéœ€è¦æŠŠæ‰€æœ‰çš„å˜æ›´åŒæ­¥çš„ç³»ç»Ÿä¸­çš„å…¶ä»–èŠ‚ç‚¹ï¼Œè¯¥åŒæ­¥ä¹Ÿæ˜¯é€šè¿‡å‘é€Append Entriesçš„æ¶ˆæ¯çš„æ–¹å¼ã€‚\né¦–å…ˆä¸€ä¸ªå®¢æˆ·ç«¯å‘é€ä¸€ä¸ªæ›´æ–°ç»™leaderï¼Œè¿™ä¸ªæ›´æ–°ä¼šæ·»åŠ åˆ°leaderçš„æ—¥å¿—ä¸­ã€‚ ç„¶åleaderä¼šåœ¨ç»™followerçš„ä¸‹æ¬¡å¿ƒè·³æ¢æµ‹ä¸­å‘é€è¯¥æ›´æ–°ã€‚ ä¸€æ—¦å¤§å¤šæ•°followeræ”¶åˆ°è¿™ä¸ªæ›´æ–°å¹¶è¿”å›ç»™leaderï¼Œleaderæäº¤è¿™ä¸ªæ›´æ–°ï¼Œç„¶åè¿”å›ç»™å®¢æˆ·ç«¯ã€‚ 2.3. ç½‘ç»œåˆ†åŒº å½“å‘ç”Ÿç½‘ç»œåˆ†åŒºçš„æ—¶å€™ï¼Œåœ¨ä¸åŒåˆ†åŒºçš„èŠ‚ç‚¹æ¥æ”¶ä¸åˆ°leaderçš„å¿ƒè·³ï¼Œåˆ™ä¼šå¼€å¯ä¸€è½®é€‰ä¸¾ï¼Œå½¢æˆä¸åŒleaderçš„å¤šä¸ªåˆ†åŒºé›†ç¾¤ã€‚ å½“å®¢æˆ·ç«¯ç»™ä¸åŒleaderçš„å‘é€æ›´æ–°æ¶ˆæ¯æ—¶ï¼Œä¸åŒåˆ†åŒºé›†ç¾¤ä¸­çš„èŠ‚ç‚¹ä¸ªæ•°å°äºåŸå…ˆé›†ç¾¤çš„ä¸€åŠæ—¶ï¼Œæ›´æ–°ä¸ä¼šè¢«æäº¤ï¼Œè€ŒèŠ‚ç‚¹ä¸ªæ•°å¤§äºé›†ç¾¤æ•°ä¸€åŠæ—¶ï¼Œæ›´æ–°ä¼šè¢«æäº¤ã€‚ å½“ç½‘ç»œåˆ†åŒºæ¢å¤åï¼Œè¢«æäº¤çš„æ›´æ–°ä¼šåŒæ­¥åˆ°å…¶ä»–çš„èŠ‚ç‚¹ä¸Šï¼Œå…¶ä»–èŠ‚ç‚¹æœªæäº¤çš„æ—¥å¿—ä¼šè¢«å›æ»šå¹¶åŒ¹é…æ–°leaderçš„æ—¥å¿—ï¼Œä¿è¯å…¨å±€çš„æ•°æ®æ˜¯ä¸€è‡´çš„ã€‚ å‚è€ƒï¼š\nhttp://thesecretlivesofdata.com/raft/ https://raft.github.io/raft.pdf https://raft.github.io/ ","categories":"","description":"","excerpt":"1. Raftåè®®[åˆ†å¸ƒå¼ä¸€è‡´æ€§ç®—æ³•] raftç®—æ³•ä¸­æ¶‰åŠä¸‰ç§è§’è‰²ï¼Œåˆ†åˆ«æ˜¯ï¼š\nfollower: è·Ÿéšè€… candidate: å€™é€‰è€…ï¼Œé€‰ä¸¾ â€¦","ref":"/kubernetes-notes/etcd/raft/","tags":["Etcd"],"title":"Raftç®—æ³•"},{"body":"1. IPåŸºç¡€ TCP/IPçš„å¿ƒè„æ˜¯äº’è”ç½‘å±‚ï¼Œè¿™ä¸€å±‚ä¸»è¦æœ‰IPå’ŒICMPä¸¤ä¸ªåè®®ç»„æˆï¼Œåœ¨OSIå‚è€ƒæ¨¡å‹ä¸­ä¸ºç¬¬ä¸‰å±‚ï¼ˆç½‘ç»œå±‚ï¼‰ã€‚ç½‘ç»œå±‚çš„ä¸»è¦ä½œç”¨æ˜¯å®ç°ç»ˆç«¯èŠ‚ç‚¹ä¹‹é—´çš„é€šä¿¡ï¼ˆç‚¹å¯¹ç‚¹é€šä¿¡ï¼‰ã€‚\n1.1. ç½‘ç»œå±‚ä¸æ•°æ®é“¾è·¯å±‚çš„å…³ç³» 1.2. IPå¯»å€ IPåœ°å€ç”¨äºåœ¨â€œè¿æ¥åˆ°ç½‘ç»œä¸­çš„æ‰€æœ‰ä¸»æœºä¸­è¯†åˆ«å‡ºè¿›è¡Œé€šä¿¡çš„ç›®æ ‡åœ°å€â€ã€‚å› æ­¤TCP/IPé€šä¿¡ä¸­æ‰€æœ‰ä¸»æœºæˆ–è·¯ç”±å™¨å¿…é¡»è®¾å®šè‡ªå·±çš„IPåœ°å€ï¼ˆæ¯å—ç½‘å¡è‡³å°‘é…ç½®ä¸€ä¸ªæˆ–ä»¥ä¸Šçš„IPåœ°å€ï¼‰ã€‚\n1.3. è·¯ç”±æ§åˆ¶ è·¯ç”±æ§åˆ¶æ˜¯æŒ‡å°†åˆ†ç»„æ•°æ®å‘é€åˆ°æœ€ç»ˆç›®æ ‡åœ°å€çš„åŠŸèƒ½ã€‚\nIPæ•°æ®åŒ…ç±»ä¼¼å¿«é€’ä¸­çš„åŒ…è£¹ï¼Œé€è´§è½¦ç±»ä¼¼æ•°æ®é“¾è·¯ï¼ŒåŒ…è£¹ä¾èµ–é€è´§è½¦æ‰¿è½½è½¬è¿ï¼Œè€Œä¸€è¾†é€è´§è½¦åªèƒ½å°†åŒ…è£¹é€åˆ°æŸä¸ªåŒºé—´å†…ï¼Œç”±æ–°çš„å¿«é€’ç‚¹å®‰æ’æ–°çš„é€è´§è½¦æ¥è¿›è¡Œä¸‹ä¸€åŒºé—´çš„è¿è¾“ã€‚\n1.3.1. è·¯ç”±æ§åˆ¶è¡¨ ä¸ºäº†å°†æ•°æ®åŒ…å‘ç»™ç›®æ ‡ä¸»æœºï¼Œæ‰€æœ‰ä¸»æœºéƒ½ç»´æŠ¤ä¸€å¼ è·¯ç”±æ§åˆ¶è¡¨ï¼ˆRouting Tableï¼‰,è¯¥è¡¨è®°å½•IPæ•°æ®åœ¨ä¸‹ä¸€æ­¥åº”è¯¥å‘ç»™å“ªä¸ªè·¯ç”±å™¨ã€‚IPåŒ…æ ¹æ®è¿™ä¸ªè·¯ç”±è¡¨åœ¨å„ä¸ªæ•°æ®é“¾è·¯ä¸Šä¼ è¾“ã€‚\n1.4. æ•°æ®é“¾è·¯çš„æŠ½è±¡åŒ– IPæ˜¯å®ç°å¤šä¸ªæ•°æ®é“¾è·¯ä¹‹é—´é€šä¿¡çš„åè®®ã€‚å¯¹ä¸åŒæ•°æ®é“¾è·¯çš„ç›¸å¼‚ç‰¹æ€§è¿›è¡ŒæŠ½è±¡åŒ–ä¹Ÿæ˜¯IPçš„é‡è¦ä½œç”¨ä¹‹ä¸€ã€‚ä¸åŒæ•°æ®é“¾è·¯æœ€å¤§çš„åŒºåˆ«åœ¨äºå®ƒä»¬å„è‡ªçš„æœ€å¤§ä¼ è¾“å•ä½ï¼ˆMTUï¼‰ä¸åŒï¼Œç±»ä¼¼å¿«é€’åŒ…è£¹æœ‰å„è‡ªçš„å¤§å°é™åˆ¶ã€‚å½“æ•°æ®åŒ…è¿‡å¤§æ—¶ï¼ŒIPè¿›è¡Œåˆ†ç‰‡å¤„ç†ï¼Œå³å°†å¤§çš„IPåŒ…åˆ†æˆå¤šä¸ªè¾ƒå°çš„IPåŒ…ï¼Œå½“åˆ°ç›®æ ‡åœ°å€åå†è¢«ç»„åˆèµ·æ¥ä¼ ç»™ä¸Šä¸€å±‚ã€‚\n1.5. IPæ˜¯é¢å‘æ— è¿æ¥å‹ IPå‘åŒ…ä¹‹å‰ä¸éœ€è¦æå‰ä¸ç›®æ ‡å»ºç«‹è¿æ¥ã€‚é‡‡ç”¨é¢å‘æ— è¿æ¥çš„åŸå› ï¼šä¸ºäº†ç®€åŒ–å’Œæé€Ÿã€‚é¢å‘è¿æ¥å‹éœ€è¦æå‰å»ºç«‹è¿æ¥ä¼šé™ä½å¤„ç†é€Ÿåº¦ã€‚IPåªè´Ÿè´£å°†æ•°æ®å‘ç»™ç›®æ ‡ä¸»æœºï¼Œä½†é€”ä¸­å¯èƒ½ä¼šå‘ç”Ÿä¸¢åŒ…ã€é”™ä½ã€æ•°æ®é‡ç¿»å€ç­‰é—®é¢˜ã€‚TCPåˆ™æ˜¯é¢å‘è¿æ¥çš„åè®®ï¼Œè´Ÿè´£ä¿è¯å¯¹ç«¯ä¸»æœºç¡®å®æ”¶åˆ°æ•°æ®ã€‚\n2. IPåœ°å€ åœ¨TCP/IPé€šä¿¡ä¸­ï¼Œç”¨IPåœ°å€è¯†åˆ«ä¸»æœºå’Œè·¯ç”±å™¨ã€‚\n2.1. IPåœ°å€çš„å®šä¹‰ IPåœ°å€ï¼ˆIPv4åœ°å€ï¼‰ç”±32ä½æ­£æ•´æ•°æ¥è¡¨ç¤ºã€‚IPåœ°å€åœ¨è®¡ç®—æœºå†…éƒ¨ä»¥äºŒè¿›åˆ¶æ–¹å¼è¢«å¤„ç†ï¼Œä½†ä¹ æƒ¯å°†32ä½çš„IPåœ°å€ä»¥8ä½ä¸ºä¸€ç»„ï¼Œåˆ†æˆ4ç»„ï¼Œæ¯ç»„ä»¥â€œ.â€éš”å¼€ï¼Œè½¬æ¢æˆ10è¿›åˆ¶æ¥è¡¨ç¤ºã€‚IPv4åœ°å€ä¸º32ä½ï¼Œæœ€å¤šå…è®¸43äº¿å°è®¡ç®—æœºè¿æ¥ç½‘ç»œã€‚\nå®é™…ä¸Šï¼ŒIPåœ°å€å¹¶éæ ¹æ®ä¸»æœºå°æ•°æ¥åˆ†é…è€Œæ˜¯æ¯ä¸€å°ä¸»æœºä¸Šçš„æ¯ä¸€å—ç½‘å¡éƒ½å¾—è®¾ç½®IPåœ°å€ï¼Œä¸€å—ç½‘å¡å¯ä»¥è®¾ç½®ä¸€ä¸ªæˆ–ä»¥ä¸Šä¸ªIP,è·¯ç”±å™¨é€šå¸¸ä¼šé…ç½®ä¸¤ä¸ªä»¥ä¸Šçš„ç½‘å¡ã€‚\n2.2. IPåœ°å€ç”±ç½‘ç»œå’Œä¸»æœºä¸¤éƒ¨åˆ†æ ‡è¯†ç»„æˆ IPåœ°å€ç”±â€œç½‘ç»œåœ°å€â€å’Œâ€œä¸»æœºåœ°å€â€ä¸¤éƒ¨åˆ†ç»„æˆã€‚\nç½‘ç»œæ ‡è¯†åœ¨æ•°æ®é“¾è·¯çš„æ¯ä¸ªæ®µé…ç½®ä¸åŒçš„å€¼ï¼Œå¿…é¡»ä¿è¯ç›¸äº’è¿æ¥çš„æ¯ä¸ªæ®µçš„åœ°å€ä¸é‡å¤ï¼Œç›¸åŒæ®µå†…è¿æ¥çš„ä¸»æœºå¿…é¡»æœ‰ç›¸åŒçš„ç½‘ç»œåœ°å€ã€‚ä¸»æœºæ ‡è¯†åˆ™ä¸å…è®¸åŒä¸€ä¸ªç½‘æ®µå†…é‡å¤å‡ºç°ã€‚åœ¨æŸä¸€èŒƒå›´å†…ï¼ŒIPåœ°å€éœ€å…·æœ‰å”¯ä¸€æ€§ã€‚\nIPåŒ…è¢«è½¬å‘åˆ°æŸä¸ªè·¯ç”±å™¨æ—¶ï¼Œæ˜¯åˆ©ç”¨ç›®æ ‡IPåœ°å€çš„ç½‘ç»œæ ‡è¯†è¿›è¡Œè·¯ç”±ï¼Œå³ä½¿ä¸çœ‹ä¸»æœºåœ°å€ï¼Œç”±ç½‘ç»œåœ°å€åˆ™å¯åˆ¤æ–­æ˜¯å¦æ˜¯è¯¥ç½‘æ®µå†…çš„ä¸»æœºã€‚\n2.3. IPåœ°å€çš„åˆ†ç±» IPåœ°å€åˆ†ä¸ºAã€Bã€Cã€Då››ç±»ã€‚\nIPåœ°å€ç±»åˆ« åœ°å€å¼€å¤´ ç½‘ç»œåœ°å€ ä¸»æœºåœ°å€ èŒƒå›´ ä¸€ä¸ªç½‘æ®µå†…ä¸»æœºåœ°å€ä¸ªæ•° å¤‡æ³¨ Aç±»åœ°å€ 0 ç¬¬1-8ä½ å24ä½ 0.0.0.0~127.0.0.0 2^24-2=16777214 Bç±»åœ°å€ 10 ç¬¬1-16ä½ å16ä½ 128.0.0.0~191.255.0.0 2^16-2=65534 Cç±»åœ°å€ 110 ç¬¬1-24ä½ å8ä½ 192.0.0.0~239.255.255.0 2^8-2=254 Dç±»åœ°å€ 1110 ç¬¬1-32ä½ æ²¡æœ‰ä¸»æœºåœ°å€ 224.0.0.0~239.255.255.255 å¸¸ç”¨äºå¤šæ’­ æ³¨æ„ï¼šåŒä¸€ä¸ªç½‘æ®µä¸­çš„ä¸»æœºåœ°å€åˆ†é…ï¼Œä¸»æœºåœ°å€å…¨ä¸º0è¡¨ç¤ºå¯¹åº”çš„ç½‘ç»œåœ°å€ï¼Œä¸»æœºåœ°å€å…¨ä¸º1é€šå¸¸ç”¨äºå¹¿æ’­åœ°å€ã€‚å› æ­¤ä¸€ä¸ªç½‘æ®µå†…ä¸»æœºçš„ä¸ªæ•°å»æ‰2ä¸ªï¼ˆä¾‹å¦‚2^8-2=254ï¼‰ã€‚\n2.4. å­ç½‘éšç  ç”¨1è¡¨ç¤ºç½‘ç»œåœ°å€çš„èŒƒå›´ï¼Œç”¨0è¡¨ç¤ºä¸»æœºåœ°å€çš„è®¿é—®ã€‚å› æ­¤Aã€Bã€Cç±»å¯è¡¨ç¤ºä¸º\nIPç±»åˆ« è¡¨ç¤º Aç±» 255.0.0.0 Bç±» 255.255.0.0 Cç±» 255.255.255.0 æŒ‰ç…§ä»¥ä¸Šçš„ç»„åˆæ–¹å¼IPæœ‰ç‚¹æµªè´¹ï¼Œå› æ­¤äº§ç”Ÿå­ç½‘éšç çš„åˆ†ç±»æ–¹æ³•å‡å°‘è¿™ç§æµªè´¹ã€‚\nå¼•å…¥å­ç½‘åï¼ŒIPåœ°å€ç”±ä¸¤ç§è¯†åˆ«ç ç»„æˆï¼šIPåœ°å€æœ¬èº«+è¡¨ç¤ºç½‘ç»œåœ°å€çš„å­ç½‘éšç ã€‚å³å°†A,B,Cç±»ä¸­çš„ä¸»æœºåœ°å€æ‹†æˆç½‘ç»œéƒ¨åˆ†å’Œä¸»æœºéƒ¨åˆ†ï¼Œé‡æ–°åˆ†é…ç½‘ç»œåœ°å€å’Œä¸»æœºåœ°å€ã€‚å­ç½‘éšç åŒæ ·æ˜¯ç”¨1è¡¨ç¤ºç½‘ç»œåœ°å€çš„èŒƒå›´ï¼Œç”¨0è¡¨ç¤ºä¸»æœºåœ°å€çš„è®¿é—®ã€‚\n2.4.1. å­ç½‘éšç çš„è¡¨ç¤ºæ–¹æ³• è¡¨ç¤ºæ–¹æ³• åœ°å€ å­ç½‘éšç  å¤‡æ³¨ æ•°å­— IPåœ°å€ 172.20.100.52 255.255.255.192 ç½‘ç»œåœ°å€ 172.20.100.0 255.255.255.192 å¹¿æ’­åœ°å€ 172.20.100.63 255.255.255.192 â€œ/26â€ï¼Œè¡¨ç¤ºå‰26ä½ä¸ºç½‘ç»œåœ°å€ IPåœ°å€ 172.20.100.52/26 ç½‘ç»œåœ°å€ 172.20.100.0/26 å¹¿æ’­åœ°å€ 172.20.100.63/26 3. è·¯ç”±æ§åˆ¶ å‘é€æ•°æ®åŒ…é™¤äº†æœ‰ç›®æ ‡IPåœ°å€å¤–ï¼Œè¿˜éœ€è¦æŒ‡æ˜è·¯ç”±å™¨å’Œä¸»æœºçš„ä¿¡æ¯ï¼Œå³è·¯ç”±æ§åˆ¶è¡¨ã€‚\nè·¯ç”±æ§åˆ¶è¡¨çš„å½¢æˆæ–¹å¼æœ‰ä¸¤ç§ï¼š\n1ã€é™æ€è·¯ç”±\nç”±ç®¡ç†å‘˜æ‰‹åŠ¨è®¾ç½®\n2ã€åŠ¨æ€è·¯ç”±\nè·¯ç”±å™¨ä¸å…¶ä»–è·¯ç”±å™¨ç›¸äº’äº¤äº’ä¿¡æ¯æ—¶è‡ªåŠ¨åˆ·æ–°ã€‚ä¸ºäº†è®©åŠ¨æ€è·¯ç”±åŠæ—¶åˆ·æ–°è·¯ç”±è¡¨ï¼Œåœ¨ç½‘ç»œä¸Šäº’è”çš„è·¯ç”±å™¨ä¹‹é—´éœ€è®¾ç½®è·¯ç”±åè®®ï¼Œä¿è¯æ­£å¸¸è¯»å–è·¯ç”±æ§åˆ¶ä¿¡æ¯ã€‚\n3.1. IPåœ°å€å’Œè·¯ç”±æ§åˆ¶ IPåœ°å€çš„ç½‘ç»œåœ°å€éƒ¨åˆ†ç”¨äºè¿›è¡Œè·¯ç”±æ§åˆ¶ã€‚è·¯ç”±æ§åˆ¶è¡¨ä¸­è®°å½•ç€ç½‘ç»œåœ°å€ä¸ä¸‹ä¸€æ­¥åº”è¯¥å‘é€è‡³è·¯ç”±å™¨çš„åœ°å€ã€‚åœ¨å‘é€IPåŒ…æ—¶ï¼Œé¦–å…ˆç¡®è®¤IPåŒ…é¦–éƒ¨ä¸­çš„ç›®æ ‡åœ°å€ï¼Œå†ä»è·¯ç”±æ§åˆ¶è¡¨ä¸­æ‰¾åˆ°ä¸è¯¥åœ°å€å…·æœ‰ç›¸åŒç½‘ç»œåœ°å€çš„è®°å½•ï¼Œæ ¹æ®è¯¥è®°å½•å°†IPåŒ…è½¬å‘ç»™ç›¸åº”çš„ä¸‹ä¸€ä¸ªè·¯ç”±å™¨ã€‚å¦‚æœå­˜åœ¨å¤šæ¡ç›¸åŒç½‘ç»œåœ°å€çš„è®°å½•ï¼Œåˆ™é€‰æ‹©æœ€ä¸ºå»åˆçš„ç½‘ç»œåœ°å€ï¼ˆç›¸åŒä½æ•°æœ€å¤šï¼‰ã€‚ä¾‹å¦‚ï¼š172.20.100.52çš„ç½‘ç»œåœ°å€ä¸172.20.0/16å’Œ172.20.100.0/24éƒ½åŒ¹é…ï¼Œåˆ™é€‰æ‹©åŒ¹é…æœ€é•¿çš„172.20.100.0/24ã€‚\n3.1.1. é»˜è®¤è·¯ç”± é»˜è®¤è·¯ç”±ä¸€èˆ¬æ ‡è®°ä¸º0.0.0.0/0æˆ–defaultã€‚å½“è·¯ç”±è¡¨ä¸­æ²¡æœ‰ä»»ä½•ä¸€ä¸ªåœ°å€ä¸ä¹‹åŒ¹é…çš„è®°å½•ï¼Œåˆ™ä½¿ç”¨é»˜è®¤è·¯ç”±ã€‚\n3.1.2. ä¸»æœºè·¯ç”± â€œIPåœ°å€/32â€ä¹Ÿè¢«ç§°ä¸ºä¸»æœºè·¯ç”±ï¼Œå³æ•´ä¸ªIPåœ°å€çš„æ‰€æœ‰ä½éƒ½å‚ä¸è·¯ç”±ã€‚è¿›è¡Œä¸»æœºè·¯ç”±æ„å‘³ç€åŸºäºä¸»æœºä¸Šç½‘å¡é…ç½®çš„IPåœ°å€æœ¬èº«è€Œä¸æ˜¯åŸºäºè¯¥åœ°å€çš„ç½‘ç»œåœ°å€è¿›è¡Œè·¯ç”±ã€‚ä¸€èˆ¬ç”¨äºä¸å¸Œæœ›é€šè¿‡ç½‘ç»œåœ°å€è·¯ç”±çš„æƒ…å†µã€‚ä½¿ç”¨ä¸»æœºè·¯ç”±ä¼šå¯¼è‡´è·¯ç”±è¡¨è†¨å¤§ï¼Œè·¯ç”±è´Ÿè·å¢åŠ ï¼Œç½‘ç»œæ€§èƒ½ä¸‹é™ã€‚\n3.1.3. ç¯å›åœ°å€ ç¯å›åœ°å€æ˜¯åœ¨åŒä¸€å°è®¡ç®—æœºä¸Šç¨‹åºä¹‹é—´è¿›è¡Œç½‘ç»œé€šä¿¡æ—¶æ‰€ä½¿ç”¨çš„ä¸€ä¸ªé»˜è®¤åœ°å€ã€‚å³IPåœ°å€ä¸º127.0.0.1ï¼Œä¸»æœºåä¸ºlocalhostã€‚\n3.2. è·¯ç”±æ§åˆ¶è¡¨çš„èšåˆ è·¯ç”±ä¿¡æ¯çš„èšåˆå¯ä»¥æœ‰æ•ˆçš„å‡å°‘è·¯ç”±è¡¨çš„æ¡ç›®ã€‚è·¯ç”±è¡¨è¶Šå¤§ï¼Œç®¡ç†å®ƒæ‰€éœ€è¦çš„å†…å­˜å’ŒCPUå°±è¶Šå¤šï¼ŒæŸ¥æ‰¾è·¯ç”±è¡¨çš„æ—¶é—´è¶Šé•¿ï¼Œå¯¼è‡´è½¬å‘IPåŒ…æ€§èƒ½ä¸‹é™ã€‚è¦æ„å»ºé«˜æ€§èƒ½ç½‘ç»œå°±éœ€è¦å°½å¯èƒ½å‡å°‘è·¯ç”±è¡¨çš„å¤§å°ã€‚\n4. IPé¦–éƒ¨ä¿¡æ¯ IPè¿›è¡Œé€šä¿¡æ—¶ï¼Œéœ€è¦åœ¨æ•°æ®å‰é¢åŠ å…¥IPé¦–éƒ¨ä¿¡æ¯ï¼ŒIPé¦–éƒ¨åŒ…å«ç€ç”¨äºIPåè®®è¿›è¡Œå‘åŒ…æ§åˆ¶æ—¶æ‰€æœ‰çš„å¿…è¦ä¿¡æ¯ã€‚\n4.1. IPv4é¦–éƒ¨ å­—æ®µ è¯´æ˜ å¤§å° ç‰ˆæœ¬ æ ‡è¯†IPé¦–éƒ¨çš„ç‰ˆæœ¬å·ï¼ŒIPv4ï¼Œå³ç‰ˆæœ¬å·ä¸º4 4æ¯”ç‰¹ é¦–éƒ¨é•¿åº¦ è¡¨ç¤ºIPé¦–éƒ¨çš„å¤§å°ï¼Œå•ä½ä¸º4å­—èŠ‚ 4æ¯”ç‰¹ åŒºåˆ†æœåŠ¡ è¡¨ç¤ºæœåŠ¡è´¨é‡ 8æ¯”ç‰¹ DSCPæ®µä¸ECNæ®µ DSCPç”¨æ¥è¿›è¡Œè´¨é‡æ§åˆ¶ï¼Œå€¼è¶Šå¤§ä¼˜å…ˆåº¦è¶Šé«˜ï¼›ECNç”¨æ¥æŠ¥å‘Šç½‘ç»œæ‹¥å µæƒ…å†µ 2æ¯”ç‰¹ æ€»é•¿åº¦ è¡¨ç¤ºIPé¦–éƒ¨ä¸æ•°æ®éƒ¨åˆ†åˆèµ·æ¥çš„æ€»å­—èŠ‚æ•° 16æ¯”ç‰¹ æ ‡è¯† ç”¨äºåˆ†ç‰‡é‡ç»„ï¼ŒåŒä¸€ä¸ªåˆ†ç‰‡æ ‡è¯†å€¼ç›¸åŒï¼Œä¸åŒåˆ†ç‰‡çš„æ ‡è¯†å€¼ä¸åŒ 16æ¯”ç‰¹ æ ‡å¿— è¡¨ç¤ºåŒ…è¢«åˆ†ç‰‡çš„ç›¸å…³ä¿¡æ¯ 3æ¯”ç‰¹ ç‰‡åç§» ç”¨æ¥æ ‡è¯†è¢«åˆ†ç‰‡çš„æ¯ä¸€ä¸ªåˆ†æ®µç›¸å¯¹äºåŸå§‹æ•°æ®çš„ä½ç½®ã€‚ 13æ¯”ç‰¹ ç”Ÿå­˜æ—¶é—´ï¼ˆTTLï¼‰ æœ¬æ„ä¸ºåŒ…çš„ç”Ÿå­˜æœŸé™ï¼Œä¸€èˆ¬è¡¨ç¤ºå¯ä»¥ä¸­è½¬å¤šå°‘ä¸ªè·¯ç”±å™¨ï¼Œæ¯ç»è¿‡ä¸€ä¸ªè·¯ç”±å™¨TTLå‡1ï¼Œç›´åˆ°å˜ä¸º0åˆ™ä¸¢å¼ƒè¯¥åŒ… 8æ¯”ç‰¹ åè®® è¡¨ç¤ºIPé¦–éƒ¨çš„ä¸‹ä¸€ä¸ªé¦–éƒ¨éš¶å±äºå“ªä¸ªåè®®ã€‚ 8æ¯”ç‰¹ é¦–éƒ¨æ ¡éªŒå’Œ IPé¦–éƒ¨æ ¡éªŒå’Œï¼Œç”¨æ¥ç¡®ä¿IPæ•°æ®æŠ¥ä¸è¢«ç ´åã€‚ 16æ¯”ç‰¹ æºåœ°å€ å‘é€ç«¯IPåœ°å€ 32æ¯”ç‰¹ ç›®æ ‡åœ°å€ æ¥æ”¶ç«¯IPåœ°å€ 32æ¯”ç‰¹ å¯é€‰é¡¹ å®‰å…¨çº§åˆ«ã€æºè·¯å¾„ã€è·¯å¾„è®°å½•ã€æ—¶é—´æˆ³ å¡«å…… å¡«è¡¥ç‰©ï¼Œè°ƒæ•´å¤§å°ä½¿ç”¨ æ•°æ® å­˜å…¥æ•°æ® 4.2. IPv6é¦–éƒ¨ å­—æ®µ è¯´æ˜ ç‰ˆæœ¬ IPv6,ç‰ˆæœ¬ä¸º6 é€šä¿¡é‡ç±» ç›¸å½“äºIPv4çš„TOSï¼ˆType Of Serviceï¼‰å­—æ®µ æµæ ‡å· ç”¨äºæœåŠ¡è´¨é‡æ§åˆ¶ æœ‰æ•ˆè½½è·é•¿åº¦ åŒ…çš„æ•°æ®éƒ¨åˆ† ä¸‹ä¸€ä¸ªé¦–éƒ¨ ç›¸å½“äºIPv4çš„åè®®å­—æ®µ è·³æ•°é™åˆ¶ Hop Limitï¼ŒåŒIPv4çš„TTL,è¡¨ç¤ºå¯é€šè¿‡çš„è·¯ç”±å™¨ä¸ªæ•° æºåœ°å€ å‘é€ç«¯çš„IPåœ°å€ ç›®æ ‡åœ°å€ æ¥æ”¶ç«¯çš„IPåœ°å€ å‚è€ƒ\nã€Šå›¾è§£TCP/IPã€‹ ","categories":"","description":"","excerpt":"1. IPåŸºç¡€ TCP/IPçš„å¿ƒè„æ˜¯äº’è”ç½‘å±‚ï¼Œè¿™ä¸€å±‚ä¸»è¦æœ‰IPå’ŒICMPä¸¤ä¸ªåè®®ç»„æˆï¼Œåœ¨OSIå‚è€ƒæ¨¡å‹ä¸­ä¸ºç¬¬ä¸‰å±‚ï¼ˆç½‘ç»œå±‚ï¼‰ã€‚ç½‘ç»œå±‚çš„ä¸»è¦ä½œç”¨ â€¦","ref":"/linux-notes/tcpip/ip/","tags":["TCPIP"],"title":"IPåè®®"},{"body":"1. Memcached å‘½ä»¤ 1.1. å­˜å‚¨å‘½ä»¤ 1.1.1. å¸¸ç”¨å‘½ä»¤ å‘½ä»¤ è¯´æ˜ set æ–°å¢æˆ–æ›´æ–° add æ–°å¢ replace æ›¿æ¢ append åœ¨åé¢è¿½åŠ  prepend åœ¨å‰é¢è¿½åŠ  cas æ£€æŸ¥å¹¶è®¾ç½® ä»¥ä¸Šå‡ ä¸ªå‘½ä»¤è¯­æ³•æ ¼å¼ç›¸ä¼¼ï¼Œä»¥setä¸ºä¾‹ï¼š\nset key flags exptime bytes [noreply] value å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\n**keyï¼š**é”®å€¼ key-value ç»“æ„ä¸­çš„ keyï¼Œç”¨äºæŸ¥æ‰¾ç¼“å­˜å€¼ã€‚ flagsï¼šå¯ä»¥åŒ…æ‹¬é”®å€¼å¯¹çš„æ•´å‹å‚æ•°ï¼Œå®¢æˆ·æœºä½¿ç”¨å®ƒå­˜å‚¨å…³äºé”®å€¼å¯¹çš„é¢å¤–ä¿¡æ¯ ã€‚ exptimeï¼šåœ¨ç¼“å­˜ä¸­ä¿å­˜é”®å€¼å¯¹çš„æ—¶é—´é•¿åº¦ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼Œ0 è¡¨ç¤ºæ°¸è¿œï¼‰ bytesï¼šåœ¨ç¼“å­˜ä¸­å­˜å‚¨çš„å­—èŠ‚æ•° noreplyï¼ˆå¯é€‰ï¼‰ï¼š è¯¥å‚æ•°å‘ŠçŸ¥æœåŠ¡å™¨ä¸éœ€è¦è¿”å›æ•°æ® valueï¼šå­˜å‚¨çš„å€¼ï¼ˆå§‹ç»ˆä½äºç¬¬äºŒè¡Œï¼‰ï¼ˆå¯ç›´æ¥ç†è§£ä¸ºkey-valueç»“æ„ä¸­çš„valueï¼‰ å®ä¾‹ï¼š\nkey â†’ runoob flag â†’ 0 exptime â†’ 900 (ä»¥ç§’ä¸ºå•ä½) bytes â†’ 9 (æ•°æ®å­˜å‚¨çš„å­—èŠ‚æ•°) value â†’ memcached set runoob 0 900 9 memcached STORED get runoob VALUE runoob 0 9 memcached END è¾“å‡ºï¼š\nå¦‚æœæ•°æ®è®¾ç½®æˆåŠŸï¼Œåˆ™è¾“å‡ºï¼š\nSTORED è¾“å‡ºä¿¡æ¯è¯´æ˜ï¼š\nSTOREDï¼šä¿å­˜æˆåŠŸåè¾“å‡ºã€‚ ERRORï¼šåœ¨ä¿å­˜å¤±è´¥åè¾“å‡ºã€‚ 1.1.2. caså‘½ä»¤ Memcached CASï¼ˆCheck-And-Set æˆ– Compare-And-Swapï¼‰ å‘½ä»¤ç”¨äºæ‰§è¡Œä¸€ä¸ª\"æ£€æŸ¥å¹¶è®¾ç½®\"çš„æ“ä½œã€‚\nå®ƒä»…åœ¨å½“å‰å®¢æˆ·ç«¯æœ€åä¸€æ¬¡å–å€¼åï¼Œè¯¥key å¯¹åº”çš„å€¼æ²¡æœ‰è¢«å…¶ä»–å®¢æˆ·ç«¯ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œ æ‰èƒ½å¤Ÿå°†å€¼å†™å…¥ã€‚\næ£€æŸ¥æ˜¯é€šè¿‡cas_tokenå‚æ•°è¿›è¡Œçš„ï¼Œ è¿™ä¸ªå‚æ•°æ˜¯MemcachæŒ‡å®šç»™å·²ç»å­˜åœ¨çš„å…ƒç´ çš„ä¸€ä¸ªå”¯ä¸€çš„64ä½å€¼ã€‚\nè¯­æ³•ï¼š\næ¯”ä»¥ä¸Šå‘½ä»¤å¤šäº†ä¸€ä¸ªunique_cas_token\ncas key flags exptime bytes unique_cas_token [noreply] value å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\nkeyï¼šé”®å€¼ key-value ç»“æ„ä¸­çš„ keyï¼Œç”¨äºæŸ¥æ‰¾ç¼“å­˜å€¼ã€‚ flagsï¼šå¯ä»¥åŒ…æ‹¬é”®å€¼å¯¹çš„æ•´å‹å‚æ•°ï¼Œå®¢æˆ·æœºä½¿ç”¨å®ƒå­˜å‚¨å…³äºé”®å€¼å¯¹çš„é¢å¤–ä¿¡æ¯ ã€‚ exptimeï¼šåœ¨ç¼“å­˜ä¸­ä¿å­˜é”®å€¼å¯¹çš„æ—¶é—´é•¿åº¦ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼Œ0 è¡¨ç¤ºæ°¸è¿œï¼‰ bytesï¼šåœ¨ç¼“å­˜ä¸­å­˜å‚¨çš„å­—èŠ‚æ•° unique_cas_tokené€šè¿‡ gets å‘½ä»¤è·å–çš„ä¸€ä¸ªå”¯ä¸€çš„64ä½å€¼ã€‚ noreplyï¼ˆå¯é€‰ï¼‰ï¼š è¯¥å‚æ•°å‘ŠçŸ¥æœåŠ¡å™¨ä¸éœ€è¦è¿”å›æ•°æ® valueï¼šå­˜å‚¨çš„å€¼ï¼ˆå§‹ç»ˆä½äºç¬¬äºŒè¡Œï¼‰ï¼ˆå¯ç›´æ¥ç†è§£ä¸ºkey-valueç»“æ„ä¸­çš„valueï¼‰ unique_cas_tokené€šè¿‡getså‘½ä»¤è·å–ã€‚\n1.2. æŸ¥æ‰¾å‘½ä»¤ å‘½ä»¤ è¯´æ˜ get è·å–ä¸€ä¸ªæˆ–å¤šä¸ªkey gets è·å–ä¸€ä¸ªæˆ–å¤šä¸ªcas token delete åˆ é™¤å·²å­˜åœ¨çš„key incr/decr å¯¹å·²å­˜åœ¨çš„ key(é”®) çš„æ•°å­—å€¼è¿›è¡Œè‡ªå¢æˆ–è‡ªå‡æ“ä½œ 1.3. ç»Ÿè®¡å‘½ä»¤ å‘½ä»¤ è¯´æ˜ stats ç”¨äºè¿”å›ç»Ÿè®¡ä¿¡æ¯ä¾‹å¦‚ PID(è¿›ç¨‹å·)ã€ç‰ˆæœ¬å·ã€è¿æ¥æ•°ç­‰ã€‚ stats items ç”¨äºæ˜¾ç¤ºå„ä¸ª slab ä¸­ item çš„æ•°ç›®å’Œå­˜å‚¨æ—¶é•¿(æœ€åä¸€æ¬¡è®¿é—®è·ç¦»ç°åœ¨çš„ç§’æ•°)ã€‚ stats slabs ç”¨äºæ˜¾ç¤ºå„ä¸ªslabçš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬chunkçš„å¤§å°ã€æ•°ç›®ã€ä½¿ç”¨æƒ…å†µç­‰ã€‚ stats sizes ç”¨äºæ˜¾ç¤ºæ‰€æœ‰itemçš„å¤§å°å’Œä¸ªæ•°ã€‚ flush_all ç”¨äºæ¸…ç†ç¼“å­˜ä¸­çš„æ‰€æœ‰ key=\u003evalue(é”®=\u003eå€¼) å¯¹ã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttp://www.runoob.com/memcached/memcached-tutorial.html ","categories":"","description":"","excerpt":"1. Memcached å‘½ä»¤ 1.1. å­˜å‚¨å‘½ä»¤ 1.1.1. å¸¸ç”¨å‘½ä»¤ å‘½ä»¤ è¯´æ˜ set æ–°å¢æˆ–æ›´æ–° add æ–°å¢ replace æ›¿ â€¦","ref":"/linux-notes/memcached/memcached-cmd/","tags":["Memcached"],"title":"Memcachedå‘½ä»¤"},{"body":"1. kuberneteså¯¹è±¡æ¦‚è¿° kubernetesä¸­çš„å¯¹è±¡æ˜¯ä¸€äº›æŒä¹…åŒ–çš„å®ä½“ï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯å¯¹é›†ç¾¤çŠ¶æ€çš„æè¿°æˆ–æœŸæœ›ã€‚\nåŒ…æ‹¬ï¼š\né›†ç¾¤ä¸­å“ªäº›nodeä¸Šè¿è¡Œäº†å“ªäº›å®¹å™¨åŒ–åº”ç”¨ åº”ç”¨çš„èµ„æºæ˜¯å¦æ»¡è¶³ä½¿ç”¨ åº”ç”¨çš„æ‰§è¡Œç­–ç•¥ï¼Œä¾‹å¦‚é‡å¯ç­–ç•¥ã€æ›´æ–°ç­–ç•¥ã€å®¹é”™ç­–ç•¥ç­‰ã€‚ kubernetesçš„å¯¹è±¡æ˜¯ä¸€ç§æ„å›¾ï¼ˆæœŸæœ›ï¼‰çš„è®°å½•ï¼Œkubernetesä¼šå§‹ç»ˆä¿æŒé¢„æœŸåˆ›å»ºçš„å¯¹è±¡å­˜åœ¨å’Œé›†ç¾¤è¿è¡Œåœ¨é¢„æœŸçš„çŠ¶æ€ä¸‹ã€‚\næ“ä½œkuberneteså¯¹è±¡ï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰éœ€è¦é€šè¿‡kubernetes APIï¼Œä¸€èˆ¬æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼ï¼š\nkubectlå‘½ä»¤å·¥å…· Client libraryçš„æ–¹å¼ï¼Œä¾‹å¦‚ client-go 2. Spec and Status æ¯ä¸ªkuberneteså¯¹è±¡çš„ç»“æ„æè¿°éƒ½åŒ…å«specå’Œstatusä¸¤ä¸ªéƒ¨åˆ†ã€‚\nspecï¼šè¯¥å†…å®¹ç”±ç”¨æˆ·æä¾›ï¼Œæè¿°ç”¨æˆ·æœŸæœ›çš„å¯¹è±¡ç‰¹å¾åŠé›†ç¾¤çŠ¶æ€ã€‚ statusï¼šè¯¥å†…å®¹ç”±kubernetesé›†ç¾¤æä¾›å’Œæ›´æ–°ï¼Œæè¿°kuberneteså¯¹è±¡çš„å®æ—¶çŠ¶æ€ã€‚ ä»»ä½•æ—¶å€™ï¼Œkuberneteséƒ½ä¼šæ§åˆ¶é›†ç¾¤çš„å®æ—¶çŠ¶æ€statusä¸ç”¨æˆ·çš„é¢„æœŸçŠ¶æ€specä¸€è‡´ã€‚\nä¾‹å¦‚ï¼šå½“ä½ å®šä¹‰Deploymentçš„æè¿°æ–‡ä»¶ï¼ŒæŒ‡å®šé›†ç¾¤ä¸­è¿è¡Œ3ä¸ªå®ä¾‹ï¼Œé‚£ä¹ˆkubernetesä¼šå§‹ç»ˆä¿æŒé›†ç¾¤ä¸­è¿è¡Œ3ä¸ªå®ä¾‹ï¼Œå¦‚æœä»»ä½•å®ä¾‹æŒ‚æ‰ï¼Œkubernetesä¼šè‡ªåŠ¨é‡å»ºæ–°çš„å®ä¾‹æ¥ä¿æŒé›†ç¾¤ä¸­å§‹ç»ˆè¿è¡Œç”¨æˆ·é¢„æœŸçš„3ä¸ªå®ä¾‹ã€‚\n3. å¯¹è±¡æè¿°æ–‡ä»¶ å½“ä½ è¦åˆ›å»ºä¸€ä¸ªkuberneteså¯¹è±¡çš„æ—¶å€™ï¼Œéœ€è¦æä¾›è¯¥å¯¹è±¡çš„æè¿°ä¿¡æ¯specï¼Œæ¥æè¿°ä½ çš„å¯¹è±¡åœ¨kubernetesä¸­çš„é¢„æœŸçŠ¶æ€ã€‚\nä¸€èˆ¬ä½¿ç”¨kubernetes APIæ¥åˆ›å»ºkuberneteså¯¹è±¡ï¼Œå…¶ä¸­specä¿¡æ¯å¯ä»¥ä»¥JSONçš„å½¢å¼å­˜æ”¾åœ¨request bodyä¸­ï¼Œä¹Ÿå¯ä»¥ä»¥.yamlæ–‡ä»¶çš„å½¢å¼é€šè¿‡kubectlå·¥å…·åˆ›å»ºã€‚\nä¾‹å¦‚ï¼Œä»¥ä¸‹ä¸ºDeploymentå¯¹è±¡å¯¹åº”çš„yamlæ–‡ä»¶ï¼š\napiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 æ‰§è¡Œkubectl createçš„å‘½ä»¤\n#create command kubectl create -f https://k8s.io/docs/user-guide/nginx-deployment.yaml --record #output deployment \"nginx-deployment\" created 4. å¿…é¡»å­—æ®µ åœ¨å¯¹è±¡æè¿°æ–‡ä»¶.yamlä¸­ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µã€‚\napiVersionï¼škubernetes APIçš„ç‰ˆæœ¬ kindï¼škuberneteså¯¹è±¡çš„ç±»å‹ metadataï¼šå”¯ä¸€æ ‡è¯†è¯¥å¯¹è±¡çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬nameï¼ŒUIDï¼Œå¯é€‰çš„namespace specï¼šæ ‡è¯†å¯¹è±¡çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸åŒå¯¹è±¡çš„specçš„æ ¼å¼ä¸åŒï¼Œå¯ä»¥åµŒå¥—å…¶ä»–å¯¹è±¡çš„å­—æ®µã€‚ æ–‡ç« å‚è€ƒï¼š\nhttps://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/\n","categories":"","description":"","excerpt":"1. kuberneteså¯¹è±¡æ¦‚è¿° kubernetesä¸­çš„å¯¹è±¡æ˜¯ä¸€äº›æŒä¹…åŒ–çš„å®ä½“ï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯å¯¹é›†ç¾¤çŠ¶æ€çš„æè¿°æˆ–æœŸæœ›ã€‚\nåŒ…æ‹¬ï¼š\né›†ç¾¤ä¸­å“ª â€¦","ref":"/kubernetes-notes/concepts/object/understanding-kubernetes-objects/","tags":["Kubernetes"],"title":"ç†è§£kuberneteså¯¹è±¡"},{"body":"[ç¼–è€…çš„è¯]\nç›®å‰å¾ˆå¤šçš„å®¹å™¨äº‘å¹³å°é€šè¿‡DockeråŠKubernetesç­‰æŠ€æœ¯æä¾›åº”ç”¨è¿è¡Œå¹³å°ï¼Œä»è€Œå®ç°è¿ç»´è‡ªåŠ¨åŒ–ï¼Œå¿«é€Ÿéƒ¨ç½²åº”ç”¨ã€å¼¹æ€§ä¼¸ç¼©å’ŒåŠ¨æ€è°ƒæ•´åº”ç”¨ç¯å¢ƒèµ„æºï¼Œæé«˜ç ”å‘è¿è¥æ•ˆç‡ã€‚\nä»å®è§‚åˆ°å¾®è§‚ï¼ˆä»æŠ½è±¡åˆ°å…·ä½“ï¼‰çš„æ€è·¯æ¥ç†è§£ï¼šäº‘è®¡ç®—â†’PaaSâ†’ App Engineâ†’XAE[XXX App Engine] ï¼ˆXAEæ³›æŒ‡ä¸€ç±»åº”ç”¨è¿è¡Œå¹³å°ï¼Œä¾‹å¦‚GAEã€SAEã€BAEç­‰ï¼‰ã€‚\næœ¬æ–‡ç®€è¦ä»‹ç»äº†ä¸å®¹å™¨äº‘ç›¸å…³çš„å‡ ä¸ªé‡è¦æ¦‚å¿µï¼šPaaSã€App Engineã€Dokcerã€Kubernetesã€‚\n1. PaaSæ¦‚è¿° 1.1. PaaSæ¦‚å¿µ PaaS(Platform as a service)ï¼Œå¹³å°å³æœåŠ¡ï¼ŒæŒ‡å°†è½¯ä»¶ç ”å‘çš„å¹³å°ï¼ˆæˆ–ä¸šåŠ¡åŸºç¡€å¹³å°ï¼‰ä½œä¸ºä¸€ç§æœåŠ¡ï¼Œä»¥SaaSçš„æ¨¡å¼æäº¤ç»™ç”¨æˆ·ã€‚ PaaSæ˜¯äº‘è®¡ç®—æœåŠ¡çš„å…¶ä¸­ä¸€ç§æ¨¡å¼ï¼Œäº‘è®¡ç®—æ˜¯ä¸€ç§æŒ‰ä½¿ç”¨é‡ä»˜è´¹çš„æ¨¡å¼çš„æœåŠ¡ï¼Œç±»ä¼¼ä¸€ç§ç§ŸèµæœåŠ¡ï¼ŒæœåŠ¡å¯ä»¥æ˜¯åŸºç¡€è®¾æ–½è®¡ç®—èµ„æºï¼ˆIaaSï¼‰ï¼Œå¹³å°ï¼ˆPaaSï¼‰ï¼Œè½¯ä»¶ï¼ˆSaaSï¼‰ã€‚ç§Ÿç”¨ITèµ„æºçš„æ–¹å¼æ¥å®ç°ä¸šåŠ¡éœ€è¦ï¼Œå¦‚åŒæ°´åŠ›ã€ç”µåŠ›èµ„æºä¸€æ ·ï¼Œè®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œå°†æˆä¸ºä¼ä¸šITè¿è¡Œçš„ä¸€ç§è¢«ä½¿ç”¨çš„èµ„æºï¼Œæ— éœ€è‡ªå·±å»ºè®¾ï¼Œå¯æŒ‰éœ€è·å¾—ã€‚ PaaSçš„å®è´¨æ˜¯å°†äº’è”ç½‘çš„èµ„æºæœåŠ¡åŒ–ä¸ºå¯ç¼–ç¨‹æ¥å£ï¼Œä¸ºç¬¬ä¸‰æ–¹å¼€å‘è€…æä¾›æœ‰å•†ä¸šä»·å€¼çš„èµ„æºå’ŒæœåŠ¡å¹³å°ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒIaaSå°±æ˜¯å–ç¡¬ä»¶åŠè®¡ç®—èµ„æºï¼ŒPaaSå°±æ˜¯å–å¼€å‘ã€è¿è¡Œç¯å¢ƒï¼ŒSaaSå°±æ˜¯å–è½¯ä»¶ã€‚ 1.2. IaaS/PaaS/SaaSè¯´æ˜ ç±»å‹ è¯´æ˜ æ¯”å–» ä¾‹å­ IaaS:Infrastructure-as-a-Service(åŸºç¡€è®¾æ–½å³æœåŠ¡) æä¾›çš„æœåŠ¡æ˜¯è®¡ç®—åŸºç¡€è®¾æ–½ åœ°çš®ï¼Œéœ€è¦è‡ªå·±ç›–æˆ¿å­ Amazon EC2ï¼ˆäºšé©¬é€Šå¼¹æ€§äº‘è®¡ç®—ï¼‰ PaaS: Platform-as-a-Service(å¹³å°å³æœåŠ¡) æä¾›çš„æœåŠ¡æ˜¯è½¯ä»¶ç ”å‘çš„å¹³å°æˆ–ä¸šåŠ¡åŸºç¡€å¹³å° å•†å“æˆ¿ï¼Œéœ€è¦è‡ªå·±è£…ä¿® GAEï¼ˆè°·æ­Œå¼€å‘è€…å¹³å°ï¼‰ SaaS: Software-as-a-Service(è½¯ä»¶å³æœåŠ¡) æä¾›çš„æœåŠ¡æ˜¯è¿è¡Œåœ¨äº‘è®¡ç®—åŸºç¡€è®¾æ–½ä¸Šçš„åº”ç”¨ç¨‹åº é…’åº—å¥—æˆ¿ï¼Œå¯ä»¥ç›´æ¥å…¥ä½ è°·æ­Œçš„Gmailé‚®ç®± 1.3. PaaSçš„ç‰¹ç‚¹ï¼ˆä¸‰ç§å±‚æ¬¡ï¼‰ ç‰¹ç‚¹ è¯´æ˜ å¹³å°å³æœåŠ¡ PaaSæä¾›çš„æœåŠ¡å°±æ˜¯ä¸ªåŸºç¡€å¹³å°ï¼Œä¸€ä¸ªç¯å¢ƒï¼Œè€Œä¸æ˜¯å…·ä½“çš„åº”ç”¨ å¹³å°åŠæœåŠ¡ ä¸ä»…æä¾›å¹³å°ï¼Œè¿˜æä¾›å¯¹è¯¥å¹³å°çš„æŠ€æœ¯æ”¯æŒã€ä¼˜åŒ–ç­‰æœåŠ¡ å¹³å°çº§æœåŠ¡ â€œå¹³å°çº§æœåŠ¡â€å³å¼ºå¤§ç¨³å®šçš„å¹³å°å’Œä¸“ä¸šçš„æŠ€æœ¯æ”¯æŒå›¢é˜Ÿï¼Œä¿éšœåº”ç”¨çš„ç¨³å®šä½¿ç”¨ 2. App Engineæ¦‚è¿° 2.1. App Engineæ¦‚å¿µ App Engineæ˜¯PaaSæ¨¡å¼çš„ä¸€ç§å®ç°æ–¹å¼ï¼ŒApp Engineå°†åº”ç”¨è¿è¡Œæ‰€éœ€çš„ IT èµ„æºå’ŒåŸºç¡€è®¾æ–½ä»¥æœåŠ¡çš„æ–¹å¼æä¾›ç»™ç”¨æˆ·ï¼ŒåŒ…æ‹¬äº†ä¸­é—´ä»¶æœåŠ¡ã€èµ„æºç®¡ç†æœåŠ¡ã€å¼¹æ€§è°ƒåº¦æœåŠ¡ã€æ¶ˆæ¯æœåŠ¡ç­‰å¤šç§æœåŠ¡å½¢å¼ã€‚App Engineçš„ç›®æ ‡æ˜¯å¯¹åº”ç”¨æä¾›å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼ˆåŒ…æ‹¬è®¾è®¡ã€å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²ç­‰é˜¶æ®µï¼‰çš„æ”¯æŒï¼Œä»è€Œå‡å°‘äº†ç”¨æˆ·åœ¨è´­ç½®å’Œç®¡ç†åº”ç”¨ç”Ÿå‘½å‘¨æœŸå†…æ‰€å¿…é¡»çš„è½¯ç¡¬ä»¶ä»¥åŠéƒ¨ç½²åº”ç”¨å’ŒIT åŸºç¡€è®¾æ–½çš„æˆæœ¬ï¼ŒåŒæ—¶ç®€åŒ–äº†ä»¥ä¸Šå·¥ä½œçš„å¤æ‚åº¦ã€‚å¸¸è§çš„App Engineæœ‰ï¼šGAE(Google App Engine)ï¼ŒSAE(Sina App Engine)ï¼ŒBAE(Baidu App Engine)ã€‚\nApp Engineåˆ©ç”¨è™šæ‹ŸåŒ–ä¸è‡ªåŠ¨åŒ–æŠ€æœ¯å®ç°å¿«é€Ÿæ­å»ºéƒ¨ç½²åº”ç”¨è¿è¡Œç¯å¢ƒå’ŒåŠ¨æ€è°ƒæ•´åº”ç”¨è¿è¡Œæ—¶ç¯å¢ƒèµ„æºè¿™ä¸¤ä¸ªç›®æ ‡ã€‚ä¸€æ–¹é¢å®ç°å³æ—¶éƒ¨ç½²ä»¥åŠå¿«é€Ÿå›æ”¶ï¼Œé™ä½äº†ç¯å¢ƒæ­å»ºæ—¶é—´ï¼Œé¿å…äº†æ‰‹å·¥é…ç½®é”™è¯¯ï¼Œå¿«é€Ÿé‡å¤æ­å»ºç¯å¢ƒï¼ŒåŠæ—¶å›æ”¶èµ„æºï¼Œ å‡å°‘äº†ä½åˆ©ç”¨ç‡ç¡¬ä»¶èµ„æºçš„ç©ºç½®ã€‚å¦ä¸€æ–¹é¢ï¼Œæ ¹æ®åº”ç”¨è¿è¡Œæ—¶çš„éœ€æ±‚å¯¹åº”ç”¨ç¯å¢ƒè¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œå®ç°äº†åº”ç”¨å¹³å°çš„å¼¹æ€§æ‰©å±•å’Œè‡ªä¼˜åŒ–ï¼Œå‡å°‘äº†éé«˜å³°æ—¶ç¡¬ä»¶èµ„æºçš„ç©ºç½®ã€‚\nç®€è€Œè¨€ä¹‹ï¼ŒApp Engineä¸»è¦ç›®æ ‡æ˜¯ï¼šEasy to maintain(ç»´æŠ¤), Easy to scale(æ‰©å®¹), Easy to build(æ„å»º)ã€‚\n2.2. æ¶æ„è®¾è®¡ 2.3. ç»„æˆæ¨¡å—è¯´æ˜ ç»„æˆæ¨¡å— æ¨¡å—è¯´æ˜ App Router[æµé‡æ¥å…¥å±‚] æ¥æ”¶ç”¨æˆ·è¯·æ±‚ï¼Œå¹¶è½¬å‘åˆ°ä¸åŒçš„App Runtimeã€‚ App Runtime[åº”ç”¨è¿è¡Œå±‚] åº”ç”¨è¿è¡Œç¯å¢ƒï¼Œä¸ºå„ä¸ªåº”ç”¨æä¾›åŸºæœ¬çš„è¿è¡Œå¼•æ“ï¼Œä»è€Œè®©appèƒ½å¤Ÿè¿è¡Œèµ·æ¥ã€‚ Services[åŸºç¡€æœåŠ¡å±‚] å„ä¸ªé€šç”¨åŸºç¡€æœåŠ¡ï¼Œä¸»è¦æ˜¯å¯¹ä¸»æµçš„æœåŠ¡æä¾›é€šç”¨çš„æ¥å…¥ï¼Œä¾‹å¦‚æ•°æ®åº“ç­‰ã€‚ Platform Control[å¹³å°æ§åˆ¶å±‚] æ•´ä¸ªå¹³å°çš„æ§åˆ¶ä¸­å¿ƒï¼Œå®ç°ä¸šåŠ¡è°ƒåº¦ï¼Œå¼¹æ€§æ‰©å®¹ã€èµ„æºå®¡è®¡ã€é›†ç¾¤ç®¡ç†ç­‰ç›¸å…³å·¥ä½œã€‚ Manage System[ç®¡ç†ç•Œé¢å±‚] æä¾›å‹å¥½å¯ç”¨çš„ç®¡ç†æ“ä½œç•Œé¢æ–¹ä¾¿å¹³å°ç®¡ç†å‘˜æ¥æ§åˆ¶ç®¡ç†æ•´ä¸ªå¹³å°ã€‚ Platform Support[å¹³å°æ”¯æŒå±‚] ä¸ºåº”ç”¨æä¾›ç›¸å…³çš„æ”¯æŒï¼Œæ¯”å¦‚åº”ç”¨ç›‘æ§ã€é—®é¢˜å®šä½ã€åˆ†å¸ƒå¼æ—¥å¿—é‡å»ºã€ç»Ÿè®¡åˆ†æç­‰ã€‚ Log Center[æ—¥å¿—ä¸­å¿ƒ] å®æ—¶æ”¶é›†ç›¸å…³åº”ç”¨åŠç³»ç»Ÿçš„æ—¥å¿—ï¼ˆæ—¥å¿—æ”¶é›†ï¼‰ï¼Œæä¾›å®æ—¶è®¡ç®—å’Œåˆ†æå¹³å°ï¼ˆæ—¥å¿—å¤„ç†ï¼‰ã€‚ Code Center[ä»£ç ä¸­å¿ƒ] å®Œæˆä»£ç å­˜å‚¨ã€éƒ¨ç½²ä¸Šçº¿ç›¸å…³çš„å·¥ä½œã€‚ 3. å®¹å™¨äº‘å¹³å°æŠ€æœ¯æ ˆ åŠŸèƒ½ç»„æˆéƒ¨åˆ† ä½¿ç”¨å·¥å…· åº”ç”¨è½½ä½“ Docker ç¼–æ’å·¥å…· Kubernetes é…ç½®æ•°æ® Etcd ç½‘ç»œç®¡ç† Flannel å­˜å‚¨ç®¡ç† Ceph åº•å±‚å®ç° Linuxå†…æ ¸çš„Namespace[èµ„æºéš”ç¦»]å’ŒCGroups[èµ„æºæ§åˆ¶] Namespace[èµ„æºéš”ç¦»] Namespacesæœºåˆ¶æä¾›ä¸€ç§èµ„æºéš”ç¦»æ–¹æ¡ˆã€‚PID,IPC,Networkç­‰ç³»ç»Ÿèµ„æºä¸å†æ˜¯å…¨å±€æ€§çš„ï¼Œè€Œæ˜¯å±äºæŸä¸ªç‰¹å®šçš„Namespaceã€‚æ¯ä¸ªnamespaceä¸‹çš„èµ„æºå¯¹äºå…¶ä»–namespaceä¸‹çš„èµ„æºéƒ½æ˜¯é€æ˜ï¼Œä¸å¯è§çš„ã€‚ CGroups[èµ„æºæ§åˆ¶] CGroupï¼ˆcontrol groupï¼‰æ˜¯å°†ä»»æ„è¿›ç¨‹è¿›è¡Œåˆ†ç»„åŒ–ç®¡ç†çš„Linuxå†…æ ¸åŠŸèƒ½ã€‚CGroupæœ¬èº«æ˜¯æä¾›å°†è¿›ç¨‹è¿›è¡Œåˆ†ç»„åŒ–ç®¡ç†çš„åŠŸèƒ½å’Œæ¥å£çš„åŸºç¡€ç»“æ„ï¼ŒI/Oæˆ–å†…å­˜çš„åˆ†é…æ§åˆ¶ç­‰å…·ä½“çš„èµ„æºç®¡ç†åŠŸèƒ½æ˜¯é€šè¿‡è¿™ä¸ªåŠŸèƒ½æ¥å®ç°çš„ã€‚CGroupså¯ä»¥é™åˆ¶ã€è®°å½•ã€éš”ç¦»è¿›ç¨‹ç»„æ‰€ä½¿ç”¨çš„ç‰©ç†èµ„æºï¼ˆåŒ…æ‹¬ï¼šCPUã€memoryã€IOç­‰ï¼‰ï¼Œä¸ºå®¹å™¨å®ç°è™šæ‹ŸåŒ–æä¾›äº†åŸºæœ¬ä¿è¯ã€‚CGroupsæœ¬è´¨æ˜¯å†…æ ¸é™„åŠ åœ¨ç¨‹åºä¸Šçš„ä¸€ç³»åˆ—é’©å­ï¼ˆhooksï¼‰ï¼Œé€šè¿‡ç¨‹åºè¿è¡Œæ—¶å¯¹èµ„æºçš„è°ƒåº¦è§¦å‘ç›¸åº”çš„é’©å­ä»¥è¾¾åˆ°èµ„æºè¿½è¸ªå’Œé™åˆ¶çš„ç›®çš„ã€‚ 4. Dockeræ¦‚è¿° æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒï¼šDockeræ•´ä½“æ¶æ„å›¾\n4.1. Dockerä»‹ç» Docker - Build, Ship, and Run Any App, Anywhere Dockeræ˜¯ä¸€ç§Linuxå®¹å™¨å·¥å…·é›†ï¼Œå®ƒæ˜¯ä¸ºâ€œæ„å»ºï¼ˆBuildï¼‰ã€äº¤ä»˜ï¼ˆShipï¼‰å’Œè¿è¡Œï¼ˆRunï¼‰â€åˆ†å¸ƒå¼åº”ç”¨è€Œè®¾è®¡çš„ã€‚ Dockerç›¸å½“äºæŠŠåº”ç”¨ä»¥åŠåº”ç”¨æ‰€ä¾èµ–çš„ç¯å¢ƒå®Œå®Œæ•´æ•´åœ°æ‰“æˆäº†ä¸€ä¸ªåŒ…ï¼Œè¿™ä¸ªåŒ…æ‹¿åˆ°å“ªé‡Œéƒ½èƒ½åŸç”Ÿè¿è¡Œã€‚å› æ­¤å¯ä»¥åœ¨å¼€å‘ã€æµ‹è¯•ã€è¿ç»´ä¸­ä¿è¯ç¯å¢ƒçš„ä¸€è‡´æ€§ã€‚ Dockerçš„æœ¬è´¨ï¼šDocker=LXC(Namespace+CGroups)+Docker Imagesï¼Œå³åœ¨Linuxå†…æ ¸çš„Namespace[èµ„æºéš”ç¦»]å’ŒCGroups[èµ„æºæ§åˆ¶]æŠ€æœ¯çš„åŸºç¡€ä¸Šé€šè¿‡é•œåƒç®¡ç†æœºåˆ¶æ¥å®ç°è½»é‡åŒ–è®¾è®¡ã€‚ 4.2. Dockerçš„åŸºæœ¬æ¦‚å¿µ 4.2.1. é•œåƒ Docker é•œåƒå°±æ˜¯ä¸€ä¸ªåªè¯»çš„æ¨¡æ¿ï¼Œå¯ä»¥æŠŠé•œåƒç†è§£æˆä¸€ä¸ªæ¨¡å­ï¼ˆæ¨¡å…·ï¼‰ï¼Œç”±æ¨¡å­ï¼ˆé•œåƒï¼‰åˆ¶ä½œçš„æˆå“ï¼ˆå®¹å™¨ï¼‰éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆé™¤éåœ¨ç”Ÿæˆæ—¶åŠ é¢å¤–å‚æ•°ï¼‰ï¼Œä¿®æ”¹æˆå“ï¼ˆå®¹å™¨ï¼‰æœ¬èº«å¹¶ä¸ä¼šå¯¹æ¨¡å­ï¼ˆé•œåƒï¼‰äº§ç”Ÿå½±å“ï¼ˆé™¤éå°†æˆå“æäº¤æˆä¸€ä¸ªæ¨¡å­ï¼‰ï¼Œå®¹å™¨é‡å¯æ—¶ï¼Œå³ç”±æ¨¡å­ï¼ˆé•œåƒï¼‰é‡æ–°åˆ¶ä½œæˆä¸€ä¸ªæˆå“ï¼ˆå®¹å™¨ï¼‰ï¼Œä¸å…¶ä»–ç”±è¯¥æ¨¡å­åˆ¶ä½œæˆçš„æˆå“å¹¶æ— åŒºåˆ«ã€‚\nä¾‹å¦‚ï¼šä¸€ä¸ªé•œåƒå¯ä»¥åŒ…å«ä¸€ä¸ªå®Œæ•´çš„ ubuntu æ“ä½œç³»ç»Ÿç¯å¢ƒï¼Œé‡Œé¢ä»…å®‰è£…äº† Apache æˆ–ç”¨æˆ·éœ€è¦çš„å…¶å®ƒåº”ç”¨ç¨‹åºã€‚é•œåƒå¯ä»¥ç”¨æ¥åˆ›å»º Docker å®¹å™¨ã€‚Docker æä¾›äº†ä¸€ä¸ªå¾ˆç®€å•çš„æœºåˆ¶æ¥åˆ›å»ºé•œåƒæˆ–è€…æ›´æ–°ç°æœ‰çš„é•œåƒï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä»å…¶ä»–äººé‚£é‡Œä¸‹è½½ä¸€ä¸ªå·²ç»åšå¥½çš„é•œåƒæ¥ç›´æ¥ä½¿ç”¨ã€‚\n4.2.2. å®¹å™¨ Docker åˆ©ç”¨å®¹å™¨æ¥è¿è¡Œåº”ç”¨ã€‚å®¹å™¨æ˜¯ä»é•œåƒåˆ›å»ºçš„è¿è¡Œå®ä¾‹ã€‚å®ƒå¯ä»¥è¢«å¯åŠ¨ã€å¼€å§‹ã€åœæ­¢ã€åˆ é™¤ã€‚æ¯ä¸ªå®¹å™¨éƒ½æ˜¯ç›¸äº’éš”ç¦»çš„ã€ä¿è¯å®‰å…¨çš„å¹³å°ã€‚å¯ä»¥æŠŠå®¹å™¨çœ‹åšæ˜¯ä¸€ä¸ªç®€æ˜“ç‰ˆçš„ Linux ç¯å¢ƒï¼ˆåŒ…æ‹¬rootç”¨æˆ·æƒé™ã€è¿›ç¨‹ç©ºé—´ã€ç”¨æˆ·ç©ºé—´å’Œç½‘ç»œç©ºé—´ç­‰ï¼‰å’Œè¿è¡Œåœ¨å…¶ä¸­çš„åº”ç”¨ç¨‹åºã€‚\n4.2.3. ä»“åº“ ä»“åº“æ˜¯é›†ä¸­å­˜æ”¾é•œåƒæ–‡ä»¶çš„åœºæ‰€ã€‚æœ‰æ—¶å€™ä¼šæŠŠä»“åº“å’Œä»“åº“æ³¨å†ŒæœåŠ¡å™¨ï¼ˆRegistryï¼‰æ··ä¸ºä¸€è°ˆï¼Œå¹¶ä¸ä¸¥æ ¼åŒºåˆ†ã€‚å®é™…ä¸Šï¼Œä»“åº“æ³¨å†ŒæœåŠ¡å™¨ä¸Šå¾€å¾€å­˜æ”¾ç€å¤šä¸ªä»“åº“ï¼Œæ¯ä¸ªä»“åº“ä¸­åˆåŒ…å«äº†å¤šä¸ªé•œåƒï¼Œæ¯ä¸ªé•œåƒæœ‰ä¸åŒçš„æ ‡ç­¾ï¼ˆtagï¼‰ã€‚\n4.3. Dockerçš„ä¼˜åŠ¿ å®¹å™¨çš„å¿«é€Ÿè½»é‡\nå®¹å™¨çš„å¯åŠ¨ï¼Œåœæ­¢å’Œé”€æ¯éƒ½æ˜¯ä»¥ç§’æˆ–æ¯«ç§’ä¸ºå•ä½çš„ï¼Œå¹¶ä¸”ç›¸æ¯”ä¼ ç»Ÿçš„è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œä½¿ç”¨å®¹å™¨åœ¨CPUã€å†…å­˜ï¼Œç½‘ç»œIOç­‰èµ„æºä¸Šçš„æ€§èƒ½æŸè€—éƒ½æœ‰åŒæ ·æ°´å¹³ç”šè‡³æ›´ä¼˜çš„è¡¨ç°ã€‚\nä¸€æ¬¡æ„å»ºï¼Œåˆ°å¤„è¿è¡Œ\nå½“å°†å®¹å™¨å›ºåŒ–æˆé•œåƒåï¼Œå°±å¯ä»¥éå¸¸å¿«é€Ÿåœ°åŠ è½½åˆ°ä»»ä½•ç¯å¢ƒä¸­éƒ¨ç½²è¿è¡Œã€‚è€Œæ„å»ºå‡ºæ¥çš„é•œåƒæ‰“åŒ…äº†åº”ç”¨è¿è¡Œæ‰€éœ€çš„ç¨‹åºã€ä¾èµ–å’Œè¿è¡Œç¯å¢ƒï¼Œ è¿™æ˜¯ä¸€ä¸ªå®Œæ•´å¯ç”¨çš„åº”ç”¨é›†è£…ç®±ï¼Œåœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½ä¿è¯ç¯å¢ƒä¸€è‡´æ€§ã€‚\nå®Œæ•´çš„ç”Ÿæ€é“¾\nå®¹å™¨æŠ€æœ¯å¹¶ä¸æ˜¯Dockeré¦–åˆ›ï¼Œä½†æ˜¯ä»¥å¾€çš„å®¹å™¨å®ç°åªå…³æ³¨äºå¦‚ä½•è¿è¡Œï¼Œè€ŒDockerç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šè¿›è¡Œæ•´åˆå’Œåˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯Dockeré•œåƒçš„è®¾è®¡ï¼Œå®Œç¾åœ°è§£å†³äº†å®¹å™¨ä»æ„å»ºã€äº¤ä»˜åˆ°è¿è¡Œï¼Œæä¾›äº†å®Œæ•´çš„ç”Ÿæ€é“¾æ”¯æŒã€‚\n5. Kubernetesæ¦‚è¿° æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒï¼šKubernetesæ€»æ¶æ„å›¾\n5.1. Kubernetesä»‹ç» Kubernetesæ˜¯Googleå¼€æºçš„å®¹å™¨é›†ç¾¤ç®¡ç†ç³»ç»Ÿã€‚å®ƒæ„å»ºDockeræŠ€æœ¯ä¹‹ä¸Šï¼Œä¸ºå®¹å™¨åŒ–çš„åº”ç”¨æä¾›èµ„æºè°ƒåº¦ã€éƒ¨ç½²è¿è¡Œã€æœåŠ¡å‘ç°ã€æ‰©å®¹ç¼©å®¹ç­‰æ•´ä¸€å¥—åŠŸèƒ½ï¼Œæœ¬è´¨ä¸Šå¯çœ‹ä½œæ˜¯åŸºäºå®¹å™¨æŠ€æœ¯çš„Micro-PaaSå¹³å°ï¼Œå³ç¬¬ä¸‰ä»£PaaSçš„ä»£è¡¨æ€§é¡¹ç›®ã€‚\n5.2. Kubernetesçš„åŸºæœ¬æ¦‚å¿µ 5.2.1. Pod Podæ˜¯è‹¥å¹²ä¸ªç›¸å…³å®¹å™¨çš„ç»„åˆï¼Œæ˜¯ä¸€ä¸ªé€»è¾‘æ¦‚å¿µï¼ŒPodåŒ…å«çš„å®¹å™¨è¿è¡Œåœ¨åŒä¸€ä¸ªå®¿ä¸»æœºä¸Šï¼Œè¿™äº›å®¹å™¨ä½¿ç”¨ç›¸åŒçš„ç½‘ç»œå‘½åç©ºé—´ã€IPåœ°å€å’Œç«¯å£ï¼Œç›¸äº’ä¹‹é—´èƒ½é€šè¿‡localhostæ¥å‘ç°å’Œé€šä¿¡ï¼Œå…±äº«ä¸€å—å­˜å‚¨å·ç©ºé—´ã€‚åœ¨Kubernetesä¸­åˆ›å»ºã€è°ƒåº¦å’Œç®¡ç†çš„æœ€å°å•ä½æ˜¯Podã€‚ä¸€ä¸ªPodä¸€èˆ¬åªæ”¾ä¸€ä¸ªä¸šåŠ¡å®¹å™¨å’Œä¸€ä¸ªç”¨äºç»Ÿä¸€ç½‘ç»œç®¡ç†çš„ç½‘ç»œå®¹å™¨ã€‚\n5.2.2. Replication Controller Replication Controlleræ˜¯ç”¨æ¥æ§åˆ¶ç®¡ç†Podå‰¯æœ¬(Replicaï¼Œæˆ–è€…ç§°å®ä¾‹)ï¼ŒReplication Controllerç¡®ä¿ä»»ä½•æ—¶å€™Kubernetesé›†ç¾¤ä¸­æœ‰æŒ‡å®šæ•°é‡çš„Podå‰¯æœ¬åœ¨è¿è¡Œï¼Œå¦‚æœå°‘äºæŒ‡å®šæ•°é‡çš„Podå‰¯æœ¬ï¼ŒReplication Controllerä¼šå¯åŠ¨æ–°çš„Podå‰¯æœ¬ï¼Œåä¹‹ä¼šæ€æ­»å¤šä½™çš„ä»¥ä¿è¯æ•°é‡ä¸å˜ã€‚å¦å¤–Replication Controlleræ˜¯å¼¹æ€§ä¼¸ç¼©ã€æ»šåŠ¨å‡çº§çš„å®ç°æ ¸å¿ƒã€‚\n5.2.3. Service Serviceæ˜¯çœŸå®åº”ç”¨æœåŠ¡çš„æŠ½è±¡ï¼Œå®šä¹‰äº†Podçš„é€»è¾‘é›†åˆå’Œè®¿é—®è¿™ä¸ªPodé›†åˆçš„ç­–ç•¥ï¼ŒServiceå°†ä»£ç†Podå¯¹å¤–è¡¨ç°ä¸ºä¸€ä¸ªå•ä¸€è®¿é—®æ¥å£ï¼Œå¤–éƒ¨ä¸éœ€è¦äº†è§£åç«¯Podå¦‚ä½•è¿è¡Œï¼Œè¿™ç»™æ‰©å±•æˆ–ç»´æŠ¤å¸¦æ¥å¾ˆå¤§çš„å¥½å¤„ï¼Œæä¾›äº†ä¸€å¥—ç®€åŒ–çš„æœåŠ¡ä»£ç†å’Œå‘ç°æœºåˆ¶ã€‚\n5.2.4. Label Labelæ˜¯ç”¨äºåŒºåˆ†Podã€Serviceã€Replication Controllerçš„Key/Valueé”®å€¼å¯¹ï¼Œå®é™…ä¸ŠKubernetesä¸­çš„ä»»æ„APIå¯¹è±¡éƒ½å¯ä»¥é€šè¿‡Labelè¿›è¡Œæ ‡è¯†ã€‚æ¯ä¸ªAPIå¯¹è±¡å¯ä»¥æœ‰å¤šä¸ªLabelï¼Œä½†æ˜¯æ¯ä¸ªLabelçš„Keyåªèƒ½å¯¹åº”ä¸€ä¸ªValueã€‚Labelæ˜¯Serviceå’ŒReplication Controllerè¿è¡Œçš„åŸºç¡€ï¼Œå®ƒä»¬éƒ½é€šè¿‡Labelæ¥å…³è”Podï¼Œç›¸æ¯”äºå¼ºç»‘å®šæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§éå¸¸å¥½çš„æ¾è€¦åˆå…³ç³»ã€‚\n5.2.5. Node Kubernetså±äºä¸»ä»çš„åˆ†å¸ƒå¼é›†ç¾¤æ¶æ„ï¼ŒKubernets Nodeï¼ˆç®€ç§°ä¸ºNodeï¼Œæ—©æœŸç‰ˆæœ¬å«åšMinionï¼‰è¿è¡Œå¹¶ç®¡ç†å®¹å™¨ã€‚Nodeä½œä¸ºKubernetesçš„æ“ä½œå•å…ƒï¼Œå°†ç”¨æ¥åˆ†é…ç»™Podï¼ˆæˆ–è€…è¯´å®¹å™¨ï¼‰è¿›è¡Œç»‘å®šï¼ŒPodæœ€ç»ˆè¿è¡Œåœ¨Nodeä¸Šï¼ŒNodeå¯ä»¥è®¤ä¸ºæ˜¯Podçš„å®¿ä¸»æœºã€‚\n5.3. Kubernetesæ¶æ„ ","categories":"","description":"","excerpt":"[ç¼–è€…çš„è¯]\nç›®å‰å¾ˆå¤šçš„å®¹å™¨äº‘å¹³å°é€šè¿‡DockeråŠKubernetesç­‰æŠ€æœ¯æä¾›åº”ç”¨è¿è¡Œå¹³å°ï¼Œä»è€Œå®ç°è¿ç»´è‡ªåŠ¨åŒ–ï¼Œå¿«é€Ÿéƒ¨ç½²åº”ç”¨ã€å¼¹æ€§ä¼¸ç¼©å’Œ â€¦","ref":"/kubernetes-notes/concepts/architecture/paas-based-on-docker-and-kubernetes/","tags":["Kubernetes"],"title":" åŸºäºDockeråŠKubernetesæŠ€æœ¯æ„å»ºå®¹å™¨äº‘ï¼ˆPaaSï¼‰å¹³å°"},{"body":"1. install-go.sh #!/bin/bash set -x set -e # default version VERSION=$1 VERSION=${VERSION:-1.14.6} PLATFORM=$2 PLATFORM=${PLATFORM:-linux} GOROOT=\"/usr/local/go\" GOPATH=$HOME/gopath GO_DOWNLOAD_URL=\"https://golang.org/dl\" # download and install case ${PLATFORM} in \"linux\") wget ${GO_DOWNLOAD_URL}/go${VERSION}.${PLATFORM}-amd64.tar.gz tar -C /usr/local -xzf go${VERSION}.${PLATFORM}-amd64.tar.gz ;; \"mac\") PLATFORM=\"darwin\" wget ${GO_DOWNLOAD_URL}/go${VERSION}.${PLATFORM}-amd64.tar.gz tar -C /usr/local -xzf go${VERSION}.${PLATFORM}-amd64.tar.gz ;; *) echo \"platform not found\" ;; esac # set golang env cat \u003e\u003e $HOME/.bashrc \u003c\u003c EOF # Golang env export GOROOT=/usr/local/go export GOPATH=\\$HOME/gopath export PATH=\\$PATH:\\$GOROOT/bin:\\$GOPATH/bin EOF source $HOME/.bashrc # mkdir gopath mkdir -p $GOPATH/src $GOPATH/pkg $GOPATH/bin 2. å®‰è£… chmod +x install-go.sh ./install-go.sh 1.14.6 linux æ›´å¤šç‰ˆæœ¬å·å¯å‚è€ƒï¼šhttps://golang.org/dl/\nå‚è€ƒï¼š\nhttps://golang.org/doc/install ","categories":"","description":"","excerpt":"1. install-go.sh #!/bin/bash set -x set -e # default version â€¦","ref":"/golang-notes/introduction/install/","tags":["Golang"],"title":"Golangå®‰è£…"},{"body":"1. æŒ‡é’ˆçš„æ¦‚å¿µ æ¦‚å¿µ è¯´æ˜ å˜é‡ æ˜¯ä¸€ç§å ä½ç¬¦ï¼Œç”¨äºå¼•ç”¨è®¡ç®—æœºçš„å†…å­˜åœ°å€ã€‚å¯ç†è§£ä¸ºå†…å­˜åœ°å€çš„æ ‡ç­¾ æŒ‡é’ˆ è¡¨ç¤ºå†…å­˜åœ°å€ï¼Œè¡¨ç¤ºåœ°å€çš„æŒ‡å‘ã€‚æŒ‡é’ˆæ˜¯ä¸€ä¸ªæŒ‡å‘å¦ä¸€ä¸ªå˜é‡å†…å­˜åœ°å€çš„å€¼ \u0026 å–åœ°å€ç¬¦ï¼Œä¾‹å¦‚ï¼š{æŒ‡é’ˆ}:=\u0026{å˜é‡} * å–å€¼ç¬¦ï¼Œä¾‹å¦‚ï¼š{å˜é‡}:=*{æŒ‡é’ˆ} 2. å†…å­˜åœ°å€è¯´æ˜ 2.1. å†…å­˜å®šä¹‰ è®¡ç®—æœºçš„å†…å­˜ RAM å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€äº›æœ‰åºçš„ç›’å­ï¼Œä¸€ä¸ªæ¥ä¸€ä¸ªçš„æ’æˆä¸€æ’ï¼Œæ¯ä¸€ä¸ªç›’å­æˆ–è€…å•å…ƒæ ¼éƒ½è¢«ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—æ ‡è®°ä¾æ¬¡é€’å¢ï¼Œè¿™ä¸ªæ•°å­—å°±æ˜¯è¯¥å•å…ƒæ ¼çš„åœ°å€ï¼Œä¹Ÿå°±æ˜¯å†…å­˜çš„åœ°å€ã€‚ ç¡¬ä»¶è§’åº¦ï¼šå†…å­˜æ˜¯CPUæ²Ÿé€šçš„æ¡¥æ¢ï¼Œç¨‹åºè¿è¡Œåœ¨å†…å­˜ä¸­ã€‚\né€»è¾‘è§’åº¦ï¼šå†…å­˜æ˜¯ä¸€å—å…·å¤‡éšæœºè®¿é—®èƒ½åŠ›ï¼Œæ”¯æŒè¯»å†™æ“ä½œï¼Œç”¨æ¥å­˜æ”¾ç¨‹åºåŠç¨‹åºè¿è¡Œä¸­äº§ç”Ÿçš„æ•°æ®çš„åŒºåŸŸã€‚\næ¦‚å¿µ æ¯”å–» å†…å­˜ ä¸€å±‚æ¥¼å±‚ å†…å­˜å— æ¥¼å±‚ä¸­çš„ä¸€ä¸ªæˆ¿é—´ å˜é‡å æˆ¿é—´çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼šæ€»ç»ç†å®¤ æŒ‡é’ˆ æˆ¿é—´çš„å…·ä½“åœ°å€ï¼ˆé—¨ç‰Œå·ï¼‰ï¼Œä¾‹å¦‚ï¼šæ€»ç»ç†å®¤åœ°å€æ˜¯2æ¥¼201å®¤ å˜é‡å€¼ æˆ¿é—´é‡Œçš„å…·ä½“å­˜å‚¨ç‰© æŒ‡é’ˆåœ°å€ æŒ‡é’ˆçš„åœ°å€ï¼šå­˜å‚¨æŒ‡é’ˆå†…å­˜å—çš„åœ°å€ 2.2. å†…å­˜å•ä½å’Œç¼–å€ 2.2.1. å†…å­˜å•ä½ å•ä½ è¯´æ˜ ä½ï¼ˆbitï¼‰ è®¡ç®—æœºä¸­æœ€å°çš„æ•°æ®å•ä½ï¼Œæ¯ä¸€ä½çš„çŠ¶æ€åªèƒ½æ˜¯0æˆ–1 å­—èŠ‚ï¼ˆByteï¼‰ 1Byte=8bitï¼Œæ˜¯å†…å­˜åŸºæœ¬çš„è®¡é‡å•ä½ å­— â€œå­—â€ç”±è‹¥å¹²ä¸ªå­—èŠ‚æ„æˆï¼Œå­—çš„ä½æ•°å«å­—é•¿ï¼Œä¸åŒæ¡£æ¬¡çš„æœºå™¨æœ‰ä¸åŒçš„å­—é•¿ KB 1KB=1024Byteï¼Œå³1024ä¸ªå­—èŠ‚ MB 1MB=1024KB GB 1GB=1024MB 2.2.2. å†…å­˜ç¼–å€ è®¡ç®—æœºä¸­çš„å†…å­˜æŒ‰å­—èŠ‚ç¼–å€ï¼Œæ¯ä¸ªåœ°å€çš„å­˜å‚¨å•å…ƒå¯ä»¥å­˜æ”¾ä¸€ä¸ªå­—èŠ‚çš„æ•°æ®ï¼ŒCPUé€šè¿‡å†…å­˜åœ°å€è·å–æŒ‡ä»¤å’Œæ•°æ®ï¼Œå¹¶ä¸å…³å¿ƒè¿™ä¸ªåœ°å€æ‰€ä»£è¡¨çš„ç©ºé—´åœ¨ä»€ä¹ˆä½ç½®ï¼Œå†…å­˜åœ°å€å’Œåœ°å€æŒ‡å‘çš„ç©ºé—´å…±åŒæ„æˆäº†ä¸€ä¸ªå†…å­˜å•å…ƒã€‚\n2.2.3. å†…å­˜åœ°å€ å†…å­˜åœ°å€é€šå¸¸ç”¨16è¿›åˆ¶çš„æ•°æ®è¡¨ç¤ºï¼Œä¾‹å¦‚0x0ffc1ã€‚\n3.å˜é‡ä¸æŒ‡é’ˆè¿ç®—ç†è§£ ç¼–å†™ä¸€æ®µç¨‹åºï¼Œæ£€ç´¢å‡ºå€¼å¹¶å­˜å‚¨åœ¨åœ°å€ä¸º 200 çš„ä¸€ä¸ªå—å†…å­˜ä¸­ï¼Œå°†å…¶ä¹˜ä»¥ 3ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨åœ°å€ä¸º 201 çš„å¦ä¸€å—å†…å­˜ä¸­\n3.1.æœ¬è´¨ æ£€ç´¢å‡ºå†…å­˜åœ°å€ä¸º 200 çš„å€¼ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ CPU ä¸­ å°†å­˜å‚¨åœ¨ CPU ä¸­çš„å€¼ä¹˜ä»¥ 3 å°† CPU ä¸­å­˜å‚¨çš„ç»“æœï¼Œå†™å…¥åœ°å€ä¸º 201 çš„å†…å­˜å—ä¸­ 3.2.åŸºäºå˜é‡çš„ç†è§£ è·å–å˜é‡ a ä¸­å­˜å‚¨çš„å€¼ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ CPU ä¸­ å°†å…¶ä¹˜ä»¥ 3 å°†ç»“æœä¿å­˜åœ¨å˜é‡ b ä¸­ var a = 6 var b = a * 3 3.3.åŸºäºæŒ‡é’ˆçš„ç†è§£ func main() { a := 200 b := \u0026a *b++ fmt.Println(a) } ä»¥ä¸Šå‡½æ•°å¯¹aè¿›è¡Œ+1æ“ä½œï¼Œå…·ä½“ç†è§£å¦‚ä¸‹ï¼š\n1.a:=200\n2. b := \u0026a\n*3. b++\n4. æŒ‡é’ˆçš„ä½¿ç”¨ 4.1. æ–¹æ³•ä¸­çš„æŒ‡é’ˆ æ–¹æ³•å³ä¸ºæœ‰æ¥å—è€…çš„å‡½æ•°ï¼Œæ¥å—è€…å¯ä»¥æ˜¯ç±»å‹çš„å®ä¾‹å˜é‡æˆ–è€…æ˜¯ç±»å‹çš„å®ä¾‹æŒ‡é’ˆå˜é‡ã€‚ä½†ä¸¤ç§æ•ˆæœä¸åŒã€‚\n1ã€ç±»å‹çš„å®ä¾‹å˜é‡\nfunc main(){ person := Person{\"vanyar\", 21} fmt.Printf(\"person\u003c%s:%d\u003e\\n\", person.name, person.age) person.sayHi() person.ModifyAge(210) person.sayHi() } type Person struct { name string age int } func (p Person) sayHi() { fmt.Printf(\"SayHi -- This is %s, my age is %d\\n\",p.name, p.age) } func (p Person) ModifyAge(age int) { fmt.Printf(\"ModifyAge\") p.age = age } //è¾“å‡ºç»“æœ person\u003cvanyar:21\u003e SayHi -- This is vanyar, my age is 21 ModifyAgeSayHi -- This is vanyar, my age is 21 å°½ç®¡ ModifyAge æ–¹æ³•ä¿®æ”¹äº†å…¶ageå­—æ®µï¼Œå¯æ˜¯æ–¹æ³•é‡Œçš„pæ˜¯personå˜é‡çš„ä¸€ä¸ªå‰¯æœ¬ï¼Œä¿®æ”¹çš„åªæ˜¯å‰¯æœ¬çš„å€¼ã€‚ä¸‹ä¸€æ¬¡è°ƒç”¨sayHiæ–¹æ³•çš„æ—¶å€™ï¼Œè¿˜æ˜¯personçš„å‰¯æœ¬ï¼Œå› æ­¤ä¿®æ”¹æ–¹æ³•å¹¶ä¸ä¼šç”Ÿæ•ˆã€‚\nå³å®ä¾‹å˜é‡çš„æ–¹å¼å¹¶ä¸ä¼šæ”¹å˜æ¥å—è€…æœ¬èº«çš„å€¼ã€‚\n2ã€ç±»å‹çš„å®ä¾‹æŒ‡é’ˆå˜é‡\nfunc (p *Person) ChangeAge(age int) { fmt.Printf(\"ModifyAge\") p.age = age } Goä¼šæ ¹æ®Personçš„ç¤ºä¾‹ç±»å‹ï¼Œè½¬æ¢æˆæŒ‡é’ˆç±»å‹å†æ‹·è´ï¼Œå³ person.ChangeAgeä¼šå˜æˆ (\u0026person).ChangeAgeã€‚\næŒ‡é’ˆç±»å‹çš„æ¥å—è€…ï¼Œå¦‚æœå®ä¾‹å¯¹è±¡æ˜¯å€¼ï¼Œé‚£ä¹ˆgoä¼šè½¬æ¢æˆæŒ‡é’ˆï¼Œç„¶åå†æ‹·è´ï¼Œå¦‚æœæœ¬èº«å°±æ˜¯æŒ‡é’ˆå¯¹è±¡ï¼Œé‚£ä¹ˆå°±ç›´æ¥æ‹·è´æŒ‡é’ˆå®ä¾‹ã€‚å› ä¸ºæŒ‡é’ˆéƒ½æŒ‡å‘ä¸€å¤„å€¼ï¼Œå°±èƒ½ä¿®æ”¹å¯¹è±¡äº†ã€‚\n5. é›¶å€¼ä¸nil(ç©ºæŒ‡é’ˆ) å˜é‡å£°æ˜è€Œæ²¡æœ‰èµ‹å€¼ï¼Œé»˜è®¤ä¸ºé›¶å€¼ï¼Œä¸åŒç±»å‹é›¶å€¼ä¸åŒï¼Œä¾‹å¦‚å­—ç¬¦ä¸²é›¶å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼›\næŒ‡é’ˆå£°æ˜è€Œæ²¡æœ‰èµ‹å€¼ï¼Œé»˜è®¤ä¸ºnilï¼Œå³è¯¥æŒ‡é’ˆæ²¡æœ‰ä»»ä½•æŒ‡å‘ã€‚å½“æŒ‡é’ˆæ²¡æœ‰æŒ‡å‘çš„æ—¶å€™ï¼Œä¸èƒ½å¯¹(*point)è¿›è¡Œæ“ä½œåŒ…æ‹¬è¯»å–ï¼Œå¦åˆ™ä¼šæŠ¥ç©ºæŒ‡é’ˆå¼‚å¸¸ã€‚\nfunc main(){ // å£°æ˜ä¸€ä¸ªæŒ‡é’ˆå˜é‡ aPot å…¶ç±»å‹ä¹Ÿæ˜¯ string var aPot *string fmt.Printf(\"aPot: %p %#v\\n\", \u0026aPot, aPot) // è¾“å‡º aPot: 0xc42000c030 (*string)(nil) *aPot = \"This is a Pointer\" // æŠ¥é”™ï¼š panic: runtime error: invalid memory address or nil pointer dereference } è§£å†³æ–¹æ³•å³ç»™è¯¥æŒ‡é’ˆåˆ†é…ä¸€ä¸ªæŒ‡å‘,å³åˆå§‹åŒ–ä¸€ä¸ªå†…å­˜ï¼Œå¹¶æŠŠè¯¥å†…å­˜åœ°å€èµ‹äºˆæŒ‡é’ˆå˜é‡ï¼Œä¾‹å¦‚ï¼š\n// å£°æ˜ä¸€ä¸ªæŒ‡é’ˆå˜é‡ aPot å…¶ç±»å‹ä¹Ÿæ˜¯ string var aPot *string fmt.Printf(\"aPot: %p %#v\\n\", \u0026aPot, aPot) // è¾“å‡º aPot: 0xc42000c030 (*string)(nil) aPot = \u0026aVar *aPot = \"This is a Pointer\" fmt.Printf(\"aVar: %p %#v \\n\", \u0026aVar, aVar) // è¾“å‡º aVar: 0xc42000e240 \"This is a Pointer\" fmt.Printf(\"aPot: %p %#v %#v \\n\", \u0026aPot, aPot, *aPot) // è¾“å‡º aPot: 0xc42000c030 (*string)(0xc42000e240) \"This is a Pointer\" æˆ–è€…é€šè¿‡newå¼€è¾Ÿä¸€ä¸ªå†…å­˜ï¼Œå¹¶è¿”å›è¿™ä¸ªå†…å­˜çš„åœ°å€ã€‚\nvar aNewPot *int aNewPot = new(int) *aNewPot = 217 fmt.Printf(\"aNewPot: %p %#v %#v \\n\", \u0026aNewPot, aNewPot, *aNewPot) // è¾“å‡º aNewPot: 0xc42007a028 (*int)(0xc42006e1f0) 217 6. æ€»ç»“ Golangæä¾›äº†æŒ‡é’ˆç”¨äºæ“ä½œæ•°æ®å†…å­˜ï¼Œå¹¶é€šè¿‡å¼•ç”¨æ¥ä¿®æ”¹å˜é‡ã€‚ åªå£°æ˜æœªèµ‹å€¼çš„å˜é‡ï¼Œgolangéƒ½ä¼šè‡ªåŠ¨ä¸ºå…¶åˆå§‹åŒ–ä¸ºé›¶å€¼ï¼ŒåŸºç¡€æ•°æ®ç±»å‹çš„é›¶å€¼æ¯”è¾ƒç®€å•ï¼Œå¼•ç”¨ç±»å‹å’ŒæŒ‡é’ˆçš„é›¶å€¼éƒ½ä¸ºnilï¼Œnilç±»å‹ä¸èƒ½ç›´æ¥èµ‹å€¼ï¼Œå› æ­¤éœ€è¦é€šè¿‡newå¼€è¾Ÿä¸€ä¸ªå†…å­˜ï¼Œæˆ–è€…é€šè¿‡makeåˆå§‹åŒ–æ•°æ®ç±»å‹ï¼Œæˆ–è€…ä¸¤è€…é…åˆï¼Œç„¶åæ‰èƒ½èµ‹å€¼ã€‚ æŒ‡é’ˆä¹Ÿæ˜¯ä¸€ç§ç±»å‹ï¼Œä¸åŒäºä¸€èˆ¬ç±»å‹ï¼ŒæŒ‡é’ˆçš„å€¼æ˜¯åœ°å€ï¼Œè¿™ä¸ªåœ°å€æŒ‡å‘å…¶ä»–çš„å†…å­˜ï¼Œé€šè¿‡æŒ‡é’ˆå¯ä»¥è¯»å–å…¶æ‰€æŒ‡å‘çš„åœ°å€æ‰€å­˜å‚¨çš„å€¼ã€‚ å‡½æ•°æ–¹æ³•çš„æ¥å—è€…ï¼Œä¹Ÿå¯ä»¥æ˜¯æŒ‡é’ˆå˜é‡ã€‚æ— è®ºæ™®é€šæ¥å—è€…è¿˜æ˜¯æŒ‡é’ˆæ¥å—è€…éƒ½ä¼šè¢«æ‹·è´ä¼ å…¥æ–¹æ³•ä¸­ï¼Œä¸åŒåœ¨äºæ‹·è´çš„æŒ‡é’ˆï¼Œå…¶æŒ‡å‘çš„åœ°æ–¹éƒ½ä¸€æ ·ï¼Œåªæ˜¯å…¶è‡ªèº«çš„åœ°å€ä¸ä¸€æ ·ã€‚ å‚è€ƒï¼š\nhttp://www.jianshu.com/p/d23f78a3922b\nhttp://www.jianshu.com/p/44b9429d7bef\n","categories":"","description":"","excerpt":"1. æŒ‡é’ˆçš„æ¦‚å¿µ æ¦‚å¿µ è¯´æ˜ å˜é‡ æ˜¯ä¸€ç§å ä½ç¬¦ï¼Œç”¨äºå¼•ç”¨è®¡ç®—æœºçš„å†…å­˜åœ°å€ã€‚å¯ç†è§£ä¸ºå†…å­˜åœ°å€çš„æ ‡ç­¾ æŒ‡é’ˆ è¡¨ç¤ºå†…å­˜åœ°å€ï¼Œè¡¨ç¤ºåœ°å€çš„æŒ‡å‘ã€‚æŒ‡é’ˆ â€¦","ref":"/golang-notes/oop/pointer/","tags":["Golang"],"title":"Golang æŒ‡é’ˆ"},{"body":"1. åå‘ä»£ç†ç®€ä»‹ Nginxå¯ä»¥ä½œä¸ºåå‘ä»£ç†ï¼Œæ¥æ”¶å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼Œå¹¶å‘ä¸Šæ¸¸æœåŠ¡å™¨å‘èµ·æ–°çš„è¯·æ±‚ã€‚è¯¥è¯·æ±‚å¯ä»¥æ ¹æ®å®¢æˆ·ç«¯è¯·æ±‚çš„URIï¼Œå®¢æˆ·æœºå‚æ•°æˆ–å…¶ä»–é€»è¾‘è¿›è¡Œæ‹†åˆ†ï¼ŒåŸå§‹URLä¸­çš„ä»»ä½•éƒ¨åˆ†å¯ä»¥ä»¥è¿™ç§æ–¹å¼è¿›è¡Œè½¬æ¢ã€‚\n1.1. ä»£ç†æ¨¡å—æŒ‡ä»¤ æŒ‡ä»¤ è¯´æ˜ proxy_connect_timeout Nginxä»æ¥å—åˆ°è¯·æ±‚åˆ°è¿æ¥è‡³ä¸Šæ¸¸æœåŠ¡å™¨çš„æœ€é•¿ç­‰å¾…æ—¶é—´ proxy_cookie_domain æ›¿ä»£ä»ä¸Šæ¸¸æœåŠ¡å™¨æ¥çš„Set-Cookieå¤´çš„åŸŸdomain proxy_cookie_path æ›¿ä»£ä»ä¸Šæ¸¸æœåŠ¡å™¨æ¥çš„Set-Cookieå¤´çš„pathå±æ€§ proxy_headers_hash_bucket_size å¤´åå­—çš„æœ€å¤§å€¼ proxy_headers_hash_max_size ä»ä¸Šæ¸¸æœåŠ¡å™¨æ¥æ”¶åˆ°å¤´çš„æ€»å¤§å° proxy_hide_header ä¸åº”è¯¥ä¼ é€’ç»™å®¢æˆ·ç«¯å¤´çš„åˆ—è¡¨ proxy_http_version ç”¨äºé€šä¸Šæ¸¸æœåŠ¡å™¨é€šä¿¡çš„Httpåè®®ç‰ˆæœ¬ proxy_ignore_client_abort å¦‚æœè®¾ç½®ä¸ºONï¼Œé‚£ä¹ˆå®¢æˆ·ç«¯æ”¾å¼ƒè¿æ¥åï¼Œnginxå°†ä¸ä¼šæ”¾å¼ƒåŒä¸Šæ¸¸æœåŠ¡å™¨çš„è¿æ¥ proxy_ignore_headers å½“å¤„ç†æ¥è‡ªä¸Šæ¸¸æœåŠ¡å™¨çš„å“åº”æ—¶ï¼Œè®¾ç½®å“ªäº›å¤´å¯ä»¥è¢«å¿½ç•¥ proxy_intercept_errors å¦‚æœå¯ç”¨è¯¥é€‰é¡¹ï¼ŒNginxå°†ä¼šæ˜¾ç¤ºé…ç½®çš„error_pageé”™è¯¯ï¼Œè€Œä¸æ˜¯æ¥è‡ªäºä¸Šæ¸¸æœåŠ¡å™¨çš„ç›´æ¥å“åº” proxy_max_temp_file_size åœ¨å†™å…¥å†…å­˜ç¼“å†²åŒºæ—¶å“åº”ä¸å†…å­˜ä¸åŒ¹é…æ—¶ä½¿ç”¨æ—¶ï¼Œç»™å‡ºæº¢å‡ºæ–‡ä»¶çš„æœ€å¤§å€¼ proxy_pass æŒ‡å®šè¯·æ±‚è¢«ä¼ é€’åˆ°çš„ä¸Šæ¸¸æœåŠ¡å™¨ï¼Œæ ¼å¼ä¸ºURL proxy_pass_header è¦†ç›–æ‰åœ¨proxy_hide_headeræŒ‡ä»¤ä¸­è®¾ç½®çš„å¤´ï¼Œå…è®¸è¿™äº›å¤´ä¼ é€’åˆ°å®¢æˆ·ç«¯ proxy_pass_request_body å¦‚æœè®¾ç½®ä¸ºoffï¼Œå°†ä¼šé˜»æ­¢è¯·æ±‚ä½“ä¼ é€’åˆ°å®¢æˆ·ç«¯ proxy_pass_request_headers å¦‚æœè®¾ç½®ä¸ºon,åˆ™é˜»æ­¢è¯·æ±‚å¤´å‘é€åˆ°ä¸Šæ¸¸æœåŠ¡å™¨ proxy_read_timeout ç»™å‡ºè¿æ¥å…³é—­å‰ä»ä¸Šæ¸¸æœåŠ¡å™¨ä¸¤æ¬¡æˆåŠŸçš„è¯»æ“ä½œè€—æ—¶ï¼Œå¦‚æœä¸Šæ¸¸æœåŠ¡å™¨å¤„ç†è¯·æ±‚æ¯”è¾ƒæ…¢ï¼Œé‚£ä¹ˆè¯¥å€¼éœ€è®¾ç½®è¾ƒé«˜äº› proxy_redirect é‡å†™æ¥è‡ªäºä¸Šæ¸¸æœåŠ¡å™¨çš„Locationå’ŒRefreshå¤´ proxy_send_timeout ç»™å‡ºè¿æ¥å…³é—­å‰ä»ä¸Šæ¸¸æœåŠ¡å™¨ä¸¤æ¬¡æˆåŠŸçš„å†™æ“ä½œè€—æ—¶ï¼Œå¦‚æœä¸Šæ¸¸æœåŠ¡å™¨å¤„ç†è¯·æ±‚æ¯”è¾ƒæ…¢ï¼Œé‚£ä¹ˆè¯¥å€¼éœ€è®¾ç½®è¾ƒé«˜äº› proxy_set_body å‘é€åˆ°ä¸Šæ¸¸æœåŠ¡å™¨çš„è¯·æ±‚ä½“å¯èƒ½ä¼šè¢«è¯¥æŒ‡ä»¤çš„è®¾ç½®å€¼ä¿®æ”¹ proxy_set_header é‡å†™å‘é€åˆ°ä¸Šæ¸¸æœåŠ¡å™¨å¤´çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å°†æŸç§å¤´çš„å€¼è®¾ç½®ä¸ºç©ºå­—ç¬¦ï¼Œè€Œä¸æ˜¯å‘é€æŸç§å¤´çš„æ–¹æ³•å®ç° proxy_temp_file_write_size åœ¨åŒä¸€æ—¶é—´å†…é™åˆ¶ç¼“å†²åˆ°ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶çš„æ•°æ®é‡ï¼Œä»¥ä½¿å¾—Nginxä¸ä¼šè¿‡é•¿åœ°é˜»æ­¢å•ä¸ªè¯·æ±‚ proxy_temp_path è®¾å®šä¸´æ—¶æ–‡ä»¶çš„ç¼“å†²ï¼Œç”¨äºç¼“å†²ä»ä¸Šæ¸¸æœåŠ¡å™¨æ¥çš„æ–‡ä»¶ï¼Œå¯ä»¥è®¾å®šç›®å½•çš„å±‚æ¬¡ 1.2. upstreamæ¨¡å— upstreamæŒ‡ä»¤å°†ä¼šå¯ç”¨ä¸€ä¸ªæ–°çš„é…ç½®åŒºåŸŸï¼Œåœ¨è¯¥åŒºåŸŸå®šä¹‰äº†ä¸€ç»„ä¸Šæ¸¸æœåŠ¡å™¨ï¼Œè¿™äº›æœåŠ¡å™¨å¯ä»¥è¢«è®¾ç½®ä¸ºä¸åŒçš„æƒé‡ï¼ˆæƒé‡é«˜çš„æœåŠ¡å™¨å°†ä¼šè¢«Nginxä¼ é€’è¶Šå¤šçš„è¿æ¥ï¼‰ã€‚\næŒ‡ä»¤ è¯´æ˜ ip_hash é€šè¿‡IPåœ°å€çš„å“ˆå¸Œå€¼ç¡®ä¿å®¢æˆ·ç«¯å‡åŒ€åœ°è¿æ¥æ‰€æœ‰çš„æœåŠ¡å™¨ï¼Œé”®å€¼åŸºäºCç±»åœ°å€ keepalive æ¯ä¸€ä¸ªworkerè¿›ç¨‹ç¼“å­˜çš„åˆ°ä¸Šæ¸¸æœåŠ¡å™¨çš„è¿æ¥æ•°ã€‚å†ä½¿ç”¨Httpè¿æ¥æ—¶ï¼Œproxy_http_verisonè®¾ç½®1.1ï¼Œå¹¶å°†proxy_set_headerè®¾ç½®ä¸ºConnection \"\" least_conn æ¿€æ´»è´Ÿè½½å‡è¡¡ç®—æ³•ï¼Œå°†è¯·æ±‚å‘é€åˆ°æ´»è·ƒè¿æ¥æ•°æœ€å°‘çš„é‚£å°æœåŠ¡å™¨ server ä¸ºupstreamå®šä¹‰ä¸€ä¸ªæœåŠ¡å™¨åœ°å€ï¼ˆå¸¦æœ‰ç«¯å£å·çš„åŸŸåã€IPåœ°å€ï¼Œæˆ–è€…æ˜¯UNIXå¥—æ¥å­—ï¼‰å’Œä¸€ä¸ªå¯é€‰çš„å‚æ•°ï¼Œå‚æ•°å¦‚ä¸‹ï¼šweightï¼šè®¾ç½®ä¸€ä¸ªæœåŠ¡å™¨çš„ä¼˜å…ˆçº§ä¼˜äºå…¶ä»–æœåŠ¡å™¨ã€‚max_failsï¼šè®¾ç½®åœ¨fail_timeoutæ—¶é—´ä¹‹å†…å°è¯•å¯¹ä¸€ä¸ªæœåŠ¡å™¨è¿æ¥çš„æœ€å¤§æ¬¡æ•°ï¼Œå¦‚æœè¶…è¿‡è¿™ä¸ªæ¬¡æ•°ï¼Œé‚£ä¹ˆå°±ä¼šè¢«æ ‡è®°ä¸ºdownã€‚fail_timeoutï¼šåœ¨è¿™ä¸ªæŒ‡å®šçš„æ—¶é—´å†…æœåŠ¡å™¨å¿…é¡»æä¾›å“åº”ï¼Œå¦‚æœåœ¨è¿™ä¸ªæ—¶é—´å†…æ²¡æœ‰æ”¶åˆ°å“åº”ï¼Œé‚£ä¹ˆæœåŠ¡å™¨å°±ä¼šè¢«æ ‡è®°ä¸ºdownçŠ¶æ€ã€‚backupï¼šä¸€æ—¦å…¶ä»–æœåŠ¡å™¨å®•æœºï¼Œé‚£ä¹ˆæœ‰è¯¥æ ‡è®°çš„æœºå™¨å°±ä¼šæ¥æ”¶è¯·æ±‚ã€‚downï¼šæ ‡è®°ä¸ºä¸€ä¸ªæœåŠ¡å™¨ä¸å†æ¥å—ä»»ä½•è¯·æ±‚ã€‚ 1.2.1. è´Ÿè½½å‡è¡¡ç®—æ³• upstreamæ¨¡å—èƒ½å¤Ÿä½¿ç”¨è½®è¯¢ã€IP hashå’Œæœ€å°‘è¿æ¥æ•°ä¸‰ç§è´Ÿè½½å‡è¡¡ç®—æ³•ä¹‹ä¸€æ¥é€‰æ‹©å“ªä¸ªä¸Šæ¸¸æœåŠ¡å™¨å°†ä¼šè¢«åœ¨ä¸‹ä¸€æ­¥ä¸­è¿æ¥ã€‚\n1.2.1.1. è½®è¯¢ é»˜è®¤æƒ…å†µä½¿ç”¨è½®è¯¢ï¼Œä¸éœ€è¦é…ç½®æŒ‡ä»¤æ¥è®¾ç½®ï¼Œè¯¥ç®—æ³•é€‰æ‹©ä¸‹ä¸€ä¸ªæœåŠ¡å™¨ï¼ŒåŸºäºå…ˆå‰é€‰æ‹©ï¼Œå†é…ç½®æ–‡ä»¶ä¸­å“ªä¸€ä¸ªæ˜¯ä¸‹ä¸€ä¸ªæœåŠ¡å™¨ï¼Œä»¥åŠæ¯ä¸ªæœåŠ¡å™¨çš„è´Ÿè½½ã€‚è½®è¯¢ç®—æ³•æ˜¯åŸºäºåœ¨é˜Ÿåˆ—ä¸­è°æ˜¯ä¸‹ä¸€ä¸ªçš„åŸç†ç¡®ä¿å°†è®¿é—®é‡å‡åŒ€çš„åˆ†é…ç»™æ¯ä¸€ä¸ªä¸Šæ¸¸æœåŠ¡å™¨ã€‚\n1.2.1.2. IP å“ˆå¸Œ é€šè¿‡ip_hashæŒ‡ä»¤æ¿€æ´»ä½¿ç”¨ï¼Œä»è€Œå°†æŸäº›IPåœ°å€æ˜ å°„åˆ°åŒä¸€ä¸ªä¸Šæ¸¸æœåŠ¡å™¨ã€‚\n1.2.1.3. æœ€å°‘è¿æ¥æ•° é€šè¿‡least_connæŒ‡ä»¤å¯ç”¨ï¼Œè¯¥ç®—æ³•é€šè¿‡é€‰æ‹©ä¸€ä¸ªæ´»è·ƒçš„æœ€å°‘è¿æ¥æ•°æœåŠ¡å™¨ï¼Œç„¶åå°†è´Ÿè½½å‡åŒ€åˆ†é…ç»™ä¸Šæ¸¸æœåŠ¡å™¨ã€‚å¦‚æœä¸Šæ¸¸æœåŠ¡å™¨çš„å¤„ç†å™¨èƒ½åŠ›ä¸åŒï¼Œé‚£ä¹ˆå¯ä»¥ä¸ºserveræŒ‡ä»¤ä½¿ç”¨weightæ¥æŒ‡ç¤ºè¯´æ˜ã€‚è¯¥ç®—æ³•å°†è€ƒè™‘åˆ°ä¸åŒæœåŠ¡å™¨çš„åŠ æƒæœ€å°è¿æ¥æ•°ã€‚\n2. UpstreamæœåŠ¡å™¨ç±»å‹ ä¸Šæ¸¸æœåŠ¡å™¨æ˜¯Ngixnä»£ç†è¿æ¥çš„ä¸€ä¸ªæœåŠ¡å™¨ï¼Œå¯ä»¥æ˜¯ç‰©ç†æœºæˆ–è™šæ‹Ÿæœºã€‚\n2.1. å•ä¸ªupstreamæœåŠ¡å™¨ æŒ‡ä»¤try_files(åŒ…æ‹¬http coreæ¨¡å—å†…)æ„å‘³ç€æŒ‰é¡ºåºå°è¯•ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…ä¸ºæ­¢ã€‚Nginxå°†ä¼šæŠ•é€’ä¸å®¢æˆ·ç«¯ç»™å®šURIåŒ¹é…çš„ä»»ä½•æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é…ç½®æ–‡ä»¶ï¼Œå°†ä¼šæŠŠè¯·æ±‚ä»£ç†åˆ°Apacheä½œè¿›ä¸€æ­¥å¤„ç†ã€‚\n2.2. å¤šä¸ªupstreamæœåŠ¡å™¨ Nginxå°†ä¼šé€šè¿‡è½®è¯¢çš„æ–¹å¼å°†è¿ç»­è¯·æ±‚ä¼ é€’ç»™3ä¸ªä¸Šæ¸¸æœåŠ¡å™¨ã€‚è¿™æ ·åº”ç”¨ç¨‹åºä¸ä¼šè¿‡è½½ã€‚\n3. è´Ÿè½½å‡è¡¡ç‰¹åˆ«è¯´æ˜ å¦‚æœå®¢æˆ·ç«¯å¸Œæœ›æ€»æ˜¯è®¿é—®åŒä¸€ä¸ªä¸Šæ¸¸æœåŠ¡å™¨ï¼Œå¯ä»¥ä½¿ç”¨ip_hashæŒ‡ä»¤ï¼› å¦‚æœè¯·æ±‚å“åº”æ—¶é—´é•¿çŸ­ä¸ä¸€ï¼Œå¯ä»¥ä½¿ç”¨least_connæŒ‡ä»¤ï¼› é»˜è®¤ä¸ºè½®è¯¢ã€‚ ","categories":"","description":"","excerpt":"1. åå‘ä»£ç†ç®€ä»‹ Nginxå¯ä»¥ä½œä¸ºåå‘ä»£ç†ï¼Œæ¥æ”¶å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼Œå¹¶å‘ä¸Šæ¸¸æœåŠ¡å™¨å‘èµ·æ–°çš„è¯·æ±‚ã€‚è¯¥è¯·æ±‚å¯ä»¥æ ¹æ®å®¢æˆ·ç«¯è¯·æ±‚çš„URIï¼Œå®¢æˆ·æœºå‚æ•°æˆ– â€¦","ref":"/linux-notes/nginx/nginx-proxy/","tags":["Nginx"],"title":"Nginxä½œä¸ºåå‘ä»£ç†"},{"body":"1. beeå·¥å…· beeå·¥å…·ç”¨æ¥è¿›è¡Œbeegoé¡¹ç›®çš„åˆ›å»ºã€çƒ­ç¼–è¯‘ã€å¼€å‘ã€æµ‹è¯•ã€å’Œéƒ¨ç½²ã€‚\nå®‰è£…:\ngo get github.com/beego/bee é…ç½®ï¼š\nå®‰è£…å®Œä¹‹åï¼Œbeeå¯æ‰§è¡Œæ–‡ä»¶é»˜è®¤å­˜æ”¾åœ¨$GOPATH/biné‡Œé¢ï¼Œæ‰€ä»¥è¦æŠŠ$GOPATH/binæ·»åŠ åˆ°ç¯å¢ƒå˜é‡ä¸­ã€‚\n2. beeå‘½ä»¤ Bee is a tool for managing beego framework. Usage: bee command [arguments] The commands are: new create an application base on beego framework run run the app which can hot compile pack compress an beego project api create an api application base on beego framework bale packs non-Go files to Go source files version show the bee \u0026 beego version generate source code generator migrate run database migrations è¯´æ˜ï¼š\n2.1. new åœ¨ $GOPATH/srcçš„ç›®å½•ä¸‹æ‰§è¡Œbee new \u003cappname\u003eï¼Œä¼šåœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š\nmyproject â”œâ”€â”€ conf â”‚ â””â”€â”€ app.conf â”œâ”€â”€ controllers â”‚ â””â”€â”€ default.go â”œâ”€â”€ main.go â”œâ”€â”€ models â”œâ”€â”€ routers â”‚ â””â”€â”€ router.go â”œâ”€â”€ static â”‚ â”œâ”€â”€ css â”‚ â”œâ”€â”€ img â”‚ â””â”€â”€ js â”œâ”€â”€ tests â”‚ â””â”€â”€ default_test.go â””â”€â”€ views â””â”€â”€ index.tpl 2.2. run å¿…é¡»åœ¨$GOPATH/src/appnameä¸‹æ‰§è¡Œbee runï¼Œé»˜è®¤ç›‘å¬8080ç«¯å£ï¼šhttp://localhost:8080/ã€‚\n2.3. api api å‘½ä»¤å°±æ˜¯ç”¨æ¥åˆ›å»º API åº”ç”¨ï¼Œç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼šå’Œ Web é¡¹ç›®ç›¸æ¯”ï¼Œå°‘äº† static å’Œ views ç›®å½•ï¼Œå¤šäº†ä¸€ä¸ª test æ¨¡å—ï¼Œç”¨æ¥åšå•å…ƒæµ‹è¯•ã€‚\napiproject â”œâ”€â”€ conf â”‚ â””â”€â”€ app.conf â”œâ”€â”€ controllers â”‚ â””â”€â”€ object.go â”‚ â””â”€â”€ user.go â”œâ”€â”€ docs â”‚ â””â”€â”€ doc.go â”œâ”€â”€ main.go â”œâ”€â”€ models â”‚ â””â”€â”€ object.go â”‚ â””â”€â”€ user.go â”œâ”€â”€ routers â”‚ â””â”€â”€ router.go â””â”€â”€ tests â””â”€â”€ default_test.go 2.4. pack pack ç›®å½•ç”¨æ¥å‘å¸ƒåº”ç”¨çš„æ—¶å€™æ‰“åŒ…ï¼Œä¼šæŠŠé¡¹ç›®æ‰“åŒ…æˆ zip åŒ…(apiproject.tar.gz)ï¼Œè¿™æ ·æˆ‘ä»¬éƒ¨ç½²çš„æ—¶å€™ç›´æ¥æŠŠæ‰“åŒ…ä¹‹åçš„é¡¹ç›®ä¸Šä¼ ï¼Œè§£å‹å°±å¯ä»¥éƒ¨ç½²äº†ï¼š\n2.5. generate ç”¨æ¥è‡ªåŠ¨åŒ–çš„ç”Ÿæˆä»£ç çš„ï¼ŒåŒ…å«äº†ä»æ•°æ®åº“ä¸€é”®ç”Ÿæˆmodelï¼Œè¿˜åŒ…å«äº†scaffoldã€‚\n2.6. migrate è¿™ä¸ªå‘½ä»¤æ˜¯åº”ç”¨çš„æ•°æ®åº“è¿ç§»å‘½ä»¤ï¼Œä¸»è¦æ˜¯ç”¨æ¥æ¯æ¬¡åº”ç”¨å‡çº§ï¼Œé™çº§çš„SQLç®¡ç†ã€‚\nå‚è€ƒï¼š\nhttps://beego.me/docs/install/bee.md ","categories":"","description":"","excerpt":"1. beeå·¥å…· beeå·¥å…·ç”¨æ¥è¿›è¡Œbeegoé¡¹ç›®çš„åˆ›å»ºã€çƒ­ç¼–è¯‘ã€å¼€å‘ã€æµ‹è¯•ã€å’Œéƒ¨ç½²ã€‚\nå®‰è£…:\ngo get â€¦","ref":"/golang-notes/web/beego/bee/","tags":["Golang"],"title":"Bee å·¥å…·ä½¿ç”¨"},{"body":"1. GDBç®€ä»‹ GDBæ˜¯FSF(è‡ªç”±è½¯ä»¶åŸºé‡‘ä¼š)å‘å¸ƒçš„ä¸€ä¸ªå¼ºå¤§çš„ç±»UNIXç³»ç»Ÿä¸‹çš„ç¨‹åºè°ƒè¯•å·¥å…·ã€‚ä½¿ç”¨GDBå¯ä»¥åšå¦‚ä¸‹äº‹æƒ…ï¼š\nå¯åŠ¨ç¨‹åºï¼Œå¯ä»¥æŒ‰ç…§å¼€å‘è€…çš„è‡ªå®šä¹‰è¦æ±‚è¿è¡Œç¨‹åºã€‚ å¯è®©è¢«è°ƒè¯•çš„ç¨‹åºåœ¨å¼€å‘è€…è®¾å®šçš„è°ƒç½®çš„æ–­ç‚¹å¤„åœä½ã€‚ï¼ˆæ–­ç‚¹å¯ä»¥æ˜¯æ¡ä»¶è¡¨è¾¾å¼ï¼‰ å½“ç¨‹åºè¢«åœä½æ—¶ï¼Œå¯ä»¥æ£€æŸ¥æ­¤æ—¶ç¨‹åºä¸­æ‰€å‘ç”Ÿçš„äº‹ã€‚ åŠ¨æ€çš„æ”¹å˜å½“å‰ç¨‹åºçš„æ‰§è¡Œç¯å¢ƒã€‚ ç›®å‰æ”¯æŒè°ƒè¯•Goç¨‹åºçš„GDBç‰ˆæœ¬å¿…é¡»å¤§äº7.1ã€‚\nç¼–è¯‘Goç¨‹åºçš„æ—¶å€™éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹\nä¼ é€’å‚æ•°-ldflags \"-s\"ï¼Œå¿½ç•¥debugçš„æ‰“å°ä¿¡æ¯ ä¼ é€’-gcflags \"-N -l\" å‚æ•°ï¼Œè¿™æ ·å¯ä»¥å¿½ç•¥Goå†…éƒ¨åšçš„ä¸€äº›ä¼˜åŒ–ï¼Œèšåˆå˜é‡å’Œå‡½æ•°ç­‰ä¼˜åŒ–ï¼Œè¿™æ ·å¯¹äºGDBè°ƒè¯•æ¥è¯´éå¸¸å›°éš¾ï¼Œæ‰€ä»¥åœ¨ç¼–è¯‘çš„æ—¶å€™åŠ å…¥è¿™ä¸¤ä¸ªå‚æ•°é¿å…è¿™äº›ä¼˜åŒ–ã€‚ 2. å¸¸ç”¨å‘½ä»¤ 2.1. list ç®€å†™å‘½ä»¤lï¼Œç”¨æ¥æ˜¾ç¤ºæºä»£ç ï¼Œé»˜è®¤æ˜¾ç¤ºåè¡Œä»£ç ï¼Œåé¢å¯ä»¥å¸¦ä¸Šå‚æ•°æ˜¾ç¤ºçš„å…·ä½“è¡Œï¼Œä¾‹å¦‚ï¼šlist 15ï¼Œæ˜¾ç¤ºåè¡Œä»£ç ï¼Œå…¶ä¸­ç¬¬15è¡Œåœ¨æ˜¾ç¤ºçš„åè¡Œé‡Œé¢çš„ä¸­é—´ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚\n10\ttime.Sleep(2 * time.Second) 11\tc \u003c- i 12\t} 13\tclose(c) 14\t} 15\t16\tfunc main() { 17\tmsg := \"Starting main\" 18\tfmt.Println(msg) 19\tbus := make(chan int) 2.2. break ç®€å†™å‘½ä»¤ b,ç”¨æ¥è®¾ç½®æ–­ç‚¹ï¼Œåé¢è·Ÿä¸Šå‚æ•°è®¾ç½®æ–­ç‚¹çš„è¡Œæ•°ï¼Œä¾‹å¦‚b 10åœ¨ç¬¬åè¡Œè®¾ç½®æ–­ç‚¹ã€‚\n2.3. delete ç®€å†™å‘½ä»¤ d,ç”¨æ¥åˆ é™¤æ–­ç‚¹ï¼Œåé¢è·Ÿä¸Šæ–­ç‚¹è®¾ç½®çš„åºå·ï¼Œè¿™ä¸ªåºå·å¯ä»¥é€šè¿‡info breakpointsè·å–ç›¸åº”çš„è®¾ç½®çš„æ–­ç‚¹åºå·ï¼Œå¦‚ä¸‹æ˜¯æ˜¾ç¤ºçš„è®¾ç½®æ–­ç‚¹åºå·ã€‚\nNum Type Disp Enb Address What 2 breakpoint keep y 0x0000000000400dc3 in main.main at /home/xiemengjun/gdb.go:23 breakpoint already hit 1 time 2.4. backtrace ç®€å†™å‘½ä»¤ bt,ç”¨æ¥æ‰“å°æ‰§è¡Œçš„ä»£ç è¿‡ç¨‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n#0 main.main () at /home/xiemengjun/gdb.go:23 #1 0x000000000040d61e in runtime.main () at /home/xiemengjun/go/src/pkg/runtime/proc.c:244 #2 0x000000000040d6c1 in schedunlock () at /home/xiemengjun/go/src/pkg/runtime/proc.c:267 #3 0x0000000000000000 in ?? () 2.5. info infoå‘½ä»¤ç”¨æ¥æ˜¾ç¤ºä¿¡æ¯ï¼Œåé¢æœ‰å‡ ç§å‚æ•°ï¼Œæˆ‘ä»¬å¸¸ç”¨çš„æœ‰å¦‚ä¸‹å‡ ç§ï¼š\n1ã€ info locals\næ˜¾ç¤ºå½“å‰æ‰§è¡Œçš„ç¨‹åºä¸­çš„å˜é‡å€¼\n2ã€ info breakpoints\næ˜¾ç¤ºå½“å‰è®¾ç½®çš„æ–­ç‚¹åˆ—è¡¨\n3ã€ info goroutines\næ˜¾ç¤ºå½“å‰æ‰§è¡Œçš„goroutineåˆ—è¡¨ï¼Œå¦‚ä¸‹ä»£ç æ‰€ç¤º,å¸¦*çš„è¡¨ç¤ºå½“å‰æ‰§è¡Œçš„\n* 1 running runtime.gosched * 2 syscall runtime.entersyscall 3 waiting runtime.gosched 4 runnable runtime.gosched 2.6. print ç®€å†™å‘½ä»¤pï¼Œç”¨æ¥æ‰“å°å˜é‡æˆ–è€…å…¶ä»–ä¿¡æ¯ï¼Œåé¢è·Ÿä¸Šéœ€è¦æ‰“å°çš„å˜é‡åï¼Œå½“ç„¶è¿˜æœ‰ä¸€äº›å¾ˆæœ‰ç”¨çš„å‡½æ•°$len()å’Œ$cap()ï¼Œç”¨æ¥è¿”å›å½“å‰stringã€slicesæˆ–è€…mapsçš„é•¿åº¦å’Œå®¹é‡ã€‚\n2.7. whatis ç”¨æ¥æ˜¾ç¤ºå½“å‰å˜é‡çš„ç±»å‹ï¼Œåé¢è·Ÿä¸Šå˜é‡åï¼Œä¾‹å¦‚whatis msg,æ˜¾ç¤ºå¦‚ä¸‹ï¼š\ntype = struct string 2.8. next ç®€å†™å‘½ä»¤ n,ç”¨æ¥å•æ­¥è°ƒè¯•ï¼Œè·³åˆ°ä¸‹ä¸€æ­¥ï¼Œå½“æœ‰æ–­ç‚¹ä¹‹åï¼Œå¯ä»¥è¾“å…¥nè·³è½¬åˆ°ä¸‹ä¸€æ­¥ç»§ç»­æ‰§è¡Œ\n2.9. coutinue ç®€ç§°å‘½ä»¤ cï¼Œç”¨æ¥è·³å‡ºå½“å‰æ–­ç‚¹å¤„ï¼Œåé¢å¯ä»¥è·Ÿå‚æ•°Nï¼Œè·³è¿‡å¤šå°‘æ¬¡æ–­ç‚¹\n2.10. set variable è¯¥å‘½ä»¤ç”¨æ¥æ”¹å˜è¿è¡Œè¿‡ç¨‹ä¸­çš„å˜é‡å€¼ï¼Œæ ¼å¼å¦‚ï¼šset variable \u003cvar\u003e=\u003cvalue\u003e\n3. è°ƒè¯•è¿‡ç¨‹ 3.1. ç¤ºä¾‹ä»£ç  package main import ( \"fmt\" \"time\" ) func counting(c chan\u003c- int) { for i := 0; i \u003c 10; i++ { time.Sleep(2 * time.Second) c \u003c- i } close(c) } func main() { msg := \"Starting main\" fmt.Println(msg) bus := make(chan int) msg = \"starting a gofunc\" go counting(bus) for count := range bus { fmt.Println(\"count:\", count) } } 3.2. è°ƒè¯•æ­¥éª¤ ç¼–è¯‘æ–‡ä»¶ï¼Œç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶gdbfile:\ngo build -gcflags \"-N -l\" gdbfile.go é€šè¿‡gdbå‘½ä»¤å¯åŠ¨è°ƒè¯•ï¼š\ngdb gdbfile å¯åŠ¨ä¹‹åé¦–å…ˆçœ‹çœ‹è¿™ä¸ªç¨‹åºæ˜¯ä¸æ˜¯å¯ä»¥è¿è¡Œèµ·æ¥ï¼Œåªè¦è¾“å…¥runå‘½ä»¤å›è½¦åç¨‹åºå°±å¼€å§‹è¿è¡Œï¼Œç¨‹åºæ­£å¸¸çš„è¯å¯ä»¥çœ‹åˆ°ç¨‹åºè¾“å‡ºå¦‚ä¸‹ï¼Œå’Œæˆ‘ä»¬åœ¨å‘½ä»¤è¡Œç›´æ¥æ‰§è¡Œç¨‹åºè¾“å‡ºæ˜¯ä¸€æ ·çš„ï¼š\n(gdb) run Starting program: /home/xiemengjun/gdbfile Starting main count: 0 count: 1 count: 2 count: 3 count: 4 count: 5 count: 6 count: 7 count: 8 count: 9 [LWP 2771 exited] [Inferior 1 (process 2771) exited normally] å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»çŸ¥é“æ€ä¹ˆè®©ç¨‹åºè·‘èµ·æ¥äº†ï¼Œæ¥ä¸‹æ¥å¼€å§‹ç»™ä»£ç è®¾ç½®æ–­ç‚¹ï¼š\n(gdb) b 23 Breakpoint 1 at 0x400d8d: file /home/xiemengjun/gdbfile.go, line 23. (gdb) run Starting program: /home/xiemengjun/gdbfile Starting main [New LWP 3284] [Switching to LWP 3284] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) ä¸Šé¢ä¾‹å­b 23è¡¨ç¤ºåœ¨ç¬¬23è¡Œè®¾ç½®äº†æ–­ç‚¹ï¼Œä¹‹åè¾“å…¥runå¼€å§‹è¿è¡Œç¨‹åºã€‚ç°åœ¨ç¨‹åºåœ¨å‰é¢è®¾ç½®æ–­ç‚¹çš„åœ°æ–¹åœä½äº†ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥çœ‹æ–­ç‚¹ç›¸åº”ä¸Šä¸‹æ–‡çš„æºç ï¼Œè¾“å…¥listå°±å¯ä»¥çœ‹åˆ°æºç æ˜¾ç¤ºä»å½“å‰åœæ­¢è¡Œçš„å‰äº”è¡Œå¼€å§‹ï¼š\n(gdb) list 18 fmt.Println(msg) 19 bus := make(chan int) 20 msg = \"starting a gofunc\" 21 go counting(bus) 22 for count := range bus { 23 fmt.Println(\"count:\", count) 24 } 25 } ç°åœ¨GDBåœ¨è¿è¡Œå½“å‰çš„ç¨‹åºçš„ç¯å¢ƒä¸­å·²ç»ä¿ç•™äº†ä¸€äº›æœ‰ç”¨çš„è°ƒè¯•ä¿¡æ¯ï¼Œæˆ‘ä»¬åªéœ€æ‰“å°å‡ºç›¸åº”çš„å˜é‡ï¼ŒæŸ¥çœ‹ç›¸åº”å˜é‡çš„ç±»å‹åŠå€¼ï¼š\n(gdb) info locals count = 0 bus = 0xf840001a50 (gdb) p count $1 = 0 (gdb) p bus $2 = (chan int) 0xf840001a50 (gdb) whatis bus type = chan int æ¥ä¸‹æ¥è¯¥è®©ç¨‹åºç»§ç»­å¾€ä¸‹æ‰§è¡Œï¼Œè¯·ç»§ç»­çœ‹ä¸‹é¢çš„å‘½ä»¤\n(gdb) c Continuing. count: 0 [New LWP 3303] [Switching to LWP 3303] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) (gdb) c Continuing. count: 1 [Switching to LWP 3302] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) æ¯æ¬¡è¾“å…¥cä¹‹åéƒ½ä¼šæ‰§è¡Œä¸€æ¬¡ä»£ç ï¼Œåˆè·³åˆ°ä¸‹ä¸€æ¬¡forå¾ªç¯ï¼Œç»§ç»­æ‰“å°å‡ºæ¥ç›¸åº”çš„ä¿¡æ¯ã€‚è®¾æƒ³ç›®å‰éœ€è¦æ”¹å˜ä¸Šä¸‹æ–‡ç›¸å…³å˜é‡çš„ä¿¡æ¯ï¼Œè·³è¿‡ä¸€äº›è¿‡ç¨‹ï¼Œå¹¶ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥ï¼Œå¾—å‡ºä¿®æ”¹åæƒ³è¦çš„ç»“æœï¼š\n(gdb) info locals count = 2 bus = 0xf840001a50 (gdb) set variable count=9 (gdb) info locals count = 9 bus = 0xf840001a50 (gdb) c Continuing. count: 9 [Switching to LWP 3302] Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(\"count:\", count) æœ€åç¨å¾®æ€è€ƒä¸€ä¸‹ï¼Œå‰é¢æ•´ä¸ªç¨‹åºè¿è¡Œçš„è¿‡ç¨‹ä¸­åˆ°åº•åˆ›å»ºäº†å¤šå°‘ä¸ªgoroutineï¼Œæ¯ä¸ªgoroutineéƒ½åœ¨åšä»€ä¹ˆï¼š\n(gdb) info goroutines * 1 running runtime.gosched * 2 syscall runtime.entersyscall 3 waiting runtime.gosched 4 runnable runtime.gosched (gdb) goroutine 1 bt #0 0x000000000040e33b in runtime.gosched () at /home/xiemengjun/go/src/pkg/runtime/proc.c:927 #1 0x0000000000403091 in runtime.chanrecv (c=void, ep=void, selected=void, received=void) at /home/xiemengjun/go/src/pkg/runtime/chan.c:327 #2 0x000000000040316f in runtime.chanrecv2 (t=void, c=void) at /home/xiemengjun/go/src/pkg/runtime/chan.c:420 #3 0x0000000000400d6f in main.main () at /home/xiemengjun/gdbfile.go:22 #4 0x000000000040d0c7 in runtime.main () at /home/xiemengjun/go/src/pkg/runtime/proc.c:244 #5 0x000000000040d16a in schedunlock () at /home/xiemengjun/go/src/pkg/runtime/proc.c:267 #6 0x0000000000000000 in ?? () é€šè¿‡æŸ¥çœ‹goroutinesçš„å‘½ä»¤æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°äº†è§£goruntineå†…éƒ¨æ˜¯æ€ä¹ˆæ‰§è¡Œçš„ï¼Œæ¯ä¸ªå‡½æ•°çš„è°ƒç”¨é¡ºåºå·²ç»æ˜æ˜ç™½ç™½åœ°æ˜¾ç¤ºå‡ºæ¥äº†ã€‚\nå‚è€ƒã€ŠGo Webç¼–ç¨‹ã€‹\n","categories":"","description":"","excerpt":"1. GDBç®€ä»‹ GDBæ˜¯FSF(è‡ªç”±è½¯ä»¶åŸºé‡‘ä¼š)å‘å¸ƒçš„ä¸€ä¸ªå¼ºå¤§çš„ç±»UNIXç³»ç»Ÿä¸‹çš„ç¨‹åºè°ƒè¯•å·¥å…·ã€‚ä½¿ç”¨GDBå¯ä»¥åšå¦‚ä¸‹äº‹æƒ…ï¼š\nå¯åŠ¨ç¨‹åºï¼Œå¯ä»¥ â€¦","ref":"/golang-notes/test/gdb/","tags":["Golang"],"title":"GDBè°ƒè¯•"},{"body":"1. Controller Managerç®€ä»‹ Controller Managerä½œä¸ºé›†ç¾¤å†…éƒ¨çš„ç®¡ç†æ§åˆ¶ä¸­å¿ƒï¼Œè´Ÿè´£é›†ç¾¤å†…çš„Nodeã€Podå‰¯æœ¬ã€æœåŠ¡ç«¯ç‚¹ï¼ˆEndpointï¼‰ã€å‘½åç©ºé—´ï¼ˆNamespaceï¼‰ã€æœåŠ¡è´¦å·ï¼ˆServiceAccountï¼‰ã€èµ„æºå®šé¢ï¼ˆResourceQuotaï¼‰çš„ç®¡ç†ï¼Œå½“æŸä¸ªNodeæ„å¤–å®•æœºæ—¶ï¼ŒController Managerä¼šåŠæ—¶å‘ç°å¹¶æ‰§è¡Œè‡ªåŠ¨åŒ–ä¿®å¤æµç¨‹ï¼Œç¡®ä¿é›†ç¾¤å§‹ç»ˆå¤„äºé¢„æœŸçš„å·¥ä½œçŠ¶æ€ã€‚\næ¯ä¸ªControlleré€šè¿‡API Serveræä¾›çš„æ¥å£å®æ—¶ç›‘æ§æ•´ä¸ªé›†ç¾¤çš„æ¯ä¸ªèµ„æºå¯¹è±¡çš„å½“å‰çŠ¶æ€ï¼Œå½“å‘ç”Ÿå„ç§æ•…éšœå¯¼è‡´ç³»ç»ŸçŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶ï¼Œä¼šå°è¯•å°†ç³»ç»ŸçŠ¶æ€ä¿®å¤åˆ°â€œæœŸæœ›çŠ¶æ€â€ã€‚\n2. Replication Controller ä¸ºäº†åŒºåˆ†ï¼Œå°†èµ„æºå¯¹è±¡Replication Controllerç®€ç§°RC,è€Œæœ¬æ–‡ä¸­æ˜¯æŒ‡Controller Managerä¸­çš„Replication Controllerï¼Œç§°ä¸ºå‰¯æœ¬æ§åˆ¶å™¨ã€‚å‰¯æœ¬æ§åˆ¶å™¨çš„ä½œç”¨å³ä¿è¯é›†ç¾¤ä¸­ä¸€ä¸ªRCæ‰€å…³è”çš„Podå‰¯æœ¬æ•°å§‹ç»ˆä¿æŒé¢„è®¾å€¼ã€‚\nåªæœ‰å½“Podçš„é‡å¯ç­–ç•¥æ˜¯Alwaysçš„æ—¶å€™ï¼ˆRestartPolicy=Alwaysï¼‰ï¼Œå‰¯æœ¬æ§åˆ¶å™¨æ‰ä¼šç®¡ç†è¯¥Podçš„æ“ä½œï¼ˆåˆ›å»ºã€é”€æ¯ã€é‡å¯ç­‰ï¼‰ã€‚ RCä¸­çš„Podæ¨¡æ¿å°±åƒä¸€ä¸ªæ¨¡å…·ï¼Œæ¨¡å…·åˆ¶é€ å‡ºæ¥çš„ä¸œè¥¿ä¸€æ—¦ç¦»å¼€æ¨¡å…·ï¼Œå®ƒä»¬ä¹‹é—´å°±å†æ²¡å…³ç³»äº†ã€‚ä¸€æ—¦Podè¢«åˆ›å»ºï¼Œæ— è®ºæ¨¡æ¿å¦‚ä½•å˜åŒ–ï¼Œä¹Ÿä¸ä¼šå½±å“åˆ°å·²ç»åˆ›å»ºçš„Podã€‚ Podå¯ä»¥é€šè¿‡ä¿®æ”¹labelæ¥è„±ç¦»RCçš„ç®¡æ§ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”¨äºå°†Podä»é›†ç¾¤ä¸­è¿ç§»ï¼Œæ•°æ®ä¿®å¤ç­‰è°ƒè¯•ã€‚ åˆ é™¤ä¸€ä¸ªRCä¸ä¼šå½±å“å®ƒæ‰€åˆ›å»ºçš„Podï¼Œå¦‚æœè¦åˆ é™¤Podéœ€è¦å°†RCçš„å‰¯æœ¬æ•°å±æ€§è®¾ç½®ä¸º0ã€‚ ä¸è¦è¶Šè¿‡RCåˆ›å»ºPodï¼Œå› ä¸ºRCå¯ä»¥å®ç°è‡ªåŠ¨åŒ–æ§åˆ¶Podï¼Œæé«˜å®¹ç¾èƒ½åŠ›ã€‚ 2.1. Replication Controllerçš„èŒè´£ ç¡®ä¿é›†ç¾¤ä¸­æœ‰ä¸”ä»…æœ‰Nä¸ªPodå®ä¾‹ï¼ŒNæ˜¯RCä¸­å®šä¹‰çš„Podå‰¯æœ¬æ•°é‡ã€‚ é€šè¿‡è°ƒæ•´RCä¸­çš„spec.replicaså±æ€§å€¼æ¥å®ç°ç³»ç»Ÿæ‰©å®¹æˆ–ç¼©å®¹ã€‚ é€šè¿‡æ”¹å˜RCä¸­çš„Podæ¨¡æ¿æ¥å®ç°ç³»ç»Ÿçš„æ»šåŠ¨å‡çº§ã€‚ 2.2. Replication Controllerä½¿ç”¨åœºæ™¯ ä½¿ç”¨åœºæ™¯ è¯´æ˜ ä½¿ç”¨å‘½ä»¤ é‡æ–°è°ƒåº¦ å½“å‘ç”ŸèŠ‚ç‚¹æ•…éšœæˆ–Podè¢«æ„å¤–ç»ˆæ­¢è¿è¡Œæ—¶ï¼Œå¯ä»¥é‡æ–°è°ƒåº¦ä¿è¯é›†ç¾¤ä¸­ä»ç„¶è¿è¡ŒæŒ‡å®šçš„å‰¯æœ¬æ•°ã€‚ å¼¹æ€§ä¼¸ç¼© é€šè¿‡æ‰‹åŠ¨æˆ–è‡ªåŠ¨æ‰©å®¹ä»£ç†ä¿®å¤å‰¯æœ¬æ§åˆ¶å™¨çš„spec.replicaså±æ€§ï¼Œå¯ä»¥å®ç°å¼¹æ€§ä¼¸ç¼©ã€‚ kubectl scale æ»šåŠ¨æ›´æ–° åˆ›å»ºä¸€ä¸ªæ–°çš„RCæ–‡ä»¶ï¼Œé€šè¿‡kubectl å‘½ä»¤æˆ–APIæ‰§è¡Œï¼Œåˆ™ä¼šæ–°å¢ä¸€ä¸ªæ–°çš„å‰¯æœ¬åŒæ—¶åˆ é™¤æ—§çš„å‰¯æœ¬ï¼Œå½“æ—§å‰¯æœ¬ä¸º0æ—¶ï¼Œåˆ é™¤æ—§çš„RCã€‚ kubectl rolling-update æ»šåŠ¨å‡çº§ï¼Œå…·ä½“å¯å‚è€ƒkubectl rolling-update --help,å®˜æ–¹æ–‡æ¡£ï¼šhttps://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/\n3. Node Controller kubeletåœ¨å¯åŠ¨æ—¶ä¼šé€šè¿‡API Serveræ³¨å†Œè‡ªèº«çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œå¹¶å®šæ—¶å‘API Serveræ±‡æŠ¥çŠ¶æ€ä¿¡æ¯ï¼ŒAPI Serveræ¥æ”¶åˆ°ä¿¡æ¯åå°†ä¿¡æ¯æ›´æ–°åˆ°etcdä¸­ã€‚\nNode Controlleré€šè¿‡API Serverå®æ—¶è·å–Nodeçš„ç›¸å…³ä¿¡æ¯ï¼Œå®ç°ç®¡ç†å’Œç›‘æ§é›†ç¾¤ä¸­çš„å„ä¸ªNodeèŠ‚ç‚¹çš„ç›¸å…³æ§åˆ¶åŠŸèƒ½ã€‚æµç¨‹å¦‚ä¸‹\n1ã€Controller Manageråœ¨å¯åŠ¨æ—¶å¦‚æœè®¾ç½®äº†--cluster-cidrå‚æ•°ï¼Œé‚£ä¹ˆä¸ºæ¯ä¸ªæ²¡æœ‰è®¾ç½®Spec.PodCIDRçš„NodeèŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªCIDRåœ°å€ï¼Œå¹¶ç”¨è¯¥CIDRåœ°å€è®¾ç½®èŠ‚ç‚¹çš„Spec.PodCIDRå±æ€§ï¼Œé˜²æ­¢ä¸åŒçš„èŠ‚ç‚¹çš„CIDRåœ°å€å‘ç”Ÿå†²çªã€‚\n2ã€å…·ä½“æµç¨‹è§ä»¥ä¸Šæµç¨‹å›¾ã€‚\n3ã€é€ä¸ªè¯»å–èŠ‚ç‚¹ä¿¡æ¯ï¼Œå¦‚æœèŠ‚ç‚¹çŠ¶æ€å˜æˆéâ€œå°±ç»ªâ€çŠ¶æ€ï¼Œåˆ™å°†èŠ‚ç‚¹åŠ å…¥å¾…åˆ é™¤é˜Ÿåˆ—ï¼Œå¦åˆ™å°†èŠ‚ç‚¹ä»è¯¥é˜Ÿåˆ—åˆ é™¤ã€‚\n4. ResourceQuota Controller èµ„æºé…é¢ç®¡ç†ç¡®ä¿æŒ‡å®šçš„èµ„æºå¯¹è±¡åœ¨ä»»ä½•æ—¶å€™éƒ½ä¸ä¼šè¶…é‡å ç”¨ç³»ç»Ÿç‰©ç†èµ„æºã€‚\næ”¯æŒä¸‰ä¸ªå±‚æ¬¡çš„èµ„æºé…ç½®ç®¡ç†ï¼š\n1ï¼‰å®¹å™¨çº§åˆ«ï¼šå¯¹CPUå’ŒMemoryè¿›è¡Œé™åˆ¶\n2ï¼‰Podçº§åˆ«ï¼šå¯¹ä¸€ä¸ªPodå†…æ‰€æœ‰å®¹å™¨çš„å¯ç”¨èµ„æºè¿›è¡Œé™åˆ¶\n3ï¼‰Namespaceçº§åˆ«ï¼šåŒ…æ‹¬\nPodæ•°é‡ Replication Controlleræ•°é‡ Serviceæ•°é‡ ResourceQuotaæ•°é‡ Secretæ•°é‡ å¯æŒæœ‰çš„PVï¼ˆPersistent Volumeï¼‰æ•°é‡ è¯´æ˜ï¼š\nk8sé…é¢ç®¡ç†æ˜¯é€šè¿‡Admission Controlï¼ˆå‡†å…¥æ§åˆ¶ï¼‰æ¥æ§åˆ¶çš„ï¼› Admission Controlæä¾›ä¸¤ç§é…é¢çº¦æŸæ–¹å¼ï¼šLimitRangerå’ŒResourceQuotaï¼› LimitRangerä½œç”¨äºPodå’ŒContainerï¼› ResourceQuotaä½œç”¨äºNamespaceä¸Šï¼Œé™å®šä¸€ä¸ªNamespaceé‡Œçš„å„ç±»èµ„æºçš„ä½¿ç”¨æ€»é¢ã€‚ ResourceQuota Controlleræµç¨‹å›¾ï¼š\n5. Namespace Controller ç”¨æˆ·é€šè¿‡API Serverå¯ä»¥åˆ›å»ºæ–°çš„Namespaceå¹¶ä¿å­˜åœ¨etcdä¸­ï¼ŒNamespace Controllerå®šæ—¶é€šè¿‡API Serverè¯»å–è¿™äº›Namespaceä¿¡æ¯ã€‚\nå¦‚æœNamespaceè¢«APIæ ‡è®°ä¸ºä¼˜é›…åˆ é™¤ï¼ˆå³è®¾ç½®åˆ é™¤æœŸé™ï¼ŒDeletionTimestampï¼‰,åˆ™å°†è¯¥NamespaceçŠ¶æ€è®¾ç½®ä¸ºâ€œTerminatingâ€,å¹¶ä¿å­˜åˆ°etcdä¸­ã€‚åŒæ—¶Namespace Controlleråˆ é™¤è¯¥Namespaceä¸‹çš„ServiceAccountã€RCã€Podç­‰èµ„æºå¯¹è±¡ã€‚\n6. Endpoint Controller Serviceã€Endpointã€Podçš„å…³ç³»ï¼š\nEndpointsè¡¨ç¤ºäº†ä¸€ä¸ªServiceå¯¹åº”çš„æ‰€æœ‰Podå‰¯æœ¬çš„è®¿é—®åœ°å€ï¼Œè€ŒEndpoints Controllerè´Ÿè´£ç”Ÿæˆå’Œç»´æŠ¤æ‰€æœ‰Endpointså¯¹è±¡çš„æ§åˆ¶å™¨ã€‚å®ƒè´Ÿè´£ç›‘å¬Serviceå’Œå¯¹åº”çš„Podå‰¯æœ¬çš„å˜åŒ–ã€‚\nå¦‚æœç›‘æµ‹åˆ°Serviceè¢«åˆ é™¤ï¼Œåˆ™åˆ é™¤å’Œè¯¥ServiceåŒåçš„Endpointså¯¹è±¡ï¼› å¦‚æœç›‘æµ‹åˆ°æ–°çš„Serviceè¢«åˆ›å»ºæˆ–ä¿®æ”¹ï¼Œåˆ™æ ¹æ®è¯¥Serviceä¿¡æ¯è·å¾—ç›¸å…³çš„Podåˆ—è¡¨ï¼Œç„¶ååˆ›å»ºæˆ–æ›´æ–°Serviceå¯¹åº”çš„Endpointså¯¹è±¡ã€‚ å¦‚æœç›‘æµ‹åˆ°Podçš„äº‹ä»¶ï¼Œåˆ™æ›´æ–°å®ƒå¯¹åº”çš„Serviceçš„Endpointså¯¹è±¡ã€‚ kube-proxyè¿›ç¨‹è·å–æ¯ä¸ªServiceçš„Endpointsï¼Œå®ç°Serviceçš„è´Ÿè½½å‡è¡¡åŠŸèƒ½ã€‚\n7. Service Controller Service Controlleræ˜¯å±äºkubernetesé›†ç¾¤ä¸å¤–éƒ¨çš„äº‘å¹³å°ä¹‹é—´çš„ä¸€ä¸ªæ¥å£æ§åˆ¶å™¨ã€‚Service Controllerç›‘å¬Serviceå˜åŒ–ï¼Œå¦‚æœæ˜¯ä¸€ä¸ªLoadBalancerç±»å‹çš„Serviceï¼Œåˆ™ç¡®ä¿å¤–éƒ¨çš„äº‘å¹³å°ä¸Šå¯¹è¯¥Serviceå¯¹åº”çš„LoadBalancerå®ä¾‹è¢«ç›¸åº”åœ°åˆ›å»ºã€åˆ é™¤åŠæ›´æ–°è·¯ç”±è½¬å‘è¡¨ã€‚\nå‚è€ƒã€ŠKubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. Controller Managerç®€ä»‹ Controller Managerä½œä¸ºé›†ç¾¤å†…éƒ¨çš„ç®¡ç†æ§åˆ¶ä¸­å¿ƒï¼Œè´Ÿè´£é›†ç¾¤å†…çš„Nodeã€Pod â€¦","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-controller-manager/","tags":["Kubernetes"],"title":"Kubernetesæ ¸å¿ƒåŸç†ï¼ˆäºŒï¼‰ä¹‹Controller Manager"},{"body":"1. kubernetesç½‘ç»œæ¨¡å‹ 1.1. åŸºç¡€åŸåˆ™ æ¯ä¸ªPodéƒ½æ‹¥æœ‰ä¸€ä¸ªç‹¬ç«‹çš„IPåœ°å€ï¼Œè€Œä¸”å‡å®šæ‰€æœ‰Podéƒ½åœ¨ä¸€ä¸ªå¯ä»¥ç›´æ¥è¿é€šçš„ã€æ‰å¹³çš„ç½‘ç»œç©ºé—´ä¸­ï¼Œä¸ç®¡æ˜¯å¦è¿è¡Œåœ¨åŒä¸€Nodeä¸Šéƒ½å¯ä»¥é€šè¿‡Podçš„IPæ¥è®¿é—®ã€‚ k8sä¸­Podçš„IPæ˜¯æœ€å°ç²’åº¦IPã€‚åŒä¸€ä¸ªPodå†…æ‰€æœ‰çš„å®¹å™¨å…±äº«ä¸€ä¸ªç½‘ç»œå †æ ˆï¼Œè¯¥æ¨¡å‹ç§°ä¸ºIP-per-Podæ¨¡å‹ã€‚ Podç”±docker0å®é™…åˆ†é…çš„IPï¼ŒPodå†…éƒ¨çœ‹åˆ°çš„IPåœ°å€å’Œç«¯å£ä¸å¤–éƒ¨ä¿æŒä¸€è‡´ã€‚åŒä¸€ä¸ªPodå†…çš„ä¸åŒå®¹å™¨å…±äº«ç½‘ç»œï¼Œå¯ä»¥é€šè¿‡localhostæ¥è®¿é—®å¯¹æ–¹çš„ç«¯å£ï¼Œç±»ä¼¼åŒä¸€ä¸ªVMå†…çš„ä¸åŒè¿›ç¨‹ã€‚ IP-per-Podæ¨¡å‹ä»ç«¯å£åˆ†é…ã€åŸŸåè§£æã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡ã€åº”ç”¨é…ç½®ç­‰è§’åº¦çœ‹ï¼ŒPodå¯ä»¥çœ‹ä½œæ˜¯ä¸€å°ç‹¬ç«‹çš„VMæˆ–ç‰©ç†æœºã€‚ 1.2. k8så¯¹é›†ç¾¤çš„ç½‘ç»œè¦æ±‚ æ‰€æœ‰å®¹å™¨éƒ½å¯ä»¥ä¸ç”¨NATçš„æ–¹å¼åŒåˆ«çš„å®¹å™¨é€šä¿¡ã€‚ æ‰€æœ‰èŠ‚ç‚¹éƒ½å¯ä»¥åœ¨ä¸åŒNATçš„æ–¹å¼ä¸‹åŒæ‰€æœ‰å®¹å™¨é€šä¿¡ï¼Œåä¹‹äº¦ç„¶ã€‚ å®¹å™¨çš„åœ°å€å’Œåˆ«äººçœ‹åˆ°çš„åœ°å€æ˜¯åŒä¸€ä¸ªåœ°å€ã€‚ ä»¥ä¸Šçš„é›†ç¾¤ç½‘ç»œè¦æ±‚å¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹å¼€æºæ–¹æ¡ˆå®ç°ï¼Œä¾‹å¦‚flannelã€‚\n1.3. ç½‘ç»œæ¶æ„å›¾ 1.4. k8sé›†ç¾¤IPæ¦‚å¿µæ±‡æ€» ç”±é›†ç¾¤å¤–éƒ¨åˆ°é›†ç¾¤å†…éƒ¨ï¼š\nIPç±»å‹ è¯´æ˜ Proxy-IP ä»£ç†å±‚å…¬ç½‘åœ°å€IPï¼Œå¤–éƒ¨è®¿é—®åº”ç”¨çš„ç½‘å…³æœåŠ¡å™¨ã€‚[å®é™…éœ€è¦å…³æ³¨çš„IP] Service-IP Serviceçš„å›ºå®šè™šæ‹ŸIPï¼ŒService-IPæ˜¯å†…éƒ¨ï¼Œå¤–éƒ¨æ— æ³•å¯»å€åˆ°ã€‚ Node-IP å®¹å™¨å®¿ä¸»æœºçš„ä¸»æœºIPã€‚ Container-Bridge-IP å®¹å™¨ç½‘æ¡¥ï¼ˆdocker0ï¼‰IPï¼Œå®¹å™¨çš„ç½‘ç»œéƒ½éœ€è¦é€šè¿‡å®¹å™¨ç½‘æ¡¥è½¬å‘ã€‚ Pod-IP Podçš„IPï¼Œç­‰æ•ˆäºPodä¸­ç½‘ç»œå®¹å™¨çš„Container-IPã€‚ Container-IP å®¹å™¨çš„IPï¼Œå®¹å™¨çš„ç½‘ç»œæ˜¯ä¸ªéš”ç¦»çš„ç½‘ç»œç©ºé—´ã€‚ 2. kubernetesçš„ç½‘ç»œå®ç° k8sç½‘ç»œåœºæ™¯\nå®¹å™¨ä¸å®¹å™¨ä¹‹é—´çš„ç›´æ¥é€šä¿¡ã€‚ Podä¸Podä¹‹é—´çš„é€šä¿¡ã€‚ Podåˆ°Serviceä¹‹é—´çš„é€šä¿¡ã€‚ é›†ç¾¤å¤–éƒ¨ä¸å†…éƒ¨ç»„ä»¶ä¹‹é—´çš„é€šä¿¡ã€‚ 2.1. Podç½‘ç»œ Podä½œä¸ºkubernetesçš„æœ€å°è°ƒåº¦å•å…ƒï¼ŒPodæ˜¯å®¹å™¨çš„é›†åˆï¼Œæ˜¯ä¸€ä¸ªé€»è¾‘æ¦‚å¿µï¼ŒPodåŒ…å«çš„å®¹å™¨éƒ½è¿è¡Œåœ¨åŒä¸€ä¸ªå®¿ä¸»æœºä¸Šï¼Œè¿™äº›å®¹å™¨å°†æ‹¥æœ‰åŒæ ·çš„ç½‘ç»œç©ºé—´ï¼Œå®¹å™¨ä¹‹é—´èƒ½å¤Ÿäº’ç›¸é€šä¿¡ï¼Œå®ƒä»¬èƒ½å¤Ÿåœ¨æœ¬åœ°è®¿é—®å…¶å®ƒå®¹å™¨çš„ç«¯å£ã€‚ å®é™…ä¸ŠPodéƒ½åŒ…å«ä¸€ä¸ªç½‘ç»œå®¹å™¨ï¼Œå®ƒä¸åšä»»ä½•äº‹æƒ…ï¼Œåªæ˜¯ç”¨æ¥æ¥ç®¡Podçš„ç½‘ç»œï¼Œä¸šåŠ¡å®¹å™¨é€šè¿‡åŠ å…¥ç½‘ç»œå®¹å™¨çš„ç½‘ç»œä»è€Œå®ç°ç½‘ç»œå…±äº«ã€‚Podç½‘ç»œæœ¬è´¨ä¸Šè¿˜æ˜¯å®¹å™¨ç½‘ç»œï¼Œæ‰€ä»¥Pod-IPå°±æ˜¯ç½‘ç»œå®¹å™¨çš„Container-IPã€‚\nä¸€èˆ¬å°†å®¹å™¨äº‘å¹³å°çš„ç½‘ç»œæ¨¡å‹æ‰“é€ æˆä¸€ä¸ªæ‰å¹³åŒ–ç½‘ç»œå¹³é¢ï¼Œåœ¨è¿™ä¸ªç½‘ç»œå¹³é¢å†…ï¼ŒPodä½œä¸ºä¸€ä¸ªç½‘ç»œå•å…ƒåŒKubernetes Nodeçš„ç½‘ç»œå¤„äºåŒä¸€å±‚çº§ã€‚\n2.2. Podå†…éƒ¨å®¹å™¨ä¹‹é—´çš„é€šä¿¡ åŒä¸€ä¸ªPodä¹‹é—´çš„ä¸åŒå®¹å™¨å› ä¸ºå…±äº«åŒä¸€ä¸ªç½‘ç»œå‘½åç©ºé—´ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥é€šè¿‡localhostç›´æ¥é€šä¿¡ã€‚\n2.3. Podä¹‹é—´çš„é€šä¿¡ 2.3.1. åŒNodeçš„Podä¹‹é—´çš„é€šä¿¡ åŒä¸€ä¸ªNodeå†…ï¼Œä¸åŒçš„Podéƒ½æœ‰ä¸€ä¸ªå…¨å±€IPï¼Œå¯ä»¥ç›´æ¥é€šè¿‡Podçš„IPè¿›è¡Œé€šä¿¡ã€‚Podåœ°å€å’Œdocker0åœ¨åŒä¸€ä¸ªç½‘æ®µã€‚\nåœ¨pauseå®¹å™¨å¯åŠ¨ä¹‹å‰ï¼Œä¼šåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿä»¥å¤ªç½‘æ¥å£å¯¹ï¼ˆveth pairï¼‰ï¼Œè¯¥æ¥å£å¯¹ä¸€ç«¯è¿ç€å®¹å™¨å†…éƒ¨çš„eth0 ï¼Œä¸€ç«¯è¿ç€å®¹å™¨å¤–éƒ¨çš„vethxxxï¼Œvethxxxä¼šç»‘å®šåˆ°å®¹å™¨è¿è¡Œæ—¶é…ç½®ä½¿ç”¨çš„ç½‘æ¡¥bridge0ä¸Šï¼Œä»è¯¥ç½‘ç»œçš„IPæ®µä¸­åˆ†é…IPç»™å®¹å™¨çš„eth0ã€‚\nå½“åŒèŠ‚ç‚¹ä¸Šçš„Pod-Aå‘åŒ…ç»™Pod-Bæ—¶ï¼ŒåŒ…ä¼ é€è·¯çº¿å¦‚ä¸‹ï¼š\npod-açš„eth0â€”\u003epod-açš„vethxxxâ€”\u003ebridge0â€”\u003epod-bçš„vethxxxâ€”\u003epod-bçš„eth0 å› ä¸ºç›¸åŒèŠ‚ç‚¹çš„bridge0æ˜¯ç›¸é€šçš„ï¼Œå› æ­¤å¯ä»¥é€šè¿‡bridge0æ¥å®Œæˆä¸åŒpodç›´æ¥çš„é€šä¿¡ï¼Œä½†æ˜¯ä¸åŒèŠ‚ç‚¹çš„bridge0æ˜¯ä¸é€šçš„ï¼Œå› æ­¤ä¸åŒèŠ‚ç‚¹çš„podä¹‹é—´çš„é€šä¿¡éœ€è¦å°†ä¸åŒèŠ‚ç‚¹çš„bridge0ç»™è¿æ¥èµ·æ¥ã€‚\n2.3.2. ä¸åŒNodeçš„Podä¹‹é—´çš„é€šä¿¡ ä¸åŒçš„Nodeä¹‹é—´ï¼ŒNodeçš„IPç›¸å½“äºå¤–ç½‘IPï¼Œå¯ä»¥ç›´æ¥è®¿é—®ï¼Œè€ŒNodeå†…çš„docker0å’ŒPodçš„IPåˆ™æ˜¯å†…ç½‘IPï¼Œæ— æ³•ç›´æ¥è·¨Nodeè®¿é—®ã€‚éœ€è¦é€šè¿‡Nodeçš„ç½‘å¡è¿›è¡Œè½¬å‘ã€‚\næ‰€ä»¥ä¸åŒNodeä¹‹é—´çš„é€šä¿¡éœ€è¦è¾¾åˆ°ä¸¤ä¸ªæ¡ä»¶ï¼š\nå¯¹æ•´ä¸ªé›†ç¾¤ä¸­çš„Pod-IPåˆ†é…è¿›è¡Œè§„åˆ’ï¼Œä¸èƒ½æœ‰å†²çªï¼ˆå¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹å¼€æºå·¥å…·æ¥ç®¡ç†ï¼Œä¾‹å¦‚flannelï¼‰ã€‚ å°†Node-IPä¸è¯¥Nodeä¸Šçš„Pod-IPå…³è”èµ·æ¥ï¼Œé€šè¿‡Node-IPå†è½¬å‘åˆ°Pod-IPã€‚ ä¸åŒèŠ‚ç‚¹çš„Podä¹‹é—´çš„é€šä¿¡éœ€è¦å°†ä¸åŒèŠ‚ç‚¹çš„bridge0ç»™è¿æ¥èµ·æ¥ã€‚è¿æ¥ä¸åŒèŠ‚ç‚¹çš„bridge0çš„æ–¹å¼æœ‰å¥½å‡ ç§ï¼Œä¸»è¦æœ‰overlayå’Œunderlayï¼Œæˆ–å¸¸è§„çš„ä¸‰å±‚è·¯ç”±ã€‚\nä¸åŒèŠ‚ç‚¹çš„bridge0éœ€è¦ä¸åŒçš„IPæ®µï¼Œä¿è¯Pod IPåˆ†é…ä¸ä¼šå†²çªï¼ŒèŠ‚ç‚¹çš„ç‰©ç†ç½‘å¡eth0ä¹Ÿè¦å’Œè¯¥èŠ‚ç‚¹çš„ç½‘æ¡¥bridge0è¿æ¥ã€‚å› æ­¤ï¼ŒèŠ‚ç‚¹aä¸Šçš„pod-aå‘åŒ…ç»™èŠ‚ç‚¹bä¸Šçš„pod-bï¼Œè·¯çº¿å¦‚ä¸‹ï¼š\nèŠ‚ç‚¹aä¸Šçš„pod-açš„eth0â€”\u003epod-açš„vethxxxâ€”\u003eèŠ‚ç‚¹açš„bridge0â€”\u003eèŠ‚ç‚¹açš„eth0â€”\u003e èŠ‚ç‚¹bçš„eth0â€”\u003eèŠ‚ç‚¹bçš„bridge0â€”\u003epod-bçš„vethxxxâ€”\u003epod-bçš„eth0 1. Podé—´å®ç°é€šä¿¡\nä¾‹å¦‚ï¼šPod1å’ŒPod2ï¼ˆåŒä¸»æœºï¼‰ï¼ŒPod1å’ŒPod3(è·¨ä¸»æœº)èƒ½å¤Ÿé€šä¿¡\nå®ç°ï¼šå› ä¸ºPodçš„Pod-IPæ˜¯Dockerç½‘æ¡¥åˆ†é…çš„ï¼ŒPod-IPæ˜¯åŒNodeä¸‹å…¨å±€å”¯ä¸€çš„ã€‚æ‰€ä»¥å°†ä¸åŒKubernetes Nodeçš„ Dockerç½‘æ¡¥é…ç½®æˆä¸åŒçš„IPç½‘æ®µå³å¯ã€‚\n2. Nodeä¸Podé—´å®ç°é€šä¿¡\nä¾‹å¦‚ï¼šNode1å’ŒPod1/ Pod2(åŒä¸»æœº)ï¼ŒPod3(è·¨ä¸»æœº)èƒ½å¤Ÿé€šä¿¡\nå®ç°ï¼šåœ¨å®¹å™¨é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ªè¦†ç›–ç½‘ç»œ(Overlay Network)ï¼Œè”é€šå„ä¸ªèŠ‚ç‚¹ï¼Œç›®å‰å¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹ç½‘ç»œæ’ä»¶æ¥åˆ›å»ºè¦†ç›–ç½‘ç»œï¼Œæ¯”å¦‚Flannelå’ŒOpen vSwitchç­‰ã€‚\nä¸åŒèŠ‚ç‚¹é—´çš„Podè®¿é—®ä¹Ÿå¯ä»¥é€šè¿‡calicoå½¢æˆçš„Pod IPçš„è·¯ç”±è¡¨æ¥è§£å†³ã€‚\n2.4. Serviceç½‘ç»œ Serviceçš„å°±æ˜¯åœ¨Podä¹‹é—´èµ·åˆ°æœåŠ¡ä»£ç†çš„ä½œç”¨ï¼Œå¯¹å¤–è¡¨ç°ä¸ºä¸€ä¸ªå•ä¸€è®¿é—®æ¥å£ï¼Œå°†è¯·æ±‚è½¬å‘ç»™Podï¼ŒServiceçš„ç½‘ç»œè½¬å‘æ˜¯Kuberneteså®ç°æœåŠ¡ç¼–æ’çš„å…³é”®ä¸€ç¯ã€‚Serviceéƒ½ä¼šç”Ÿæˆä¸€ä¸ªè™šæ‹ŸIPï¼Œç§°ä¸ºService-IPï¼Œ Kuberenetes Porxyç»„ä»¶è´Ÿè´£å®ç°Service-IPè·¯ç”±å’Œè½¬å‘ï¼Œåœ¨å®¹å™¨è¦†ç›–ç½‘ç»œä¹‹ä¸Šåˆå®ç°äº†è™šæ‹Ÿè½¬å‘ç½‘ç»œã€‚\nKubernetes Porxyå®ç°äº†ä»¥ä¸‹åŠŸèƒ½ï¼š\nè½¬å‘è®¿é—®Serviceçš„Service-IPçš„è¯·æ±‚åˆ°Endpoints(å³Pod-IP)ã€‚ ç›‘æ§Serviceå’ŒEndpointsçš„å˜åŒ–ï¼Œå®æ—¶åˆ·æ–°è½¬å‘è§„åˆ™ã€‚ è´Ÿè½½å‡è¡¡èƒ½åŠ›ã€‚ 3. å¼€æºçš„ç½‘ç»œç»„ä»¶ 3.1. Flannel å…·ä½“å‚è€ƒFlannelä»‹ç»\nå‚è€ƒã€ŠKubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. kubernetesç½‘ç»œæ¨¡å‹ 1.1. åŸºç¡€åŸåˆ™ æ¯ä¸ªPodéƒ½æ‹¥æœ‰ä¸€ä¸ªç‹¬ç«‹çš„IPåœ°å€ï¼Œè€Œä¸”å‡å®šæ‰€æœ‰Podéƒ½åœ¨ä¸€ä¸ªå¯ä»¥ç›´æ¥è¿é€šçš„ã€æ‰å¹³çš„ â€¦","ref":"/kubernetes-notes/network/kubernetes-network/","tags":["Kubernetes"],"title":"K8Sç½‘ç»œ"},{"body":"1. æ¦‚è¿° 1.1. cAdvisor cAdvisorå¯¹Nodeæœºå™¨ä¸Šçš„èµ„æºåŠå®¹å™¨è¿›è¡Œå®æ—¶ç›‘æ§å’Œæ€§èƒ½æ•°æ®é‡‡é›†ï¼ŒåŒ…æ‹¬CPUä½¿ç”¨æƒ…å†µã€å†…å­˜ä½¿ç”¨æƒ…å†µã€ç½‘ç»œååé‡åŠæ–‡ä»¶ç³»ç»Ÿä½¿ç”¨æƒ…å†µï¼ŒcAdvisoré›†æˆåœ¨Kubeletä¸­ï¼Œå½“kubeletå¯åŠ¨æ—¶ä¼šè‡ªåŠ¨å¯åŠ¨cAdvisorï¼Œå³ä¸€ä¸ªcAdvisorä»…å¯¹ä¸€å°Nodeæœºå™¨è¿›è¡Œç›‘æ§ã€‚kubeletçš„å¯åŠ¨å‚æ•°--cadvisor-portå¯ä»¥å®šä¹‰cAdvisorå¯¹å¤–æä¾›æœåŠ¡çš„ç«¯å£ï¼Œé»˜è®¤ä¸º4194ã€‚å¯ä»¥é€šè¿‡æµè§ˆå™¨Node_IP:portè®¿é—®ã€‚é¡¹ç›®ä¸»é¡µï¼šhttp://github.com/google/cadvisorã€‚\n1.2. Heapster æ˜¯å¯¹é›†ç¾¤ä¸­çš„å„ä¸ªNodeã€Podçš„èµ„æºä½¿ç”¨æ•°æ®è¿›è¡Œé‡‡é›†ï¼Œé€šè¿‡è®¿é—®æ¯ä¸ªNodeä¸ŠKubeletçš„APIï¼Œå†é€šè¿‡Kubeletè°ƒç”¨cAdvisorçš„APIæ¥é‡‡é›†è¯¥èŠ‚ç‚¹ä¸Šæ‰€æœ‰å®¹å™¨çš„æ€§èƒ½æ•°æ®ã€‚ç”±Heapsterè¿›è¡Œæ•°æ®æ±‡èšï¼Œä¿å­˜åˆ°åç«¯å­˜å‚¨ç³»ç»Ÿä¸­ï¼Œä¾‹å¦‚InfluxDBï¼ŒGoogle Cloud Loggingç­‰ã€‚é¡¹ç›®ä¸»é¡µä¸ºï¼šhttps://github.com/kubernetes/heapsterã€‚\n1.3. InfluxDB æ˜¯åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“ï¼ˆæ¯æ¡è®°å½•å¸¦æœ‰æ—¶é—´æˆ³å±æ€§ï¼‰ï¼Œä¸»è¦ç”¨äºå®æ—¶æ•°æ®é‡‡é›†ã€äº‹ä»¶è·Ÿè¸ªè®°å½•ã€å­˜å‚¨æ—¶é—´å›¾è¡¨ã€åŸå§‹æ•°æ®ç­‰ã€‚æä¾›REST APIç”¨äºæ•°æ®çš„å­˜å‚¨å’ŒæŸ¥è¯¢ã€‚é¡¹ç›®ä¸»é¡µä¸ºhttp://InfluxDB.comã€‚\n1.4. Grafana é€šè¿‡Dashboardå°†InfluxDBçš„æ—¶åºæ•°æ®å±•ç°æˆå›¾è¡¨å½¢å¼ï¼Œä¾¿äºæŸ¥çœ‹é›†ç¾¤è¿è¡ŒçŠ¶æ€ã€‚é¡¹ç›®ä¸»é¡µä¸ºhttp://Grafana.orgã€‚\n1.5. æ€»ä½“æ¶æ„å›¾ å…¶ä¸­å½“å‰Kubernetesä¸­ï¼ŒHeapsterã€InfluxDBã€Grafanaå‡ä»¥Podçš„å½¢å¼å¯åŠ¨å’Œè¿è¡Œã€‚Heapsterä¸Masteréœ€é…ç½®å®‰å…¨è¿æ¥ã€‚\n2. éƒ¨ç½²ä¸ä½¿ç”¨ 2.1. cAdvisor kubeletçš„å¯åŠ¨å‚æ•°--cadvisor-portå¯ä»¥å®šä¹‰cAdvisorå¯¹å¤–æä¾›æœåŠ¡çš„ç«¯å£ï¼Œé»˜è®¤ä¸º4194ã€‚å¯ä»¥é€šè¿‡æµè§ˆå™¨Node_IP:portè®¿é—®ã€‚ä¹Ÿæä¾›äº†REST APIä¾›å®¢æˆ·ç«¯è¿œç¨‹è°ƒç”¨ï¼ŒAPIè¿”å›çš„æ ¼å¼ä¸ºJSONï¼Œå¯ä»¥é‡‡ç”¨URLè®¿é—®ï¼šhttp://hostname:port/api/version/request/\nä¾‹å¦‚ï¼šhttp://14.152.49.100:4194/api/v1.3/machine è·å–ä¸»æœºä¿¡æ¯ã€‚\n2.2. Service 2.2.1. heapster-service heapster-service.yaml\napiVersion:v1 kind:Service metadata: label: kubenetes.io/cluster-service:\"true\" kubernetes.io/name:Heapster name:heapster namespace:kube-system spec: ports: - port:80 targetPort:8082 selector: k8s-app:heapster 2.2.2. influxdb-service influxdb-service.yaml\napiVersion:v1 kind:Service metadata: label:null name:monitoring-InfluxDB namespace:kube-system spec: type:Nodeport ports: - name:http port:80 targetPort:8083 - name:api port:8086 targetPort:8086 Nodeport:8086 selector: name:influxGrafana 2.2.3. grafana-service grafana-service.yaml\napiVersion:v1 kind:Service metadata: label: kubenetes.io/cluster-service:\"true\" kubernetes.io/name:monitoring-Grafana name:monitoring-Grafana namespace:kube-system spec: type:Nodeport ports: port:80 targetPort:8080 Nodeport:8085 selector: name:influxGrafana ä½¿ç”¨type=NodePortå°†InfluxDBå’ŒGrafanaæš´éœ²åœ¨Nodeçš„ç«¯å£ä¸Šï¼Œä»¥ä¾¿é€šè¿‡æµè§ˆå™¨è¿›è¡Œè®¿é—®ã€‚\n2.2.4. åˆ›å»ºservice kubectl create -f heapster-service.yaml kubectl create -f InfluxDB-service.yaml kubectl create -f Grafana-service.yaml 2.3. ReplicationController 2.3.1. influxdb-grafana-controller influxdb-grafana-controller-v3.yaml\napiVersion:v1 kind:ReplicationController metadata: name:monitoring-influxdb-grafana-v3 namespace:kube-system labels: k8s-app:influxGrafana version:v3 kubernetes.io/cluster-service:\"true spec: replicas:1 selector: k8s-app:influxGrafana version:v3 template: metadata: labels: k8s-app:influxGrafana version:v3 kubernetes.io/cluster-service:\"true spec: containers: - image:gcr.io/google_containers/heapster_influxdb:v0.5 name:influxdb resources: limits: cpu:100m memory:500Mi requests: cpu:100m memory:500Mi ports: - containerPort:8083 - containerPort:8086 volumeMounts: -name:influxdb-persistent-storage mountPath:/data - image:grc.io/google_containers/heapster_grafana:v2.6.0-2 name:grafana resources: limits: cpu:100m memory:100Mi requests: cpu:100m memory:100Mi env: - name:INFLUXDB_SERVICE_URL value:http://monitoring-influxdb:8086 - name:GF_AUTH_BASIC_ENABLED value:\"false\" - name:GF_AUTH_ANONYMOUS_ENABLED value:\"true\" - name:GF_AUTH_ANONYMOUS_ORG_ROLE value:Admin - name:GF_SERVER_ROOT_URL value:/api/v1/proxy/namespace/kube-system/services/monitoring-grafana/ volumeMounts: - name:grafana-persistent-storage mountPath:/var volumes: - name:influxdb-persistent-storage emptyDir{} - name:grafana-persistent-storage emptyDir{} 2.3.2. heapster-controller heapster-controller.yaml\napiVersion:v1 kind:ReplicationController metadata: labels: k8s-app:heapster name:heapster version:v6 name:heapster namespace:kube-system spec: replicas:1 selector: name:heapster k8s-app:heapster version:v6 template: metadata: labels: k8s-app:heapster version:v6 spec: containers: - image:gcr.io/google_containers/heapster:v0.17.0 name:heapster command: - /heapster - --source=kubernetes:http://192.168.1.128:8080?inClusterConfig=flase\u0026kubeletHttps=true\u0026useServiceAccount=true\u0026auth= - --sink=InfluxDB:http://monitoring-InfluxDB:8086 Heapsterè®¾ç½®å¯åŠ¨å‚æ•°è¯´æ˜ï¼š\n1ã€â€“source\né…ç½®ç›‘æ§æ¥æºï¼Œæœ¬ä¾‹ä¸­è¡¨ç¤ºä»k8s-Masterè·å–å„ä¸ªNodeçš„ä¿¡æ¯ã€‚åœ¨URLçš„å‚æ•°éƒ¨åˆ†ï¼Œä¿®æ”¹kubeletHttpsã€inClusterConfigã€useServiceAccountçš„å€¼ã€‚\n2ã€â€“sink\né…ç½®åç«¯çš„å­˜å‚¨ç³»ç»Ÿï¼Œæœ¬ä¾‹ä¸­ä½¿ç”¨InfluxDBã€‚URLä¸­ä¸»æœºåçš„åœ°å€æ˜¯InfluxDBçš„Serviceåå­—ï¼Œéœ€è¦DNSæœåŠ¡æ­£å¸¸å·¥ä½œï¼Œå¦‚æœæ²¡æœ‰é…ç½®DNSæœåŠ¡å¯ä½¿ç”¨Serviceçš„ClusterIPåœ°å€ã€‚\n2.3.3. åˆ›å»ºReplicationController kubelet create -f InfluxDB-Grafana-controller.yaml kubelet create -f heapster-controller.yaml 3. æŸ¥çœ‹ç•Œé¢åŠæ•°æ® 3.1. InfluxDB è®¿é—®ä»»æ„ä¸€å°Nodeæœºå™¨çš„30083ç«¯å£ã€‚\n3.2. Grafana è®¿é—®ä»»æ„ä¸€å°Nodeæœºå™¨çš„30080ç«¯å£ã€‚\n4. å®¹å™¨åŒ–éƒ¨ç½² 4.1. æ‹‰å–é•œåƒ docker pull influxdb:latest docker pull cadvisor:latest docker pull grafana:latest docker pull heapster:latest 4.2. è¿è¡Œå®¹å™¨ 4.2.1. influxdb #influxdb docker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --volume=/opt/data/influxdb:/data --name influxsrv influxdb:latest 4.2.2. cadvisor #cadvisor docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --link influxsrv:influxsrv --name=cadvisor cadvisor:latest -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 4.2.3. grafana #grafana docker run -d -p 3000:3000 -e INFLUXDB_HOST=influxsrv -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=cadvisor -e INFLUXDB_USER=root -e INFLUXDB_PASS=root --link influxsrv:influxsrv --name grafana grafana:latest 4.2.4. heapster docker run -d -p 8082:8082 --net=host heapster:canary --source=kubernetes:http://`k8s-server-ip`:8080?inClusterConfig=false/\u0026useServiceAccount=false --sink=influxdb:http://`influxdb-ip`:8086 4.3. è®¿é—® åœ¨æµè§ˆå™¨è¾“å…¥IP:PORT\n","categories":"","description":"","excerpt":"1. æ¦‚è¿° 1.1. cAdvisor cAdvisorå¯¹Nodeæœºå™¨ä¸Šçš„èµ„æºåŠå®¹å™¨è¿›è¡Œå®æ—¶ç›‘æ§å’Œæ€§èƒ½æ•°æ®é‡‡é›†ï¼ŒåŒ…æ‹¬CPUä½¿ç”¨æƒ…å†µã€å†…å­˜ä½¿ç”¨ â€¦","ref":"/kubernetes-notes/monitor/kubernetes-cluster-monitoring/","tags":["Monitor"],"title":"Kubernetesé›†ç¾¤ç›‘æ§"},{"body":"1. ç³»ç»Ÿç®¡ç† 1.1. è¿æ¥mysql å¿«é€Ÿéƒ¨ç½²docker mysql\ndocker pull mysql:5.7 å¯åŠ¨MySQL\nmkdir -p ~/data/mysql docker run --name my-mysql -v ~/data/mysql:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 æ ¼å¼ï¼š mysql -hä¸»æœºåœ°å€ -uç”¨æˆ·å ï¼pç”¨æˆ·å¯†ç \n#è¿æ¥æœ¬åœ° mysql -h\u003clocalhost/127.0.0.1\u003e -P \u003cPORT\u003e -uç”¨æˆ·å ï¼pç”¨æˆ·å¯†ç  #è¿æ¥è¿œç¨‹ mysql -h \u003cmysqlåœ°å€\u003e -P \u003cPORT\u003e -u \u003cuser\u003e -p \u003cpassword\u003e \u003cdb_name\u003e # ä½¿ç”¨mycli, apt install -y mycli mycli -h \u003cmysqlåœ°å€\u003e -P \u003cPORT\u003e -u \u003cuser\u003e -p \u003cpassword\u003e \u003cdb_name\u003e #é€€å‡ºè¿æ¥ exit 1.2. å¤‡ä»½æ•°æ®åº“ 1.å¯¼å‡ºæ•´ä¸ªæ•°æ®åº“\nå¯¼å‡ºæ–‡ä»¶é»˜è®¤æ˜¯å­˜åœ¨mysql\\binç›®å½•ä¸‹\n#1ï¼‰å¤‡ä»½å•ä¸ªæ•°æ®åº“ mysqldump -u ç”¨æˆ·å -p æ•°æ®åº“å \u003e å¯¼å‡ºçš„æ–‡ä»¶å mysqldump -u user_name -p123456 database_name \u003e outfile_name.sql #2ï¼‰åŒæ—¶å¤‡ä»½å¤šä¸ªæ•°æ®åº“ï¼Œä¾‹å¦‚database1_nameï¼Œdatabase2_name mysqldump -u user_name -p123456 --databases database1_name database2_name \u003e outfile_name.sql #3ï¼‰å¤‡ä»½å…¨éƒ¨æ•°æ®åº“ mysqldump -u user_name -p123456 --all-databases \u003e outfile_name.sql 2.å¯¼å‡ºä¸€ä¸ªè¡¨\nmysqldump -u ç”¨æˆ·å -p æ•°æ®åº“å è¡¨å\u003e å¯¼å‡ºçš„æ–‡ä»¶å mysqldump -u user_name -p database_name table_name \u003e outfile_name.sql 3.å¯¼å‡ºä¸€ä¸ªæ•°æ®åº“ç»“æ„\nmysqldump -u user_name -p -d â€“add-drop-table database_name \u003e outfile_name.sql -d æ²¡æœ‰æ•°æ® â€“add-drop-table åœ¨æ¯ä¸ªcreateè¯­å¥ä¹‹å‰å¢åŠ ä¸€ä¸ªdrop table 4.å¸¦è¯­è¨€å‚æ•°å¯¼å‡º\nmysqldump -uroot -p â€“default-character-set=latin1 â€“set-charset=gbk â€“skip-opt database_name \u003e outfile_name.sql 5ã€å¯¼å…¥æ•°æ®åº“\n#1ï¼‰å¤šä¸ªä¸ªæ•°æ®åº“ mysql -u root â€“p \u003c [å¤‡ä»½æ–‡ä»¶çš„ä¿å­˜è·¯å¾„] æˆ–è€…source [å¤‡ä»½æ–‡ä»¶çš„ä¿å­˜è·¯å¾„] #2ï¼‰å•ä¸ªæ•°æ®åº“ mysql -uroot â€“p database_name \u003c [å¤‡ä»½æ–‡ä»¶çš„ä¿å­˜è·¯å¾„] æˆ–è€…source [å¤‡ä»½æ–‡ä»¶çš„ä¿å­˜è·¯å¾„] 1.3. ç”¨æˆ·ç®¡ç† #åˆ›å»ºç”¨æˆ· create user 'ç”¨æˆ·å'@'IPåœ°å€' identified by 'å¯†ç '; #åˆ é™¤ç”¨æˆ· drop user 'ç”¨æˆ·å'@'IPåœ°å€'; delete from user where user='ç”¨æˆ·å' and host='localhost'; #ä¿®æ”¹ç”¨æˆ· rename user 'ç”¨æˆ·å'@'IPåœ°å€'; to 'æ–°ç”¨æˆ·å'@'IPåœ°å€';; #ä¿®æ”¹å¯†ç  set password for 'ç”¨æˆ·å'@'IPåœ°å€' = Password('æ–°å¯†ç ') mysqladmin -uç”¨æˆ·å -pæ—§å¯†ç  password æ–°å¯†ç  1.4. æƒé™ç®¡ç† 1.4.1. grant 1ã€grant æƒé™ on æ•°æ®åº“å¯¹è±¡ to ç”¨æˆ·\næ•°æ®åº“å¯¹è±¡çš„æ ¼å¼ä¸º\u003cdatabase\u003e.\u003ctable\u003eã€‚\u003cdatabase\u003e.*ï¼šè¡¨ç¤ºæˆæƒæ•°æ®åº“å¯¹è±¡è¯¥æ•°æ®åº“çš„æ‰€æœ‰è¡¨ï¼›*.*ï¼šè¡¨ç¤ºæˆæƒæ•°æ®åº“å¯¹è±¡ä¸ºæ‰€æœ‰æ•°æ®åº“çš„æ‰€æœ‰è¡¨ã€‚\ngrant all privileges on . to \u003cuser\u003e@'\u003cip\u003e' identified by '\u003cpasswd\u003e';å¦‚æœ\u003cip\u003eä¸º'%'è¡¨ç¤ºä¸é™åˆ¶IPã€‚ 2ã€æ’¤é”€æƒé™ï¼š\nrevoke all on . from \u003cuser\u003e@\u003cip\u003e; 1.4.2. æ™®é€šæ•°æ®åº“ç”¨æˆ· æŸ¥è¯¢ã€æ’å…¥ã€æ›´æ–°ã€åˆ é™¤ æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨æ•°æ®çš„æƒåˆ©\ngrant select, insert, update, delete on testdb.* to \u003cuser\u003e@'\u003cip\u003e'; 1.4.3. DBA ç”¨æˆ· #1ã€æˆæƒ grant all privileges on . to \u003cdba\u003e@'\u003cip\u003e' identified by '\u003cpasswd\u003e'; #2ã€åˆ·æ–°ç³»ç»Ÿæƒé™ flush privileges; 1.4.4. æŸ¥çœ‹ç”¨æˆ·æƒé™ #æŸ¥çœ‹å½“å‰ç”¨æˆ·ï¼ˆè‡ªå·±ï¼‰æƒé™ show grants; #æŸ¥çœ‹æŒ‡å®šMySQL ç”¨æˆ·æƒé™ show grants for \u003cuser\u003e@\u003clocalhost\u003e; #æŸ¥çœ‹userå’Œhost select user,host from mysql.user order by user; 1.4.5. æƒé™åˆ—è¡¨ æƒé™ è¯´æ˜ ç½‘ç«™ä½¿ç”¨è´¦æˆ·æ˜¯å¦ç»™äºˆ Select å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡ŒæŸ¥è¯¢ å»ºè®®ç»™äºˆ Insert å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡Œæ’å…¥ å»ºè®®ç»™äºˆ Update å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡Œæ›´æ–° å»ºè®®ç»™äºˆ Delete å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡Œåˆ é™¤ å»ºè®®ç»™äºˆ Create å¯åœ¨æ­¤æ•°æ®åº“ä¸‹åˆ›å»ºè¡¨æˆ–ç´¢å¼• å»ºè®®ç»™äºˆ Drop å¯åˆ é™¤æ­¤æ•°æ®åº“åŠæ•°æ®åº“ä¸‹æ‰€æœ‰è¡¨ ä¸å»ºè®®ç»™äºˆ Grant èµ‹äºˆæƒé™é€‰é¡¹ ä¸å»ºè®®ç»™äºˆ References æœªæ¥MySQLç‰¹æ€§çš„å ä½ç¬¦ ä¸å»ºè®®ç»™äºˆ Index å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡Œç´¢å¼• å»ºè®®ç»™äºˆ Alter å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡Œæ›´æ”¹ å»ºè®®ç»™äºˆ Create_tmp_table åˆ›å»ºä¸´æ—¶è¡¨ ä¸å»ºè®®ç»™äºˆ Lock_tables å¯å¯¹å…¶ä¸‹æ‰€æœ‰è¡¨è¿›è¡Œé”å®š ä¸å»ºè®®ç»™äºˆ Create_view å¯åœ¨æ­¤æ•°æ®ä¸‹åˆ›å»ºè§†å›¾ å»ºè®®ç»™äºˆ Show_view å¯åœ¨æ­¤æ•°æ®ä¸‹æŸ¥çœ‹è§†å›¾ å»ºè®®ç»™äºˆ Create_routine å¯åœ¨æ­¤æ•°æ®ä¸‹åˆ›å»ºå­˜å‚¨è¿‡ç¨‹ ä¸å»ºè®®ç»™äºˆ Alter_routine å¯åœ¨æ­¤æ•°æ®ä¸‹æ›´æ”¹å­˜å‚¨è¿‡ç¨‹ ä¸å»ºè®®ç»™äºˆ Execute å¯åœ¨æ­¤æ•°æ®ä¸‹æ‰§è¡Œå­˜å‚¨è¿‡ç¨‹ ä¸å»ºè®®ç»™äºˆ Event å¯åœ¨æ­¤æ•°æ®ä¸‹åˆ›å»ºäº‹ä»¶è°ƒåº¦å™¨ ä¸å»ºè®®ç»™äºˆ Trigger å¯åœ¨æ­¤æ•°æ®ä¸‹åˆ›å»ºè§¦å‘å™¨ ä¸å»ºè®®ç»™äºˆ 1.4.6.æŸ¥çœ‹ä¸»ä»å…³ç³» #ç™»å½•ä¸»æœº show slave hosts; #ç™»å½•ä»æœº show slave status; ","categories":"","description":"","excerpt":"1. ç³»ç»Ÿç®¡ç† 1.1. è¿æ¥mysql å¿«é€Ÿéƒ¨ç½²docker mysql\ndocker pull mysql:5.7 å¯åŠ¨MySQL â€¦","ref":"/linux-notes/mysql/system-manage/","tags":["Mysql"],"title":"Mysqlå¸¸ç”¨å‘½ä»¤ä¹‹ç³»ç»Ÿç®¡ç†"},{"body":"1. Podçš„åŸºæœ¬ç”¨æ³• 1.1. è¯´æ˜ Podå®é™…ä¸Šæ˜¯å®¹å™¨çš„é›†åˆï¼Œåœ¨k8sä¸­å¯¹è¿è¡Œå®¹å™¨çš„è¦æ±‚ä¸ºï¼šå®¹å™¨çš„ä¸»ç¨‹åºéœ€è¦ä¸€ç›´åœ¨å‰å°è¿è¡Œï¼Œè€Œä¸æ˜¯åå°è¿è¡Œã€‚åº”ç”¨å¯ä»¥æ”¹é€ æˆå‰å°è¿è¡Œçš„æ–¹å¼ï¼Œä¾‹å¦‚Goè¯­è¨€çš„ç¨‹åºï¼Œç›´æ¥è¿è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶ï¼›javaè¯­è¨€åˆ™è¿è¡Œä¸»ç±»ï¼›tomcatç¨‹åºå¯ä»¥å†™ä¸ªè¿è¡Œè„šæœ¬ã€‚æˆ–è€…é€šè¿‡supervisorçš„è¿›ç¨‹ç®¡ç†å·¥å…·ï¼Œå³supervisoråœ¨å‰å°è¿è¡Œï¼Œåº”ç”¨ç¨‹åºç”±supervisorç®¡ç†åœ¨åå°è¿è¡Œã€‚å…·ä½“å¯å‚è€ƒsupervisordã€‚ å½“å¤šä¸ªåº”ç”¨ä¹‹é—´æ˜¯ç´§è€¦åˆçš„å…³ç³»æ—¶ï¼Œå¯ä»¥å°†å¤šä¸ªåº”ç”¨ä¸€èµ·æ”¾åœ¨ä¸€ä¸ªPodä¸­ï¼ŒåŒä¸ªPodä¸­çš„å¤šä¸ªå®¹å™¨ä¹‹é—´äº’ç›¸è®¿é—®å¯ä»¥é€šè¿‡localhostæ¥é€šä¿¡ï¼ˆå¯ä»¥æŠŠPodç†è§£æˆä¸€ä¸ªè™šæ‹Ÿæœºï¼Œå…±äº«ç½‘ç»œå’Œå­˜å‚¨å·ï¼‰ã€‚ 1.2. Podç›¸å…³å‘½ä»¤ æ“ä½œ å‘½ä»¤ è¯´æ˜ åˆ›å»º kubectl create -f frontend-localredis-pod.yaml æŸ¥è¯¢Podè¿è¡ŒçŠ¶æ€ kubectl get pods --namespace=\u003cNAMESPACE\u003e æŸ¥è¯¢Podè¯¦æƒ… kebectl describe pod \u003cPOD_NAME\u003e --namespace=\u003cNAMESPACE\u003e è¯¥å‘½ä»¤å¸¸ç”¨æ¥æ’æŸ¥é—®é¢˜ï¼ŒæŸ¥çœ‹Eventäº‹ä»¶ åˆ é™¤ kubectl delete pod \u003cPOD_NAME\u003e ;kubectl delete pod --all æ›´æ–° kubectl replace pod.yaml - 2. Podçš„å®šä¹‰æ–‡ä»¶ apiVersion: v1 kind: Pod metadata: name: string namaspace: string labels: - name: string annotations: - name: string spec: containers: - name: string images: string imagePullPolice: [Always | Never | IfNotPresent] command: [string] args: [string] workingDir: string volumeMounts: - name: string mountPath: string readOnly: boolean ports: - name: string containerPort: int hostPort: int protocol: string env: - name: string value: string resources: limits: cpu: string memory: string requests: cpu: string memory: string livenessProbe: exec: command: [string] httpGet: path: string port: int host: string scheme: string httpHeaders: - name: string value: string tcpSocket: port: int initialDelaySeconds: number timeoutSeconds: number periodSeconds: number successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] nodeSelector: object imagePullSecrets: - name: string hostNetwork: false volumes: - name: string emptyDir: {} hostPath: path: string secret: secretName: string items: - key: string path: string configMap: name: string items: - key: string path: string 3. é™æ€pod é™æ€Podæ˜¯ç”±kubeletè¿›è¡Œç®¡ç†ï¼Œä»…å­˜åœ¨äºç‰¹å®šNodeä¸Šçš„Podã€‚å®ƒä»¬ä¸èƒ½é€šè¿‡API Serverè¿›è¡Œç®¡ç†ï¼Œæ— æ³•ä¸ReplicationControllerã€Deploymentæˆ–DaemonSetè¿›è¡Œå…³è”ï¼Œå¹¶ä¸”kubeletä¹Ÿæ— æ³•å¯¹å…¶å¥åº·æ£€æŸ¥ã€‚\né™æ€Podæ€»æ˜¯ç”±kubeletåˆ›å»ºï¼Œå¹¶ä¸”æ€»åœ¨kubeletæ‰€åœ¨çš„Nodeä¸Šè¿è¡Œã€‚\nåˆ›å»ºé™æ€Podçš„æ–¹å¼ï¼š\n3.1. é€šè¿‡é…ç½®æ–‡ä»¶æ–¹å¼ éœ€è¦è®¾ç½®kubeletçš„å¯åŠ¨å‚æ•°â€œâ€“configâ€ï¼ŒæŒ‡å®škubeletéœ€è¦ç›‘æ§çš„é…ç½®æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œkubeletä¼šå®šæœŸæ‰«æè¯¥ç›®å½•ï¼Œå¹¶æ ¹æ®è¯¥ç›®å½•çš„.yamlæˆ–.jsonæ–‡ä»¶è¿›è¡Œåˆ›å»ºæ“ä½œã€‚é™æ€Podæ— æ³•é€šè¿‡API Serveråˆ é™¤ï¼ˆè‹¥åˆ é™¤ä¼šå˜æˆpendingçŠ¶æ€ï¼‰ï¼Œå¦‚éœ€åˆ é™¤è¯¥Podåˆ™å°†yamlæˆ–jsonæ–‡ä»¶ä»è¿™ä¸ªç›®å½•ä¸­åˆ é™¤ã€‚\nä¾‹å¦‚ï¼š\né…ç½®ç›®å½•ä¸º/etc/kubelet.d/ï¼Œé…ç½®å¯åŠ¨å‚æ•°ï¼š--config=/etc/kubelet.d/ï¼Œè¯¥ç›®å½•ä¸‹æ”¾å…¥static-web.yamlã€‚\napiVersion: v1 kind: Pod metadata: name: static-web labels: name: static-web spec: containers: - name: static-web image: nginx ports: - name: web containerPort: 80 å‚è€ƒæ–‡ç« \nã€ŠKubernetesæƒå¨æŒ‡å—ã€‹ ","categories":"","description":"","excerpt":"1. Podçš„åŸºæœ¬ç”¨æ³• 1.1. è¯´æ˜ Podå®é™…ä¸Šæ˜¯å®¹å™¨çš„é›†åˆï¼Œåœ¨k8sä¸­å¯¹è¿è¡Œå®¹å™¨çš„è¦æ±‚ä¸ºï¼šå®¹å™¨çš„ä¸»ç¨‹åºéœ€è¦ä¸€ç›´åœ¨å‰å°è¿è¡Œï¼Œè€Œä¸æ˜¯åå°è¿ â€¦","ref":"/kubernetes-notes/concepts/pod/pod-definition/","tags":["Kubernetes"],"title":"Podå®šä¹‰æ–‡ä»¶"},{"body":"1. etcdctlä»‹ç» etcdctlæ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œçš„å®¢æˆ·ç«¯ï¼Œå®ƒæä¾›äº†ä¸€ä¸‹ç®€æ´çš„å‘½ä»¤ï¼Œå¯ç†è§£ä¸ºå‘½ä»¤å·¥å…·é›†ï¼Œå¯ä»¥æ–¹ä¾¿æˆ‘ä»¬åœ¨å¯¹æœåŠ¡è¿›è¡Œæµ‹è¯•æˆ–è€…æ‰‹åŠ¨ä¿®æ”¹æ•°æ®åº“å†…å®¹ã€‚etcdctlä¸å…¶ä»–xxxctlçš„å‘½ä»¤åŸç†åŠæ“ä½œç±»ä¼¼ï¼ˆä¾‹å¦‚kubectlï¼Œsystemctlï¼‰ã€‚\nç”¨æ³•ï¼šetcdctl [global options] command [command options][args...]\n2. Etcdå¸¸ç”¨å‘½ä»¤ 2.1. æ•°æ®åº“æ“ä½œå‘½ä»¤ etcd åœ¨é”®çš„ç»„ç»‡ä¸Šé‡‡ç”¨äº†å±‚æ¬¡åŒ–çš„ç©ºé—´ç»“æ„ï¼ˆç±»ä¼¼äºæ–‡ä»¶ç³»ç»Ÿä¸­ç›®å½•çš„æ¦‚å¿µï¼‰ï¼Œæ•°æ®åº“æ“ä½œå›´ç»•å¯¹é”®å€¼å’Œç›®å½•çš„ CRUD [å¢åˆ æ”¹æŸ¥]ï¼ˆç¬¦åˆ REST é£æ ¼çš„ä¸€å¥—æ“ä½œï¼šCreate, Read, Update, Deleteï¼‰å®Œæ•´ç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†ã€‚\nå…·ä½“çš„å‘½ä»¤é€‰é¡¹å‚æ•°å¯ä»¥é€šè¿‡ etcdctl command --helpæ¥è·å–ç›¸å…³å¸®åŠ©ã€‚\n2.1.1. å¯¹è±¡ä¸ºé”®å€¼ set[å¢:æ— è®ºæ˜¯å¦å­˜åœ¨]:etcdctl set key value mk[å¢:å¿…é¡»ä¸å­˜åœ¨]:etcdctl mk key value rm[åˆ ]:etcdctl rm key update[æ”¹]:etcdctl update key value get[æŸ¥]:etcdctl get key 2.1.2. å¯¹è±¡ä¸ºç›®å½• setdir[å¢:æ— è®ºæ˜¯å¦å­˜åœ¨]:etcdctl setdir dir mkdir[å¢:å¿…é¡»ä¸å­˜åœ¨]: etcdctl mkdir dir rmdir[åˆ ]:etcdctl rmdir dir updatedir[æ”¹]:etcdctl updatedir dir ls[æŸ¥]:etcdclt ls 2.2. éæ•°æ®åº“æ“ä½œå‘½ä»¤ backup[å¤‡ä»½ etcd çš„æ•°æ®] etcdctl backup\nwatch[ç›‘æµ‹ä¸€ä¸ªé”®å€¼çš„å˜åŒ–ï¼Œä¸€æ—¦é”®å€¼å‘ç”Ÿæ›´æ–°ï¼Œå°±ä¼šè¾“å‡ºæœ€æ–°çš„å€¼å¹¶é€€å‡º] etcdctl watch key\nexec-watch[ç›‘æµ‹ä¸€ä¸ªé”®å€¼çš„å˜åŒ–ï¼Œä¸€æ—¦é”®å€¼å‘ç”Ÿæ›´æ–°ï¼Œå°±æ‰§è¡Œç»™å®šå‘½ä»¤] etcdctl exec-watch key --sh -c \"ls\"\nmember[é€šè¿‡ listã€addã€removeã€update å‘½ä»¤åˆ—å‡ºã€æ·»åŠ ã€åˆ é™¤ ã€æ›´æ–°etcd å®ä¾‹åˆ° etcd é›†ç¾¤ä¸­] etcdctl member listï¼›etcdctl member add å®ä¾‹ï¼›etcdctl member remove å®ä¾‹ï¼›etcdctl member update å®ä¾‹ã€‚\netcdctl cluster-health[æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€] 2.3. å¸¸ç”¨é…ç½®å‚æ•° è®¾ç½®é…ç½®æ–‡ä»¶ï¼Œé»˜è®¤ä¸º/etc/etcd/etcd.confã€‚\né…ç½®å‚æ•° å‚æ•°è¯´æ˜ é…ç½®å‚æ•° å‚æ•°è¯´æ˜ -name èŠ‚ç‚¹åç§° -data-dir ä¿å­˜æ—¥å¿—å’Œå¿«ç…§çš„ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰å·¥ä½œç›®å½•ï¼ŒæŒ‡å®šèŠ‚ç‚¹çš„æ•°æ®å­˜å‚¨ç›®å½• -addr å…¬å¸ƒçš„ipåœ°å€å’Œç«¯å£ã€‚ é»˜è®¤ä¸º127.0.0.1:2379 -bind-addr ç”¨äºå®¢æˆ·ç«¯è¿æ¥çš„ç›‘å¬åœ°å€ï¼Œé»˜è®¤ä¸º-addré…ç½® -peers é›†ç¾¤æˆå‘˜é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ 127.0.0.1:2380,127.0.0.1:2381 -peer-addr é›†ç¾¤æœåŠ¡é€šè®¯çš„å…¬å¸ƒçš„IPåœ°å€ï¼Œé»˜è®¤ä¸º 127.0.0.1:2380. -peer-bind-addr é›†ç¾¤æœåŠ¡é€šè®¯çš„ç›‘å¬åœ°å€ï¼Œé»˜è®¤ä¸º-peer-addré…ç½® -wal-dir æŒ‡å®šèŠ‚ç‚¹çš„wasæ–‡ä»¶çš„å­˜å‚¨ç›®å½•ï¼Œè‹¥æŒ‡å®šäº†è¯¥å‚æ•°ï¼Œwalæ–‡ä»¶ä¼šå’Œå…¶ä»–æ•°æ®æ–‡ä»¶åˆ†å¼€å­˜å‚¨ -listen-client-urls -listen-peer-urls ç›‘å¬URLï¼Œç”¨äºä¸å…¶ä»–èŠ‚ç‚¹é€šè®¯ -initial-advertise-peer-urls å‘ŠçŸ¥é›†ç¾¤å…¶ä»–èŠ‚ç‚¹url. -advertise-client-urls å‘ŠçŸ¥å®¢æˆ·ç«¯url, ä¹Ÿå°±æ˜¯æœåŠ¡çš„url -initial-cluster-token é›†ç¾¤çš„ID -initial-cluster é›†ç¾¤ä¸­æ‰€æœ‰èŠ‚ç‚¹ -initial-cluster-state -initial-cluster-state=new è¡¨ç¤ºä»æ— åˆ°æœ‰æ­å»ºetcdé›†ç¾¤ -discovery-srv ç”¨äºDNSåŠ¨æ€æœåŠ¡å‘ç°ï¼ŒæŒ‡å®šDNS SRVåŸŸå -discovery ç”¨äºetcdåŠ¨æ€å‘ç°ï¼ŒæŒ‡å®šetcdå‘ç°æœåŠ¡çš„URL [https://discovery.etcd.io/],ç”¨ç¯å¢ƒå˜é‡è¡¨ç¤º ","categories":"","description":"","excerpt":"1. etcdctlä»‹ç» etcdctlæ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œçš„å®¢æˆ·ç«¯ï¼Œå®ƒæä¾›äº†ä¸€ä¸‹ç®€æ´çš„å‘½ä»¤ï¼Œå¯ç†è§£ä¸ºå‘½ä»¤å·¥å…·é›†ï¼Œå¯ä»¥æ–¹ä¾¿æˆ‘ä»¬åœ¨å¯¹æœåŠ¡è¿›è¡Œæµ‹è¯•æˆ–è€…æ‰‹ â€¦","ref":"/kubernetes-notes/etcd/etcdctl/etcdctl-v2/","tags":["Etcd"],"title":"etcdctl-V2"},{"body":" ä»¥ä¸‹ä¸»è¦ä»‹ç»PaaSå¹³å°è®¾è®¡æ¶æ„ä¸­ä½¿ç”¨åˆ°çš„æ–¹æ³•è®ºï¼Œç»Ÿç§°ä¸º12-Factor(è¦ç´ )\nç®€ä»‹ è½¯ä»¶é€šå¸¸ä¼šä½œä¸ºä¸€ç§æœåŠ¡æ¥äº¤ä»˜ï¼Œå³è½¯ä»¶å³æœåŠ¡(SaaS)ã€‚12-FactoråŸåˆ™ä¸ºæ„å»ºSaaSåº”ç”¨æä¾›äº†ä»¥ä¸‹çš„æ–¹æ³•è®ºï¼š\nä½¿ç”¨æ ‡å‡†åŒ–æµç¨‹è‡ªåŠ¨é…ç½®ï¼Œå‡å°‘å¼€å‘è€…çš„å­¦ä¹ æˆæœ¬ã€‚ å’Œæ“ä½œç³»ç»Ÿè§£è€¦ï¼Œä½¿å…¶å¯ä»¥åœ¨å„ä¸ªç³»ç»Ÿé—´æä¾›æœ€å¤§çš„ç§»æ¤æ€§ã€‚ é€‚åˆéƒ¨ç½²åœ¨ç°ä»£çš„äº‘è®¡ç®—å¹³å°ä¸Šï¼Œä»è€Œåœ¨æœåŠ¡å™¨å’Œç³»ç»Ÿç®¡ç†æ–¹é¢èŠ‚çœèµ„æºã€‚ å°†å¼€å‘ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒçš„å·®å¼‚é™è‡³æœ€ä½ï¼Œå¹¶ä½¿ç”¨æŒç»­äº¤ä»˜å®æ–½æ•æ·å¼€å‘ã€‚ å¯ä»¥åœ¨å·¥å…·ã€æ¶æ„å’Œå¼€å‘æµç¨‹ä¸å‘ç”Ÿæ˜æ˜¾å˜åŒ–çš„å‰æä¸‹å®ç°æ‹“å±• è¯¥ç†è®ºé€‚åº”äºä»»ä½•è¯­è¨€å’Œåç«¯æœåŠ¡(æ•°æ®åº“ã€æ¶ˆæ¯é˜Ÿåˆ—ã€ç¼“å­˜ç­‰)å¼€å‘çš„åº”ç”¨ç¨‹åºã€‚\n1. åŸºå‡†ä»£ç  ä¸€ä»½åŸºå‡†ä»£ç ï¼Œå¤šä»½éƒ¨ç½²\nåº”ç”¨ä»£ç ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿæ¥ç®¡ç†ï¼Œå¸¸ç”¨çš„æœ‰Gitã€SVNç­‰ã€‚ä¸€ä»½ç”¨æ¥è·Ÿè¸ªä»£ç æ‰€æœ‰ä¿®è®¢ç‰ˆæœ¬çš„æ•°æ®åº“ç§°ä¸ºä»£ç åº“ã€‚\n1.1. ä¸€ä»½åŸºå‡†ä»£ç  åŸºå‡†ä»£ç å’Œåº”ç”¨ä¹‹é—´æ€»æ˜¯ä¿æŒä¸€ä¸€å¯¹åº”çš„å…³ç³»ï¼š\nä¸€æ—¦æœ‰å¤šä¸ªåŸºå‡†ä»£ç ï¼Œåˆ™ä¸èƒ½ç§°ä¹‹ä¸ºä¸€ä¸ªåº”ç”¨ï¼Œè€Œæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿã€‚åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ¯ä¸ªç»„ä»¶éƒ½æ˜¯ä¸€ä¸ªåº”ç”¨ï¼Œæ¯ä¸ªåº”ç”¨éƒ½å¯ä»¥ä½¿ç”¨12-FactoråŸåˆ™è¿›è¡Œå¼€å‘ã€‚ å¤šä¸ªåº”ç”¨å…±äº«ä¸€ä»½åŸºå‡†ä»£ç æœ‰æ‚–äº12-FactoråŸåˆ™ã€‚è§£å†³æ–¹æ³•æ˜¯å°†å…±äº«çš„ä»£ç æ‹†æˆç‹¬ç«‹çš„ç±»åº“ï¼Œé€šè¿‡ä¾èµ–ç®¡ç†å»ä½¿ç”¨å®ƒä»¬ã€‚ 1.2. å¤šä»½éƒ¨ç½² æ¯ä¸ªåº”ç”¨åªå¯¹åº”ä¸€ä»½åŸºå‡†ä»£ç ï¼Œä½†å¯ä»¥åŒæ—¶å­˜åœ¨å¤šä»½çš„éƒ¨ç½²ï¼Œæ¯ä»½éƒ¨ç½²ç›¸å½“äºè¿è¡Œäº†ä¸€ä¸ªåº”ç”¨çš„å®ä¾‹ã€‚\nå¤šä»½éƒ¨ç½²çš„åŒºåˆ«åœ¨äºï¼š\nå¯ä»¥å­˜åœ¨ä¸åŒçš„é…ç½®æ–‡ä»¶å¯¹åº”ä¸åŒçš„ç¯å¢ƒã€‚ä¾‹å¦‚å¼€å‘ç¯å¢ƒã€é¢„å‘å¸ƒç¯å¢ƒã€ç”Ÿäº§ç¯å¢ƒç­‰ã€‚ å¯ä»¥ä½¿ç”¨ä¸åŒçš„ç‰ˆæœ¬ã€‚ä¾‹å¦‚å¼€å‘ç¯å¢ƒçš„ç‰ˆæœ¬å¯èƒ½é«˜äºé¢„å‘å¸ƒç¯å¢ƒç‰ˆæœ¬ï¼Œè¿˜æ²¡åŒæ­¥åˆ°é¢„å‘å¸ƒç¯å¢ƒç‰ˆæœ¬ï¼ŒåŒç†ï¼Œé¢„å‘å¸ƒç¯å¢ƒç‰ˆæœ¬å¯èƒ½é«˜äºç”Ÿäº§ç¯å¢ƒç‰ˆæœ¬ã€‚ 2. ä¾èµ– æ˜¾å¼å£°æ˜ä¾èµ–å…³ç³»\nå¤§å¤šæ•°çš„ç¼–ç¨‹è¯­è¨€éƒ½ä¼šæä¾›ä¸€ä¸ªåŒ…ç®¡ç†ç³»ç»Ÿæˆ–å·¥å…·ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰çš„ä¾èµ–åº“ï¼Œä¾‹å¦‚Golangçš„vendorç›®å½•å­˜æ”¾äº†è¯¥åº”ç”¨çš„æ‰€æœ‰ä¾èµ–åŒ…ã€‚\n12-FactoråŸåˆ™ä¸‹çš„åº”ç”¨ä¼šé€šè¿‡ä¾èµ–æ¸…å•æ¥æ˜¾å¼ç¡®åˆ‡åœ°å£°æ˜æ‰€æœ‰çš„ä¾èµ–é¡¹ã€‚åœ¨è¿è¡Œå·¥ç¨‹ä¸­é€šè¿‡ä¾èµ–éš”ç¦»å·¥å…·æ¥ä¿è¯åº”ç”¨ä¸ä¼šå»è°ƒç”¨ç³»ç»Ÿä¸­å­˜åœ¨ä½†ä¾èµ–æ¸…å•ä¸­æœªå£°æ˜çš„ä¾èµ–é¡¹ã€‚\næ˜¾å¼å£°æ˜ä¾èµ–é¡¹çš„ä¼˜ç‚¹åœ¨äºå¯ä»¥ç®€åŒ–ç¯å¢ƒé…ç½®æµç¨‹ï¼Œå¼€å‘è€…å…³æ³¨åº”ç”¨çš„åŸºå‡†ä»£ç ï¼Œè€Œä¾èµ–åº“åˆ™ç”±ä¾èµ–åº“ç®¡ç†å·¥å…·æ¥ç®¡ç†å’Œé…ç½®ã€‚ä¾‹å¦‚ï¼ŒGolangä¸­çš„åŒ…ç®¡ç†å·¥å…·depç­‰ã€‚\n3. é…ç½® åœ¨ç¯å¢ƒä¸­å­˜å‚¨é…ç½®\né€šå¸¸ï¼Œåº”ç”¨çš„é…ç½®åœ¨ä¸åŒçš„å‘å¸ƒç¯å¢ƒä¸­(ä¾‹å¦‚ï¼šå¼€å‘ã€é¢„å‘å¸ƒã€ç”Ÿäº§ç¯å¢ƒ)ä¼šæœ‰å¾ˆå¤§çš„å·®å¼‚ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š\næ•°æ®åº“ã€Redisç­‰åç«¯æœåŠ¡çš„é…ç½® æ¯ä»½éƒ¨ç½²ç‰¹æœ‰çš„é…ç½®ï¼Œä¾‹å¦‚åŸŸå ç¬¬ä¸‰æ–¹æœåŠ¡çš„è¯ä¹¦ç­‰ 12-FactoråŸåˆ™è¦æ±‚ä»£ç å’Œé…ç½®ä¸¥æ ¼åˆ†ç¦»ï¼Œè€Œä¸åº”è¯¥é€šè¿‡ä»£ç å¸¸é‡çš„å½¢å¼å†™åœ¨ä»£ç†é‡Œé¢ã€‚é…ç½®åœ¨ä¸åŒçš„éƒ¨ç½²ç¯å¢ƒä¸­å­˜åœ¨å¤§å¹…å·®å¼‚ï¼Œä½†æ˜¯ä»£ç å´æ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚\nåˆ¤æ–­ä¸€ä¸ªåº”ç”¨æ˜¯å¦æ­£ç¡®åœ°å°†é…ç½®æ’é™¤åœ¨ä»£ç å¤–ï¼Œå¯ä»¥çœ‹åº”ç”¨çš„åŸºå‡†ä»£ç æ˜¯å¦å¯ä»¥ç«‹å³å¼€æºè€Œä¸æ‹…å¿ƒæš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚\n12-FactoråŸåˆ™å»ºè®®å°†åº”ç”¨çš„é…ç½®å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­ï¼Œç¯å¢ƒå˜é‡å¯ä»¥æ–¹ä¾¿åœ¨ä¸åŒçš„éƒ¨ç½²ç¯å¢ƒä¸­ä¿®æ”¹ï¼Œè€Œä¸ä¾µå…¥åŸæœ‰çš„ä»£ç ã€‚(ä¾‹å¦‚ï¼Œk8sçš„å¤§éƒ¨åˆ†ä»£ç é…ç½®æ˜¯é€šè¿‡ç¯å¢ƒå˜é‡çš„æ–¹å¼æ¥ä¼ å…¥çš„)ã€‚\n12-Factoråº”ç”¨ä¸­ï¼Œç¯å¢ƒå˜é‡çš„ç²’åº¦è¦è¶³å¤Ÿå°ä¸”ç›¸å¯¹ç‹¬ç«‹ã€‚å½“åº”ç”¨éœ€è¦æ‹“å±•æ—¶ï¼Œå¯ä»¥å¹³æ»‘è¿‡æ¸¡ã€‚\n4. åç«¯æœåŠ¡ æŠŠåç«¯æœåŠ¡å½“ä½œé™„åŠ èµ„æº\nåç«¯æœåŠ¡æŒ‡ç¨‹åºè¿è¡Œæ—¶æ‰€éœ€è¦é€šè¿‡ç½‘ç»œè°ƒç”¨çš„å„ç§æœåŠ¡ï¼Œä¾‹å¦‚ï¼šæ•°æ®åº“ï¼ˆMySQLï¼ŒCouchDBï¼‰ï¼Œæ¶ˆæ¯/é˜Ÿåˆ—ç³»ç»Ÿï¼ˆRabbitMQï¼ŒBeanstalkdï¼‰ï¼ŒSMTP é‚®ä»¶å‘é€æœåŠ¡ï¼ˆPostfixï¼‰ï¼Œä»¥åŠç¼“å­˜ç³»ç»Ÿï¼ˆMemcachedï¼‰ã€‚\nå…¶ä¸­å¯ä»¥æ ¹æ®ç®¡ç†å¯¹è±¡åˆ†ä¸ºæœ¬åœ°æœåŠ¡(ä¾‹å¦‚æœ¬åœ°æ•°æ®åº“)å’Œç¬¬ä¸‰æ–¹æœåŠ¡(ä¾‹å¦‚Amason S3)ã€‚å¯¹äº12-Factoråº”ç”¨æ¥è¯´éƒ½æ˜¯é™„åŠ èµ„æºï¼Œæ²¡æœ‰åŒºåˆ«å¯¹å¾…ï¼Œå½“å…¶ä¸­ä¸€ä»½åç«¯æœåŠ¡å¤±æ•ˆåï¼Œå¯ä»¥é€šè¿‡åˆ‡æ¢åˆ°åŸå…ˆå¤‡ä»½çš„åç«¯æœåŠ¡ä¸­ï¼Œè€Œä¸éœ€è¦ä¿®æ”¹ä»£ç (ä½†å¯èƒ½éœ€è¦ä¿®æ”¹é…ç½®)ã€‚12-Factoråº”ç”¨ä¸åç«¯æœåŠ¡ä¿æŒæ¾è€¦åˆçš„å…³ç³»ã€‚\n5. æ„å»ºï¼Œå‘å¸ƒï¼Œè¿è¡Œ ä¸¥æ ¼åˆ†ç¦»æ„å»ºå’Œè¿è¡Œ\nåŸºå‡†ä»£ç è½¬åŒ–æˆä¸€ä»½éƒ¨ç½²éœ€è¦ç»è¿‡ä¸‰ä¸ªé˜¶æ®µï¼š\næ„å»ºé˜¶æ®µï¼šæŒ‡ä»£ç è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŒ…çš„è¿‡ç¨‹ã€‚æ„å»ºè¿‡ç¨‹ä¼šä½¿ç”¨æŒ‡å®šç‰ˆæœ¬çš„ä»£ç ï¼Œè·å–ä¾èµ–é¡¹ï¼Œç¼–è¯‘ç”ŸæˆäºŒè¿›åˆ¶æ–‡ä»¶å’Œèµ„æºæ–‡ä»¶ã€‚ å‘å¸ƒé˜¶æ®µï¼šå°†æ„å»ºçš„ç»“æœä¸å½“å‰éƒ¨ç½²æ‰€éœ€çš„é…ç½®ç»“åˆï¼Œå¹¶å¯ä»¥åœ¨è¿è¡Œç¯å¢ƒä¸­ä½¿ç”¨ã€‚ è¿è¡Œé˜¶æ®µï¼ˆè¿è¡Œæ—¶ï¼‰ï¼šæŒ‡é’ˆå¯¹æŒ‡å®šçš„å‘å¸ƒç‰ˆæœ¬åœ¨æ‰§è¡Œç¯å¢ƒä¸­å¯åŠ¨ä¸€ç³»åˆ—åº”ç”¨ç¨‹åºçš„è¿›ç¨‹ã€‚ 12-Factoråº”ç”¨ä¸¥æ ¼åŒºåˆ†æ„å»ºã€å‘å¸ƒã€è¿è¡Œä¸‰ä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªå‘å¸ƒç‰ˆæœ¬å¯¹åº”ä¸€ä¸ªå”¯ä¸€çš„å‘å¸ƒIDï¼Œå¯ä»¥ä½¿ç”¨æ—¶é—´æˆ³æˆ–é€’å¢çš„ç‰ˆæœ¬åºåˆ—å·ã€‚\nå¦‚æœéœ€è¦ä¿®æ”¹åˆ™éœ€è¦äº§ç”Ÿä¸€ä¸ªæ–°çš„å‘å¸ƒç‰ˆæœ¬ï¼Œå¦‚æœéœ€è¦å›é€€ï¼Œåˆ™å›é€€åˆ°ä¹‹å‰æŒ‡å®šçš„å‘å¸ƒç‰ˆæœ¬ã€‚\næ–°ä»£ç éƒ¨ç½²ä¹‹å‰ï¼Œç”±å¼€å‘äººå‘˜è§¦å‘æ„å»ºæ“ä½œï¼Œæ„å»ºé˜¶æ®µå¯ä»¥ç›¸å¯¹å¤æ‚ä¸€äº›ï¼Œæ–¹ä¾¿é”™è¯¯ä¿¡æ¯å¯ä»¥å±•ç¤ºå‡ºæ¥å¾—åˆ°å¦¥å–„å¤„ç†ã€‚è¿è¡Œé˜¶æ®µå¯ä»¥äººä¸ºè§¦å‘æˆ–è‡ªåŠ¨è¿è¡Œï¼Œè¿è¡Œé˜¶æ®µåº”è¯¥ä¿æŒå°½å¯èƒ½å°‘çš„æ¨¡å—ã€‚\n6. è¿›ç¨‹ ä»¥ä¸€ä¸ªæˆ–å¤šä¸ªæ— çŠ¶æ€è¿›ç¨‹è¿è¡Œåº”ç”¨\n12-Factoråº”æœ‰çš„è¿›ç¨‹å¿…é¡»æ˜¯æ— çŠ¶æ€ä¸”æ— å…±äº«çš„ï¼Œä»»ä½•éœ€è¦æŒä¹…åŒ–çš„æ•°æ®å­˜å‚¨åœ¨åç«¯æœåŠ¡ä¸­ï¼Œä¾‹å¦‚æ•°æ®åº“ã€‚\nå†…å­˜åŒºåŸŸå’Œç£ç›˜ç©ºé—´å¯ä»¥ä½œä¸ºè¿›ç¨‹çš„ç¼“å­˜ï¼Œ12-Factoråº”ç”¨ä¸éœ€è¦å…³æ³¨è¿™äº›ç¼“å­˜çš„æŒä¹…åŒ–ï¼Œè€Œæ˜¯å…è®¸å…¶ä¸¢å¤±ï¼Œä¾‹å¦‚é‡å¯çš„æ—¶å€™ã€‚\nè¿›ç¨‹çš„äºŒè¿›åˆ¶æ–‡ä»¶åº”è¯¥åœ¨æ„å»ºé˜¶æ®µæ‰§è¡Œç¼–è¯‘è€Œä¸æ˜¯è¿è¡Œé˜¶æ®µã€‚\nå½“åº”ç”¨ä½¿ç”¨åˆ°ç²˜æ€§Sessionï¼Œå³å°†ç”¨æˆ·çš„sessionæ•°æ®ç¼“å­˜åˆ°è¿›ç¨‹çš„å†…å­˜ä¸­ï¼Œå°†åŒä¸€ç”¨æˆ·çš„åç»­è¯·æ±‚è·¯ç”±åˆ°åŒä¸€ä¸ªè¿›ç¨‹ã€‚12-Factoråº”ç”¨åå¯¹è¿™ç§å¤„ç†æ–¹å¼ï¼Œè€Œæ˜¯å»ºè®®å°†sessionçš„æ•°æ®ä¿å­˜åœ¨redis/memcachedå¸¦æœ‰è¿‡æœŸæ—¶é—´çš„ç¼“å­˜ä¸­ã€‚\n7. ç«¯å£ç»‘å®š é€šè¿‡ç«¯å£ç»‘å®šæä¾›æœåŠ¡\nåº”ç”¨é€šè¿‡ç«¯å£ç»‘å®šæ¥æä¾›æœåŠ¡ï¼Œå¹¶ç›‘å¬å‘é€è‡³è¯¥ç«¯å£çš„è¯·æ±‚ã€‚ç«¯å£ç»‘å®šçš„æ–¹å¼æ„å‘³ç€ä¸€ä¸ªåº”ç”¨ä¹Ÿå¯ä»¥æˆä¸ºå¦ä¸€ä¸ªåº”ç”¨çš„åç«¯æœåŠ¡ï¼Œä¾‹å¦‚æä¾›æŸäº›APIè¯·æ±‚ã€‚\n8. å¹¶å‘ é€šè¿‡è¿›ç¨‹æ¨¡å‹è¿›è¡Œæ‰©å±•\n12-Factoråº”ç”¨ä¸­ï¼Œå¼€å‘äººå‘˜å¯ä»¥å°†ä¸åŒçš„å·¥ä½œåˆ†é…ç»™ä¸åŒç±»å‹è¿›ç¨‹ï¼Œä¾‹å¦‚HTTPè¯·æ±‚ç”±webè¿›ç¨‹æ¥å¤„ç†ï¼Œå¸¸é©»çš„åå°å·¥ä½œç”±workerè¿›ç¨‹æ¥å¤„ç†ï¼ˆk8sçš„è®¾è®¡ä¸­å°±ç»å¸¸ç”¨ä¸åŒç±»å‹çš„manageræ¥å¤„ç†ä¸åŒçš„ä»»åŠ¡ï¼‰ã€‚\n12-Factoråº”ç”¨çš„è¿›ç¨‹å…·å¤‡æ— å…±äº«ã€æ°´å¹³åˆ†åŒºçš„ç‰¹æ€§ï¼Œä½¿å¾—æ°´å¹³æ‰©å±•è¾ƒä¸ºå®¹æ˜“ã€‚\n12-Factoråº”ç”¨çš„è¿›ç¨‹ä¸éœ€è¦å®ˆæŠ¤è¿›ç¨‹æˆ–æ˜¯å†™å…¥PIDæ–‡ä»¶ï¼Œè€Œæ˜¯é€šè¿‡è¿›ç¨‹ç®¡ç†å™¨ï¼ˆä¾‹å¦‚ systemdï¼‰æ¥ç®¡ç†è¾“å‡ºæµï¼Œå“åº”å´©æºƒçš„è¿›ç¨‹ï¼Œä»¥åŠå¤„ç†ç”¨æˆ·è§¦å‘çš„é‡å¯æˆ–å…³é—­è¶…çº§è¿›ç¨‹çš„æ“ä½œã€‚\n9. æ˜“å¤„ç† å¿«é€Ÿå¯åŠ¨å’Œä¼˜é›…ç»ˆæ­¢å¯æœ€å¤§åŒ–å¥å£®æ€§\n12-Factoråº”ç”¨çš„è¿›ç¨‹æ˜¯æ˜“å¤„ç†çš„ï¼Œå³å®ƒä»¬å¯ä»¥å¿«é€Ÿçš„å¼€å¯æˆ–åœæ­¢ï¼Œè¿™æ ·æœ‰åˆ©äºå¿«é€Ÿéƒ¨ç½²è¿­ä»£å’Œå¼¹æ€§ä¼¸ç¼©å®ä¾‹ã€‚\nè¿›ç¨‹åº”è¯¥è¿½æ±‚æœ€å°çš„å¯åŠ¨æ—¶é—´ï¼Œè¿™æ ·å¯ä»¥æ•æ·å‘å¸ƒï¼Œå¢åŠ å¥å£®æ€§ï¼Œå½“å‡ºç°é—®é¢˜å¯ä»¥å¿«é€Ÿåœ¨åˆ«çš„æœºå™¨éƒ¨ç½²ä¸€ä¸ªå®ä¾‹ã€‚\nè¿›ç¨‹ä¸€æ—¦æ¥æ”¶åˆ°ç»ˆæ­¢ä¿¡å·(SIGTERM)å°±ä¼šä¼˜é›…ç»ˆæ­¢ã€‚ä¼˜é›…ç»ˆæ­¢æŒ‡åœæ­¢ç›‘å¬æœåŠ¡çš„ç«¯å£ï¼Œæ‹’ç»æ‰€æœ‰æ–°çš„è¯·æ±‚ï¼Œå¹¶ç»§ç»­æ‰§è¡Œå½“å‰å·²æ¥æ”¶çš„è¯·æ±‚ï¼Œç„¶åé€€å‡ºã€‚\nè¿›ç¨‹è¿˜éœ€åœ¨é¢å¯¹çªç„¶æŒ‚æ‰çš„æƒ…å†µä¸‹ä¿æŒå¥å£®æ€§ï¼Œä¾‹å¦‚é€šè¿‡ä»»åŠ¡é˜Ÿåˆ—çš„æ–¹å¼æ¥è§£å†³è¿›ç¨‹çªç„¶æŒ‚æ‰è€Œæ²¡æœ‰å®Œæˆå¤„ç†çš„äº‹æƒ…ï¼Œæ‰€ä»¥åº”è¯¥è®¾è®¡ä¸ºä»»åŠ¡æ‰§è¡Œæ˜¯å¹‚ç­‰çš„ï¼Œå¯ä»¥è¢«é‡å¤æ‰§è¡Œï¼Œé‡å¤æ‰§è¡Œçš„ç»“æœæ˜¯ä¸€è‡´çš„ã€‚\n10. å¼€å‘ç¯å¢ƒä¸çº¿ä¸Šç¯å¢ƒç­‰ä»· å°½å¯èƒ½çš„ä¿æŒå¼€å‘ï¼Œé¢„å‘å¸ƒï¼Œçº¿ä¸Šç¯å¢ƒç›¸åŒ\nä¸åŒçš„å‘å¸ƒç¯å¢ƒå¯èƒ½å­˜åœ¨ä»¥ä¸‹å·®å¼‚ï¼š\næ—¶é—´å·®å¼‚ï¼šå¼€å‘åˆ°éƒ¨ç½²çš„å‘¨æœŸè¾ƒé•¿ã€‚ äººå‘˜å·®å¼‚ï¼šå¼€å‘äººå‘˜åªè´Ÿè´£å¼€å‘ï¼Œè¿ç»´äººå‘˜åªè´Ÿè´£éƒ¨ç½²ã€‚åˆ†å·¥è¿‡äºéš”ç¦»ã€‚ å·¥å…·å·®å¼‚ï¼šä¸åŒç¯å¢ƒçš„é…ç½®å’Œè¿è¡Œç¯å¢ƒï¼Œä½¿ç”¨çš„åç«¯ç±»å‹å¯èƒ½å­˜åœ¨ä¸åŒã€‚ åº”å°½é‡ç¼©å°æœ¬åœ°ä¸çº¿ä¸Šçš„å·®å¼‚ï¼Œç¼©çŸ­ä¸Šçº¿å‘¨æœŸï¼Œå¼€å‘è¿ç»´ä¸€ä½“åŒ–ï¼Œä¿è¯å¼€å‘ç¯å¢ƒä¸çº¿ä¸Šè¿è¡Œçš„ç¯å¢ƒä¸€è‡´ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡Dockerå®¹å™¨çš„æ–¹å¼ï¼‰ã€‚\n11. æ—¥å¿— æŠŠæ—¥å¿—å½“ä½œäº‹ä»¶æµ\næ—¥å¿—åº”è¯¥æ˜¯äº‹ä»¶æµçš„æ±‡æ€»ã€‚12-Factoråº”ç”¨æœ¬èº«ä¸è€ƒè™‘å­˜å‚¨è‡ªå·±çš„æ—¥å¿—è¾“å‡ºæµï¼Œä¸å»å†™æˆ–ç®¡ç†æ—¥å¿—æ–‡ä»¶ï¼Œè€Œæ˜¯é€šè¿‡æ ‡å‡†è¾“å‡ºï¼ˆstdoutï¼‰çš„æ–¹å¼ã€‚\næ—¥å¿—çš„æ ‡å‡†è¾“å‡ºæµå¯ä»¥é€šè¿‡å…¶ä»–ç»„ä»¶æˆªè·ï¼Œæ•´åˆå…¶ä»–çš„æ—¥å¿—è¾“å‡ºæµï¼Œä¸€å¹¶å‘ç»™ç»Ÿä¸€çš„æ—¥å¿—ä¸­å¿ƒå¤„ç†ï¼Œç”¨äºæŸ¥çœ‹æˆ–å­˜æ¡£ã€‚ä¾‹å¦‚ï¼šæ—¥å¿—æ”¶é›†å¼€æºå·¥å…·Fluentdã€‚\næˆªè·çš„æ—¥å¿—æµå¯ä»¥è¾“å‡ºè‡³æ–‡ä»¶ï¼Œæˆ–è€…åœ¨ç»ˆç«¯å®æ—¶æŸ¥çœ‹ã€‚æœ€é‡è¦çš„æ˜¯å¯ä»¥å‘é€åˆ°Splunkè¿™æ ·çš„æ—¥å¿—ç´¢å¼•åŠåˆ†æç³»ç»Ÿï¼Œæä¾›åç»­çš„åˆ†æç»Ÿè®¡åŠç›‘æ§å‘Šè­¦ç­‰åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼š\næ‰¾å‡ºè¿‡å»ä¸€æ®µæ—¶é—´çš„ç‰¹æ®Šäº‹ä»¶ã€‚ å›¾å½¢åŒ–ä¸€ä¸ªå¤§è§„æ¨¡çš„è¶‹åŠ¿ï¼Œå¦‚æ¯åˆ†é’Ÿçš„è¯·æ±‚é‡ã€‚ æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ¡ä»¶è§¦å‘å‘Šè­¦ï¼Œå¦‚æ¯åˆ†é’ŸæŠ¥é”™æ•°è¶…è¿‡æŸä¸ªè­¦æˆ’çº¿ã€‚ 12. ç®¡ç†è¿›ç¨‹ åå°ç®¡ç†ä»»åŠ¡å½“ä½œä¸€æ¬¡æ€§è¿›ç¨‹è¿è¡Œ\nå¼€å‘äººå‘˜ç»å¸¸éœ€è¦æ‰§è¡Œä¸€äº›ç®¡ç†æˆ–ç»´æŠ¤åº”ç”¨çš„ä¸€æ¬¡æ€§ä»»åŠ¡ï¼Œä¸€æ¬¡æ€§ç®¡ç†è¿›ç¨‹åº”è¯¥å’Œå¸¸é©»è¿›ç¨‹ä½¿ç”¨ç›¸åŒçš„è¿è¡Œç¯å¢ƒï¼Œå¼€å‘äººå‘˜å¯ä»¥é€šè¿‡sshæ–¹å¼æ¥æ‰§è¡Œä¸€æ¬¡æ€§è„šæœ¬æˆ–ä»»åŠ¡ã€‚\nå‚è€ƒï¼š\nhttps://12factor.net/ ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä¸»è¦ä»‹ç»PaaSå¹³å°è®¾è®¡æ¶æ„ä¸­ä½¿ç”¨åˆ°çš„æ–¹æ³•è®ºï¼Œç»Ÿç§°ä¸º12-Factor(è¦ç´ )\nç®€ä»‹ è½¯ä»¶é€šå¸¸ä¼šä½œä¸ºä¸€ç§æœåŠ¡æ¥äº¤ä»˜ï¼Œå³è½¯ä»¶å³æœ â€¦","ref":"/kubernetes-notes/paas/12-factor/","tags":["Kubernetes"],"title":"12 Factor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/web/beego/","tags":"","title":"Beego Webæ¡†æ¶"},{"body":"é—®é¢˜æè¿° æœºå™¨å†…æ ¸ç‰ˆæœ¬è¾ƒä½ï¼Œkubeletå¯åŠ¨å¼‚å¸¸ï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š\nFailed to start ContainerManager failed to initialize top level QOS containers: failed to update top level Burstable QOS cgroup : failed to set supported cgroup subsystems for cgroup [kubepods burstable]: Failed to find subsystem mount for required subsystem: pids åŸå› åˆ†æ ä½ç‰ˆæœ¬å†…æ ¸çš„cgroupä¸æ”¯æŒpidsèµ„æºçš„åŠŸèƒ½ï¼Œ\ncat /proc/cgroups #subsys_name\thierarchy\tnum_cgroups\tenabled cpuset\t5\t6\t1 cpu\t2\t76\t1 cpuacct\t2\t76\t1 memory\t4\t76\t1 devices\t10\t76\t1 freezer\t7\t6\t1 net_cls\t3\t6\t1 blkio\t8\t76\t1 perf_event\t9\t6\t1 hugetlb\t6\t6\t1 æ­£å¸¸æœºå™¨çš„cgroup\nroot@host:~# cat /proc/cgroups #subsys_name\thierarchy\tnum_cgroups\tenabled cpuset\t5\t17\t1 cpu\t7\t80\t1 cpuacct\t7\t80\t1 memory\t12\t80\t1 devices\t10\t80\t1 freezer\t2\t17\t1 net_cls\t4\t17\t1 blkio\t8\t80\t1 perf_event\t6\t17\t1 hugetlb\t11\t17\t1 pids\t3\t80\t1 # æ­¤å¤„æ”¯æŒpidsèµ„æº oom\t9\t1\t1 è§£å†³æ–¹æ¡ˆ 1ã€å‡çº§å†…æ ¸ç‰ˆæœ¬ï¼Œä½¿å¾—cgroupæ”¯æŒpidsèµ„æºã€‚\næˆ–è€…\n2ã€å°†kubeletçš„å¯åŠ¨å‚æ•°æ·»åŠ  SupportPodPidsLimit=false,SupportNodePidsLimit=false\nvi /etc/systemd/system/kubelet.service # æ·»åŠ  kubelet å¯åŠ¨å‚æ•° --feature-gates=... ,SupportPodPidsLimit=false,SupportNodePidsLimit=false \\ systemctl daemon-reload \u0026\u0026 systemctl restart kubelet.service æ–‡æ¡£å‚è€ƒï¼š\nKubernetes 1.14 ç¨³å®šæ€§æ”¹è¿›ä¸­çš„è¿›ç¨‹IDé™åˆ¶\nhttps://blog.csdn.net/qq_38900565/article/details/100707025\nhttps://adoyle.me/Today-I-Learned/k8s/k8s-deployment.html\n","categories":"","description":"","excerpt":"é—®é¢˜æè¿° æœºå™¨å†…æ ¸ç‰ˆæœ¬è¾ƒä½ï¼Œkubeletå¯åŠ¨å¼‚å¸¸ï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š\nFailed to start ContainerManager â€¦","ref":"/kubernetes-notes/trouble-shooting/node/cgroup-pid-error/","tags":["é—®é¢˜æ’æŸ¥"],"title":"Cgroupä¸æ”¯æŒpidèµ„æº"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/containerd/","tags":"","title":"Containerd"},{"body":"crictl #!/bin/bash CrictlVersion=$1 CrictlVersion=${CrictlVersion:-1.24.2} echo \"--------------install crictl--------------\" wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v${CrictlVersion}/crictl-v${CrictlVersion}-linux-amd64.tar.gz tar Cxzvf /usr/local/bin nerdctl-${NerdctlVersion}-linux-amd64.tar.gz è®¾ç½®é…ç½®æ–‡ä»¶\ncat \u003e /etc/crictl.yaml \u003c\u003c \\EOF runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 2 debug: false pull-image-on-create: false EOF å‚è€ƒï¼š\nä½¿ç”¨ crictl å¯¹ Kubernetes èŠ‚ç‚¹è¿›è¡Œè°ƒè¯• | Kubernetes\nhttps://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.mdCrictlVersion=${CrictlVersion:-1.24.2}\n","categories":"","description":"","excerpt":"crictl #!/bin/bash CrictlVersion=$1 â€¦","ref":"/kubernetes-notes/runtime/containerd/containerd-ctl/","tags":["Containerd"],"title":"Containerdå‘½ä»¤å·¥å…·"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/storage/csi/","tags":"","title":"CSI"},{"body":"1. depç®€ä»‹ depæ˜¯ä¸€ä¸ªgolangé¡¹ç›®çš„åŒ…ç®¡ç†å·¥å…·ï¼Œä¸€èˆ¬åªéœ€è¦2-3ä¸ªå‘½ä»¤å°±å¯ä»¥å°†goä¾èµ–åŒ…è‡ªåŠ¨ä¸‹è½½å¹¶å½’æ¡£åˆ°vendorçš„ç›®å½•ä¸­ã€‚depå®˜ç½‘å‚è€ƒï¼šhttps://github.com/golang/dep\n2. depå®‰è£… go get -u github.com/golang/dep/cmd/dep 3. depä½¿ç”¨ #è¿›å…¥åˆ°é¡¹ç›®ç›®å½• cd /home/gopath/src/demo #depåˆå§‹åŒ–ï¼Œåˆå§‹åŒ–é…ç½®æ–‡ä»¶Gopkg.toml dep init #depåŠ è½½ä¾èµ–åŒ…ï¼Œè‡ªåŠ¨å½’æ¡£åˆ°vendorç›®å½• dep ensure # æœ€ç»ˆä¼šç”Ÿæˆvendorç›®å½•ï¼ŒGopkg.tomlå’ŒGopkg.lockçš„æ–‡ä»¶ 4. depçš„é…ç½®æ–‡ä»¶ Gopkg.tomlè®°å½•ä¾èµ–åŒ…åˆ—è¡¨ã€‚\n# Gopkg.toml example # # Refer to https://golang.github.io/dep/docs/Gopkg.toml.html # for detailed Gopkg.toml documentation. # # required = [\"github.com/user/thing/cmd/thing\"] # ignored = [\"github.com/user/project/pkgX\", \"bitbucket.org/user/project/pkgA/pkgY\"] # # [[constraint]] # name = \"github.com/user/project\" # version = \"1.0.0\" # # [[constraint]] # name = \"github.com/user/project2\" # branch = \"dev\" # source = \"github.com/myfork/project2\" # # [[override]] # name = \"github.com/x/y\" # version = \"2.4.0\" # # [prune] # non-go = false # go-tests = true # unused-packages = true ignored = [\"demo\"] [[constraint]] name = \"github.com/BurntSushi/toml\" version = \"0.3.0\" [prune] go-tests = true unused-packages = true 5. dep-help æ›´å¤šdepçš„å‘½ä»¤å¸®åŠ©å‚è€ƒdepã€‚\n$ dep Dep is a tool for managing dependencies for Go projects Usage: \"dep [command]\" Commands: init Set up a new Go project, or migrate an existing one status Report the status of the project's dependencies ensure Ensure a dependency is safely vendored in the project prune Pruning is now performed automatically by dep ensure. version Show the dep version information Examples: dep init set up a new project dep ensure install the project's dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project Use \"dep help [command]\" for more information about a command. ","categories":"","description":"","excerpt":"1. depç®€ä»‹ depæ˜¯ä¸€ä¸ªgolangé¡¹ç›®çš„åŒ…ç®¡ç†å·¥å…·ï¼Œä¸€èˆ¬åªéœ€è¦2-3ä¸ªå‘½ä»¤å°±å¯ä»¥å°†goä¾èµ–åŒ…è‡ªåŠ¨ä¸‹è½½å¹¶å½’æ¡£åˆ°vendorçš„ç›®å½• â€¦","ref":"/golang-notes/introduction/package/dep-usage/","tags":["Golang"],"title":"depçš„ä½¿ç”¨"},{"body":"Golandé…ç½®å¼•ç”¨modç›®å½•ç´¢å¼• åœ¨preferences-Go-Go moduleä¸‹ï¼Œå¯ç”¨goæ¨¡å—é›†æˆï¼Œé…ç½®ç¯å¢ƒå˜é‡å¦‚ä¸‹ï¼Œç‚¹å‡»åº”ç”¨ã€‚\nGOPROXY=https://goproxy.cn,direct ","categories":"","description":"","excerpt":"Golandé…ç½®å¼•ç”¨modç›®å½•ç´¢å¼• åœ¨preferences-Go-Go moduleä¸‹ï¼Œå¯ç”¨goæ¨¡å—é›†æˆï¼Œé…ç½®ç¯å¢ƒå˜é‡å¦‚ä¸‹ï¼Œç‚¹å‡»åº”ç”¨ã€‚ â€¦","ref":"/linux-notes/ide/goland/","tags":["IDE"],"title":"Golandé…ç½®"},{"body":"golangci-lintæ˜¯ä¸€ç§go linterçš„å·¥å…·ï¼Œæ”¯æŒå¿«é€Ÿï¼Œå¯é…ç½®å¤šç§linterå‚æ•°çš„åŠŸèƒ½ã€‚åœ¨goé¡¹ç›®ä¸­ä½¿ç”¨golangci-lintå¯ä»¥å¸®åŠ©å¤šäººå›¢é˜Ÿå¼€å‘çš„ä»£ç é£æ ¼åŠè´¨é‡çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å¸®åŠ©å¼€å‘è€…æé«˜ä»£ç è´¨é‡ã€‚å¯ä»¥ç»“åˆGolang ä»£ç è§„èŒƒé…ç½®golangci-lintçš„å‚æ•°ã€‚\næœ¬æ–‡ä¸»è¦ä»‹ç»è¯¥å·¥å…·çš„ä½¿ç”¨åŠå¸¸è§æ¨èé…ç½®ã€‚\n1. golangci-lintå®‰è£… golangci-lintæ¨èåœ¨Makefileæ–‡ä»¶ä¸­é…ç½®å®‰è£…å’Œæ‰§è¡Œå‘½ä»¤ï¼Œå‚è€ƒå¦‚ä¸‹Makefileå†…å®¹ï¼š\n## golangci-lintçš„å®‰è£…åŠå‘½ä»¤ GOLANGCI_LINT = $(shell pwd)/bin/golangci-lint GOLANGCI_LINT_VERSION ?= v1.54.2 golangci-lint: @[ -f $(GOLANGCI_LINT) ] || { \\ set -e ;\\ curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(shell dirname $(GOLANGCI_LINT)) $(GOLANGCI_LINT_VERSION) ;\\ } .PHONY: lint lint: golangci-lint ## Run golangci-lint linter \u0026 yamllint $(GOLANGCI_LINT) run --timeout=10m .PHONY: lint-fix lint-fix: golangci-lint ## Run golangci-lint linter and perform fixes $(GOLANGCI_LINT) run --fix ## é…ç½®åœ¨buildé˜¶æ®µæ‰§è¡Œlintå‘½ä»¤ ##@ Build .PHONY: build build: fmt vet lint ## Build manager binary. bash hack/build.sh å¯ä»¥åœ¨makefileä¸­é›†æˆgolangci-lintï¼ŒåŒæ—¶é›†æˆåˆ°ä»£ç CIçš„æµç¨‹ä¸­ï¼Œåœ¨ä»£ç æäº¤å’Œmergeå‰è‡ªåŠ¨æ‰§è¡Œgolangci-lintçš„æ“ä½œã€‚\né€šè¿‡æ‰§è¡Œmake lintçš„å‘½ä»¤å³å¯è¿è¡Œgolangci-lintã€‚\n2. é…ç½®å‚æ•°è¯´æ˜ åŸºç¡€æ£€æµ‹ï¼ˆæ¨èæ‰€æœ‰é¡¹ç›®å¼€å¯ï¼‰ è¿™äº› linters æ£€æŸ¥å¸¸è§çš„ä»£ç é”™è¯¯å’Œæ½œåœ¨é—®é¢˜ï¼š\ngovet: [é»˜è®¤å¼€å¯]Go å†…ç½®çš„é™æ€åˆ†æå·¥å…·ï¼Œç”¨äºæ£€æŸ¥å¯èƒ½å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯çš„é—®é¢˜ã€‚ gosimple: [é»˜è®¤å¼€å¯]æç¤ºå¯ä»¥ç®€åŒ–çš„ä»£ç ã€‚ staticcheck: [é»˜è®¤å¼€å¯]æ£€æµ‹æ½œåœ¨é”™è¯¯ã€ä¸è‰¯ä»£ç é£æ ¼ä»¥åŠæ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚ errcheck: [é»˜è®¤å¼€å¯] æ£€æŸ¥æœªå¤„ç†çš„é”™è¯¯ã€‚ ineffassign: [é»˜è®¤å¼€å¯] æ£€æµ‹æ— æ•ˆçš„å˜é‡èµ‹å€¼ã€‚ unused: [é»˜è®¤å¼€å¯]æ£€æµ‹æœªä½¿ç”¨çš„ä»£ç ï¼ˆå˜é‡ã€å‡½æ•°ç­‰ï¼‰ã€‚ ä»£ç é£æ ¼å’Œä¸€è‡´æ€§ è¿™äº› linters ç¡®ä¿ä»£ç é£æ ¼ä¸€è‡´ï¼Œæ˜“äºç»´æŠ¤ï¼š\ngofmt: å¼ºåˆ¶æ ¼å¼åŒ–ä»£ç ã€‚ goimports: æ ¼å¼åŒ–ä»£ç å¹¶ç®¡ç†å¯¼å…¥çš„åŒ…ã€‚ typecheck: æ£€æŸ¥ç±»å‹é”™è¯¯ã€‚ misspell: æ£€æµ‹å¹¶ä¿®å¤æ‹¼å†™é”™è¯¯ã€‚ stylecheck: æä¾›é£æ ¼å»ºè®®ï¼Œç¡®ä¿ä»£ç ç¬¦åˆ Go çš„çº¦å®šã€‚ dupl: æ£€æµ‹ä»£ç ä¸­çš„é‡å¤éƒ¨åˆ†ã€‚ wsl: ç¡®ä¿ if è¯­å¥ç­‰å—ä»£ç ä¹‹é—´æœ‰é€‚å½“çš„ç©ºè¡Œã€‚ å¤æ‚åº¦å’Œæ€§èƒ½ ç”¨äºä¼˜åŒ–ä»£ç çš„å¯è¯»æ€§å’Œæ€§èƒ½ï¼š\ngocyclo: æ£€æµ‹ä»£ç çš„åœˆå¤æ‚åº¦ï¼Œç¡®ä¿å‡½æ•°å¤æ‚åº¦ä¸ä¼šè¿‡é«˜ï¼ˆé»˜è®¤é˜ˆå€¼ä¸º 30ï¼‰ã€‚ funlen: æ£€æŸ¥å‡½æ•°å’Œæ–‡ä»¶çš„é•¿åº¦ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼‰ã€‚ megacheck: ç»„åˆäº† gosimple, unused, å’Œ staticcheckã€‚ prealloc: æ£€æµ‹åœ¨å¤§å‹åˆ‡ç‰‡ä¸­æ˜¯å¦æå‰åˆ†é…äº†å®¹é‡ã€‚ å®‰å…¨æ€§ æ£€æµ‹å®‰å…¨é—®é¢˜ï¼Œé€‚åˆå¯¹ä»£ç å®‰å…¨æ€§æœ‰è¦æ±‚çš„é¡¹ç›®ï¼š\nnakedret: æ£€æŸ¥æ˜¯å¦æœ‰è£¸è¿”å›å€¼ï¼Œå¯èƒ½å¯¼è‡´æ··æ·†æˆ–é”™è¯¯ã€‚ gosec: æ£€æµ‹å¸¸è§çš„å®‰å…¨é—®é¢˜ï¼Œä¾‹å¦‚ SQL æ³¨å…¥ã€å¼±å¯†ç ç­‰ã€‚ maligned: æ£€æµ‹ç»“æ„ä½“å­—æ®µæ’åˆ—æ˜¯å¦å½±å“å†…å­˜å¯¹é½ã€‚ 3. golangci.ymlæ¨èé…ç½® å¯ä»¥é€šè¿‡.golangci.ymlé…ç½®æ–‡ä»¶æ¥é…ç½®è¯¦ç»†çš„lintå‚æ•°ï¼Œåœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹åˆ›å»ºè¯¥æ–‡ä»¶ã€‚å¸¸ç”¨æ¨èé…ç½®å¦‚ä¸‹ï¼š\nä¸ºäº†å¯ä»¥ç²¾å‡†æ§åˆ¶å¼€å¯çš„ç±»å‹ï¼Œå¯ä»¥æŠŠdisable-allè®¾ç½®ä¸ºtrueï¼Œç„¶åå†æ ¹æ®å›¢é˜Ÿéœ€è¦é€æ­¥æ·»åŠ enableçš„ç±»å‹ï¼Œé¿å…å¼€å¯æ£€æŸ¥è¿‡å¤šå½±å“å¼€å‘æ•ˆç‡ã€‚\nlintersï¼šä¸»è¦é…ç½®å¼€å¯æˆ–å…³é—­çš„æ£€æŸ¥ç±»å‹ã€‚ linters-settingsï¼šé’ˆå¯¹lintersä¸­çš„å…·ä½“æ£€æŸ¥ç±»å‹é…ç½®è¯¥ç±»å‹çš„å‚æ•°ã€‚ run: timeout: \"10m\" linters: disable-all: true enable: # basic - govet - staticcheck - errcheck - ineffassign - gosimple - unused # style - gofmt - goimports - misspell - stylecheck - dupl - wsl - goconst # complexity - funlen - gocyclo - lll # security - gosec linters-settings: funlen: # Checks the number of lines in a function. lines: 80 # Checks the number of statements in a function. statements: 40 # Ignore comments when counting lines. ignore-comments: true lines-in-file: 800 gocyclo: # Minimal code complexity to report. # Default: 30 (but we recommend 10-20) min-complexity: 15 lll: # Max line length, lines longer will be reported. # Default: 120. line-length: 120 dupl: # Tokens count to trigger issue. # Default: 150 threshold: 100 output: format: colored-line-number print-issued-lines: true print-config: true å‚è€ƒï¼š\nhttps://golangci-lint.run/ https://golangci-lint.run/usage/configuration/ https://golangci-lint.run/usage/linters/ ","categories":"","description":"","excerpt":"golangci-lintæ˜¯ä¸€ç§go linterçš„å·¥å…·ï¼Œæ”¯æŒå¿«é€Ÿï¼Œå¯é…ç½®å¤šç§linterå‚æ•°çš„åŠŸèƒ½ã€‚åœ¨goé¡¹ç›®ä¸­ä½¿ â€¦","ref":"/golang-notes/standard/golangci-lint/","tags":"","title":"golangci-lintçš„ä½¿ç”¨"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/","tags":"","title":"Golangå­¦ä¹ ç¬”è®°"},{"body":" æœ¬æ–‡åŸºäºã€ŠKubernetesæºç å‰–æã€‹æ•´ç†ï¼Œç»“åˆk8s v1.22.0ä»£ç åˆ†æ\næ¦‚è¿° k8såŸºäºEtcdä½œä¸ºå­˜å‚¨ï¼ŒEtcdæ˜¯åˆ†å¸ƒå¼çš„KVå­˜å‚¨é›†ç¾¤ï¼ŒEtcdä¸­å­˜å‚¨äº†k8sçš„å…ƒæ•°æ®ã€äº‹ä»¶æ•°æ®ã€çŠ¶æ€æ•°æ®ç­‰ï¼Œæ•°æ®å‰ç¼€ä¸º/registryä¸‹ï¼Œå…·ä½“çš„å„ç±»å¯¹è±¡çš„keyå¯ä»¥å‚è€ƒEtcdä¸­çš„k8sæ•°æ®ã€‚\nEtcdä½œä¸ºk8så”¯ä¸€å­˜å‚¨ï¼Œå…¼å…·äº†MySQLå­˜å‚¨å…ƒæ•°æ®å’Œæ¶ˆæ¯é˜Ÿåˆ—å­˜å‚¨ä»»åŠ¡äº‹ä»¶çš„åŠŸèƒ½ã€‚\nEtcdå­˜å‚¨æ¶æ„è®¾è®¡ k8så¯¹etcdçš„æ“ä½œè¿›è¡Œäº†åˆ†å±‚å°è£…ï¼Œä»ä¸Š(k8s)åˆ°ä¸‹(etcd)ï¼Œåˆ†åˆ«ä¸ºï¼š\nRESTStorageï¼ˆk8så¯¹è±¡æ“ä½œå°è£…ï¼‰ RegistryStoreï¼ˆBeforeFunc,AfterFuncï¼‰ Storage.Interfaceï¼ˆå¢åˆ æ”¹æŸ¥æ–¹æ³•å°è£…ï¼‰ CacherStorageï¼ˆEtcdç¼“å­˜å±‚ï¼‰ UnderlyingStorageï¼ˆä¸Etcdäº¤äº’ï¼‰ RESTStorageå­˜å‚¨æ¥å£ æ¯ç§k8sèµ„æºå®ç°RESTStorageæ¥å£ï¼Œæ¥å£ä»£ç å¦‚ä¸‹ï¼š\nkubernetes/staging/src/k8s.io/apiserver/pkg/registry/rest/rest.go\n// Storage is a generic interface for RESTful storage services. // Resources which are exported to the RESTful API of apiserver need to implement this interface. It is expected // that objects may implement any of the below interfaces. type Storage interface { // New returns an empty object that can be used with Create and Update after request data has been put into it. // This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) New() runtime.Object } k8såŸºäºetcdç›¸å…³å°è£…ä»£ç ä¸»è¦åœ¨/pkg/registryç›®å½•ä¸‹ã€‚\nå…¶ä¸­æ¯ç§èµ„æºå¯¹äºstorageæ¥å£çš„å®ç°å®šä¹‰åœ¨/pkg/registry/\u003cèµ„æºç»„\u003e/\u003cèµ„æº\u003e/storage/storage.goã€‚ä»¥ä¸‹ä»¥deploymentä¸ºä¾‹ä¸²è”k8sä¸­å…³äºetcdçš„è°ƒç”¨æµç¨‹ï¼Œè°ƒç”¨é¡ºåºä»ä¸Š(k8s)åˆ°ä¸‹(etcd)ã€‚\nDeploymentStorage kubernetes/pkg/registry/apps/deployment/storage/storage.go\n// DeploymentStorage includes dummy storage for Deployments and for Scale subresource. type DeploymentStorage struct { Deployment *REST Status *StatusREST Scale *ScaleREST Rollback *RollbackREST } // REST implements a RESTStorage for Deployments. type REST struct { *genericregistry.Store categories []string } // StatusREST implements the REST endpoint for changing the status of a deployment type StatusREST struct { store *genericregistry.Store } // ScaleREST implements a Scale for Deployment. type ScaleREST struct { store *genericregistry.Store } // RollbackREST implements the REST endpoint for initiating the rollback of a deployment type RollbackREST struct { store *genericregistry.Store } ","categories":"","description":"","excerpt":" æœ¬æ–‡åŸºäºã€ŠKubernetesæºç å‰–æã€‹æ•´ç†ï¼Œç»“åˆk8s v1.22.0ä»£ç åˆ†æ\næ¦‚è¿° k8såŸºäºEtcdä½œä¸ºå­˜å‚¨ï¼ŒEtcdæ˜¯åˆ†å¸ƒå¼çš„KV â€¦","ref":"/k8s-source-code-analysis/summary/etcd-storage/","tags":["æºç åˆ†æ"],"title":"k8sä¸­Etcdå­˜å‚¨çš„å®ç°"},{"body":"1. é…ç½®æ–‡ä»¶è·¯å¾„ é»˜è®¤çš„é…ç½®æ–‡ä»¶ä½äº/usr/share/defaults/kata-containers/configuration.tomlï¼Œå¦‚æœ/etc/kata-containers/configuration.tomlçš„é…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™ä¼šæ›¿ä»£é»˜è®¤çš„é…ç½®æ–‡ä»¶ã€‚\næŸ¥çœ‹é…ç½®æ–‡ä»¶çš„è·¯å¾„å‘½ä»¤å¦‚ä¸‹ï¼š\n# kata-runtime --kata-show-default-config-paths /etc/kata-containers/configuration.toml /usr/share/defaults/kata-containers/configuration.toml æŒ‡å®šè‡ªå®šä¹‰é…ç½®æ–‡ä»¶è¿è¡Œ\nkata-runtime --kata-config=/some/where/configuration.toml ... 2. kata-env æŸ¥çœ‹runtimeä½¿ç”¨åˆ°çš„ç¯å¢ƒå‚æ•°ï¼Œ\nkata-runtime kata-env è¾“å‡ºå†…å®¹å¦‚ä¸‹ï¼š\n[Meta] Version = \"1.0.23\" [Runtime] Debug = false Trace = false DisableGuestSeccomp = true DisableNewNetNs = false Path = \"/usr/bin/kata-runtime\" [Runtime.Version] Semver = \"1.7.2\" Commit = \"9b9282693cfbcf70d442916bea56771cc6fc3afe\" OCI = \"1.0.1-dev\" [Runtime.Config] Path = \"/usr/share/defaults/kata-containers/configuration.toml\" [Hypervisor] MachineType = \"pc\" Version = \"QEMU emulator version 2.11.0\\nCopyright (c) 2003-2017 Fabrice Bellard and the QEMU Project developers\" Path = \"/usr/bin/qemu-lite-system-x86_64\" BlockDeviceDriver = \"virtio-scsi\" EntropySource = \"/dev/urandom\" Msize9p = 8192 MemorySlots = 10 Debug = false UseVSock = false SharedFS = \"virtio-9p\" [Image] Path = \"/usr/share/kata-containers/kata-containers-image_centos_1.7.2_agent_20190702.img\" [Kernel] Path = \"/usr/share/kata-containers/vmlinuz-4.19.28.42-6.1.container\" Parameters = \"init=/usr/lib/systemd/systemd systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket systemd.mask=systemd-journald.service systemd.mask=systemd-journald.socket systemd.mask=systemd-journal-flush.service systemd.mask=systemd-journald-dev-log.socket systemd.mask=systemd-udevd.service systemd.mask=systemd-udevd.socket systemd.mask=systemd-udev-trigger.service systemd.mask=systemd-udevd-kernel.socket systemd.mask=systemd-udevd-control.socket systemd.mask=systemd-timesyncd.service systemd.mask=systemd-update-utmp.service systemd.mask=systemd-tmpfiles-setup.service systemd.mask=systemd-tmpfiles-cleanup.service systemd.mask=systemd-tmpfiles-cleanup.timer systemd.mask=tmp.mount systemd.mask=systemd-random-seed.service systemd.mask=systemd-coredump@.service\" [Initrd] Path = \"\" [Proxy] Type = \"kataProxy\" Version = \"kata-proxy version 1.7.2-a56df7c\" Path = \"/usr/libexec/kata-containers/kata-proxy\" Debug = false [Shim] Type = \"kataShim\" Version = \"kata-shim version 1.7.2-2ea178c\" Path = \"/usr/libexec/kata-containers/kata-shim\" Debug = false [Agent] Type = \"kata\" Debug = false Trace = false TraceMode = \"\" TraceType = \"\" [Host] Kernel = \"4.14.105-1-tlinux3-0008\" Architecture = \"amd64\" VMContainerCapable = true SupportVSocks = true [Host.Distro] Name = \"Tencent tlinux\" Version = \"2.2\" [Host.CPU] Vendor = \"GenuineIntel\" Model = \"Intel(R) Xeon(R) CPU X3440 @ 2.53GHz\" [Netmon] Version = \"kata-netmon version 1.7.2\" Path = \"/usr/libexec/kata-containers/kata-netmon\" Debug = false Enable = false 3. configuration.toml # Copyright (c) 2017-2019 Intel Corporation # # SPDX-License-Identifier: Apache-2.0 # # XXX: WARNING: this file is auto-generated. # XXX: # XXX: Source file: \"cli/config/configuration-qemu.toml.in\" # XXX: Project: # XXX: Name: Kata Containers # XXX: Type: kata [hypervisor.qemu] path = \"/usr/bin/qemu-lite-system-x86_64\" kernel = \"/usr/share/kata-containers/vmlinuz.container\" image = \"/usr/share/kata-containers/kata-containers.img\" machine_type = \"pc\" # Optional space-separated list of options to pass to the guest kernel. # For example, use `kernel_params = \"vsyscall=emulate\"` if you are having # trouble running pre-2.15 glibc. # # WARNING: - any parameter specified here will take priority over the default # parameter value of the same name used to start the virtual machine. # Do not set values here unless you understand the impact of doing so as you # may stop the virtual machine from booting. # To see the list of default parameters, enable hypervisor debug, create a # container and look for 'default-kernel-parameters' log entries. kernel_params = \"\" # Path to the firmware. # If you want that qemu uses the default firmware leave this option empty firmware = \"\" # Machine accelerators # comma-separated list of machine accelerators to pass to the hypervisor. # For example, `machine_accelerators = \"nosmm,nosmbus,nosata,nopit,static-prt,nofw\"` machine_accelerators=\"\" # Default number of vCPUs per SB/VM: # unspecified or 0 --\u003e will be set to 1 # \u003c 0 --\u003e will be set to the actual number of physical cores # \u003e 0 \u003c= number of physical cores --\u003e will be set to the specified number # \u003e number of physical cores --\u003e will be set to the actual number of physical cores default_vcpus = 1 # Default maximum number of vCPUs per SB/VM: # unspecified or == 0 --\u003e will be set to the actual number of physical cores or to the maximum number # of vCPUs supported by KVM if that number is exceeded # \u003e 0 \u003c= number of physical cores --\u003e will be set to the specified number # \u003e number of physical cores --\u003e will be set to the actual number of physical cores or to the maximum number # of vCPUs supported by KVM if that number is exceeded # WARNING: Depending of the architecture, the maximum number of vCPUs supported by KVM is used when # the actual number of physical cores is greater than it. # WARNING: Be aware that this value impacts the virtual machine's memory footprint and CPU # the hotplug functionality. For example, `default_maxvcpus = 240` specifies that until 240 vCPUs # can be added to a SB/VM, but the memory footprint will be big. Another example, with # `default_maxvcpus = 8` the memory footprint will be small, but 8 will be the maximum number of # vCPUs supported by the SB/VM. In general, we recommend that you do not edit this variable, # unless you know what are you doing. default_maxvcpus = 0 # Bridges can be used to hot plug devices. # Limitations: # * Currently only pci bridges are supported # * Until 30 devices per bridge can be hot plugged. # * Until 5 PCI bridges can be cold plugged per VM. # This limitation could be a bug in qemu or in the kernel # Default number of bridges per SB/VM: # unspecified or 0 --\u003e will be set to 1 # \u003e 1 \u003c= 5 --\u003e will be set to the specified number # \u003e 5 --\u003e will be set to 5 default_bridges = 1 # Default memory size in MiB for SB/VM. # If unspecified then it will be set 2048 MiB. default_memory = 2048 # # Default memory slots per SB/VM. # If unspecified then it will be set 10. # This is will determine the times that memory will be hotadded to sandbox/VM. #memory_slots = 10 # The size in MiB will be plused to max memory of hypervisor. # It is the memory address space for the NVDIMM devie. # If set block storage driver (block_device_driver) to \"nvdimm\", # should set memory_offset to the size of block device. # Default 0 #memory_offset = 0 # Disable block device from being used for a container's rootfs. # In case of a storage driver like devicemapper where a container's # root file system is backed by a block device, the block device is passed # directly to the hypervisor for performance reasons. # This flag prevents the block device from being passed to the hypervisor, # 9pfs is used instead to pass the rootfs. disable_block_device_use = false # Shared file system type: # - virtio-9p (default) # - virtio-fs shared_fs = \"virtio-9p\" # Path to vhost-user-fs daemon. virtio_fs_daemon = \"/usr/bin/virtiofsd-x86_64\" # Default size of DAX cache in MiB virtio_fs_cache_size = 1024 # Cache mode: # # - none # Metadata, data, and pathname lookup are not cached in guest. They are # always fetched from host and any changes are immediately pushed to host. # # - auto # Metadata and pathname lookup cache expires after a configured amount of # time (default is 1 second). Data is cached while the file is open (close # to open consistency). # # - always # Metadata, data, and pathname lookup are cached in guest and never expire. virtio_fs_cache = \"always\" # Block storage driver to be used for the hypervisor in case the container # rootfs is backed by a block device. This is virtio-scsi, virtio-blk # or nvdimm. block_device_driver = \"virtio-scsi\" # Specifies cache-related options will be set to block devices or not. # Default false #block_device_cache_set = true # Specifies cache-related options for block devices. # Denotes whether use of O_DIRECT (bypass the host page cache) is enabled. # Default false #block_device_cache_direct = true # Specifies cache-related options for block devices. # Denotes whether flush requests for the device are ignored. # Default false #block_device_cache_noflush = true # Enable iothreads (data-plane) to be used. This causes IO to be # handled in a separate IO thread. This is currently only implemented # for SCSI. # enable_iothreads = false # Enable pre allocation of VM RAM, default false # Enabling this will result in lower container density # as all of the memory will be allocated and locked # This is useful when you want to reserve all the memory # upfront or in the cases where you want memory latencies # to be very predictable # Default false #enable_mem_prealloc = true # Enable huge pages for VM RAM, default false # Enabling this will result in the VM memory # being allocated using huge pages. # This is useful when you want to use vhost-user network # stacks within the container. This will automatically # result in memory pre allocation #enable_hugepages = true # Enable swap of vm memory. Default false. # The behaviour is undefined if mem_prealloc is also set to true #enable_swap = true # This option changes the default hypervisor and kernel parameters # to enable debug output where available. This extra output is added # to the proxy logs, but only when proxy debug is also enabled. # # Default false #enable_debug = true # Disable the customizations done in the runtime when it detects # that it is running on top a VMM. This will result in the runtime # behaving as it would when running on bare metal. # #disable_nesting_checks = true # This is the msize used for 9p shares. It is the number of bytes # used for 9p packet payload. #msize_9p = 8192 # If true and vsocks are supported, use vsocks to communicate directly # with the agent and no proxy is started, otherwise use unix # sockets and start a proxy to communicate with the agent. # Default false #use_vsock = true # VFIO devices are hotplugged on a bridge by default. # Enable hotplugging on root bus. This may be required for devices with # a large PCI bar, as this is a current limitation with hotplugging on # a bridge. This value is valid for \"pc\" machine type. # Default false #hotplug_vfio_on_root_bus = true # If host doesn't support vhost_net, set to true. Thus we won't create vhost fds for nics. # Default false #disable_vhost_net = true # # Default entropy source. # The path to a host source of entropy (including a real hardware RNG) # /dev/urandom and /dev/random are two main options. # Be aware that /dev/random is a blocking source of entropy. If the host # runs out of entropy, the VMs boot time will increase leading to get startup # timeouts. # The source of entropy /dev/urandom is non-blocking and provides a # generally acceptable source of entropy. It should work well for pretty much # all practical purposes. #entropy_source= \"/dev/urandom\" # Path to OCI hook binaries in the *guest rootfs*. # This does not affect host-side hooks which must instead be added to # the OCI spec passed to the runtime. # # You can create a rootfs with hooks by customizing the osbuilder scripts: # https://github.com/kata-containers/osbuilder # # Hooks must be stored in a subdirectory of guest_hook_path according to their # hook type, i.e. \"guest_hook_path/{prestart,postart,poststop}\". # The agent will scan these directories for executable files and add them, in # lexicographical order, to the lifecycle of the guest container. # Hooks are executed in the runtime namespace of the guest. See the official documentation: # https://github.com/opencontainers/runtime-spec/blob/v1.0.1/config.md#posix-platform-hooks # Warnings will be logged if any error is encountered will scanning for hooks, # but it will not abort container execution. #guest_hook_path = \"/usr/share/oci/hooks\" [factory] # VM templating support. Once enabled, new VMs are created from template # using vm cloning. They will share the same initial kernel, initramfs and # agent memory by mapping it readonly. It helps speeding up new container # creation and saves a lot of memory if there are many kata containers running # on the same host. # # When disabled, new VMs are created from scratch. # # Note: Requires \"initrd=\" to be set (\"image=\" is not supported). # # Default false #enable_template = true # Specifies the path of template. # # Default \"/run/vc/vm/template\" #template_path = \"/run/vc/vm/template\" # The number of caches of VMCache: # unspecified or == 0 --\u003e VMCache is disabled # \u003e 0 --\u003e will be set to the specified number # # VMCache is a function that creates VMs as caches before using it. # It helps speed up new container creation. # The function consists of a server and some clients communicating # through Unix socket. The protocol is gRPC in protocols/cache/cache.proto. # The VMCache server will create some VMs and cache them by factory cache. # It will convert the VM to gRPC format and transport it when gets # requestion from clients. # Factory grpccache is the VMCache client. It will request gRPC format # VM and convert it back to a VM. If VMCache function is enabled, # kata-runtime will request VM from factory grpccache when it creates # a new sandbox. # # Default 0 #vm_cache_number = 0 # Specify the address of the Unix socket that is used by VMCache. # # Default /var/run/kata-containers/cache.sock #vm_cache_endpoint = \"/var/run/kata-containers/cache.sock\" [proxy.kata] path = \"/usr/libexec/kata-containers/kata-proxy\" # If enabled, proxy messages will be sent to the system log # (default: disabled) #enable_debug = true [shim.kata] path = \"/usr/libexec/kata-containers/kata-shim\" # If enabled, shim messages will be sent to the system log # (default: disabled) #enable_debug = true # If enabled, the shim will create opentracing.io traces and spans. # (See https://www.jaegertracing.io/docs/getting-started). # # Note: By default, the shim runs in a separate network namespace. Therefore, # to allow it to send trace details to the Jaeger agent running on the host, # it is necessary to set 'disable_new_netns=true' so that it runs in the host # network namespace. # # (default: disabled) #enable_tracing = true [agent.kata] # If enabled, make the agent display debug-level messages. # (default: disabled) #enable_debug = true # Enable agent tracing. # # If enabled, the default trace mode is \"dynamic\" and the # default trace type is \"isolated\". The trace mode and type are set # explicity with the `trace_type=` and `trace_mode=` options. # # Notes: # # - Tracing is ONLY enabled when `enable_tracing` is set: explicitly # setting `trace_mode=` and/or `trace_type=` without setting `enable_tracing` # will NOT activate agent tracing. # # - See https://github.com/kata-containers/agent/blob/master/TRACING.md for # full details. # # (default: disabled) #enable_tracing = true # #trace_mode = \"dynamic\" #trace_type = \"isolated\" [netmon] # If enabled, the network monitoring process gets started when the # sandbox is created. This allows for the detection of some additional # network being added to the existing network namespace, after the # sandbox has been created. # (default: disabled) #enable_netmon = true # Specify the path to the netmon binary. path = \"/usr/libexec/kata-containers/kata-netmon\" # If enabled, netmon messages will be sent to the system log # (default: disabled) #enable_debug = true [runtime] # If enabled, the runtime will log additional debug messages to the # system log # (default: disabled) #enable_debug = true # # Internetworking model # Determines how the VM should be connected to the # the container network interface # Options: # # - bridged # Uses a linux bridge to interconnect the container interface to # the VM. Works for most cases except macvlan and ipvlan. # # - macvtap # Used when the Container network interface can be bridged using # macvtap. # # - none # Used when customize network. Only creates a tap device. No veth pair. # # - tcfilter # Uses tc filter rules to redirect traffic from the network interface # provided by plugin to a tap interface connected to the VM. # internetworking_model=\"tcfilter\" # disable guest seccomp # Determines whether container seccomp profiles are passed to the virtual # machine and applied by the kata agent. If set to true, seccomp is not applied # within the guest # (default: true) disable_guest_seccomp=true # If enabled, the runtime will create opentracing.io traces and spans. # (See https://www.jaegertracing.io/docs/getting-started). # (default: disabled) #enable_tracing = true # If enabled, the runtime will not create a network namespace for shim and hypervisor processes. # This option may have some potential impacts to your host. It should only be used when you know what you're doing. # `disable_new_netns` conflicts with `enable_netmon` # `disable_new_netns` conflicts with `internetworking_model=bridged` and `internetworking_model=macvtap`. It works only # with `internetworking_model=none`. The tap device will be in the host network namespace and can connect to a bridge # (like OVS) directly. # If you are using docker, `disable_new_netns` only works with `docker run --net=none` # (default: false) #disable_new_netns = true # Enabled experimental feature list, format: [\"a\", \"b\"]. # Experimental features are features not stable enough for production, # They may break compatibility, and are prepared for a big version bump. # Supported experimental features: # 1. \"newstore\": new persist storage driver which breaks backward compatibility, #\texpected to move out of experimental in 2.0.0. # (default: []) experimental=[] å‚è€ƒï¼š\nhttps://github.com/kata-containers/runtime#configuration ","categories":"","description":"","excerpt":"1. é…ç½®æ–‡ä»¶è·¯å¾„ é»˜è®¤çš„é…ç½®æ–‡ä»¶ä½ â€¦","ref":"/kubernetes-notes/runtime/kata/kata-container-conf/","tags":["Kubernetes"],"title":"kataé…ç½®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kube-apiserver/","tags":"","title":"kube-apiserver"},{"body":"æœ¬æ–‡ä¸»è¦è¯´æ˜å¦‚ä½•ä½¿ç”¨kubeadmæ¥å‡çº§k8sé›†ç¾¤ã€‚\n1. ç‰ˆæœ¬æ³¨æ„äº‹é¡¹ å‡è®¾k8sçš„ç‰ˆæœ¬æ ¼å¼ä¸ºx.y.zï¼Œé‚£ä¹ˆä½¿ç”¨kubeadmæœ€å¤šåªèƒ½å‡çº§åˆ°y+1ç‰ˆæœ¬ï¼Œæˆ–è€…æ˜¯å½“å‰yç‰ˆæœ¬çš„æœ€æ–°ç‰ˆæœ¬ã€‚ä¾‹å¦‚ä½ k8sé›†ç¾¤çš„ç‰ˆæœ¬ä¸º1.24.xï¼Œé‚£ä¹ˆä½ æœ€å¤§ç‰ˆæœ¬åªèƒ½ä¸‹è½½1.25.xçš„kubeadmæ¥å‡çº§ç‰ˆæœ¬ã€‚\nå› æ­¤å‡çº§å‰éœ€è¦æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥éªŒè¯å¯å‡çº§çš„ç‰ˆæœ¬ã€‚\nkubeadm upgrade plan 1.1. ç‰ˆæœ¬è·¨åº¦è¿‡å¤§ å¦‚æœå‡ºç°ä»¥ä¸‹æŠ¥é”™ï¼Œè¯´æ˜å‡çº§çš„ç‰ˆæœ¬è·¨åº¦è¿‡å¤§ã€‚\n# kubeadmçš„ç‰ˆæœ¬ä¸ºv1.26.7 ./kubeadm version kubeadm version: \u0026version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:22:13Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"} # å½“å‰k8sé›†ç¾¤ç‰ˆæœ¬1.24.2 ç‰ˆæœ¬è·¨åº¦è¿‡å¤§ã€‚ ./kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [upgrade/config] FATAL: this version of kubeadm only supports deploying clusters with the control plane version \u003e= 1.25.0. Current version: v1.24.2 To see the stack trace of this error execute with --v=5 or higher 1.2. å¯å‡çº§çš„ç‰ˆæœ¬è®¡åˆ’ å¯å‡çº§çš„ç‰ˆæœ¬è®¡åˆ’å¦‚ä¸‹\nå½“å‰çš„k8sç‰ˆæœ¬ä¸º1.24.2ï¼Œå¯å‡çº§çš„ç‰ˆæœ¬æ˜¯1.24.16æˆ–1.25.12ï¼Œå…¶ä¸­etcdå‡çº§çš„ç‰ˆæœ¬ä¸º3.5.6-0ã€‚\n./kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.24.2 [upgrade/versions] kubeadm version: v1.25.12 I0815 21:41:34.096199 2934255 version.go:256] remote version is much newer: v1.27.4; falling back to: stable-1.25 [upgrade/versions] Target version: v1.25.12 [upgrade/versions] Latest version in the v1.24 series: v1.24.16 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT TARGET kubelet 4 x v1.24.2 v1.24.16 Upgrade to the latest version in the v1.24 series: COMPONENT CURRENT TARGET kube-apiserver v1.24.2 v1.24.16 kube-controller-manager v1.24.2 v1.24.16 kube-scheduler v1.24.2 v1.24.16 kube-proxy v1.24.2 v1.24.16 CoreDNS v1.8.6 v1.9.3 etcd 3.5.3-0 3.5.6-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.24.16 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT TARGET kubelet 4 x v1.24.2 v1.25.12 Upgrade to the latest stable version: COMPONENT CURRENT TARGET kube-apiserver v1.24.2 v1.25.12 kube-controller-manager v1.24.2 v1.25.12 kube-scheduler v1.24.2 v1.25.12 kube-proxy v1.24.2 v1.25.12 CoreDNS v1.8.6 v1.9.3 etcd 3.5.3-0 3.5.6-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.25.12 _____________________________________________________________________ The table below shows the current state of component configs as understood by this version of kubeadm. Configs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually upgrade to is denoted in the \"PREFERRED VERSION\" column. API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED kubeproxy.config.k8s.io v1alpha1 v1alpha1 no kubelet.config.k8s.io v1beta1 v1beta1 no _____________________________________________________________________ 2. ç‰ˆæœ¬å‡çº§æ­¥éª¤ 2.1. å‡†å¤‡å·¥ä½œ ä¸‹è½½æŒ‡å®šç‰ˆæœ¬çš„kubeadmäºŒè¿›åˆ¶\næŸ¥çœ‹å‡çº§è®¡åˆ’ï¼škubeadm upgrade plan\n2.2. å‡çº§masterèŠ‚ç‚¹ 2.2.1. å‡çº§ç¬¬ä¸€ä¸ªmasterèŠ‚ç‚¹ kubeadm upgrade apply v1.25.x -f å‡çº§ç»“æŸä¼šæŸ¥çœ‹éƒ½ä»¥ä¸‹è¾“å‡ºï¼š\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.25.x\". Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. 2.2.2. æ‰‹åŠ¨å‡çº§ä½ çš„ CNI é©±åŠ¨æ’ä»¶ã€‚ ä½ çš„å®¹å™¨ç½‘ç»œæ¥å£ï¼ˆCNIï¼‰é©±åŠ¨åº”è¯¥æä¾›äº†ç¨‹åºè‡ªèº«çš„å‡çº§è¯´æ˜ã€‚ å‚é˜…æ’ä»¶é¡µé¢æŸ¥æ‰¾ä½ çš„ CNI é©±åŠ¨ï¼Œ å¹¶æŸ¥çœ‹æ˜¯å¦éœ€è¦å…¶ä»–å‡çº§æ­¥éª¤ã€‚\nå¦‚æœ CNI é©±åŠ¨ä½œä¸º DaemonSet è¿è¡Œï¼Œåˆ™åœ¨å…¶ä»–æ§åˆ¶å¹³é¢èŠ‚ç‚¹ä¸Šä¸éœ€è¦æ­¤æ­¥éª¤ã€‚\n2.2.3. å‡çº§å…¶ä»–masterèŠ‚ç‚¹ ä¸‹è½½æŒ‡å®šç‰ˆæœ¬çš„kubeadmç»„ä»¶ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å‡çº§ï¼Œæ³¨æ„åŒºåˆ«äºç¬¬ä¸€ä¸ªmasterçš„å‡çº§å‘½ä»¤ã€‚\nkubeadm upgrade node 2.2.4. å‡çº§masterçš„kubeletç»„ä»¶ å°†èŠ‚ç‚¹æ ‡è®°ä¸ºä¸å¯è°ƒåº¦å¹¶é©±é€æ‰€æœ‰è´Ÿè½½ï¼Œå‡†å¤‡èŠ‚ç‚¹çš„ç»´æŠ¤ï¼š\n# å°† \u003cnode-to-drain\u003e æ›¿æ¢ä¸ºä½ è¦è…¾ç©ºçš„æ§åˆ¶é¢èŠ‚ç‚¹åç§° kubectl drain \u003cnode-to-drain\u003e --ignore-daemonsets ä¸‹è½½kubeletå’Œkubectl\n# ç”¨æœ€æ–°çš„è¡¥ä¸ç‰ˆæœ¬æ›¿æ¢ 1.27.x-00 ä¸­çš„ x apt-mark unhold kubelet kubectl \u0026\u0026 \\ apt-get update \u0026\u0026 apt-get install -y kubelet=1.27.x-00 kubectl=1.27.x-00 \u0026\u0026 \\ apt-mark hold kubelet kubectl é‡å¯kubelet\nsudo systemctl daemon-reload sudo systemctl restart kubelet è§£é™¤èŠ‚ç‚¹ä¿æŠ¤\n# å°† \u003cnode-to-uncordon\u003e æ›¿æ¢ä¸ºä½ çš„èŠ‚ç‚¹åç§° kubectl uncordon \u003cnode-to-uncordon\u003e 2.3. å‡çº§workerèŠ‚ç‚¹ åŒ 2.2.4. å‡çº§masterçš„kubeletç»„ä»¶çš„æ­¥éª¤ï¼ŒworkerèŠ‚ç‚¹åªéœ€è¦å‡çº§kubeletã€‚\n3. å‡çº§ç‰ˆæœ¬å›æ»š kubeadmå‡çº§è¿‡ç¨‹ä¸­ä¼šæŠŠç›¸å…³ç›®å½•å¤‡ä»½åˆ°/etc/kubernetes/tmpç›®å½•ï¼Œå¤‡ä»½å†…å®¹å¦‚ä¸‹ï¼š\ntmp/ â”œâ”€â”€ kubeadm-backup-etcd-2023-08-16-14-50-50 â”‚Â â””â”€â”€ etcd â””â”€â”€ kubeadm-backup-manifests-2023-08-16-14-50-50 â”œâ”€â”€ etcd.yaml â”œâ”€â”€ kube-apiserver.yaml â”œâ”€â”€ kube-controller-manager.yaml â””â”€â”€ kube-scheduler.yaml å¦‚æœÂ kubeadm upgradeÂ å¤±è´¥å¹¶ä¸”æ²¡æœ‰å›æ»šï¼Œä¾‹å¦‚ç”±äºæ‰§è¡ŒæœŸé—´èŠ‚ç‚¹æ„å¤–å…³é—­ï¼Œ ä½ å¯ä»¥å†æ¬¡è¿è¡ŒÂ kubeadm upgradeã€‚ æ­¤å‘½ä»¤æ˜¯å¹‚ç­‰çš„ï¼Œå¹¶æœ€ç»ˆç¡®ä¿å®é™…çŠ¶æ€æ˜¯ä½ å£°æ˜çš„æœŸæœ›çŠ¶æ€ã€‚\nè¦ä»æ•…éšœçŠ¶æ€æ¢å¤ï¼Œä½ è¿˜å¯ä»¥è¿è¡ŒÂ kubeadm upgrade apply --forceÂ è€Œæ— éœ€æ›´æ”¹é›†ç¾¤æ­£åœ¨è¿è¡Œçš„ç‰ˆæœ¬ã€‚\nåœ¨å‡çº§æœŸé—´ï¼Œkubeadm å‘Â /etc/kubernetes/tmpÂ ç›®å½•ä¸‹çš„å¦‚ä¸‹å¤‡ä»½æ–‡ä»¶å¤¹å†™å…¥æ•°æ®ï¼š\nkubeadm-backup-etcd-\u003cdate\u003e-\u003ctime\u003e kubeadm-backup-manifests-\u003cdate\u003e-\u003ctime\u003e kubeadm-backup-etcdÂ åŒ…å«å½“å‰æ§åˆ¶é¢èŠ‚ç‚¹æœ¬åœ° etcd æˆå‘˜æ•°æ®çš„å¤‡ä»½ã€‚ å¦‚æœ etcd å‡çº§å¤±è´¥å¹¶ä¸”è‡ªåŠ¨å›æ»šä¹Ÿæ— æ³•ä¿®å¤ï¼Œåˆ™å¯ä»¥å°†æ­¤æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹å¤åˆ¶åˆ°Â /var/lib/etcdÂ è¿›è¡Œæ‰‹å·¥ä¿®å¤ã€‚å¦‚æœä½¿ç”¨çš„æ˜¯å¤–éƒ¨çš„ etcdï¼Œåˆ™æ­¤å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºç©ºã€‚\nkubeadm-backup-manifestsÂ åŒ…å«å½“å‰æ§åˆ¶é¢èŠ‚ç‚¹çš„é™æ€ Pod æ¸…å•æ–‡ä»¶çš„å¤‡ä»½ç‰ˆæœ¬ã€‚ å¦‚æœå‡çº§å¤±è´¥å¹¶ä¸”æ— æ³•è‡ªåŠ¨å›æ»šï¼Œåˆ™æ­¤æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹å¯ä»¥å¤åˆ¶åˆ°Â /etc/kubernetes/manifestsÂ ç›®å½•å®ç°æ‰‹å·¥æ¢å¤ã€‚ å¦‚æœç”±äºæŸäº›åŸå› ï¼Œåœ¨å‡çº§å‰åæŸä¸ªç»„ä»¶çš„æ¸…å•æœªå‘ç”Ÿå˜åŒ–ï¼Œåˆ™ kubeadm ä¹Ÿä¸ä¼šä¸ºä¹‹ç”Ÿæˆå¤‡ä»½ç‰ˆæœ¬ã€‚\n4. é—®é¢˜æ’æŸ¥ åœ¨å‡çº§k8s ä»1.25.12åˆ°1.26.7çš„è¿‡ç¨‹ä¸­ï¼Œé‡åˆ°masterèŠ‚ç‚¹çš„æœåŠ¡èµ·ä¸æ¥ï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š\n\"CreatePodSandbox for pod failed\" err=\"open /run/systemd/resolve/resolv.conf: no such file or directory\" pod=\"kube-system/kube-apiserver\" ç°è±¡ä¸»è¦æ˜¯é™æ€podèµ·ä¸æ¥ï¼ŒåŒ…æ‹¬etcdç­‰ã€‚\nå…·ä½“è§£å†³æ–¹æ³•å‚è€ƒï¼šopen /run/systemd/resolve/resolv.conf\n# æŸ¥çœ‹ä»¥ä¸‹çš„resolv.confæ˜¯å¦å­˜åœ¨ cat /var/lib/kubelet/config.yaml | grep resolvConf /run/systemd/resolve/resolv.conf # å¦‚æœä¸å­˜åœ¨ï¼Œæ£€æŸ¥systemd-resolvedæ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œ systemctl status systemd-resolved # å¦‚æœæ²¡æœ‰è¿è¡Œï¼Œåˆ™è¿è¡Œè¯¥æœåŠ¡ systemctl start systemd-resolved # æˆ–è€…æ–°å»ºæ–‡ä»¶/run/systemd/resolve/resolv.confï¼Œå¹¶å°†å…¶ä»–masterçš„æ–‡ä»¶æ‹·è´è¿‡æ¥ã€‚ å‚è€ƒï¼š\nå‡çº§ kubeadm é›†ç¾¤ | Kubernetes å‡çº§ Linux èŠ‚ç‚¹ | Kubernetes å®‰è£…æ‰©å±•ï¼ˆAddonï¼‰ | Kubernetes ","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦è¯´æ˜å¦‚ä½•ä½¿ç”¨kubeadmæ¥å‡çº§k8sé›†ç¾¤ã€‚\n1. ç‰ˆæœ¬æ³¨æ„äº‹é¡¹ å‡è®¾k8sçš„ç‰ˆæœ¬æ ¼å¼ä¸ºx.y.zï¼Œé‚£ä¹ˆä½¿ç”¨kubeadmæœ€å¤šåªèƒ½å‡ â€¦","ref":"/kubernetes-notes/setup/kubeadm-upgrade/","tags":["kubeadm"],"title":"kubeadmå‡çº§k8sé›†ç¾¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/kubectl/","tags":"","title":"kubectlå·¥å…·"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/kubeedge/code-analysis/","tags":"","title":"KubeEdgeæºç åˆ†æ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/object/","tags":"","title":"kuberneteså¯¹è±¡"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/kvm/kubevirt/","tags":"","title":"KubeVirt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/","tags":"","title":"Linuxå­¦ä¹ ç¬”è®°"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-controller-manager éƒ¨åˆ†çš„ä»£ç ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æ kubernetes/cmd/kube-controller-manageréƒ¨åˆ†ï¼Œè¯¥éƒ¨åˆ†ä¸»è¦æ¶‰åŠå„ç§ç±»å‹çš„controllerçš„å‚æ•°è§£æï¼ŒåŠåˆå§‹åŒ–ï¼Œä¾‹å¦‚ deployment controller å’Œstatefulset controllerã€‚å¹¶æ²¡æœ‰å…·ä½“controllerè¿è¡Œçš„è¯¦ç»†é€»è¾‘ï¼Œè¯¥éƒ¨åˆ†ä½äºkubernetes/pkg/controlleræ¨¡å—ï¼Œå¾…åç»­æ–‡ç« åˆ†æã€‚\nkube-controller-managerçš„cmdéƒ¨åˆ†ä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\nkube-controller-manager â”œâ”€â”€ app â”‚Â â”œâ”€â”€ apps.go # åŒ…å«:startDeploymentControllerã€startReplicaSetControllerã€startStatefulSetControllerã€startDaemonSetController â”‚Â â”œâ”€â”€ autoscaling.go # startHPAController â”‚Â â”œâ”€â”€ batch.go # startJobControllerã€startCronJobController â”‚Â â”œâ”€â”€ bootstrap.go â”‚Â â”œâ”€â”€ certificates.go â”‚Â â”œâ”€â”€ cloudproviders.go â”‚Â â”œâ”€â”€ config â”‚Â â”‚Â â””â”€â”€ config.go # config: controller manageræ‰§è¡Œçš„ä¸Šä¸‹æ–‡ â”‚Â â”œâ”€â”€ controllermanager.go # åŒ…å«:NewControllerManagerCommandã€Runã€NewControllerInitializersã€StartControllersç­‰ â”‚Â â”œâ”€â”€ core.go # startServiceControllerã€startNodeIpamControllerã€startPersistentVolumeBinderControllerã€startNamespaceControllerç­‰ â”‚Â â”œâ”€â”€ options # åŒ…å«ä¸åŒcontrollerçš„optionå‚æ•° â”‚Â â”‚Â â”œâ”€â”€ attachdetachcontroller.go â”‚Â â”‚Â â”œâ”€â”€ csrsigningcontroller.go â”‚Â â”‚Â â”œâ”€â”€ daemonsetcontroller.go # DaemonSetControllerOptions â”‚Â â”‚Â â”œâ”€â”€ deploymentcontroller.go # DeploymentControllerOptions â”‚Â â”‚Â â”œâ”€â”€ deprecatedcontroller.go â”‚Â â”‚Â â”œâ”€â”€ endpointcontroller.go â”‚Â â”‚Â â”œâ”€â”€ garbagecollectorcontroller.go â”‚Â â”‚Â â”œâ”€â”€ hpacontroller.go â”‚Â â”‚Â â”œâ”€â”€ jobcontroller.go â”‚Â â”‚Â â”œâ”€â”€ namespacecontroller.go # NamespaceControllerOptions â”‚Â â”‚Â â”œâ”€â”€ nodeipamcontroller.go â”‚Â â”‚Â â”œâ”€â”€ nodelifecyclecontroller.go â”‚Â â”‚Â â”œâ”€â”€ options.go # KubeControllerManagerOptionsã€NewKubeControllerManagerOptions â”‚Â â”‚Â â”œâ”€â”€ persistentvolumebindercontroller.go â”‚Â â”‚Â â”œâ”€â”€ podgccontroller.go â”‚Â â”‚Â â”œâ”€â”€ replicasetcontroller.go # ReplicaSetControllerOptions â”‚Â â”‚Â â”œâ”€â”€ replicationcontroller.go â”‚Â â”‚Â â”œâ”€â”€ resourcequotacontroller.go â”‚Â â”‚Â â”œâ”€â”€ serviceaccountcontroller.go â”‚Â â”‚Â â””â”€â”€ ttlafterfinishedcontroller.go â””â”€â”€ controller-manager.go # mainå…¥å£å‡½æ•° 1. Mainå‡½æ•° kube-controller-managerçš„å…¥å£å‡½æ•°Mainå‡½æ•°ï¼Œä»ç„¶æ˜¯é‡‡ç”¨ç»Ÿä¸€çš„ä»£ç é£æ ¼ï¼Œä½¿ç”¨Cobraå‘½ä»¤è¡Œæ¡†æ¶ã€‚\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewControllerManagerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } æ ¸å¿ƒä»£ç ï¼š\n// åˆå§‹åŒ–å‘½ä»¤è¡Œç»“æ„ä½“ command := app.NewControllerManagerCommand() // æ‰§è¡ŒExecute err := command.Execute() 2. NewControllerManagerCommand è¯¥éƒ¨åˆ†ä»£ç ä½äºï¼škubernetes/cmd/kube-controller-manager/app/controllermanager.go\n// NewControllerManagerCommand creates a *cobra.Command object with default parameters func NewControllerManagerCommand() *cobra.Command { ... cmd := \u0026cobra.Command{ Use: \"kube-controller-manager\", Long: `The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } if err := Run(c.Complete(), wait.NeverStop); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } ... } æ„å»ºä¸€ä¸ª*cobra.Commandå¯¹è±¡ï¼Œç„¶åæ‰§è¡ŒRunå‡½æ•°ã€‚\n2.1. NewKubeControllerManagerOptions s, err := options.NewKubeControllerManagerOptions() if err != nil { glog.Fatalf(\"unable to initialize command options: %v\", err) } åˆå§‹åŒ–controllerManagerçš„å‚æ•°ï¼Œå…¶ä¸­ä¸»è¦åŒ…æ‹¬äº†å„ç§controllerçš„optionï¼Œä¾‹å¦‚DeploymentControllerOptions:\n// DeploymentControllerOptions holds the DeploymentController options. type DeploymentControllerOptions struct { ConcurrentDeploymentSyncs int32 DeploymentControllerSyncPeriod metav1.Duration } å…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// NewKubeControllerManagerOptions creates a new KubeControllerManagerOptions with a default config. func NewKubeControllerManagerOptions() (*KubeControllerManagerOptions, error) { componentConfig, err := NewDefaultComponentConfig(ports.InsecureKubeControllerManagerPort) if err != nil { return nil, err } s := KubeControllerManagerOptions{ Generic: cmoptions.NewGenericControllerManagerConfigurationOptions(componentConfig.Generic), KubeCloudShared: cmoptions.NewKubeCloudSharedOptions(componentConfig.KubeCloudShared), AttachDetachController: \u0026AttachDetachControllerOptions{ ReconcilerSyncLoopPeriod: componentConfig.AttachDetachController.ReconcilerSyncLoopPeriod, }, CSRSigningController: \u0026CSRSigningControllerOptions{ ClusterSigningCertFile: componentConfig.CSRSigningController.ClusterSigningCertFile, ClusterSigningKeyFile: componentConfig.CSRSigningController.ClusterSigningKeyFile, ClusterSigningDuration: componentConfig.CSRSigningController.ClusterSigningDuration, }, DaemonSetController: \u0026DaemonSetControllerOptions{ ConcurrentDaemonSetSyncs: componentConfig.DaemonSetController.ConcurrentDaemonSetSyncs, }, DeploymentController: \u0026DeploymentControllerOptions{ ConcurrentDeploymentSyncs: componentConfig.DeploymentController.ConcurrentDeploymentSyncs, DeploymentControllerSyncPeriod: componentConfig.DeploymentController.DeploymentControllerSyncPeriod, }, DeprecatedFlags: \u0026DeprecatedControllerOptions{ RegisterRetryCount: componentConfig.DeprecatedController.RegisterRetryCount, }, EndpointController: \u0026EndpointControllerOptions{ ConcurrentEndpointSyncs: componentConfig.EndpointController.ConcurrentEndpointSyncs, }, GarbageCollectorController: \u0026GarbageCollectorControllerOptions{ ConcurrentGCSyncs: componentConfig.GarbageCollectorController.ConcurrentGCSyncs, EnableGarbageCollector: componentConfig.GarbageCollectorController.EnableGarbageCollector, }, HPAController: \u0026HPAControllerOptions{ HorizontalPodAutoscalerSyncPeriod: componentConfig.HPAController.HorizontalPodAutoscalerSyncPeriod, HorizontalPodAutoscalerUpscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerUpscaleForbiddenWindow, HorizontalPodAutoscalerDownscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleForbiddenWindow, HorizontalPodAutoscalerDownscaleStabilizationWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleStabilizationWindow, HorizontalPodAutoscalerCPUInitializationPeriod: componentConfig.HPAController.HorizontalPodAutoscalerCPUInitializationPeriod, HorizontalPodAutoscalerInitialReadinessDelay: componentConfig.HPAController.HorizontalPodAutoscalerInitialReadinessDelay, HorizontalPodAutoscalerTolerance: componentConfig.HPAController.HorizontalPodAutoscalerTolerance, HorizontalPodAutoscalerUseRESTClients: componentConfig.HPAController.HorizontalPodAutoscalerUseRESTClients, }, JobController: \u0026JobControllerOptions{ ConcurrentJobSyncs: componentConfig.JobController.ConcurrentJobSyncs, }, NamespaceController: \u0026NamespaceControllerOptions{ NamespaceSyncPeriod: componentConfig.NamespaceController.NamespaceSyncPeriod, ConcurrentNamespaceSyncs: componentConfig.NamespaceController.ConcurrentNamespaceSyncs, }, NodeIPAMController: \u0026NodeIPAMControllerOptions{ NodeCIDRMaskSize: componentConfig.NodeIPAMController.NodeCIDRMaskSize, }, NodeLifecycleController: \u0026NodeLifecycleControllerOptions{ EnableTaintManager: componentConfig.NodeLifecycleController.EnableTaintManager, NodeMonitorGracePeriod: componentConfig.NodeLifecycleController.NodeMonitorGracePeriod, NodeStartupGracePeriod: componentConfig.NodeLifecycleController.NodeStartupGracePeriod, PodEvictionTimeout: componentConfig.NodeLifecycleController.PodEvictionTimeout, }, PersistentVolumeBinderController: \u0026PersistentVolumeBinderControllerOptions{ PVClaimBinderSyncPeriod: componentConfig.PersistentVolumeBinderController.PVClaimBinderSyncPeriod, VolumeConfiguration: componentConfig.PersistentVolumeBinderController.VolumeConfiguration, }, PodGCController: \u0026PodGCControllerOptions{ TerminatedPodGCThreshold: componentConfig.PodGCController.TerminatedPodGCThreshold, }, ReplicaSetController: \u0026ReplicaSetControllerOptions{ ConcurrentRSSyncs: componentConfig.ReplicaSetController.ConcurrentRSSyncs, }, ReplicationController: \u0026ReplicationControllerOptions{ ConcurrentRCSyncs: componentConfig.ReplicationController.ConcurrentRCSyncs, }, ResourceQuotaController: \u0026ResourceQuotaControllerOptions{ ResourceQuotaSyncPeriod: componentConfig.ResourceQuotaController.ResourceQuotaSyncPeriod, ConcurrentResourceQuotaSyncs: componentConfig.ResourceQuotaController.ConcurrentResourceQuotaSyncs, }, SAController: \u0026SAControllerOptions{ ConcurrentSATokenSyncs: componentConfig.SAController.ConcurrentSATokenSyncs, }, ServiceController: \u0026cmoptions.ServiceControllerOptions{ ConcurrentServiceSyncs: componentConfig.ServiceController.ConcurrentServiceSyncs, }, TTLAfterFinishedController: \u0026TTLAfterFinishedControllerOptions{ ConcurrentTTLSyncs: componentConfig.TTLAfterFinishedController.ConcurrentTTLSyncs, }, SecureServing: apiserveroptions.NewSecureServingOptions().WithLoopback(), InsecureServing: (\u0026apiserveroptions.DeprecatedInsecureServingOptions{ BindAddress: net.ParseIP(componentConfig.Generic.Address), BindPort: int(componentConfig.Generic.Port), BindNetwork: \"tcp\", }).WithLoopback(), Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(), Authorization: apiserveroptions.NewDelegatingAuthorizationOptions(), } s.Authentication.RemoteKubeConfigFileOptional = true s.Authorization.RemoteKubeConfigFileOptional = true s.Authorization.AlwaysAllowPaths = []string{\"/healthz\"} s.SecureServing.ServerCert.CertDirectory = \"/var/run/kubernetes\" s.SecureServing.ServerCert.PairName = \"kube-controller-manager\" s.SecureServing.BindPort = ports.KubeControllerManagerPort gcIgnoredResources := make([]kubectrlmgrconfig.GroupResource, 0, len(garbagecollector.DefaultIgnoredResources())) for r := range garbagecollector.DefaultIgnoredResources() { gcIgnoredResources = append(gcIgnoredResources, kubectrlmgrconfig.GroupResource{Group: r.Group, Resource: r.Resource}) } s.GarbageCollectorController.GCIgnoredResources = gcIgnoredResources return \u0026s, nil } 2.2. AddFlagSet æ·»åŠ å‚æ•°åŠå¸®åŠ©å‡½æ•°ã€‚\nfs := cmd.Flags() namedFlagSets := s.Flags(KnownControllers(), ControllersDisabledByDefault.List()) for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := apiserverflag.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStderr(), namedFlagSets, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStdout(), namedFlagSets, cols) }) 3. Run æ­¤éƒ¨åˆ†çš„ä»£ç ä½äºcmd/kube-controller-manager/app/controllermanager.go\nåŸºäºKubeControllerManagerOptionsè¿è¡ŒcontrollerManagerï¼Œä¸é€€å‡ºã€‚\n// Run runs the KubeControllerManagerOptions. This should never exit. func Run(c *config.CompletedConfig, stopCh \u003c-chan struct{}) error { ... run := func(ctx context.Context) { ... controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) if err != nil { glog.Fatalf(\"error building controller context: %v\", err) } saTokenControllerInitFunc := serviceAccountTokenControllerStarter{rootClientBuilder: rootClientBuilder}.startServiceAccountTokenController if err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil { glog.Fatalf(\"error starting controllers: %v\", err) } controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) select {} } ... } Runå‡½æ•°æ¶‰åŠçš„æ ¸å¿ƒä»£ç å¦‚ä¸‹ï¼š\n// åˆ›å»ºcontrollerçš„context controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) // å¯åŠ¨å„ç§controller err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux) å…¶ä¸­StartControllersä¸­çš„å…¥å‚NewControllerInitializersåˆå§‹åŒ–äº†å„ç§controllerã€‚\n3.1. CreateControllerContext CreateControllerContextæ„å»ºäº†å„ç§controlleræ‰€éœ€çš„èµ„æºçš„ä¸Šä¸‹æ–‡ï¼Œå„ç§controlleråœ¨å¯åŠ¨æ—¶ï¼Œå…¥å‚ä¸ºè¯¥contextï¼Œå…·ä½“å‚è€ƒinitFn(ctx)ã€‚\n// CreateControllerContext creates a context struct containing references to resources needed by the // controllers such as the cloud provider and clientBuilder. rootClientBuilder is only used for // the shared-informers client and token controller. func CreateControllerContext(s *config.CompletedConfig, rootClientBuilder, clientBuilder controller.ControllerClientBuilder, stop \u003c-chan struct{}) (ControllerContext, error) { versionedClient := rootClientBuilder.ClientOrDie(\"shared-informers\") sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)()) // If apiserver is not running we should wait for some time and fail only then. This is particularly // important when we start apiserver and controller manager at the same time. if err := genericcontrollermanager.WaitForAPIServer(versionedClient, 10*time.Second); err != nil { return ControllerContext{}, fmt.Errorf(\"failed to wait for apiserver being healthy: %v\", err) } // Use a discovery client capable of being refreshed. discoveryClient := rootClientBuilder.ClientOrDie(\"controller-discovery\") cachedClient := cacheddiscovery.NewMemCacheClient(discoveryClient.Discovery()) restMapper := restmapper.NewDeferredDiscoveryRESTMapper(cachedClient) go wait.Until(func() { restMapper.Reset() }, 30*time.Second, stop) availableResources, err := GetAvailableResources(rootClientBuilder) if err != nil { return ControllerContext{}, err } cloud, loopMode, err := createCloudProvider(s.ComponentConfig.KubeCloudShared.CloudProvider.Name, s.ComponentConfig.KubeCloudShared.ExternalCloudVolumePlugin, s.ComponentConfig.KubeCloudShared.CloudProvider.CloudConfigFile, s.ComponentConfig.KubeCloudShared.AllowUntaggedCloud, sharedInformers) if err != nil { return ControllerContext{}, err } ctx := ControllerContext{ ClientBuilder: clientBuilder, InformerFactory: sharedInformers, ComponentConfig: s.ComponentConfig, RESTMapper: restMapper, AvailableResources: availableResources, Cloud: cloud, LoopMode: loopMode, Stop: stop, InformersStarted: make(chan struct{}), ResyncPeriod: ResyncPeriod(s), } return ctx, nil } æ ¸å¿ƒä»£ç ä¸ºNewSharedInformerFactoryã€‚\n// åˆ›å»ºSharedInformerFactory sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)()) // èµ‹å€¼ç»™ControllerContext ctx := ControllerContext{ InformerFactory: sharedInformers, } SharedInformerFactoryæä¾›äº†å…¬å…±çš„k8så¯¹è±¡çš„informersã€‚\n// SharedInformerFactory provides shared informers for resources in all known // API group versions. type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh \u003c-chan struct{}) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Coordination() coordination.Interface Core() core.Interface Events() events.Interface Extensions() extensions.Interface Networking() networking.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Settings() settings.Interface Storage() storage.Interface } 3.2. NewControllerInitializers NewControllerInitializerså®šä¹‰äº†å„ç§controllerçš„ç±»å‹å’Œå…¶å¯¹äºçš„å¯åŠ¨å‡½æ•°ï¼Œä¾‹å¦‚deployment``ã€statefulsetã€replicasetã€replicationcontrollerã€namespaceç­‰ã€‚\n// NewControllerInitializers is a public map of named controller groups (you can start more than one in an init func) // paired to their InitFunc. This allows for structured downstream composition and subdivision. func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { controllers := map[string]InitFunc{} controllers[\"endpoint\"] = startEndpointController controllers[\"replicationcontroller\"] = startReplicationController controllers[\"podgc\"] = startPodGCController controllers[\"resourcequota\"] = startResourceQuotaController controllers[\"namespace\"] = startNamespaceController controllers[\"serviceaccount\"] = startServiceAccountController controllers[\"garbagecollector\"] = startGarbageCollectorController controllers[\"daemonset\"] = startDaemonSetController controllers[\"job\"] = startJobController controllers[\"deployment\"] = startDeploymentController controllers[\"replicaset\"] = startReplicaSetController controllers[\"horizontalpodautoscaling\"] = startHPAController controllers[\"disruption\"] = startDisruptionController controllers[\"statefulset\"] = startStatefulSetController controllers[\"cronjob\"] = startCronJobController controllers[\"csrsigning\"] = startCSRSigningController controllers[\"csrapproving\"] = startCSRApprovingController controllers[\"csrcleaner\"] = startCSRCleanerController controllers[\"ttl\"] = startTTLController controllers[\"bootstrapsigner\"] = startBootstrapSignerController controllers[\"tokencleaner\"] = startTokenCleanerController controllers[\"nodeipam\"] = startNodeIpamController if loopMode == IncludeCloudLoops { controllers[\"service\"] = startServiceController controllers[\"route\"] = startRouteController // TODO: volume controller into the IncludeCloudLoops only set. // TODO: Separate cluster in cloud check from node lifecycle controller. } controllers[\"nodelifecycle\"] = startNodeLifecycleController controllers[\"persistentvolume-binder\"] = startPersistentVolumeBinderController controllers[\"attachdetach\"] = startAttachDetachController controllers[\"persistentvolume-expander\"] = startVolumeExpandController controllers[\"clusterrole-aggregation\"] = startClusterRoleAggregrationController controllers[\"pvc-protection\"] = startPVCProtectionController controllers[\"pv-protection\"] = startPVProtectionController controllers[\"ttl-after-finished\"] = startTTLAfterFinishedController return controllers } 3.3. StartControllers func StartControllers(ctx ControllerContext, startSATokenController InitFunc, controllers map[string]InitFunc, unsecuredMux *mux.PathRecorderMux) error { ... for controllerName, initFn := range controllers { if !ctx.IsControllerEnabled(controllerName) { glog.Warningf(\"%q is disabled\", controllerName) continue } time.Sleep(wait.Jitter(ctx.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter)) glog.V(1).Infof(\"Starting %q\", controllerName) debugHandler, started, err := initFn(ctx) if err != nil { glog.Errorf(\"Error starting %q\", controllerName) return err } if !started { glog.Warningf(\"Skipping %q\", controllerName) continue } if debugHandler != nil \u0026\u0026 unsecuredMux != nil { basePath := \"/debug/controllers/\" + controllerName unsecuredMux.UnlistedHandle(basePath, http.StripPrefix(basePath, debugHandler)) unsecuredMux.UnlistedHandlePrefix(basePath+\"/\", http.StripPrefix(basePath, debugHandler)) } glog.Infof(\"Started %q\", controllerName) } return nil } æ ¸å¿ƒä»£ç ï¼š\nfor controllerName, initFn := range controllers { debugHandler, started, err := initFn(ctx) } å¯åŠ¨å„ç§controllerï¼Œcontrollerçš„å¯åŠ¨å‡½æ•°åœ¨NewControllerInitializersä¸­å®šä¹‰äº†ï¼Œä¾‹å¦‚ï¼š\n// deployment controllers[\"deployment\"] = startDeploymentController // statefulset controllers[\"statefulset\"] = startStatefulSetController 3.4. InformerFactory.Start InformerFactoryå®é™…ä¸Šæ˜¯SharedInformerFactoryï¼Œå…·ä½“çš„å®ç°é€»è¾‘åœ¨client-goä¸­çš„informerçš„å®ç°æœºåˆ¶ã€‚\ncontrollerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) 3.4.1. SharedInformerFactory SharedInformerFactoryæ˜¯ä¸€ä¸ªinformerå·¥å‚çš„æ¥å£å®šä¹‰ã€‚\n// SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh \u003c-chan struct{}) InformerFor(obj runtime.Object, newFunc NewInformerFunc) cache.SharedIndexInformer } 3.4.2. sharedInformerFactory.Start Startæ–¹æ³•åˆå§‹åŒ–å„ç§ç±»å‹çš„informer\n// Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh \u003c-chan struct{}) { f.lock.Lock() defer f.lock.Unlock() for informerType, informer := range f.informers { if !f.startedInformers[informerType] { go informer.Run(stopCh) f.startedInformers[informerType] = true } } } 3.4.3. sharedIndexInformer.Run sharedIndexInformer.Runå…·ä½“è¿è¡Œäº†sharedIndexInformerçš„å®ç°é€»è¾‘ï¼Œè¯¥éƒ¨åˆ†å¾…åç»­å¯¹informeræœºåˆ¶åšä¸“é¢˜åˆ†æã€‚\nfunc (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) cfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) } 4. initFn(ctx) initFnå®é™…è°ƒç”¨çš„å°±æ˜¯å„ç§ç±»å‹çš„controllerï¼Œä»£ç ä½äºkubernetes/cmd/kube-controller-manager/app/apps.goï¼Œæœ¬æ–‡ä»¥startStatefulSetControllerå’ŒstartDeploymentControllerä¸ºä¾‹ï¼Œcontrollerä¸­å®é™…è°ƒç”¨çš„å‡½æ•°é€»è¾‘ä½äºkubernetes/pkg/controllerä¸­ï¼Œå¾…åç»­åˆ†æã€‚\n4.1. startStatefulSetController func startStatefulSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"statefulsets\"}] { return nil, false, nil } go statefulset.NewStatefulSetController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Apps().V1().StatefulSets(), ctx.InformerFactory.Core().V1().PersistentVolumeClaims(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.ClientBuilder.ClientOrDie(\"statefulset-controller\"), ).Run(1, ctx.Stop) return nil, true, nil } å…¶ä¸­ä½¿ç”¨åˆ°äº†InformerFactoryï¼ŒåŒ…å«äº†Podsã€StatefulSetsã€PersistentVolumeClaimsã€ControllerRevisionsçš„informerã€‚\nstartStatefulSetControllerä¸»è¦è°ƒç”¨çš„å‡½æ•°ä¸ºNewStatefulSetControllerå’Œå¯¹åº”çš„Runå‡½æ•°ã€‚\n4.2. startDeploymentController func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"}] { return nil, false, nil } dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil } startDeploymentControllerä¸»è¦è°ƒç”¨çš„å‡½æ•°ä¸ºNewDeploymentControllerå’Œå¯¹åº”çš„Runå‡½æ•°ã€‚è¯¥éƒ¨åˆ†é€»è¾‘åœ¨kubernetes/pkg/controllerä¸­ã€‚\n5. æ€»ç»“ Kube-controller-managerçš„ä»£ç é£æ ¼ä»ç„¶æ˜¯Cobraå‘½ä»¤è¡Œæ¡†æ¶ã€‚é€šè¿‡æ„é€ ControllerManagerCommandï¼Œç„¶åæ‰§è¡Œcommand.Execute()å‡½æ•°ã€‚åŸºæœ¬çš„æµç¨‹å°±æ˜¯æ„é€ optionï¼Œæ·»åŠ Flagsï¼Œæ‰§è¡ŒRunå‡½æ•°ã€‚ cmdéƒ¨åˆ†çš„è°ƒç”¨æµç¨‹å¦‚ä¸‹ï¼šMain--\u003eNewControllerManagerCommand--\u003e Run(c.Complete(), wait.NeverStop)--\u003eStartControllers--\u003einitFn(ctx)--\u003estartDeploymentController/startStatefulSetController--\u003ests.NewStatefulSetController.Run/dc.NewDeploymentController.Run--\u003epkg/controllerã€‚ å…¶ä¸­CreateControllerContextå‡½æ•°ç”¨æ¥åˆ›å»ºå„ç±»å‹controlleræ‰€éœ€è¦ä½¿ç”¨çš„contextï¼ŒNewControllerInitializersåˆå§‹åŒ–äº†å„ç§ç±»å‹çš„controllerï¼Œå…¶ä¸­å°±åŒ…æ‹¬DeploymentControllerå’ŒStatefulSetControllerç­‰ã€‚ åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\næ„é€ controller manager optionï¼Œå¹¶è½¬åŒ–ä¸ºConfigå¯¹è±¡ï¼Œæ‰§è¡ŒRunå‡½æ•°ã€‚ åŸºäºConfigå¯¹è±¡åˆ›å»ºControllerContextï¼Œå…¶ä¸­åŒ…å«InformerFactoryã€‚ åŸºäºControllerContextè¿è¡Œå„ç§controllerï¼Œå„ç§controllerçš„å®šä¹‰åœ¨NewControllerInitializersä¸­ã€‚ æ‰§è¡ŒInformerFactory.Startã€‚ æ¯ç§controlleréƒ½ä¼šæ„é€ è‡ªèº«çš„ç»“æ„ä½“å¹¶æ‰§è¡Œå¯¹åº”çš„Runå‡½æ•°ã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-controller-manager https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/controller-manager.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/controllermanager.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/apps.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ† â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/newcontrollermanagercommand/","tags":["æºç åˆ†æ"],"title":"kube-controller-manageræºç åˆ†æï¼ˆä¸€ï¼‰ä¹‹ NewControllerManagerCommand"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æ https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kubelet éƒ¨åˆ†çš„ä»£ç ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æ kubernetes/cmd/kubeletéƒ¨åˆ†ï¼Œè¯¥éƒ¨åˆ†ä¸»è¦æ¶‰åŠkubeletçš„å‚æ•°è§£æï¼ŒåŠåˆå§‹åŒ–å’Œæ„é€ ç›¸å…³çš„ä¾èµ–ç»„ä»¶ï¼ˆä¸»è¦åœ¨kubeDepsç»“æ„ä½“ä¸­ï¼‰ï¼Œå¹¶æ²¡æœ‰kubeletè¿è¡Œçš„è¯¦ç»†é€»è¾‘ï¼Œè¯¥éƒ¨åˆ†ä½äºkubernetes/pkg/kubeletæ¨¡å—ï¼Œå¾…åç»­æ–‡ç« åˆ†æã€‚\nkubeletçš„cmdä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\nkubelet â”œâ”€â”€ app â”‚Â â”œâ”€â”€ auth.go â”‚Â â”œâ”€â”€ init_others.go â”‚Â â”œâ”€â”€ init_windows.go â”‚Â â”œâ”€â”€ options # åŒ…æ‹¬kubeletä½¿ç”¨åˆ°çš„option â”‚Â â”‚Â â”œâ”€â”€ container_runtime.go â”‚Â â”‚Â â”œâ”€â”€ globalflags.go â”‚Â â”‚Â â”œâ”€â”€ globalflags_linux.go â”‚Â â”‚Â â”œâ”€â”€ globalflags_other.go â”‚Â â”‚Â â”œâ”€â”€ options.go # åŒ…æ‹¬KubeletFlagsã€AddFlagsã€AddKubeletConfigFlagsç­‰ â”‚Â â”‚Â â”œâ”€â”€ osflags_others.go â”‚Â â”‚Â â””â”€â”€ osflags_windows.go â”‚Â â”œâ”€â”€ plugins.go â”‚Â â”œâ”€â”€ server.go # åŒ…æ‹¬NewKubeletCommandã€Runã€RunKubeletã€CreateAndInitKubeletã€startKubeletç­‰ â”‚Â â”œâ”€â”€ server_linux.go â”‚Â â””â”€â”€ server_unsupported.go â””â”€â”€ kubelet.go # kubeletçš„mainå…¥å£å‡½æ•° 1. Main å‡½æ•° kubeletçš„å…¥å£å‡½æ•° Main å‡½æ•°ï¼Œå…·ä½“ä»£ç å‚è€ƒï¼šhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kubelet/kubelet.goã€‚\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewKubeletCommand(server.SetupSignalHandler()) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } kubeletä»£ç ä¸»è¦é‡‡ç”¨äº†Cobraå‘½ä»¤è¡Œæ¡†æ¶ï¼Œæ ¸å¿ƒä»£ç å¦‚ä¸‹ï¼š\n// åˆå§‹åŒ–å‘½ä»¤è¡Œ command := app.NewKubeletCommand(server.SetupSignalHandler()) // æ‰§è¡ŒExecute err := command.Execute() 2. NewKubeletCommand NewKubeletCommandåŸºäºå‚æ•°åˆ›å»ºäº†ä¸€ä¸ª*cobra.Commandå¯¹è±¡ã€‚å…¶ä¸­æ ¸å¿ƒéƒ¨åˆ†ä»£ç ä¸ºå‚æ•°è§£æéƒ¨åˆ†å’ŒRunå‡½æ•°ã€‚\n// NewKubeletCommand creates a *cobra.Command object with default parameters func NewKubeletCommand(stopCh \u003c-chan struct{}) *cobra.Command { ... cmd := \u0026cobra.Command{ Use: componentKubelet, Long: `...`, // The Kubelet has special flag parsing requirements to enforce flag precedence rules, // so we do all our parsing manually in Run, below. // DisableFlagParsing=true provides the full set of flags passed to the kubelet in the // `args` arg to Run, without Cobra's interference. DisableFlagParsing: true, Run: func(cmd *cobra.Command, args []string) { ... // run the kubelet glog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil { glog.Fatal(err) } }, } ... return cmd } 2.1. å‚æ•°è§£æ kubeletå¼€å¯äº†DisableFlagParsingå‚æ•°ï¼Œæ²¡æœ‰ä½¿ç”¨Cobraæ¡†æ¶ä¸­çš„é»˜è®¤å‚æ•°è§£æï¼Œè€Œæ˜¯è‡ªå®šä¹‰å‚æ•°è§£æã€‚\n2.1.1. åˆå§‹åŒ–å‚æ•°å’Œé…ç½® åˆå§‹åŒ–å‚æ•°è§£æï¼Œåˆå§‹åŒ–cleanFlagSetï¼ŒkubeletFlagsï¼ŒkubeletConfigã€‚\ncleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() 2.1.2. æ‰“å°å¸®åŠ©ä¿¡æ¯å’Œç‰ˆæœ¬ä¿¡æ¯ å¦‚æœè¾“å…¥éæ³•å‚æ•°åˆ™æ‰“å°ä½¿ç”¨å¸®åŠ©ä¿¡æ¯ã€‚\n// initial flag parse, since we disable cobra's flag parsing if err := cleanFlagSet.Parse(args); err != nil { cmd.Usage() glog.Fatal(err) } // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) \u003e 0 { cmd.Usage() glog.Fatalf(\"unknown command: %s\", cmds[0]) } é‡åˆ°helpå’Œversionå‚æ•°åˆ™æ‰“å°ç›¸å…³å†…å®¹å¹¶é€€å‡ºã€‚\n// short-circuit on help help, err := cleanFlagSet.GetBool(\"help\") if err != nil { glog.Fatal(`\"help\" flag is non-bool, programmer error, please correct`) } if help { cmd.Help() return } // short-circuit on verflag verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cleanFlagSet) 2.1.3. kubelet config åŠ è½½å¹¶æ ¡éªŒkubelet configã€‚å…¶ä¸­åŒ…æ‹¬æ ¡éªŒåˆå§‹åŒ–çš„kubeletFlagsï¼Œå¹¶ä»kubeletFlagsçš„KubeletConfigFileå‚æ•°è·å–kubelet configçš„å†…å®¹ã€‚\n// set feature gates from initial flags-based config if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } // validate the initial KubeletFlags if err := options.ValidateKubeletFlags(kubeletFlags); err != nil { glog.Fatal(err) } if kubeletFlags.ContainerRuntime == \"remote\" \u0026\u0026 cleanFlagSet.Changed(\"pod-infra-container-image\") { glog.Warning(\"Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead\") } // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) \u003e 0 { kubeletConfig, err = loadConfigFile(configFile) if err != nil { glog.Fatal(err) } // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil { glog.Fatal(err) } // update feature gates based on new config if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } } // We always validate the local configuration (command line + config file). // This is the default \"last-known-good\" config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil { glog.Fatal(err) } 2.1.4. dynamic kubelet config å¦‚æœå¼€å¯ä½¿ç”¨åŠ¨æ€kubeletçš„é…ç½®ï¼Œåˆ™ç”±åŠ¨æ€é…ç½®æ–‡ä»¶æ›¿æ¢kubeleté…ç½®æ–‡ä»¶ã€‚\n// use dynamic kubelet config, if enabled var kubeletConfigController *dynamickubeletconfig.Controller if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) \u003e 0 { var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir, func(kc *kubeletconfiginternal.KubeletConfiguration) error { // Here, we enforce flag precedence inside the controller, prior to the controller's validation sequence, // so that we get a complete validation at the same point where we can decide to reject dynamic config. // This fixes the flag-precedence component of issue #63305. // See issue #56171 for general details on flag precedence. return kubeletConfigFlagPrecedence(kc, args) }) if err != nil { glog.Fatal(err) } // If we should just use our existing, local config, the controller will return a nil config if dynamicKubeletConfig != nil { kubeletConfig = dynamicKubeletConfig // Note: flag precedence was already enforced in the controller, prior to validation, // by our above transform function. Now we simply update feature gates from the new config. if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } } } æ€»ç»“ï¼šä»¥ä¸Šé€šè¿‡å¯¹å„ç§ç‰¹å®šå‚æ•°çš„è§£æï¼Œæœ€ç»ˆç”ŸæˆkubeletFlagså’ŒkubeletConfigä¸¤ä¸ªé‡è¦çš„å‚æ•°å¯¹è±¡ï¼Œç”¨æ¥æ„é€ kubeletServerå’Œå…¶ä»–éœ€æ±‚ã€‚\n2.2. åˆå§‹åŒ–kubeletServerå’ŒkubeletDeps 2.2.1. kubeletServer // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := \u0026options.KubeletServer{ KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, } 2.2.2. kubeletDeps // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer) if err != nil { glog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController 2.2.3. docker shim å¦‚æœå¼€å¯äº†docker shimå‚æ•°ï¼Œåˆ™æ‰§è¡ŒRunDockershimã€‚\n// start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(\u0026kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { glog.Fatal(err) } return } 2.3. AddFlags // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) // ugly, but necessary, because Cobra's default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = \"Usage:\\n %s\\n\\nFlags:\\n%s\" cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) }) å…¶ä¸­ï¼š\nAddFlagsä»£ç å¯å‚è€ƒï¼škubernetes/cmd/kubelet/app/options/options.go#L323 AddKubeletConfigFlagsä»£ç å¯å‚è€ƒï¼škubernetes/cmd/kubelet/app/options/options.go#L424 2.4. è¿è¡Œkubelet è¿è¡Œkubeletå¹¶ä¸”ä¸é€€å‡ºã€‚ç”±Runå‡½æ•°è¿›å…¥åç»­çš„æ“ä½œã€‚\n// run the kubelet glog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil { glog.Fatal(err) } 3. Run // Run runs the specified KubeletServer with the given Dependencies. This should never exit. // The kubeDeps argument may be nil - if so, it is initialized from the settings on KubeletServer. // Otherwise, the caller is assumed to have set up the Dependencies object and a default one will // not be generated. func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh \u003c-chan struct{}) error { // To help debugging, immediately log version glog.Infof(\"Version: %+v\", version.Get()) if err := initForOS(s.KubeletFlags.WindowsService); err != nil { return fmt.Errorf(\"failed OS init: %v\", err) } if err := run(s, kubeDeps, stopCh); err != nil { return fmt.Errorf(\"failed to run Kubelet: %v\", err) } return nil } å½“è¿è¡Œç¯å¢ƒæ˜¯Windowsçš„æ—¶å€™ï¼Œåˆå§‹åŒ–æ“ä½œï¼Œä½†æ˜¯è¯¥æ“ä½œä¸ºç©ºï¼Œåªæ˜¯é¢„ç•™ã€‚å…·ä½“æ‰§è¡Œrun(s, kubeDeps, stopCh)å‡½æ•°ã€‚\n3.1. æ„é€ kubeDeps 3.1.1. clientConfig åˆ›å»ºclientConfigï¼Œè¯¥å¯¹è±¡ç”¨æ¥åˆ›å»ºå„ç§çš„kubeDepså±æ€§ä¸­åŒ…å«çš„clientã€‚\nclientConfig, err := createAPIServerClientConfig(s) if err != nil { return fmt.Errorf(\"invalid kubeconfig: %v\", err) } 3.1.2. kubeClient kubeClient, err = clientset.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"New kubeClient from clientConfig error: %v\", err) } else if kubeClient.CertificatesV1beta1() != nil \u0026\u0026 clientCertificateManager != nil { glog.V(2).Info(\"Starting client certificate rotation.\") clientCertificateManager.SetCertificateSigningRequestClient(kubeClient.CertificatesV1beta1().CertificateSigningRequests()) clientCertificateManager.Start() } 3.1.3. dynamicKubeClient dynamicKubeClient, err = dynamic.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"Failed to initialize dynamic KubeClient: %v\", err) } 3.1.4. eventClient // make a separate client for events eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) eventClient, err = v1core.NewForConfig(\u0026eventClientConfig) if err != nil { glog.Warningf(\"Failed to create API Server client for Events: %v\", err) } 3.1.5. heartbeatClient // make a separate client for heartbeat with throttling disabled and a timeout attached heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration // if the NodeLease feature is enabled, the timeout is the minimum of the lease duration and status update frequency if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout \u003e leaseTimeout { heartbeatClientConfig.Timeout = leaseTimeout } } heartbeatClientConfig.QPS = float32(-1) heartbeatClient, err = clientset.NewForConfig(\u0026heartbeatClientConfig) if err != nil { glog.Warningf(\"Failed to create API Server client for heartbeat: %v\", err) } 3.1.6. csiClient // csiClient works with CRDs that support json only clientConfig.ContentType = \"application/json\" csiClient, err := csiclientset.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"Failed to create CSI API client: %v\", err) } clientèµ‹å€¼\nkubeDeps.KubeClient = kubeClient kubeDeps.DynamicKubeClient = dynamicKubeClient if heartbeatClient != nil { kubeDeps.HeartbeatClient = heartbeatClient kubeDeps.OnHeartbeatFailure = closeAllConns } if eventClient != nil { kubeDeps.EventClient = eventClient } kubeDeps.CSIClient = csiClient 3.1.7. CAdvisorInterface if kubeDeps.CAdvisorInterface == nil { imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil { return err } } 3.1.8. ContainerManager if kubeDeps.ContainerManager == nil { if s.CgroupsPerQOS \u0026\u0026 s.CgroupRoot == \"\" { glog.Infof(\"--cgroups-per-qos enabled, but --cgroup-root was not specified. defaulting to /\") s.CgroupRoot = \"/\" } kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil { return err } systemReserved, err := parseResourceList(s.SystemReserved) if err != nil { return err } var hardEvictionThresholds []evictionapi.Threshold // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here. if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold { hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil) if err != nil { return err } } experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil { return err } devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig{ RuntimeCgroupsName: s.RuntimeCgroups, SystemCgroupsName: s.SystemCgroups, KubeletCgroupsName: s.KubeletCgroups, ContainerRuntime: s.ContainerRuntime, CgroupsPerQOS: s.CgroupsPerQOS, CgroupRoot: s.CgroupRoot, CgroupDriver: s.CgroupDriver, KubeletRootDir: s.RootDirectory, ProtectKernelDefaults: s.ProtectKernelDefaults, NodeAllocatableConfig: cm.NodeAllocatableConfig{ KubeReservedCgroupName: s.KubeReservedCgroup, SystemReservedCgroupName: s.SystemReservedCgroup, EnforceNodeAllocatable: sets.NewString(s.EnforceNodeAllocatable...), KubeReserved: kubeReserved, SystemReserved: systemReserved, HardEvictionThresholds: hardEvictionThresholds, }, QOSReserved: *experimentalQOSReserved, ExperimentalCPUManagerPolicy: s.CPUManagerPolicy, ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration, ExperimentalPodPidsLimit: s.PodPidsLimit, EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, }, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) if err != nil { return err } } 3.1.9. oomAdjuster // TODO(vmarmol): Do this through container config. oomAdjuster := kubeDeps.OOMAdjuster if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil { glog.Warning(err) } 3.2. Health check if s.HealthzPort \u003e 0 { healthz.DefaultHealthz() go wait.Until(func() { err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil) if err != nil { glog.Errorf(\"Starting health server failed: %v\", err) } }, 5*time.Second, wait.NeverStop) } 3.3. RunKubelet é€šè¿‡å„ç§èµ‹å€¼æ„é€ äº†å®Œæ•´çš„kubeDepsç»“æ„ä½“ï¼Œæœ€åå†æ‰§è¡ŒRunKubeletè½¬å…¥åç»­çš„kubeletæ‰§è¡Œæµç¨‹ã€‚\nif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil { return err } 4. RunKubelet // RunKubelet is responsible for setting up and running a kubelet. It is used in three different applications: // 1 Integration tests // 2 Kubelet binary // 3 Standalone 'kubernetes' binary // Eventually, #2 will be replaced with instances of #3 func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(\u0026kubeServer.KubeletConfiguration, ... kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } // NewMainKubelet should have set up a pod source config if one didn't exist // when the builder was run. This is just a precaution. if kubeDeps.PodConfig == nil { return fmt.Errorf(\"failed to create kubelet, pod source config was nil\") } podCfg := kubeDeps.PodConfig rlimit.RlimitNumFiles(uint64(kubeServer.MaxOpenFiles)) // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } glog.Infof(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, \u0026kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(\"Started kubelet\") } return nil } RunKubeletå‡½æ•°æ ¸å¿ƒä»£ç ä¸ºæ‰§è¡Œäº†CreateAndInitKubeletå’ŒstartKubeletä¸¤ä¸ªå‡½æ•°çš„æ“ä½œï¼Œä»¥ä¸‹å¯¹è¿™ä¸¤ä¸ªå‡½æ•°è¿›è¡Œåˆ†æã€‚\n4.1. CreateAndInitKubelet é€šè¿‡ä¼ å…¥kubeDepsè°ƒç”¨CreateAndInitKubeletåˆå§‹åŒ–Kubeletã€‚\nk, err := CreateAndInitKubelet(\u0026kubeServer.KubeletConfiguration, kubeDeps, \u0026kubeServer.ContainerRuntimeOptions, kubeServer.ContainerRuntime, kubeServer.RuntimeCgroups, kubeServer.HostnameOverride, kubeServer.NodeIP, kubeServer.ProviderID, kubeServer.CloudProvider, kubeServer.CertDirectory, kubeServer.RootDirectory, kubeServer.RegisterNode, kubeServer.RegisterWithTaints, kubeServer.AllowedUnsafeSysctls, kubeServer.RemoteRuntimeEndpoint, kubeServer.RemoteImageEndpoint, kubeServer.ExperimentalMounterPath, kubeServer.ExperimentalKernelMemcgNotification, kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount, kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold, kubeServer.MinimumGCAge, kubeServer.MaxPerPodContainerCount, kubeServer.MaxContainerCount, kubeServer.MasterServiceNamespace, kubeServer.RegisterSchedulable, kubeServer.NonMasqueradeCIDR, kubeServer.KeepTerminatedPodVolumes, kubeServer.NodeLabels, kubeServer.SeccompProfileRoot, kubeServer.BootstrapCheckpointPath, kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } 4.1.1. NewMainKubelet CreateAndInitKubeletæ–¹æ³•ä¸­æ‰§è¡Œçš„æ ¸å¿ƒå‡½æ•°æ˜¯NewMainKubeletï¼ŒNewMainKubeletå®ä¾‹åŒ–ä¸€ä¸ªkubeletå¯¹è±¡ï¼Œè¯¥éƒ¨åˆ†çš„å…·ä½“ä»£ç åœ¨kubernetes/pkg/kubeletä¸­ï¼Œå…·ä½“å‚è€ƒï¼škubernetes/pkg/kubelet/kubelet.go#L325ã€‚\nfunc CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ... nodeStatusMaxImages int32) (k kubelet.Bootstrap, err error) { // TODO: block until all sources have delivered at least one update to the channel, or break the sync loop // up into \"per source\" synchronizations k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions, containerRuntime, runtimeCgroups, hostnameOverride, nodeIP, providerID, cloudProvider, certDirectory, rootDirectory, registerNode, registerWithTaints, allowedUnsafeSysctls, remoteRuntimeEndpoint, remoteImageEndpoint, experimentalMounterPath, experimentalKernelMemcgNotification, experimentalCheckNodeCapabilitiesBeforeMount, experimentalNodeAllocatableIgnoreEvictionThreshold, minimumGCAge, maxPerPodContainerCount, maxContainerCount, masterServiceNamespace, registerSchedulable, nonMasqueradeCIDR, keepTerminatedPodVolumes, nodeLabels, seccompProfileRoot, bootstrapCheckpointPath, nodeStatusMaxImages) if err != nil { return nil, err } k.BirthCry() k.StartGarbageCollection() return k, nil } 4.1.2. PodConfig if kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil { return nil, err } } NewMainKubelet--\u003ePodConfig--\u003eNewPodConfig--\u003ekubetypes.PodUpdateã€‚ä¼šç”Ÿæˆä¸€ä¸ªpodUpdateçš„channelæ¥ç›‘å¬podçš„å˜åŒ–ï¼Œè¯¥channelä¼šåœ¨k.Run(podCfg.Updates())ä¸­ä½œä¸ºå…³é”®å…¥å‚ã€‚\n4.2. startKubelet // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } glog.Infof(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, \u0026kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(\"Started kubelet\") } å¦‚æœè®¾ç½®äº†åªè¿è¡Œä¸€æ¬¡çš„å‚æ•°ï¼Œåˆ™æ‰§è¡Œk.RunOnceï¼Œå¦åˆ™æ‰§è¡Œæ ¸å¿ƒå‡½æ•°startKubeletã€‚å…·ä½“å®ç°å¦‚ä¸‹ï¼š\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) // start the kubelet server if enableServer { go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) } if kubeCfg.ReadOnlyPort \u003e 0 { go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } } 4.2.1. k.Run // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) é€šè¿‡é•¿é©»è¿›ç¨‹çš„æ–¹å¼è¿è¡Œk.Runï¼Œä¸é€€å‡ºï¼Œå°†kubeletçš„è¿è¡Œé€»è¾‘å¼•å…¥kubernetes/pkg/kubelet/kubelet.goéƒ¨åˆ†ï¼Œkubernetes/pkg/kubeletéƒ¨åˆ†çš„è¿è¡Œé€»è¾‘å¾…åç»­æ–‡ç« åˆ†æã€‚\n5. æ€»ç»“ kubeleté‡‡ç”¨Cobraå‘½ä»¤è¡Œæ¡†æ¶å’Œpflagå‚æ•°è§£ææ¡†æ¶ï¼Œå’Œapiserverã€schedulerã€controller-managerå½¢æˆç»Ÿä¸€çš„ä»£ç é£æ ¼ã€‚\nkubernetes/cmd/kubeletéƒ¨åˆ†ä¸»è¦å¯¹è¿è¡Œå‚æ•°è¿›è¡Œå®šä¹‰å’Œè§£æï¼Œåˆå§‹åŒ–å’Œæ„é€ ç›¸å…³çš„ä¾èµ–ç»„ä»¶ï¼ˆä¸»è¦åœ¨kubeDepsç»“æ„ä½“ä¸­ï¼‰ï¼Œå¹¶æ²¡æœ‰kubeletè¿è¡Œçš„è¯¦ç»†é€»è¾‘ï¼Œè¯¥éƒ¨åˆ†ä½äºkubernetes/pkg/kubeletæ¨¡å—ã€‚\ncmdéƒ¨åˆ†è°ƒç”¨æµç¨‹å¦‚ä¸‹ï¼šMain--\u003eNewKubeletCommand--\u003eRun(kubeletServer, kubeletDeps, stopCh)--\u003erun(s *options.KubeletServer, kubeDeps ..., stopCh ...)--\u003e RunKubelet(s, kubeDeps, s.RunOnce)--\u003estartKubelet--\u003ek.Run(podCfg.Updates())--\u003epkg/kubeletã€‚\nåŒæ—¶RunKubelet(s, kubeDeps, s.RunOnce)--\u003eCreateAndInitKubelet--\u003ekubelet.NewMainKubelet--\u003epkg/kubeletã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0 ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚ â€¦","ref":"/k8s-source-code-analysis/kubelet/newkubeletcommand/","tags":["æºç åˆ†æ"],"title":"kubeletæºç åˆ†æï¼ˆä¸€ï¼‰ä¹‹ NewKubeletCommand"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\nschedulerçš„cmdä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\nkube-scheduler â”œâ”€â”€ BUILD â”œâ”€â”€ OWNERS â”œâ”€â”€ app # appçš„ç›®å½•ä¸‹ä¸»è¦ä¸ºè¿è¡Œschedulerç›¸å…³çš„å¯¹è±¡ â”‚Â â”œâ”€â”€ BUILD â”‚Â â”œâ”€â”€ config â”‚Â â”‚Â â”œâ”€â”€ BUILD â”‚Â â”‚Â â””â”€â”€ config.go # Schedulerçš„é…ç½®å¯¹è±¡config â”‚Â â”œâ”€â”€ options # optionsä¸»è¦è®°å½• Scheduler ä½¿ç”¨åˆ°çš„å‚æ•° â”‚Â â”‚Â â”œâ”€â”€ BUILD â”‚Â â”‚Â â”œâ”€â”€ configfile.go â”‚Â â”‚Â â”œâ”€â”€ deprecated.go â”‚Â â”‚Â â”œâ”€â”€ deprecated_test.go â”‚Â â”‚Â â”œâ”€â”€ insecure_serving.go â”‚Â â”‚Â â”œâ”€â”€ insecure_serving_test.go â”‚Â â”‚Â â”œâ”€â”€ options.go # ä¸»è¦åŒ…æ‹¬Optionsã€NewOptionsã€AddFlagsã€Configç­‰å‡½æ•° â”‚Â â”‚Â â””â”€â”€ options_test.go â”‚Â â””â”€â”€ server.go # ä¸»è¦åŒ…æ‹¬ NewSchedulerCommandã€NewSchedulerConfigã€Runç­‰å‡½æ•° â””â”€â”€ scheduler.go # mainå…¥å£å‡½æ•° 1. Mainå‡½æ•° æ­¤éƒ¨åˆ†çš„ä»£ç ä¸º/cmd/kube-scheduler/scheduler.go\nkube-schedulerçš„å…¥å£å‡½æ•°Mainå‡½æ•°ï¼Œä»ç„¶æ˜¯é‡‡ç”¨ç»Ÿä¸€çš„ä»£ç é£æ ¼ï¼Œä½¿ç”¨Cobraå‘½ä»¤è¡Œæ¡†æ¶ã€‚\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewSchedulerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } æ ¸å¿ƒä»£ç ï¼š\n// åˆå§‹åŒ–schedulerå‘½ä»¤ç»“æ„ä½“ command := app.NewSchedulerCommand() // æ‰§è¡ŒExecute err := command.Execute() 2. NewSchedulerCommand æ­¤éƒ¨åˆ†çš„ä»£ç ä¸º/cmd/kube-scheduler/app/server.go\nNewSchedulerCommandä¸»è¦ç”¨æ¥æ„é€ å’Œåˆå§‹åŒ–SchedulerCommandç»“æ„ä½“ï¼Œ\n// NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { opts, err := options.NewOptions() if err != nil { glog.Fatalf(\"unable to initialize command options: %v\", err) } cmd := \u0026cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) if len(args) != 0 { fmt.Fprint(os.Stderr, \"arguments are not supported\\n\") } if errs := opts.Validate(); len(errs) \u003e 0 { fmt.Fprintf(os.Stderr, \"%v\\n\", utilerrors.NewAggregate(errs)) os.Exit(1) } if len(opts.WriteConfigTo) \u003e 0 { if err := options.WriteConfigFile(opts.WriteConfigTo, \u0026opts.ComponentConfig); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } glog.Infof(\"Wrote configuration to: %s\\n\", opts.WriteConfigTo) return } c, err := opts.Config() if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } stopCh := make(chan struct{}) if err := Run(c.Complete(), stopCh); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } opts.AddFlags(cmd.Flags()) cmd.MarkFlagFilename(\"config\", \"yaml\", \"yml\", \"json\") return cmd } æ ¸å¿ƒä»£ç ï¼š\n// æ„é€ option opts, err := options.NewOptions() // åˆå§‹åŒ–configå¯¹è±¡ c, err := opts.Config() // æ‰§è¡Œrunå‡½æ•° err := Run(c.Complete(), stopCh) // æ·»åŠ å‚æ•° opts.AddFlags(cmd.Flags()) 2.1. NewOptions NewOptionsä¸»è¦ç”¨æ¥æ„é€ SchedulerServerä½¿ç”¨çš„å‚æ•°å’Œä¸Šä¸‹æ–‡ï¼Œå…¶ä¸­æ ¸å¿ƒå‚æ•°æ˜¯KubeSchedulerConfigurationã€‚\nopts, err := options.NewOptions() NewOptions:\n// NewOptions returns default scheduler app options. func NewOptions() (*Options, error) { cfg, err := newDefaultComponentConfig() if err != nil { return nil, err } hhost, hport, err := splitHostIntPort(cfg.HealthzBindAddress) if err != nil { return nil, err } o := \u0026Options{ ComponentConfig: *cfg, SecureServing: nil, // TODO: enable with apiserveroptions.NewSecureServingOptions() CombinedInsecureServing: \u0026CombinedInsecureServingOptions{ Healthz: \u0026apiserveroptions.DeprecatedInsecureServingOptions{ BindNetwork: \"tcp\", }, Metrics: \u0026apiserveroptions.DeprecatedInsecureServingOptions{ BindNetwork: \"tcp\", }, BindPort: hport, BindAddress: hhost, }, Authentication: nil, // TODO: enable with apiserveroptions.NewDelegatingAuthenticationOptions() Authorization: nil, // TODO: enable with apiserveroptions.NewDelegatingAuthorizationOptions() Deprecated: \u0026DeprecatedOptions{ UseLegacyPolicyConfig: false, PolicyConfigMapNamespace: metav1.NamespaceSystem, }, } return o, nil } 2.2. Options.Config Configåˆå§‹åŒ–è°ƒåº¦å™¨çš„é…ç½®å¯¹è±¡ã€‚\nc, err := opts.Config() Configå‡½æ•°ä¸»è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\næ„å»ºscheduler clientã€leaderElectionClientã€eventClientã€‚ åˆ›å»ºevent recorder è®¾ç½®leaderé€‰ä¸¾ åˆ›å»ºinformerå¯¹è±¡ï¼Œä¸»è¦å‡½æ•°æœ‰NewSharedInformerFactoryå’ŒNewPodInformerã€‚ Configå…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// Config return a scheduler config object func (o *Options) Config() (*schedulerappconfig.Config, error) { c := \u0026schedulerappconfig.Config{} if err := o.ApplyTo(c); err != nil { return nil, err } // prepare kube clients. client, leaderElectionClient, eventClient, err := createClients(c.ComponentConfig.ClientConnection, o.Master, c.ComponentConfig.LeaderElection.RenewDeadline.Duration) if err != nil { return nil, err } // Prepare event clients. eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, corev1.EventSource{Component: c.ComponentConfig.SchedulerName}) // Set up leader election if enabled. var leaderElectionConfig *leaderelection.LeaderElectionConfig if c.ComponentConfig.LeaderElection.LeaderElect { leaderElectionConfig, err = makeLeaderElectionConfig(c.ComponentConfig.LeaderElection, leaderElectionClient, recorder) if err != nil { return nil, err } } c.Client = client c.InformerFactory = informers.NewSharedInformerFactory(client, 0) c.PodInformer = factory.NewPodInformer(client, 0) c.EventClient = eventClient c.Recorder = recorder c.Broadcaster = eventBroadcaster c.LeaderElection = leaderElectionConfig return c, nil } 2.3. AddFlags AddFlagsä¸ºSchedulerServeræ·»åŠ æŒ‡å®šçš„å‚æ•°ã€‚\nopts.AddFlags(cmd.Flags()) AddFlagså‡½æ•°çš„å…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// AddFlags adds flags for the scheduler options. func (o *Options) AddFlags(fs *pflag.FlagSet) { fs.StringVar(\u0026o.ConfigFile, \"config\", o.ConfigFile, \"The path to the configuration file. Flags override values in this file.\") fs.StringVar(\u0026o.WriteConfigTo, \"write-config-to\", o.WriteConfigTo, \"If set, write the configuration values to this file and exit.\") fs.StringVar(\u0026o.Master, \"master\", o.Master, \"The address of the Kubernetes API server (overrides any value in kubeconfig)\") o.SecureServing.AddFlags(fs) o.CombinedInsecureServing.AddFlags(fs) o.Authentication.AddFlags(fs) o.Authorization.AddFlags(fs) o.Deprecated.AddFlags(fs, \u0026o.ComponentConfig) leaderelectionconfig.BindFlags(\u0026o.ComponentConfig.LeaderElection.LeaderElectionConfiguration, fs) utilfeature.DefaultFeatureGate.AddFlag(fs) } 3. Run æ­¤éƒ¨åˆ†çš„ä»£ç ä¸º/cmd/kube-scheduler/app/server.go\nerr := Run(c.Complete(), stopCh) Runè¿è¡Œä¸€ä¸ªä¸é€€å‡ºçš„å¸¸é©»è¿›ç¨‹ï¼Œæ¥æ‰§è¡Œschedulerçš„ç›¸å…³æ“ä½œã€‚\nRunå‡½æ•°çš„ä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\né€šè¿‡scheduler configæ¥åˆ›å»ºschedulerçš„ç»“æ„ä½“ã€‚ è¿è¡Œevent broadcasterã€healthz serverã€metrics serverã€‚ è¿è¡Œæ‰€æœ‰çš„informerå¹¶åœ¨è°ƒåº¦å‰ç­‰å¾…cacheçš„åŒæ­¥ï¼ˆé‡ç‚¹ï¼‰ã€‚ æ‰§è¡Œsched.Run()æ¥è¿è¡Œschedulerçš„è°ƒåº¦é€»è¾‘ã€‚ å¦‚æœå¤šä¸ªschedulerå¹¶å¼€å¯äº†LeaderElectï¼Œåˆ™æ‰§è¡Œleaderé€‰ä¸¾ã€‚ ä»¥ä¸‹å¯¹é‡ç‚¹ä»£ç åˆ†å¼€åˆ†æï¼š\n3.1. NewSchedulerConfig NewSchedulerConfigåˆå§‹åŒ–SchedulerConfigï¼ˆæ­¤éƒ¨åˆ†å…·ä½“é€»è¾‘å¾…åç»­ä¸“é—¨åˆ†æï¼‰ï¼Œæœ€ååˆå§‹åŒ–ç”Ÿæˆschedulerç»“æ„ä½“ã€‚\n// Build a scheduler config from the provided algorithm source. schedulerConfig, err := NewSchedulerConfig(c) if err != nil { return err } // Create the scheduler. sched := scheduler.NewFromConfig(schedulerConfig) 3.2. InformerFactory.Start è¿è¡ŒPodInformerï¼Œå¹¶è¿è¡ŒInformerFactoryã€‚æ­¤éƒ¨åˆ†çš„é€»è¾‘ä¸ºclient-goçš„informeræœºåˆ¶ï¼Œåœ¨Informeræœºåˆ¶ä¸­æœ‰è¯¦ç»†åˆ†æã€‚\n// Start all informers. go c.PodInformer.Informer().Run(stopCh) c.InformerFactory.Start(stopCh) 3.3. WaitForCacheSync åœ¨è°ƒåº¦å‰ç­‰å¾…cacheåŒæ­¥ã€‚\n// Wait for all caches to sync before scheduling. c.InformerFactory.WaitForCacheSync(stopCh) controller.WaitForCacheSync(\"scheduler\", stopCh, c.PodInformer.Informer().HasSynced) 3.3.1. InformerFactory.WaitForCacheSync InformerFactory.WaitForCacheSyncç­‰å¾…æ‰€æœ‰å¯åŠ¨çš„informerçš„cacheè¿›è¡ŒåŒæ­¥ï¼Œä¿æŒæœ¬åœ°çš„storeä¿¡æ¯ä¸etcdçš„ä¿¡æ¯æ˜¯æœ€æ–°ä¸€è‡´çš„ã€‚\n// WaitForCacheSync waits for all started informers' cache were synced. func (f *sharedInformerFactory) WaitForCacheSync(stopCh \u003c-chan struct{}) map[reflect.Type]bool { informers := func() map[reflect.Type]cache.SharedIndexInformer { f.lock.Lock() defer f.lock.Unlock() informers := map[reflect.Type]cache.SharedIndexInformer{} for informerType, informer := range f.informers { if f.startedInformers[informerType] { informers[informerType] = informer } } return informers }() res := map[reflect.Type]bool{} for informType, informer := range informers { res[informType] = cache.WaitForCacheSync(stopCh, informer.HasSynced) } return res } æ¥ç€è°ƒç”¨ cache.WaitForCacheSyncã€‚\n// WaitForCacheSync waits for caches to populate. It returns true if it was successful, false // if the controller should shutdown func WaitForCacheSync(stopCh \u003c-chan struct{}, cacheSyncs ...InformerSynced) bool { err := wait.PollUntil(syncedPollPeriod, func() (bool, error) { for _, syncFunc := range cacheSyncs { if !syncFunc() { return false, nil } } return true, nil }, stopCh) if err != nil { glog.V(2).Infof(\"stop requested\") return false } glog.V(4).Infof(\"caches populated\") return true } 3.3.2. controller.WaitForCacheSync controller.WaitForCacheSyncæ˜¯å¯¹cache.WaitForCacheSyncçš„ä¸€å±‚å°è£…ï¼Œé€šè¿‡ä¸åŒçš„controllerçš„åå­—æ¥è®°å½•ä¸åŒcontrollerç­‰å¾…cacheåŒæ­¥ã€‚\ncontroller.WaitForCacheSync(\"scheduler\", stop, s.PodInformer.Informer().HasSynced) controller.WaitForCacheSyncå…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// WaitForCacheSync is a wrapper around cache.WaitForCacheSync that generates log messages // indicating that the controller identified by controllerName is waiting for syncs, followed by // either a successful or failed sync. func WaitForCacheSync(controllerName string, stopCh \u003c-chan struct{}, cacheSyncs ...cache.InformerSynced) bool { glog.Infof(\"Waiting for caches to sync for %s controller\", controllerName) if !cache.WaitForCacheSync(stopCh, cacheSyncs...) { utilruntime.HandleError(fmt.Errorf(\"Unable to sync caches for %s controller\", controllerName)) return false } glog.Infof(\"Caches are synced for %s controller\", controllerName) return true } 3.4. LeaderElection å¦‚æœæœ‰å¤šä¸ªschedulerï¼Œå¹¶å¼€å¯leaderé€‰ä¸¾ï¼Œåˆ™è¿è¡ŒLeaderElectorç›´åˆ°é€‰ä¸¾ç»“æŸæˆ–é€€å‡ºã€‚\n// If leader election is enabled, run via LeaderElector until done and exit. if c.LeaderElection != nil { c.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: run, OnStoppedLeading: func() { utilruntime.HandleError(fmt.Errorf(\"lost master\")) }, } leaderElector, err := leaderelection.NewLeaderElector(*c.LeaderElection) if err != nil { return fmt.Errorf(\"couldn't create leader elector: %v\", err) } leaderElector.Run(ctx) return fmt.Errorf(\"lost lease\") } 3.5. Scheduler.Run // Prepare a reusable run function. run := func(ctx context.Context) { sched.Run() \u003c-ctx.Done() } ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here defer cancel() go func() { select { case \u003c-stopCh: cancel() case \u003c-ctx.Done(): } }() ... run(ctx) Scheduler.Runå…ˆç­‰å¾…cacheåŒæ­¥ï¼Œç„¶åå¼€å¯è°ƒåº¦é€»è¾‘çš„goroutineã€‚\nScheduler.Runçš„å…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } ä»¥ä¸Šæ˜¯å¯¹/cmd/kube-scheduler/scheduler.goéƒ¨åˆ†ä»£ç çš„åˆ†æï¼ŒScheduler.Runåç»­çš„å…·ä½“ä»£ç ä½äºpkg/scheduler/scheduler.goå¾…åç»­æ–‡ç« åˆ†æã€‚\nå‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-scheduler https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-scheduler/app/server.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\nschedulerçš„cmdä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\nkube-scheduler â€¦","ref":"/k8s-source-code-analysis/kube-scheduler/newschedulercommand/","tags":["æºç åˆ†æ"],"title":"kube-scheduleræºç åˆ†æï¼ˆä¸€ï¼‰ä¹‹ NewSchedulerCommand"},{"body":" å¦‚æœè¦å¼€å‘ä¸€ä¸ªDynamic Provisionerï¼Œéœ€è¦ä½¿ç”¨åˆ°the helper libraryã€‚\n1. Dynamic Provisioner 1.1. Provisioner Interface å¼€å‘Dynamic Provisioneréœ€è¦å®ç°Provisioneræ¥å£ï¼Œè¯¥æ¥å£æœ‰ä¸¤ä¸ªæ–¹æ³•ï¼Œåˆ†åˆ«æ˜¯ï¼š\nProvisionï¼šåˆ›å»ºå­˜å‚¨èµ„æºï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªPVå¯¹è±¡ã€‚ Deleteï¼šç§»é™¤å¯¹åº”çš„å­˜å‚¨èµ„æºï¼Œä½†å¹¶æ²¡æœ‰åˆ é™¤PVå¯¹è±¡ã€‚ Provisioner æ¥å£æºç å¦‚ä¸‹ï¼š\n// Provisioner is an interface that creates templates for PersistentVolumes // and can create the volume as a new resource in the infrastructure provider. // It can also remove the volume it created from the underlying storage // provider. type Provisioner interface { // Provision creates a volume i.e. the storage asset and returns a PV object // for the volume Provision(VolumeOptions) (*v1.PersistentVolume, error) // Delete removes the storage asset that was created by Provision backing the // given PV. Does not delete the PV object itself. // // May return IgnoredError to indicate that the call has been ignored and no // action taken. Delete(*v1.PersistentVolume) error } 1.2. VolumeOptions Provisioneræ¥å£çš„Provisionæ–¹æ³•çš„å…¥å‚æ˜¯ä¸€ä¸ªVolumeOptionså¯¹è±¡ã€‚VolumeOptionså¯¹è±¡åŒ…å«äº†åˆ›å»ºPVå¯¹è±¡æ‰€éœ€è¦çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼šPVçš„å›æ”¶ç­–ç•¥ï¼ŒPVçš„åå­—ï¼ŒPVæ‰€å¯¹åº”çš„PVCå¯¹è±¡ä»¥åŠPVCçš„StorageClasså¯¹è±¡ä½¿ç”¨çš„å‚æ•°ç­‰ã€‚\nVolumeOptions æºç å¦‚ä¸‹ï¼š\n// VolumeOptions contains option information about a volume // https://github.com/kubernetes/kubernetes/blob/release-1.4/pkg/volume/plugins.go type VolumeOptions struct { // Reclamation policy for a persistent volume PersistentVolumeReclaimPolicy v1.PersistentVolumeReclaimPolicy // PV.Name of the appropriate PersistentVolume. Used to generate cloud // volume name. PVName string // PV mount options. Not validated - mount of the PVs will simply fail if one is invalid. MountOptions []string // PVC is reference to the claim that lead to provisioning of a new PV. // Provisioners *must* create a PV that would be matched by this PVC, // i.e. with required capacity, accessMode, labels matching PVC.Selector and // so on. PVC *v1.PersistentVolumeClaim // Volume provisioning parameters from StorageClass Parameters map[string]string // Node selected by the scheduler for the volume. SelectedNode *v1.Node // Topology constraint parameter from StorageClass AllowedTopologies []v1.TopologySelectorTerm } 1.3. ProvisionController ProvisionControlleræ˜¯ä¸€ä¸ªç»™PVCæä¾›PVçš„æ§åˆ¶å™¨ï¼Œå…·ä½“æ‰§è¡ŒProvisioneræ¥å£çš„Provisionå’ŒDeleteçš„æ–¹æ³•çš„æ‰€æœ‰é€»è¾‘ã€‚\n1.4. å¼€å‘provisionerçš„æ­¥éª¤ å†™ä¸€ä¸ªprovisionerå®ç°Provisioneræ¥å£ï¼ˆåŒ…å«Provisionå’ŒDeleteçš„æ–¹æ³•ï¼‰ã€‚ é€šè¿‡è¯¥provisioneræ„å»ºProvisionControllerã€‚ æ‰§è¡ŒProvisionControllerçš„Runæ–¹æ³•ã€‚ 2. NFS Client Provisioner nfs-client-provisioneræ˜¯ä¸€ä¸ªautomatic provisionerï¼Œä½¿ç”¨NFSä½œä¸ºå­˜å‚¨ï¼Œè‡ªåŠ¨åˆ›å»ºPVå’Œå¯¹åº”çš„PVCï¼Œæœ¬èº«ä¸æä¾›NFSå­˜å‚¨ï¼Œéœ€è¦å¤–éƒ¨å…ˆæœ‰ä¸€å¥—NFSå­˜å‚¨æœåŠ¡ã€‚\nPVä»¥ ${namespace}-${pvcName}-${pvName}çš„å‘½åæ ¼å¼æä¾›ï¼ˆåœ¨NFSæœåŠ¡å™¨ä¸Šï¼‰ PVå›æ”¶çš„æ—¶å€™ä»¥ archieved-${namespace}-${pvcName}-${pvName} çš„å‘½åæ ¼å¼ï¼ˆåœ¨NFSæœåŠ¡å™¨ä¸Šï¼‰ ä»¥ä¸‹é€šè¿‡nfs-client-provisionerçš„æºç åˆ†ææ¥è¯´æ˜å¼€å‘è‡ªå®šä¹‰provisioneræ•´ä¸ªè¿‡ç¨‹ã€‚nfs-client-provisionerçš„ä¸»è¦ä»£ç éƒ½åœ¨provisioner.goçš„æ–‡ä»¶ä¸­ã€‚\nnfs-client-provisioneræºç åœ°å€ï¼šhttps://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client\n2.1. Mainå‡½æ•° 2.1.1. è¯»å–ç¯å¢ƒå˜é‡ æºç å¦‚ä¸‹ï¼š\nfunc main() { flag.Parse() flag.Set(\"logtostderr\", \"true\") server := os.Getenv(\"NFS_SERVER\") if server == \"\" { glog.Fatal(\"NFS_SERVER not set\") } path := os.Getenv(\"NFS_PATH\") if path == \"\" { glog.Fatal(\"NFS_PATH not set\") } provisionerName := os.Getenv(provisionerNameKey) if provisionerName == \"\" { glog.Fatalf(\"environment variable %s is not set! Please set it.\", provisionerNameKey) } ... } mainå‡½æ•°å…ˆè·å–NFS_SERVERã€NFS_PATHã€PROVISIONER_NAMEä¸‰ä¸ªç¯å¢ƒå˜é‡çš„å€¼ï¼Œå› æ­¤åœ¨éƒ¨ç½²nfs-client-provisionerçš„æ—¶å€™ï¼Œéœ€è¦å°†è¿™ä¸‰ä¸ªç¯å¢ƒå˜é‡çš„å€¼ä¼ å…¥ã€‚\nNFS_SERVERï¼šNFSæœåŠ¡ç«¯çš„IPåœ°å€ã€‚ NFS_PATHï¼šNFSæœåŠ¡ç«¯è®¾ç½®çš„å…±äº«ç›®å½• PROVISIONER_NAMEï¼šprovisionerçš„åå­—ï¼Œéœ€è¦å’ŒStorageClasså¯¹è±¡ä¸­çš„provisionerå­—æ®µä¸€è‡´ã€‚ ä¾‹å¦‚StorageClasså¯¹è±¡çš„yamlæ–‡ä»¶å¦‚ä¸‹ï¼š\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. 2.1.2. è·å–clientsetå¯¹è±¡ æºç å¦‚ä¸‹ï¼š\n// Create an InClusterConfig and use it to create a client for the controller // to use to communicate with Kubernetes config, err := rest.InClusterConfig() if err != nil { glog.Fatalf(\"Failed to create config: %v\", err) } clientset, err := kubernetes.NewForConfig(config) if err != nil { glog.Fatalf(\"Failed to create client: %v\", err) } é€šè¿‡è¯»å–å¯¹åº”çš„k8sçš„é…ç½®ï¼Œåˆ›å»ºclientsetå¯¹è±¡ï¼Œç”¨æ¥æ‰§è¡Œk8så¯¹åº”çš„APIï¼Œå…¶ä¸­ä¸»è¦åŒ…æ‹¬å¯¹PVå’ŒPVCç­‰å¯¹è±¡çš„åˆ›å»ºåˆ é™¤ç­‰æ“ä½œã€‚\n2.1.3. æ„é€ nfsProvisionerå¯¹è±¡ æºç å¦‚ä¸‹ï¼š\n// The controller needs to know what the server version is because out-of-tree // provisioners aren't officially supported until 1.5 serverVersion, err := clientset.Discovery().ServerVersion() if err != nil { glog.Fatalf(\"Error getting server version: %v\", err) } clientNFSProvisioner := \u0026nfsProvisioner{ client: clientset, server: server, path: path, } é€šè¿‡clientsetã€serverã€pathç­‰å€¼æ„é€ nfsProvisionerå¯¹è±¡ï¼ŒåŒæ—¶è¿˜è·å–äº†k8sçš„ç‰ˆæœ¬ä¿¡æ¯ï¼Œå› ä¸ºprovisionersçš„åŠŸèƒ½åœ¨k8s 1.5åŠä»¥ä¸Šç‰ˆæœ¬æ‰æ”¯æŒã€‚\nnfsProvisionerç±»å‹å®šä¹‰å¦‚ä¸‹ï¼š\ntype nfsProvisioner struct { client kubernetes.Interface server string path string } var _ controller.Provisioner = \u0026nfsProvisioner{} nfsProvisioneræ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„provisionerï¼Œç”¨æ¥å®ç°Provisionerçš„æ¥å£ï¼Œå…¶ä¸­çš„å±æ€§é™¤äº†serverã€pathè¿™ä¸¤ä¸ªå…³äºNFSç›¸å…³çš„å‚æ•°ï¼Œè¿˜åŒ…å«äº†clientï¼Œä¸»è¦ç”¨æ¥è°ƒç”¨k8sçš„APIã€‚\nvar _ controller.Provisioner = \u0026nfsProvisioner{} ä»¥ä¸Šç”¨æ³•ç”¨æ¥æ£€æµ‹nfsProvisioneræ˜¯å¦å®ç°äº†Provisionerçš„æ¥å£ã€‚\n2.1.4. æ„å»ºå¹¶è¿è¡ŒProvisionController æºç å¦‚ä¸‹ï¼š\n// Start the provision controller which will dynamically provision efs NFS // PVs pc := controller.NewProvisionController(clientset, provisionerName, clientNFSProvisioner, serverVersion.GitVersion) pc.Run(wait.NeverStop) é€šè¿‡nfsProvisioneræ„é€ ProvisionControllerå¯¹è±¡å¹¶æ‰§è¡ŒRunæ–¹æ³•ï¼ŒProvisionControllerå®ç°äº†å…·ä½“çš„PVå’ŒPVCçš„ç›¸å…³é€»è¾‘ï¼ŒRunæ–¹æ³•ä»¥å¸¸é©»è¿›ç¨‹çš„æ–¹å¼è¿è¡Œã€‚\n2.2. Provisionå’ŒDeleteæ–¹æ³• 2.2.1. Provisionæ–¹æ³• nfsProvisionerçš„Provisionæ–¹æ³•å…·ä½“æºç å‚è€ƒï¼šhttps://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/cmd/nfs-client-provisioner/provisioner.go#L56\nProvisionæ–¹æ³•ç”¨æ¥åˆ›å»ºå­˜å‚¨èµ„æºï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªPVå¯¹è±¡ã€‚å…¶ä¸­å…¥å‚æ˜¯VolumeOptionsï¼Œç”¨æ¥æŒ‡å®šPVå¯¹è±¡çš„ç›¸å…³å±æ€§ã€‚\n1ã€æ„å»ºPVå’ŒPVCçš„åç§°\nfunc (p *nfsProvisioner) Provision(options controller.VolumeOptions) (*v1.PersistentVolume, error) { if options.PVC.Spec.Selector != nil { return nil, fmt.Errorf(\"claim Selector is not supported\") } glog.V(4).Infof(\"nfs provisioner: VolumeOptions %v\", options) pvcNamespace := options.PVC.Namespace pvcName := options.PVC.Name pvName := strings.Join([]string{pvcNamespace, pvcName, options.PVName}, \"-\") fullPath := filepath.Join(mountPath, pvName) glog.V(4).Infof(\"creating path %s\", fullPath) if err := os.MkdirAll(fullPath, 0777); err != nil { return nil, errors.New(\"unable to create directory to provision new pv: \" + err.Error()) } os.Chmod(fullPath, 0777) path := filepath.Join(p.path, pvName) ... } é€šè¿‡VolumeOptionsçš„å…¥å‚ï¼Œæ„å»ºPVå’ŒPVCçš„åç§°ï¼Œä»¥åŠåˆ›å»ºè·¯å¾„pathã€‚\n2ã€æ„é€ PVå¯¹è±¡\npv := \u0026v1.PersistentVolume{ ObjectMeta: metav1.ObjectMeta{ Name: options.PVName, }, Spec: v1.PersistentVolumeSpec{ PersistentVolumeReclaimPolicy: options.PersistentVolumeReclaimPolicy, AccessModes: options.PVC.Spec.AccessModes, MountOptions: options.MountOptions, Capacity: v1.ResourceList{ v1.ResourceName(v1.ResourceStorage): options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)], }, PersistentVolumeSource: v1.PersistentVolumeSource{ NFS: \u0026v1.NFSVolumeSource{ Server: p.server, Path: path, ReadOnly: false, }, }, }, } return pv, nil ç»¼ä¸Šå¯ä»¥çœ‹å‡ºï¼ŒProvisionæ–¹æ³•åªæ˜¯é€šè¿‡VolumeOptionså‚æ•°æ¥æ„å»ºPVå¯¹è±¡ï¼Œå¹¶æ²¡æœ‰æ‰§è¡Œå…·ä½“PVçš„åˆ›å»ºæˆ–åˆ é™¤çš„æ“ä½œã€‚\nä¸åŒç±»å‹çš„Provisionerçš„ï¼Œä¸€èˆ¬æ˜¯PersistentVolumeSourceç±»å‹å’Œå‚æ•°ä¸åŒï¼Œä¾‹å¦‚nfs-provisionerå¯¹åº”çš„PersistentVolumeSourceä¸ºNFSï¼Œå¹¶ä¸”éœ€è¦ä¼ å…¥NFSç›¸å…³çš„å‚æ•°ï¼šServerï¼ŒPathç­‰ã€‚\n2.2.2. Deleteæ–¹æ³• nfsProvisionerçš„deleteæ–¹æ³•å…·ä½“æºç å‚è€ƒï¼šhttps://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/cmd/nfs-client-provisioner/provisioner.go#L99\n1ã€è·å–pvNameå’Œpathç­‰ç›¸å…³å‚æ•°\nfunc (p *nfsProvisioner) Delete(volume *v1.PersistentVolume) error { path := volume.Spec.PersistentVolumeSource.NFS.Path pvName := filepath.Base(path) oldPath := filepath.Join(mountPath, pvName) if _, err := os.Stat(oldPath); os.IsNotExist(err) { glog.Warningf(\"path %s does not exist, deletion skipped\", oldPath) return nil } ... } é€šè¿‡pathå’ŒpvNameç”ŸæˆoldPathï¼Œå…¶ä¸­oldPathæ˜¯åŸå…ˆNFSæœåŠ¡å™¨ä¸Špodå¯¹åº”çš„æ•°æ®æŒä¹…åŒ–å­˜å‚¨è·¯å¾„ã€‚\n2ã€è·å–archiveOnDeleteå‚æ•°å¹¶åˆ é™¤æ•°æ®\n// Get the storage class for this volume. storageClass, err := p.getClassForVolume(volume) if err != nil { return err } // Determine if the \"archiveOnDelete\" parameter exists. // If it exists and has a falsey value, delete the directory. // Otherwise, archive it. archiveOnDelete, exists := storageClass.Parameters[\"archiveOnDelete\"] if exists { archiveBool, err := strconv.ParseBool(archiveOnDelete) if err != nil { return err } if !archiveBool { return os.RemoveAll(oldPath) } } å¦‚æœstorageClasså¯¹è±¡ä¸­æŒ‡å®šarchiveOnDeleteå‚æ•°å¹¶ä¸”å€¼ä¸ºfalseï¼Œåˆ™ä¼šè‡ªåŠ¨åˆ é™¤oldPathä¸‹çš„æ‰€æœ‰æ•°æ®ï¼Œå³podå¯¹åº”çš„æ•°æ®æŒä¹…åŒ–å­˜å‚¨æ•°æ®ã€‚\narchiveOnDeleteå­—é¢æ„æ€ä¸ºåˆ é™¤æ—¶æ˜¯å¦å­˜æ¡£ï¼Œfalseè¡¨ç¤ºä¸å­˜æ¡£ï¼Œå³åˆ é™¤æ•°æ®ï¼Œtrueè¡¨ç¤ºå­˜æ¡£ï¼Œå³é‡å‘½åè·¯å¾„ã€‚\n3ã€é‡å‘½åæ—§æ•°æ®è·¯å¾„\narchivePath := filepath.Join(mountPath, \"archived-\"+pvName) glog.V(4).Infof(\"archiving path %s to %s\", oldPath, archivePath) return os.Rename(oldPath, archivePath) å¦‚æœstorageClasså¯¹è±¡ä¸­æ²¡æœ‰æŒ‡å®šarchiveOnDeleteå‚æ•°æˆ–è€…å€¼ä¸ºtrueï¼Œè¡¨æ˜éœ€è¦åˆ é™¤æ—¶å­˜æ¡£ï¼Œå³å°†oldPathé‡å‘½åï¼Œå‘½åæ ¼å¼ä¸ºoldPathå‰é¢å¢åŠ archived-çš„å‰ç¼€ã€‚\n3. ProvisionController 3.1. ProvisionControllerç»“æ„ä½“ æºç å…·ä½“å‚è€ƒï¼šhttps://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/controller.go#L82\nProvisionControlleræ˜¯ä¸€ä¸ªç»™PVCæä¾›PVçš„æ§åˆ¶å™¨ï¼Œå…·ä½“æ‰§è¡ŒProvisioneræ¥å£çš„Provisionå’ŒDeleteçš„æ–¹æ³•çš„æ‰€æœ‰é€»è¾‘ã€‚\n3.1.1. å…¥å‚ // ProvisionController is a controller that provisions PersistentVolumes for // PersistentVolumeClaims. type ProvisionController struct { client kubernetes.Interface // The name of the provisioner for which this controller dynamically // provisions volumes. The value of annDynamicallyProvisioned and // annStorageProvisioner to set \u0026 watch for, respectively provisionerName string // The provisioner the controller will use to provision and delete volumes. // Presumably this implementer of Provisioner carries its own // volume-specific options and such that it needs in order to provision // volumes. provisioner Provisioner // Kubernetes cluster server version: // * 1.4: storage classes introduced as beta. Technically out-of-tree dynamic // provisioning is not officially supported, though it works // * 1.5: storage classes stay in beta. Out-of-tree dynamic provisioning is // officially supported // * 1.6: storage classes enter GA kubeVersion *utilversion.Version ... } clientã€provisionerNameã€provisionerã€kubeVersionç­‰å±æ€§ä½œä¸ºNewProvisionControllerçš„å…¥å‚ã€‚\nclientï¼šclientsetå®¢æˆ·ç«¯ï¼Œç”¨æ¥è°ƒç”¨k8sçš„APIã€‚ provisionerNameï¼šprovisionerçš„åå­—ï¼Œéœ€è¦å’ŒStorageClasså¯¹è±¡ä¸­çš„provisionerå­—æ®µä¸€è‡´ã€‚ provisionerï¼šå…·ä½“çš„provisionerçš„å®ç°è€…ï¼Œæœ¬æ–‡ä¸ºnfsProvisionerã€‚ kubeVersionï¼šk8sçš„ç‰ˆæœ¬ä¿¡æ¯ã€‚ 3.1.2. Controllerå’ŒInformer type ProvisionController struct { ... claimInformer cache.SharedInformer claims cache.Store claimController cache.Controller volumeInformer cache.SharedInformer volumes cache.Store volumeController cache.Controller classInformer cache.SharedInformer classes cache.Store classController cache.Controller ... } ProvisionControllerç»“æ„ä½“ä¸­åŒ…å«äº†PVã€PVCã€StorageClassä¸‰ä¸ªå¯¹è±¡çš„Controllerã€Informerå’ŒStoreï¼Œä¸»è¦ç”¨æ¥æ‰§è¡Œè¿™ä¸‰ä¸ªå¯¹è±¡çš„ç›¸å…³æ“ä½œã€‚\nControllerï¼šé€šç”¨çš„æ§åˆ¶æ¡†æ¶ Informerï¼šæ¶ˆæ¯é€šçŸ¥å™¨ Storeï¼šé€šç”¨çš„å¯¹è±¡å­˜å‚¨æ¥å£ 3.1.3. workqueue type ProvisionController struct { ... claimQueue workqueue.RateLimitingInterface volumeQueue workqueue.RateLimitingInterface ... } claimQueueå’ŒvolumeQueueåˆ†åˆ«æ˜¯PVå’ŒPVCçš„ä»»åŠ¡é˜Ÿåˆ—ã€‚\n3.1.4. å…¶ä»– // Identity of this controller, generated at creation time and not persisted // across restarts. Useful only for debugging, for seeing the source of // events. controller.provisioner may have its own, different notion of // identity which may/may not persist across restarts id string component string eventRecorder record.EventRecorder resyncPeriod time.Duration exponentialBackOffOnError bool threadiness int createProvisionedPVRetryCount int createProvisionedPVInterval time.Duration failedProvisionThreshold, failedDeleteThreshold int // The port for metrics server to serve on. metricsPort int32 // The IP address for metrics server to serve on. metricsAddress string // The path of metrics endpoint path. metricsPath string // Parameters of leaderelection.LeaderElectionConfig. leaseDuration, renewDeadline, retryPeriod time.Duration hasRun bool hasRunLock *sync.Mutex 3.2. NewProvisionControlleræ–¹æ³• æºç åœ°å€ï¼šhttps://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/controller.go#L418\nNewProvisionControlleræ–¹æ³•ä¸»è¦ç”¨æ¥æ„é€ ProvisionControllerã€‚\n3.2.1. åˆå§‹åŒ–é»˜è®¤å€¼ // NewProvisionController creates a new provision controller using // the given configuration parameters and with private (non-shared) informers. func NewProvisionController( client kubernetes.Interface, provisionerName string, provisioner Provisioner, kubeVersion string, options ...func(*ProvisionController) error, ) *ProvisionController { ... controller := \u0026ProvisionController{ client: client, provisionerName: provisionerName, provisioner: provisioner, kubeVersion: utilversion.MustParseSemantic(kubeVersion), id: id, component: component, eventRecorder: eventRecorder, resyncPeriod: DefaultResyncPeriod, exponentialBackOffOnError: DefaultExponentialBackOffOnError, threadiness: DefaultThreadiness, createProvisionedPVRetryCount: DefaultCreateProvisionedPVRetryCount, createProvisionedPVInterval: DefaultCreateProvisionedPVInterval, failedProvisionThreshold: DefaultFailedProvisionThreshold, failedDeleteThreshold: DefaultFailedDeleteThreshold, leaseDuration: DefaultLeaseDuration, renewDeadline: DefaultRenewDeadline, retryPeriod: DefaultRetryPeriod, metricsPort: DefaultMetricsPort, metricsAddress: DefaultMetricsAddress, metricsPath: DefaultMetricsPath, hasRun: false, hasRunLock: \u0026sync.Mutex{}, } ... } 3.2.2. åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ— ratelimiter := workqueue.NewMaxOfRateLimiter( workqueue.NewItemExponentialFailureRateLimiter(15*time.Second, 1000*time.Second), \u0026workqueue.BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) if !controller.exponentialBackOffOnError { ratelimiter = workqueue.NewMaxOfRateLimiter( workqueue.NewItemExponentialFailureRateLimiter(15*time.Second, 15*time.Second), \u0026workqueue.BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) } controller.claimQueue = workqueue.NewNamedRateLimitingQueue(ratelimiter, \"claims\") controller.volumeQueue = workqueue.NewNamedRateLimitingQueue(ratelimiter, \"volumes\") 3.2.3. ListWatch // PVC claimSource := \u0026cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { return client.CoreV1().PersistentVolumeClaims(v1.NamespaceAll).List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { return client.CoreV1().PersistentVolumeClaims(v1.NamespaceAll).Watch(options) }, } // PV volumeSource := \u0026cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { return client.CoreV1().PersistentVolumes().List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { return client.CoreV1().PersistentVolumes().Watch(options) }, } // StorageClass classSource = \u0026cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { return client.StorageV1().StorageClasses().List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { return client.StorageV1().StorageClasses().Watch(options) }, } list-watchæœºåˆ¶æ˜¯k8sä¸­ç”¨æ¥ç›‘å¬å¯¹è±¡å˜åŒ–çš„æ ¸å¿ƒæœºåˆ¶ï¼ŒListWatchåŒ…å«ListFuncå’ŒWatchFuncä¸¤ä¸ªå‡½æ•°ï¼Œä¸”ä¸èƒ½ä¸ºç©ºï¼Œä»¥ä¸Šä»£ç åˆ†åˆ«æ„é€ äº†PVã€PVCã€StorageClassä¸‰ä¸ªå¯¹è±¡çš„ListWatchç»“æ„ä½“ã€‚è¯¥æœºåˆ¶çš„å®ç°åœ¨client-goçš„cacheåŒ…ä¸­ï¼Œå…·ä½“å‚è€ƒï¼šhttps://godoc.org/k8s.io/client-go/tools/cacheã€‚\næ›´å¤šListWatchä»£ç å¦‚ä¸‹:\nå…·ä½“å‚è€ƒï¼šhttps://github.com/kubernetes-incubator/external-storage/blob/89b0aaf6413b249b37834b124fc314ef7b8ee949/vendor/k8s.io/client-go/tools/cache/listwatch.go#L34\n// ListerWatcher is any object that knows how to perform an initial list and start a watch on a resource. type ListerWatcher interface { // List should return a list type object; the Items field will be extracted, and the // ResourceVersion field will be used to start the watch in the right place. List(options metav1.ListOptions) (runtime.Object, error) // Watch should begin a watch at the specified version. Watch(options metav1.ListOptions) (watch.Interface, error) } // ListFunc knows how to list resources type ListFunc func(options metav1.ListOptions) (runtime.Object, error) // WatchFunc knows how to watch resources type WatchFunc func(options metav1.ListOptions) (watch.Interface, error) // ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface. // It is a convenience function for users of NewReflector, etc. // ListFunc and WatchFunc must not be nil type ListWatch struct { ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool } 3.2.4. ResourceEventHandlerFuncs // PVC claimHandler := cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { controller.enqueueWork(controller.claimQueue, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.claimQueue, newObj) }, DeleteFunc: func(obj interface{}) { controller.forgetWork(controller.claimQueue, obj) }, } // PV volumeHandler := cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { controller.enqueueWork(controller.volumeQueue, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.volumeQueue, newObj) }, DeleteFunc: func(obj interface{}) { controller.forgetWork(controller.volumeQueue, obj) }, } // StorageClass classHandler := cache.ResourceEventHandlerFuncs{ // We don't need an actual event handler for StorageClasses, // but we must pass a non-nil one to cache.NewInformer() AddFunc: nil, UpdateFunc: nil, DeleteFunc: nil, } ResourceEventHandlerFuncsæ˜¯èµ„æºäº‹ä»¶å¤„ç†å‡½æ•°ï¼Œä¸»è¦ç”¨æ¥å¯¹k8sèµ„æºå¯¹è±¡å¢åˆ æ”¹å˜åŒ–çš„äº‹ä»¶è¿›è¡Œæ¶ˆæ¯é€šçŸ¥ï¼Œè¯¥å‡½æ•°å®ç°äº†ResourceEventHandlerçš„æ¥å£ã€‚å…·ä½“ä»£ç é€»è¾‘åœ¨client-goçš„cacheåŒ…ä¸­ã€‚\næ›´å¤šResourceEventHandlerFuncsä»£ç å¯å‚è€ƒï¼š\n// ResourceEventHandler can handle notifications for events that happen to a // resource. The events are informational only, so you can't return an // error. // * OnAdd is called when an object is added. // * OnUpdate is called when an object is modified. Note that oldObj is the // last known state of the object-- it is possible that several changes // were combined together, so you can't use this to see every single // change. OnUpdate is also called when a re-list happens, and it will // get called even if nothing changed. This is useful for periodically // evaluating or syncing something. // * OnDelete will get the final state of the item if it is known, otherwise // it will get an object of type DeletedFinalStateUnknown. This can // happen if the watch is closed and misses the delete event and we don't // notice the deletion until the subsequent re-list. type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } // ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or // as few of the notification functions as you want while still implementing // ResourceEventHandler. type ResourceEventHandlerFuncs struct { AddFunc func(obj interface{}) UpdateFunc func(oldObj, newObj interface{}) DeleteFunc func(obj interface{}) } 3.2.5. æ„é€ Storeå’ŒController 1ã€PVC\nif controller.claimInformer != nil { controller.claimInformer.AddEventHandlerWithResyncPeriod(claimHandler, controller.resyncPeriod) controller.claims, controller.claimController = controller.claimInformer.GetStore(), controller.claimInformer.GetController() } else { controller.claims, controller.claimController = cache.NewInformer( claimSource, \u0026v1.PersistentVolumeClaim{}, controller.resyncPeriod, claimHandler, ) } 2ã€PV\nif controller.volumeInformer != nil { controller.volumeInformer.AddEventHandlerWithResyncPeriod(volumeHandler, controller.resyncPeriod) controller.volumes, controller.volumeController = controller.volumeInformer.GetStore(), controller.volumeInformer.GetController() } else { controller.volumes, controller.volumeController = cache.NewInformer( volumeSource, \u0026v1.PersistentVolume{}, controller.resyncPeriod, volumeHandler, ) } 3ã€StorageClass\nif controller.classInformer != nil { // no resource event handler needed for StorageClasses controller.classes, controller.classController = controller.classInformer.GetStore(), controller.classInformer.GetController() } else { controller.classes, controller.classController = cache.NewInformer( classSource, versionedClassType, controller.resyncPeriod, classHandler, ) } é€šè¿‡cache.NewInformerçš„æ–¹æ³•æ„é€ ï¼Œå…¥å‚æ˜¯ListWatchç»“æ„ä½“å’ŒResourceEventHandlerFuncså‡½æ•°ç­‰ï¼Œè¿”å›å€¼æ˜¯Storeå’ŒControllerã€‚\né€šè¿‡ä»¥ä¸Šå„ä¸ªéƒ¨åˆ†çš„æ„é€ ï¼Œæœ€åè¿”å›ä¸€ä¸ªå…·ä½“çš„ProvisionControllerå¯¹è±¡ã€‚\n3.3. ProvisionController.Runæ–¹æ³• ProvisionControllerçš„Runæ–¹æ³•æ˜¯ä»¥å¸¸é©»è¿›ç¨‹çš„æ–¹å¼è¿è¡Œï¼Œå‡½æ•°å†…éƒ¨å†è¿è¡Œå…¶ä»–çš„controllerã€‚\n3.3.1. prometheusæ•°æ®æ”¶é›† // Run starts all of this controller's control loops func (ctrl *ProvisionController) Run(stopCh \u003c-chan struct{}) { run := func(stopCh \u003c-chan struct{}) { ... if ctrl.metricsPort \u003e 0 { prometheus.MustRegister([]prometheus.Collector{ metrics.PersistentVolumeClaimProvisionTotal, metrics.PersistentVolumeClaimProvisionFailedTotal, metrics.PersistentVolumeClaimProvisionDurationSeconds, metrics.PersistentVolumeDeleteTotal, metrics.PersistentVolumeDeleteFailedTotal, metrics.PersistentVolumeDeleteDurationSeconds, }...) http.Handle(ctrl.metricsPath, promhttp.Handler()) address := net.JoinHostPort(ctrl.metricsAddress, strconv.FormatInt(int64(ctrl.metricsPort), 10)) glog.Infof(\"Starting metrics server at %s\\n\", address) go wait.Forever(func() { err := http.ListenAndServe(address, nil) if err != nil { glog.Errorf(\"Failed to listen on %s: %v\", address, err) } }, 5*time.Second) } ... } 3.3.2. Controller.Run // If a SharedInformer has been passed in, this controller should not // call Run again if ctrl.claimInformer == nil { go ctrl.claimController.Run(stopCh) } if ctrl.volumeInformer == nil { go ctrl.volumeController.Run(stopCh) } if ctrl.classInformer == nil { go ctrl.classController.Run(stopCh) } è¿è¡Œæ¶ˆæ¯é€šçŸ¥å™¨Informerã€‚\n3.3.3. Worker for i := 0; i \u003c ctrl.threadiness; i++ { go wait.Until(ctrl.runClaimWorker, time.Second, stopCh) go wait.Until(ctrl.runVolumeWorker, time.Second, stopCh) } runClaimWorkerå’ŒrunVolumeWorkeråˆ†åˆ«ä¸ºPVCå’ŒPVçš„workerï¼Œè¿™ä¸¤ä¸ªçš„å…·ä½“æ‰§è¡Œä½“åˆ†åˆ«æ˜¯processNextClaimWorkItemå’ŒprocessNextVolumeWorkItemã€‚\næ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š\nPVCçš„å‡½æ•°è°ƒç”¨æµç¨‹\nrunClaimWorkerâ†’processNextClaimWorkItemâ†’syncClaimHandlerâ†’syncClaimâ†’provisionClaimOperation PVçš„å‡½æ•°è°ƒç”¨æµç¨‹\nrunVolumeWorkerâ†’processNextVolumeWorkItemâ†’syncVolumeHandlerâ†’syncVolumeâ†’deleteVolumeOperation å¯è§æœ€åæ‰§è¡Œçš„å‡½æ•°åˆ†åˆ«æ˜¯provisionClaimOperationå’ŒdeleteVolumeOperationã€‚\n3.4. Operation 3.4.1. provisionClaimOperation 1ã€provisionClaimOperationå…¥å‚æ˜¯PVCï¼Œé€šè¿‡PVCè·å¾—PVå¯¹è±¡ï¼Œå¹¶åˆ¤æ–­PVå¯¹è±¡æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™é€€å‡ºåç»­æ“ä½œã€‚\n// provisionClaimOperation attempts to provision a volume for the given claim. // Returns error, which indicates whether provisioning should be retried // (requeue the claim) or not func (ctrl *ProvisionController) provisionClaimOperation(claim *v1.PersistentVolumeClaim) error { // Most code here is identical to that found in controller.go of kube's PV controller... claimClass := helper.GetPersistentVolumeClaimClass(claim) operation := fmt.Sprintf(\"provision %q class %q\", claimToClaimKey(claim), claimClass) glog.Infof(logOperation(operation, \"started\")) // A previous doProvisionClaim may just have finished while we were waiting for // the locks. Check that PV (with deterministic name) hasn't been provisioned // yet. pvName := ctrl.getProvisionedVolumeNameForClaim(claim) volume, err := ctrl.client.CoreV1().PersistentVolumes().Get(pvName, metav1.GetOptions{}) if err == nil \u0026\u0026 volume != nil { // Volume has been already provisioned, nothing to do. glog.Infof(logOperation(operation, \"persistentvolume %q already exists, skipping\", pvName)) return nil } ... } 2ã€è·å–StorageClasså¯¹è±¡ä¸­çš„Provisionerå’ŒReclaimPolicyå‚æ•°ï¼Œå¦‚æœprovisionerNameå’ŒStorageClasså¯¹è±¡ä¸­çš„provisionerå­—æ®µä¸ä¸€è‡´åˆ™æŠ¥é”™å¹¶é€€å‡ºæ‰§è¡Œã€‚\nprovisioner, parameters, err := ctrl.getStorageClassFields(claimClass) if err != nil { glog.Errorf(logOperation(operation, \"error getting claim's StorageClass's fields: %v\", err)) return nil } if provisioner != ctrl.provisionerName { // class.Provisioner has either changed since shouldProvision() or // annDynamicallyProvisioned contains different provisioner than // class.Provisioner. glog.Errorf(logOperation(operation, \"unknown provisioner %q requested in claim's StorageClass\", provisioner)) return nil } // Check if this provisioner can provision this claim. if err = ctrl.canProvision(claim); err != nil { ctrl.eventRecorder.Event(claim, v1.EventTypeWarning, \"ProvisioningFailed\", err.Error()) glog.Errorf(logOperation(operation, \"failed to provision volume: %v\", err)) return nil } reclaimPolicy := v1.PersistentVolumeReclaimDelete if ctrl.kubeVersion.AtLeast(utilversion.MustParseSemantic(\"v1.8.0\")) { reclaimPolicy, err = ctrl.fetchReclaimPolicy(claimClass) if err != nil { return err } } 3ã€æ‰§è¡Œå…·ä½“çš„provisioner.Provisionæ–¹æ³•ï¼Œæ„å»ºPVå¯¹è±¡ï¼Œä¾‹å¦‚æœ¬æ–‡ä¸­çš„provisioneræ˜¯nfs-provisionerã€‚\noptions := VolumeOptions{ PersistentVolumeReclaimPolicy: reclaimPolicy, PVName: pvName, PVC: claim, MountOptions: mountOptions, Parameters: parameters, SelectedNode: selectedNode, AllowedTopologies: allowedTopologies, } ctrl.eventRecorder.Event(claim, v1.EventTypeNormal, \"Provisioning\", fmt.Sprintf(\"External provisioner is provisioning volume for claim %q\", claimToClaimKey(claim))) volume, err = ctrl.provisioner.Provision(options) if err != nil { if ierr, ok := err.(*IgnoredError); ok { // Provision ignored, do nothing and hope another provisioner will provision it. glog.Infof(logOperation(operation, \"volume provision ignored: %v\", ierr)) return nil } err = fmt.Errorf(\"failed to provision volume with StorageClass %q: %v\", claimClass, err) ctrl.eventRecorder.Event(claim, v1.EventTypeWarning, \"ProvisioningFailed\", err.Error()) return err } 4ã€åˆ›å»ºk8sçš„PVå¯¹è±¡ã€‚\n// Try to create the PV object several times for i := 0; i \u003c ctrl.createProvisionedPVRetryCount; i++ { glog.Infof(logOperation(operation, \"trying to save persistentvvolume %q\", volume.Name)) if _, err = ctrl.client.CoreV1().PersistentVolumes().Create(volume); err == nil || apierrs.IsAlreadyExists(err) { // Save succeeded. if err != nil { glog.Infof(logOperation(operation, \"persistentvolume %q already exists, reusing\", volume.Name)) err = nil } else { glog.Infof(logOperation(operation, \"persistentvolume %q saved\", volume.Name)) } break } // Save failed, try again after a while. glog.Infof(logOperation(operation, \"failed to save persistentvolume %q: %v\", volume.Name, err)) time.Sleep(ctrl.createProvisionedPVInterval) } 5ã€åˆ›å»ºPVå¤±è´¥ï¼Œæ¸…ç†å­˜å‚¨èµ„æºã€‚\nif err != nil { // Save failed. Now we have a storage asset outside of Kubernetes, // but we don't have appropriate PV object for it. // Emit some event here and try to delete the storage asset several // times. ... for i := 0; i \u003c ctrl.createProvisionedPVRetryCount; i++ { if err = ctrl.provisioner.Delete(volume); err == nil { // Delete succeeded glog.Infof(logOperation(operation, \"cleaning volume %q succeeded\", volume.Name)) break } // Delete failed, try again after a while. glog.Infof(logOperation(operation, \"failed to clean volume %q: %v\", volume.Name, err)) time.Sleep(ctrl.createProvisionedPVInterval) } if err != nil { // Delete failed several times. There is an orphaned volume and there // is nothing we can do about it. strerr := fmt.Sprintf(\"Error cleaning provisioned volume for claim %s: %v. Please delete manually.\", claimToClaimKey(claim), err) glog.Error(logOperation(operation, strerr)) ctrl.eventRecorder.Event(claim, v1.EventTypeWarning, \"ProvisioningCleanupFailed\", strerr) } } å¦‚æœåˆ›å»ºæˆåŠŸï¼Œåˆ™æ‰“å°æˆåŠŸçš„æ—¥å¿—ï¼Œå¹¶è¿”å›nilã€‚\n3.4.2. deleteVolumeOperation 1ã€deleteVolumeOperationå…¥å‚æ˜¯PVï¼Œå…ˆè·å¾—PVå¯¹è±¡ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ é™¤ã€‚\n// deleteVolumeOperation attempts to delete the volume backing the given // volume. Returns error, which indicates whether deletion should be retried // (requeue the volume) or not func (ctrl *ProvisionController) deleteVolumeOperation(volume *v1.PersistentVolume) error { ... // This method may have been waiting for a volume lock for some time. // Our check does not have to be as sophisticated as PV controller's, we can // trust that the PV controller has set the PV to Released/Failed and it's // ours to delete newVolume, err := ctrl.client.CoreV1().PersistentVolumes().Get(volume.Name, metav1.GetOptions{}) if err != nil { return nil } if !ctrl.shouldDelete(newVolume) { glog.Infof(logOperation(operation, \"persistentvolume no longer needs deletion, skipping\")) return nil } ... } 2ã€è°ƒç”¨å…·ä½“çš„provisionerçš„Deleteæ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ˜¯nfs-provisionerï¼Œåˆ™æ˜¯è°ƒç”¨nfs-provisionerçš„Deleteæ–¹æ³•ã€‚\nerr = ctrl.provisioner.Delete(volume) if err != nil { if ierr, ok := err.(*IgnoredError); ok { // Delete ignored, do nothing and hope another provisioner will delete it. glog.Infof(logOperation(operation, \"volume deletion ignored: %v\", ierr)) return nil } // Delete failed, emit an event. glog.Errorf(logOperation(operation, \"volume deletion failed: %v\", err)) ctrl.eventRecorder.Event(volume, v1.EventTypeWarning, \"VolumeFailedDelete\", err.Error()) return err } 3ã€åˆ é™¤k8sä¸­çš„PVå¯¹è±¡ã€‚\n// Delete the volume if err = ctrl.client.CoreV1().PersistentVolumes().Delete(volume.Name, nil); err != nil { // Oops, could not delete the volume and therefore the controller will // try to delete the volume again on next update. glog.Infof(logOperation(operation, \"failed to delete persistentvolume: %v\", err)) return err } 4. æ€»ç»“ Provisioneræ¥å£åŒ…å«Provisionå’ŒDeleteä¸¤ä¸ªæ–¹æ³•ï¼Œè‡ªå®šä¹‰çš„provisioneréœ€è¦å®ç°è¿™ä¸¤ä¸ªæ–¹æ³•ï¼Œè¿™ä¸¤ä¸ªæ–¹æ³•åªæ˜¯å¤„ç†äº†è·Ÿå­˜å‚¨ç±»å‹ç›¸å…³çš„äº‹é¡¹ï¼Œå¹¶æ²¡æœ‰é’ˆå¯¹PVã€PVCå¯¹è±¡çš„å¢åˆ ç­‰æ“ä½œã€‚ Provisionæ–¹æ³•ä¸»è¦ç”¨æ¥æ„é€ PVå¯¹è±¡ï¼Œä¸åŒç±»å‹çš„Provisionerçš„ï¼Œä¸€èˆ¬æ˜¯PersistentVolumeSourceç±»å‹å’Œå‚æ•°ä¸åŒï¼Œä¾‹å¦‚nfs-provisionerå¯¹åº”çš„PersistentVolumeSourceä¸ºNFSï¼Œå¹¶ä¸”éœ€è¦ä¼ å…¥NFSç›¸å…³çš„å‚æ•°ï¼šServerï¼ŒPathç­‰ã€‚ Deleteæ–¹æ³•ä¸»è¦é’ˆå¯¹å¯¹åº”çš„å­˜å‚¨ç±»å‹ï¼Œåšæ•°æ®å­˜æ¡£ï¼ˆå¤‡ä»½ï¼‰æˆ–åˆ é™¤çš„å¤„ç†ã€‚ StorageClasså¯¹è±¡éœ€è¦å•ç‹¬åˆ›å»ºï¼Œç”¨æ¥æŒ‡å®šå…·ä½“çš„provisioneræ¥æ‰§è¡Œç›¸å…³é€»è¾‘ã€‚ provisionClaimOperationå’ŒdeleteVolumeOperationå…·ä½“æ‰§è¡Œäº†k8sä¸­PVå¯¹è±¡çš„åˆ›å»ºå’Œåˆ é™¤æ“ä½œï¼ŒåŒæ—¶è°ƒç”¨äº†å…·ä½“provisionerçš„Provisionå’ŒDeleteä¸¤ä¸ªæ–¹æ³•æ¥å¯¹å­˜å‚¨æ•°æ®åšå¤„ç†ã€‚ å‚è€ƒæ–‡ç« \nhttps://github.com/kubernetes-incubator/external-storage/tree/master/docs/demo/hostpath-provisioner https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client https://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/controller.go https://github.com/kubernetes-incubator/external-storage/blob/master/lib/controller/volume.go ","categories":"","description":"","excerpt":" å¦‚æœè¦å¼€å‘ä¸€ä¸ªDynamic Provisionerï¼Œéœ€è¦ä½¿ç”¨åˆ°the helper libraryã€‚\n1. Dynamic â€¦","ref":"/kubernetes-notes/develop/csi/nfs-client-provisioner/","tags":["æºç åˆ†æ"],"title":"nfs-client-provisioneræºç åˆ†æ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/openyurt/","tags":"","title":"OpenYurt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/develop/operator/","tags":"","title":"operatorå¼€å‘"},{"body":"1. PVæ¦‚è¿° PersistentVolumeï¼ˆç®€ç§°PVï¼‰ æ˜¯ Volume ä¹‹ç±»çš„å·æ’ä»¶ï¼Œä¹Ÿæ˜¯é›†ç¾¤ä¸­çš„èµ„æºï¼Œä½†ç‹¬ç«‹äºPodçš„ç”Ÿå‘½å‘¨æœŸï¼ˆå³ä¸ä¼šå› Podåˆ é™¤è€Œè¢«åˆ é™¤ï¼‰ï¼Œä¸å½’å±äºæŸä¸ªNamespaceã€‚\n2. PVå’ŒPVCçš„ç”Ÿå‘½å‘¨æœŸ 2.1. é…ç½®ï¼ˆProvisionï¼‰ æœ‰ä¸¤ç§æ–¹å¼æ¥é…ç½® PVï¼šé™æ€æˆ–åŠ¨æ€ã€‚\n1ã€é™æ€\næ‰‹åŠ¨åˆ›å»ºPVï¼Œå¯ä¾›k8sé›†ç¾¤ä¸­çš„å¯¹è±¡æ¶ˆè´¹ã€‚\n2ã€åŠ¨æ€\nå¯ä»¥é€šè¿‡StorageClasså’Œå…·ä½“çš„Provisionerï¼ˆä¾‹å¦‚nfs-client-provisionerï¼‰æ¥åŠ¨æ€åœ°åˆ›å»ºå’Œåˆ é™¤PVã€‚\n2.2. ç»‘å®š åœ¨åŠ¨æ€é…ç½®çš„æƒ…å†µä¸‹ï¼Œç”¨æˆ·åˆ›å»ºäº†ç‰¹å®šçš„PVCï¼Œk8sä¼šç›‘å¬æ–°çš„PVCï¼Œå¹¶å¯»æ‰¾åŒ¹é…çš„PVç»‘å®šã€‚ä¸€æ—¦ç»‘å®šåï¼Œè¿™ç§ç»‘å®šæ˜¯æ’ä»–æ€§çš„ï¼ŒPVCå’ŒPVçš„ç»‘å®šæ˜¯ä¸€å¯¹ä¸€çš„æ˜ å°„ã€‚\n2.3. ä½¿ç”¨ Pod ä½¿ç”¨PVCä½œä¸ºå·ã€‚é›†ç¾¤æ£€æŸ¥PVCä»¥æŸ¥æ‰¾ç»‘å®šçš„å·å¹¶ä¸ºé›†ç¾¤æŒ‚è½½è¯¥å·ã€‚ç”¨æˆ·é€šè¿‡åœ¨ Pod çš„ volume é…ç½®ä¸­åŒ…å«Â persistentVolumeClaimÂ æ¥è°ƒåº¦ Pod å¹¶è®¿é—®ç”¨æˆ·å£°æ˜çš„ PVã€‚\n2.4. å›æ”¶ PVçš„å›æ”¶ç­–ç•¥å¯ä»¥è®¾å®šPVCåœ¨é‡Šæ”¾åå¦‚ä½•å¤„ç†å¯¹åº”çš„Volumeï¼Œç›®å‰æœ‰Â Retainedï¼Œ Recycled å’Œ Deletedä¸‰ç§ç­–ç•¥ã€‚\n1ã€ä¿ç•™ï¼ˆRetainï¼‰\nä¿ç•™ç­–ç•¥å…è®¸æ‰‹åŠ¨å›æ”¶èµ„æºï¼Œå½“åˆ é™¤PVCçš„æ—¶å€™ï¼ŒPVä»ç„¶å­˜åœ¨ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å›æ”¶å·ï¼š\nåˆ é™¤PV æ‰‹åŠ¨æ¸…ç†å¤–éƒ¨å­˜å‚¨çš„æ•°æ®èµ„æº æ‰‹åŠ¨åˆ é™¤æˆ–é‡æ–°ä½¿ç”¨å…³è”çš„å­˜å‚¨èµ„äº§ 2ã€å›æ”¶ï¼ˆResycleï¼‰\nè¯¥ç­–ç•¥å·²åºŸå¼ƒï¼Œæ¨èä½¿ç”¨dynamic provisioning\nå›æ”¶ç­–ç•¥ä¼šåœ¨ volumeä¸Šæ‰§è¡ŒåŸºæœ¬æ“¦é™¤ï¼ˆrm -rf / thevolume / *ï¼‰ï¼Œå¯è¢«å†æ¬¡å£°æ˜ä½¿ç”¨ã€‚\n3ã€åˆ é™¤ï¼ˆDeleteï¼‰\nåˆ é™¤ç­–ç•¥ï¼Œå½“å‘ç”Ÿåˆ é™¤æ“ä½œçš„æ—¶å€™ï¼Œä¼šä»k8sé›†ç¾¤ä¸­åˆ é™¤PVå¯¹è±¡ï¼Œå¹¶æ‰§è¡Œå¤–éƒ¨å­˜å‚¨èµ„æºçš„åˆ é™¤æ“ä½œï¼ˆæ ¹æ®ä¸åŒçš„provisionerå®šä¹‰çš„åˆ é™¤é€»è¾‘ä¸åŒï¼Œæœ‰çš„æ˜¯é‡å‘½åï¼‰ã€‚\nåŠ¨æ€é…ç½®çš„å·ç»§æ‰¿å…¶StorageClassçš„å›æ”¶ç­–ç•¥ï¼Œé»˜è®¤ä¸ºDeleteï¼Œå³å½“ç”¨æˆ·åˆ é™¤PVCçš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨æ‰§è¡ŒPVçš„åˆ é™¤ç­–ç•¥ã€‚\nå¦‚æœè¦ä¿®æ”¹PVçš„å›æ”¶ç­–ç•¥ï¼Œå¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n# Get pv kubectl get pv # Change policy to Retaion kubectl patch pv \u003cpv_name\u003e -p â€˜{â€œspecâ€:{â€œpersistentVolumeReclaimPolicyâ€:â€œRetainâ€}}â€™ 3. PVçš„ç±»å‹ PersistentVolumeÂ ç±»å‹ä»¥æ’ä»¶å½¢å¼å®ç°ã€‚ä»¥ä¸‹ä»…åˆ—éƒ¨åˆ†å¸¸ç”¨ç±»å‹ï¼š\nGCEPersistentDisk AWSElasticBlockStore NFS RBD (Ceph Block Device) CephFS Glusterfs 4. PVçš„å±æ€§ æ¯ä¸ª PV é…ç½®ä¸­éƒ½åŒ…å«ä¸€ä¸ª sepc è§„æ ¼å­—æ®µå’Œä¸€ä¸ª status å·çŠ¶æ€å­—æ®µã€‚\napiVersion: v1 kind: PersistentVolume metadata: annotations: pv.kubernetes.io/provisioned-by: fuseim.pri/ifs creationTimestamp: 2018-07-12T06:46:48Z name: default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985 resourceVersion: \"100163256\" selfLink: /api/v1/persistentvolumes/default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985 uid: 59796ba3-859f-11e8-9c50-c81f66bcff65 spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi volumeMode: Filesystem claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: test-web-0 namespace: default resourceVersion: \"100163248\" uid: 58cf5ec1-859f-11e8-bb61-005056b83985 nfs: path: /data/nfs-storage/default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985 server: 172.16.201.54 persistentVolumeReclaimPolicy: Delete storageClassName: managed-nfs-storage mountOptions: - hard - nfsvers=4.1 status: phase: Bound 4.1. Capacity ç»™PVè®¾ç½®ç‰¹å®šçš„å­˜å‚¨å®¹é‡ï¼Œæ›´å¤šÂ capacityÂ å¯å‚è€ƒKubernetesÂ èµ„æºæ¨¡å‹Â ã€‚\n4.2. Volume Mode volumeMode çš„æœ‰æ•ˆå€¼å¯ä»¥æ˜¯Filesystemæˆ–Blockã€‚å¦‚æœæœªæŒ‡å®šï¼ŒvolumeMode å°†é»˜è®¤ä¸ºFilesystemã€‚\n4.3. Access Modes è®¿é—®æ¨¡å¼åŒ…æ‹¬ï¼š\nReadWriteOnceâ€”â€”è¯¥å·å¯ä»¥è¢«å•ä¸ªèŠ‚ç‚¹ä»¥è¯»/å†™æ¨¡å¼æŒ‚è½½ ReadOnlyManyâ€”â€”è¯¥å·å¯ä»¥è¢«å¤šä¸ªèŠ‚ç‚¹ä»¥åªè¯»æ¨¡å¼æŒ‚è½½ ReadWriteManyâ€”â€”è¯¥å·å¯ä»¥è¢«å¤šä¸ªèŠ‚ç‚¹ä»¥è¯»/å†™æ¨¡å¼æŒ‚è½½ åœ¨å‘½ä»¤è¡Œä¸­ï¼Œè®¿é—®æ¨¡å¼ç¼©å†™ä¸ºï¼š\nRWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany ä¸€ä¸ªå·ä¸€æ¬¡åªèƒ½ä½¿ç”¨ä¸€ç§è®¿é—®æ¨¡å¼æŒ‚è½½ï¼Œå³ä½¿å®ƒæ”¯æŒå¾ˆå¤šè®¿é—®æ¨¡å¼ã€‚\nä»¥ä¸‹åªåˆ—ä¸¾éƒ¨åˆ†å¸¸ç”¨æ’ä»¶ï¼š\nVolume æ’ä»¶ ReadWriteOnce ReadOnlyMany ReadWriteMany AWSElasticBlockStore âœ“ - - CephFS âœ“ âœ“ âœ“ GCEPersistentDisk âœ“ âœ“ - Glusterfs âœ“ âœ“ âœ“ HostPath âœ“ - - NFS âœ“ âœ“ âœ“ RBD âœ“ âœ“ - ... - 4.4. Class PVå¯ä»¥æŒ‡å®šä¸€ä¸ªStorageClassæ¥åŠ¨æ€ç»‘å®šPVå’ŒPVCï¼Œå…¶ä¸­é€šè¿‡Â storageClassNameÂ å±æ€§æ¥æŒ‡å®šå…·ä½“çš„StorageClassï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šè¯¥å±æ€§çš„PVï¼Œå®ƒåªèƒ½ç»‘å®šåˆ°ä¸éœ€è¦ç‰¹å®šç±»çš„ PVCã€‚\n4.5. Reclaim Policy å›æ”¶ç­–ç•¥åŒ…æ‹¬ï¼š\nRetainï¼ˆä¿ç•™ï¼‰â€”â€”æ‰‹åŠ¨å›æ”¶ Recycleï¼ˆå›æ”¶ï¼‰â€”â€”åŸºæœ¬æ“¦é™¤ï¼ˆrm -rf /thevolume/*ï¼‰ Deleteï¼ˆåˆ é™¤ï¼‰â€”â€”å…³è”çš„å­˜å‚¨èµ„äº§ï¼ˆä¾‹å¦‚ AWS EBSã€GCE PDã€Azure Disk å’Œ OpenStack Cinder å·ï¼‰å°†è¢«åˆ é™¤ å½“å‰ï¼Œåªæœ‰ NFS å’Œ HostPath æ”¯æŒå›æ”¶ç­–ç•¥ã€‚AWS EBSã€GCE PDã€Azure Disk å’Œ Cinder å·æ”¯æŒåˆ é™¤ç­–ç•¥ã€‚\n4.6. Mount Options Kubernetes ç®¡ç†å‘˜å¯ä»¥æŒ‡å®šåœ¨èŠ‚ç‚¹ä¸Šä¸ºæŒ‚è½½æŒä¹…å·æŒ‡å®šæŒ‚è½½é€‰é¡¹ã€‚\næ³¨æ„ï¼šä¸æ˜¯æ‰€æœ‰çš„æŒä¹…åŒ–å·ç±»å‹éƒ½æ”¯æŒæŒ‚è½½é€‰é¡¹ã€‚\næ”¯æŒæŒ‚è½½é€‰é¡¹å¸¸ç”¨çš„ç±»å‹æœ‰ï¼š\nGCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk NFS RBD ï¼ˆCeph Block Deviceï¼‰ CephFS Cinder ï¼ˆOpenStack å·å­˜å‚¨ï¼‰ Glusterfs 4.7. Phase PVå¯ä»¥å¤„äºä»¥ä¸‹çš„æŸç§çŠ¶æ€ï¼š\nAvailableï¼ˆå¯ç”¨ï¼‰â€”â€”ä¸€å—ç©ºé—²èµ„æºè¿˜æ²¡æœ‰è¢«ä»»ä½•å£°æ˜ç»‘å®š Boundï¼ˆå·²ç»‘å®šï¼‰â€”â€”å·å·²ç»è¢«å£°æ˜ç»‘å®š Releasedï¼ˆå·²é‡Šæ”¾ï¼‰â€”â€”å£°æ˜è¢«åˆ é™¤ï¼Œä½†æ˜¯èµ„æºè¿˜æœªè¢«é›†ç¾¤é‡æ–°å£°æ˜ Failedï¼ˆå¤±è´¥ï¼‰â€”â€”è¯¥å·çš„è‡ªåŠ¨å›æ”¶å¤±è´¥ å‘½ä»¤è¡Œä¼šæ˜¾ç¤ºç»‘å®šåˆ° PV çš„ PVC çš„åç§°ã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/ ","categories":"","description":"","excerpt":"1. PVæ¦‚è¿° PersistentVolumeï¼ˆç®€ç§°PVï¼‰ æ˜¯ Volume ä¹‹ç±»çš„å·æ’ä»¶ï¼Œä¹Ÿæ˜¯é›†ç¾¤ä¸­çš„èµ„æºï¼Œä½†ç‹¬ç«‹äºPodçš„ç”Ÿå‘½å‘¨æœŸï¼ˆå³ â€¦","ref":"/kubernetes-notes/storage/volume/persistent-volume/","tags":["Kubernetes"],"title":"PersistentVolume ä»‹ç»"},{"body":"é—®é¢˜æè¿° èŠ‚ç‚¹Podè¢«é©±é€\nåŸå›  1. æŸ¥çœ‹èŠ‚ç‚¹å’Œè¯¥èŠ‚ç‚¹podçŠ¶æ€ æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€ä¸ºReadyï¼ŒæŸ¥çœ‹è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰podï¼Œå‘ç°å­˜åœ¨è¢«é©±é€çš„podå’Œnvidia-device-pluginä¸ºpending\nroot@host:~$ kgpoallowide |grep 192.168.1.1 department-56 173e397c-ea35-4aac-85d8-07106e55d7b7 0/1 Evicted 0 52d \u003cnone\u003e 192.168.1.1 \u003cnone\u003e kube-system nvidia-device-plugin-daemonset-d58d2 0/1 Pending 0 1s \u003cnone\u003e 192.168.1.1 \u003cnone\u003e 2. æŸ¥çœ‹å¯¹åº”èŠ‚ç‚¹kubeletçš„æ—¥å¿— 0905 15:42:13.182280 23506 eviction_manager.go:142] Failed to admit pod rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432) - node has conditions: [DiskPressure] I0905 15:42:14.827343 23506 kubelet.go:1836] SyncLoop (ADD, \"api\"): \"nvidia-device-plugin-daemonset-88sm6_kube-system(adbd9227-cfb0-11e9-9729-6c92bf5e2432)\" W0905 15:42:14.827372 23506 eviction_manager.go:142] Failed to admit pod nvidia-device-plugin-daemonset-88sm6_kube-system(adbd9227-cfb0-11e9-9729-6c92bf5e2432) - node has conditions: [DiskPressure] I0905 15:42:15.722378 23506 kubelet_node_status.go:607] Update capacity for nvidia.com/gpu-share to 0 I0905 15:42:16.692488 23506 kubelet.go:1852] SyncLoop (DELETE, \"api\"): \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\" W0905 15:42:16.698445 23506 status_manager.go:489] Failed to delete status for pod \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\": pod \"rdma-device-plugin-daemonset-8nwb8\" not found I0905 15:42:16.698490 23506 kubelet.go:1846] SyncLoop (REMOVE, \"api\"): \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\" I0905 15:42:16.699267 23506 kubelet.go:2040] Failed to delete pod \"rdma-device-plugin-daemonset-8nwb8_kube-system(acc28a85-cfb0-11e9-9729-6c92bf5e2432)\", err: pod not found W0905 15:42:16.777355 23506 eviction_manager.go:332] eviction manager: attempting to reclaim nodefs I0905 15:42:16.777384 23506 eviction_manager.go:346] eviction manager: must evict pod(s) to reclaim nodefs E0905 15:42:16.777390 23506 eviction_manager.go:357] eviction manager: eviction thresholds have been met, but no pods are active to evict å­˜åœ¨å…³äºpodé©±é€ç›¸å…³çš„æ—¥å¿—ï¼Œé©±é€çš„åŸå› ä¸ºnode has conditions: [DiskPressure]ã€‚\n3. æŸ¥çœ‹ç£ç›˜ç›¸å…³ä¿¡æ¯ [root@host /]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 126G 0 126G 0% /dev tmpfs 126G 0 126G 0% /dev/shm tmpfs 126G 27M 126G 1% /run tmpfs 126G 0 126G 0% /sys/fs/cgroup /dev/sda1 20G 19G 0 100% / # æ ¹ç›®å½•ç£ç›˜æ»¡ /dev/nvme1n1 3.0T 191G 2.8T 7% /data2 /dev/nvme0n1 3.0T 1.3T 1.7T 44% /data1 /dev/sda4 182G 95G 87G 53% /data /dev/sda3 20G 3.8G 15G 20% /usr/local tmpfs 26G 0 26G 0% /run/user/0 å‘ç°æ ¹ç›®å½•çš„ç£ç›˜ç›˜ï¼Œæ¥ç€æŸ¥çœ‹å“ªäº›æ–‡ä»¶å ç”¨ç£ç›˜ã€‚\n[root@host ~/kata]# du -sh ./* 1.0M\t./log 944K\t./netlink 6.6G\t./kernel3 /var/log/ä¸‹å­˜åœ¨7G çš„æ—¥å¿—ã€‚æ¸…ç†ç›¸å…³æ—¥å¿—å’Œæ— ç”¨æ–‡ä»¶åï¼Œæ ¹ç›®å½•æ¢å¤ç©ºé—´ã€‚\n[root@host /data]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 126G 0 126G 0% /dev tmpfs 126G 0 126G 0% /dev/shm tmpfs 126G 27M 126G 1% /run tmpfs 126G 0 126G 0% /sys/fs/cgroup /dev/sda1 20G 5.8G 13G 32% / # æ ¹ç›®å½•æ­£å¸¸ /dev/nvme1n1 3.0T 191G 2.8T 7% /data2 æŸ¥çœ‹èŠ‚ç‚¹podçŠ¶æ€ï¼Œç›¸å…³pluginçš„podæ¢å¤æ­£å¸¸ã€‚\nroot@host:~$ kgpoallowide |grep 192.168.1.1 kube-system nvidia-device-plugin-daemonset-h4pjc 1/1 Running 0 16m 192.168.1.1 192.168.1.1 \u003cnone\u003e kube-system rdma-device-plugin-daemonset-xlkbv 1/1 Running 0 16m 192.168.1.1 192.168.1.1 \u003cnone\u003e 4. æŸ¥çœ‹kubeleté…ç½® æŸ¥çœ‹kubeletå…³äºpodé©±é€ç›¸å…³çš„å‚æ•°é…ç½®ï¼Œå¯è§èŠ‚ç‚¹kubeletå¼€å¯äº†é©±é€æœºåˆ¶ï¼Œæ­£å¸¸æƒ…å†µä¸‹è¯¥é…ç½®åº”è¯¥æ˜¯å…³é—­çš„ã€‚\nExecStart=/usr/local/bin/kubelet \\ ... --eviction-hard=nodefs.available\u003c1% \\ è§£å†³æ–¹æ¡ˆ æ€»ç»“ä»¥ä¸ŠåŸå› ä¸ºï¼Œkubeletå¼€å¯äº†podé©±é€çš„æœºåˆ¶ï¼Œæ ¹ç›®å½•çš„ç£ç›˜è¾¾åˆ°100%ï¼Œpodè¢«é©±é€ï¼Œä¸”æ— æ³•å†æ­£å¸¸åˆ›å»ºåœ¨è¯¥èŠ‚ç‚¹ã€‚\nè§£å†³æ–¹æ¡ˆå¦‚ä¸‹ï¼š\n1ã€å…³é—­kubeletçš„é©±é€æœºåˆ¶ã€‚\n2ã€æ¸…é™¤æ ¹ç›®å½•çš„æ–‡ä»¶ï¼Œæ¢å¤æ ¹ç›®å½•ç©ºé—´ï¼Œå¹¶åç»­å¢åŠ æ ¹ç›®å½•çš„ç£ç›˜ç›‘æ§ã€‚\n","categories":"","description":"","excerpt":"é—®é¢˜æè¿° èŠ‚ç‚¹Podè¢«é©±é€\nåŸå›  1. æŸ¥çœ‹èŠ‚ç‚¹å’Œè¯¥èŠ‚ç‚¹podçŠ¶æ€ æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€ä¸ºReadyï¼ŒæŸ¥çœ‹è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰podï¼Œå‘ç°å­˜åœ¨è¢«é©±é€çš„pod â€¦","ref":"/kubernetes-notes/trouble-shooting/pod-evicted/","tags":["é—®é¢˜æ’æŸ¥"],"title":"Podé©±é€"},{"body":"Podé™é¢ï¼ˆLimitRangeï¼‰ ResourceQuotaå¯¹è±¡æ˜¯é™åˆ¶æŸä¸ªnamespaceä¸‹æ‰€æœ‰Pod(å®¹å™¨)çš„èµ„æºé™é¢\nLimitRangeå¯¹è±¡æ˜¯é™åˆ¶æŸä¸ªnamespaceå•ä¸ªPod(å®¹å™¨)çš„èµ„æºé™é¢\nLimitRangeå¯¹è±¡ç”¨æ¥å®šä¹‰æŸä¸ªå‘½åç©ºé—´ä¸‹æŸç§èµ„æºå¯¹è±¡çš„ä½¿ç”¨é™é¢ï¼Œå…¶ä¸­èµ„æºå¯¹è±¡åŒ…æ‹¬ï¼šPodã€Containerã€PersistentVolumeClaimã€‚\n1. ä¸ºnamespaceé…ç½®CPUå’Œå†…å­˜çš„é»˜è®¤å€¼ å¦‚æœåœ¨ä¸€ä¸ªæ‹¥æœ‰é»˜è®¤å†…å­˜æˆ–CPUé™é¢çš„å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªå®¹å™¨ï¼Œå¹¶ä¸”è¿™ä¸ªå®¹å™¨æœªæŒ‡å®šå®ƒè‡ªå·±çš„å†…å­˜æˆ–CPUçš„limitï¼Œ å®ƒä¼šè¢«åˆ†é…è¿™ä¸ªé»˜è®¤çš„å†…å­˜æˆ–CPUçš„limitã€‚æ—¢æ²¡æœ‰è®¾ç½®podçš„limitå’Œrequestæ‰ä¼šåˆ†é…é»˜è®¤çš„å†…å­˜æˆ–CPUçš„requestã€‚\n1.1. namespaceçš„å†…å­˜é»˜è®¤å€¼ # åˆ›å»ºnamespace $ kubectl create namespace default-mem-example # åˆ›å»ºLimitRange $ cat memory-defaults.yaml apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults.yaml --namespace=default-mem-example # åˆ›å»ºPod,æœªæŒ‡å®šå†…å­˜çš„limitå’Œrequest $ cat memory-defaults-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-mem-demo spec: containers: - name: default-mem-demo-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults-pod.yaml --namespace=default-mem-example # æŸ¥çœ‹Pod $ kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example containers: - image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi 1.2. namespaceçš„CPUé»˜è®¤å€¼ # åˆ›å»ºnamespace $ kubectl create namespace default-cpu-example # åˆ›å»ºLimitRange $ cat cpu-defaults.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-limit-range spec: limits: - default: cpu: 1 defaultRequest: cpu: 0.5 type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-defaults.yaml --namespace=default-cpu-example # åˆ›å»ºPodï¼ŒæœªæŒ‡å®šCPUçš„limitå’Œrequest $ cat cpu-defaults-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-cpu-demo spec: containers: - name: default-cpu-demo-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-defaults-pod.yaml --namespace=default-cpu-example # æŸ¥çœ‹Pod $ kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example containers: - image: nginx imagePullPolicy: Always name: default-cpu-demo-ctr resources: limits: cpu: \"1\" requests: cpu: 500m 1.3 è¯´æ˜ å¦‚æœæ²¡æœ‰æŒ‡å®špodçš„requestå’Œlimitï¼Œåˆ™åˆ›å»ºçš„podä¼šä½¿ç”¨LimitRangeå¯¹è±¡å®šä¹‰çš„é»˜è®¤å€¼ï¼ˆrequestå’Œlimitï¼‰ å¦‚æœæŒ‡å®špodçš„limitä½†æœªæŒ‡å®šrequestï¼Œåˆ™åˆ›å»ºçš„podçš„requestå€¼ä¼šå–limitçš„å€¼ï¼Œè€Œä¸ä¼šå–LimitRangeå¯¹è±¡å®šä¹‰çš„requesté»˜è®¤å€¼ã€‚ å¦‚æœæŒ‡å®špodçš„requestä½†æœªæŒ‡å®šlimitï¼Œåˆ™åˆ›å»ºçš„podçš„limitå€¼ä¼šå–LimitRangeå¯¹è±¡å®šä¹‰çš„limité»˜è®¤å€¼ã€‚ é»˜è®¤Limitå’Œrequestçš„åŠ¨æœº\nå¦‚æœå‘½åç©ºé—´å…·æœ‰èµ„æºé…é¢ï¼ˆResourceQuotaï¼‰, å®ƒä¸ºå†…å­˜é™é¢ï¼ˆCPUé™é¢ï¼‰è®¾ç½®é»˜è®¤å€¼æ˜¯æœ‰æ„ä¹‰çš„ã€‚ ä»¥ä¸‹æ˜¯èµ„æºé…é¢å¯¹å‘½åç©ºé—´æ–½åŠ çš„ä¸¤ä¸ªé™åˆ¶ï¼š\nåœ¨å‘½åç©ºé—´è¿è¡Œçš„æ¯ä¸€ä¸ªå®¹å™¨å¿…é¡»æœ‰å®ƒè‡ªå·±çš„å†…å­˜é™é¢ï¼ˆCPUé™é¢ï¼‰ã€‚ åœ¨å‘½åç©ºé—´ä¸­æ‰€æœ‰çš„å®¹å™¨ä½¿ç”¨çš„å†…å­˜æ€»é‡ï¼ˆCPUæ€»é‡ï¼‰ä¸èƒ½è¶…å‡ºæŒ‡å®šçš„é™é¢ã€‚ å¦‚æœä¸€ä¸ªå®¹å™¨æ²¡æœ‰æŒ‡å®šå®ƒè‡ªå·±çš„å†…å­˜é™é¢ï¼ˆCPUé™é¢ï¼‰ï¼Œå®ƒå°†è¢«èµ‹äºˆé»˜è®¤çš„é™é¢å€¼ï¼Œç„¶åå®ƒæ‰å¯ä»¥åœ¨è¢«é…é¢é™åˆ¶çš„å‘½åç©ºé—´ä¸­è¿è¡Œã€‚\n2. ä¸ºnamespaceé…ç½®CPUå’Œå†…å­˜çš„æœ€å¤§æœ€å°å€¼ 2.1. å†…å­˜çš„æœ€å¤§æœ€å°å€¼ åˆ›å»ºLimitRange\n# åˆ›å»ºnamespace $ kubectl create namespace constraints-mem-example # åˆ›å»ºLimitRange $ cat memory-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: mem-min-max-demo-lr spec: limits: - max: memory: 1Gi min: memory: 500Mi type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints.yaml --namespace=constraints-mem-example # æŸ¥çœ‹LimitRange $ kubectl get limitrange cpu-min-max-demo --namespace=constraints-mem-example --output=yaml ... limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container ... # LimitRangeè®¾ç½®äº†æœ€å¤§æœ€å°å€¼ï¼Œä½†æ²¡æœ‰è®¾ç½®é»˜è®¤å€¼ï¼Œä¹Ÿä¼šè¢«è‡ªåŠ¨è®¾ç½®é»˜è®¤å€¼ã€‚ åˆ›å»ºç¬¦åˆè¦æ±‚çš„Pod\n# åˆ›å»ºç¬¦åˆè¦æ±‚çš„Pod $ cat memory-constraints-pod.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo spec: containers: - name: constraints-mem-demo-ctr image: nginx resources: limits: memory: \"800Mi\" requests: memory: \"600Mi\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod.yaml --namespace=constraints-mem-example # æŸ¥çœ‹Pod $ kubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example ... resources: limits: memory: 800Mi requests: memory: 600Mi ... åˆ›å»ºè¶…è¿‡æœ€å¤§å†…å­˜limitçš„pod\n$ cat memory-constraints-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-2 spec: containers: - name: constraints-mem-demo-2-ctr image: nginx resources: limits: memory: \"1.5Gi\" # è¶…è¿‡æœ€å¤§å€¼ 1Gi requests: memory: \"800Mi\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-2.yaml --namespace=constraints-mem-example # Podåˆ›å»ºå¤±è´¥ï¼Œå› ä¸ºå®¹å™¨æŒ‡å®šçš„limitè¿‡å¤§ Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/memory-constraints-pod-2.yaml\": pods \"constraints-mem-demo-2\" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi. åˆ›å»ºå°äºæœ€å°å†…å­˜requestçš„Pod\n$ cat memory-constraints-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-3 spec: containers: - name: constraints-mem-demo-3-ctr image: nginx resources: limits: memory: \"800Mi\" requests: memory: \"100Mi\" # å°äºæœ€å°å€¼500Mi $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-3.yaml --namespace=constraints-mem-example # Podåˆ›å»ºå¤±è´¥ï¼Œå› ä¸ºå®¹å™¨æŒ‡å®šçš„å†…å­˜requestè¿‡å° Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/memory-constraints-pod-3.yaml\": pods \"constraints-mem-demo-3\" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi. åˆ›å»ºæ²¡æœ‰æŒ‡å®šä»»ä½•å†…å­˜limitå’Œrequestçš„pod\n$ cat memory-constraints-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-4 spec: containers: - name: constraints-mem-demo-4-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-4.yaml --namespace=constraints-mem-example # æŸ¥çœ‹Pod $ kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml ... resources: limits: memory: 1Gi requests: memory: 1Gi ... å®¹å™¨æ²¡æœ‰æŒ‡å®šè‡ªå·±çš„ CPU è¯·æ±‚å’Œé™åˆ¶ï¼Œæ‰€ä»¥å®ƒå°†ä» LimitRange è·å–é»˜è®¤çš„ CPU è¯·æ±‚å’Œé™åˆ¶å€¼ã€‚\n2.2. CPUçš„æœ€å¤§æœ€å°å€¼ åˆ›å»ºLimitRange\n# åˆ›å»ºnamespace $ kubectl create namespace constraints-cpu-example # åˆ›å»ºLimitRange $ cat cpu-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits: - max: cpu: \"800m\" min: cpu: \"200m\" type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints.yaml --namespace=constraints-cpu-example # æŸ¥çœ‹LimitRange $ kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example ... limits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container ... åˆ›å»ºç¬¦åˆè¦æ±‚çš„Pod\n$ cat cpu-constraints-pod.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo spec: containers: - name: constraints-cpu-demo-ctr image: nginx resources: limits: cpu: \"800m\" requests: cpu: \"500m\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod.yaml --namespace=constraints-cpu-example # æŸ¥çœ‹Pod $ kubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example ... resources: limits: cpu: 800m requests: cpu: 500m ... åˆ›å»ºè¶…è¿‡æœ€å¤§CPU limitçš„Pod\n$ cat cpu-constraints-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-2 spec: containers: - name: constraints-cpu-demo-2-ctr image: nginx resources: limits: cpu: \"1.5\" requests: cpu: \"500m\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example # Podåˆ›å»ºå¤±è´¥ï¼Œå› ä¸ºå®¹å™¨æŒ‡å®šçš„CPU limitè¿‡å¤§ Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/cpu-constraints-pod-2.yaml\": pods \"constraints-cpu-demo-2\" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m. åˆ›å»ºå°äºæœ€å°CPU requestçš„Pod\n$ cat cpu-constraints-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-4 spec: containers: - name: constraints-cpu-demo-4-ctr image: nginx resources: limits: cpu: \"800m\" requests: cpu: \"100m\" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example # Podåˆ›å»ºå¤±è´¥ï¼Œå› ä¸ºå®¹å™¨æŒ‡å®šçš„CPU requestè¿‡å° Error from server (Forbidden): error when creating \"docs/tasks/administer-cluster/cpu-constraints-pod-3.yaml\": pods \"constraints-cpu-demo-4\" is forbidden: minimum cpu usage per Container is 200m, but request is 100m. åˆ›å»ºæ²¡æœ‰æŒ‡å®šä»»ä½•CPU limitå’Œrequestçš„pod\n$ cat cpu-constraints-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-4 spec: containers: - name: constraints-cpu-demo-4-ctr image: vish/stress $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example # æŸ¥çœ‹Pod kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml ... resources: limits: cpu: 800m requests: cpu: 800m ... å®¹å™¨æ²¡æœ‰æŒ‡å®šè‡ªå·±çš„ CPU è¯·æ±‚å’Œé™åˆ¶ï¼Œæ‰€ä»¥å®ƒå°†ä» LimitRange è·å–é»˜è®¤çš„ CPU è¯·æ±‚å’Œé™åˆ¶å€¼ã€‚\n2.3. è¯´æ˜ LimitRange åœ¨ namespace ä¸­æ–½åŠ çš„æœ€å°å’Œæœ€å¤§å†…å­˜ï¼ˆCPUï¼‰é™åˆ¶åªæœ‰åœ¨åˆ›å»ºå’Œæ›´æ–° Pod æ—¶æ‰ä¼šè¢«åº”ç”¨ã€‚æ”¹å˜ LimitRange ä¸ä¼šå¯¹ä¹‹å‰åˆ›å»ºçš„ Pod é€ æˆå½±å“ã€‚\nKubernetes éƒ½ä¼šæ‰§è¡Œä¸‹åˆ—æ­¥éª¤ï¼š\nå¦‚æœå®¹å™¨æ²¡æœ‰æŒ‡å®šè‡ªå·±çš„å†…å­˜ï¼ˆCPUï¼‰è¯·æ±‚ï¼ˆrequestï¼‰å’Œé™åˆ¶ï¼ˆlimitï¼‰ï¼Œç³»ç»Ÿå°†ä¼šä¸ºå…¶åˆ†é…é»˜è®¤å€¼ã€‚ éªŒè¯å®¹å™¨çš„å†…å­˜ï¼ˆCPUï¼‰è¯·æ±‚å¤§äºç­‰äºæœ€å°å€¼ã€‚ éªŒè¯å®¹å™¨çš„å†…å­˜ï¼ˆCPUï¼‰é™åˆ¶å°äºç­‰äºæœ€å¤§å€¼ã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/\nhttps://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/\n","categories":"","description":"","excerpt":"Podé™é¢ï¼ˆLimitRangeï¼‰ ResourceQuotaå¯¹è±¡æ˜¯é™åˆ¶æŸä¸ªnamespaceä¸‹æ‰€æœ‰Pod(å®¹å™¨) â€¦","ref":"/kubernetes-notes/resource/limit-range/","tags":["Kubernetes"],"title":"èµ„æºé…é¢"},{"body":"pvcæµç¨‹ æµç¨‹å¦‚ä¸‹ï¼š\nç”¨æˆ·åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« PVC çš„ Podï¼Œè¯¥ PVC è¦æ±‚ä½¿ç”¨åŠ¨æ€å­˜å‚¨å·ï¼› Scheduler æ ¹æ® Pod é…ç½®ã€èŠ‚ç‚¹çŠ¶æ€ã€PV é…ç½®ç­‰ä¿¡æ¯ï¼ŒæŠŠ Pod è°ƒåº¦åˆ°ä¸€ä¸ªåˆé€‚çš„ Worker èŠ‚ç‚¹ä¸Šï¼› PV æ§åˆ¶å™¨ watch åˆ°è¯¥ Pod ä½¿ç”¨çš„ PVC å¤„äº Pending çŠ¶æ€ï¼Œäºæ˜¯è°ƒç”¨ Volume Pluginï¼ˆin-treeï¼‰åˆ›å»ºå­˜å‚¨å·ï¼Œå¹¶åˆ›å»º PV å¯¹è±¡ï¼ˆout-of-tree ç”± External Provisioner æ¥å¤„ç†ï¼‰ï¼› AD æ§åˆ¶å™¨å‘ç° Pod å’Œ PVC å¤„äºå¾…æŒ‚æ¥çŠ¶æ€ï¼Œäºæ˜¯è°ƒç”¨ Volume Plugin æŒ‚æ¥å­˜å‚¨è®¾å¤‡åˆ°ç›®æ ‡ Worker èŠ‚ç‚¹ä¸Š åœ¨ Worker èŠ‚ç‚¹ä¸Šï¼ŒKubelet ä¸­çš„ Volume Manager ç­‰å¾…å­˜å‚¨è®¾å¤‡æŒ‚æ¥å®Œæˆï¼Œå¹¶é€šè¿‡ Volume Plugin å°†è®¾å¤‡æŒ‚è½½åˆ°å…¨å±€ç›®å½•ï¼š/var/lib/kubelet/pods/[pod uid]/volumes/kubernetes.io~iscsi/[PVname]ï¼ˆä»¥ iscsi ä¸ºä¾‹ï¼‰ï¼› Kubelet é€šè¿‡ Docker å¯åŠ¨ Pod çš„ Containersï¼Œç”¨ bind mount æ–¹å¼å°†å·²æŒ‚è½½åˆ°æœ¬åœ°å…¨å±€ç›®å½•çš„å·æ˜ å°„åˆ°å®¹å™¨ä¸­ã€‚ è¯¦ç»†æµç¨‹å›¾ ","categories":"","description":"","excerpt":"pvcæµç¨‹ æµç¨‹å¦‚ä¸‹ï¼š\nç”¨æˆ·åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« PVC çš„ Podï¼Œè¯¥ PVC è¦æ±‚ä½¿ç”¨åŠ¨æ€å­˜å‚¨å·ï¼› Scheduler æ ¹æ® Pod é…ç½®ã€èŠ‚ â€¦","ref":"/kubernetes-notes/principle/flow/pvc-flow/","tags":["Kubernetes"],"title":"PVCåˆ›å»ºæµç¨‹"},{"body":" æœ¬æ–‡ä¸»è¦ä»‹ç»redfish apiçš„è°ƒç”¨è·¯å¾„åŠæ ¼å¼ã€‚\nRedfishæ˜¯ä¸€ç§åŸºäºHTTPsæœåŠ¡çš„ç®¡ç†æ ‡å‡†ï¼Œåˆ©ç”¨RESTfulæ¥å£å®ç°è®¾å¤‡ç®¡ç†ã€‚å¯ä»¥ç†è§£ä¸ºredfish apiå°±æ˜¯é€šè¿‡httpçš„è°ƒç”¨æ–¹å¼æ¥æ“ä½œæœåŠ¡å™¨çš„bmcè®¾å¤‡ï¼Œä»è€Œå®ç°å¯¹è®¾å¤‡çš„è¿œç¨‹æ§åˆ¶ã€‚\n1. BMCå¸¸ç”¨åŠŸèƒ½æ“ä½œ BIOS ç®¡ç†\nå¯åŠ¨è®¾ç½®ï¼ˆboot orderï¼‰\nè™šæ‹Ÿåª’ä½“ç®¡ç†\nç”µæºç®¡ç†ï¼ˆå¼€æœºã€å…³æœºã€é‡å¯ï¼‰\nå›ºä»¶å‡çº§\nè¿œç¨‹æ§åˆ¶ï¼ˆVNC/SOLï¼‰\nç›‘æ§å’Œæ—¥å¿—\n2. é€šç”¨ RedfishÂ API æ¥å£æ ¼å¼ 2.1. é€šç”¨ RedfishÂ API æ¥å£æ ¼å¼ 2.1.1. åŸºç¡€è·¯å¾„ç»“æ„ /redfish/v1/ 2.1.2. ä¸»è¦èµ„æºç±»å‹ ç³»ç»Ÿèµ„æºÂ (Systems) /redfish/v1/Systems/{systemId}/ â”œâ”€â”€ Bios/ # BIOSè®¾ç½® â”œâ”€â”€ Boot/ # å¯åŠ¨è®¾ç½® â”œâ”€â”€ Memory/ # å†…å­˜ä¿¡æ¯ â”œâ”€â”€ Processors/ # å¤„ç†å™¨ä¿¡æ¯ â”œâ”€â”€ Storage/ # å­˜å‚¨ä¿¡æ¯ â”œâ”€â”€ EthernetInterfaces/ # ç½‘ç»œæ¥å£ â””â”€â”€ Actions/ # ç³»ç»Ÿæ“ä½œ æœºç®±èµ„æº (Chassis) /redfish/v1/Chassis/{chassisId}/ â”œâ”€â”€ Power/ # ç”µæºç®¡ç† â”œâ”€â”€ Thermal/ # æ¸©åº¦ç®¡ç† â”œâ”€â”€ NetworkAdapters/ # ç½‘ç»œé€‚é…å™¨ â””â”€â”€ Actions/ # æœºç®±æ“ä½œ ç®¡ç†æ§åˆ¶å™¨ (Managers) /redfish/v1/Managers/{managerId}/ â”œâ”€â”€ NetworkProtocol/ # ç½‘ç»œåè®® â”œâ”€â”€ VirtualMedia/ # è™šæ‹Ÿåª’ä½“ â”œâ”€â”€ LogServices/ # æ—¥å¿—æœåŠ¡ â””â”€â”€ Actions/ # ç®¡ç†æ“ä½œ æ›´æ–°æœåŠ¡ (UpdateService) /redfish/v1/UpdateService/ â”œâ”€â”€ FirmwareInventory/ # å›ºä»¶æ¸…å• â””â”€â”€ Actions/ # æ›´æ–°æ“ä½œ 3. BIOS ç®¡ç† 1. é€šç”¨ BIOS æ¥å£è·¯å¾„ /redfish/v1/Systems/{systemId}/Bios è·å– BIOS å±æ€§\nGET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§\nPATCH /redfish/v1/Systems/{systemId}/Bios/Settings 2. å„å‚å•†è·¯å¾„ å‚å•† æ“ä½œ æ–¹æ³• è·¯å¾„ request Dell è·å– BIOS å±æ€§ GET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§ PATCH /redfish/v1/Systems/{systemId}/Bios/Settings {\"Attributes\": attrs,\n\"@Redfish.SettingsApplyTime\": \"OnReset\"} HPE è·å– BIOS å±æ€§ GET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§ PATCH /redfish/v1/Systems/{systemId}/Bios/Settings {\"Attributes\": attrs,\n\"@Redfish.SettingsApplyTime\": \"OnReset\"} HUAWEI è·å– BIOS å±æ€§ GET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§ PATCH /redfish/v1/Systems/{systemId}/Bios/Settings { \"Attributes\": attrs} Inspur è·å– BIOS å±æ€§ GET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§ PATCH Lenovo è·å– BIOS å±æ€§ GET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§ PATCH /redfish/v1/Systems/{systemId}/Bios/Settings { \"Attributes\": attrs} xFusion è·å– BIOS å±æ€§ GET /redfish/v1/Systems/{systemId}/Bios è®¾ç½® BIOS å±æ€§ PATCH /redfish/v1/Systems/{systemId}/Bios/Settings { \"Attributes\": attrs} 4. å¯åŠ¨è®¾ç½®ï¼ˆboot orderï¼‰ 5. VirtualMedia çš„é€šç”¨æ¥å£è·¯å¾„ 1.Â é€šç”¨ VirtualMedia æ¥å£è·¯å¾„ /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/ è·å–è™šæ‹Ÿåª’ä½“ä¿¡æ¯\nGET /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/ æ’å…¥è™šæ‹Ÿåª’ä½“\nPOST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.InsertMedia å¼¹å‡ºè™šæ‹Ÿåª’ä½“\nPOST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.EjectMedia 2. å„å‚å•†è·¯å¾„ å‚å•† æ“ä½œ æ–¹æ³• è·¯å¾„ request Dell è·å–VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/CD æ’å…¥VirtualMedia å¼¹å‡ºVirtualMedia HPE è·å–VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId} æ’å…¥VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.InsertMedia { \"Image\": \"string\", \"Inserted\": boolean, \"WriteProtected\": boolean} å¼¹å‡ºVirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.EjectMedia HUAWEI è·å–VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/CD æ’å…¥VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/Huawei/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Connect\", \"Image\": imageURL} å¼¹å‡ºVirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/Huawei/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Disconnect\"} Inspur(M6) è·å–VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId} mediaId=CD/CD1 æ’å…¥VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.InsertMedia {\"Image\": image,\"TransferProtocolType\": \"NFS\"} å¼¹å‡ºVirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/{mediaId}/Actions/VirtualMedia.EjectMedia Lenovo è·å–VirtualMedia GET /redfish/v1/Systems/{systemId}/VirtualMedia/EXT1 æ’å…¥VirtualMedia å¼¹å‡ºVirtualMedia PATCH /redfish/v1/Systems/{systemId}/VirtualMedia/EXT1 {\"Inserted\": false} xFusion(åä¸ºå­å“ç‰Œ) è·å–VirtualMedia GET /redfish/v1/Managers/{managerId}/VirtualMedia/CD æ’å…¥VirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/xFusion/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Connect\", \"Image\": imageURL} å¼¹å‡ºVirtualMedia POST /redfish/v1/Managers/{managerId}/VirtualMedia/CD/Oem/xFusion/Actions/VirtualMedia.VmmControl {\"VmmControlType\": \"Disconnect\"} 6. ç”µæºç®¡ç†ï¼ˆå¼€æœºã€å…³æœºã€é‡å¯ï¼‰ 7. å›ºä»¶å‡çº§ ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦ä»‹ç»redfish apiçš„è°ƒç”¨è·¯å¾„åŠæ ¼å¼ã€‚\nRedfishæ˜¯ä¸€ç§åŸºäºHTTPsæœåŠ¡çš„ç®¡ç†æ ‡å‡†ï¼Œåˆ©ç”¨RESTfulæ¥å£å®ç°è®¾å¤‡ç®¡ â€¦","ref":"/linux-notes/baremetal/redfish-api/","tags":["è£¸é‡‘å±"],"title":"Redfish API"},{"body":"1. Rediséƒ¨ç½² ä»¥ä¸‹ä»¥Linuxç³»ç»Ÿä¸ºä¾‹\n1.1 ä¸‹è½½å’Œç¼–è¯‘ $ wget http://download.redis.io/releases/redis-4.0.7.tar.gz $ tar xzf redis-4.0.7.tar.gz $ cd redis-4.0.7 $ make ç¼–è¯‘å®Œæˆåä¼šåœ¨srcç›®å½•ä¸‹ç”ŸæˆRedisæœåŠ¡ç«¯ç¨‹åºredis-serverå’Œå®¢æˆ·ç«¯ç¨‹åºredis-cliã€‚\n1.2 å¯åŠ¨æœåŠ¡ 1ã€å‰å°è¿è¡Œ\nsrc/redis-server è¯¥æ–¹å¼å¯åŠ¨é»˜è®¤ä¸ºå‰å°æ–¹å¼è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤é…ç½®ã€‚\n2ã€åå°è¿è¡Œ\nå¯ä»¥ä¿®æ”¹redis.confæ–‡ä»¶çš„daemonizeå‚æ•°ä¸ºyesï¼ŒæŒ‡å®šé…ç½®æ–‡ä»¶å¯åŠ¨ï¼Œä¾‹å¦‚ï¼š\nvi redis.conf # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize yes æŒ‡å®šé…ç½®æ–‡ä»¶å¯åŠ¨ã€‚\nsrc/redis-server redis.conf ä¾‹å¦‚ï¼š\n#æŒ‡å®šé…ç½®æ–‡ä»¶åå°å¯åŠ¨ [root@kube-node-1 redis-4.0.7]# src/redis-server redis.conf 95778:C 30 Jan 00:44:37.633 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 95778:C 30 Jan 00:44:37.634 # Redis version=4.0.7, bits=64, commit=00000000, modified=0, pid=95778, just started 95778:C 30 Jan 00:44:37.634 # Configuration loaded #æŸ¥çœ‹Redisè¿›ç¨‹ [root@kube-node-1 redis-4.0.7]# ps aux|grep redis root 95779 0.0 0.0 145268 468 ? Ssl 00:44 0:00 src/redis-server 127.0.0.1:6379 æ›´å¤šå¯åŠ¨å‚æ•°å¦‚ä¸‹ï¼š\n[root@kube-node-1 src]# ./redis-server --help Usage: ./redis-server [/path/to/redis.conf] [options] ./redis-server - (read config from stdin) ./redis-server -v or --version ./redis-server -h or --help ./redis-server --test-memory \u003cmegabytes\u003e Examples: ./redis-server (run the server with default conf) ./redis-server /etc/redis/6379.conf ./redis-server --port 7777 ./redis-server --port 7777 --slaveof 127.0.0.1 8888 ./redis-server /etc/myredis.conf --loglevel verbose Sentinel mode: ./redis-server /etc/sentinel.conf --sentinel 1.3 å®¢æˆ·ç«¯æµ‹è¯• $ src/redis-cli redis\u003e set foo bar OK redis\u003e get foo \"bar\" 2. Redisé›†ç¾¤éƒ¨ç½² Redisçš„é›†ç¾¤éƒ¨ç½²éœ€è¦åœ¨æ¯å°é›†ç¾¤éƒ¨ç½²çš„æœºå™¨ä¸Šå®‰è£…Redisï¼ˆå¯å‚è€ƒä¸Šè¿°çš„[Rediså®‰è£…] ï¼‰ï¼Œç„¶åä¿®æ”¹é…ç½®ä»¥é›†ç¾¤çš„æ–¹å¼å¯åŠ¨ã€‚\n2.1 æ‰‹åŠ¨éƒ¨ç½²é›†ç¾¤ 2.1.1 è®¾ç½®é…ç½®æ–‡ä»¶åŠå¯åŠ¨å®ä¾‹ ä¿®æ”¹é…ç½®æ–‡ä»¶redis.confï¼Œé›†ç¾¤æ¨¡å¼çš„æœ€å°åŒ–é…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š\n#å¯é€‰æ“ä½œï¼Œè¯¥é¡¹è®¾ç½®åå°æ–¹å¼è¿è¡Œï¼Œ daemonize yes port 7000 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 appendonly yes æ›´å¤šé›†ç¾¤é…ç½®å‚æ•°å¯å‚è€ƒé»˜è®¤é…ç½®æ–‡ä»¶redis.confä¸­Clusteræ¨¡å—çš„è¯´æ˜\næœ€å°é›†ç¾¤æ¨¡å¼éœ€è¦ä¸‰ä¸ªmasterå®ä¾‹ï¼Œä¸€èˆ¬å»ºè®®èµ·å…­ä¸ªå®ä¾‹ï¼Œå³ä¸‰ä¸»ä¸‰ä»ã€‚å› æ­¤æˆ‘ä»¬åˆ›å»º6ä¸ªä»¥ç«¯å£å·å‘½åçš„ç›®å½•å­˜æ”¾å®ä¾‹çš„é…ç½®æ–‡ä»¶å’Œå…¶ä»–ä¿¡æ¯ã€‚\nmkdir cluster-test cd cluster-test mkdir 7000 7001 7002 7003 7004 7005 åœ¨å¯¹åº”ç«¯å£å·çš„ç›®å½•ä¸­åˆ›å»ºredis.confçš„æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶çš„å†…å®¹å¯å‚è€ƒä¸Šè¿°çš„é›†ç¾¤æ¨¡å¼é…ç½®ã€‚æ¯ä¸ªé…ç½®æ–‡ä»¶ä¸­çš„ç«¯å£å·portå‚æ•°æ”¹ä¸ºå¯¹åº”ç›®å½•çš„ç«¯å£å·ã€‚\nå¤åˆ¶redis-serverçš„äºŒè¿›åˆ¶æ–‡ä»¶åˆ°cluster-testç›®å½•ä¸­ï¼Œé€šè¿‡æŒ‡å®šé…ç½®æ–‡ä»¶çš„æ–¹å¼å¯åŠ¨redisæœåŠ¡ï¼Œä¾‹å¦‚ï¼š\ncd 7000 ../redis-server ./redis.conf å¦‚æœæ˜¯ä»¥å‰å°æ–¹å¼è¿è¡Œï¼Œåˆ™ä¼šåœ¨æ§åˆ¶å°è¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š\n[82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1 æ¯ä¸ªå®ä¾‹éƒ½ä¼šç”Ÿæˆä¸€ä¸ªNode IDï¼Œç±»ä¼¼97a3a64667477371c4479320d683e4c8db5858b1ï¼Œç”¨æ¥ä½œä¸ºRediså®ä¾‹åœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€æ ‡è¯†ï¼Œè€Œä¸æ˜¯é€šè¿‡IPå’ŒPortï¼ŒIPå’ŒPortå¯èƒ½ä¼šæ”¹å˜ï¼Œè¯¥Node IDä¸ä¼šæ”¹å˜ã€‚\nç›®å½•ç»“æ„å¯å‚è€ƒï¼š\ncluster-test/ â”œâ”€â”€ 7000 â”‚Â â”œâ”€â”€ appendonly.aof â”‚Â â”œâ”€â”€ dump.rdb â”‚Â â”œâ”€â”€ nodes.conf â”‚Â â””â”€â”€ redis.conf â”œâ”€â”€ 7001 â”‚Â â”œâ”€â”€ appendonly.aof â”‚Â â”œâ”€â”€ dump.rdb â”‚Â â”œâ”€â”€ nodes.conf â”‚Â â””â”€â”€ redis.conf â”œâ”€â”€ 7002 â”‚Â â”œâ”€â”€ appendonly.aof â”‚Â â”œâ”€â”€ dump.rdb â”‚Â â”œâ”€â”€ nodes.conf â”‚Â â””â”€â”€ redis.conf â”œâ”€â”€ 7003 â”‚Â â”œâ”€â”€ appendonly.aof â”‚Â â”œâ”€â”€ dump.rdb â”‚Â â”œâ”€â”€ nodes.conf â”‚Â â””â”€â”€ redis.conf â”œâ”€â”€ 7004 â”‚Â â”œâ”€â”€ appendonly.aof â”‚Â â”œâ”€â”€ dump.rdb â”‚Â â”œâ”€â”€ nodes.conf â”‚Â â””â”€â”€ redis.conf â”œâ”€â”€ 7005 â”‚Â â”œâ”€â”€ appendonly.aof â”‚Â â”œâ”€â”€ dump.rdb â”‚Â â”œâ”€â”€ nodes.conf â”‚Â â””â”€â”€ redis.conf â”œâ”€â”€ redis-cli â””â”€â”€ redis-server 2.1.2 redis-tribåˆ›å»ºé›†ç¾¤ Redisçš„å®ä¾‹å…¨éƒ¨è¿è¡Œä¹‹åï¼Œè¿˜éœ€è¦redis-trib.rbå·¥å…·æ¥å®Œæˆé›†ç¾¤çš„åˆ›å»ºï¼Œredis-trib.rbäºŒè¿›åˆ¶æ–‡ä»¶åœ¨RedisåŒ…ä¸»ç›®å½•ä¸‹çš„srcç›®å½•ä¸­ï¼Œè¿è¡Œè¯¥å·¥å…·ä¾èµ–Rubyç¯å¢ƒå’Œgemï¼Œå› æ­¤éœ€è¦æå‰å®‰è£…ã€‚\n1ã€å®‰è£…Ruby\nyum -y install ruby rubygems æŸ¥çœ‹Rubyç‰ˆæœ¬ä¿¡æ¯ã€‚\n[root@kube-node-1 src]# ruby --version ruby 2.0.0p648 (2015-12-16) [x86_64-linux] ç”±äºcentosç³»ç»Ÿé»˜è®¤æ”¯æŒRubyç‰ˆæœ¬ä¸º2.0.0ï¼Œå› æ­¤æ‰§è¡Œgem install rediså‘½ä»¤æ—¶ä¼šæŠ¥ä»¥ä¸‹é”™è¯¯ã€‚\n[root@kube-node-1 src]# gem install redis Fetching: redis-4.0.1.gem (100%) ERROR: Error installing redis: redis requires Ruby version \u003e= 2.2.2. è§£å†³æ–¹æ³•æ˜¯å…ˆå®‰è£…rvmï¼Œå†å‡çº§rubyç‰ˆæœ¬ã€‚\n2ã€å®‰è£…rvm\ncurl -L get.rvm.io | bash -s stable å¦‚æœé‡åˆ°ä»¥ä¸‹æŠ¥é”™ï¼Œåˆ™æ‰§è¡ŒæŠ¥é”™ä¸­çš„gpg2 --recv-keys çš„å‘½ä»¤ã€‚\n[root@kube-node-1 ~]# curl -L get.rvm.io | bash -s stable % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 194 100 194 0 0 335 0 --:--:-- --:--:-- --:--:-- 335 100 24090 100 24090 0 0 17421 0 0:00:01 0:00:01 --:--:-- 44446 Downloading https://github.com/rvm/rvm/archive/1.29.3.tar.gz Downloading https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc gpg: äº 2017å¹´09æœˆ11æ—¥ æ˜ŸæœŸä¸€ 04æ—¶59åˆ†21ç§’ CST åˆ›å»ºçš„ç­¾åï¼Œä½¿ç”¨ RSAï¼Œé’¥åŒ™å· BF04FF17 gpg: æ— æ³•æ£€æŸ¥ç­¾åï¼šæ²¡æœ‰å…¬é’¥ Warning, RVM 1.26.0 introduces signed releases and automated check of signatures when GPG software found. Assuming you trust Michal Papis import the mpapis public key (downloading the signatures). GPG signature verification failed for '/usr/local/rvm/archives/rvm-1.29.3.tgz' - 'https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc'! Try to install GPG v2 and then fetch the public key: gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 or if it fails: command curl -sSL https://rvm.io/mpapis.asc | gpg2 --import - the key can be compared with: https://rvm.io/mpapis.asc https://keybase.io/mpapis NOTE: GPG version 2.1.17 have a bug which cause failures during fetching keys from remote server. Please downgrade or upgrade to newer version (if available) or use the second method described above. æ‰§è¡ŒæŠ¥é”™ä¸­çš„gpg2 --recv-keys çš„å‘½ä»¤ã€‚\nä¾‹å¦‚ï¼š\n[root@kube-node-1 ~]# gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 gpg: é’¥åŒ™ç¯â€˜/root/.gnupg/secring.gpgâ€™å·²å»ºç«‹ gpg: ä¸‹è½½å¯†é’¥â€˜D39DC0E3â€™ï¼Œä» hkp æœåŠ¡å™¨ keys.gnupg.net gpg: /root/.gnupg/trustdb.gpgï¼šå»ºç«‹äº†ä¿¡ä»»åº¦æ•°æ®åº“ gpg: å¯†é’¥ D39DC0E3ï¼šå…¬é’¥â€œMichal Papis (RVM signing) \u003cmpapis@gmail.com\u003eâ€å·²å¯¼å…¥ gpg: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»å¯¹ä¿¡ä»»çš„å¯†é’¥ gpg: åˆè®¡è¢«å¤„ç†çš„æ•°é‡ï¼š1 gpg: å·²å¯¼å…¥ï¼š1 (RSA: 1) å†æ¬¡æ‰§è¡Œå‘½ä»¤curl -L get.rvm.io | bash -s stableã€‚ä¾‹å¦‚ï¼š\n[root@kube-node-1 ~]# curl -L get.rvm.io | bash -s stable % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 194 100 194 0 0 310 0 --:--:-- --:--:-- --:--:-- 309 100 24090 100 24090 0 0 18230 0 0:00:01 0:00:01 --:--:-- 103k Downloading https://github.com/rvm/rvm/archive/1.29.3.tar.gz Downloading https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc gpg: äº 2017å¹´09æœˆ11æ—¥ æ˜ŸæœŸä¸€ 04æ—¶59åˆ†21ç§’ CST åˆ›å»ºçš„ç­¾åï¼Œä½¿ç”¨ RSAï¼Œé’¥åŒ™å· BF04FF17 gpg: å®Œå¥½çš„ç­¾åï¼Œæ¥è‡ªäºâ€œMichal Papis (RVM signing) \u003cmpapis@gmail.com\u003eâ€ gpg: äº¦å³â€œMichal Papis \u003cmichal.papis@toptal.com\u003eâ€ gpg: äº¦å³â€œ[jpeg image of size 5015]â€ gpg: è­¦å‘Šï¼šè¿™æŠŠå¯†é’¥æœªç»å—ä¿¡ä»»çš„ç­¾åè®¤è¯ï¼ gpg: æ²¡æœ‰è¯æ®è¡¨æ˜è¿™ä¸ªç­¾åå±äºå®ƒæ‰€å£°ç§°çš„æŒæœ‰è€…ã€‚ ä¸»é’¥æŒ‡çº¹ï¼š 409B 6B17 96C2 7546 2A17 0311 3804 BB82 D39D C0E3 å­é’¥æŒ‡çº¹ï¼š 62C9 E5F4 DA30 0D94 AC36 166B E206 C29F BF04 FF17 GPG verified '/usr/local/rvm/archives/rvm-1.29.3.tgz' Creating group 'rvm' Installing RVM to /usr/local/rvm/ Installation of RVM in /usr/local/rvm/ is almost complete: * First you need to add all users that will be using rvm to 'rvm' group, and logout - login again, anyone using rvm will be operating with `umask u=rwx,g=rwx,o=rx`. * To start using RVM you need to run `source /etc/profile.d/rvm.sh` in all your open bash windows, in rare cases you need to reopen all bash windows. ä»¥ä¸Šè¡¨ç¤ºæ‰§è¡ŒæˆåŠŸï¼Œ\nsource /usr/local/rvm/scripts/rvm æŸ¥çœ‹rvmåº“ä¸­å·²çŸ¥çš„rubyç‰ˆæœ¬\nrvm list known ä¾‹å¦‚ï¼š\n[root@kube-node-1 ~]# rvm list known # MRI Rubies [ruby-]1.8.6[-p420] [ruby-]1.8.7[-head] # security released on head [ruby-]1.9.1[-p431] [ruby-]1.9.2[-p330] [ruby-]1.9.3[-p551] [ruby-]2.0.0[-p648] [ruby-]2.1[.10] [ruby-]2.2[.7] [ruby-]2.3[.4] [ruby-]2.4[.1] ruby-head ... 3ã€å‡çº§Ruby\n#å®‰è£…ruby rvm install 2.4.0 #ä½¿ç”¨æ–°ç‰ˆæœ¬ rvm use 2.4.0 #ç§»é™¤æ—§ç‰ˆæœ¬ rvm remove 2.0.0 #æŸ¥çœ‹å½“å‰ç‰ˆæœ¬ ruby --version ä¾‹å¦‚ï¼š\n[root@kube-node-1 ~]# rvm install 2.4.0 Searching for binary rubies, this might take some time. Found remote file https://rvm_io.global.ssl.fastly.net/binaries/centos/7/x86_64/ruby-2.4.0.tar.bz2 Checking requirements for centos. Installing requirements for centos. Installing required packages: autoconf, automake, bison, bzip2, gcc-c++, libffi-devel, libtool, readline-devel, sqlite-devel, zlib-devel, libyaml-devel, openssl-devel................................ Requirements installation successful. ruby-2.4.0 - #configure ruby-2.4.0 - #download % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 14.0M 100 14.0M 0 0 852k 0 0:00:16 0:00:16 --:--:-- 980k No checksum for downloaded archive, recording checksum in user configuration. ruby-2.4.0 - #validate archive ruby-2.4.0 - #extract ruby-2.4.0 - #validate binary ruby-2.4.0 - #setup ruby-2.4.0 - #gemset created /usr/local/rvm/gems/ruby-2.4.0@global ruby-2.4.0 - #importing gemset /usr/local/rvm/gemsets/global.gems.............................. ruby-2.4.0 - #generating global wrappers........ ruby-2.4.0 - #gemset created /usr/local/rvm/gems/ruby-2.4.0 ruby-2.4.0 - #importing gemsetfile /usr/local/rvm/gemsets/default.gems evaluated to empty gem list ruby-2.4.0 - #generating default wrappers........ [root@kube-node-1 ~]# rvm use 2.4.0 Using /usr/local/rvm/gems/ruby-2.4.0 [root@kube-node-1 ~]# rvm remove 2.0.0 ruby-2.0.0-p648 - #already gone Using /usr/local/rvm/gems/ruby-2.4.0 [root@kube-node-1 ~]# ruby --version ruby 2.4.0p0 (2016-12-24 revision 57164) [x86_64-linux] 4ã€å®‰è£…gem\ngem install redis ä¾‹å¦‚ï¼š\n[root@kube-node-1 ~]# gem install redis Fetching: redis-4.0.1.gem (100%) Successfully installed redis-4.0.1 Parsing documentation for redis-4.0.1 Installing ri documentation for redis-4.0.1 Done installing documentation for redis after 2 seconds 1 gem installed 5ã€æ‰§è¡Œredis-trib.rbå‘½ä»¤\nä»¥ä¸Šè¡¨ç¤ºå®‰è£…æˆåŠŸï¼Œå¯ä»¥æ‰§è¡Œredis-trib.rbå‘½ä»¤ã€‚\ncd src #æ‰§è¡Œredis-trib.rbå‘½ä»¤ ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ \u003e 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 å‚æ•°createè¡¨ç¤ºåˆ›å»ºä¸€ä¸ªæ–°çš„é›†ç¾¤ï¼Œ--replicas 1è¡¨ç¤ºä¸ºæ¯ä¸ªmasteråˆ›å»ºä¸€ä¸ªslaveã€‚\nå¦‚æœåˆ›å»ºæˆåŠŸä¼šæ˜¾ç¤ºä»¥ä¸‹ä¿¡æ¯\n[OK] All 16384 slots covered ä¾‹å¦‚ï¼š\n[root@kube-node-1 src]# ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ \u003e 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \u003e\u003e\u003e Creating cluster \u003e\u003e\u003e Performing hash slots allocation on 6 nodes... Using 3 masters: 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 Adding replica 127.0.0.1:7004 to 127.0.0.1:7000 Adding replica 127.0.0.1:7005 to 127.0.0.1:7001 Adding replica 127.0.0.1:7003 to 127.0.0.1:7002 \u003e\u003e\u003e Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000 slots:0-5460 (5461 slots) master M: 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001 slots:5461-10922 (5462 slots) master M: be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002 slots:10923-16383 (5461 slots) master S: 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003 replicates 13d0c397604a0b2644244c37b666fce83f29faa8 S: dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004 replicates be2718476eba4e56f696e56b75e67df720b7fc24 S: 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005 replicates d5a834d075fd93eefab877c6ebb86efff680650f Can I set the above configuration? (type 'yes' to accept): yes \u003e\u003e\u003e Nodes configuration updated \u003e\u003e\u003e Assign a different config epoch to each node \u003e\u003e\u003e Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join.... \u003e\u003e\u003e Performing Cluster Check (using node 127.0.0.1:7000) M: d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s) M: be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003 slots: (0 slots) slave replicates 13d0c397604a0b2644244c37b666fce83f29faa8 S: 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005 slots: (0 slots) slave replicates d5a834d075fd93eefab877c6ebb86efff680650f S: dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004 slots: (0 slots) slave replicates be2718476eba4e56f696e56b75e67df720b7fc24 [OK] All nodes agree about slots configuration. \u003e\u003e\u003e Check for open slots... \u003e\u003e\u003e Check slots coverage... [OK] All 16384 slots covered. 2.1.3 éƒ¨ç½²ç»“æœéªŒè¯ 1ã€å®¢æˆ·ç«¯è®¿é—®\nä½¿ç”¨å®¢æˆ·ç«¯redis-cli äºŒè¿›åˆ¶è®¿é—®æŸä¸ªå®ä¾‹ï¼Œæ‰§è¡Œsetå’Œgetçš„æµ‹è¯•ã€‚\n$ redis-cli -c -p 7000 redis 127.0.0.1:7000\u003e set foo bar -\u003e Redirected to slot [12182] located at 127.0.0.1:7002 OK redis 127.0.0.1:7002\u003e set hello world -\u003e Redirected to slot [866] located at 127.0.0.1:7000 OK redis 127.0.0.1:7000\u003e get foo -\u003e Redirected to slot [12182] located at 127.0.0.1:7002 \"bar\" redis 127.0.0.1:7000\u003e get hello -\u003e Redirected to slot [866] located at 127.0.0.1:7000 \"world\" 2ã€æŸ¥çœ‹é›†ç¾¤çŠ¶æ€\nä½¿ç”¨cluster infoå‘½ä»¤æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ã€‚\n127.0.0.1:7000\u003e cluster info cluster_state:ok #é›†ç¾¤çŠ¶æ€ cluster_slots_assigned:16384 #è¢«åˆ†é…çš„æ§½ä½æ•° cluster_slots_ok:16384 #æ­£ç¡®åˆ†é…çš„æ§½ä½ cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 #å½“å‰èŠ‚ç‚¹ cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:48273 cluster_stats_messages_pong_sent:49884 cluster_stats_messages_sent:98157 cluster_stats_messages_ping_received:49879 cluster_stats_messages_pong_received:48273 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:98157 3ã€æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€\nä½¿ç”¨cluster nodeså‘½ä»¤æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€ã€‚\n127.0.0.1:7000\u003e cluster nodes be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002@17002 master - 0 1517303607000 3 connected 10923-16383 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001@17001 master - 0 1517303606000 2 connected 5461-10922 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003@17003 slave 13d0c397604a0b2644244c37b666fce83f29faa8 0 1517303606030 4 connected d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000@17000 myself,master - 0 1517303604000 1 connected 0-5460 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005@17005 slave d5a834d075fd93eefab877c6ebb86efff680650f 0 1517303607060 6 connected dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004@17004 slave be2718476eba4e56f696e56b75e67df720b7fc24 0 1517303608082 5 connected å‚è€ƒæ–‡ç« ï¼š\nhttps://redis.io/download\nhttps://redis.io/topics/cluster-tutorial\n","categories":"","description":"","excerpt":"1. Rediséƒ¨ç½² ä»¥ä¸‹ä»¥Linuxç³»ç»Ÿä¸ºä¾‹\n1.1 ä¸‹è½½å’Œç¼–è¯‘ $ wget â€¦","ref":"/linux-notes/redis/redis-cluster/","tags":["Redis"],"title":"Redisé›†ç¾¤æ¨¡å¼éƒ¨ç½²"},{"body":"1. shellå˜é‡ Shellæ”¯æŒè‡ªå®šä¹‰å˜é‡ã€‚\n1.1. å®šä¹‰å˜é‡ å®šä¹‰å˜é‡æ—¶ï¼Œå˜é‡åä¸åŠ ç¾å…ƒç¬¦å·ï¼ˆ$ï¼‰ï¼Œå¦‚ï¼š\nvariableName=\"value\" æ³¨æ„ï¼Œå˜é‡åå’Œç­‰å·ä¹‹é—´ä¸èƒ½æœ‰ç©ºæ ¼ï¼Œè¿™å¯èƒ½å’Œä½ ç†Ÿæ‚‰çš„æ‰€æœ‰ç¼–ç¨‹è¯­è¨€éƒ½ä¸ä¸€æ ·ã€‚åŒæ—¶ï¼Œå˜é‡åçš„å‘½åé¡»éµå¾ªå¦‚ä¸‹è§„åˆ™ï¼š\né¦–ä¸ªå­—ç¬¦å¿…é¡»ä¸ºå­—æ¯ï¼ˆa-zï¼ŒA-Zï¼‰ã€‚ ä¸­é—´ä¸èƒ½æœ‰ç©ºæ ¼ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹åˆ’çº¿ï¼ˆ_ï¼‰ã€‚ ä¸èƒ½ä½¿ç”¨æ ‡ç‚¹ç¬¦å·ã€‚ ä¸èƒ½ä½¿ç”¨bashé‡Œçš„å…³é”®å­—ï¼ˆå¯ç”¨helpå‘½ä»¤æŸ¥çœ‹ä¿ç•™å…³é”®å­—ï¼‰ã€‚ 1.2. ä½¿ç”¨å˜é‡ ä½¿ç”¨ä¸€ä¸ªå®šä¹‰è¿‡çš„å˜é‡ï¼Œåªè¦åœ¨å˜é‡åå‰é¢åŠ ç¾å…ƒç¬¦å·ï¼ˆ$ï¼‰å³å¯ï¼Œå¦‚ï¼š\nyour_name=\"mozhiyan\" echo $your_name echo ${your_name} å˜é‡åå¤–é¢çš„èŠ±æ‹¬å·æ˜¯å¯é€‰çš„ï¼ŒåŠ ä¸åŠ éƒ½è¡Œï¼Œï¼Œæ¯”å¦‚ä¸‹é¢è¿™ç§æƒ…å†µï¼š\nfor skill in Ada Coffe Action Java do echo \"I am good at ${skill}Script\" done å¦‚æœä¸ç»™skillå˜é‡åŠ èŠ±æ‹¬å·ï¼Œå†™æˆecho \"I am good at $skillScript\"ï¼Œè§£é‡Šå™¨å°±ä¼šæŠŠ$skillScriptå½“æˆä¸€ä¸ªå˜é‡ï¼ˆå…¶å€¼ä¸ºç©ºï¼‰ï¼Œä»£ç æ‰§è¡Œç»“æœå°±ä¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„æ ·å­äº†ã€‚æ¨èç»™æ‰€æœ‰å˜é‡åŠ ä¸ŠèŠ±æ‹¬å·ï¼Œè¿™æ˜¯ä¸ªå¥½çš„ç¼–ç¨‹ä¹ æƒ¯ã€‚\n1.3. é‡æ–°å®šä¹‰å˜é‡ å·²å®šä¹‰çš„å˜é‡ï¼Œå¯ä»¥è¢«é‡æ–°å®šä¹‰ï¼Œå¦‚ï¼š\nmyUrl=\"http://see.xidian.edu.cn/cpp/linux/\" echo ${myUrl} myUrl=\"http://see.xidian.edu.cn/cpp/shell/\" echo ${myUrl} è¿™æ ·å†™æ˜¯åˆæ³•çš„ï¼Œä½†æ³¨æ„ï¼Œç¬¬äºŒæ¬¡èµ‹å€¼çš„æ—¶å€™ä¸èƒ½å†™ $myUrl=\"http://see.xidian.edu.cn/cpp/shell/\"ï¼Œ ä½¿ç”¨å˜é‡çš„æ—¶å€™æ‰åŠ ç¾å…ƒç¬¦ï¼ˆ$ï¼‰ã€‚\n1.4. åªè¯»å˜é‡ ä½¿ç”¨ readonly å‘½ä»¤å¯ä»¥å°†å˜é‡å®šä¹‰ä¸ºåªè¯»å˜é‡ï¼Œåªè¯»å˜é‡çš„å€¼ä¸èƒ½è¢«æ”¹å˜ã€‚\nä¸‹é¢çš„ä¾‹å­å°è¯•æ›´æ”¹åªè¯»å˜é‡ï¼Œç»“æœæŠ¥é”™ï¼š\n#!/bin/bash myUrl=\"http://see.xidian.edu.cn/cpp/shell/\" readonly myUrl myUrl=\"http://see.xidian.edu.cn/cpp/danpianji/\" è¿è¡Œè„šæœ¬ï¼Œç»“æœå¦‚ä¸‹ï¼š\n/bin/sh: NAME: This variable is read only. 1.5. åˆ é™¤å˜é‡ ä½¿ç”¨ unset å‘½ä»¤å¯ä»¥åˆ é™¤å˜é‡ã€‚è¯­æ³•ï¼š\nunset variable_name å˜é‡è¢«åˆ é™¤åä¸èƒ½å†æ¬¡ä½¿ç”¨ï¼›unset å‘½ä»¤ä¸èƒ½åˆ é™¤åªè¯»å˜é‡ã€‚\nä¸¾ä¸ªä¾‹å­ï¼š\n#!/bin/sh myUrl=\"http://see.xidian.edu.cn/cpp/u/xitong/\" unset myUrl echo $myUrl ä¸Šé¢çš„è„šæœ¬æ²¡æœ‰ä»»ä½•è¾“å‡ºã€‚\n1.6. å˜é‡ç±»å‹ è¿è¡Œshellæ—¶ï¼Œä¼šåŒæ—¶å­˜åœ¨ä¸‰ç§å˜é‡ï¼š\n1) å±€éƒ¨å˜é‡ å±€éƒ¨å˜é‡åœ¨è„šæœ¬æˆ–å‘½ä»¤ä¸­å®šä¹‰ï¼Œä»…åœ¨å½“å‰shellå®ä¾‹ä¸­æœ‰æ•ˆï¼Œå…¶ä»–shellå¯åŠ¨çš„ç¨‹åºä¸èƒ½è®¿é—®å±€éƒ¨å˜é‡ã€‚\n2) ç¯å¢ƒå˜é‡ æ‰€æœ‰çš„ç¨‹åºï¼ŒåŒ…æ‹¬shellå¯åŠ¨çš„ç¨‹åºï¼Œéƒ½èƒ½è®¿é—®ç¯å¢ƒå˜é‡ï¼Œæœ‰äº›ç¨‹åºéœ€è¦ç¯å¢ƒå˜é‡æ¥ä¿è¯å…¶æ­£å¸¸è¿è¡Œã€‚å¿…è¦çš„æ—¶å€™shellè„šæœ¬ä¹Ÿå¯ä»¥å®šä¹‰ç¯å¢ƒå˜é‡ã€‚\n3) shellå˜é‡ shellå˜é‡æ˜¯ç”±shellç¨‹åºè®¾ç½®çš„ç‰¹æ®Šå˜é‡ã€‚shellå˜é‡ä¸­æœ‰ä¸€éƒ¨åˆ†æ˜¯ç¯å¢ƒå˜é‡ï¼Œæœ‰ä¸€éƒ¨åˆ†æ˜¯å±€éƒ¨å˜é‡ï¼Œè¿™äº›å˜é‡ä¿è¯äº†shellçš„æ­£å¸¸è¿è¡Œã€‚\n2. shellçš„ç‰¹æ®Šå˜é‡ ç‰¹æ®Šå˜é‡åˆ—è¡¨\nå˜é‡ å«ä¹‰ $0 å½“å‰è„šæœ¬çš„æ–‡ä»¶å $n ä¼ é€’ç»™è„šæœ¬æˆ–å‡½æ•°çš„å‚æ•°ã€‚n æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºç¬¬å‡ ä¸ªå‚æ•°ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯$1ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯$2ã€‚ $# ä¼ é€’ç»™è„šæœ¬æˆ–å‡½æ•°çš„å‚æ•°ä¸ªæ•°ã€‚ $* ä¼ é€’ç»™è„šæœ¬æˆ–å‡½æ•°çš„æ‰€æœ‰å‚æ•°ã€‚ $@ ä¼ é€’ç»™è„šæœ¬æˆ–å‡½æ•°çš„æ‰€æœ‰å‚æ•°ã€‚è¢«åŒå¼•å·(\" \")åŒ…å«æ—¶ï¼Œä¸ $* ç¨æœ‰ä¸åŒï¼Œä¸‹é¢å°†ä¼šè®²åˆ°ã€‚ $? ä¸Šä¸ªå‘½ä»¤çš„é€€å‡ºçŠ¶æ€ï¼Œæˆ–å‡½æ•°çš„è¿”å›å€¼ã€‚ $$ å½“å‰Shellè¿›ç¨‹IDã€‚å¯¹äº Shell è„šæœ¬ï¼Œå°±æ˜¯è¿™äº›è„šæœ¬æ‰€åœ¨çš„è¿›ç¨‹IDã€‚ 2.1. å‘½ä»¤è¡Œå‚æ•° è¿è¡Œè„šæœ¬æ—¶ä¼ é€’ç»™è„šæœ¬çš„å‚æ•°ç§°ä¸ºå‘½ä»¤è¡Œå‚æ•°ã€‚å‘½ä»¤è¡Œå‚æ•°ç”¨\\ $n è¡¨ç¤ºï¼Œä¾‹å¦‚ï¼Œ$1 è¡¨ç¤ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œ$2 è¡¨ç¤ºç¬¬äºŒä¸ªå‚æ•°ï¼Œä¾æ¬¡ç±»æ¨ã€‚\n2.2. $* å’Œ$@ çš„åŒºåˆ« $* å’Œ $@ éƒ½è¡¨ç¤ºä¼ é€’ç»™å‡½æ•°æˆ–è„šæœ¬çš„æ‰€æœ‰å‚æ•°ï¼Œä¸è¢«åŒå¼•å·(\" \")åŒ…å«æ—¶ï¼Œéƒ½ä»¥\"$1\" \"$2\" â€¦ \"$n\" çš„å½¢å¼è¾“å‡ºæ‰€æœ‰å‚æ•°ã€‚ ä½†æ˜¯å½“å®ƒä»¬è¢«åŒå¼•å·(\" \")åŒ…å«æ—¶ï¼Œ\n\"$*\" ä¼šå°†æ‰€æœ‰çš„å‚æ•°ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼Œä»¥\"$1 $2 â€¦ $n\"çš„å½¢å¼è¾“å‡ºæ‰€æœ‰å‚æ•°ï¼›\n\"$@\" ä¼šå°†å„ä¸ªå‚æ•°åˆ†å¼€ï¼Œä»¥\"$1\" \"$2\" â€¦ \"$n\" çš„å½¢å¼è¾“å‡ºæ‰€æœ‰å‚æ•°ã€‚\n2.3. é€€å‡ºçŠ¶æ€ $? å¯ä»¥è·å–ä¸Šä¸€ä¸ªå‘½ä»¤çš„é€€å‡ºçŠ¶æ€ã€‚æ‰€è°“é€€å‡ºçŠ¶æ€ï¼Œå°±æ˜¯ä¸Šä¸€ä¸ªå‘½ä»¤æ‰§è¡Œåçš„è¿”å›ç»“æœã€‚ é€€å‡ºçŠ¶æ€æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¤§éƒ¨åˆ†å‘½ä»¤æ‰§è¡ŒæˆåŠŸä¼šè¿”å› 0ï¼Œå¤±è´¥è¿”å› 1ã€‚ ä¸è¿‡ï¼Œä¹Ÿæœ‰ä¸€äº›å‘½ä»¤è¿”å›å…¶ä»–å€¼ï¼Œè¡¨ç¤ºä¸åŒç±»å‹çš„é”™è¯¯ã€‚$? ä¹Ÿå¯ä»¥è¡¨ç¤ºå‡½æ•°çš„è¿”å›å€¼ã€‚\n3. è½¬ä¹‰å­—ç¬¦ å¦‚æœè¡¨è¾¾å¼ä¸­åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ŒShell å°†ä¼šè¿›è¡Œæ›¿æ¢ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒå¼•å·ä¸­ä½¿ç”¨å˜é‡å°±æ˜¯ä¸€ç§æ›¿æ¢ï¼Œè½¬ä¹‰å­—ç¬¦ä¹Ÿæ˜¯ä¸€ç§æ›¿æ¢ã€‚\necho -e \"Value of a is $a \\n\" -e è¡¨ç¤ºå¯¹è½¬ä¹‰å­—ç¬¦è¿›è¡Œæ›¿æ¢\nä¸‹é¢çš„è½¬ä¹‰å­—ç¬¦éƒ½å¯ä»¥ç”¨åœ¨ echo ä¸­ï¼š\nè½¬ä¹‰å­—ç¬¦ å«ä¹‰ \\ åæ–œæ  \\a è­¦æŠ¥ï¼Œå“é“ƒ \\b é€€æ ¼ï¼ˆåˆ é™¤é”®ï¼‰ \\f æ¢é¡µ(FF)ï¼Œå°†å½“å‰ä½ç½®ç§»åˆ°ä¸‹é¡µå¼€å¤´ \\n æ¢è¡Œ \\r å›è½¦ \\t æ°´å¹³åˆ¶è¡¨ç¬¦ï¼ˆtabé”®ï¼‰ \\v å‚ç›´åˆ¶è¡¨ç¬¦ å¯ä»¥ä½¿ç”¨ echo å‘½ä»¤çš„ -E é€‰é¡¹ç¦æ­¢è½¬ä¹‰ï¼Œé»˜è®¤ä¹Ÿæ˜¯ä¸è½¬ä¹‰çš„ï¼›ä½¿ç”¨ -n é€‰é¡¹å¯ä»¥ç¦æ­¢æ’å…¥æ¢è¡Œç¬¦ã€‚\n4. å˜é‡æ›¿æ¢ 4.1. å‘½ä»¤æ›¿æ¢ å‘½ä»¤æ›¿æ¢æ˜¯æŒ‡Shellå¯ä»¥å…ˆæ‰§è¡Œå‘½ä»¤ï¼Œå°†è¾“å‡ºç»“æœæš‚æ—¶ä¿å­˜ï¼Œåœ¨é€‚å½“çš„åœ°æ–¹è¾“å‡ºã€‚ å‘½ä»¤æ›¿æ¢çš„è¯­æ³•ï¼š\n`command` æ³¨æ„æ˜¯åå¼•å·ï¼Œä¸æ˜¯å•å¼•å·ï¼Œè¿™ä¸ªé”®ä½äº Esc é”®ä¸‹æ–¹ã€‚\nDATE=`date` echo \"Date is $DATE\" 4.2. å˜é‡æ›¿æ¢ å˜é‡æ›¿æ¢å¯ä»¥æ ¹æ®å˜é‡çš„çŠ¶æ€ï¼ˆæ˜¯å¦ä¸ºç©ºã€æ˜¯å¦å®šä¹‰ç­‰ï¼‰æ¥æ”¹å˜å®ƒçš„å€¼ å¯ä»¥ä½¿ç”¨çš„å˜é‡æ›¿æ¢å½¢å¼ï¼š\nå½¢å¼ è¯´æ˜ ${var} å˜é‡æœ¬æ¥çš„å€¼ ${var:-word} å¦‚æœå˜é‡ var ä¸ºç©ºæˆ–å·²è¢«åˆ é™¤(unset)ï¼Œé‚£ä¹ˆè¿”å› wordï¼Œä½†ä¸æ”¹å˜ var çš„å€¼ã€‚ ${var:=word} å¦‚æœå˜é‡ var ä¸ºç©ºæˆ–å·²è¢«åˆ é™¤(unset)ï¼Œé‚£ä¹ˆè¿”å› wordï¼Œå¹¶å°† var çš„å€¼è®¾ç½®ä¸º wordã€‚ ${var:?message} å¦‚æœå˜é‡ var ä¸ºç©ºæˆ–å·²è¢«åˆ é™¤(unset)ï¼Œé‚£ä¹ˆå°†æ¶ˆæ¯ message é€åˆ°æ ‡å‡†é”™è¯¯è¾“å‡ºï¼Œå¯ä»¥ç”¨æ¥æ£€æµ‹å˜é‡ var æ˜¯å¦å¯ä»¥è¢«æ­£å¸¸èµ‹å€¼ã€‚ è‹¥æ­¤æ›¿æ¢å‡ºç°åœ¨Shellè„šæœ¬ä¸­ï¼Œé‚£ä¹ˆè„šæœ¬å°†åœæ­¢è¿è¡Œã€‚ ${var:+word} å¦‚æœå˜é‡ var è¢«å®šä¹‰ï¼Œé‚£ä¹ˆè¿”å› wordï¼Œä½†ä¸æ”¹å˜ var çš„å€¼ã€‚ ä½œç”¨ï¼šç”¨æ¥æ£€æµ‹å˜é‡æ˜¯å¦ä¸ºç©ºï¼Œå¹¶æç¤ºç›¸å…³ä¿¡æ¯ã€‚\nå‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. shellå˜é‡ Shellæ”¯æŒè‡ªå®šä¹‰å˜é‡ã€‚\n1.1. å®šä¹‰å˜é‡ å®šä¹‰å˜é‡æ—¶ï¼Œå˜é‡åä¸åŠ ç¾å…ƒç¬¦å·ï¼ˆ$ï¼‰ï¼Œå¦‚ï¼š â€¦","ref":"/linux-notes/shell/shell-var/","tags":["Shell"],"title":"Shellå˜é‡"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/multi-cluster/virtual-kubelet/","tags":"","title":"Virtual Kubelet"},{"body":"1. è¿ç§»Pod 1.1. è®¾ç½®èŠ‚ç‚¹æ˜¯å¦å¯è°ƒåº¦ ç¡®å®šéœ€è¦è¿ç§»å’Œè¢«è¿ç§»çš„èŠ‚ç‚¹ï¼Œå°†ä¸å…è®¸è¢«è¿ç§»çš„èŠ‚ç‚¹è®¾ç½®ä¸ºä¸å¯è°ƒåº¦ã€‚\n# æŸ¥çœ‹èŠ‚ç‚¹ kubectl get nodes # è®¾ç½®èŠ‚ç‚¹ä¸ºä¸å¯è°ƒåº¦ kubectl cordon \u003cNodeName\u003e # è®¾ç½®èŠ‚ç‚¹ä¸ºå¯è°ƒåº¦ kubectl uncordon \u003cNodeName\u003e 1.2. æ‰§è¡Œkubectl drainå‘½ä»¤ # é©±é€èŠ‚ç‚¹çš„æ‰€æœ‰pod kubectl drain \u003cNodeName\u003e --force --ignore-daemonsets # é©±é€æŒ‡å®šèŠ‚ç‚¹çš„pod kubectl drain \u003cNodeName\u003e --ignore-daemonsets\t--pod-selector=pod-template-hash=88964949c ç¤ºä¾‹ï¼š\n$ kubectl drain bjzw-prek8sredis-99-40 --force --ignore-daemonsets node \"bjzw-prek8sredis-99-40\" already cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: kube-proxy-bjzw-prek8sredis-99-40; Ignoring DaemonSet-managed pods: calicoopsmonitor-mfpqs, arachnia-agent-j56n8 pod \"pre-test-pro2-r-0-redis-2-8-19-1\" evicted pod \"pre-test-hwh1-r-8-redis-2-8-19-2\" evicted pod \"pre-eos-hdfs-vector-eos-hdfs-redis-2-8-19-0\" evicted 1.3. ç‰¹åˆ«è¯´æ˜ å¯¹äºstatefulsetåˆ›å»ºçš„Podï¼Œkubectl drainçš„è¯´æ˜å¦‚ä¸‹ï¼š\nkubectl drainæ“ä½œä¼šå°†ç›¸åº”èŠ‚ç‚¹ä¸Šçš„æ—§Podåˆ é™¤ï¼Œå¹¶åœ¨å¯è°ƒåº¦èŠ‚ç‚¹ä¸Šé¢èµ·ä¸€ä¸ªå¯¹åº”çš„Podã€‚å½“æ—§Podæ²¡æœ‰è¢«æ­£å¸¸åˆ é™¤çš„æƒ…å†µä¸‹ï¼Œæ–°Podä¸ä¼šèµ·æ¥ã€‚ä¾‹å¦‚ï¼šæ—§Podä¸€ç›´å¤„äºTerminatingçŠ¶æ€ã€‚\nå¯¹åº”çš„è§£å†³æ–¹å¼æ˜¯é€šè¿‡é‡å¯ç›¸åº”èŠ‚ç‚¹çš„kubeletï¼Œæˆ–è€…å¼ºåˆ¶åˆ é™¤è¯¥Podã€‚\nç¤ºä¾‹ï¼š\n# é‡å¯å‘ç”Ÿ`Terminating`èŠ‚ç‚¹çš„kubelet systemctl restart kubelet # å¼ºåˆ¶åˆ é™¤`Terminating`çŠ¶æ€çš„Pod kubectl delete pod \u003cPodName\u003e --namespace=\u003cNamespace\u003e --force --grace-period=0 2. kubectl drain æµç¨‹å›¾ 3. TroubleShooting 1ã€å­˜åœ¨ä¸æ˜¯é€šè¿‡ReplicationController, ReplicaSet, Job, DaemonSet æˆ–è€… StatefulSetåˆ›å»ºçš„Podï¼ˆå³é™æ€podï¼Œé€šè¿‡æ–‡ä»¶æ–¹å¼åˆ›å»ºçš„ï¼‰ï¼Œæ‰€ä»¥éœ€è¦è®¾ç½®å¼ºåˆ¶æ‰§è¡Œçš„å‚æ•°--forceã€‚\n$ kubectl drain bjzw-prek8sredis-99-40 node \"bjzw-prek8sredis-99-40\" already cordoned error: unable to drain node \"bjzw-prek8sredis-99-40\", aborting command... There are pending nodes to be drained: bjzw-prek8sredis-99-40 error: DaemonSet-managed pods (use --ignore-daemonsets to ignore): calicoopsmonitor-mfpqs, arachnia-agent-j56n8; pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): kube-proxy-bjzw-prek8sredis-99-40 2ã€å­˜åœ¨DaemonSetæ–¹å¼ç®¡ç†çš„Podï¼Œéœ€è¦è®¾ç½®--ignore-daemonsetså‚æ•°å¿½ç•¥æŠ¥é”™ã€‚\n$ kubectl drain bjzw-prek8sredis-99-40 --force node \"bjzw-prek8sredis-99-40\" already cordoned error: unable to drain node \"bjzw-prek8sredis-99-40\", aborting command... There are pending nodes to be drained: bjzw-prek8sredis-99-40 error: DaemonSet-managed pods (use --ignore-daemonsets to ignore): calicoopsmonitor-mfpqs, arachnia-agent-j56n8 4. kubectl drain $ kubectl drain --help Drain node in preparation for maintenance. The given node will be marked unschedulable to prevent new pods from arriving. 'drain' evicts the pods if the API server supports https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ . Otherwise, it will use normal DELETE to delete the pods. The 'drain' evicts or deletes all pods except mirror pods (which cannot be deleted through the API server). If there are daemon set-managed pods, drain will not proceed without --ignore-daemonsets, and regardless it will not delete any daemon set-managed pods, because those pods would be immediately replaced by the daemon set controller, which ignores unschedulable markings. If there are any pods that are neither mirror pods nor managed by a replication controller, replica set, daemon set, stateful set, or job, then drain will not delete any pods unless you use --force. --force will also allow deletion to proceed if the managing resource of one or more pods is missing. 'drain' waits for graceful termination. You should not operate on the machine until the command completes. When you are ready to put the node back into service, use kubectl uncordon, which will make the node schedulable again. https://kubernetes.io/images/docs/kubectl_drain.svg Examples: # Drain node \"foo\", even if there are pods not managed by a replication controller, replica set, job, daemon set or stateful set on it kubectl drain foo --force # As above, but abort if there are pods not managed by a replication controller, replica set, job, daemon set or stateful set, and use a grace period of 15 minutes kubectl drain foo --grace-period=900 Options: --chunk-size=500: Return large lists in chunks rather than all at once. Pass 0 to disable. This flag is beta and may change in the future. --delete-emptydir-data=false: Continue even if there are pods using emptyDir (local data that will be deleted when the node is drained). --disable-eviction=false: Force drain to use delete, even if eviction is supported. This will bypass checking PodDisruptionBudgets, use with caution. --dry-run='none': Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. --force=false: Continue even if there are pods not managed by a ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet. --grace-period=-1: Period of time in seconds given to each pod to terminate gracefully. If negative, the default value specified in the pod will be used. --ignore-daemonsets=false: Ignore DaemonSet-managed pods. --ignore-errors=false: Ignore errors occurred between drain nodes in group. --pod-selector='': Label selector to filter pods on the node -l, --selector='': Selector (label query) to filter on --skip-wait-for-delete-timeout=0: If pod DeletionTimestamp older than N seconds, skip waiting for the pod. Seconds must be greater than 0 to skip. --timeout=0s: The length of time to wait before giving up, zero means infinite Usage: kubectl drain NODE [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). å‚è€ƒæ–‡æ¡£ï¼š\nhttps://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/ https://kubernetes.io/docs/tasks/run-application/configure-pdb/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain ","categories":"","description":"","excerpt":"1. è¿ç§»Pod 1.1. è®¾ç½®èŠ‚ç‚¹æ˜¯å¦å¯è°ƒåº¦ ç¡®å®šéœ€è¦è¿ç§»å’Œè¢«è¿ç§»çš„èŠ‚ç‚¹ï¼Œå°†ä¸å…è®¸è¢«è¿ç§»çš„èŠ‚ç‚¹è®¾ç½®ä¸ºä¸å¯è°ƒåº¦ã€‚\n# â€¦","ref":"/kubernetes-notes/operation/node/safely-drain-node/","tags":["Kubernetes"],"title":"å®‰å…¨è¿ç§»èŠ‚ç‚¹"},{"body":"1. CentOS å®‰è£…Docker å»ºè®®ä½¿ç”¨centos7\n1.1. å®‰è£…Docker 1.1.1. å¸è½½æ—§ç‰ˆæœ¬ æ—§ç‰ˆæœ¬çš„Dockerå‘½åä¸ºdockeræˆ–docker-engineï¼Œå¦‚æœæœ‰å®‰è£…æ—§ç‰ˆæœ¬ï¼Œå…ˆå¸è½½æ—§ç‰ˆæœ¬\n$ sudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 1.1.2. ä½¿ç”¨ä»“åº“å®‰è£… 1ã€å®‰è£…yum-utilsã€device-mapper-persistent-dataã€lvm2\n$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 2ã€æ·»åŠ è½¯ä»¶æº\n$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1.1.3. å®‰è£…Docker å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„Docker CEã€‚\n$ sudo yum install -y docker-ce 1.1.4. å¯åŠ¨Docker # å¯åŠ¨Docker $ sudo systemctl start docker # è¿è¡Œå®¹å™¨ $ sudo docker run hello-world 1.2. å®‰è£…æŒ‡å®šç‰ˆæœ¬Docker 1ã€åˆ—å‡ºå¯å®‰è£…ç‰ˆæœ¬\n$ yum list docker-ce --showduplicates | sort -r docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable 2ã€å®‰è£…æŒ‡å®šç‰ˆæœ¬\nä¾‹å¦‚ï¼šdocker-ce-18.03.0.ce\n$ sudo yum install docker-ce-\u003cVERSION STRING\u003e 1.3. å‡çº§Docker ä¾æ®1.2çš„æ–¹æ³•é€‰æ‹©æŒ‡å®šç‰ˆæœ¬å®‰è£…ã€‚\n1.4. å¸è½½Docker # å¸è½½Docker $ sudo yum remove docker-ce # æ¸…ç†é•œåƒã€å®¹å™¨ã€å­˜å‚¨å·ç­‰ $ sudo rm -rf /var/lib/docker 2. Ubuntu å®‰è£…Docker 2.1. å®‰è£…Docker 2.1.1. å¸è½½æ—§ç‰ˆæœ¬ æ—§ç‰ˆæœ¬çš„Dockerå‘½åä¸ºdockeræˆ–docker-engineï¼Œå¦‚æœæœ‰å®‰è£…æ—§ç‰ˆæœ¬ï¼Œå…ˆå¸è½½æ—§ç‰ˆæœ¬\nsudo apt-get remove docker docker-engine docker.io 2.1.2. ä½¿ç”¨ä»“åº“å®‰è£… 1ã€å‡çº§apt\nsudo apt-get update 2ã€å…è®¸aptä½¿ç”¨https\nsudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common 3ã€æ·»åŠ Docker å®˜æ–¹çš„GPGå¯†é’¥\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 4ã€æ·»åŠ Dockerè½¯ä»¶æº\nsudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 2.1.3. å®‰è£…Docker # update sudo apt-get update # install docker sudo apt-get install docker-ce 2.1.4. å¯åŠ¨Docker # è®¾ç½®ä¸ºå¼€æœºå¯åŠ¨ sudo systemctl enable docker # å¯åŠ¨docker sudo systemctl start docker 2.2. å®‰è£…æŒ‡å®šç‰ˆæœ¬Docker 1ã€åˆ—å‡ºä»“åº“çš„å¯å®‰è£…ç‰ˆæœ¬ï¼Œapt-cache madison docker-ceã€‚\n# apt-cache madison docker-ce docker-ce | 18.06.0~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.03.1~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages 2ã€æŒ‡å®šç‰ˆæœ¬å®‰è£…\nä¾‹å¦‚ï¼šdocker-ce=18.03.0~ce-0~ubuntu\nsudo apt-get install docker-ce=\u003cVERSION\u003e 2.3. å‡çº§Docker # æ›´æ–°æº sudo apt-get update # ä¾æ®ä¸Šè¿°æ–¹æ³•ï¼ŒæŒ‡å®šç‰ˆæœ¬å®‰è£… 2.4. å¸è½½Docker # å¸è½½ docker ce sudo apt-get purge docker-ce # æ¸…ç†é•œåƒã€å®¹å™¨ã€å­˜å‚¨å·ç­‰ sudo rm -rf /var/lib/docker 3. ç¦»çº¿rpmåŒ…å®‰è£…Docker 3.1. ä¸‹è½½docker rpmåŒ… rpmåŒ…åœ°å€ï¼šhttps://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/\nä¸‹è½½æŒ‡å®šç‰ˆæœ¬çš„containerd.ioã€docker-ceã€docker-ce-cli\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm wget https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-18.09.9-3.el7.x86_64.rpm wget https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-cli-18.09.9-3.el7.x86_64.rpm ä¸‹è½½container-selinux\nåœ°å€ï¼šhttp://mirror.centos.org/centos/7/extras/x86_64/Packages/\nwget http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm 3.2. å®‰è£…rpmåŒ… # container-selinux rpm -ivh container-selinux*.rpm # containerd.io rpm -ivh containerd.io*.rpm # docker-ce rpm -ivh docker-ce*.rpm # docker-ce-cli rpm -ivh docker-ce-cli*.rpm 3.3. å¯åŠ¨dockeræœåŠ¡ # å¯åŠ¨ systemctl start docker # æŸ¥çœ‹çŠ¶æ€ systemctl status docker æ–‡ç« å‚è€ƒï¼š\nhttps://docs.docker.com/install/linux/docker-ce/centos/\nhttps://docs.docker.com/install/linux/docker-ce/ubuntu/\n","categories":"","description":"","excerpt":"1. CentOS å®‰è£…Docker å»ºè®®ä½¿ç”¨centos7\n1.1. å®‰è£…Docker 1.1.1. å¸è½½æ—§ç‰ˆæœ¬ æ—§ç‰ˆæœ¬çš„Dockerå‘½å â€¦","ref":"/kubernetes-notes/runtime/docker/install-docker/","tags":["Docker"],"title":"å®‰è£…Docker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/introduction/","tags":"","title":"å®‰è£…ä¸é…ç½®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/setup/","tags":"","title":"å®‰è£…ä¸é…ç½®"},{"body":"0. è¯´æ˜ è¦æ±‚Kubernetesçš„ç‰ˆæœ¬åœ¨1.11åŠä»¥ä¸Šï¼Œk8sé›†ç¾¤å¿…é¡»å…è®¸ç‰¹æƒPodï¼ˆprivileged podsï¼‰ï¼Œå³apiserverå’Œkubeletéœ€è¦è®¾ç½®--allow-privilegedä¸ºtrueã€‚èŠ‚ç‚¹çš„Docker daemonéœ€è¦å…è®¸æŒ‚è½½å…±äº«å·ã€‚\næ¶‰åŠé•œåƒ quay.io/k8scsi/csi-provisioner:v0.3.0 quay.io/k8scsi/csi-attacher:v0.3.0 quay.io/k8scsi/driver-registrar:v0.3.0 quay.io/cephcsi/cephfsplugin:v0.3.0 1. éƒ¨ç½²RBAC éƒ¨ç½²service accounts, cluster roles å’Œ cluster role bindingsï¼Œè¿™äº›å¯ä¾›RBDå’ŒCephFS CSI pluginså…±åŒä½¿ç”¨ï¼Œä»–ä»¬æ‹¥æœ‰ç›¸åŒçš„æƒé™ã€‚\n$ kubectl create -f csi-attacher-rbac.yaml $ kubectl create -f csi-provisioner-rbac.yaml $ kubectl create -f csi-nodeplugin-rbac.yaml 1.1. csi-attacher-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: csi-attacher --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: external-attacher-runner rules: - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-attacher-role subjects: - kind: ServiceAccount name: csi-attacher namespace: default roleRef: kind: ClusterRole name: external-attacher-runner apiGroup: rbac.authorization.k8s.io 1.2. csi-provisioner-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: csi-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: external-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"list\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-provisioner-role subjects: - kind: ServiceAccount name: csi-provisioner namespace: default roleRef: kind: ClusterRole name: external-provisioner-runner apiGroup: rbac.authorization.k8s.io 1.3. csi-nodeplugin-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: csi-nodeplugin --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-nodeplugin rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"update\"] - apiGroups: [\"\"] resources: [\"namespaces\"] verbs: [\"get\", \"list\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-nodeplugin subjects: - kind: ServiceAccount name: csi-nodeplugin namespace: default roleRef: kind: ClusterRole name: csi-nodeplugin apiGroup: rbac.authorization.k8s.io 2. éƒ¨ç½²CSI sidecar containers é€šè¿‡StatefulSetçš„æ–¹å¼éƒ¨ç½²external-attacherå’Œexternal-provisionerä¾›CSI CephFSä½¿ç”¨ã€‚\n$ kubectl create -f csi-cephfsplugin-attacher.yaml $ kubectl create -f csi-cephfsplugin-provisioner.yaml 2.1. csi-cephfsplugin-provisioner.yaml kind: Service apiVersion: v1 metadata: name: csi-cephfsplugin-provisioner labels: app: csi-cephfsplugin-provisioner spec: selector: app: csi-cephfsplugin-provisioner ports: - name: dummy port: 12345 --- kind: StatefulSet apiVersion: apps/v1beta1 metadata: name: csi-cephfsplugin-provisioner spec: serviceName: \"csi-cephfsplugin-provisioner\" replicas: 1 template: metadata: labels: app: csi-cephfsplugin-provisioner spec: serviceAccount: csi-provisioner containers: - name: csi-provisioner image: quay.io/k8scsi/csi-provisioner:v0.3.0 args: - \"--provisioner=csi-cephfsplugin\" - \"--csi-address=$(ADDRESS)\" - \"--v=5\" env: - name: ADDRESS value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin volumes: - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate 2.2. csi-cephfsplugin-attacher.yaml kind: Service apiVersion: v1 metadata: name: csi-cephfsplugin-attacher labels: app: csi-cephfsplugin-attacher spec: selector: app: csi-cephfsplugin-attacher ports: - name: dummy port: 12345 --- kind: StatefulSet apiVersion: apps/v1beta1 metadata: name: csi-cephfsplugin-attacher spec: serviceName: \"csi-cephfsplugin-attacher\" replicas: 1 template: metadata: labels: app: csi-cephfsplugin-attacher spec: serviceAccount: csi-attacher containers: - name: csi-cephfsplugin-attacher image: quay.io/k8scsi/csi-attacher:v0.3.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" env: - name: ADDRESS value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin volumes: - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate 3. éƒ¨ç½²CSI-CephFS-driver(plugin) csi-cephfs-plugin çš„ä½œç”¨ç±»ä¼¼nfs-clientï¼Œéƒ¨ç½²åœ¨æ‰€æœ‰nodeèŠ‚ç‚¹ä¸Šï¼Œæ‰§è¡Œcephçš„æŒ‚è½½ç­‰ç›¸å…³ä»»åŠ¡ã€‚\né€šè¿‡DaemonSetçš„æ–¹å¼éƒ¨ç½²ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªå®¹å™¨ï¼šCSI driver-registrar å’Œ CSI CephFS driverã€‚\n$ kubectl create -f csi-cephfsplugin.yaml 3.1. csi-cephfsplugin.yaml kind: DaemonSet apiVersion: apps/v1beta2 metadata: name: csi-cephfsplugin spec: selector: matchLabels: app: csi-cephfsplugin template: metadata: labels: app: csi-cephfsplugin spec: serviceAccount: csi-nodeplugin hostNetwork: true # to use e.g. Rook orchestrated cluster, and mons' FQDN is # resolved through k8s service, set dns policy to cluster first dnsPolicy: ClusterFirstWithHostNet containers: - name: driver-registrar image: quay.io/k8scsi/driver-registrar:v0.3.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" - \"--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\" env: - name: ADDRESS value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock - name: DRIVER_REG_SOCK_PATH value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock - name: KUBE_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumeMounts: - name: socket-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin - name: registration-dir mountPath: /registration - name: csi-cephfsplugin securityContext: privileged: true capabilities: add: [\"SYS_ADMIN\"] allowPrivilegeEscalation: true image: quay.io/cephcsi/cephfsplugin:v0.3.0 args : - \"--nodeid=$(NODE_ID)\" - \"--endpoint=$(CSI_ENDPOINT)\" - \"--v=5\" - \"--drivername=csi-cephfsplugin\" env: - name: NODE_ID valueFrom: fieldRef: fieldPath: spec.nodeName - name: CSI_ENDPOINT value: unix://var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: plugin-dir mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin - name: pods-mount-dir mountPath: /var/lib/kubelet/pods mountPropagation: \"Bidirectional\" - mountPath: /sys name: host-sys - name: lib-modules mountPath: /lib/modules readOnly: true - name: host-dev mountPath: /dev volumes: - name: plugin-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate - name: registration-dir hostPath: path: /var/lib/kubelet/plugins/ type: Directory - name: pods-mount-dir hostPath: path: /var/lib/kubelet/pods type: Directory - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/csi-cephfsplugin type: DirectoryOrCreate - name: host-sys hostPath: path: /sys - name: lib-modules hostPath: path: /lib/modules - name: host-dev hostPath: path: /dev 4. ç¡®è®¤éƒ¨ç½²ç»“æœ $ kubectl get all NAME READY STATUS RESTARTS AGE pod/csi-cephfsplugin-attacher-0 1/1 Running 0 26s pod/csi-cephfsplugin-provisioner-0 1/1 Running 0 25s pod/csi-cephfsplugin-rljcv 2/2 Running 0 24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/csi-cephfsplugin-attacher ClusterIP 10.104.116.218 \u003cnone\u003e 12345/TCP 27s service/csi-cephfsplugin-provisioner ClusterIP 10.101.78.75 \u003cnone\u003e 12345/TCP 26s ... å‚è€ƒæ–‡æ¡£ï¼š\nhttps://github.com/ceph/ceph-csi https://github.com/ceph/ceph-csi/blob/master/docs/deploy-cephfs.md https://github.com/ceph/ceph-csi/tree/master/deploy/cephfs/kubernetes ","categories":"","description":"","excerpt":"0. è¯´æ˜ è¦æ±‚Kubernetesçš„ç‰ˆæœ¬åœ¨1.11åŠä»¥ä¸Šï¼Œk8sé›†ç¾¤å¿…é¡»å…è®¸ç‰¹æƒPodï¼ˆprivileged podsï¼‰ï¼Œ â€¦","ref":"/kubernetes-notes/storage/csi/ceph/deploy-csi-cephfs/","tags":["CSI"],"title":"éƒ¨ç½²csi-cephfs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/disk/","tags":"","title":"ç£ç›˜"},{"body":"1. è®¡ç®—æœºè¯­è¨€æ¦‚è¿° å­¦ä¹ ä¸€é—¨è®¡ç®—æœºè¯­è¨€ï¼Œå°†è®¡ç®—æœºè¯­è¨€åˆ†ä¸ºä»¥ä¸‹å‡ å¤§éƒ¨åˆ†ï¼š\nè¯­è¨€ç‰¹ç‚¹ ç¯å¢ƒå‡†å¤‡ åŸºæœ¬è¯­æ³• æ•°æ®ç±»å‹ å˜é‡ å¸¸é‡ å¼•ç”¨ç±»å‹ æµç¨‹è¯­å¥ åˆ¤æ–­è¯­å¥ å¾ªç¯è¯­å¥ é€‰æ‹©è¯­å¥ å‡½æ•° é¢å‘å¯¹è±¡ç¼–ç¨‹ å°è£…ï¼ˆç±»ä¸æ–¹æ³•ï¼‰ ç»§æ‰¿ å¤šæ€ï¼ˆæ¥å£ï¼‰ å¹¶å‘ç¼–ç¨‹ ç‰¹æ®Šå±æ€§ åŒ…ç®¡ç† æ ‡å‡†åº“ 2. æ€ç»´å¯¼å›¾ ","categories":"","description":"","excerpt":"1. è®¡ç®—æœºè¯­è¨€æ¦‚è¿° å­¦ä¹ ä¸€é—¨è®¡ç®—æœºè¯­è¨€ï¼Œå°†è®¡ç®—æœºè¯­è¨€åˆ†ä¸ºä»¥ä¸‹å‡ å¤§éƒ¨åˆ†ï¼š\nè¯­è¨€ç‰¹ç‚¹ ç¯å¢ƒå‡†å¤‡ åŸºæœ¬è¯­æ³• æ•°æ®ç±»å‹ å˜é‡ å¸¸é‡ å¼•ç”¨ç±»å‹ æµç¨‹ â€¦","ref":"/golang-notes/summary/language/","tags":["Golang"],"title":"è®¡ç®—æœºè¯­è¨€æ¦‚è¿°"},{"body":" æœ¬æ–‡ä»‹ç»é€šè¿‡podæŒ‡å®š ImagePullSecretsæ¥æ‹‰å–ç§æœ‰é•œåƒä»“åº“çš„é•œåƒ\n1. åˆ›å»ºsecret secretæ˜¯namespaceçº§åˆ«çš„ï¼Œåˆ›å»ºæ—¶å€™éœ€è¦æŒ‡å®šnamespaceã€‚\nkubectl create secret docker-registry \u003cname\u003e --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD -n \u003cNAMESPACE\u003e 2. æ·»åŠ ImagePullSecretsåˆ°serviceAccount å¯ä»¥é€šè¿‡å°†ImagePullSecretsåˆ°serviceAccountçš„æ–¹å¼æ¥è‡ªåŠ¨ç»™podæ·»åŠ imagePullSecretså‚æ•°å€¼ã€‚\nserviceAccountåŒæ ·æ˜¯namespaceçº§åˆ«ï¼Œåªå¯¹è¯¥namespaceç”Ÿæ•ˆã€‚\n#kubectl get secrets -n dev NAME TYPE DATA AGE docker.xxxx.com kubernetes.io/dockerconfigjson 1 6h23m å°†ImagePullSecretsæ·»åŠ åˆ°serviceAccountå¯¹è±¡ä¸­ã€‚\né»˜è®¤serviceAccountå¯¹è±¡å¦‚ä¸‹\n#kubectl get serviceaccount default -n dev -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-02-27T03:30:38Z\" name: default namespace: dev resourceVersion: \"11651567\" selfLink: /api/v1/namespaces/dev/serviceaccounts/default uid: 85bcdd31-5911-11ea-9429-6c92bf3b7c33 secrets: - name: default-token-s7wfn ç¼–è¾‘æˆ–ä¿®æ”¹serviceAccountå†…å®¹ï¼Œå¢åŠ imagePullSecretså­—æ®µã€‚\nimagePullSecrets: - name: docker.xxxx.com kubectl edit serviceaccount default -n dev\nä¿®æ”¹åå†…å®¹ä¸ºï¼š\napiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-02-27T03:30:38Z\" name: default namespace: dev resourceVersion: \"11651567\" selfLink: /api/v1/namespaces/dev/serviceaccounts/default uid: 85bcdd31-5911-11ea-9429-6c92bf3b7c33 secrets: - name: default-token-s7wfn imagePullSecrets: - name: docker.xxxx.com å…¶ä¸­imagePullSecretså­—æ®µæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥é…ç½®å¤šä¸ªé•œåƒä»“åº“çš„è´¦å·å¯†ç ã€‚\nä¾‹å¦‚ï¼š\napiVersion: v1 kind: ServiceAccount ... imagePullSecrets: - name: docker.xxxx.com - name: docker.test.xxxx.com 3. åˆ›å»ºå¸¦æœ‰imagePullSecretsçš„pod å¦‚æœå·²ç»æ‰§è¡Œäº†ç¬¬äºŒæ­¥æ“ä½œï¼Œæ·»åŠ ImagePullSecretsåˆ°serviceAccountï¼Œåˆ™æ— éœ€åœ¨podä¸­æŒ‡å®šimagePullSecretså‚æ•°ï¼Œé»˜è®¤ä¼šè‡ªåŠ¨æ·»åŠ ã€‚\nå¦‚æœæ²¡æœ‰æ·»åŠ ImagePullSecretsåˆ°serviceAccountï¼Œåˆ™åœ¨podä¸­æŒ‡å®šimagePullSecretså‚æ•°å¼•ç”¨åˆ›å»ºçš„é•œåƒä»“åº“çš„secretã€‚\nspec: imagePullSecrets: - name: docker.xxxx.com 4. è¯´æ˜ ç”±äºsecretå’Œserviceaccountå¯¹è±¡æ˜¯å¯¹namespaceçº§åˆ«ç”Ÿæ•ˆï¼Œå› æ­¤ä¸åŒçš„namespaceéœ€è¦å†æ¬¡åˆ›å»ºå’Œæ›´æ–°è¿™ä¸¤ä¸ªå¯¹è±¡ã€‚è¯¥åœºæ™¯é€‚åˆä¸åŒç”¨æˆ·å…·æœ‰ç‹¬ç«‹çš„é•œåƒä»“åº“çš„å¯†ç ï¼Œå¯ä»¥é€šè¿‡è¯¥æ–¹å¼åˆ›å»ºä¸åŒçš„é•œåƒå¯†ç ä½¿ç”¨çš„secretæ¥æ‹‰å–ä¸åŒçš„é•œåƒéƒ¨ç½²ã€‚\nå‚è€ƒï¼š\nhttps://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account ","categories":"","description":"","excerpt":" æœ¬æ–‡ä»‹ç»é€šè¿‡podæŒ‡å®š ImagePullSecretsæ¥æ‹‰å–ç§æœ‰é•œåƒä»“åº“çš„é•œåƒ\n1. åˆ›å»ºsecret secretæ˜¯namespace â€¦","ref":"/kubernetes-notes/operation/registry/imagepullsecrets/","tags":["Kubernetes"],"title":"æ‹‰å–ç§æœ‰é•œåƒ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/principle/flow/","tags":"","title":"æµç¨‹å›¾"},{"body":"1. å†…å­˜åˆ†é…å™¨çš„é—®é¢˜ å½“ç»™ä¸åŒå¤§å°çš„å˜é‡åˆ†é…è¿ç»­åœ°å€çš„å†…å­˜çš„æ—¶å€™ï¼Œå¯èƒ½å› ä¸ºéƒ¨åˆ†å˜é‡å†…å­˜çš„å›æ”¶å¯¼è‡´åœ¨åˆ†é…æ–°çš„å†…å­˜éœ€æ±‚æ—¶æ— æ³•åˆ©ç”¨è¢«å›æ”¶çš„å†…å­˜åœ°å€ï¼Œå› æ­¤å†…å­˜ç®¡ç†ä¸å½“ï¼Œå®¹æ˜“å¯¼è‡´å†…å­˜çš„ç¢ç‰‡ã€‚\n2.åŸºæœ¬ç­–ç•¥ï¼š æ¯æ¬¡ä»æ“ä½œç³»ç»Ÿç”³è¯·â¼€â¼¤å—å†…å­˜ï¼ˆâ½å¦‚ 1MBï¼‰ï¼Œä»¥å‡å°‘ç³»ç»Ÿè°ƒâ½¤ã€‚ å°†ç”³è¯·åˆ°çš„â¼¤å—å†…å­˜æŒ‰ç…§ç‰¹å®šâ¼¤â¼©é¢„å…ˆåˆ‡åˆ†æˆâ¼©å—ï¼Œæ„æˆé“¾è¡¨ã€‚ ä¸ºå¯¹è±¡åˆ†é…å†…å­˜æ—¶ï¼Œåªéœ€ä»â¼¤â¼©åˆé€‚çš„é“¾è¡¨æå–â¼€ä¸ªâ¼©å—å³å¯ã€‚ å›æ”¶å¯¹è±¡å†…å­˜æ—¶ï¼Œå°†è¯¥â¼©å—å†…å­˜é‡æ–°å½’è¿˜åˆ°åŸé“¾è¡¨ï¼Œä»¥ä¾¿å¤â½¤ã€‚ å¦‚é—²ç½®å†…å­˜è¿‡å¤šï¼Œåˆ™å°è¯•å½’è¿˜éƒ¨åˆ†å†…å­˜ç»™æ“ä½œç³»ç»Ÿï¼Œé™ä½æ•´ä½“å¼€é”€ 3.å†…å­˜åˆ†é…çš„æœ¬è´¨ é’ˆå¯¹ä¸åŒå¤§å°çš„å¯¹è±¡ï¼Œåœ¨ä¸åŒçš„ cache å±‚ä¸­ï¼Œä½¿ç”¨ä¸åŒçš„å†…å­˜ç»“æ„ï¼›å°†ä»ç³»ç»Ÿä¸­è·å¾—çš„ä¸€å—è¿ç»­å†…å­˜åˆ†å‰²æˆå¤šå±‚æ¬¡çš„ cacheï¼Œä»¥å‡å°‘é”çš„ä½¿ç”¨ä»¥æé«˜å†…å­˜åˆ†é…æ•ˆç‡ï¼›ç”³è¯·ä¸åŒç±»å¤§å°çš„å†…å­˜å—æ¥å‡å°‘å†…å­˜ç¢ç‰‡ï¼ŒåŒæ—¶åŠ é€Ÿå†…å­˜é‡Šæ”¾åçš„åƒåœ¾å›æ”¶ã€‚\ngoçš„å†…å­˜åˆ†é…å™¨å°†å†…å­˜é¡µåˆ†æˆ67ä¸ªä¸åŒå¤§å°è§„æ ¼ï¼ˆsize classï¼‰çš„å—ï¼Œæœ€å°ä¸º8KBï¼Œæœ€å¤§ä¸º32768KBã€‚\nå†…å­˜å—çš„åˆ†ç±»ï¼š\nspan:ç”±å¤šä¸ªåœ°å€è¿ç»­çš„é¡µï¼ˆpageï¼‰ç»„æˆçš„å¤§å—å†…å­˜ã€‚é¢å‘å†…éƒ¨ç®¡ç†ã€‚ objectï¼šå°†spanæŒ‰ç…§ç‰¹å®šçš„å¤§å°åˆ‡åˆ†æˆå¤šä¸ªå°å—ï¼Œæ¯ä¸ªå°å—å¯ä»¥å­˜å‚¨ä¸€ä¸ªå¯¹è±¡ã€‚é¢å‘å¯¹è±¡åˆ†é…ã€‚ å†…å­˜åˆ†é…å™¨çš„ä¸‰ä¸ªæ•°æ®ç»“æ„ï¼ˆç”³è¯·é€çº§å‘ä¸Šï¼‰ï¼š\nmcacheï¼šgoroutine cacheï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ æœ¬åœ° cacheã€‚ä¸æ¶‰åŠé”ç«äº‰ã€‚ mcentralï¼šå…¨å±€cacheï¼Œmcache ä¸å¤Ÿç”¨çš„æ—¶å€™å‘ mcentral ç”³è¯·ã€‚æ¶‰åŠé”ç«äº‰ã€‚ mheapï¼šå½“mcentral ä¹Ÿä¸å¤Ÿç”¨çš„æ—¶å€™ï¼Œé€šè¿‡ mheap å‘æ“ä½œç³»ç»Ÿç”³è¯·ã€‚ 4.å†…å­˜åˆ†é…çš„æµç¨‹ object size \u003c 16Kï¼Œä½¿ç”¨ mcache çš„å°å¯¹è±¡åˆ†é…å™¨ tiny ç›´æ¥åˆ†é…ã€‚ object size \u003e 32Kï¼Œåˆ™ä½¿ç”¨ mheap ç›´æ¥åˆ†é…ã€‚ object size \u003e 16K \u0026\u0026 object size \u003c 32Kï¼Œå…ˆä½¿ç”¨ mcache ä¸­å¯¹åº”çš„ size class åˆ†é…ã€‚ å¦‚æœ mcache å¯¹åº”çš„ size class çš„ span å·²ç»æ²¡æœ‰å¯ç”¨çš„å—ï¼Œåˆ™å‘ mcentral è¯·æ±‚ã€‚ å¦‚æœ mcentral ä¹Ÿæ²¡æœ‰å¯ç”¨çš„å—ï¼Œåˆ™å‘ mheapç”³è¯·ï¼Œå¹¶åˆ‡åˆ†ã€‚ å¦‚æœ mheap ä¹Ÿæ²¡æœ‰åˆé€‚çš„ spanï¼Œåˆ™æƒ³æ“ä½œç³»ç»Ÿç”³è¯·ã€‚ 5.å†…å­˜å›æ”¶çš„æµç¨‹ mcache å½’è¿˜å†…å­˜åˆ†ä¸¤éƒ¨åˆ†ï¼šå½’è¿˜mcentralå†…å­˜ï¼Œå¯èƒ½æ¶‰åŠé”ç«äº‰ï¼›é™¤æ­¤ä¹‹å¤–ï¼Œå½’è¿˜åˆ°mheapï¼Œç›´æ¥æ’å…¥é“¾è¡¨å¤´ã€‚ mcentral å½’è¿˜mheapã€‚ mheap å®šæ—¶å½’è¿˜ç³»ç»Ÿå†…å­˜ã€‚ 6.tcmalloc(thread-caching mallo) æ˜¯googleæ¨å‡ºçš„ä¸€ç§å†…å­˜åˆ†é…å™¨ã€‚\nå…·ä½“ç­–ç•¥ï¼šå…¨å±€ç¼“å­˜å †å’Œè¿›ç¨‹çš„ç§æœ‰ç¼“å­˜ã€‚\n1.å¯¹äºä¸€äº›å°å®¹é‡çš„å†…å­˜ç”³è¯·è¯•ç”¨è¿›ç¨‹çš„ç§æœ‰ç¼“å­˜ï¼Œç§æœ‰ç¼“å­˜ä¸è¶³çš„æ—¶å€™å¯ä»¥å†ä»å…¨å±€ç¼“å­˜ç”³è¯·ä¸€éƒ¨åˆ†ä½œä¸ºç§æœ‰ç¼“å­˜ã€‚\n2.å¯¹äºå¤§å®¹é‡çš„å†…å­˜ç”³è¯·åˆ™éœ€è¦ä»å…¨å±€ç¼“å­˜ä¸­è¿›è¡Œç”³è¯·ã€‚è€Œå¤§å°å®¹é‡çš„è¾¹ç•Œå°±æ˜¯32kã€‚ç¼“å­˜çš„ç»„ç»‡æ–¹å¼æ˜¯ä¸€ä¸ªå•é“¾è¡¨æ•°ç»„ï¼Œæ•°ç»„çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå•é“¾è¡¨ï¼Œé“¾è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ å…·æœ‰ç›¸åŒçš„å¤§å°ã€‚\ngolangè¯­è¨€ä¸­MHeapå°±æ˜¯å…¨å±€ç¼“å­˜å †ï¼ŒMCacheä½œä¸ºçº¿ç¨‹ç§æœ‰ç¼“å­˜ã€‚\nå†…å­˜æ± å°±æ˜¯åˆ©ç”¨MHeapå®ç°ï¼Œå¤§å°åˆ‡åˆ†åˆ™æ˜¯åœ¨ç”³è¯·å†…å­˜çš„æ—¶å€™å°±åšäº†ï¼ŒåŒæ—¶MCacheåˆ†é…å†…å­˜æ—¶ï¼Œå¯ä»¥ç”¨MCentralå»å–å¯¹åº”çš„sizeClassï¼Œå¤šçº¿ç¨‹ç®¡ç†æ–¹é¢åˆ™æ˜¯é€šè¿‡MCacheå»å®ç°ã€‚\næ€»ç»“ 1.MHeapæ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œè´Ÿè´£å‘ç³»ç»Ÿç”³è¯·å†…å­˜ï¼Œmallocinit()å‡½æ•°è¿›è¡Œåˆå§‹åŒ–ã€‚å¦‚æœåˆ†é…å†…å­˜å¯¹è±¡å¤§äº32Kç›´æ¥å‘MHeapç”³è¯·ã€‚\n2.MCacheçº¿ç¨‹çº§åˆ«ç®¡ç†å†…å­˜æ± ï¼Œå…³è”ç»“æ„ä½“Pï¼Œä¸»è¦æ˜¯è´Ÿè´£çº¿ç¨‹å†…éƒ¨å†…å­˜ç”³è¯·ã€‚\n3.MCentralè¿æ¥MHeapä¸MCacheçš„ï¼ŒMCacheå†…å­˜ä¸å¤Ÿåˆ™å‘MCentralç”³è¯·ï¼ŒMCentralä¸å¤Ÿæ—¶å‘MHeapç”³è¯·å†…å­˜ã€‚\n","categories":"","description":"","excerpt":"1. å†…å­˜åˆ†é…å™¨çš„é—®é¢˜ å½“ç»™ä¸åŒå¤§å°çš„å˜é‡åˆ†é…è¿ç»­åœ°å€çš„å†…å­˜çš„æ—¶å€™ï¼Œå¯èƒ½å› ä¸ºéƒ¨åˆ†å˜é‡å†…å­˜çš„å›æ”¶å¯¼è‡´åœ¨åˆ†é…æ–°çš„å†…å­˜éœ€æ±‚æ—¶æ— æ³•åˆ©ç”¨è¢«å›æ”¶çš„å†…å­˜åœ° â€¦","ref":"/golang-notes/principle/memory-allocation/","tags":["Golang"],"title":"å†…å­˜åˆ†é…"},{"body":"1. ç¯å¢ƒå‡†å¤‡ 1.1. éƒ¨ç½²æœºå™¨ ä»¥ä¸‹æœºå™¨ä¸ºè™šæ‹Ÿæœº\næœºå™¨IP ä¸»æœºå è§’è‰² ç³»ç»Ÿç‰ˆæœ¬ å¤‡æ³¨ 172.16.94.140 kube-master-0 k8s master Centos 4.17.14 å†…å­˜ï¼š3G 172.16.94.141 kube-node-41 k8s node Centos 4.17.14 å†…å­˜ï¼š3G 172.16.94.142 kube-node-42 k8s node Centos 4.17.14 å†…å­˜ï¼š3G 172.16.94.135 éƒ¨ç½²ç®¡ç†æœº - 1.2. é…ç½®ç®¡ç†æœº ç®¡ç†æœºä¸»è¦ç”¨æ¥éƒ¨ç½²k8sé›†ç¾¤ï¼Œéœ€è¦å®‰è£…ä»¥ä¸‹ç‰ˆæœ¬çš„è½¯ä»¶ï¼Œå…·ä½“å¯å‚è€ƒï¼š\nhttps://github.com/kubernetes-incubator/kubespray#requirements\nhttps://github.com/kubernetes-incubator/kubespray/blob/master/requirements.txt\nansible\u003e=2.4.0 jinja2\u003e=2.9.6 netaddr pbr\u003e=1.6 ansible-modules-hashivault\u003e=3.9.4 hvac 1ã€å®‰è£…åŠé…ç½®ansible\nå‚è€ƒansibleçš„ä½¿ç”¨ã€‚ ç»™éƒ¨ç½²æœºå™¨é…ç½®SSHçš„å…å¯†ç™»å½•æƒé™ï¼Œå…·ä½“å‚è€ƒsshå…å¯†ç™»å½•ã€‚ 2ã€å®‰è£…python-netaddr\n# å®‰è£…pip yum -y install epel-release yum -y install python-pip # å®‰è£…python-netaddr pip install netaddr 3ã€å‡çº§Jinja\n# Jinja 2.9 (or newer) pip install --upgrade jinja2 1.3. é…ç½®éƒ¨ç½²æœºå™¨ éƒ¨ç½²æœºå™¨å³ç”¨æ¥è¿è¡Œk8sé›†ç¾¤çš„æœºå™¨ï¼ŒåŒ…æ‹¬Masterå’ŒNodeã€‚\n1ã€ç¡®è®¤ç³»ç»Ÿç‰ˆæœ¬\næœ¬æ–‡é‡‡ç”¨centos7çš„ç³»ç»Ÿï¼Œå»ºè®®å°†ç³»ç»Ÿå†…æ ¸å‡çº§åˆ°4.x.xä»¥ä¸Šã€‚\n2ã€å…³é—­é˜²ç«å¢™\nsystemctl stop firewalld systemctl disable firewalld iptables -F 3ã€å…³é—­swap\nKubespary v2.5.0çš„ç‰ˆæœ¬éœ€è¦å…³é—­swapï¼Œå…·ä½“å‚è€ƒ\nhttps://github.com/kubernetes-incubator/kubespray/blob/02cd5418c22d51e40261775908d55bc562206023/roles/kubernetes/preinstall/tasks/verify-settings.yml#L75 - name: Stop if swap enabled assert: that: ansible_swaptotal_mb == 0 when: kubelet_fail_swap_on|default(true) ignore_errors: \"{{ ignore_assert_errors }}\" V2.6.0 ç‰ˆæœ¬å»é™¤äº†swapçš„æ£€æŸ¥ï¼Œå…·ä½“å‚è€ƒï¼š\nhttps://github.com/kubernetes-incubator/kubespray/commit/b902602d161f8c147f3d155d2ac5360244577127#diff-b92ae64dd18d34a96fbeb7f7e48a6a9b æ‰§è¡Œå…³é—­swapå‘½ä»¤swapoff -aã€‚\n[root@master ~]#swapoff -a [root@master ~]# [root@master ~]# free -m total used free shared buff/cache available Mem: 976 366 135 6 474 393 Swap: 0 0 0 # swap ä¸€æ ä¸º0ï¼Œè¡¨ç¤ºå·²ç»å…³é—­äº†swap 4ã€ç¡®è®¤éƒ¨ç½²æœºå™¨å†…å­˜\nç”±äºæœ¬æ–‡é‡‡ç”¨è™šæ‹Ÿæœºéƒ¨ç½²ï¼Œå†…å­˜å¯èƒ½å­˜åœ¨ä¸è¶³çš„é—®é¢˜ï¼Œå› æ­¤å°†è™šæ‹Ÿæœºå†…å­˜è°ƒæ•´ä¸º3Gæˆ–ä»¥ä¸Šï¼›å¦‚æœæ˜¯ç‰©ç†æœºä¸€èˆ¬ä¸ä¼šæœ‰å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚å…·ä½“å‚è€ƒï¼š\nhttps://github.com/kubernetes-incubator/kubespray/blob/95f1e4634a1c50fa77312d058a2b713353f4307e/roles/kubernetes/preinstall/tasks/verify-settings.yml#L52 - name: Stop if memory is too small for masters assert: that: ansible_memtotal_mb \u003e= 1500 ignore_errors: \"{{ ignore_assert_errors }}\" when: inventory_hostname in groups['kube-master'] - name: Stop if memory is too small for nodes assert: that: ansible_memtotal_mb \u003e= 1024 ignore_errors: \"{{ ignore_assert_errors }}\" when: inventory_hostname in groups['kube-node'] 1.4. æ¶‰åŠé•œåƒ Dockerç‰ˆæœ¬ä¸º17.03.2-ceã€‚\n1ã€MasterèŠ‚ç‚¹\né•œåƒ ç‰ˆæœ¬ å¤§å° é•œåƒID å¤‡æ³¨ gcr.io/google-containers/hyperkube v1.9.5 620 MB a7e7fdbc5fee k8s quay.io/coreos/etcd v3.2.4 35.7 MB 498ffffcfd05 gcr.io/google_containers/pause-amd64 3.0 747 kB 99e59f495ffa quay.io/calico/node v2.6.8 282 MB e96a297310fd calico quay.io/calico/cni v1.11.4 70.8 MB 4c4cb67d7a88 calico quay.io/calico/ctl v1.6.3 44.4 MB 46d3aace8bc6 calico 2ã€NodeèŠ‚ç‚¹\né•œåƒ ç‰ˆæœ¬ å¤§å° é•œåƒID å¤‡æ³¨ gcr.io/google-containers/hyperkube v1.9.5 620 MB a7e7fdbc5fee k8s gcr.io/google_containers/pause-amd64 3.0 747 kB 99e59f495ffa quay.io/calico/node v2.6.8 282 MB e96a297310fd calico quay.io/calico/cni v1.11.4 70.8 MB 4c4cb67d7a88 calico quay.io/calico/ctl v1.6.3 44.4 MB 46d3aace8bc6 calico gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64 1.14.8 40.9 MB c2ce1ffb51ed dns gcr.io/google_containers/k8s-dns-sidecar-amd64 1.14.8 42.2 MB 6f7f2dc7fab5 dns gcr.io/google_containers/k8s-dns-kube-dns-amd64 1.14.8 50.5 MB 80cc5ea4b547 dns gcr.io/google_containers/cluster-proportional-autoscaler-amd64 1.1.2 50.5 MB 78cf3f492e6b gcr.io/google_containers/kubernetes-dashboard-amd64 v1.8.3 102 MB 0c60bcf89900 dashboard nginx 1.13 109 MB ae513a47849c - 3ã€è¯´æ˜\né•œåƒè¢«å¢™å¹¶ä¸”å…¨éƒ¨é•œåƒä¸‹è½½éœ€è¦è¾ƒå¤šæ—¶é—´ï¼Œå»ºè®®æå‰ä¸‹è½½åˆ°éƒ¨ç½²æœºå™¨ä¸Šã€‚ hyperkubeé•œåƒä¸»è¦ç”¨æ¥è¿è¡Œk8sæ ¸å¿ƒç»„ä»¶ï¼ˆä¾‹å¦‚kube-apiserverç­‰ï¼‰ã€‚ æ­¤å¤„ä½¿ç”¨çš„ç½‘ç»œç»„ä»¶ä¸ºcalicoã€‚ 2. éƒ¨ç½²é›†ç¾¤ 2.1. ä¸‹è½½kubesparyçš„æºç  git clone https://github.com/kubernetes-incubator/kubespray.git 2.2. ç¼–è¾‘é…ç½®æ–‡ä»¶ 2.2.1. hosts.ini hosts.iniä¸»è¦ä¸ºéƒ¨ç½²èŠ‚ç‚¹æœºå™¨ä¿¡æ¯çš„æ–‡ä»¶ï¼Œè·¯å¾„ä¸ºï¼škubespray/inventory/sample/hosts.iniã€‚\ncd kubespray # å¤åˆ¶ä¸€ä»½é…ç½®è¿›è¡Œä¿®æ”¹ cp -rfp inventory/sample inventory/k8s vi inventory/k8s/hosts.ini ä¾‹å¦‚ï¼š\nhosts.iniæ–‡ä»¶å¯ä»¥å¡«å†™éƒ¨ç½²æœºå™¨çš„ç™»å½•å¯†ç ï¼Œä¹Ÿå¯ä»¥ä¸å¡«å¯†ç è€Œè®¾ç½®sshçš„å…å¯†ç™»å½•ã€‚\n# Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # ä¸»æœºå sshç™»é™†IP sshç”¨æˆ·å sshç™»é™†å¯†ç  æœºå™¨IP å­ç½‘æ©ç  kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 [etcd] kube-master-0 [kube-node] kube-node-41 kube-node-42 [k8s-cluster:children] kube-node kube-master [calico-rr] 2.2.2. k8s-cluster.yml k8s-cluster.ymlä¸»è¦ä¸ºk8sé›†ç¾¤çš„é…ç½®æ–‡ä»¶ï¼Œè·¯å¾„ä¸ºï¼škubespray/inventory/k8s/group_vars/k8s-cluster.ymlã€‚è¯¥æ–‡ä»¶å¯ä»¥ä¿®æ”¹å®‰è£…çš„k8sé›†ç¾¤çš„ç‰ˆæœ¬ï¼Œå‚æ•°ä¸ºï¼škube_version: v1.9.5ã€‚å…·ä½“å¯å‚è€ƒï¼š\nhttps://github.com/kubernetes-incubator/kubespray/blob/master/inventory/sample/group_vars/k8s-cluster.yml#L22 2.3. æ‰§è¡Œéƒ¨ç½²æ“ä½œ æ¶‰åŠæ–‡ä»¶ä¸ºcluster.ymlã€‚\n# è¿›å…¥ä¸»ç›®å½• cd kubespray # æ‰§è¡Œéƒ¨ç½²å‘½ä»¤ ansible-playbook -i inventory/k8s/hosts.ini cluster.yml -b -vvv -vvv å‚æ•°è¡¨ç¤ºè¾“å‡ºè¿è¡Œæ—¥å¿—\nå¦‚æœéœ€è¦é‡ç½®å¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\næ¶‰åŠæ–‡ä»¶ä¸ºreset.ymlã€‚\nansible-playbook -i inventory/k8s/hosts.ini reset.yml -b -vvv 3. ç¡®è®¤éƒ¨ç½²ç»“æœ 3.1. ansibleçš„éƒ¨ç½²ç»“æœ ansibleå‘½ä»¤æ‰§è¡Œå®Œï¼Œå‡ºç°ä»¥ä¸‹æ—¥å¿—ï¼Œåˆ™è¯´æ˜éƒ¨ç½²æˆåŠŸï¼Œå¦åˆ™æ ¹æ®æŠ¥é”™å†…å®¹è¿›è¡Œä¿®æ”¹ã€‚\nPLAY RECAP ***************************************************************************** kube-master-0 : ok=309 changed=30 unreachable=0 failed=0 kube-node-41 : ok=203 changed=8 unreachable=0 failed=0 kube-node-42 : ok=203 changed=8 unreachable=0 failed=0 localhost : ok=2 changed=0 unreachable=0 failed=0 ä»¥ä¸‹ä¸ºéƒ¨åˆ†éƒ¨ç½²æ‰§è¡Œæ—¥å¿—ï¼š\nkubernetes/preinstall : Update package management cache (YUM) --------------------23.96s /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/main.yml:121 kubernetes/master : Master | wait for the apiserver to be running ----------------23.44s /root/gopath/src/kubespray/roles/kubernetes/master/handlers/main.yml:79 kubernetes/preinstall : Install packages requirements ----------------------------20.20s /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/main.yml:203 kubernetes/secrets : Check certs | check if a cert already exists on node --------13.94s /root/gopath/src/kubespray/roles/kubernetes/secrets/tasks/check-certs.yml:17 gather facts from all instances --------------------------------------------------9.98s /root/gopath/src/kubespray/cluster.yml:25 kubernetes/node : install | Compare host kubelet with hyperkube container --------9.66s /root/gopath/src/kubespray/roles/kubernetes/node/tasks/install_host.yml:2 kubernetes-apps/ansible : Kubernetes Apps | Start Resources -----------------------9.27s /root/gopath/src/kubespray/roles/kubernetes-apps/ansible/tasks/main.yml:37 kubernetes-apps/ansible : Kubernetes Apps | Lay Down KubeDNS Template ------------8.47s /root/gopath/src/kubespray/roles/kubernetes-apps/ansible/tasks/kubedns.yml:3 download : Sync container ---------------------------------------------------------8.23s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 kubernetes-apps/network_plugin/calico : Start Calico resources --------------------7.82s /root/gopath/src/kubespray/roles/kubernetes-apps/network_plugin/calico/tasks/main.yml:2 download : Download items ---------------------------------------------------------7.67s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------7.48s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Sync container ---------------------------------------------------------7.35s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 download : Download items ---------------------------------------------------------7.16s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 network_plugin/calico : Calico | Copy cni plugins from calico/cni container -------7.10s /root/gopath/src/kubespray/roles/network_plugin/calico/tasks/main.yml:62 download : Download items ---------------------------------------------------------7.04s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------7.01s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Sync container ---------------------------------------------------------7.00s /root/gopath/src/kubespray/roles/download/tasks/main.yml:15 download : Download items ---------------------------------------------------------6.98s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 download : Download items ---------------------------------------------------------6.79s /root/gopath/src/kubespray/roles/download/tasks/main.yml:6 3.2. k8sé›†ç¾¤è¿è¡Œç»“æœ 1ã€k8sç»„ä»¶ä¿¡æ¯\n# kubectl get all --namespace=kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds/calico-node 3 3 3 3 3 \u003cnone\u003e 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 2 2 2 2 2h deploy/kubedns-autoscaler 1 1 1 1 2h deploy/kubernetes-dashboard 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/kube-dns-79d99cdcd5 2 2 2 2h rs/kubedns-autoscaler-5564b5585f 1 1 1 2h rs/kubernetes-dashboard-69cb58d748 1 1 1 2h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds/calico-node 3 3 3 3 3 \u003cnone\u003e 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 2 2 2 2 2h deploy/kubedns-autoscaler 1 1 1 1 2h deploy/kubernetes-dashboard 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/kube-dns-79d99cdcd5 2 2 2 2h rs/kubedns-autoscaler-5564b5585f 1 1 1 2h rs/kubernetes-dashboard-69cb58d748 1 1 1 2h NAME READY STATUS RESTARTS AGE po/calico-node-22vsg 1/1 Running 0 2h po/calico-node-t7zgw 1/1 Running 0 2h po/calico-node-zqnx8 1/1 Running 0 2h po/kube-apiserver-kube-master-0 1/1 Running 0 22h po/kube-controller-manager-kube-master-0 1/1 Running 0 2h po/kube-dns-79d99cdcd5-f2t6t 3/3 Running 0 2h po/kube-dns-79d99cdcd5-gw944 3/3 Running 0 2h po/kube-proxy-kube-master-0 1/1 Running 2 22h po/kube-proxy-kube-node-41 1/1 Running 3 22h po/kube-proxy-kube-node-42 1/1 Running 3 22h po/kube-scheduler-kube-master-0 1/1 Running 0 2h po/kubedns-autoscaler-5564b5585f-lt9bb 1/1 Running 0 2h po/kubernetes-dashboard-69cb58d748-wmb9x 1/1 Running 0 2h po/nginx-proxy-kube-node-41 1/1 Running 3 22h po/nginx-proxy-kube-node-42 1/1 Running 3 22h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.233.0.3 \u003cnone\u003e 53/UDP,53/TCP 2h svc/kubernetes-dashboard ClusterIP 10.233.27.24 \u003cnone\u003e 443/TCP 2h 2ã€k8sèŠ‚ç‚¹ä¿¡æ¯\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kube-master-0 Ready master 22h v1.9.5 kube-node-41 Ready node 22h v1.9.5 kube-node-42 Ready node 22h v1.9.5 3ã€ç»„ä»¶å¥åº·ä¿¡æ¯\n# kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} 4. k8sé›†ç¾¤æ‰©å®¹èŠ‚ç‚¹ 4.1. ä¿®æ”¹hosts.iniæ–‡ä»¶ å¦‚æœéœ€è¦æ‰©å®¹NodeèŠ‚ç‚¹ï¼Œåˆ™ä¿®æ”¹hosts.iniæ–‡ä»¶ï¼Œå¢åŠ æ–°å¢çš„æœºå™¨ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œè¦å¢åŠ èŠ‚ç‚¹æœºå™¨kube-node-43ï¼ˆIPä¸º172.16.94.143ï¼‰ï¼Œä¿®æ”¹åçš„æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š\n# Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # ä¸»æœºå sshç™»é™†IP sshç”¨æˆ·å sshç™»é™†å¯†ç  æœºå™¨IP å­ç½‘æ©ç  kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 kube-node-43 ansible_ssh_host=172.16.94.143 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.143 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 [etcd] kube-master-0 [kube-node] kube-node-41 kube-node-42 kube-node-43 [k8s-cluster:children] kube-node kube-master [calico-rr] 4.2. æ‰§è¡Œæ‰©å®¹å‘½ä»¤ æ¶‰åŠæ–‡ä»¶ä¸ºscale.ymlã€‚\n# è¿›å…¥ä¸»ç›®å½• cd kubespray # æ‰§è¡Œéƒ¨ç½²å‘½ä»¤ ansible-playbook -i inventory/k8s/hosts.ini scale.yml -b -vvv 4.3. æ£€æŸ¥æ‰©å®¹ç»“æœ 1ã€ansibleçš„æ‰§è¡Œç»“æœ\nPLAY RECAP *************************************** kube-node-41 : ok=228 changed=11 unreachable=0 failed=0 kube-node-42 : ok=197 changed=6 unreachable=0 failed=0 kube-node-43 : ok=227 changed=69 unreachable=0 failed=0 # æ–°å¢NodeèŠ‚ç‚¹ localhost : ok=2 changed=0 unreachable=0 failed=0 2ã€k8sçš„èŠ‚ç‚¹ä¿¡æ¯\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kube-master-0 Ready master 1d v1.9.5 kube-node-41 Ready node 1d v1.9.5 kube-node-42 Ready node 1d v1.9.5 kube-node-43 Ready node 1m v1.9.5 #è¯¥èŠ‚ç‚¹ä¸ºæ–°å¢NodeèŠ‚ç‚¹ å¯ä»¥çœ‹åˆ°æ–°å¢çš„kube-node-43èŠ‚ç‚¹å·²ç»æ‰©å®¹å®Œæˆã€‚\n3ã€k8sç»„ä»¶ä¿¡æ¯\n# kubectl get po --namespace=kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE calico-node-22vsg 1/1 Running 0 10h 172.16.94.140 kube-master-0 calico-node-8fz9x 1/1 Running 2 27m 172.16.94.143 kube-node-43 calico-node-t7zgw 1/1 Running 0 10h 172.16.94.142 kube-node-42 calico-node-zqnx8 1/1 Running 0 10h 172.16.94.141 kube-node-41 kube-apiserver-kube-master-0 1/1 Running 0 1d 172.16.94.140 kube-master-0 kube-controller-manager-kube-master-0 1/1 Running 0 10h 172.16.94.140 kube-master-0 kube-dns-79d99cdcd5-f2t6t 3/3 Running 0 10h 10.233.100.194 kube-node-41 kube-dns-79d99cdcd5-gw944 3/3 Running 0 10h 10.233.107.1 kube-node-42 kube-proxy-kube-master-0 1/1 Running 2 1d 172.16.94.140 kube-master-0 kube-proxy-kube-node-41 1/1 Running 3 1d 172.16.94.141 kube-node-41 kube-proxy-kube-node-42 1/1 Running 3 1d 172.16.94.142 kube-node-42 kube-proxy-kube-node-43 1/1 Running 0 26m 172.16.94.143 kube-node-43 kube-scheduler-kube-master-0 1/1 Running 0 10h 172.16.94.140 kube-master-0 kubedns-autoscaler-5564b5585f-lt9bb 1/1 Running 0 10h 10.233.100.193 kube-node-41 kubernetes-dashboard-69cb58d748-wmb9x 1/1 Running 0 10h 10.233.107.2 kube-node-42 nginx-proxy-kube-node-41 1/1 Running 3 1d 172.16.94.141 kube-node-41 nginx-proxy-kube-node-42 1/1 Running 3 1d 172.16.94.142 kube-node-42 nginx-proxy-kube-node-43 1/1 Running 0 26m 172.16.94.143 kube-node-43 5. éƒ¨ç½²é«˜å¯ç”¨é›†ç¾¤ å°†hosts.iniæ–‡ä»¶ä¸­çš„masterå’Œetcdçš„æœºå™¨å¢åŠ åˆ°å¤šå°ï¼Œæ‰§è¡Œéƒ¨ç½²å‘½ä»¤ã€‚\nansible-playbook -i inventory/k8s/hosts.ini cluster.yml -b -vvv ä¾‹å¦‚ï¼š\n# Configure 'ip' variable to bind kubernetes services on a # different ip than the default iface # ä¸»æœºå sshç™»é™†IP sshç”¨æˆ·å sshç™»é™†å¯†ç  æœºå™¨IP å­ç½‘æ©ç  kube-master-0 ansible_ssh_host=172.16.94.140 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.140 mask=/24 kube-master-1 ansible_ssh_host=172.16.94.144 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.144 mask=/24 kube-master-2 ansible_ssh_host=172.16.94.145 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.145 mask=/24 kube-node-41 ansible_ssh_host=172.16.94.141 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.141 mask=/24 kube-node-42 ansible_ssh_host=172.16.94.142 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.142 mask=/24 kube-node-43 ansible_ssh_host=172.16.94.143 ansible_ssh_user=root ansible_ssh_pass=123 ip=172.16.94.143 mask=/24 # configure a bastion host if your nodes are not directly reachable # bastion ansible_ssh_host=x.x.x.x [kube-master] kube-master-0 kube-master-1 kube-master-2 [etcd] kube-master-0 kube-master-1 kube-master-2 [kube-node] kube-node-41 kube-node-42 kube-node-43 [k8s-cluster:children] kube-node kube-master [calico-rr] 6. å‡çº§k8sé›†ç¾¤ é€‰æ‹©å¯¹åº”çš„k8sç‰ˆæœ¬ä¿¡æ¯ï¼Œæ‰§è¡Œå‡çº§å‘½ä»¤ã€‚æ¶‰åŠæ–‡ä»¶ä¸ºupgrade-cluster.ymlã€‚\nansible-playbook upgrade-cluster.yml -b -i inventory/k8s/hosts.ini -e kube_version=v1.10.4 -vvv 7. troubles shooting åœ¨ä½¿ç”¨kubesparyéƒ¨ç½²k8sé›†ç¾¤æ—¶ï¼Œä¸»è¦é‡åˆ°ä»¥ä¸‹æŠ¥é”™ã€‚\n7.1. python-netaddræœªå®‰è£… æŠ¥é”™å†…å®¹ï¼š fatal: [node1]: FAILED! =\u003e {\"failed\": true, \"msg\": \"The ipaddr filter requires python-netaddr be installed on the ansible controller\"} è§£å†³æ–¹æ³•ï¼š éœ€è¦å®‰è£… python-netaddrï¼Œå…·ä½“å‚è€ƒä¸Šè¿°[ç¯å¢ƒå‡†å¤‡]å†…å®¹ã€‚\n7.2. swapæœªå…³é—­ æŠ¥é”™å†…å®¹ï¼š fatal: [kube-master-0]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-41]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-42]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false } è§£å†³æ–¹æ³•ï¼š æ‰€æœ‰éƒ¨ç½²æœºå™¨æ‰§è¡Œswapoff -aå…³é—­swapï¼Œå…·ä½“å‚è€ƒä¸Šè¿°[ç¯å¢ƒå‡†å¤‡]å†…å®¹ã€‚\n7.3. éƒ¨ç½²æœºå™¨å†…å­˜è¿‡å° æŠ¥é”™å†…å®¹ï¼š TASK [kubernetes/preinstall : Stop if memory is too small for masters] ********************************************************************************************************************************************************************************************************* task path: /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml:52 Friday 10 August 2018 21:50:26 +0800 (0:00:00.940) 0:01:14.088 ********* fatal: [kube-master-0]: FAILED! =\u003e { \"assertion\": \"ansible_memtotal_mb \u003e= 1500\", \"changed\": false, \"evaluated_to\": false } TASK [kubernetes/preinstall : Stop if memory is too small for nodes] *********************************************************************************************************************************************************************************************************** task path: /root/gopath/src/kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml:58 Friday 10 August 2018 21:50:27 +0800 (0:00:00.570) 0:01:14.659 ********* fatal: [kube-node-41]: FAILED! =\u003e { \"assertion\": \"ansible_memtotal_mb \u003e= 1024\", \"changed\": false, \"evaluated_to\": false } fatal: [kube-node-42]: FAILED! =\u003e { \"assertion\": \"ansible_memtotal_mb \u003e= 1024\", \"changed\": false, \"evaluated_to\": false } to retry, use: --limit @/root/gopath/src/kubespray/cluster.retry è§£å†³æ–¹æ³•ï¼š è°ƒå¤§æ‰€æœ‰éƒ¨ç½²æœºå™¨çš„å†…å­˜ï¼Œæœ¬ç¤ºä¾‹ä¸­è°ƒæ•´ä¸º3Gæˆ–ä»¥ä¸Šã€‚\n7.4. kube-schedulerç»„ä»¶è¿è¡Œå¤±è´¥ kube-schedulerç»„ä»¶è¿è¡Œå¤±è´¥ï¼Œå¯¼è‡´http://localhost:10251/healthzè°ƒç”¨å¤±è´¥ã€‚\næŠ¥é”™å†…å®¹ï¼š FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left). FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left). fatal: [node1]: FAILED! =\u003e {\"attempts\": 60, \"changed\": false, \"content\": \"\", \"failed\": true, \"msg\": \"Status code was not [200]: Request failed: \u003curlopen error [Errno 111] Connection refused\u003e\", \"redirected\": false, \"status\": -1, \"url\": \"http://localhost:10251/healthz\"} è§£å†³æ–¹æ³•ï¼š å¯èƒ½æ˜¯å†…å­˜ä¸è¶³å¯¼è‡´ï¼Œæœ¬ç¤ºä¾‹ä¸­è°ƒå¤§äº†éƒ¨ç½²æœºå™¨çš„å†…å­˜ã€‚\n7.5. dockerå®‰è£…åŒ…å†²çª æŠ¥é”™å†…å®¹ï¼š failed: [k8s-node-1] (item={u'name': u'docker-engine-1.13.1-1.el7.centos'}) =\u003e { \"attempts\": 4, \"changed\": false, ... \"item\": { \"name\": \"docker-engine-1.13.1-1.el7.centos\" }, \"msg\": \"Error: docker-ce-selinux conflicts with 2:container-selinux-2.66-1.el7.noarch\\n\", \"rc\": 1, \"results\": [ \"Loaded plugins: fastestmirror\\nLoading mirror speeds from cached hostfile\\n * elrepo: mirrors.tuna.tsinghua.edu.cn\\n * epel: mirrors.tongji.edu.cn\\nPackage docker-engine is obsoleted by docker-ce, trying to install docker-ce-17.03.2.ce-1.el7.centos.x86_64 instead\\nResolving Dependencies\\n--\u003e Running transaction check\\n---\u003e Package docker-ce.x86_64 0:17.03.2.ce-1.el7.centos will be installed\\n--\u003e Processing Dependency: docker-ce-selinux \u003e= 17.03.2.ce-1.el7.centos for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64\\n--\u003e Processing Dependency: libltdl.so.7()(64bit) for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64\\n--\u003e Running transaction check\\n---\u003e Package docker-ce-selinux.noarch 0:17.03.2.ce-1.el7.centos will be installed\\n---\u003e Package libtool-ltdl.x86_64 0:2.4.2-22.el7_3 will be installed\\n--\u003e Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux\\n--\u003e Restarting Dependency Resolution with new changes.\\n--\u003e Running transaction check\\n---\u003e Package container-selinux.noarch 2:2.55-1.el7 will be updated\\n---\u003e Package container-selinux.noarch 2:2.66-1.el7 will be an update\\n--\u003e Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux\\n--\u003e Finished Dependency Resolution\\n You could try using --skip-broken to work around the problem\\n You could try running: rpm -Va --nofiles --nodigest\\n\" ] } è§£å†³æ–¹æ³•ï¼š å¸è½½æ—§çš„dockerç‰ˆæœ¬ï¼Œç”±kubesparyè‡ªåŠ¨å®‰è£…ã€‚\nsudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes-incubator/kubespray https://github.com/kubernetes-incubator/kubespray/blob/master/docs/upgrades.md ","categories":"","description":"","excerpt":"1. ç¯å¢ƒå‡†å¤‡ 1.1. éƒ¨ç½²æœºå™¨ ä»¥ä¸‹æœºå™¨ä¸ºè™šæ‹Ÿæœº\næœºå™¨IP ä¸»æœºå è§’è‰² ç³»ç»Ÿç‰ˆæœ¬ å¤‡æ³¨ 172.16.94.140 â€¦","ref":"/kubernetes-notes/setup/installer/install-k8s-by-kubespray/","tags":["Kubernetes"],"title":"ä½¿ç”¨kubesprayå®‰è£…kubernetes"},{"body":"æ–‡ä»¶æ“ä½œ æ›´å¤šæ–‡ä»¶æ“ä½œè§Goçš„osåŒ…ã€‚\n1. ç›®å½•æ“ä½œ func Mkdir(name string, perm FileMode) error\nåˆ›å»ºåç§°ä¸º name çš„ç›®å½•ï¼Œæƒé™è®¾ç½®æ˜¯ permï¼Œä¾‹å¦‚ 0777\nfunc MkdirAll(path string, perm FileMode) error æ ¹æ® path åˆ›å»ºå¤šçº§å­ç›®å½•ï¼Œä¾‹å¦‚ astaxie/test1/test2ã€‚\nfunc Remove(name string) error åˆ é™¤åç§°ä¸º name çš„ç›®å½•ï¼Œå½“ç›®å½•ä¸‹æœ‰æ–‡ä»¶æˆ–è€…å…¶ä»–ç›®å½•æ˜¯ä¼šå‡ºé”™\nfunc RemoveAll(path string) error æ ¹æ® path åˆ é™¤å¤šçº§å­ç›®å½•ï¼Œå¦‚æœ path æ˜¯å•ä¸ªåç§°ï¼Œé‚£ä¹ˆè¯¥ç›®å½•ä¸åˆ é™¤\n2. æ–‡ä»¶æ“ä½œ 2.1. å»ºç«‹ä¸æ‰“å¼€æ–‡ä»¶ æ–°å»ºæ–‡ä»¶ï¼š\nfunc Create(name string) (file *File, err Error) æ ¹æ®æä¾›çš„æ–‡ä»¶ååˆ›å»ºæ–°çš„æ–‡ä»¶ï¼Œè¿”å›ä¸€ä¸ªæ–‡ä»¶å¯¹è±¡ï¼Œé»˜è®¤æƒé™æ˜¯ 0666 çš„æ–‡ä»¶ï¼Œè¿”å›çš„æ–‡ä»¶å¯¹è±¡æ˜¯å¯è¯»å†™çš„ã€‚ ** func NewFile(fd uintptr, name string) *File** æ ¹æ®æ–‡ä»¶æè¿°ç¬¦åˆ›å»ºç›¸åº”çš„æ–‡ä»¶ï¼Œè¿”å›ä¸€ä¸ªæ–‡ä»¶å¯¹è±¡ æ‰“å¼€æ–‡ä»¶ï¼š\nfunc Open(name string) (file *File, err Error) è¯¥æ–¹æ³•æ‰“å¼€ä¸€ä¸ªåç§°ä¸º name çš„æ–‡ä»¶ï¼Œä½†æ˜¯æ˜¯åªè¯»æ–¹å¼ï¼Œå†…éƒ¨å®ç°å…¶å®è°ƒç”¨äº† OpenFileã€‚ func OpenFile(name string, flag int, perm uint32) (file *File, err Error) æ‰“å¼€åç§°ä¸º name çš„æ–‡ä»¶ï¼Œflag æ˜¯æ‰“å¼€çš„æ–¹å¼ï¼Œåªè¯»ã€è¯»å†™ç­‰ï¼Œperm æ˜¯æƒé™ 2.2. å†™æ–‡ä»¶ å†™æ–‡ä»¶å‡½æ•°ï¼š\nfunc (file *File) Write(b []byte) (n int, err Error) å†™å…¥ byte ç±»å‹çš„ä¿¡æ¯åˆ°æ–‡ä»¶ func (file *File) WriteAt(b []byte, off int64) (n int, err Error) åœ¨æŒ‡å®šä½ç½®å¼€å§‹å†™å…¥ byte ç±»å‹çš„ä¿¡æ¯ func (file *File) WriteString(s string) (ret int, err Error) å†™å…¥ string ä¿¡æ¯åˆ°æ–‡ä»¶ package main import ( \"fmt\" \"os\" ) func main() { userFile := \"test.txt\" fout, err := os.Create(userFile) defer fout.Close() if err != nil { fmt.Println(userFile, err) return } for i := 0; i \u003c 10; i++ { fout.WriteString(\"Just a test!\\r\\n\") fout.Write([]byte(\"Just a test!\\r\\n\")) } } 2.3. è¯»æ–‡ä»¶ è¯»æ–‡ä»¶å‡½æ•°ï¼š\nfunc (file *File) Read(b []byte) (n int, err Error) è¯»å–æ•°æ®åˆ° b ä¸­\nfunc (file *File) ReadAt(b []byte, off int64) (n int, err Error) ä» off å¼€å§‹è¯»å–æ•°æ®åˆ° b ä¸­\npackage main import ( \"fmt\" \"os\" ) func main() { userFile := \"text.txt\" fl, err := os.Open(userFile) defer fl.Close() if err != nil { fmt.Println(userFile, err) return } buf := make([]byte, 1024) for { n, _ := fl.Read(buf) if 0 == n { break } os.Stdout.Write(buf[:n]) } } 2.4. åˆ é™¤æ–‡ä»¶ Go è¯­è¨€é‡Œé¢åˆ é™¤æ–‡ä»¶å’Œåˆ é™¤æ–‡ä»¶å¤¹æ˜¯åŒä¸€ä¸ªå‡½æ•°\nfunc Remove(name string) Error è°ƒç”¨è¯¥å‡½æ•°å°±å¯ä»¥åˆ é™¤æ–‡ä»¶åä¸º name çš„æ–‡ä»¶ ","categories":"","description":"","excerpt":"æ–‡ä»¶æ“ä½œ æ›´å¤šæ–‡ä»¶æ“ä½œè§Goçš„osåŒ…ã€‚\n1. ç›®å½•æ“ä½œ func Mkdir(name string, perm FileMode) â€¦","ref":"/golang-notes/text/file/","tags":["Golang"],"title":"æ–‡ä»¶æ“ä½œ"},{"body":"1. go pprofå·¥å…·ç®€ä»‹ åœ¨ Go è¯­è¨€ä¸­ï¼ŒPProf æ˜¯ç”¨äºå¯è§†åŒ–å’Œåˆ†ææ€§èƒ½åˆ†ææ•°æ®çš„å·¥å…·ï¼ŒPProf ä»¥ profile.proto è¯»å–åˆ†ææ ·æœ¬çš„é›†åˆï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šä»¥å¯è§†åŒ–å¹¶å¸®åŠ©åˆ†ææ•°æ®ï¼ˆæ”¯æŒæ–‡æœ¬å’Œå›¾å½¢æŠ¥å‘Šï¼‰ã€‚\nruntime/pprofï¼šé‡‡é›†ç¨‹åºï¼ˆé Serverï¼‰çš„æŒ‡å®šåŒºå—çš„è¿è¡Œæ•°æ®è¿›è¡Œåˆ†æã€‚ net/http/pprofï¼šåŸºäº HTTP Server è¿è¡Œï¼Œå¹¶ä¸”å¯ä»¥é‡‡é›†è¿è¡Œæ—¶æ•°æ®è¿›è¡Œåˆ†æã€‚ 1.1. åˆ†æå†…å®¹ cpuï¼ˆCPU Profilingï¼‰:Â $HOST/debug/pprof/profileï¼Œé»˜è®¤è¿›è¡Œ 30s çš„ CPU Profilingï¼Œå¾—åˆ°ä¸€ä¸ªåˆ†æç”¨çš„ profile æ–‡ä»¶ blockï¼ˆBlock Profilingï¼‰ï¼š$HOST/debug/pprof/blockï¼ŒæŸ¥çœ‹å¯¼è‡´é˜»å¡åŒæ­¥çš„å †æ ˆè·Ÿè¸ª goroutineï¼š$HOST/debug/pprof/goroutineï¼ŒæŸ¥çœ‹å½“å‰æ‰€æœ‰è¿è¡Œçš„ goroutines å †æ ˆè·Ÿè¸ª heapï¼ˆMemory Profilingï¼‰:Â $HOST/debug/pprof/heapï¼ŒæŸ¥çœ‹æ´»åŠ¨å¯¹è±¡çš„å†…å­˜åˆ†é…æƒ…å†µ mutexï¼ˆMutex Profilingï¼‰ï¼š$HOST/debug/pprof/mutexï¼ŒæŸ¥çœ‹å¯¼è‡´äº’æ–¥é”çš„ç«äº‰æŒæœ‰è€…çš„å †æ ˆè·Ÿè¸ª threadcreateï¼š$HOST/debug/pprof/threadcreateï¼ŒæŸ¥çœ‹åˆ›å»ºæ–°OSçº¿ç¨‹çš„å †æ ˆè·Ÿè¸ª 1.2. åˆ†ææ–¹å¼ go tool pprofå‘½ä»¤äº¤äº’æ–¹å¼\nwebç½‘é¡µæ–¹å¼æŸ¥çœ‹\nhttp://ip:port/debug/pprof/ 2. ä»£ç é…ç½®pprof 2.1. Ginæ¡†æ¶é›†æˆpprof è°ƒç”¨github.com/gin-contrib/pprofï¼Œæ‰§è¡Œpprof.Register(*gin.Engine)ã€‚\nç¤ºä¾‹ä»£ç ï¼š\nimport ( \"github.com/gin-contrib/pprof\" \"github.com/gin-gonic/gin\" ) type server struct { conf *config.Config gin *gin.Engine } func (s *server) setupServer() *gin.Engine { // æ³¨å†Œpprof pprof.Register(s.gin) // register routers s.setupRoutes() return s.gin } 2.2. éGinæ¡†æ¶é›†æˆpprof import ( \"net/http\" \"net/http/pprof\" \"github.com/gorilla/mux\" ) // Install adds the Profiling webservice to the given mux. func Install(c *mux.Router) { c.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) c.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) c.HandleFunc(\"/debug/pprof/trace\", pprof.Trace) c.HandleFunc(\"/debug/pprof\", redirectTo(\"/debug/pprof/\")) c.PathPrefix(\"/debug/pprof/\").HandlerFunc(pprof.Index) } func RegisterPprof(){ // NewRouter muxHandler := mux.NewRouter() // register handler for pprof Install(muxHandler) } 3. go tool pprof 3.1. å†…å­˜ go tool pprof http://ip:port/debug/pprof/heap ç¤ºä¾‹ï¼š\n# go tool pprof http://ip:port/debug/pprof/heap Fetching profile over HTTP from http://ip:port/debug/pprof/heap Saved profile in /root/pprof/pprof.yurt-tunnel-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz File: yurt-tunnel-server Type: inuse_space Time: May 10, 2023 at 11:03am (+08) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 46.08GB, 94.72% of 48.65GB total Dropped 319 nodes (cum \u003c= 0.24GB) Showing top 10 nodes out of 31 flat flat% sum% cum cum% 25.91GB 53.27% 53.27% 25.91GB 53.27% bufio.NewWriterSize (inline) 12.91GB 26.54% 79.80% 12.91GB 26.54% bufio.NewReaderSize 1.67GB 3.43% 83.23% 1.74GB 3.59% net/textproto.(*Reader).ReadMIMEHeader 1.26GB 2.59% 85.83% 1.26GB 2.59% runtime.malg 1.22GB 2.51% 88.34% 5.72GB 11.75% net/http.(*conn).readRequest 0.86GB 1.77% 90.11% 3.39GB 6.96% net/http.readRequest 0.68GB 1.39% 91.50% 13.72GB 28.20% sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP 0.58GB 1.20% 92.70% 0.86GB 1.78% context.propagateCancel 0.51GB 1.06% 93.76% 0.51GB 1.06% net/http.(*Server).newConn 0.47GB 0.96% 94.72% 1.33GB 2.74% context.WithCancel 3.2. CPU go tool pprof http://ip:port/debug/pprof/profile ç¤ºä¾‹ï¼š\n# go tool pprof http://ip:port/debug/pprof/profile Fetching profile over HTTP from http://ip:port/debug/pprof/profile Saved profile in /root/pprof/pprof.yurt-tunnel-server.samples.cpu.001.pb.gz File: yurt-tunnel-server Type: cpu Time: May 10, 2023 at 10:58am (+08) Duration: 30.14s, Total samples = 35.72s (118.52%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 25120ms, 70.32% of 35720ms total Dropped 240 nodes (cum \u003c= 178.60ms) Showing top 10 nodes out of 58 flat flat% sum% cum cum% 12100ms 33.87% 33.87% 12120ms 33.93% runtime.(*lfstack).pop (inline) 3360ms 9.41% 43.28% 3360ms 9.41% runtime.(*lfstack).push 2200ms 6.16% 49.44% 2200ms 6.16% runtime.pageIndexOf (inline) 1350ms 3.78% 53.22% 1520ms 4.26% runtime.findObject 1350ms 3.78% 57.00% 3490ms 9.77% runtime.scanobject 1120ms 3.14% 60.13% 3550ms 9.94% runtime.sweepone 1000ms 2.80% 62.93% 10850ms 30.38% runtime.scanblock 910ms 2.55% 65.48% 1310ms 3.67% runtime.step 890ms 2.49% 67.97% 890ms 2.49% runtime.markBits.isMarked (inline) 840ms 2.35% 70.32% 20300ms 56.83% runtime.gentraceback 4. ç”Ÿæˆç«ç„°å›¾ å®‰è£…graphvizï¼Œç”¨äºç”Ÿæˆç«ç„°å›¾ã€‚\n# ubuntu apt-get install -y graphviz # centos yum install -y graphviz # mac brew install graphviz å½“æ‰§è¡Œgo pprofçš„å‘½ä»¤æ—¶ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆ.pb.gzæ–‡ä»¶ï¼Œä¾‹å¦‚ï¼š\n# å†…å­˜ Saved profile in /root/pprof/pprof.yurt-tunnel-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz # CPU Saved profile in /root/pprof/pprof.yurt-tunnel-server.samples.cpu.001.pb.gz å°†.pb.gzæ–‡ä»¶æ‹·è´åˆ°macæœ¬åœ°ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåœ¨æµè§ˆå™¨æŸ¥çœ‹ç›¸å…³è§†å›¾ã€‚\n$ go tool pprof -http=:8081 pprof.yurt-tunnel-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz Serving web UI on http://localhost:8081 4.1. å†…å­˜åˆ†å¸ƒå›¾ 4.2. ç«ç„°å›¾ 5. æŸ¥çœ‹goroutineæ³„éœ² å¯ä»¥é€šè¿‡è®¿é—®webåœ°å€çš„/debug/pprof/goroutineè·¯å¾„ï¼ŒæŸ¥çœ‹goroutineçš„è¯¦ç»†åˆ†å¸ƒæƒ…å†µã€‚goroutineæ•°é‡åˆ†å¸ƒè¿‡å¤šçš„åœ°æ–¹å¯èƒ½ä¼šå­˜åœ¨å†…å­˜æ³„éœ²çš„æƒ…å†µã€‚\nç¤ºä¾‹ï¼š\ngoroutine profile: total 3337450 3336630 @ 0x437fb6 0x40640c 0x405e38 0x1406b18 0x7c19bb 0x7bd528 0x4684c1 # 0x1406b17 sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP+0xbb7 /go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:105 # 0x7c19ba net/http.serverHandler.ServeHTTP+0x43a /usr/local/go/src/net/http/server.go:2878 # 0x7bd527 net/http.(*conn).serve+0xb07 /usr/local/go/src/net/http/server.go:1929 195 @ 0x437fb6 0x43095e 0x462d69 0x4c90b2 0x4ca41a 0x4ca408 0x525649 0x536665 0x7b7b8d 0x4fd5e6 0x140709a 0x7c19bb 0x7bd528 0x4684c1 # 0x462d68 internal/poll.runtime_pollWait+0x88 /usr/local/go/src/runtime/netpoll.go:229 # 0x4c90b1 internal/poll.(*pollDesc).wait+0x31 /usr/local/go/src/internal/poll/fd_poll_runtime.go:84 # 0x4ca419 internal/poll.(*pollDesc).waitRead+0x259 /usr/local/go/src/internal/poll/fd_poll_runtime.go:89 # 0x4ca407 internal/poll.(*FD).Read+0x247 /usr/local/go/src/internal/poll/fd_unix.go:167 # 0x525648 net.(*netFD).Read+0x28 /usr/local/go/src/net/fd_posix.go:56 # 0x536664 net.(*conn).Read+0x44 /usr/local/go/src/net/net.go:183 # 0x7b7b8c net/http.(*connReader).Read+0x16c /usr/local/go/src/net/http/server.go:780 # 0x4fd5e5 bufio.(*Reader).Read+0x105 /usr/local/go/src/bufio/bufio.go:213 # 0x1407099 sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP+0x1139 /go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:138 # 0x7c19ba net/http.serverHandler.ServeHTTP+0x43a /usr/local/go/src/net/http/server.go:2878 # 0x7bd527 net/http.(*conn).serve+0xb07 /usr/local/go/src/net/http/server.go:1929 ä¸Šè¿°goroutineçš„åˆ†å¸ƒè¶…è¿‡300ä¸‡ä¸ªï¼Œä¸»è¦éƒ½åˆ†å¸ƒåœ¨ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œå› æ­¤å¯ä»¥å¾—å‡ºä»¥ä¸‹å¯èƒ½çš„ç»“è®ºï¼š\n/go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:105 ä»£ç å¯èƒ½å­˜åœ¨å†…å­˜æ³„éœ²çš„æƒ…å†µã€‚\n# 0x1406b17 sigs.k8s.io/apiserver-network-proxy/pkg/server.(*Tunnel).ServeHTTP+0xbb7 /go/pkg/mod/github.com/openyurtio/apiserver-network-proxy@v1.18.8/pkg/server/tunnel.go:105 å› æ­¤æˆ‘ä»¬è¿½è¸ªtunnel.go:105çš„æºç :\nselect { #105 case \u003c-connection.connected: // Waiting for response before we begin full communication. } select æ²¡æœ‰è¿”å›ï¼Œå¯¼è‡´goroutineä¸æ–­ç´¯è®¡ã€‚\n5.1. è§£å†³goroutineæ³„éœ² ä»¥ä¸Šåˆ†æçš„ä»£ç æ¥è‡ªapiserver-network-proxyã€‚\næˆ‘ä»¬å¯ä»¥æŸ¥çœ‹è¯¥å†…å­˜æ³„éœ²çš„issue:\nhttps://github.com/kubernetes-sigs/apiserver-network-proxy/pull/270 ä¿®å¤å†…å­˜æ³„éœ²çš„commitï¼š\nFix obscure HTTP CONNECT goroutine leak by jveski Â· Pull Request #270 Â· kubernetes-sigs/apiserver-network-proxy Â· GitHub æ¶‰åŠä»£ç ä¿®æ”¹å¦‚ä¸‹ï¼š\nselect { case \u003c-connection.connected: // Waiting for response before we begin full communication. case \u003c-closed: // Connection was closed before being established } å¢åŠ closedç±»å‹ï¼Œé€€å‡ºgoroutineã€‚\nå‚è€ƒï¼š\nGolang å¤§æ€å™¨ä¹‹æ€§èƒ½å‰–æ PProf - SegmentFault æ€å¦\nhttps://geektutu.com/post/hpg-pprof.html\nhttps://coder.today/tech/2018-11-10_profiling-your-golang-app-in-3-steps/\nProfiling Go Programs - The Go Programming Language\nGo å¤§æ€å™¨ä¹‹æ€§èƒ½å‰–æ PProfï¼ˆä¸Šï¼‰ | Go è¯­è¨€ç¼–ç¨‹ä¹‹æ—…\nDockerdèµ„æºæ³„éœ²ç³»åˆ—\n","categories":"","description":"","excerpt":"1. go pprofå·¥å…·ç®€ä»‹ åœ¨ Go è¯­è¨€ä¸­ï¼ŒPProf æ˜¯ç”¨äºå¯è§†åŒ–å’Œåˆ†ææ€§èƒ½åˆ†ææ•°æ®çš„å·¥å…·ï¼ŒPProf â€¦","ref":"/golang-notes/framework/go-pprof-usage/","tags":"","title":"æ€§èƒ½åˆ†æä¹‹go pprofå·¥å…·ä½¿ç”¨"},{"body":"åœ¨ Go ä¸­å®ç°ä¸€ä¸ªç®€å•çš„ Rate Limiterï¼ˆé™æµå™¨ï¼‰ å¯ä»¥æœ‰å¤šç§æ–¹å¼ï¼Œå¸¸è§çš„æœ‰åŸºäº ä»¤ç‰Œæ¡¶ï¼ˆToken Bucketï¼‰ å’Œ æ¼æ¡¶ï¼ˆLeaky Bucketï¼‰ çš„å®ç°ã€‚ä¸‹é¢æˆ‘ä»¬å…ˆä»‹ç»ä¸€ç§ç»å…¸æ–¹å¼ â€”â€” ä»¤ç‰Œæ¡¶ç®—æ³• å®ç°ã€‚\n1. ä»¤ç‰Œæ¡¶ç®—æ³• â€œä»¤ç‰Œæ¡¶ç®—æ³•ï¼ˆToken Bucketï¼‰â€æ˜¯ä¸€ç§ é™æµç®—æ³•ï¼Œç”¨äºæ§åˆ¶è¯·æ±‚çš„é€Ÿç‡ï¼Œæ˜¯å¸¸è§äº API ç½‘å…³ã€è´Ÿè½½å‡è¡¡å™¨ã€æµæ§ä¸­é—´ä»¶ç­‰çš„æ ¸å¿ƒç­–ç•¥ä¹‹ä¸€ã€‚\n1.1. æ ¸å¿ƒæ€æƒ³ ç³»ç»Ÿä»¥å›ºå®šé€Ÿç‡å¾€â€œæ¡¶â€é‡Œæ”¾ä»¤ç‰Œï¼Œæ¯æ¬¡è¯·æ±‚å¿…é¡»æ‹¿åˆ°ä¸€ä¸ªä»¤ç‰Œæ‰èƒ½è¢«å¤„ç†ï¼Œå¦åˆ™è¢«æ‹’ç»ï¼ˆé™æµï¼‰ã€‚\nåç§° å«ä¹‰ æ¡¶ï¼ˆBucketï¼‰ ä¿å­˜ä»¤ç‰Œçš„å®¹å™¨ï¼Œæœ€å¤šå¯ä»¥è£… burst ä¸ªä»¤ç‰Œï¼ˆçªå‘å®¹é‡ï¼‰ ä»¤ç‰Œï¼ˆTokenï¼‰ æ¯ä¸ªä»¤ç‰Œä»£è¡¨ä¸€æ¬¡å…è®¸çš„æ“ä½œï¼ˆå¦‚ä¸€æ¬¡ HTTP è¯·æ±‚ï¼‰ é€Ÿç‡ï¼ˆRateï¼‰ å‘æ¡¶ä¸­æ·»åŠ ä»¤ç‰Œçš„é€Ÿåº¦ï¼Œæ¯”å¦‚æ¯ç§’æ”¾ 5 ä¸ª è¯·æ±‚åˆ°æ¥æ—¶ è‹¥æ¡¶ä¸­æœ‰ä»¤ç‰Œï¼Œè¯·æ±‚å°±â€œå–å‡ºä¸€ä¸ªä»¤ç‰Œâ€è¢«å…è®¸ï¼›å¦åˆ™å°±è¢«æ‹’ç»æˆ–ç­‰å¾… 1.2. åŠ¨æ€è¡Œä¸ºå›¾ç¤º å‡è®¾ï¼šæ¯ç§’ç”Ÿæˆ1ä¸ªä»¤ç‰Œï¼Œæ¡¶æœ€å¤šè£…3ä¸ªã€‚\næ—¶é—´ï¼š0s æ¡¶ï¼š[â—] è¯·æ±‚ï¼šå…è®¸ï¼ˆæ¶ˆè€—1ä¸ªä»¤ç‰Œï¼‰ æ—¶é—´ï¼š1s æ¡¶ï¼š[â—] è¯·æ±‚ï¼šå…è®¸ï¼ˆæ¶ˆè€—1ä¸ªä»¤ç‰Œï¼‰ æ—¶é—´ï¼š2s æ¡¶ï¼š[â—â—] è¯·æ±‚ï¼šå…è®¸ æ—¶é—´ï¼š3s æ¡¶ï¼š[â—â—â—] è¯·æ±‚ï¼šå…è®¸ æ—¶é—´ï¼š4s æ¡¶ï¼š[â—â—â—] è¯·æ±‚ï¼šæ²¡æœ‰æ¶ˆè´¹ï¼Œæ¡¶æ»¡äº†ï¼ˆä¸å†å¢åŠ ï¼‰ æ—¶é—´ï¼š5s æ¡¶ï¼š[â—â—â—] æ¥5ä¸ªè¯·æ±‚ï¼Œåªå…è®¸3ä¸ªï¼Œå…¶ä½™è¢«é™æµ 1.3. å’Œæ¼æ¡¶ç®—æ³•çš„å¯¹æ¯” æ¯”è¾ƒé¡¹ ä»¤ç‰Œæ¡¶ï¼ˆToken Bucketï¼‰ æ¼æ¡¶ï¼ˆLeaky Bucketï¼‰ æ§åˆ¶æ–¹å¼ æ§åˆ¶è¯·æ±‚è¿›å…¥é€Ÿç‡ æ§åˆ¶è¯·æ±‚å¤„ç†é€Ÿç‡ æ”¯æŒçªå‘è¯·æ±‚ âœ… æ”¯æŒ âŒ ä¸¥æ ¼åŒ€é€Ÿå¤„ç† åº”ç”¨åœºæ™¯ é™æµï¼ˆAPIã€æ¥å£ï¼‰ æ’é˜Ÿï¼ˆç½‘ç»œã€å¤„ç†ä»»åŠ¡ï¼‰ 1.4. åº”ç”¨åœºæ™¯ä¸¾ä¾‹ API è¯·æ±‚é€Ÿç‡æ§åˆ¶ï¼ˆæ¯ä¸ªç”¨æˆ·æœ€å¤š1ç§’10æ¬¡ï¼‰\nç™»å½•/æ³¨å†Œé˜²æš´åŠ›æ”»å‡»ï¼ˆæ¯ IP é™1åˆ†é’Ÿ5æ¬¡ï¼‰\nCDN è¾¹ç¼˜é™æµï¼ˆé˜²æ­¢æºç«™è¢«æ‰“çˆ†ï¼‰\nåç«¯ä»»åŠ¡æŠ•é€’ï¼ˆé˜²æ­¢ RabbitMQ æ‹¥å µï¼‰\n2. å®ç°ç®€å• Token Bucket é™æµå™¨ 2.1. ä½¿ç”¨ time.Ticker å®ç° package main import ( \"fmt\" \"time\" ) type RateLimiter struct { tokens chan struct{} ticker *time.Ticker maxTokens int refillPeriod time.Duration } func NewRateLimiter(rps int, burst int) *RateLimiter { rl := \u0026RateLimiter{ tokens: make(chan struct{}, burst), ticker: time.NewTicker(time.Second / time.Duration(rps)), maxTokens: burst, refillPeriod: time.Second / time.Duration(rps), } // åˆå§‹åŒ– token æ¡¶ for i := 0; i \u003c burst; i++ { rl.tokens \u003c- struct{}{} } // åå°åç¨‹å®šæ—¶æ”¾ token go func() { for range rl.ticker.C { select { case rl.tokens \u003c- struct{}{}: default: // æ¡¶æ»¡æ—¶ä¸¢å¼ƒ token } } }() return rl } // Allow ä¼šé˜»å¡ç›´åˆ°æœ‰ token å¯ç”¨ï¼ˆå¯æ”¹æˆ TryAllow éé˜»å¡ç‰ˆæœ¬ï¼‰ func (rl *RateLimiter) Allow() bool { select { case \u003c-rl.tokens: return true default: return false } } func main() { limiter := NewRateLimiter(5, 10) // æ¯ç§’ 5 ä¸ªè¯·æ±‚ï¼Œæœ€å¤šç¼“å†² 10 ä¸ª for i := 0; i \u003c 20; i++ { if limiter.Allow() { fmt.Println(\"Request\", i, \"allowed at\", time.Now()) } else { fmt.Println(\"Request\", i, \"denied at\", time.Now()) } time.Sleep(100 * time.Millisecond) } } 2.2. ä½¿ç”¨Go rateåŒ…å®ç° Go çš„ golang.org/x/time/rate åŒ…å°±å®ç°äº† ä»¤ç‰Œæ¡¶ç®—æ³•ï¼Œä¾‹å¦‚ï¼š\n// æ¯ç§’5ä¸ªä»¤ç‰Œï¼Œæœ€å¤šèƒ½è£…10ä¸ªï¼ˆå…è®¸çªå‘10æ¬¡ï¼‰ limiter := rate.NewLimiter(5, 10) ä½ å¯ä»¥ä½¿ç”¨ .Allow() æ¥åˆ¤æ–­æ˜¯å¦è·å¾—ä»¤ç‰Œï¼š\nif limiter.Allow() { // æœ‰ä»¤ç‰Œï¼Œè¯·æ±‚é€šè¿‡ } else { // æ²¡ä»¤ç‰Œï¼Œè¯·æ±‚è¢«é™æµ } 3. å®ç°GinæŒ‰ç”¨æˆ·æˆ– IP é™æµä¸­é—´ä»¶ 3.1. é™æµå™¨ç»“æ„ï¼ˆä½¿ç”¨ rate.Limiterï¼‰ package ratelimiter import ( \"sync\" \"golang.org/x/time/rate\" ) type RateLimiter struct { rate rate.Limit burst int buckets map[string]*rate.Limiter mutex sync.Mutex } func NewRateLimiter(r rate.Limit, b int) *RateLimiter { return \u0026RateLimiter{ rate: r, burst: b, buckets: make(map[string]*rate.Limiter), } } func (rl *RateLimiter) getLimiter(key string) *rate.Limiter { rl.mutex.Lock() defer rl.mutex.Unlock() if limiter, exists := rl.buckets[key]; exists { return limiter } limiter := rate.NewLimiter(rl.rate, rl.burst) rl.buckets[key] = limiter return limiter } func (rl *RateLimiter) Allow(key string) bool { return rl.getLimiter(key).Allow() } 3.2. Gin ä¸­é—´ä»¶ package ratelimiter import ( \"net/http\" \"github.com/gin-gonic/gin\" ) type KeyFunc func(c *gin.Context) string func Middleware(rl *RateLimiter, keyFn KeyFunc) gin.HandlerFunc { return func(c *gin.Context) { key := keyFn(c) if !rl.Allow(key) { c.AbortWithStatusJSON(http.StatusTooManyRequests, gin.H{ \"error\": \"rate limit exceeded\", }) return } c.Next() } } 3.3. åœ¨ main.go ä¸­ä½¿ç”¨ï¼ˆæŒ‰ IP é™æµï¼‰ package main import ( \"github.com/gin-gonic/gin\" \"golang.org/x/time/rate\" \"your_project/ratelimiter\" ) func main() { // æ¯ç§’2ä¸ªè¯·æ±‚ï¼Œæœ€å¤šçªå‘5ä¸ª rl := ratelimiter.NewRateLimiter(2, 5) r := gin.Default() r.Use(ratelimiter.Middleware(rl, func(c *gin.Context) string { return c.ClientIP() // æˆ–ä½¿ç”¨ c.GetString(\"userID\") å®ç°æŒ‰ç”¨æˆ·é™æµ })) r.GET(\"/hello\", func(c *gin.Context) { c.JSON(200, gin.H{\"msg\": \"hello\"}) }) r.Run(\":8080\") } ","categories":"","description":"","excerpt":"åœ¨ Go ä¸­å®ç°ä¸€ä¸ªç®€å•çš„ Rate Limiterï¼ˆé™æµå™¨ï¼‰ å¯ä»¥æœ‰å¤šç§æ–¹å¼ï¼Œå¸¸è§çš„æœ‰åŸºäº ä»¤ç‰Œæ¡¶ï¼ˆToken Bucketï¼‰ å’Œ æ¼ â€¦","ref":"/golang-notes/web/go-ratelimiter/","tags":["Golang"],"title":"Goå®ç°é™æµå™¨"},{"body":"1. éƒ¨ç½²qemu-system-x86_64 # æ›´æ–°åŒ… sudo apt-get update # å®‰è£…QEMUå’ŒKVMç›¸å…³çš„åŒ…ã€‚KVMï¼ˆKernel-based Virtual Machineï¼‰å¯ä»¥æ˜¾è‘—æé«˜QEMUçš„æ€§èƒ½ã€‚ sudo apt-get install -y qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils # å®‰è£…qemu-system-x86 sudo apt-get install -y qemu qemu-system-x86 # ä¸ºäº†åœ¨érootç”¨æˆ·ä¸‹ä½¿ç”¨QEMU/KVMï¼Œéœ€è¦å°†å½“å‰ç”¨æˆ·æ·»åŠ åˆ° libvirt å’Œ kvm ç»„ã€‚ sudo usermod -aG libvirt $(whoami) sudo usermod -aG kvm $(whoami) # æŸ¥çœ‹ç‰ˆæœ¬ qemu-system-x86_64 --version # éªŒè¯å‘½ä»¤ virsh list --all ","categories":"","description":"","excerpt":"1. éƒ¨ç½²qemu-system-x86_64 # æ›´æ–°åŒ… sudo apt-get update # å®‰è£…QEMUå’ŒKVMç›¸å…³çš„ â€¦","ref":"/kubernetes-notes/kvm/qemu-system/","tags":["KubeVirt"],"title":"qemuåˆ›å»ºè™šæ‹Ÿæœº"},{"body":"1. Raidæ˜¯ä»€ä¹ˆ RAIDè‹±æ–‡å…¨ç§°Redundant Array of Independent Diskï¼Œç¿»è¯‘è¿‡æ¥å°±æ˜¯â€œç‹¬ç«‹ç£ç›˜å†—ä½™ç³»ç»Ÿâ€ã€‚RAIDæ˜¯ä¸€ç§å¯æé«˜æ€§èƒ½æˆ–æä¾›å®¹é”™åŠŸèƒ½çš„ç£ç›˜å­ç³»ç»Ÿã€‚\nRAIDçš„åŸºæœ¬æ€æƒ³æ˜¯å°†å¤šä¸ªå®¹é‡è¾ƒå°ã€é€Ÿåº¦è¾ƒæ…¢ã€å¯é æ€§è¾ƒå·®çš„å»‰ä»·ç£ç›˜ï¼Œç»„åˆæˆä¸€ä¸ªç£ç›˜ç»„ï¼Œä»è€Œä»¥è¾ƒä½çš„æˆæœ¬è·å¾—ä¸æ˜‚è´µçš„å¤§å®¹é‡ã€é«˜é€Ÿç£ç›˜ç›¸ä¼¼çš„æ€§èƒ½ã€‚\nä¼˜åŠ¿ï¼š\né«˜æ€§èƒ½\nå¯é æ€§å¯ç”¨æ€§\nå¤§å®¹é‡\nå¯ç®¡ç†æ€§\n2. Raidçš„åŸºæœ¬åŸç† æ¶æ„ä¸Šï¼Œé€šè¿‡RAIDæ§åˆ¶å™¨å°±å°†å¤šå—ç£ç›˜ç»„åˆï¼Œåœ¨æ“ä½œç³»ç»Ÿå±‚æˆ‘ä»¬çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªé€»è¾‘ç£ç›˜ã€‚\næŠ€æœ¯ä¸Šï¼ŒRAIDä¸»è¦é‡‡ç”¨ç£ç›˜é•œåƒæŠ€æœ¯ã€æ¡å¸¦åŒ–æŠ€æœ¯å’Œå¥‡å¶æ ¡éªŒæŠ€æœ¯å®ç°é«˜æ€§èƒ½ã€å¯é æ€§ã€å®¹é”™èƒ½åŠ›å’Œæ‰©å±•æ€§ã€‚\nç£ç›˜é•œåƒæŠ€æœ¯ï¼šå°†æ•°æ®å¤åˆ¶åˆ°å¤šä¸ªç£ç›˜ï¼Œä¸€æ–¹é¢å¯ä»¥æé«˜å¯é æ€§ï¼Œå¦ä¸€æ–¹é¢å¯ä»¥æé«˜è¯»æ€§èƒ½ã€‚å› ä¸ºéœ€è¦å†™å…¥å¤šä¸ªç£ç›˜ä¼šå¯¼è‡´å†™æ€§èƒ½é™ä½ã€‚\næ•°æ®æ¡å¸¦æŠ€æœ¯ï¼šå°†æ•°æ®åˆ†ç‰‡ä¿å­˜åˆ°å¤šä¸ªä¸åŒçš„ç£ç›˜ï¼Œå¤šä¸ªæ•°æ®åˆ†ç‰‡å…±åŒç»„æˆå®Œæ•´çš„æ•°æ®ã€‚ä¸»è¦ç”¨äºæç¤ºç£ç›˜IOæ€§èƒ½ï¼Œå½“è®¿é—®æ•°æ®æ—¶å¯ä»¥åŒæ—¶å¯¹å¤šä¸ªç£ç›˜è¿›è¡Œè¯»å–ã€‚\næ•°æ®æ ¡éªŒæŠ€æœ¯ï¼šåˆ©ç›Šå†—ä½™æ•°æ®å¯¹æ•°æ®è¿›è¡Œæ ¡éªŒå’Œä¿®å¤ã€‚å†—ä½™æ•°æ®é€šå¸¸é‡‡ç”¨æµ·æ˜ç ã€å¼‚æˆ–æ“ä½œç­‰ç®—æ³•æ¥è®¡ç®—è·å¾—ã€‚å¯ä»¥æé«˜ç£ç›˜é˜µåˆ—çš„å¯é æ€§å’Œå®¹é”™èƒ½åŠ›ã€‚ä¸è¿‡ï¼Œæ•°æ®æ ¡éªŒéœ€è¦ä»å¤šå¤„è¯»å–æ•°æ®å¹¶è¿›è¡Œè®¡ç®—å’Œå¯¹æ¯”ï¼Œä¼šå½±å“ç³»ç»Ÿæ€§èƒ½ã€‚\n3. Raidçš„çº§åˆ« RAIDçº§åˆ«å‘½åæ–¹å¼å¦‚ï¼ŒRAID 0ã€RAID 1/RAID 5ã€RAID 10ç­‰ã€‚RAIDæ¯ä¸€ä¸ªçº§åˆ«ä»£è¡¨ä¸€ç§RAIDç»„åˆå®ç°æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œçº§åˆ«ä¹‹é—´å¹¶æ— é«˜ä½ä¹‹åˆ†ã€‚ä¸åŒç­‰çº§çš„ RAID é‡‡ç”¨å…¶ä¸­ä¸€ä¸ªæˆ–å¤šä¸ªæŠ€æœ¯ï¼Œå¾—åˆ°çš„æ˜¯ä¸åŒçš„å®¹é‡ã€I/Oæ€§èƒ½ã€å¯é æ€§ã€å¯ç”¨æ€§ã€‚åœ¨å†³å®šé€‰æ‹©å“ªç§RAIDçº§åˆ«ä¹‹å‰ï¼Œéœ€è¦æ·±å…¥ç†è§£ç³»ç»Ÿéœ€æ±‚ï¼Œæ ¹æ®è‡ªèº«æƒ…å†µè¿›è¡Œåˆç†é€‰æ‹©ï¼Œç»¼åˆè¯„ä¼°å¯é æ€§ã€æ€§èƒ½å’Œæˆæœ¬æ¥è¿›è¡ŒæŠ˜ä¸­çš„é€‰æ‹©ã€‚\nRaidç­‰çº§ Raid0 Raid1 Raid3 Raid5 Raid6 Raid10 åˆ«å æ¡å¸¦ é•œåƒ ä¸“ç”¨å¥‡å¶æ ¡éªŒæ¡å¸¦ åˆ†å¸ƒå¥‡å¶æ ¡éªŒæ¡å¸¦ åŒé‡å¥‡å¶æ ¡éªŒæ¡å¸¦ é•œåƒ+æ¡å¸¦ å®¹é”™æ€§ æ—  æœ‰ æœ‰ æœ‰ æœ‰ æœ‰ å†—ä½™ç±»å‹ æ—  æœ‰ æœ‰ æœ‰ æœ‰ æœ‰ çƒ­å¤‡ä»½é€‰æ‹© æ—  æœ‰ æœ‰ æœ‰ æœ‰ æœ‰ è¯»æ€§èƒ½ é«˜ ä½ é«˜ é«˜ é«˜ é«˜ éšæœºå†™æ€§èƒ½ é«˜ ä½ ä½ ä¸€èˆ¬ ä½ ä¸€èˆ¬ è¿ç»­å†™æ€§èƒ½ é«˜ ä½ ä½ ä½ ä½ ä¸€èˆ¬ éœ€è¦ç£ç›˜æ•° n\u003e=1 2n(n\u003e=1) n\u003e=3 n\u003e=3 n\u003e=4 2n\u003e=4 å¯ç”¨å®¹é‡ å…¨éƒ¨ 50% ï¼ˆn-1ï¼‰/n ï¼ˆn-1ï¼‰/n ï¼ˆn-2ï¼‰/n 50% 4. Raidçº§åˆ«ä»‹ç» 4.1. Raid0ï¼ˆä½¿ç”¨è¾ƒå¤šï¼‰ Raid0æ˜¯å°†å¤šå—ç‰©ç†ç£ç›˜ç»„åˆæˆä¸€ä¸ªå¤§çš„é€»è¾‘ç£ç›˜ï¼Œåœ¨è¯»å†™çš„æ—¶å€™ä¼šä»¥ç‹¬ç«‹çš„æ–¹å¼å¯¹Nå—ç£ç›˜è¿›è¡Œè¯»å†™ï¼Œå› æ­¤æ‰§è¡Œæ•ˆç‡éå¸¸é«˜ã€‚ç†è®ºä¸Šè¯»å†™çš„æ€§èƒ½æ˜¯å•å—ç£ç›˜çš„Nå€ã€‚ä½†æ˜¯å•å—ç£ç›˜çš„æŸåä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œæ— æ³•æ¢å¤ã€‚å› æ­¤é€‚ç”¨äºå¯¹å¯é æ€§ä¸é«˜ï¼Œè¯»å†™æ€§èƒ½è¦æ±‚é«˜çš„åœºæ™¯ä¸­ã€‚\nç‰¹ç‚¹ï¼šRAID 0é€šè¿‡æ¡å¸¦åŒ–æŠ€æœ¯ï¼Œå°†æ•°æ®åˆ†æ•£åˆ°å¤šä¸ªç£ç›˜ä¸Šï¼Œä»è€Œæé«˜å­˜å‚¨æ€§èƒ½ã€‚å®ƒä¸æä¾›æ•°æ®å†—ä½™ï¼Œå› æ­¤æ²¡æœ‰å®¹é”™èƒ½åŠ›ã€‚\né€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºå¯¹æ€§èƒ½è¦æ±‚æé«˜ï¼Œä½†å¯¹æ•°æ®å®‰å…¨æ€§è¦æ±‚ä¸é«˜çš„åœºæ™¯ï¼Œå¦‚è§†é¢‘ç¼–è¾‘ã€é«˜æ€§èƒ½è®¡ç®—ç­‰ã€‚\n4.2. Raid1ï¼ˆé•œåƒï¼‰ Raid1 æ˜¯ç£ç›˜é˜µåˆ—ä¸­å•ä½æˆæœ¬æœ€é«˜çš„ä¸€ç§æ–¹å¼ã€‚å› ä¸ºå®ƒçš„åŸç†æ˜¯åœ¨å¾€ç£ç›˜å†™æ•°æ®çš„æ—¶å€™ï¼Œå°†åŒä¸€ä»½æ•°æ®æ— å·®åˆ«çš„å†™ä¸¤ä»½åˆ°ç£ç›˜ï¼Œåˆ†åˆ«å†™åˆ°å·¥ä½œç£ç›˜å’Œé•œåƒç£ç›˜ï¼Œé‚£ä¹ˆå®ƒçš„å®é™…ç©ºé—´ä½¿ç”¨ç‡åªæœ‰50%äº†ï¼Œä¸¤å—ç£ç›˜å½“åšä¸€å—ç”¨ï¼Œè¿™æ˜¯ä¸€ç§æ¯”è¾ƒæ˜‚è´µçš„æ–¹æ¡ˆã€‚å¯é æ€§é«˜ï¼Œä½†æ˜¯æ€§èƒ½ä½ã€‚\nç‰¹ç‚¹ï¼šRAID 1é€šè¿‡é•œåƒæŠ€æœ¯ï¼Œå°†æ•°æ®åŒæ—¶å†™å…¥ä¸¤ä¸ªæˆ–å¤šä¸ªç›¸åŒçš„ç£ç›˜.ä¸­ï¼Œæä¾›æ•°æ®å†—ä½™å’Œå®¹é”™èƒ½åŠ›ã€‚å¦‚æœä¸€ä¸ªç£ç›˜å‘ç”Ÿæ•…éšœï¼Œå¦ä¸€ä¸ªç£ç›˜ä¸Šçš„æ•°æ®ä»ç„¶å¯ç”¨ã€‚\né€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºå¯¹æ•°æ®å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ï¼Œå¦‚é‡‘èæœºæ„ã€åŒ»ç–—æœºæ„ç­‰ã€‚\n4.3. Raid5ï¼ˆä½¿ç”¨æœ€å¤šï¼‰ Raid5æ˜¯å…¼å…·Raid0å’ŒRaid1çš„ä¼˜ç‚¹ï¼Œåœ¨æ€§èƒ½å’Œå¯é æ€§ä¸Šå…¼å…·çš„ä¸€ç§æ–¹å¼ï¼Œä¹Ÿæ˜¯ä½¿ç”¨æœ€å¤šçš„ä¸€ç§æ–¹æ¡ˆã€‚\nRaid5æ–¹æ¡ˆæ˜¯æ€»å…±æœ‰Nå—ç£ç›˜ï¼Œä¼šå°†è¦å†™å…¥çš„æ•°æ®åˆ†æˆNä»½ï¼Œå¹¶å‘çš„å†™å…¥åˆ°Nå—ç£ç›˜ä¸­ï¼ŒåŒæ—¶è¿˜å°†æ•°æ®çš„æ ¡éªŒç ä¿¡æ¯ä¹Ÿå†™å…¥åˆ°è¿™Nå—ç£ç›˜ä¸­ï¼ˆæ•°æ®ä¸å¯¹åº”çš„æ ¡éªŒç ä¿¡æ¯å¿…é¡»å¾—åˆ†å¼€å­˜å‚¨åœ¨ä¸åŒçš„ç£ç›˜ä¸Šï¼‰ã€‚ä¸€æ—¦æŸä¸€å—ç£ç›˜æŸåäº†ï¼Œå°±å¯ä»¥ç”¨å‰©ä¸‹çš„æ•°æ®å’Œå¯¹åº”çš„å¥‡å¶æ ¡éªŒç ä¿¡æ¯å»æ¢å¤æŸåçš„æ•°æ®ã€‚\nRAID5çš„æ–¹å¼ï¼Œæœ€å°‘éœ€è¦ä¸‰å—ç£ç›˜æ¥ç»„å»ºç£ç›˜é˜µåˆ—ï¼Œå…è®¸æœ€å¤šåŒæ—¶åä¸€å—ç£ç›˜ã€‚å¦‚æœæœ‰ä¸¤å—ç£ç›˜åŒæ—¶æŸåäº†ï¼Œé‚£æ•°æ®å°±æ— æ³•æ¢å¤äº†ã€‚\nç‰¹ç‚¹ï¼šRAID 5ç»“åˆäº†æ¡å¸¦åŒ–å’Œæ•°æ®å†—ä½™æŠ€æœ¯ã€‚å®ƒå°†æ•°æ®åˆ†å‰²æˆå—å¹¶åˆ†å¸ƒåœ¨å¤šä¸ªç£ç›˜ä¸Šï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªç£ç›˜å­˜å‚¨å†—ä½™ä¿¡æ¯(æ ¡éªŒå—)ã€‚å¦‚æœä¸€ä¸ªç£ç›˜å‘ç”Ÿæ•…éšœï¼Œç³»ç»Ÿå¯ä»¥ä½¿ç”¨å†—ä½™ä¿¡æ¯æ¥é‡å»ºæ•…éšœç£ç›˜ä¸Šçš„æ•°æ®ã€‚\né€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºå¯¹æ•°æ®å®‰å…¨æ€§å’Œæ€§èƒ½éƒ½æœ‰ä¸€å®šè¦æ±‚çš„åœºæ™¯ï¼Œå¦‚ä¼ä¸šæ•°æ®åº“ã€æ–‡ä»¶æœåŠ¡å™¨ç­‰ã€‚\n4.4. Raid6 RAID6åœ¨RAID5çš„åŸºç¡€ä¸Šå†æ¬¡æ”¹è¿›ï¼Œå¼•å…¥äº†åŒé‡æ ¡éªŒçš„æ¦‚å¿µã€‚RAID6é™¤äº†æ¯å—ç£ç›˜ä¸Šéƒ½æœ‰åŒçº§æ•°æ®XORæ ¡éªŒåŒºä»¥å¤–ï¼Œè¿˜æœ‰é’ˆå¯¹æ¯ä¸ªæ•°æ®å—çš„XORæ ¡éªŒåŒºï¼Œè¿™æ ·çš„è¯ï¼Œç›¸å½“äºæ¯ä¸ªæ•°æ®å—æœ‰ä¸¤ä¸ªæ ¡éªŒä¿æŠ¤æªæ–½ï¼Œå› æ­¤æ•°æ®çš„å†—ä½™æ€§æ›´é«˜äº†ã€‚ä½†æ˜¯RAID6çš„è¿™ç§è®¾è®¡ä¹Ÿå¸¦æ¥äº†å¾ˆé«˜çš„å¤æ‚åº¦ï¼Œè™½ç„¶æ•°æ®å†—ä½™æ€§å¥½ï¼Œè¯»å–çš„æ•ˆç‡ä¹Ÿæ¯”è¾ƒé«˜ï¼Œä½†æ˜¯å†™æ•°æ®çš„æ€§èƒ½å°±å¾ˆå·®ã€‚å› æ­¤RAID6åœ¨å®é™…ç¯å¢ƒä¸­åº”ç”¨çš„æ¯”è¾ƒå°‘ã€‚\nç‰¹ç‚¹ï¼šRAID 6ä¸RAID 5ç±»ä¼¼ï¼Œä½†ä½¿ç”¨äº†ä¸¤ä¸ªå†—ä½™ä¿¡æ¯æ¥æä¾›æ›´é«˜çš„å®¹é”™èƒ½åŠ›ã€‚è¿™æ„å‘³ç€å³ä½¿ä¸¤ä¸ªç£ç›˜åŒæ—¶å‘ç”Ÿæ•…éšœï¼Œç³»ç»Ÿä»ç„¶å¯ä»¥ä½¿ç”¨å†—ä½™ä¿¡æ¯æ¥é‡å»ºæ•°æ®ã€‚\né€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºå¯¹æ•°æ®å®‰å…¨æ€§è¦æ±‚éå¸¸é«˜çš„åœºæ™¯ï¼Œå¦‚å¤§å‹æ•°æ®åº“ã€å…³é”®ä¸šåŠ¡ç³»ç»Ÿç­‰ã€‚\n4.5. Raid10 RAID10å…¶å®å°±æ˜¯RAID1ä¸RAID0çš„ä¸€ä¸ªåˆä½“ã€‚RAID10å…¼å¤‡äº†RAID1å’ŒRAID0çš„æœ‰ä¼˜ç‚¹ã€‚\né¦–å…ˆåŸºäºRAID1æ¨¡å¼å°†ç£ç›˜åˆ†ä¸º2ä»½ï¼Œå½“è¦å†™å…¥æ•°æ®çš„æ—¶å€™ï¼Œå°†æ‰€æœ‰çš„æ•°æ®åœ¨ä¸¤ä»½ç£ç›˜ä¸ŠåŒæ—¶å†™å…¥ï¼Œç›¸å½“äºå†™äº†åŒä»½æ•°æ®ï¼Œèµ·åˆ°äº†æ•°æ®ä¿éšœçš„ä½œç”¨ã€‚ä¸”åœ¨æ¯ä¸€ä»½ç£ç›˜ä¸Šåˆä¼šåŸºäºRAID0æŠ€æœ¯è®²æ•°æ®åˆ†ä¸ºNä»½å¹¶å‘çš„è¯»å†™ï¼Œè¿™æ ·ä¹Ÿä¿éšœäº†æ•°æ®çš„æ•ˆç‡ã€‚RAID10æ¨¡å¼æ˜¯æœ‰ä¸€åŠçš„ç£ç›˜ç©ºé—´ç”¨äºå­˜å‚¨å†—ä½™æ•°æ®çš„ï¼Œæµªè´¹çš„å¾ˆä¸¥é‡ï¼Œå› æ­¤ç”¨çš„ä¹Ÿä¸æ˜¯å¾ˆå¤šã€‚\n5. ç¡¬Raidå’Œè½¯RaidåŒºåˆ« ç¡¬raidä¸»è¦æ˜¯é€šè¿‡å‚å•†çš„ç¡¬ä»¶raidå¡æ§åˆ¶å™¨æ¥å®ç°ï¼Œè€Œè½¯raidä¸»è¦æ˜¯æ“ä½œç³»ç»Ÿçš„raidå‘½ä»¤æ¥å®ç°ï¼ˆä¾‹å¦‚ mdadm å‘½ä»¤ï¼‰ã€‚\nä»¥ä¸‹æ˜¯2è€…çš„åŒºåˆ«ï¼š\nåˆ†ç±» ç¡¬raid è½¯raid å®ç°æ–¹å¼ ç¡¬ä»¶ RAID ä½¿ç”¨ä¸“ç”¨çš„ RAID æ§åˆ¶å™¨å¡ï¼Œè¿™æ˜¯ä¸€å—ç‹¬ç«‹çš„ç¡¬ä»¶ï¼Œé€šå¸¸æ’å…¥åˆ°æœåŠ¡å™¨æˆ–å·¥ä½œç«™çš„ PCIe æ’æ§½ä¸­ã€‚è¯¥æ§åˆ¶å™¨å¤„ç†æ‰€æœ‰ RAID è®¡ç®—å’Œç£ç›˜ç®¡ç†ä»»åŠ¡ï¼Œå‡è½»äº†ä¸»æœº CPU çš„è´Ÿæ‹…ã€‚ è½¯ä»¶ RAID é€šè¿‡æ“ä½œç³»ç»Ÿæä¾›çš„ RAID åŠŸèƒ½æ¥å®ç°ï¼ˆå¦‚ Linux çš„ mdadm æˆ– Windows çš„ Storage Spacesï¼‰ã€‚RAID æ“ä½œç”±ä¸»æœºçš„ CPU å¤„ç†ã€‚ æ€§èƒ½ ç¡¬ä»¶ RAID æ§åˆ¶å™¨æœ‰ä¸“ç”¨çš„å¤„ç†å™¨å’Œç¼“å­˜ï¼Œç”¨äºå¤„ç† RAID æ“ä½œï¼Œå› æ­¤é€šå¸¸æä¾›æ›´é«˜çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜è´Ÿè½½æˆ–å¤æ‚ RAID é…ç½®ï¼ˆå¦‚ RAID 5 æˆ– RAID 6ï¼‰ä¸‹ã€‚ ç”±äº RAID æ“ä½œç”±ä¸»æœº CPU å¤„ç†ï¼Œè½¯ä»¶ RAID åœ¨é«˜è´Ÿè½½æˆ–å¤æ‚ RAID é…ç½®ä¸‹çš„æ€§èƒ½å¯èƒ½ä¸å¦‚ç¡¬ä»¶ RAIDã€‚ ç³»ç»Ÿä¾èµ–æ€§ RAID é…ç½®å­˜å‚¨åœ¨æ§åˆ¶å™¨ä¸Šï¼Œå› æ­¤æ›´æ¢ä¸»æœºç³»ç»Ÿæˆ–æ“ä½œç³»ç»Ÿä¸ä¼šå½±å“ RAID é˜µåˆ—çš„æ•°æ®å®Œæ•´æ€§ã€‚ RAID é…ç½®ä¾èµ–äºæ“ä½œç³»ç»Ÿï¼Œå› æ­¤æ›´æ¢æ“ä½œç³»ç»Ÿæˆ–é‡æ–°å®‰è£…ç³»ç»Ÿå¯èƒ½éœ€è¦é‡æ–°é…ç½® RAID é˜µåˆ—ã€‚ æˆæœ¬ ç¡¬ä»¶ RAID æ§åˆ¶å™¨é€šå¸¸ä»·æ ¼è¾ƒé«˜ï¼Œå¢åŠ äº†ç³»ç»Ÿçš„æ€»ä½“æˆæœ¬ã€‚ ä¸éœ€è¦é¢å¤–çš„ç¡¬ä»¶ï¼Œé™ä½äº†æˆæœ¬ã€‚å¯¹äºé¢„ç®—æœ‰é™çš„å°å‹ä¼ä¸šæˆ–ä¸ªäººç”¨æˆ·æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªç»æµçš„é€‰æ‹©ã€‚ æ•°æ®å®‰å…¨ä¸å¯é æ€§ ç¡¬ä»¶ RAID æä¾›æ›´é«˜çš„å¯é æ€§å’Œæ•°æ®ä¿æŠ¤åŠŸèƒ½ï¼Œå¦‚çƒ­å¤‡ç”¨ç›˜å’Œç”µæ± å¤‡ä»½ç¼“å­˜ã€‚ è½¯ä»¶ RAID åœ¨æ•°æ®å®‰å…¨å’Œå¯é æ€§ä¸Šä¾èµ–äºæ“ä½œç³»ç»Ÿçš„å®ç°å’Œé…ç½®ã€‚ çµæ´»æ€§ ä¾èµ–ä¸åŒå‚å•†ä¸åŒçš„raidå¡å‘½ä»¤ï¼Œé…ç½®æ¯”è¾ƒå¤æ‚ è½¯ä»¶ RAID é…ç½®å’Œç®¡ç†æ›´çµæ´»ï¼Œå¯ä»¥é€šè¿‡æ“ä½œç³»ç»Ÿçš„å‘½ä»¤è¡Œå·¥å…·æˆ–å›¾å½¢ç•Œé¢è½»æ¾å®ç°ã€‚ é«˜çº§åŠŸèƒ½ ç¡¬ä»¶ RAID æ§åˆ¶å™¨é€šå¸¸æä¾›é«˜çº§åŠŸèƒ½ï¼Œå¦‚çƒ­æ’æ‹”ã€åœ¨çº¿æ‰©å±•ã€çƒ­å¤‡ç”¨ç›˜ã€ç¡¬ä»¶çº§åˆ«çš„åŠ å¯†å’Œç”µæ± å¤‡ä»½ç¼“å­˜ã€‚ è½¯ä»¶ RAID é€šå¸¸ç¼ºä¹ä¸€äº›é«˜çº§åŠŸèƒ½ï¼Œå¦‚ç¡¬ä»¶ RAID æä¾›çš„ç”µæ± å¤‡ä»½ç¼“å­˜å’Œä¸“ç”¨å¤„ç†å™¨ã€‚ 6. å¦‚ä½•é…ç½®ç¡¬Raid é…ç½®ç¡¬ä»¶raidå¯ä»¥é€šè¿‡raidé…ç½®ç•Œé¢æ“ä½œï¼Œä¹Ÿå¯ä»¥é€šè¿‡å‚å•†æä¾›çš„cliå·¥å…·é…ç½®ç¡¬Raidã€‚å…·ä½“å¯å‚è€ƒåˆ›å»ºç¡¬ä»¶Raid\nå‚è€ƒï¼š\nhttps://www.chinastor.com/baike/raid/ https://zhuanlan.zhihu.com/p/51170719 https://mp.weixin.qq.com/s/xWD5p7pmzdl_YpMXY5YO4g ","categories":"","description":"","excerpt":"1. Raidæ˜¯ä»€ä¹ˆ RAIDè‹±æ–‡å…¨ç§°Redundant Array of Independent Diskï¼Œç¿»è¯‘è¿‡æ¥å°±æ˜¯â€œç‹¬ç«‹ç£ç›˜å†—ä½™ç³» â€¦","ref":"/linux-notes/disk/disk-raid/","tags":["disk"],"title":"Raidä»‹ç»"},{"body":"1. Tunnel-Agentç®€ä»‹ tunnel-agentæ˜¯é€šè¿‡daemonsetéƒ¨ç½²åœ¨æ¯ä¸ªworkerèŠ‚ç‚¹ï¼Œé€šè¿‡grpcåè®®ä¸äº‘ç«¯çš„tunnel-serverå»ºç«‹è¿æ¥ã€‚ä»¥ä¸‹åˆ†ætunnel-agentçš„æºç é€»è¾‘ã€‚\nå¸¸ç”¨çš„å¯åŠ¨å‚æ•°ï¼š\n- args: - --node-name=$(NODE_NAME) - --node-ip=$(POD_IP) - --tunnelserver-addr=tunnel-server-address[ip:port] - --v=2 command: - yurt-tunnel-agent 2. NewYurttunnelAgentCommand NewYurttunnelAgentCommandè¿˜æ˜¯å¸¸ç”¨å‘½ä»¤ä»£ç ä¸‰æ¿æ–§ï¼Œæ­¤å¤„ä¸åšå±•å¼€ï¼Œç›´æ¥åˆ†æRunå‡½æ•°ã€‚\n// NewYurttunnelAgentCommand creates a new yurttunnel-agent command func NewYurttunnelAgentCommand(stopCh \u003c-chan struct{}) *cobra.Command { agentOptions := options.NewAgentOptions() // å·²ç»åˆ é™¤éé‡è¦çš„ä»£ç  cmd := \u0026cobra.Command{ RunE: func(c *cobra.Command, args []string) error { cfg, err := agentOptions.Config() if err := Run(cfg.Complete(), stopCh); err != nil { return err } }, } agentOptions.AddFlags(cmd.Flags()) return cmd } 3. Run Runå‡½æ•°å³å¯åŠ¨ä¸€ä¸ªagentæœåŠ¡ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\nå…ˆè·å–é…ç½®é¡¹tunnelserver-addrä¸­çš„åœ°å€ï¼Œå¦‚æœåœ°å€ä¸å­˜åœ¨ï¼Œåˆ™è·å–x-tunnel-server-svcçš„service åœ°å€ã€‚ï¼ˆè¯´æ˜ï¼šä¸€èˆ¬æƒ…å†µä¸‹ï¼Œtunnel-serverè·Ÿagentä¸åœ¨åŒä¸€ä¸ªç½‘ç»œåŸŸï¼Œå› æ­¤ç½‘ç»œä¼šä¸é€šï¼Œæ‰€ä»¥ä¸€èˆ¬éœ€è¦é…ç½®ç‹¬ç«‹ä¸”å¯è¿é€šçš„åœ°å€ï¼Œå¯ä»¥é€šè¿‡Nginxè½¬å‘ï¼‰\nagenté€šè¿‡hostçš„ç½‘ç»œæ¨¡å¼è¿è¡Œåœ¨å®¿ä¸»æœºä¸Šï¼Œå¯åŠ¨è¯ä¹¦managerã€‚å¹¶ç­‰å¾…è¯ä¹¦ç”Ÿæˆã€‚\nç”Ÿæˆè¿æ¥tunnel-serverçš„è¯ä¹¦ã€‚\nå¯åŠ¨ yurttunnel-agentã€‚\nå¯åŠ¨meta serverã€‚\n// Run starts the yurttunel-agent func Run(cfg *config.CompletedConfig, stopCh \u003c-chan struct{}) error { // 1. get the address of the yurttunnel-server tunnelServerAddr = cfg.TunnelServerAddr if tunnelServerAddr == \"\" { if tunnelServerAddr, err = serveraddr.GetTunnelServerAddr(cfg.Client); err != nil { return err } } // 2. create a certificate manager // As yurttunnel-agent will run on the edge node with Host network mode, // we can use the status.podIP as the node IP nodeIP := os.Getenv(constants.YurttunnelAgentPodIPEnv) agentCertMgr, err = certfactory.NewCertManagerFactory(cfg.Client).New(\u0026certfactory.CertManagerConfig{ ComponentName: projectinfo.GetAgentName(), CertDir: cfg.CertDir, SignerName: certificatesv1.KubeAPIServerClientSignerName, CommonName: constants.YurtTunnelAgentCSRCN, Organizations: []string{constants.YurtTunnelCSROrg}, DNSNames: []string{os.Getenv(\"NODE_NAME\")}, IPs: []net.IP{net.ParseIP(nodeIP)}, }) agentCertMgr.Start() // 2.1. waiting for the certificate is generated _ = wait.PollUntil(5*time.Second, func() (bool, error) { if agentCertMgr.Current() != nil { return true, nil } klog.Infof(\"certificate %s not signed, waiting...\", projectinfo.GetAgentName()) return false, nil }, stopCh) // 3. generate a TLS configuration for securing the connection to server tlsCfg, err := certmanager.GenTLSConfigUseCertMgrAndCA(agentCertMgr, tunnelServerAddr, constants.YurttunnelCAFile) // 4. start the yurttunnel-agent ta := agent.NewTunnelAgent(tlsCfg, tunnelServerAddr, cfg.NodeName, cfg.AgentIdentifiers) ta.Run(stopCh) // 5. start meta server util.RunMetaServer(cfg.AgentMetaAddr) \u003c-stopCh return nil } 4. TunnelAgent TunnelAgentä¸tunnel-serverå»ºç«‹tunnelï¼Œæ¥æ”¶serverçš„è¯·æ±‚ï¼Œå¹¶è½¬å‘ç»™kubeletã€‚\n// TunnelAgent sets up tunnel to TunnelServer, receive requests // from tunnel, and forwards requests to kubelet type TunnelAgent interface { Run(\u003c-chan struct{}) } // NewTunnelAgent generates a new TunnelAgent func NewTunnelAgent(tlsCfg *tls.Config, tunnelServerAddr, nodeName, agentIdentifiers string) TunnelAgent { ata := anpTunnelAgent{ tlsCfg: tlsCfg, tunnelServerAddr: tunnelServerAddr, nodeName: nodeName, agentIdentifiers: agentIdentifiers, } return \u0026ata } 5. anpTunnelAgent.Run anpTunnelAgentä½¿ç”¨apiserver-network-proxyåŒ…æ¥å®ç°tunnelé€»è¾‘ã€‚é¡¹ç›®å…·ä½“å‚è€ƒï¼šhttps://github.com/kubernetes-sigs/apiserver-network-proxy)\nä»£ç ï¼š/pkg/yurttunnel/agent/anpagent.go\n// RunAgent runs the yurttunnel-agent which will try to connect yurttunnel-server func (ata *anpTunnelAgent) Run(stopChan \u003c-chan struct{}) { dialOption := grpc.WithTransportCredentials(credentials.NewTLS(ata.tlsCfg)) cc := \u0026anpagent.ClientSetConfig{ Address: ata.tunnelServerAddr, // æŒ‡å®šåå‘è¿æ¥çš„ç›®æ ‡åœ°å€ AgentID: ata.nodeName, AgentIdentifiers: ata.agentIdentifiers, SyncInterval: 5 * time.Second, ProbeInterval: 5 * time.Second, DialOptions: []grpc.DialOption{dialOption}, ServiceAccountTokenPath: \"\", } // è°ƒç”¨apiserver-network-proxyçš„åŒ…æ¥åˆ›å»ºåŒå‘çš„grpcè¿æ¥ã€‚ cs := cc.NewAgentClientSet(stopChan) cs.Serve() klog.Infof(\"start serving grpc request redirected from %s: %s\", projectinfo.GetServerName(), ata.tunnelServerAddr) } ä»¥ä¸‹æ˜¯apiserver-network-proxyçš„æºç åˆ†æã€‚\n6. apiserver-network-proxy.Clientåˆ†æ å…·ä½“ä»£ç å‚è€ƒï¼š\nhttps://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/clientset.go é€šè¿‡NewAgentClientSetåˆ›å»ºä¸€ä¸ªclientç»“æ„ä½“ã€‚\nfunc (cc *ClientSetConfig) NewAgentClientSet(stopCh \u003c-chan struct{}) *ClientSet { return \u0026ClientSet{ clients: make(map[string]*Client), agentID: cc.AgentID, agentIdentifiers: cc.AgentIdentifiers, address: cc.Address, syncInterval: cc.SyncInterval, probeInterval: cc.ProbeInterval, dialOptions: cc.DialOptions, serviceAccountTokenPath: cc.ServiceAccountTokenPath, stopCh: stopCh, } } 6.1. client.Serve client.Serveè¿è¡Œä¸€ä¸ªsyncçš„goroutineçš„å¸¸é©»è¿›ç¨‹ï¼Œå†è°ƒç”¨syncOnceå‡½æ•°ã€‚\n// è¿è¡Œä¸€ä¸ªsyncçš„goroutine func (cs *ClientSet) Serve() { go cs.sync() } // sync makes sure that #clients \u003e= #proxy servers func (cs *ClientSet) sync() { defer cs.shutdown() backoff := cs.resetBackoff() var duration time.Duration for { if err := cs.syncOnce(); err != nil { klog.ErrorS(err, \"cannot sync once\") duration = backoff.Step() } else { backoff = cs.resetBackoff() duration = wait.Jitter(backoff.Duration, backoff.Jitter) } time.Sleep(duration) select { case \u003c-cs.stopCh: return default: } } } syncOnceè¿è¡Œäº†çœŸæ­£æ‰§è¡Œgrpcé€šä¿¡çš„clientã€‚\nfunc (cs *ClientSet) syncOnce() error { if cs.serverCount != 0 \u0026\u0026 cs.ClientsCount() \u003e= cs.serverCount { return nil } // åˆ›å»ºå°è£…çš„grpc client c, serverCount, err := cs.newAgentClient() if err != nil { return err } if cs.serverCount != 0 \u0026\u0026 cs.serverCount != serverCount { klog.V(2).InfoS(\"Server count change suggestion by server\", \"current\", cs.serverCount, \"serverID\", c.serverID, \"actual\", serverCount) } cs.serverCount = serverCount if err := cs.AddClient(c.serverID, c); err != nil { klog.ErrorS(err, \"closing connection failure when adding a client\") c.Close() return nil } klog.V(2).InfoS(\"sync added client connecting to proxy server\", \"serverID\", c.serverID) // è¿è¡Œå°è£…åçš„grpc è¿æ¥ go c.Serve() return nil } 7. Grpc client ä»£ç å‚è€ƒï¼š\nhttps://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/client.go 7.1. newAgentClient newAgentClientåˆå§‹åŒ–ä¸€ä¸ªgrpc clientï¼Œå¹¶å¯åŠ¨è¿æ¥ã€‚\nfunc newAgentClient(address, agentID, agentIdentifiers string, cs *ClientSet, opts ...grpc.DialOption) (*Client, int, error) { a := \u0026Client{ cs: cs, address: address, agentID: agentID, agentIdentifiers: agentIdentifiers, opts: opts, probeInterval: cs.probeInterval, stopCh: make(chan struct{}), serviceAccountTokenPath: cs.serviceAccountTokenPath, connManager: newConnectionManager(), } // å¯åŠ¨clientçš„è¿æ¥ serverCount, err := a.Connect() if err != nil { return nil, 0, err } return a, serverCount, nil } 7.2. connect Connectä½¿grpcè¿æ¥ä»£ç†æœåŠ¡å™¨ã€‚å®ƒè¿”å›æœåŠ¡å™¨ID\n// Connect makes the grpc dial to the proxy server. It returns the serverID // it connects to. func (a *Client) Connect() (int, error) { // è¿è¡Œgrpc dialè¿æ¥ conn, err := grpc.Dial(a.address, a.opts...) if err != nil { return 0, err } // å·²åˆ é™¤éå¿…è¦çš„ä»£ç  // åˆ›å»ºstream stream, err := agent.NewAgentServiceClient(conn).Connect(ctx) if err != nil { conn.Close() /* #nosec G104 */ return 0, err } serverID, err := serverID(stream) if err != nil { conn.Close() /* #nosec G104 */ return 0, err } serverCount, err := serverCount(stream) if err != nil { conn.Close() /* #nosec G104 */ return 0, err } a.conn = conn a.stream = stream a.serverID = serverID klog.V(2).InfoS(\"Connect to\", \"server\", serverID) return serverCount, nil } 7.3. Serve() Serveä¸»è¦å¯åŠ¨grpcåŒå‘ä¼ è¾“é€šé“çš„goroutine, ä¸»è¦åŒ…æ‹¬ sendï¼ˆå‘ï¼‰å’Œrecvï¼ˆæ”¶ï¼‰2ä¸ªæ“ä½œã€‚\nfunc (a *Client) Serve() { // å·²ç»åˆ é™¤æ¬¡è¦ä»£ç  for { // æ”¶åŒ… pkt, err := a.Recv() klog.V(5).InfoS(\"[tracing] recv packet\", \"type\", pkt.Type) // æ ¹æ®ä¸åŒåŒ…ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç† switch pkt.Type { case client.PacketType_DIAL_REQ: resp := \u0026client.Packet{ Type: client.PacketType_DIAL_RSP, Payload: \u0026client.Packet_DialResponse{DialResponse: \u0026client.DialResponse{}}, } if err := a.Send(resp); err != nil { } // è¿è¡Œproxy go a.remoteToProxy(connID, ctx) go a.proxyToRemote(connID, ctx) // æ¥æ”¶åˆ°æ•°æ® case client.PacketType_DATA: data := pkt.GetData() ctx, ok := a.connManager.Get(data.ConnectID) if ok { ctx.dataCh \u003c- data.Data } case client.PacketType_CLOSE_REQ: // å·²åˆ é™¤ } } } 8. remoteToProxyå’ŒproxyToRemote remoteToProxy\nfunc (a *Client) remoteToProxy(connID int64, ctx *connContext) { defer ctx.cleanup() var buf [1 \u003c\u003c 12]byte resp := \u0026client.Packet{ Type: client.PacketType_DATA, } for { n, err := ctx.conn.Read(buf[:]) klog.V(4).InfoS(\"received data from remote\", \"bytes\", n, \"connID\", connID) // åˆ é™¤æ¬¡è¦ä»£ç  resp.Payload = \u0026client.Packet_Data{Data: \u0026client.Data{ Data: buf[:n], ConnectID: connID, }} if err := a.Send(resp); err != nil { klog.ErrorS(err, \"stream send failure\", \"connID\", connID) } } } } proxyToRemote\nfunc (a *Client) proxyToRemote(connID int64, ctx *connContext) { defer ctx.cleanup() for d := range ctx.dataCh { pos := 0 for { n, err := ctx.conn.Write(d[pos:]) if err == nil { klog.V(4).InfoS(\"write to remote\", \"connID\", connID, \"lastData\", n) break } else if n \u003e 0 { klog.ErrorS(err, \"write to remote with failure\", \"connID\", connID, \"lastData\", n) pos += n } else { if !strings.Contains(err.Error(), \"use of closed network connection\") { klog.ErrorS(err, \"conn write failure\", \"connID\", connID) } return } } } } 9. Recv() å’ŒSend() func (a *Client) Send(pkt *client.Packet) error { a.sendLock.Lock() defer a.sendLock.Unlock() err := a.stream.Send(pkt) if err != nil \u0026\u0026 err != io.EOF { metrics.Metrics.ObserveFailure(metrics.DirectionToServer) a.cs.RemoveClient(a.serverID) } return err } func (a *Client) Recv() (*client.Packet, error) { a.recvLock.Lock() defer a.recvLock.Unlock() pkt, err := a.stream.Recv() if err != nil \u0026\u0026 err != io.EOF { metrics.Metrics.ObserveFailure(metrics.DirectionFromServer) a.cs.RemoveClient(a.serverID) } return pkt, err } 10. æ€»ç»“ Tunnel-agentæœ¬è´¨æ˜¯å°è£…äº†apiserver-network-proxyåº“ï¼Œæœ€ç»ˆè¿è¡Œä¸€ä¸ªgrpcçš„åŒå‘æ”¶å‘æ•°æ®åŒ…çš„é€šé“ï¼Œæ‰€ä»¥**æœ¬è´¨ä¸Štunnelæ˜¯é€šè¿‡grpcåå‘å»ºç«‹è¿æ¥ï¼Œå¹¶å®ç°åŒå‘é€šä¿¡çš„èƒ½åŠ›ã€‚**å› æ­¤è¯¥åå‘éš§é“èƒ½åŠ›çš„ä¹Ÿå¯ä»¥é€šè¿‡å…¶ä»–åŒå‘é€šä¿¡çš„åè®®æ¥å®ç°ï¼Œä¾‹å¦‚websocketï¼ˆç±»ä¼¼kubeedgeé€šè¿‡websocketæ¥å®ç°åå‘éš§é“ï¼‰ã€‚\nå‚è€ƒï¼š\n/pkg/yurttunnel/agent/anpagent.go https://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/client.go https://github.com/kubernetes-sigs/apiserver-network-proxy/blob/master/pkg/agent/clientset.go ","categories":"","description":"","excerpt":"1. Tunnel-Agentç®€ä»‹ tunnel-agentæ˜¯é€šè¿‡daemonsetéƒ¨ç½²åœ¨æ¯ä¸ªworkerèŠ‚ç‚¹ï¼Œé€šè¿‡grpcåè®®ä¸äº‘ç«¯ â€¦","ref":"/kubernetes-notes/edge/openyurt/code-analysis/tennel-agent-code-analysis/","tags":["OpenYurt"],"title":"OpenYurtä¹‹Tunnel-Agentæºç åˆ†æ"},{"body":"1. Podçš„DNSç­–ç•¥ å¯ä»¥åœ¨podä¸­å®šä¹‰dnsPolicyå­—æ®µæ¥è®¾ç½®dnsçš„ç­–ç•¥ã€‚\n\"Default\": Pod ä»è¿è¡Œæ‰€åœ¨çš„èŠ‚ç‚¹ç»§æ‰¿åç§°è§£æé…ç½®ã€‚å°±æ˜¯è¯¥Podçš„DNSé…ç½®ä¼šè·Ÿå®¿ä¸»æœºå®Œå…¨ä¸€è‡´ã€‚\n\"ClusterFirst\": å¦‚æœæ²¡æœ‰é…ç½®ï¼Œå³ä¸ºé»˜è®¤çš„DNSç­–ç•¥ã€‚é¢„å…ˆæŠŠkube-dnsï¼ˆæˆ–CoreDNSï¼‰çš„ä¿¡æ¯å½“ä½œé¢„è®¾å‚æ•°å†™å…¥åˆ°è¯¥Podå†…çš„DNSé…ç½®ã€‚ä¸é…ç½®çš„é›†ç¾¤åŸŸåç¼€ä¸åŒ¹é…çš„ä»»ä½• DNS æŸ¥è¯¢ï¼ˆä¾‹å¦‚ \"www.kubernetes.io\"ï¼‰ éƒ½ä¼šç”± DNS æœåŠ¡å™¨è½¬å‘åˆ°ä¸Šæ¸¸åç§°æœåŠ¡å™¨ã€‚\n\"ClusterFirstWithHostNet\": å¯¹äºä»¥ hostNetwork æ–¹å¼è¿è¡Œçš„ Podï¼Œåº”å°†å…¶ DNS ç­–ç•¥æ˜¾å¼è®¾ç½®ä¸º \"ClusterFirstWithHostNet\"ã€‚å¦åˆ™ï¼Œä»¥ hostNetwork æ–¹å¼å’ŒÂ \"ClusterFirst\"Â ç­–ç•¥è¿è¡Œçš„ Pod å°†ä¼šåšå‡ºå›é€€è‡³Â \"Default\"Â ç­–ç•¥çš„è¡Œä¸ºã€‚\n\"None\": æ­¤è®¾ç½®å…è®¸ Pod å¿½ç•¥ Kubernetes ç¯å¢ƒä¸­çš„ DNS è®¾ç½®ã€‚Pod ä¼šä½¿ç”¨å…¶Â dnsConfigÂ å­—æ®µæ‰€æä¾›çš„ DNS è®¾ç½®ã€‚\n2. Pod DNSçš„é…ç½® å½“ Pod çš„Â dnsPolicyÂ è®¾ç½®ä¸º \"None\" æ—¶ï¼Œå¿…é¡»æŒ‡å®šÂ dnsConfigÂ å­—æ®µã€‚\ndnsConfigÂ å­—æ®µä¸­å±æ€§ï¼š\nnameserversï¼šå°†ç”¨ä½œäº Pod çš„ DNS æœåŠ¡å™¨çš„ IP åœ°å€åˆ—è¡¨ã€‚ æœ€å¤šå¯ä»¥æŒ‡å®š 3 ä¸ª IP åœ°å€ã€‚ä¾‹å¦‚ corednsçš„Cluster IPã€‚\nsearchesï¼šç”¨äºåœ¨ Pod ä¸­æŸ¥æ‰¾ä¸»æœºåçš„ DNS æœç´¢åŸŸçš„åˆ—è¡¨ã€‚æ­¤å±æ€§æ˜¯å¯é€‰çš„ã€‚\noptionsï¼šå¯é€‰çš„å¯¹è±¡åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è±¡å¯èƒ½å…·æœ‰Â nameÂ å±æ€§ï¼ˆå¿…éœ€ï¼‰å’ŒÂ valueÂ å±æ€§ï¼ˆå¯é€‰ï¼‰ã€‚\nç¤ºä¾‹ï¼š\napiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \"None\" dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster-domain.example - my.dns.search.suffix options: - name: ndots value: \"2\" - name: edns0 é€šè¿‡ä»¥ä¸Šé…ç½®ï¼Œå®¹å™¨å†…çš„/etc/resolv.confæ–‡ä»¶å†…å®¹ä¸ºï¼š\nkubectl exec -it dns-example -- cat /etc/resolv.conf nameserver 1.2.3.4 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0 3. è‡ªå®šä¹‰DNSæœåŠ¡ é»˜è®¤ä¸€èˆ¬ä½¿ç”¨corednsæ¥ä½œä¸ºk8sçš„dnsæœåŠ¡å™¨ã€‚é»˜è®¤ä½¿ç”¨deploymentçš„æ–¹å¼æ¥è¿è¡Œcorednsï¼Œä¼šåˆ›å»ºä¸€ä¸ªåä¸ºkube-dnsçš„serviceï¼Œå¹¶ç”¨ClusterIPï¼ˆé»˜è®¤ä¸º10.96.0.10ï¼‰æ¥ä½œä¸ºé›†ç¾¤å†…çš„podçš„nameserverã€‚\nkubelet ä½¿ç”¨Â --cluster-dns=\u003cDNS æœåŠ¡ IP\u003eÂ æ ‡å¿—å°† DNS è§£æå™¨çš„ä¿¡æ¯ä¼ é€’ç»™æ¯ä¸ªå®¹å™¨ã€‚ä½¿ç”¨Â --cluster-domain=\u003cé»˜è®¤æœ¬åœ°åŸŸå\u003eÂ æ ‡å¿—é…ç½®æœ¬åœ°åŸŸåã€‚\nå¯æŸ¥çœ‹é»˜è®¤é…ç½®ï¼š\n# cat /var/lib/kubelet/config.yaml ... clusterDNS: - 10.96.0.10 clusterDomain: cluster.local æ€»ç»“ï¼š\nå½“æ²¡æœ‰ç»™podè®¾ç½®ä»»ä½•dnsç­–ç•¥æ—¶ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ClusterFirstçš„ç­–ç•¥ï¼Œå³nameserverçš„IPä¸ºcorednsçš„ClusterIPã€‚é€šè¿‡corednsæ¥è§£ææœåŠ¡ã€‚\n3.1. é…ç½®ç»§æ‰¿èŠ‚ç‚¹çš„DNSè§£æ å¦‚æœ Pod çš„Â dnsPolicyÂ è®¾ç½®ä¸ºÂ defaultï¼Œåˆ™å®ƒå°†ä» Pod è¿è¡Œæ‰€åœ¨èŠ‚ç‚¹ç»§æ‰¿åç§°è§£æé…ç½®ã€‚\nä½¿ç”¨ kubelet çš„Â --resolv-confÂ æ ‡å¿—è®¾ç½®ä¸ºå®¿ä¸»æœºçš„/etc/resolv.confæ–‡ä»¶ã€‚\n3.2. é…ç½®CoreDNS é…ç½®\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefi: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } é…ç½®è¯´æ˜ï¼š\nCorefile é…ç½®åŒ…æ‹¬ä»¥ä¸‹ CoreDNSÂ æ’ä»¶ï¼š\nerrorsï¼šé”™è¯¯è®°å½•åˆ°æ ‡å‡†è¾“å‡ºã€‚\nhealthï¼šåœ¨Â http://localhost:8080/healthÂ å¤„æä¾› CoreDNS çš„å¥åº·æŠ¥å‘Šã€‚ åœ¨è¿™ä¸ªæ‰©å±•è¯­æ³•ä¸­ï¼ŒlameduckÂ ä¼šä½¿æ­¤è¿›ç¨‹ä¸å¥åº·ï¼Œç­‰å¾… 5 ç§’åè¿›ç¨‹è¢«å…³é—­ã€‚\nreadyï¼šåœ¨ç«¯å£ 8181 ä¸Šæä¾›çš„ä¸€ä¸ª HTTP ç«¯ç‚¹ï¼Œ å½“æ‰€æœ‰èƒ½å¤Ÿè¡¨è¾¾è‡ªèº«å°±ç»ªçš„æ’ä»¶éƒ½å·²å°±ç»ªæ—¶ï¼Œåœ¨æ­¤ç«¯ç‚¹è¿”å› 200 OKã€‚\nkubernetesï¼šCoreDNS å°†åŸºäºæœåŠ¡å’Œ Pod çš„ IP æ¥åº”ç­” DNS æŸ¥è¯¢ã€‚ ä½ å¯ä»¥åœ¨ CoreDNS ç½‘ç«™æ‰¾åˆ°æœ‰å…³æ­¤æ’ä»¶çš„æ›´å¤šç»†èŠ‚ã€‚\nä½ å¯ä»¥ä½¿ç”¨Â ttlÂ æ¥å®šåˆ¶å“åº”çš„ TTLã€‚é»˜è®¤å€¼æ˜¯ 5 ç§’é’Ÿã€‚TTL çš„æœ€å°å€¼å¯ä»¥æ˜¯ 0 ç§’é’Ÿï¼Œ æœ€å¤§å€¼ä¸º 3600 ç§’ã€‚å°† TTL è®¾ç½®ä¸º 0 å¯ä»¥ç¦æ­¢å¯¹ DNS è®°å½•è¿›è¡Œç¼“å­˜ã€‚\npods insecureÂ é€‰é¡¹æ˜¯ä¸ºäº†ä¸ kube-dns å‘åå…¼å®¹ã€‚\nä½ å¯ä»¥ä½¿ç”¨Â pods verifiedÂ é€‰é¡¹ï¼Œè¯¥é€‰é¡¹ä½¿å¾—ä»…åœ¨ç›¸åŒåå­—ç©ºé—´ä¸­å­˜åœ¨å…·æœ‰åŒ¹é… IP çš„ Pod æ—¶æ‰è¿”å› A è®°å½•ã€‚\nå¦‚æœä½ ä¸ä½¿ç”¨ Pod è®°å½•ï¼Œåˆ™å¯ä»¥ä½¿ç”¨Â pods disabledÂ é€‰é¡¹ã€‚\nprometheusï¼šCoreDNS çš„åº¦é‡æŒ‡æ ‡å€¼ä»¥Â PrometheusÂ æ ¼å¼ï¼ˆä¹Ÿç§°ä¸º OpenMetricsï¼‰åœ¨Â http://localhost:9153/metricsÂ ä¸Šæä¾›ã€‚\nforward: ä¸åœ¨ Kubernetes é›†ç¾¤åŸŸå†…çš„ä»»ä½•æŸ¥è¯¢éƒ½å°†è½¬å‘åˆ°é¢„å®šä¹‰çš„è§£æå™¨ (/etc/resolv.conf)ã€‚\ncacheï¼šå¯ç”¨å‰ç«¯ç¼“å­˜ã€‚\nloopï¼šæ£€æµ‹ç®€å•çš„è½¬å‘ç¯ï¼Œå¦‚æœå‘ç°æ­»å¾ªç¯ï¼Œåˆ™ä¸­æ­¢ CoreDNS è¿›ç¨‹ã€‚\nreloadï¼šå…è®¸è‡ªåŠ¨é‡æ–°åŠ è½½å·²æ›´æ”¹çš„ Corefileã€‚ ç¼–è¾‘ ConfigMap é…ç½®åï¼Œè¯·ç­‰å¾…ä¸¤åˆ†é’Ÿï¼Œä»¥ä½¿æ›´æ”¹ç”Ÿæ•ˆã€‚\nloadbalanceï¼šè¿™æ˜¯ä¸€ä¸ªè½®è½¬å¼ DNS è´Ÿè½½å‡è¡¡å™¨ï¼Œ å®ƒåœ¨åº”ç­”ä¸­éšæœºåˆ†é… Aã€AAAA å’Œ MX è®°å½•çš„é¡ºåºã€‚\n3.3. é…ç½®å­˜æ ¹åŸŸå’Œä¸Šæ¸¸åŸŸåæœåŠ¡å™¨ CoreDNS èƒ½å¤Ÿä½¿ç”¨Â forward æ’ä»¶é…ç½®å­˜æ ¹åŸŸå’Œä¸Šæ¸¸åŸŸåæœåŠ¡å™¨ã€‚\nç¤ºä¾‹ï¼š\nåœ¨ \"10.150.0.1\" å¤„è¿è¡Œäº†Â ConsulÂ åŸŸæœåŠ¡å™¨ï¼Œ ä¸”æ‰€æœ‰ Consul åç§°éƒ½å¸¦æœ‰åç¼€Â .consul.localã€‚\nconsul.local:53 { errors cache 30 forward . 10.150.0.1 } è¦æ˜¾å¼å¼ºåˆ¶æ‰€æœ‰éé›†ç¾¤ DNS æŸ¥æ‰¾é€šè¿‡ç‰¹å®šçš„åŸŸåæœåŠ¡å™¨ï¼ˆä½äº 172.16.0.1ï¼‰ï¼Œå¯å°†Â forwardÂ æŒ‡å‘è¯¥åŸŸåæœåŠ¡å™¨ï¼Œè€Œä¸æ˜¯Â /etc/resolv.confã€‚\nforward . 172.16.0.1 å®Œæ•´ç¤ºä¾‹ï¼›\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 172.16.0.1 cache 30 loop reload loadbalance } consul.local:53 { errors cache 30 forward . 10.150.0.1 } 4. è°ƒè¯•DNSé—®é¢˜ åˆ›å»ºä¸€ä¸ªè°ƒè¯•çš„pod\napiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: - name: dnsutils image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3 command: - sleep - \"infinity\" imagePullPolicy: IfNotPresent restartPolicy: Always éƒ¨ç½²è°ƒè¯•pod\n4.1. æŸ¥çœ‹CorednsæœåŠ¡æ˜¯å¦æ­£å¸¸ kubectl get svc --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... kube-dns ClusterIP 10.0.0.10 \u003cnone\u003e 53/UDP,53/TCP 1h ... 4.2. æŸ¥çœ‹/etc/resolv.conf æŸ¥çœ‹å®¹å™¨å†…dnsé…ç½®æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚\nkubectl exec -ti dnsutils -- cat /etc/resolv.conf 4.3. nslookupæŸ¥çœ‹è§£ææŠ¥é”™ kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 Name: kubernetes.default Address 1: 10.0.0.1 å‚è€ƒï¼š\nService ä¸ Pod çš„ DNS | Kubernetes\nè‡ªå®šä¹‰ DNS æœåŠ¡ | Kubernetes\nè°ƒè¯• DNS é—®é¢˜ | Kubernetes\nä½¿ç”¨ CoreDNS è¿›è¡ŒæœåŠ¡å‘ç° | Kubernetes\nè‡ªåŠ¨æ‰©ç¼©é›†ç¾¤ DNS æœåŠ¡ | Kubernetes\nresolv.conf(5) - Linux manual page\ndns/specification.md at master Â· kubernetes/dns Â· GitHub\ndeployment/CoreDNS-k8s_version.md at master Â· coredns/deployment Â· GitHub\nforward\n","categories":"","description":"","excerpt":"1. Podçš„DNSç­–ç•¥ å¯ä»¥åœ¨podä¸­å®šä¹‰dnsPolicyå­—æ®µæ¥è®¾ç½®dnsçš„ç­–ç•¥ã€‚\n\"Default\": Pod ä»è¿è¡Œæ‰€åœ¨çš„èŠ‚ç‚¹ç»§æ‰¿å â€¦","ref":"/kubernetes-notes/network/pod-dns/","tags":["Network"],"title":"Podçš„DNSç­–ç•¥"},{"body":"ingress-controlleræ¶æ„å›¾ ingress-controlleræµç¨‹å›¾ ApisixRouteåŒæ­¥é€»è¾‘ æ•°æ®ç»“æ„è½¬æ¢ å‚è€ƒï¼š\nhttps://apisix.apache.org/zh/docs/ingress-controller/getting-started/\nhttps://apisix.apache.org/zh/docs/ingress-controller/design/\n","categories":"","description":"","excerpt":"ingress-controlleræ¶æ„å›¾ ingress-controlleræµç¨‹å›¾ ApisixRouteåŒæ­¥é€»è¾‘ æ•°æ®ç»“æ„è½¬æ¢ å‚è€ƒï¼š â€¦","ref":"/kubernetes-notes/network/gateway/ingress-controller-design/","tags":["ApiSix"],"title":"ingress-controlleråŸç†"},{"body":"1. ç®€ä»‹ macvlanå¯ä»¥çœ‹åšæ˜¯ç‰©ç†æ¥å£ethï¼ˆçˆ¶æ¥å£ï¼‰çš„å­æ¥å£ï¼Œæ¯ä¸ªmacvlanéƒ½æ‹¥æœ‰ç‹¬ç«‹çš„macåœ°å€ï¼Œå¯ä»¥è¢«ç»‘å®šIPä½œä¸ºæ­£å¸¸çš„ç½‘å¡æ¥å£ä½¿ç”¨ã€‚é€šè¿‡è¿™ä¸ªç‰¹æ€§ï¼Œå¯ä»¥å®ç°åœ¨ä¸€ä¸ªç‰©ç†ç½‘ç»œè®¾å¤‡ç»‘å®šå¤šä¸ªIPï¼Œæ¯ä¸ªIPæ‹¥æœ‰ç‹¬ç«‹çš„macåœ°å€ã€‚è¯¥ç‰¹æ€§ç»å¸¸è¢«åº”ç”¨åœ¨å®¹å™¨è™šæ‹ŸåŒ–ä¸­ï¼ˆå®¹å™¨å¯ä»¥é…ç½®macvlançš„ç½‘ç»œï¼Œå°†macvlan interfaceç§»åŠ¨åˆ°å®¹å™¨çš„namespaceä¸­ï¼‰ã€‚\nç¤ºæ„å›¾ï¼š\n2. å››ç§å·¥ä½œæ¨¡å¼ 2.1. VEPA (Virtual Ethernet Port Aggregator) VEPAä¸ºé»˜è®¤çš„å·¥ä½œæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰macvlanå‘å‡ºçš„æµé‡éƒ½ä¼šç»è¿‡çˆ¶æ¥å£ï¼Œä¸ç®¡ç›®çš„åœ°æ˜¯å¦ä¸è¯¥macvlanå…±ç”¨ä¸€ä¸ªçˆ¶æ¥å£ã€‚\n2.2. Bridge mode è¯¥bridgeæ¨¡å¼ç±»ä¼¼äºä¼ ç»Ÿçš„ç½‘æ¡¥æ¨¡å¼ï¼Œæ‹¥æœ‰ç›¸åŒçˆ¶æ¥å£çš„macvlanå¯ä»¥ç›´æ¥è¿›è¡Œé€šä¿¡ï¼Œä¸éœ€è¦å°†æ•°æ®å‘ç»™çˆ¶æ¥å£è½¬å‘ã€‚è¯¥æ¨¡å¼ä¸‹ä¸éœ€è¦äº¤æ¢æœºæ”¯æŒhairpinæ¨¡å¼ï¼Œæ€§èƒ½æ¯”VEPAæ¨¡å¼å¥½ã€‚å¦å¤–ç›¸å¯¹äºä¼ ç»Ÿçš„ç½‘æ¡¥æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸éœ€è¦å­¦ä¹ macåœ°å€ï¼Œä¸éœ€è¦STPï¼Œä½¿å¾—å…¶æ€§èƒ½æ¯”ä¼ ç»Ÿçš„ç½‘æ¡¥æ€§èƒ½å¥½å¾—å¤šã€‚ä½†æ˜¯å¦‚æœçˆ¶æ¥å£downæ‰ï¼Œåˆ™æ‰€æœ‰å­æ¥å£ä¹Ÿä¼šdownï¼ŒåŒæ—¶æ— æ³•é€šä¿¡ã€‚\n2.3. Private mode è¯¥æ¨¡å¼æ˜¯VEPAæ¨¡å¼çš„å¢å¼ºç‰ˆï¼Œ\n2.4. Passthru mode .Â å¾…å®Œå–„\nå‚è€ƒï¼š\nhttps://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/ https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan https://github.com/containernetworking/plugins/blob/master/plugins/main/macvlan/macvlan.go http://wikibon.org/wiki/v/Edge_Virtual_Bridging Linux è™šæ‹Ÿç½‘å¡æŠ€æœ¯ï¼šMacvlan http://hicu.be/bridge-vs-macvlan http://hicu.be/docker-networking-macvlan-bridge-mode-configuration ","categories":"","description":"","excerpt":"1. ç®€ä»‹ macvlanå¯ä»¥çœ‹åšæ˜¯ç‰©ç†æ¥å£ethï¼ˆçˆ¶æ¥å£ï¼‰çš„å­æ¥å£ï¼Œæ¯ä¸ªmacvlanéƒ½æ‹¥æœ‰ç‹¬ç«‹çš„macåœ°å€ï¼Œå¯ä»¥è¢«ç»‘å®šIPä½œä¸ºæ­£å¸¸çš„ç½‘å¡ â€¦","ref":"/kubernetes-notes/network/cni/macvlan/","tags":["CNI"],"title":"Macvlanä»‹ç»"},{"body":"æ–‡ä»¶å­˜å‚¨ç»“æ„ å¤§éƒ¨åˆ†çš„Linuxæ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚ext2ã€ext3ï¼‰è§„å®šï¼Œä¸€ä¸ªæ–‡ä»¶ç”±ç›®å½•é¡¹ã€inodeå’Œæ•°æ®å—ç»„æˆ\nç›®å½•é¡¹ï¼šåŒ…æ‹¬æ–‡ä»¶åå’ŒinodeèŠ‚ç‚¹å·ã€‚ Inodeï¼šåˆç§°æ–‡ä»¶ç´¢å¼•èŠ‚ç‚¹ï¼ŒåŒ…å«æ–‡ä»¶çš„åŸºç¡€ä¿¡æ¯ä»¥åŠæ•°æ®å—çš„æŒ‡é’ˆã€‚ æ•°æ®å—ï¼šåŒ…å«æ–‡ä»¶çš„å…·ä½“å†…å®¹ã€‚ 1. inode ç†è§£inodeï¼Œè¦ä»æ–‡ä»¶å‚¨å­˜è¯´èµ·ã€‚æ–‡ä»¶å‚¨å­˜åœ¨ç¡¬ç›˜ä¸Šï¼Œç¡¬ç›˜çš„æœ€å°å­˜å‚¨å•ä½å«åš\"æ‰‡åŒº\"ï¼ˆSectorï¼‰ï¼Œæ¯ä¸ªæ‰‡åŒºå‚¨å­˜512å­—èŠ‚ï¼ˆç›¸å½“äº0.5KBï¼‰ã€‚\næ“ä½œç³»ç»Ÿè¯»å–ç¡¬ç›˜çš„æ—¶å€™ï¼Œä¸ä¼šä¸€ä¸ªæ‰‡åŒºä¸€ä¸ªæ‰‡åŒºåœ°è¯»å–ï¼Œè¿™æ ·æ•ˆç‡å¤ªä½ï¼Œè€Œæ˜¯ä¸€æ¬¡æ€§è¿ç»­è¯»å–å¤šä¸ªæ‰‡åŒºï¼Œå³ä¸€æ¬¡æ€§è¯»å–ä¸€ä¸ª\"å—\"ï¼ˆblockï¼‰ã€‚è¿™ç§ç”±å¤šä¸ªæ‰‡åŒºç»„æˆçš„\"å—\"ï¼Œæ˜¯æ–‡ä»¶å­˜å–çš„æœ€å°å•ä½ã€‚ï¼Œå³è¿ç»­å…«ä¸ª sectorç»„æˆä¸€ä¸ª blockã€‚\næ–‡ä»¶æ•°æ®éƒ½å‚¨å­˜åœ¨\"å—\"ä¸­ï¼Œé‚£ä¹ˆå¾ˆæ˜¾ç„¶ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»æ‰¾åˆ°ä¸€ä¸ªåœ°æ–¹å‚¨å­˜æ–‡ä»¶çš„å…ƒä¿¡æ¯ï¼Œæ¯”å¦‚æ–‡ä»¶çš„åˆ›å»ºè€…ã€æ–‡ä»¶çš„åˆ›å»ºæ—¥æœŸã€æ–‡ä»¶çš„å¤§å°ç­‰ç­‰ã€‚è¿™ç§å‚¨å­˜æ–‡ä»¶å…ƒä¿¡æ¯çš„åŒºåŸŸå°±å«åšinodeï¼Œä¸­æ–‡è¯‘åä¸º\"ç´¢å¼•èŠ‚ç‚¹\"ã€‚\ninodeåŒ…å«æ–‡ä»¶çš„å…ƒä¿¡æ¯ï¼Œå…·ä½“æ¥è¯´æœ‰ä»¥ä¸‹å†…å®¹ï¼š\næ–‡ä»¶çš„å­—èŠ‚æ•°ã€‚ æ–‡ä»¶æ‹¥æœ‰è€…çš„User IDã€‚ æ–‡ä»¶çš„Group IDã€‚ æ–‡ä»¶çš„è¯»ã€å†™ã€æ‰§è¡Œæƒé™ã€‚ æ–‡ä»¶çš„æ—¶é—´æˆ³ï¼Œå…±æœ‰ä¸‰ä¸ªï¼šctimeæŒ‡inodeä¸Šä¸€æ¬¡å˜åŠ¨çš„æ—¶é—´ï¼ŒmtimeæŒ‡æ–‡ä»¶å†…å®¹ä¸Šä¸€æ¬¡å˜åŠ¨çš„æ—¶é—´ï¼ŒatimeæŒ‡æ–‡ä»¶ä¸Šä¸€æ¬¡æ‰“å¼€çš„æ—¶é—´ã€‚ é“¾æ¥æ•°ï¼Œå³æœ‰å¤šå°‘æ–‡ä»¶åæŒ‡å‘è¿™ä¸ªinodeã€‚ æ–‡ä»¶æ•°æ®blockçš„ä½ç½®ã€‚ å¯ä»¥ç”¨statå‘½ä»¤ï¼ŒæŸ¥çœ‹æŸä¸ªæ–‡ä»¶çš„inodeä¿¡æ¯ï¼š\nstat demo.txt æ€»ä¹‹ï¼Œé™¤äº†æ–‡ä»¶åä»¥å¤–çš„æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œéƒ½å­˜åœ¨inodeä¹‹ä¸­ã€‚\nå½“æŸ¥çœ‹æŸä¸ªæ–‡ä»¶æ—¶ï¼Œä¼šå…ˆä»inodeè¡¨ä¸­æŸ¥å‡ºæ–‡ä»¶å±æ€§åŠæ•°æ®å­˜æ”¾ç‚¹ï¼Œå†ä»æ•°æ®å—ä¸­è¯»å–æ•°æ®ã€‚\nè¯·çœ‹æ–‡ä»¶å­˜å‚¨ç»“æ„ç¤ºæ„å›¾ï¼š\n1.1. inodeçš„å¤§å° inodeä¹Ÿä¼šæ¶ˆè€—ç¡¬ç›˜ç©ºé—´ï¼Œæ‰€ä»¥ç¡¬ç›˜æ ¼å¼åŒ–çš„æ—¶å€™ï¼Œæ“ä½œç³»ç»Ÿè‡ªåŠ¨å°†ç¡¬ç›˜åˆ†æˆä¸¤ä¸ªåŒºåŸŸã€‚ä¸€ä¸ªæ˜¯æ•°æ®åŒºï¼Œå­˜æ”¾æ–‡ä»¶æ•°æ®ï¼›å¦ä¸€ä¸ªæ˜¯inodeåŒºï¼ˆinode tableï¼‰ï¼Œå­˜æ”¾inodeæ‰€åŒ…å«çš„ä¿¡æ¯ã€‚\næ¯ä¸ªinodeèŠ‚ç‚¹çš„å¤§å°ï¼Œä¸€èˆ¬æ˜¯128å­—èŠ‚æˆ–256å­—èŠ‚ã€‚inodeèŠ‚ç‚¹çš„æ€»æ•°ï¼Œåœ¨æ ¼å¼åŒ–æ—¶å°±ç»™å®šï¼Œä¸€èˆ¬æ˜¯æ¯1KBæˆ–æ¯2KBå°±è®¾ç½®ä¸€ä¸ªinodeã€‚å‡å®šåœ¨ä¸€å—1GBçš„ç¡¬ç›˜ä¸­ï¼Œæ¯ä¸ªinodeèŠ‚ç‚¹çš„å¤§å°ä¸º128å­—èŠ‚ï¼Œæ¯1KBå°±è®¾ç½®ä¸€ä¸ªinodeï¼Œé‚£ä¹ˆinode tableçš„å¤§å°å°±ä¼šè¾¾åˆ°128MBï¼Œå æ•´å—ç¡¬ç›˜çš„12.8%ã€‚\næŸ¥çœ‹æ¯ä¸ªç¡¬ç›˜åˆ†åŒºçš„inodeæ€»æ•°å’Œå·²ç»ä½¿ç”¨çš„æ•°é‡ï¼Œå¯ä»¥ä½¿ç”¨df -i å‘½ä»¤ã€‚\næŸ¥çœ‹æ¯ä¸ªinodeèŠ‚ç‚¹çš„å¤§å°ï¼Œå¯ä»¥ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š\nsudo dumpe2fs -h /dev/hda | grep \"Inode size\" ç”±äºæ¯ä¸ªæ–‡ä»¶éƒ½å¿…é¡»æœ‰ä¸€ä¸ªinodeï¼Œå› æ­¤æœ‰å¯èƒ½å‘ç”Ÿinodeå·²ç»ç”¨å…‰ï¼Œä½†æ˜¯ç¡¬ç›˜è¿˜æœªå­˜æ»¡çš„æƒ…å†µã€‚è¿™æ—¶ï¼Œå°±æ— æ³•åœ¨ç¡¬ç›˜ä¸Šåˆ›å»ºæ–°æ–‡ä»¶ã€‚\n1.2. inodeå·ç  æ¯ä¸ªinodeéƒ½æœ‰ä¸€ä¸ªå·ç ï¼Œæ“ä½œç³»ç»Ÿç”¨inodeå·ç æ¥è¯†åˆ«ä¸åŒçš„æ–‡ä»¶ã€‚\nLinuxç³»ç»Ÿå†…éƒ¨ä¸ä½¿ç”¨æ–‡ä»¶åï¼Œè€Œä½¿ç”¨inodeå·ç æ¥è¯†åˆ«æ–‡ä»¶ã€‚å¯¹äºç³»ç»Ÿæ¥è¯´ï¼Œæ–‡ä»¶ååªæ˜¯inodeå·ç ä¾¿äºè¯†åˆ«çš„åˆ«ç§°æˆ–è€…ç»°å·ã€‚è¡¨é¢ä¸Šï¼Œç”¨æˆ·é€šè¿‡æ–‡ä»¶åï¼Œæ‰“å¼€æ–‡ä»¶ã€‚å®é™…ä¸Šï¼Œç³»ç»Ÿå†…éƒ¨è¿™ä¸ªè¿‡ç¨‹åˆ†æˆä¸‰æ­¥ï¼šé¦–å…ˆï¼Œç³»ç»Ÿæ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶åå¯¹åº”çš„inodeå·ç ï¼›å…¶æ¬¡ï¼Œé€šè¿‡inodeå·ç ï¼Œè·å–inodeä¿¡æ¯ï¼›æœ€åï¼Œæ ¹æ®inodeä¿¡æ¯ï¼Œæ‰¾åˆ°æ–‡ä»¶æ•°æ®æ‰€åœ¨çš„blockï¼Œè¯»å‡ºæ•°æ®ã€‚\nä½¿ç”¨ls -iå‘½ä»¤ï¼Œå¯ä»¥çœ‹åˆ°æ–‡ä»¶åå¯¹åº”çš„inodeå·ç ï¼Œä¾‹å¦‚ï¼š\nls -i demo.txt 3. ç›®å½•é¡¹ Linuxç³»ç»Ÿä¸­ï¼Œç›®å½•ï¼ˆdirectoryï¼‰ä¹Ÿæ˜¯ä¸€ç§æ–‡ä»¶ã€‚æ‰“å¼€ç›®å½•ï¼Œå®é™…ä¸Šå°±æ˜¯æ‰“å¼€ç›®å½•æ–‡ä»¶ã€‚\nç›®å½•æ–‡ä»¶çš„ç»“æ„éå¸¸ç®€å•ï¼Œå°±æ˜¯ä¸€ç³»åˆ—ç›®å½•é¡¹ï¼ˆdirentï¼‰çš„åˆ—è¡¨ã€‚æ¯ä¸ªç›®å½•é¡¹ï¼Œç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šæ‰€åŒ…å«æ–‡ä»¶çš„æ–‡ä»¶åï¼Œä»¥åŠè¯¥æ–‡ä»¶åå¯¹åº”çš„inodeå·ç ã€‚\nlså‘½ä»¤åªåˆ—å‡ºç›®å½•æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ–‡ä»¶åï¼š\nls /etc ls -iå‘½ä»¤åˆ—å‡ºæ•´ä¸ªç›®å½•æ–‡ä»¶ï¼Œå³æ–‡ä»¶åå’Œinodeå·ç ï¼š\nls -i /etc å¦‚æœè¦æŸ¥çœ‹æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼Œå°±å¿…é¡»æ ¹æ®inodeå·ç ï¼Œè®¿é—®inodeèŠ‚ç‚¹ï¼Œè¯»å–ä¿¡æ¯ã€‚ls -lå‘½ä»¤åˆ—å‡ºæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ã€‚\nls -l /etc 4. ç¡¬é“¾æ¥å’Œè½¯é“¾æ¥ 4.1. ç¡¬é“¾æ¥ ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ–‡ä»¶åå’Œinodeå·ç æ˜¯\"ä¸€ä¸€å¯¹åº”\"å…³ç³»ï¼Œæ¯ä¸ªinodeå·ç å¯¹åº”ä¸€ä¸ªæ–‡ä»¶åã€‚ä½†æ˜¯ï¼ŒLinuxç³»ç»Ÿå…è®¸ï¼Œå¤šä¸ªæ–‡ä»¶åæŒ‡å‘åŒä¸€ä¸ªinodeå·ç ã€‚è¿™æ„å‘³ç€ï¼Œå¯ä»¥ç”¨ä¸åŒçš„æ–‡ä»¶åè®¿é—®åŒæ ·çš„å†…å®¹ï¼›å¯¹æ–‡ä»¶å†…å®¹è¿›è¡Œä¿®æ”¹ï¼Œä¼šå½±å“åˆ°æ‰€æœ‰æ–‡ä»¶åï¼›ä½†æ˜¯ï¼Œåˆ é™¤ä¸€ä¸ªæ–‡ä»¶åï¼Œä¸å½±å“å¦ä¸€ä¸ªæ–‡ä»¶åçš„è®¿é—®ã€‚è¿™ç§æƒ…å†µå°±è¢«ç§°ä¸º\"ç¡¬é“¾æ¥\"ï¼ˆhard linkï¼‰ã€‚\nlnå‘½ä»¤å¯ä»¥åˆ›å»ºç¡¬é“¾æ¥\nln source_file target_file è¿è¡Œä¸Šé¢è¿™æ¡å‘½ä»¤ä»¥åï¼Œæºæ–‡ä»¶ä¸ç›®æ ‡æ–‡ä»¶çš„inodeå·ç ç›¸åŒï¼Œéƒ½æŒ‡å‘åŒä¸€ä¸ªinodeã€‚inodeä¿¡æ¯ä¸­æœ‰ä¸€é¡¹å«åš\"é“¾æ¥æ•°\"ï¼Œè®°å½•æŒ‡å‘è¯¥inodeçš„æ–‡ä»¶åæ€»æ•°ï¼Œè¿™æ—¶å°±ä¼šå¢åŠ 1ã€‚åè¿‡æ¥ï¼Œåˆ é™¤ä¸€ä¸ªæ–‡ä»¶åï¼Œå°±ä¼šä½¿å¾—inodeèŠ‚ç‚¹ä¸­çš„\"é“¾æ¥æ•°\"å‡1ã€‚å½“è¿™ä¸ªå€¼å‡åˆ°0ï¼Œè¡¨æ˜æ²¡æœ‰æ–‡ä»¶åæŒ‡å‘è¿™ä¸ªinodeï¼Œç³»ç»Ÿå°±ä¼šå›æ”¶è¿™ä¸ªinodeå·ç ï¼Œä»¥åŠå…¶æ‰€å¯¹åº”blockåŒºåŸŸã€‚\nè¿™é‡Œé¡ºä¾¿è¯´ä¸€ä¸‹ç›®å½•æ–‡ä»¶çš„\"é“¾æ¥æ•°\"ã€‚åˆ›å»ºç›®å½•æ—¶ï¼Œé»˜è®¤ä¼šç”Ÿæˆä¸¤ä¸ªç›®å½•é¡¹ï¼š\".\"å’Œ\"..\"ã€‚å‰è€…çš„inodeå·ç å°±æ˜¯å½“å‰ç›®å½•çš„inodeå·ç ï¼Œç­‰åŒäºå½“å‰ç›®å½•çš„\"ç¡¬é“¾æ¥\"ï¼›åè€…çš„inodeå·ç å°±æ˜¯å½“å‰ç›®å½•çš„çˆ¶ç›®å½•çš„inodeå·ç ï¼Œç­‰åŒäºçˆ¶ç›®å½•çš„\"ç¡¬é“¾æ¥\"ã€‚æ‰€ä»¥ï¼Œä»»ä½•ä¸€ä¸ªç›®å½•çš„\"ç¡¬é“¾æ¥\"æ€»æ•°ï¼Œæ€»æ˜¯ç­‰äº2åŠ ä¸Šå®ƒçš„å­ç›®å½•æ€»æ•°ï¼ˆå«éšè—ç›®å½•ï¼‰,è¿™é‡Œçš„2æ˜¯çˆ¶ç›®å½•å¯¹å…¶çš„â€œç¡¬é“¾æ¥â€å’Œå½“å‰ç›®å½•ä¸‹çš„\".ç¡¬é“¾æ¥â€œã€‚\n4.2. è½¯é“¾æ¥ é™¤äº†ç¡¬é“¾æ¥ä»¥å¤–ï¼Œè¿˜æœ‰ä¸€ç§ç‰¹æ®Šæƒ…å†µã€‚æ–‡ä»¶Aå’Œæ–‡ä»¶Bçš„inodeå·ç è™½ç„¶ä¸ä¸€æ ·ï¼Œä½†æ˜¯æ–‡ä»¶Açš„å†…å®¹æ˜¯æ–‡ä»¶Bçš„è·¯å¾„ã€‚è¯»å–æ–‡ä»¶Aæ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†è®¿é—®è€…å¯¼å‘æ–‡ä»¶Bã€‚å› æ­¤ï¼Œæ— è®ºæ‰“å¼€å“ªä¸€ä¸ªæ–‡ä»¶ï¼Œæœ€ç»ˆè¯»å–çš„éƒ½æ˜¯æ–‡ä»¶Bã€‚è¿™æ—¶ï¼Œæ–‡ä»¶Aå°±ç§°ä¸ºæ–‡ä»¶Bçš„\"è½¯é“¾æ¥\"ï¼ˆsoft linkï¼‰æˆ–è€…\"ç¬¦å·é“¾æ¥ï¼ˆsymbolic linkï¼‰ã€‚\nè¿™æ„å‘³ç€ï¼Œæ–‡ä»¶Aä¾èµ–äºæ–‡ä»¶Bè€Œå­˜åœ¨ï¼Œå¦‚æœåˆ é™¤äº†æ–‡ä»¶Bï¼Œæ‰“å¼€æ–‡ä»¶Aå°±ä¼šæŠ¥é”™ï¼š\"No such file or directory\"ã€‚è¿™æ˜¯è½¯é“¾æ¥ä¸ç¡¬é“¾æ¥æœ€å¤§çš„ä¸åŒï¼šæ–‡ä»¶AæŒ‡å‘æ–‡ä»¶Bçš„æ–‡ä»¶åï¼Œè€Œä¸æ˜¯æ–‡ä»¶Bçš„inodeå·ç ï¼Œæ–‡ä»¶Bçš„inode\"é“¾æ¥æ•°\"ä¸ä¼šå› æ­¤å‘ç”Ÿå˜åŒ–ã€‚\nln -så‘½ä»¤å¯ä»¥åˆ›å»ºè½¯é“¾æ¥\nln -s source_file target_file å‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/linux/ ","categories":"","description":"","excerpt":"æ–‡ä»¶å­˜å‚¨ç»“æ„ å¤§éƒ¨åˆ†çš„Linuxæ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚ext2ã€ext3ï¼‰è§„å®šï¼Œä¸€ä¸ªæ–‡ä»¶ç”±ç›®å½•é¡¹ã€inodeå’Œæ•°æ®å—ç»„æˆ\nç›®å½•é¡¹ï¼šåŒ…æ‹¬æ–‡ä»¶å â€¦","ref":"/linux-notes/file/linux-file-storage/","tags":["Linux"],"title":"Linuxæ–‡ä»¶å­˜å‚¨"},{"body":"æµç¨‹è¯­å¥ 1. æ¡ä»¶è¯­å¥ //åœ¨ifä¹‹åæ¡ä»¶è¯­å¥ä¹‹å‰å¯ä»¥æ·»åŠ å˜é‡åˆå§‹åŒ–è¯­å¥ï¼Œç”¨;å·éš”ç¦» if \u003cæ¡ä»¶è¯­å¥\u003e { //æ¡ä»¶è¯­å¥ä¸éœ€è¦ç”¨æ‹¬å·æ‹¬èµ·æ¥ï¼ŒèŠ±æ‹¬å·å¿…é¡»å­˜åœ¨ //è¯­å¥ä½“ }else{ //è¯­å¥ä½“ } //åœ¨æœ‰è¿”å›å€¼çš„å‡½æ•°ä¸­ï¼Œä¸å…è®¸å°†æœ€åçš„returnè¯­å¥æ”¾åœ¨if...else...çš„ç»“æ„ä¸­ï¼Œå¦åˆ™ä¼šç¼–è¯‘å¤±è´¥ //ä¾‹å¦‚ä»¥ä¸‹ä¸ºé”™è¯¯èŒƒä¾‹ func example(x int) int{ if x==0{ return 5 }else{ return x //æœ€åçš„returnè¯­å¥æ”¾åœ¨if-elseç»“æ„ä¸­ï¼Œæ‰€ä»¥ç¼–è¯‘å¤±è´¥ } } 2. é€‰æ‹©è¯­å¥ //1ã€æ ¹æ®æ¡ä»¶ä¸åŒï¼Œå¯¹åº”ä¸åŒçš„æ‰§è¡Œä½“ switch i{ case 0: fmt.Printf(\"0\") case 1: //æ»¡è¶³æ¡ä»¶å°±ä¼šé€€å‡ºï¼Œåªæœ‰æ·»åŠ fallthroughæ‰ä¼šç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªcaseè¯­å¥ fmt.Prinntf(\"1\") case 2,3,1: //å•ä¸ªcaseå¯ä»¥å‡ºç°å¤šä¸ªé€‰é¡¹ fmt.Printf(\"2,3,1\") default: //å½“éƒ½ä¸æ»¡è¶³ä»¥ä¸Šæ¡ä»¶æ—¶ï¼Œæ‰§è¡Œdefaultè¯­å¥ fmt.Printf(\"Default\") } //2ã€è¯¥æ¨¡å¼ç­‰ä»·äºå¤šä¸ªif-elseçš„åŠŸèƒ½ switch { case \u003cæ¡ä»¶è¡¨è¾¾å¼1\u003e: è¯­å¥ä½“1 case \u003cæ¡ä»¶è¡¨è¾¾å¼2\u003e: è¯­å¥ä½“2 } 3. å¾ªç¯è¯­å¥ //1ã€Goåªæ”¯æŒforå…³é”®å­—ï¼Œä¸æ”¯æŒwhileï¼Œdo-whileç»“æ„ for i,j:=0,1;i\u003c10;i++{ //æ”¯æŒå¤šä¸ªèµ‹å€¼ //è¯­å¥ä½“ } //2ã€æ— é™å¾ªç¯ sum:=1 for{ //ä¸æ¥æ¡ä»¶è¡¨è¾¾å¼è¡¨ç¤ºæ— é™å¾ªç¯ sum++ if sum \u003e 100{ break //æ»¡è¶³æ¡ä»¶è·³å‡ºå¾ªç¯ } } //3ã€æ”¯æŒcontinueå’Œbreakï¼Œbreakå¯ä»¥æŒ‡å®šä¸­æ–­å“ªä¸ªå¾ªç¯ï¼Œbreak JLoop(æ ‡ç­¾) for j:=0;j\u003c5;j++{ for i:=0;i\u003c10;i++{ if i\u003e5{ break JLoop //ç»ˆæ­¢JLoopæ ‡ç­¾å¤„çš„å¤–å±‚å¾ªç¯ } fmt.Println(i) } JLoop: //æ ‡ç­¾å¤„ ... 4. è·³è½¬è¯­å¥ //å…³é”®å­—gotoæ”¯æŒè·³è½¬ func myfunc(){ i:=0 HERE: //å®šä¹‰æ ‡ç­¾å¤„ fmt.Println(i) i++ if i\u003c10{ goto HERE //è·³è½¬åˆ°æ ‡ç­¾å¤„ } } ","categories":"","description":"","excerpt":"æµç¨‹è¯­å¥ 1. æ¡ä»¶è¯­å¥ //åœ¨ifä¹‹åæ¡ä»¶è¯­å¥ä¹‹å‰å¯ä»¥æ·»åŠ å˜é‡åˆå§‹åŒ–è¯­å¥ï¼Œç”¨;å·éš”ç¦» if \u003cæ¡ä»¶è¯­å¥\u003e { //æ¡ä»¶è¯­å¥ä¸éœ€è¦ç”¨æ‹¬å·æ‹¬èµ· â€¦","ref":"/golang-notes/basis/control-structures/","tags":["Golang"],"title":"æµç¨‹è¯­å¥"},{"body":"æ·»åŠ Flags 1. Persistent Flags Persistent Flagsè¡¨ç¤ºè¯¥ç±»å‚æ•°å¯ä»¥è¢«ç”¨äºå½“å‰å‘½ä»¤åŠå…¶å­å‘½ä»¤ã€‚\nä¾‹å¦‚ï¼Œä»¥ä¸‹è¡¨ç¤ºverboseå‚æ•°å¯ä»¥è¢«ç”¨äºrootCmdåŠå…¶å­å‘½ä»¤ã€‚\nrootCmd.PersistentFlags().BoolVarP(\u0026Verbose, \"verbose\", \"v\", false, \"verbose output\") 2. Local Flags Local Flagsè¡¨ç¤ºè¯¥ç±»å‚æ•°åªèƒ½ç”¨äºå½“å‰å‘½ä»¤ã€‚\nä¾‹å¦‚ï¼Œä»¥ä¸‹è¡¨ç¤ºsourceåªèƒ½ç”¨äºlocalCmdè¿™ä¸ªå‘½ä»¤ï¼Œä¸èƒ½ç”¨äºå…¶å­å‘½ä»¤ã€‚\nlocalCmd.Flags().StringVarP(\u0026Source, \"source\", \"s\", \"\", \"Source directory to read from\") 3. Local Flag on Parent Commands cobraé»˜è®¤åªè§£æå½“å‰å‘½ä»¤çš„local flagsï¼Œé€šè¿‡å¼€å¯Command.TraverseChildrenå‚æ•°ï¼Œå¯ä»¥è§£ææ¯ä¸ªå‘½ä»¤çš„local flagsã€‚\ncommand := cobra.Command{ Use: \"print [OPTIONS] [COMMANDS]\", TraverseChildren: true, } 4. Bind Flags with Config å¯ä»¥é€šè¿‡ viperæ¥ç»‘å®šflagsã€‚\nvar author string func init() { rootCmd.PersistentFlags().StringVar(\u0026author, \"author\", \"YOUR NAME\", \"Author name for copyright attribution\") viper.BindPFlag(\"author\", rootCmd.PersistentFlags().Lookup(\"author\")) } æ›´å¤šå‚è€ƒï¼š viper documentationã€‚\n5. Required flags é»˜è®¤æ·»åŠ çš„flagsçš„å¯é€‰å‚æ•°ï¼Œå¦‚æœéœ€è¦åœ¨äºŒè¿›åˆ¶è¿è¡Œæ—¶æ·»åŠ å¿…è¦å‚æ•°ï¼Œå³å½“è¯¥å‚æ•°æ²¡æŒ‡å®šæ—¶ä¼šæŠ¥é”™ã€‚å¯ä½¿ç”¨ä»¥ä¸‹è®¾ç½®ã€‚\nrootCmd.Flags().StringVarP(\u0026Region, \"region\", \"r\", \"\", \"AWS region (required)\") rootCmd.MarkFlagRequired(\"region\") å‚è€ƒï¼š\nhttps://github.com/spf13/cobra https://github.com/spf13/cobra/blob/master/cobra/README.md ","categories":"","description":"","excerpt":"æ·»åŠ Flags 1. Persistent Flags Persistent Flagsè¡¨ç¤ºè¯¥ç±»å‚æ•°å¯ä»¥è¢«ç”¨äºå½“å‰å‘½ä»¤åŠå…¶å­å‘½ä»¤ã€‚\nä¾‹å¦‚ï¼Œä»¥ â€¦","ref":"/golang-notes/framework/cobra/cobra-flags/","tags":["Golang"],"title":"cobra flags"},{"body":"æ·»åŠ iptablesè§„åˆ™ # å•ä¸ªç«¯å£ iptables -A INPUT -p tcp --dport 22 -j ACCEPT # å¤šä¸ªç«¯å£ iptables -A INPUT -p tcp -m multiport --dports 6443,8443,2379,2380,10250 -j ACCEPT åˆ é™¤iptablesè§„åˆ™ # æ˜¾ç¤ºiptablesè§„åˆ™è¡Œå· iptables -nL --line-numbers # åˆ é™¤æŸè¡Œè§„åˆ™ iptables -D INPUT 11 æŒä¹…åŒ–iptablesï¼ˆé‡å¯ä»ç”Ÿæ•ˆï¼‰ æŒä¹…åŒ–iptablesè§„åˆ™ï¼Œæ·»åŠ è§„åˆ™åˆ°æ–‡ä»¶ä¸­/etc/sysconfig/iptables\n# vi /etc/sysconfig/iptables -A INPUT -p vrrp -j ACCEPT -A OUTPUT -p vrrp -j ACCEPT æˆ–è€…\napt-get install iptables-persistent netfilter-persistent save netfilter-persistent reload # ç”Ÿæˆçš„è§„åˆ™å­˜å‚¨åœ¨ä»¥ä¸‹æ–‡ä»¶ /etc/iptables/rules.v4 /etc/iptables/rules.v6 ","categories":"","description":"","excerpt":"æ·»åŠ iptablesè§„åˆ™ # å•ä¸ªç«¯å£ iptables -A INPUT -p tcp --dport 22 -j ACCEPT # å¤šä¸ª â€¦","ref":"/linux-notes/network/iptables-command/","tags":["iptables"],"title":"iptableså‘½ä»¤"},{"body":"chromeå¿«æ·é”® 1. æ ‡ç­¾é¡µå’Œçª—å£å¿«æ·é”® æ“ä½œ å¿«æ·é”® æ‰“å¼€æ–°çª—å£ âŒ˜ + n åœ¨æ— ç—•æ¨¡å¼ä¸‹æ‰“å¼€æ–°çª—å£ âŒ˜ + Shift + n æ‰“å¼€æ–°çš„æ ‡ç­¾é¡µï¼Œå¹¶è·³è½¬åˆ°è¯¥æ ‡ç­¾é¡µ âŒ˜ + t é‡æ–°æ‰“å¼€æœ€åå…³é—­çš„æ ‡ç­¾é¡µï¼Œå¹¶è·³è½¬åˆ°è¯¥æ ‡ç­¾é¡µ âŒ˜ + Shift + t è·³è½¬åˆ°ä¸‹ä¸€ä¸ªæ‰“å¼€çš„æ ‡ç­¾é¡µ âŒ˜ + Option + å‘å³ç®­å¤´é”® è·³è½¬åˆ°ä¸Šä¸€ä¸ªæ‰“å¼€çš„æ ‡ç­¾é¡µ âŒ˜ + Option + å‘å·¦ç®­å¤´é”® è·³è½¬åˆ°ç‰¹å®šæ ‡ç­¾é¡µ âŒ˜ + 1 åˆ° âŒ˜ + 8 é¡ºåºåˆ‡æ¢æ ‡ç­¾é¡µ Ctrl + Tab è·³è½¬åˆ°æœ€åä¸€ä¸ªæ ‡ç­¾é¡µ âŒ˜ + 9 æ‰“å¼€å½“å‰æ ‡ç­¾é¡µæµè§ˆè®°å½•ä¸­è®°å½•çš„ä¸Šä¸€ä¸ªé¡µé¢ âŒ˜ + [ æˆ– âŒ˜ + å‘å·¦ç®­å¤´é”® æ‰“å¼€å½“å‰æ ‡ç­¾é¡µæµè§ˆè®°å½•ä¸­è®°å½•çš„ä¸‹ä¸€ä¸ªé¡µé¢ âŒ˜ + ] æˆ– âŒ˜ + å‘å³ç®­å¤´é”® å…³é—­å½“å‰æ ‡ç­¾é¡µæˆ–å¼¹å‡ºå¼çª—å£ âŒ˜ + w å…³é—­å½“å‰çª—å£ âŒ˜ + Shift + w æœ€å°åŒ–çª—å£ âŒ˜ + m éšè— Google Chrome âŒ˜ + h é€€å‡º Google Chrome âŒ˜ + q 2. Google Chrome åŠŸèƒ½å¿«æ·é”® æ“ä½œ å¿«æ·é”® æ˜¾ç¤ºæˆ–éšè—ä¹¦ç­¾æ  âŒ˜ + Shift + b æ‰“å¼€ä¹¦ç­¾ç®¡ç†å™¨ âŒ˜ + Option + b åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€â€œè®¾ç½®â€é¡µ âŒ˜ + , åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€â€œå†å²è®°å½•â€é¡µ âŒ˜ + y åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€â€œä¸‹è½½å†…å®¹â€é¡µ âŒ˜ + Shift + j æ‰“å¼€æŸ¥æ‰¾æ æœç´¢å½“å‰ç½‘é¡µ âŒ˜ + f è·³è½¬åˆ°ä¸æŸ¥æ‰¾æ ä¸­æœç´¢å­—è¯ç›¸åŒ¹é…çš„ä¸‹ä¸€æ¡å†…å®¹ âŒ˜ + g è·³è½¬åˆ°ä¸æŸ¥æ‰¾æ ä¸­æœç´¢å­—è¯ç›¸åŒ¹é…çš„ä¸Šä¸€æ¡å†…å®¹ âŒ˜ + Shift + g æ‰“å¼€æŸ¥æ‰¾æ åï¼Œæœç´¢é€‰å®šæ–‡æœ¬ âŒ˜ + e æ‰“å¼€â€œå¼€å‘è€…å·¥å…·â€ âŒ˜ + Option + i æ‰“å¼€â€œæ¸…é™¤æµè§ˆæ•°æ®â€é€‰é¡¹ âŒ˜ + Shift + Delete ä½¿ç”¨å…¶ä»–å¸å·ç™»å½•æˆ–ä»¥è®¿å®¢èº«ä»½æµè§ˆ âŒ˜ + Shift + m 3. åœ°å€æ å¿«æ·é”® åœ¨åœ°å€æ ä¸­å¯ä½¿ç”¨ä»¥ä¸‹å¿«æ·é”®ï¼š\næ“ä½œ å¿«æ·é”® ä½¿ç”¨é»˜è®¤æœç´¢å¼•æ“è¿›è¡Œæœç´¢ è¾“å…¥æœç´¢å­—è¯å¹¶æŒ‰ Enter é”® ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“è¿›è¡Œæœç´¢ è¾“å…¥æœç´¢å¼•æ“åç§°å¹¶æŒ‰ Tab é”® ä¸ºç½‘ç«™åç§°æ·»åŠ  www. å’Œ .comï¼Œå¹¶åœ¨å½“å‰æ ‡ç­¾é¡µä¸­æ‰“å¼€è¯¥ç½‘ç«™ è¾“å…¥ç½‘ç«™åç§°å¹¶æŒ‰ Control + Enter é”® ä¸ºç½‘ç«™åç§°æ·»åŠ  www. å’Œ .comï¼Œå¹¶åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€è¯¥ç½‘ç«™ è¾“å…¥ç½‘ç«™åç§°å¹¶æŒ‰ Control + Shift + Enter é”® åœ¨æ–°çš„åå°æ ‡ç­¾é¡µä¸­æ‰“å¼€ç½‘ç«™ è¾“å…¥ç½‘å€å¹¶æŒ‰ âŒ˜ + Enter é”® è·³è½¬åˆ°åœ°å€æ  âŒ˜ + l ä»åœ°å€æ ä¸­ç§»é™¤è”æƒ³æŸ¥è¯¢å†…å®¹ æŒ‰å‘ä¸‹ç®­å¤´é”®ä»¥çªå‡ºæ˜¾ç¤ºç›¸åº”å†…å®¹ï¼Œç„¶åæŒ‰ Shift + fn + Delete 4. ç½‘é¡µå¿«æ·é”® æ“ä½œ å¿«æ·é”® æ‰“å¼€é€‰é¡¹ä»¥æ‰“å°å½“å‰ç½‘é¡µ âŒ˜ + p æ‰“å¼€é€‰é¡¹ä»¥ä¿å­˜å½“å‰ç½‘é¡µ âŒ˜ + s æ‰“å¼€â€œé¡µé¢è®¾ç½®â€å¯¹è¯æ¡† âŒ˜ + Option + p é€šè¿‡ç”µå­é‚®ä»¶å‘é€å½“å‰ç½‘é¡µ âŒ˜ + Shift + i é‡æ–°åŠ è½½å½“å‰ç½‘é¡µ âŒ˜ + r é‡æ–°åŠ è½½å½“å‰ç½‘é¡µï¼ˆå¿½ç•¥ç¼“å­˜çš„å†…å®¹ï¼‰ âŒ˜ + Shift + r åœæ­¢åŠ è½½ç½‘é¡µ Esc æµè§ˆä¸‹ä¸€ä¸ªå¯ç‚¹å‡»é¡¹ Tab æµè§ˆä¸Šä¸€ä¸ªå¯ç‚¹å‡»é¡¹ Shift + Tab ä½¿ç”¨ Google Chrome æ‰“å¼€è®¡ç®—æœºä¸­çš„æ–‡ä»¶ æŒ‰ä½ âŒ˜ + o é”®å¹¶é€‰æ‹©æ–‡ä»¶ æ˜¾ç¤ºå½“å‰ç½‘é¡µçš„ HTML æºä»£ç ï¼ˆä¸å¯ä¿®æ”¹ï¼‰ âŒ˜ + Option + u æ‰“å¼€ JavaScript æ§åˆ¶å° âŒ˜ + Option + j å°†å½“å‰ç½‘é¡µä¿å­˜ä¸ºä¹¦ç­¾ âŒ˜ + d å°†æ‰€æœ‰æ‰“å¼€çš„æ ‡ç­¾é¡µä»¥ä¹¦ç­¾çš„å½¢å¼ä¿å­˜åœ¨æ–°æ–‡ä»¶å¤¹ä¸­ âŒ˜ + Shift + d å¼€å¯æˆ–å…³é—­å…¨å±æ¨¡å¼ âŒ˜ + Ctrl + f æ”¾å¤§ç½‘é¡µä¸Šçš„æ‰€æœ‰å†…å®¹ âŒ˜ å’Œ + ç¼©å°ç½‘é¡µä¸Šçš„æ‰€æœ‰å†…å®¹ âŒ˜ å’Œ - å°†ç½‘é¡µä¸Šçš„æ‰€æœ‰å†…å®¹æ¢å¤åˆ°é»˜è®¤å¤§å° âŒ˜ + 0 å‘ä¸‹æ»šåŠ¨ç½‘é¡µï¼Œä¸€æ¬¡ä¸€ä¸ªå±å¹• ç©ºæ ¼é”® å‘ä¸Šæ»šåŠ¨ç½‘é¡µï¼Œä¸€æ¬¡ä¸€ä¸ªå±å¹• Shift + ç©ºæ ¼é”® æœç´¢ç½‘ç»œ âŒ˜ + Option + f å°†å…‰æ ‡ç§»åˆ°æ–‡æœ¬å­—æ®µä¸­çš„ä¸Šä¸€ä¸ªå­—è¯å‰é¢ Option + å‘å·¦ç®­å¤´é”® å°†å…‰æ ‡ç§»åˆ°æ–‡æœ¬å­—æ®µä¸­çš„ä¸Šä¸€ä¸ªå­—è¯åé¢ Option + å‘å³ç®­å¤´é”® åˆ é™¤æ–‡æœ¬å­—æ®µä¸­çš„ä¸Šä¸€ä¸ªå­—è¯ Option + Delete åœ¨å½“å‰æ ‡ç­¾é¡µä¸­æ‰“å¼€ä¸»é¡µ âŒ˜ + Shift + h 5. é¼ æ ‡å¿«æ·é”® ä»¥ä¸‹å¿«æ·é”®è¦æ±‚æ‚¨ä½¿ç”¨é¼ æ ‡ï¼š\næ“ä½œ å¿«æ·é”® åœ¨å½“å‰æ ‡ç­¾é¡µä¸­æ‰“å¼€é“¾æ¥ï¼ˆä»…é™é¼ æ ‡ï¼‰ å°†é“¾æ¥æ‹–åˆ°æ ‡ç­¾é¡µä¸­ åœ¨æ–°çš„åå°æ ‡ç­¾é¡µä¸­æ‰“å¼€é“¾æ¥ æŒ‰ä½ âŒ˜ é”®çš„åŒæ—¶ç‚¹å‡»é“¾æ¥ æ‰“å¼€é“¾æ¥ï¼Œå¹¶è·³è½¬åˆ°è¯¥é“¾æ¥ æŒ‰ä½ âŒ˜ + Shift é”®çš„åŒæ—¶ç‚¹å‡»é“¾æ¥ æ‰“å¼€é“¾æ¥ï¼Œå¹¶è·³è½¬åˆ°è¯¥é“¾æ¥ï¼ˆä»…ä½¿ç”¨é¼ æ ‡ï¼‰ å°†é“¾æ¥æ‹–åˆ°æ ‡ç­¾æ çš„ç©ºç™½åŒºåŸŸ åœ¨æ–°çª—å£ä¸­æ‰“å¼€é“¾æ¥ æŒ‰ä½ Shift é”®çš„åŒæ—¶ç‚¹å‡»é“¾æ¥ åœ¨æ–°çª—å£ä¸­æ‰“å¼€æ ‡ç­¾é¡µï¼ˆä»…ä½¿ç”¨é¼ æ ‡ï¼‰ å°†æ ‡ç­¾é¡µæ‹–å‡ºæ ‡ç­¾æ  å°†æ ‡ç­¾é¡µç§»è‡³å½“å‰çª—å£ï¼ˆä»…é™é¼ æ ‡ï¼‰ å°†æ ‡ç­¾é¡µæ‹–åˆ°ç°æœ‰çª—å£ä¸­ å°†æ ‡ç­¾é¡µç§»å›å…¶åŸå§‹ä½ç½® æ‹–åŠ¨æ ‡ç­¾é¡µçš„åŒæ—¶æŒ‰ Esc å°†å½“å‰ç½‘é¡µä¿å­˜ä¸ºä¹¦ç­¾ å°†ç›¸åº”ç½‘å€æ‹–åŠ¨åˆ°ä¹¦ç­¾æ ä¸­ ä¸‹è½½é“¾æ¥ç›®æ ‡ æŒ‰ä½ Option é”®çš„åŒæ—¶ç‚¹å‡»é“¾æ¥ æ˜¾ç¤ºæµè§ˆè®°å½• å³é”®ç‚¹å‡»â€œåé€€â€ç®­å¤´ æˆ–â€œå‰è¿›â€ç®­å¤´ ï¼Œæˆ–è€…å·¦é”®ç‚¹å‡»ï¼ˆå¹¶æŒ‰ä½é¼ æ ‡å·¦é”®ä¸æ”¾ï¼‰â€œåé€€â€ç®­å¤´æˆ–â€œå‰è¿›â€ç®­å¤´ å°†çª—å£é«˜åº¦æœ€å¤§åŒ– åŒå‡»æ ‡ç­¾æ çš„ç©ºç™½åŒºåŸŸ æ¥è‡ªï¼šhttps://support.google.com/chrome/answer/157179?hl=zh-Hans\n","categories":"","description":"","excerpt":"chromeå¿«æ·é”® 1. æ ‡ç­¾é¡µå’Œçª—å£å¿«æ·é”® æ“ä½œ å¿«æ·é”® æ‰“å¼€æ–°çª—å£ âŒ˜ + n åœ¨æ— ç—•æ¨¡å¼ä¸‹æ‰“å¼€æ–°çª—å£ âŒ˜ + Shift + n æ‰“å¼€ â€¦","ref":"/linux-notes/keymap/chrome-keymap/","tags":["å¿«æ·é”®"],"title":"chromeå¿«æ·é”®"},{"body":"Git å‘½ä»¤è¯¦è§£ 1. ç¤ºæ„å›¾ Workspaceï¼šå·¥ä½œåŒº Index / Stageï¼šæš‚å­˜åŒº Repositoryï¼šä»“åº“åŒºï¼ˆæˆ–æœ¬åœ°ä»“åº“ï¼‰ Remoteï¼šè¿œç¨‹ä»“åº“ 2. Git å‘½ä»¤åˆ†ç±» 2.1. æ–°å»ºä»£ç åº“ # åœ¨å½“å‰ç›®å½•æ–°å»ºä¸€ä¸ªGitä»£ç åº“ $ git init # æ–°å»ºä¸€ä¸ªç›®å½•ï¼Œå°†å…¶åˆå§‹åŒ–ä¸ºGitä»£ç åº“ $ git init [project-name] # ä¸‹è½½ä¸€ä¸ªé¡¹ç›®å’Œå®ƒçš„æ•´ä¸ªä»£ç å†å² $ git clone [url] 2.2. é…ç½® Gitçš„è®¾ç½®æ–‡ä»¶ä¸º.gitconfigï¼Œå®ƒå¯ä»¥åœ¨ç”¨æˆ·ä¸»ç›®å½•ä¸‹ï¼ˆå…¨å±€é…ç½®ï¼‰ï¼Œä¹Ÿå¯ä»¥åœ¨é¡¹ç›®ç›®å½•ä¸‹ï¼ˆé¡¹ç›®é…ç½®ï¼‰ã€‚\n# æ˜¾ç¤ºå½“å‰çš„Gité…ç½® $ git config --list # ç¼–è¾‘Gité…ç½®æ–‡ä»¶ $ git config -e [--global] # è®¾ç½®æäº¤ä»£ç æ—¶çš„ç”¨æˆ·ä¿¡æ¯ $ git config [--global] user.name \"[name]\" $ git config [--global] user.email \"[email address]\" 2.3. å¢åŠ /åˆ é™¤æ–‡ä»¶ # æ·»åŠ æŒ‡å®šæ–‡ä»¶åˆ°æš‚å­˜åŒº $ git add [file1] [file2] ... # æ·»åŠ æŒ‡å®šç›®å½•åˆ°æš‚å­˜åŒºï¼ŒåŒ…æ‹¬å­ç›®å½• $ git add [dir] # æ·»åŠ å½“å‰ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶åˆ°æš‚å­˜åŒº $ git add . # æ·»åŠ æ¯ä¸ªå˜åŒ–å‰ï¼Œéƒ½ä¼šè¦æ±‚ç¡®è®¤ # å¯¹äºåŒä¸€ä¸ªæ–‡ä»¶çš„å¤šå¤„å˜åŒ–ï¼Œå¯ä»¥å®ç°åˆ†æ¬¡æäº¤ $ git add -p # åˆ é™¤å·¥ä½œåŒºæ–‡ä»¶ï¼Œå¹¶ä¸”å°†è¿™æ¬¡åˆ é™¤æ”¾å…¥æš‚å­˜åŒº $ git rm [file1] [file2] ... # åœæ­¢è¿½è¸ªæŒ‡å®šæ–‡ä»¶ï¼Œä½†è¯¥æ–‡ä»¶ä¼šä¿ç•™åœ¨å·¥ä½œåŒº $ git rm --cached [file] # æ”¹åæ–‡ä»¶ï¼Œå¹¶ä¸”å°†è¿™ä¸ªæ”¹åæ”¾å…¥æš‚å­˜åŒº $ git mv [file-original] [file-renamed] 2.4. ä»£ç æäº¤ # æäº¤æš‚å­˜åŒºåˆ°ä»“åº“åŒº $ git commit -m [message] # æäº¤æš‚å­˜åŒºçš„æŒ‡å®šæ–‡ä»¶åˆ°ä»“åº“åŒº $ git commit [file1] [file2] ... -m [message] # æäº¤å·¥ä½œåŒºè‡ªä¸Šæ¬¡commitä¹‹åçš„å˜åŒ–ï¼Œç›´æ¥åˆ°ä»“åº“åŒº $ git commit -a # æäº¤æ—¶æ˜¾ç¤ºæ‰€æœ‰diffä¿¡æ¯ $ git commit -v # ä½¿ç”¨ä¸€æ¬¡æ–°çš„commitï¼Œæ›¿ä»£ä¸Šä¸€æ¬¡æäº¤ # å¦‚æœä»£ç æ²¡æœ‰ä»»ä½•æ–°å˜åŒ–ï¼Œåˆ™ç”¨æ¥æ”¹å†™ä¸Šä¸€æ¬¡commitçš„æäº¤ä¿¡æ¯ $ git commit --amend -m [message] # é‡åšä¸Šä¸€æ¬¡commitï¼Œå¹¶åŒ…æ‹¬æŒ‡å®šæ–‡ä»¶çš„æ–°å˜åŒ– $ git commit --amend [file1] [file2] ... 2.5. åˆ†æ”¯ # åˆ—å‡ºæ‰€æœ‰æœ¬åœ°åˆ†æ”¯ $ git branch # åˆ—å‡ºæ‰€æœ‰è¿œç¨‹åˆ†æ”¯ $ git branch -r # åˆ—å‡ºæ‰€æœ‰æœ¬åœ°åˆ†æ”¯å’Œè¿œç¨‹åˆ†æ”¯ $ git branch -a # æ–°å»ºä¸€ä¸ªåˆ†æ”¯ï¼Œä½†ä¾ç„¶åœç•™åœ¨å½“å‰åˆ†æ”¯ $ git branch [branch-name] # æ–°å»ºä¸€ä¸ªåˆ†æ”¯ï¼Œå¹¶åˆ‡æ¢åˆ°è¯¥åˆ†æ”¯ $ git checkout -b [branch] # æ–°å»ºä¸€ä¸ªåˆ†æ”¯ï¼ŒæŒ‡å‘æŒ‡å®šcommit $ git branch [branch] [commit] # æ–°å»ºä¸€ä¸ªåˆ†æ”¯ï¼Œä¸æŒ‡å®šçš„è¿œç¨‹åˆ†æ”¯å»ºç«‹è¿½è¸ªå…³ç³» $ git branch --track [branch] [remote-branch] # åˆ‡æ¢åˆ°æŒ‡å®šåˆ†æ”¯ï¼Œå¹¶æ›´æ–°å·¥ä½œåŒº $ git checkout [branch-name] # åˆ‡æ¢åˆ°ä¸Šä¸€ä¸ªåˆ†æ”¯ $ git checkout - # å»ºç«‹è¿½è¸ªå…³ç³»ï¼Œåœ¨ç°æœ‰åˆ†æ”¯ä¸æŒ‡å®šçš„è¿œç¨‹åˆ†æ”¯ä¹‹é—´ $ git branch --set-upstream [branch] [remote-branch] # åˆå¹¶æŒ‡å®šåˆ†æ”¯åˆ°å½“å‰åˆ†æ”¯ $ git merge [branch] # é€‰æ‹©ä¸€ä¸ªcommitï¼Œåˆå¹¶è¿›å½“å‰åˆ†æ”¯ $ git cherry-pick [commit] # åˆ é™¤åˆ†æ”¯ $ git branch -d [branch-name] # åˆ é™¤è¿œç¨‹åˆ†æ”¯ $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 2.6. æ ‡ç­¾ # åˆ—å‡ºæ‰€æœ‰tag $ git tag # æ–°å»ºä¸€ä¸ªtagåœ¨å½“å‰commit $ git tag [tag] # æ–°å»ºä¸€ä¸ªtagåœ¨æŒ‡å®šcommit $ git tag [tag] [commit] # åˆ é™¤æœ¬åœ°tag $ git tag -d [tag] # åˆ é™¤è¿œç¨‹tag $ git push origin :refs/tags/[tagName] # æŸ¥çœ‹tagä¿¡æ¯ $ git show [tag] # æäº¤æŒ‡å®štag $ git push [remote] [tag] # æäº¤æ‰€æœ‰tag $ git push [remote] --tags # æ–°å»ºä¸€ä¸ªåˆ†æ”¯ï¼ŒæŒ‡å‘æŸä¸ªtag $ git checkout -b [branch] [tag] 2.7. æŸ¥çœ‹ä¿¡æ¯ # æ˜¾ç¤ºæœ‰å˜æ›´çš„æ–‡ä»¶ $ git status # æ˜¾ç¤ºå½“å‰åˆ†æ”¯çš„ç‰ˆæœ¬å†å² $ git log # æ˜¾ç¤ºcommitå†å²ï¼Œä»¥åŠæ¯æ¬¡commitå‘ç”Ÿå˜æ›´çš„æ–‡ä»¶ $ git log --stat # æœç´¢æäº¤å†å²ï¼Œæ ¹æ®å…³é”®è¯ $ git log -S [keyword] # æ˜¾ç¤ºæŸä¸ªcommitä¹‹åçš„æ‰€æœ‰å˜åŠ¨ï¼Œæ¯ä¸ªcommitå æ®ä¸€è¡Œ $ git log [tag] HEAD --pretty=format:%s # æ˜¾ç¤ºæŸä¸ªcommitä¹‹åçš„æ‰€æœ‰å˜åŠ¨ï¼Œå…¶\"æäº¤è¯´æ˜\"å¿…é¡»ç¬¦åˆæœç´¢æ¡ä»¶ $ git log [tag] HEAD --grep feature # æ˜¾ç¤ºæŸä¸ªæ–‡ä»¶çš„ç‰ˆæœ¬å†å²ï¼ŒåŒ…æ‹¬æ–‡ä»¶æ”¹å $ git log --follow [file] $ git whatchanged [file] # æ˜¾ç¤ºæŒ‡å®šæ–‡ä»¶ç›¸å…³çš„æ¯ä¸€æ¬¡diff $ git log -p [file] # æ˜¾ç¤ºè¿‡å»5æ¬¡æäº¤ $ git log -5 --pretty --oneline # æ˜¾ç¤ºæ‰€æœ‰æäº¤è¿‡çš„ç”¨æˆ·ï¼ŒæŒ‰æäº¤æ¬¡æ•°æ’åº $ git shortlog -sn # æ˜¾ç¤ºæŒ‡å®šæ–‡ä»¶æ˜¯ä»€ä¹ˆäººåœ¨ä»€ä¹ˆæ—¶é—´ä¿®æ”¹è¿‡ $ git blame [file] # æ˜¾ç¤ºæš‚å­˜åŒºå’Œå·¥ä½œåŒºçš„å·®å¼‚ $ git diff # æ˜¾ç¤ºæš‚å­˜åŒºå’Œä¸Šä¸€ä¸ªcommitçš„å·®å¼‚ $ git diff --cached [file] # æ˜¾ç¤ºå·¥ä½œåŒºä¸å½“å‰åˆ†æ”¯æœ€æ–°commitä¹‹é—´çš„å·®å¼‚ $ git diff HEAD # æ˜¾ç¤ºä¸¤æ¬¡æäº¤ä¹‹é—´çš„å·®å¼‚ $ git diff [first-branch]...[second-branch] # æ˜¾ç¤ºä»Šå¤©ä½ å†™äº†å¤šå°‘è¡Œä»£ç  $ git diff --shortstat \"@{0 day ago}\" # æ˜¾ç¤ºæŸæ¬¡æäº¤çš„å…ƒæ•°æ®å’Œå†…å®¹å˜åŒ– $ git show [commit] # æ˜¾ç¤ºæŸæ¬¡æäº¤å‘ç”Ÿå˜åŒ–çš„æ–‡ä»¶ $ git show --name-only [commit] # æ˜¾ç¤ºæŸæ¬¡æäº¤æ—¶ï¼ŒæŸä¸ªæ–‡ä»¶çš„å†…å®¹ $ git show [commit]:[filename] # æ˜¾ç¤ºå½“å‰åˆ†æ”¯çš„æœ€è¿‘å‡ æ¬¡æäº¤ $ git reflog 2.8. è¿œç¨‹åŒæ­¥ # ä¸‹è½½è¿œç¨‹ä»“åº“çš„æ‰€æœ‰å˜åŠ¨ $ git fetch [remote] # æ˜¾ç¤ºæ‰€æœ‰è¿œç¨‹ä»“åº“ $ git remote -v # æ˜¾ç¤ºæŸä¸ªè¿œç¨‹ä»“åº“çš„ä¿¡æ¯ $ git remote show [remote] # å¢åŠ ä¸€ä¸ªæ–°çš„è¿œç¨‹ä»“åº“ï¼Œå¹¶å‘½å $ git remote add [shortname] [url] # å–å›è¿œç¨‹ä»“åº“çš„å˜åŒ–ï¼Œå¹¶ä¸æœ¬åœ°åˆ†æ”¯åˆå¹¶ $ git pull [remote] [branch] # ä¸Šä¼ æœ¬åœ°æŒ‡å®šåˆ†æ”¯åˆ°è¿œç¨‹ä»“åº“ $ git push [remote] [branch] # å¼ºè¡Œæ¨é€å½“å‰åˆ†æ”¯åˆ°è¿œç¨‹ä»“åº“ï¼Œå³ä½¿æœ‰å†²çª $ git push [remote] --force # æ¨é€æ‰€æœ‰åˆ†æ”¯åˆ°è¿œç¨‹ä»“åº“ $ git push [remote] --all 2.9. æ’¤é”€ # æ¢å¤æš‚å­˜åŒºçš„æŒ‡å®šæ–‡ä»¶åˆ°å·¥ä½œåŒº $ git checkout [file] # æ¢å¤æŸä¸ªcommitçš„æŒ‡å®šæ–‡ä»¶åˆ°æš‚å­˜åŒºå’Œå·¥ä½œåŒº $ git checkout [commit] [file] # æ¢å¤æš‚å­˜åŒºçš„æ‰€æœ‰æ–‡ä»¶åˆ°å·¥ä½œåŒº $ git checkout . # é‡ç½®æš‚å­˜åŒºçš„æŒ‡å®šæ–‡ä»¶ï¼Œä¸ä¸Šä¸€æ¬¡commitä¿æŒä¸€è‡´ï¼Œä½†å·¥ä½œåŒºä¸å˜ $ git reset [file] # é‡ç½®æš‚å­˜åŒºä¸å·¥ä½œåŒºï¼Œä¸ä¸Šä¸€æ¬¡commitä¿æŒä¸€è‡´ $ git reset --hard # é‡ç½®å½“å‰åˆ†æ”¯çš„æŒ‡é’ˆä¸ºæŒ‡å®šcommitï¼ŒåŒæ—¶é‡ç½®æš‚å­˜åŒºï¼Œä½†å·¥ä½œåŒºä¸å˜ $ git reset [commit] # é‡ç½®å½“å‰åˆ†æ”¯çš„HEADä¸ºæŒ‡å®šcommitï¼ŒåŒæ—¶é‡ç½®æš‚å­˜åŒºå’Œå·¥ä½œåŒºï¼Œä¸æŒ‡å®šcommitä¸€è‡´ $ git reset --hard [commit] # é‡ç½®å½“å‰HEADä¸ºæŒ‡å®šcommitï¼Œä½†ä¿æŒæš‚å­˜åŒºå’Œå·¥ä½œåŒºä¸å˜ $ git reset --keep [commit] # æ–°å»ºä¸€ä¸ªcommitï¼Œç”¨æ¥æ’¤é”€æŒ‡å®šcommit # åè€…çš„æ‰€æœ‰å˜åŒ–éƒ½å°†è¢«å‰è€…æŠµæ¶ˆï¼Œå¹¶ä¸”åº”ç”¨åˆ°å½“å‰åˆ†æ”¯ $ git revert [commit] # æš‚æ—¶å°†æœªæäº¤çš„å˜åŒ–ç§»é™¤ï¼Œç¨åå†ç§»å…¥ $ git stash $ git stash pop 2.10. å…¶ä»– # ç”Ÿæˆä¸€ä¸ªå¯ä¾›å‘å¸ƒçš„å‹ç¼©åŒ… $ git archive # è®¾ç½®æ¢è¡Œç¬¦ä¸ºLF git config --global core.autocrlf false #æ‹’ç»æäº¤åŒ…å«æ··åˆæ¢è¡Œç¬¦çš„æ–‡ä»¶ git config --global core.safecrlf true å‚è€ƒæ–‡ç« ï¼š\nhttp://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html ","categories":"","description":"","excerpt":"Git å‘½ä»¤è¯¦è§£ 1. ç¤ºæ„å›¾ Workspaceï¼šå·¥ä½œåŒº Index / Stageï¼šæš‚å­˜åŒº Repositoryï¼šä»“åº“åŒºï¼ˆæˆ–æœ¬åœ°ä»“åº“ï¼‰ â€¦","ref":"/linux-notes/git/git-commands/","tags":["Git"],"title":"Gitå‘½ä»¤åˆ†ç±»"},{"body":"1. å¸¸ç”¨å‘½ä»¤ 1.1. æŸ¥çœ‹å½“å‰VIPåœ¨å“ªä¸ªèŠ‚ç‚¹ä¸Š # æŸ¥çœ‹VIPæ˜¯å¦åœ¨ç­›é€‰ç»“æœä¸­ ip addr show|grep \"scope global\" # æˆ–è€… ip addr show|grep {vip} 1.2. æŸ¥çœ‹keepalivedçš„æ—¥å¿— tail /var/log/messages 1.3. æŠ“åŒ…å‘½ä»¤ # æŠ“åŒ… tcpdump -nn vrrp # å¯ä»¥ç”¨è¿™æ¡å‘½ä»¤æ¥æŸ¥çœ‹è¯¥ç½‘ç»œä¸­æ‰€å­˜åœ¨çš„vrid tcpdump -nn -i any net 224.0.0.0/8 # tcpdump -nn -i any net 224.0.0.0/8 # tcpdump -nn vrrp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 14:40:00.576387 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:01.577605 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:02.578429 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:03.579605 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:04.580443 IP 192.168.98.57 \u003e 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 1.4. VIPæ“ä½œ # è§£ç»‘VIP ip addr del dev # ç»‘å®šVIP ip addr add dev 1.5. keepalived åˆ‡ VIP ä¾‹å¦‚å°† A æœºå™¨ä¸Šçš„ VIP è¿ç§»åˆ°B æœºå™¨ä¸Šã€‚\n1.5.1. åœæ­¢keepalivedæœåŠ¡ åœæ­¢è¢«è¿ç§»çš„æœºå™¨ï¼ˆAæœºå™¨ï¼‰çš„keepalivedæœåŠ¡ã€‚\nsystemctl stop keepalived 1.5.2. æŸ¥çœ‹æ—¥å¿— è§£ç»‘ Aæœºå™¨ VIPçš„æ—¥å¿—\nSep 19 14:28:09 localhost systemd: Stopping LVS and VRRP High Availability Monitor... Sep 19 14:28:09 localhost Keepalived[45705]: Stopping Sep 19 14:28:09 localhost Keepalived_vrrp[45707]: VRRP_Instance(twemproxy) sent 0 priority Sep 19 14:28:09 localhost Keepalived_vrrp[45707]: VRRP_Instance(twemproxy) removing protocol VIPs. Sep 19 14:28:09 localhost Keepalived_healthcheckers[45706]: Stopped Sep 19 14:28:10 localhost Keepalived_vrrp[45707]: Stopped Sep 19 14:28:10 localhost Keepalived[45705]: Stopped Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 Sep 19 14:28:10 localhost systemd: Stopped LVS and VRRP High Availability Monitor. Sep 19 14:28:10 localhost ntpd[1186]: Deleting interface #10 bond0, 192.168.99.9#123, interface stats: received=0, sent=0, dropped=0, active_time=6755768 secs ç»‘å®š B æœºå™¨ VIPçš„æ—¥å¿—\nSep 17 17:20:25 localhost systemd: Starting LVS and VRRP High Availability Monitor... Sep 17 17:20:26 localhost Keepalived[34566]: Starting Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 Sep 17 17:20:26 localhost Keepalived[34566]: Opening file '/etc/keepalived/keepalived.conf'. Sep 17 17:20:26 localhost Keepalived[34568]: Starting Healthcheck child process, pid=34569 Sep 17 17:20:26 localhost Keepalived[34568]: Starting VRRP child process, pid=34570 Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering Kernel netlink reflector Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering Kernel netlink command channel Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering gratuitous ARP shared channel Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Opening file '/etc/keepalived/keepalived.conf'. Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Truncating auth_pass to 8 characters Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP_Instance(twemproxy) removing protocol VIPs. Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Using LinkWatch kernel netlink reflector... Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP_Instance(twemproxy) Entering BACKUP STATE Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP sockpool: [ifindex(4), proto(112), unicast(0), fd(10,11)] Sep 17 17:20:26 localhost systemd: Started LVS and VRRP High Availability Monitor. Sep 17 17:20:26 localhost kernel: IPVS: Registered protocols (TCP, UDP, SCTP, AH, ESP) Sep 17 17:20:26 localhost kernel: IPVS: Connection hash table configured (size=4096, memory=64Kbytes) Sep 17 17:20:26 localhost kernel: IPVS: Creating netns size=2192 id=0 Sep 17 17:20:26 localhost kernel: IPVS: Creating netns size=2192 id=1 Sep 17 17:20:26 localhost kernel: IPVS: ipvs loaded. Sep 17 17:20:26 localhost Keepalived_healthcheckers[34569]: Opening file '/etc/keepalived/keepalived.conf'. 2. æŒ‡å®škeepalivedçš„è¾“å‡ºæ—¥å¿—æ–‡ä»¶ 2.1. ä¿®æ”¹ /etc/sysconfig/keepalived å°†KEEPALIVED_OPTIONS=\"-D\"æ”¹ä¸ºKEEPALIVED_OPTIONS=\"-D -d -S 0\"ã€‚\n# Options for keepalived. See `keepalived --help' output and keepalived(8) and # keepalived.conf(5) man pages for a list of all options. Here are the most # common ones : # # --vrrp -P Only run with VRRP subsystem. # --check -C Only run with Health-checker subsystem. # --dont-release-vrrp -V Dont remove VRRP VIPs \u0026 VROUTEs on daemon stop. # --dont-release-ipvs -I Dont remove IPVS topology on daemon stop. # --dump-conf -d Dump the configuration data. # --log-detail -D Detailed log messages. # --log-facility -S 0-7 Set local syslog facility (default=LOG_DAEMON) # KEEPALIVED_OPTIONS=\"-D -d -S 0\" 2.2. ä¿®æ”¹rsyslogçš„é…ç½® /etc/rsyslog.conf åœ¨/etc/rsyslog.conf æ·»åŠ  keepalivedçš„æ—¥å¿—è·¯å¾„\nvi /etc/rsyslog.conf ... # keepalived log local0.* /etc/keepalived/keepalived.log 2.3. é‡å¯rsyslogå’Œkeepalived # é‡å¯rsyslog systemctl restart rsyslog # é‡å¯keepalived systemctl restart keepalived 3. Troubleshooting 3.1. virtual_router_id åŒç½‘æ®µé‡å¤ æ—¥å¿—æŠ¥é”™å¦‚ä¸‹ï¼š\nMar 09 21:28:28 k8s4 Keepalived_vrrp[8548]: bogus VRRP packet received on eth0 !!! Mar 09 21:28:28 k8s4 Keepalived_vrrp[8548]: VRRP_Instance(VI-kube-master) ignoring received advertisment... Mar 09 21:28:43 k8s4 Keepalived_vrrp[8548]: ip address associated with VRID not present in received packet : 192.168.1.10 Mar 09 21:28:43 k8s4 Keepalived_vrrp[8548]: one or more VIP associated with VRID mismatch actual MASTER advert è§£å†³æ–¹æ³•:\nåŒä¸€ç½‘æ®µå†…LBèŠ‚ç‚¹é…ç½®çš„ virtual_router_id å€¼æœ‰é‡å¤äº†ï¼Œé€‰æ‹©ä¸€ä¸ªä¸é‡å¤çš„0~255ä¹‹é—´çš„å€¼ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å·²å­˜åœ¨çš„vridã€‚\ntcpdump -nn -i any net 224.0.0.0/8 3.2. Operation not permitted é—®é¢˜ï¼š\nä¸¤å°ä¸»å¤‡æœºå™¨éƒ½ç»‘å®šäº†VIPï¼ŒæŸ¥çœ‹æ—¥å¿—å¦‚ä¸‹ï¼š\nSep 28 14:28:37 node Keepalived_vrrp[1686]: (VI_1): send advert error 1 (Operation not permitted) Sep 28 14:28:39 node Keepalived_vrrp[1686]: (VI_1): send advert error 1 (Operation not permitted) åŸå› ï¼š\nç”±äºiptables vrrpåè®®æ²¡æœ‰æ”¾é€šï¼Œå¯¼è‡´keepalivedç›´æ¥æ— æ³•äº’ç›¸æ¢æµ‹é€‰ä¸»ã€‚\nè§£å†³æ–¹æ³•ï¼š\næ·»åŠ iptabels vrrpåè®®è§„åˆ™\niptables -A INPUT -p vrrp -j ACCEPT iptables -A OUTPUT -p vrrp -j ACCEPT æŒä¹…åŒ–iptablesè§„åˆ™ï¼Œæ·»åŠ è§„åˆ™åˆ°æ–‡ä»¶ä¸­/etc/sysconfig/iptables\n# vi /etc/sysconfig/iptables -A INPUT -p vrrp -j ACCEPT -A OUTPUT -p vrrp -j ACCEPT ","categories":"","description":"","excerpt":"1. å¸¸ç”¨å‘½ä»¤ 1.1. æŸ¥çœ‹å½“å‰VIPåœ¨å“ªä¸ªèŠ‚ç‚¹ä¸Š # æŸ¥çœ‹VIPæ˜¯å¦åœ¨ç­›é€‰ç»“æœä¸­ ip addr show|grep \"scope â€¦","ref":"/linux-notes/keepalived/keepalived-operation/","tags":["Keepalived"],"title":"Keepalivedç›¸å…³æ“ä½œ"},{"body":"1. kubectl-aliases kubectl-aliaseså¼€æºå·¥å…·æ˜¯ç”±è„šæœ¬é€šè¿‡æ‹¼æ¥å„ç§kubectlç›¸å…³å…ƒç´ ç»„æˆçš„aliaså‘½ä»¤åˆ«ååˆ—è¡¨ï¼Œå…¶ä¸­å‘½ä»¤åˆ«åæ‹¼æ¥å…ƒç´ å¦‚ä¸‹ï¼š\nbase [system?] [operation] [resource] [flags] kubectl -n=kube-system get\ndescribe rm:delete\nlogs\nexec\napply pods\ndeployment\nsecret\ningress\nnode svc\nns cm oyaml ojson\nowide all\nwatch\nfile\nl k=kubectl sys=--namespace kube-system commands: g=get d=describe rm=delete a:apply -f ex: exec -i -t lo: logs -f resources: po=pod dep=deployment ing=ingress svc=service cm=configmap sec=secret ns=namespace no=node flags: output format: oyaml, ojson, owide all: --all or --all-namespaces depending on the command sl: --show-labels w=-w/--watch value flags (should be at the end): f=-f/--filename l=-l/--selector 2. ç¤ºä¾‹ # ç¤ºä¾‹1 kd â†’ kubectl describe # ç¤ºä¾‹2 kgdepallw â†’ kubectl get deployment â€”all-namespaces â€”watch alias getç¤ºä¾‹ï¼š\nalias k='kubectl' alias kg='kubectl get' alias kgpo='kubectl get pods' alias kgpoojson='kubectl get pods -o=json' alias kgpon='kubectl get pods --namespace' alias ksysgpooyamll='kubectl --namespace=kube-system get pods -o=yaml -l' 3. å®‰è£… # å°† .kubectl_aliasesä¸‹è½½åˆ° home ç›®å½• cd ~ \u0026\u0026 wget https://raw.githubusercontent.com/ahmetb/kubectl-aliases/master/.kubectl_aliases # å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ° .bashrcä¸­ï¼Œå¹¶æ‰§è¡Œ source .bashrc [ -f ~/.kubectl_aliases ] \u0026\u0026 source ~/.kubectl_aliases function kubectl() { command kubectl $@; } # å¦‚æœéœ€è¦æç¤ºåˆ«åçš„å®Œæ•´å‘½ä»¤ï¼Œåˆ™å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ° .bashrcä¸­ï¼Œå¹¶æ‰§è¡Œ source .bashrc [ -f ~/.kubectl_aliases ] \u0026\u0026 source ~/.kubectl_aliases function kubectl() { echo \"+ kubectl $@\"; command kubectl $@; } å‚è€ƒï¼š\nhttps://ahmet.im/blog/kubectl-aliases/ https://github.com/ahmetb/kubectl-aliases ","categories":"","description":"","excerpt":"1. kubectl-aliases kubectl-aliaseså¼€æºå·¥å…·æ˜¯ç”±è„šæœ¬é€šè¿‡æ‹¼æ¥å„ç§kubectlç›¸å…³å…ƒç´ ç»„æˆçš„aliaså‘½ä»¤åˆ« â€¦","ref":"/kubernetes-notes/operation/kubectl/kubectl-alias/","tags":["Kubernetes"],"title":"kubectlå‘½ä»¤åˆ«å"},{"body":"1. ä¼ è¾“å±‚çš„ä½œç”¨ 1.1. ä¼ è¾“å±‚çš„å®šä¹‰ IPé¦–éƒ¨æœ‰ä¸ªåè®®å­—æ®µï¼Œç”¨æ¥æ ‡è¯†ä¼ è¾“å±‚åè®®ï¼Œè¯†åˆ«æ•°æ®æ˜¯TCPçš„å†…å®¹è¿˜æ˜¯UDPçš„å†…å®¹ã€‚åŒæ ·ï¼Œä¼ è¾“å±‚ï¼Œä¸ºäº†è¯†åˆ«æ•°æ®åº”è¯¥å‘ç»™å“ªä¸ªåº”ç”¨ä¹Ÿè®¾å®šäº†è¿™æ ·çš„ç¼–å·ï¼Œå³ç«¯å£ã€‚\n1.2. é€šä¿¡å¤„ç† åº”ç”¨åè®®å¤§å¤šä»¥C/Så½¢å¼è¿è¡Œï¼Œå³æœåŠ¡ç«¯éœ€æå‰å¯åŠ¨æœåŠ¡ï¼Œç›‘å¬æŸä¸ªç«¯å£ï¼Œå½“å®¢æˆ·ç«¯å¾€è¯¥ç«¯å£å‘é€æ•°æ®æ—¶ï¼Œå¯ä»¥åŠæ—¶å¤„ç†è¯·æ±‚ã€‚\næœåŠ¡ç«¯ç¨‹åºåœ¨UNIXç³»ç»Ÿä¸­ç§°ä¸ºå®ˆæŠ¤è¿›ç¨‹ï¼Œä¾‹å¦‚HTTPçš„æœåŠ¡ç«¯ç¨‹åºä¸ºhttpdï¼›sshçš„æœåŠ¡ç«¯ç¨‹åºä¸ºsshdã€‚UNIXä¸­ä¸å¿…è¦é€ä¸ªå¯åŠ¨è¿™äº›å®ˆæŠ¤è¿›ç¨‹ï¼Œè€Œæ˜¯ç”±è¶…çº§å®ˆæŠ¤è¿›ç¨‹inetd(äº’è”ç½‘å®ˆæŠ¤è¿›ç¨‹)å¯åŠ¨ï¼Œå½“æ”¶åˆ°å®¢æˆ·ç«¯è¯·æ±‚æ—¶ä¼šåˆ›å»ºï¼ˆforkï¼‰æ–°çš„è¿›ç¨‹å¹¶è½¬æ¢ï¼ˆexecï¼‰ä¸ºhttpdç­‰å„ä¸ªå®ˆæŠ¤è¿›ç¨‹ã€‚æ ¹æ®è¯·æ±‚ç«¯å£åˆ†é…åˆ°å¯¹åº”çš„æœåŠ¡ç«¯å®ˆæŠ¤è¿›ç¨‹ä¸Šå¤„ç†ã€‚\n1.3. TCPå’ŒUDP 1.3.1. TCP TCPæ˜¯é¢å‘è¿æ¥ã€å¯é çš„æ•°æ®æµã€‚æµå°±æ˜¯ä¸é—´æ–­çš„æ•°æ®ç»“æ„ï¼Œå¯ç†è§£ä¸ºæ°´ç®¡ä¸­çš„æ°´æµã€‚è™½ç„¶å¯ä»¥ä¿è¯å‘é€é¡ºåºï¼Œä½†çŠ¹å¦‚æ²¡æœ‰é—´éš”çš„å‘é€æ•°æ®æµç»™æ¥æ”¶ç«¯ã€‚ä¾‹å¦‚ï¼šå‘é€10æ¬¡100å­—èŠ‚çš„æ¶ˆæ¯ï¼Œæ¥æ”¶ç«¯å¯èƒ½ä¼šæ”¶åˆ°ä¸€ä¸ª1000å­—èŠ‚è¿ç»­ä¸æ–­çš„æ•°æ®ã€‚TCPä¸ºå®ç°å¯é ä¼ è¾“ï¼Œå®è¡Œâ€œé¡ºåºæ§åˆ¶â€å’Œâ€œé‡å‘æ§åˆ¶â€ï¼›è¿˜å…·å¤‡â€œæµé‡æ§åˆ¶â€ã€â€œæ‹¥å¡æ§åˆ¶â€ã€æé«˜ç½‘ç»œåˆ©ç”¨ç‡ç­‰ã€‚TCPå¯ä»¥ç±»æ¯”ä¸ºâ€œæ‰“ç”µè¯â€ï¼Œæœ‰å»æœ‰å›ã€‚\n1.3.2. UDP UDPæ˜¯ä¸å…·å¤‡å¯é æ€§çš„æ•°æ®æŠ¥åè®®ï¼Œå¯ä»¥ç¡®ä¿å‘é€æ¶ˆæ¯çš„å¤§å°ï¼Œä½†ä¸èƒ½ä¿è¯æ¶ˆæ¯ä¸€å®šåˆ°è¾¾ï¼Œåº”ç”¨æœ‰æ—¶ä¼šæ ¹æ®è‡ªå·±çš„éœ€è¦è¿›è¡Œé‡å‘å¤„ç†ã€‚UDPå¯ä»¥ç±»æ¯”â€œå‘çŸ­ä¿¡â€ï¼Œæœ‰å»æ— å›ã€‚\n1.3.3. å¥—æ¥å­— åº”ç”¨åœ¨ä½¿ç”¨TCPæˆ–UDPæ—¶ä¼šç”¨åˆ°ç³»ç»Ÿæä¾›çš„ç±»åº“ï¼Œå³APIï¼ˆåº”ç”¨ç¼–ç¨‹æ¥å£ï¼‰ï¼Œé€šä¿¡æ—¶ä¼šç”¨åˆ°å¥—æ¥å­—ï¼ˆsocketï¼‰çš„APIã€‚åº”ç”¨ç¨‹åºåˆ©ç”¨å¥—æ¥å­—ï¼Œå¯ä»¥è®¾ç½®å¯¹ç«¯çš„IPåœ°å€ã€ç«¯å£å·ï¼Œå¹¶å®ç°æ•°æ®çš„å‘é€ä¸æ¥æ”¶ã€‚\n2. ç«¯å£å· 2.1. ç«¯å£å·çš„å®šä¹‰ ç±»åˆ« åœ°å€ å±‚ è¯´æ˜ ç«¯å£å· ç¨‹åºåœ°å€ ä¼ è¾“å±‚ åŒä¸€ä¸ªè®¡ç®—æœºä¸­ä¸åŒçš„åº”ç”¨ç¨‹åº IPåœ°å€ ä¸»æœºåœ°å€ ç½‘ç»œå±‚ è¯†åˆ«TCP/IPç½‘ç»œä¸­ä¸åŒçš„ä¸»æœºæˆ–è·¯ç”±å™¨ MACåœ°å€ ç‰©ç†åœ°å€ æ•°æ®é“¾è·¯å±‚ åœ¨åŒä¸€ä¸ªæ•°æ®é“¾è·¯ä¸­è¯†åˆ«ä¸åŒçš„è®¡ç®—æœº æŠŠæ•°æ®ä¼ è¾“æ¯”ä½œå¿«é€’ä¼ é€’ï¼›IPåœ°å€å°±åƒä½ çš„å®¶åº­åœ°å€ï¼›é‚£ä¹ˆç«¯å£å·ç›¸å½“äºä½ å®¶å…·ä½“çš„æ”¶ä»¶äººï¼›çŸ¥é“äº†å®¶åº­åœ°å€å’Œæ”¶ä»¶äººæ‰èƒ½å°†å¿«é€’å‡†ç¡®é€è¾¾ã€‚\n2.2. æ ¹æ®ç«¯å£å·è¯†åˆ«åº”ç”¨ 2.3. é€šè¿‡IPåœ°å€ã€ç«¯å£å·ã€åè®®å·è¿›è¡Œé€šä¿¡ 5ä¸ªä¿¡æ¯å”¯ä¸€æ ‡è¯†ä¸€ä¸ªé€šä¿¡ï¼šæºåœ°å€IPã€ç›®æ ‡åœ°å€IPã€åè®®å·ã€æºç«¯å£å·ã€ç›®æ ‡ç«¯å£å·ã€‚\n2.4. ç«¯å£å·å¦‚ä½•ç¡®å®š 2.4.1. æ ‡å‡†æ—¢å®šçš„ç«¯å£å· è¯¥æ–¹æ³•ä¹Ÿå«é™æ€æ–¹æ³•ï¼Œæ˜¯æŒ‡æ¯ä¸ªåº”ç”¨ç¨‹åºéƒ½æœ‰å…¶æŒ‡å®šçš„ç«¯å£å·ã€‚ä¾‹å¦‚HTTPã€FTPç­‰åº”ç”¨åè®®ä½¿ç”¨çš„ç«¯å£å·ï¼Œè¿™ç±»ç«¯å£å·ç§°ä¸ºçŸ¥åç«¯å£å·ï¼Œä¸€èˆ¬ç”±0-1023çš„æ•°å­—åˆ†é…è€Œæˆã€‚é™¤çŸ¥åç«¯å£å·å¤–ï¼Œè¿˜æœ‰ä¸€äº›ç«¯å£å·ä¹Ÿè¢«æ­£å¼æ³¨å†Œï¼Œåˆ†å¸ƒåœ¨1024-49151çš„æ•°å­—ä¹‹é—´ã€‚è¿™äº›ç«¯å£å¯ç”¨äºä»»ä½•é€šä¿¡ç”¨é€”ã€‚\n2.4.2. æ—¶åºåˆ†é…æ³• è¯¥æ–¹æ³•ä¹Ÿå«åŠ¨æ€åˆ†é…æ³•ï¼ŒæœåŠ¡ç«¯æœ‰å¿…è¦ç¡®å®šç›‘å¬ç«¯å£å·ï¼Œä½†æ¥å—æœåŠ¡çš„å®¢æˆ·ç«¯æ²¡å¿…è¦ç¡®å®šç«¯å£å·ã€‚å®¢æˆ·ç«¯å¯ä»¥ä¸ç”¨è‡ªå·±è®¾ç½®ç«¯å£å·ï¼Œç”±æ“ä½œç³»ç»Ÿè¿›è¡Œåˆ†é…ã€‚æ“ä½œç³»ç»Ÿä¸ºæ¯ä¸ªåº”ç”¨ç¨‹åºåˆ†é…äº’ä¸å†²çªçš„ç«¯å£å·ã€‚ä¾‹å¦‚ï¼Œæ–°å¢ä¸€ä¸ªç«¯å£å·åˆ™åœ¨ä¹‹å‰çš„ç«¯å£å·ä¸ŠåŠ 1ï¼ŒåŠ¨æ€åˆ†é…çš„ç«¯å£å·å–å€¼èŒƒå›´ï¼š49152-65535ã€‚\n3. TCPåè®®æ¦‚è¿° TCP:Transmission Control Protocol (ä¼ è¾“æ§åˆ¶åè®®)ï¼ŒTCPå®ç°äº†æ•°æ®ä¼ è¾“æ—¶çš„å„ç§æ§åˆ¶åŠŸèƒ½ï¼Œå¯ä»¥è¿›è¡Œä¸¢åŒ…é‡å‘ï¼Œä¹±åºçº æ­£ï¼Œæ§åˆ¶é€šä¿¡æµé‡çš„æµªè´¹ã€‚\nå‚è€ƒï¼š\nã€Šå›¾è§£TCP/IPã€‹ ","categories":"","description":"","excerpt":"1. ä¼ è¾“å±‚çš„ä½œç”¨ 1.1. ä¼ è¾“å±‚çš„å®šä¹‰ IPé¦–éƒ¨æœ‰ä¸ªåè®®å­—æ®µï¼Œç”¨æ¥æ ‡è¯†ä¼ è¾“å±‚åè®®ï¼Œè¯†åˆ«æ•°æ®æ˜¯TCPçš„å†…å®¹è¿˜æ˜¯UDPçš„å†…å®¹ã€‚åŒæ ·ï¼Œä¼ è¾“å±‚ï¼Œ â€¦","ref":"/linux-notes/tcpip/tcp/","tags":["TCPIP"],"title":"TCPåè®®"},{"body":"1. Nginxçš„ç³»ç»Ÿæ¶æ„ NginxåŒ…å«ä¸€ä¸ªå•ä¸€çš„masterè¿›ç¨‹å’Œå¤šä¸ªworkerè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æ˜¯å•è¿›ç¨‹ï¼Œå¹¶ä¸”è®¾è®¡ä¸ºåŒæ—¶å¤„ç†æˆåƒä¸Šä¸‡ä¸ªè¿æ¥ã€‚ workerè¿›ç¨‹æ˜¯å¤„ç†è¿æ¥çš„åœ°æ–¹ï¼ŒNginxä½¿ç”¨äº†æ“ä½œç³»ç»Ÿäº‹ä»¶æœºåˆ¶æ¥å¿«é€Ÿå“åº”è¿™äº›è¯·æ±‚ã€‚ masterè¿›ç¨‹è´Ÿè´£è¯»å–é…ç½®æ–‡ä»¶ã€å¤„ç†å¥—æ¥å­—ã€æ´¾ç”Ÿworkerè¿›ç¨‹ã€æ‰“å¼€æ—¥å¿—æ–‡ä»¶å’Œç¼–è¯‘åµŒå…¥å¼çš„perlè„šæœ¬ã€‚masterè¿›ç¨‹æ˜¯ä¸€ä¸ªå¯ä»¥é€šè¿‡å¤„ç†ä¿¡å·é‡æ¥ç®¡ç†è¯·æ±‚çš„è¿›ç¨‹ã€‚ workerè¿›ç¨‹è¿è¡Œåœ¨ä¸€ä¸ªå¿™ç¢Œçš„äº‹ä»¶å¾ªç¯å¤„ç†ä¸­ï¼Œç”¨äºå¤„ç†è¿›å…¥çš„è¿æ¥ã€‚æ¯ä¸€ä¸ªnginxæ¨¡å—è¢«æ„ç­‘åœ¨workerä¸­ã€‚ä»»ä½•è¯·æ±‚å¤„ç†ã€è¿‡æ»¤ã€å¤„ç†ä»£ç†çš„è¿æ¥å’Œæ›´å¤šæ“ä½œéƒ½åœ¨workerä¸­å®Œæˆã€‚ å¦‚æœæ²¡æœ‰é˜»å¡workerè¿›ç¨‹çš„è¿›ç¨‹ï¼ˆä¾‹å¦‚ç£ç›˜I/Oï¼‰ï¼Œé‚£ä¹ˆéœ€è¦é…ç½®çš„workerè¿›ç¨‹è¦å¤šäºCPUå†…æ ¸æ•°ï¼Œä»¥ä¾¿å¤„ç†è´Ÿè½½ã€‚ 2. Httpæ ¸å¿ƒæ¨¡å— 2.1.1. server æŒ‡ä»¤serverå¼€å§‹ä¸€ä¸ªæ–°çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ã€‚\nhttp serveræŒ‡ä»¤\næŒ‡ä»¤ è¯´æ˜ port_in_redirect ç¡®è®¤nginxæ˜¯å¦å¯¹ç«¯å£æŒ‡å®šé‡å®šå‘ server åˆ›å»ºä¸€ä¸ªæ–°çš„é…ç½®åŒºåŸŸï¼Œå®šä¹‰ä¸€ä¸ªè™šæ‹Ÿä¸»æœºã€‚listenæŒ‡ä»¤æŒ‡å®šIPå’Œç«¯å£ï¼›server_nameåˆ—ä¸¾ç”¨äºåŒ¹é…çš„Hostå¤´å€¼ server_name é…ç½®ç”¨äºå“åº”è¯·æ±‚çš„è™šæ‹Ÿä¸»æœºåç§° server_name_in_redirect server_tokens åœ¨é”™è¯¯ä¿¡æ¯ä¸­ç¦æ­¢å‘é€nginxçš„ç‰ˆæœ¬å·å’Œserverå“åº”å¤´ 2.1.2. æ—¥å¿—æ ¼å¼ å‚æ•° è¯´æ˜ ç¤ºä¾‹ $remote_addr å®¢æˆ·ç«¯åœ°å€ 211.28.65.253 $remote_user å®¢æˆ·ç«¯ç”¨æˆ·åç§° -- $time_local è®¿é—®æ—¶é—´å’Œæ—¶åŒº 18/Jul/2012:17:00:01 +0800 $request è¯·æ±‚çš„URIå’ŒHTTPåè®® \"GET /article-10000.html HTTP/1.1\" $http_host è¯·æ±‚åœ°å€ï¼Œå³æµè§ˆå™¨ä¸­ä½ è¾“å…¥çš„åœ°å€ï¼ˆIPæˆ–åŸŸåï¼‰ www.it300.com192.168.100.100 $status HTTPè¯·æ±‚çŠ¶æ€ 200 $upstream_status upstreamçŠ¶æ€ 200 $body_bytes_sent å‘é€ç»™å®¢æˆ·ç«¯æ–‡ä»¶å†…å®¹å¤§å° 1547 $http_referer urlè·³è½¬æ¥æº https://www.baidu.com/ $http_user_agent ç”¨æˆ·ç»ˆç«¯æµè§ˆå™¨ç­‰ä¿¡æ¯ \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; GTB7.0; .NET4.0C; $ssl_protocol SSLåè®®ç‰ˆæœ¬ TLSv1 $ssl_cipher äº¤æ¢æ•°æ®ä¸­çš„ç®—æ³• RC4-SHA $upstream_addr åå°upstreamçš„åœ°å€ï¼Œå³çœŸæ­£æä¾›æœåŠ¡çš„ä¸»æœºåœ°å€ 10.10.10.100:80 $request_time æ•´ä¸ªè¯·æ±‚çš„æ€»æ—¶é—´ 0.205 $upstream_response_time è¯·æ±‚è¿‡ç¨‹ä¸­ï¼Œupstreamå“åº”æ—¶é—´ 0.002 æ—¥å¿—åˆ‡å‰²\n# vim /etc/logrotate.d/nginx /usr/local/nginx/logs/*.log{ #æŒ‡å®šè½¬å‚¨å‘¨æœŸä¸ºæ¯å¤© daily #ä¿ç•™30ä¸ªå¤‡ä»½ rotate 30 #éœ€è¦å‹ç¼© delaycompress #YYYYMMDDæ—¥æœŸæ ¼å¼ dateext #å¿½ç•¥é”™è¯¯ missingok #å¦‚æœæ—¥å¿—ä¸ºç©ºåˆ™ä¸åšè½®è¯¢ notifempty #åªä¸ºæ•´ä¸ªæ—¥å¿—ç»„è¿è¡Œä¸€æ¬¡çš„è„šæœ¬ sharedscripts #æ—¥å¿—è½®è¯¢åæ‰§è¡Œçš„è„šæœ¬ postrotate service nginx reload endscript } ","categories":"","description":"","excerpt":"1. Nginxçš„ç³»ç»Ÿæ¶æ„ NginxåŒ…å«ä¸€ä¸ªå•ä¸€çš„masterè¿›ç¨‹å’Œå¤šä¸ªworkerè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æ˜¯å•è¿›ç¨‹ï¼Œå¹¶ä¸”è®¾è®¡ä¸ºåŒæ—¶å¤„ç†æˆåƒä¸Šä¸‡ä¸ª â€¦","ref":"/linux-notes/nginx/nginx-http/","tags":["Nginx"],"title":"Nginx httpæœåŠ¡å™¨"},{"body":"beegoé¡¹ç›®é€»è¾‘ 1. è·¯ç”±è®¾ç½® 1.1. beego.Router å…¥å£æ–‡ä»¶main.go\npackage main import ( _ \"quickstart/routers\" \"github.com/astaxie/beego\" ) func main() { beego.Run() } goä¸­å¯¼å…¥åŒ…ä¸­initå‡½æ•°çš„æ‰§è¡Œé€»è¾‘\n_ \"quickstart/routers\",åŒ…åªå¼•å…¥æ‰§è¡Œäº†é‡Œé¢çš„initå‡½æ•°\npackage routers import ( \"quickstart/controllers\" \"github.com/astaxie/beego\" ) func init() { beego.Router(\"/\", \u0026controllers.MainController{}) } è·¯ç”±åŒ…é‡Œæ‰§è¡Œäº†è·¯ç”±æ³¨å†Œbeego.Router, è¿™ä¸ªå‡½æ•°çš„åŠŸèƒ½æ˜¯æ˜ å°„URLåˆ°controllerï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯URL(ç”¨æˆ·è¯·æ±‚çš„åœ°å€)ï¼Œè¿™é‡Œæ˜¯ /ï¼Œä¹Ÿå°±æ˜¯è®¿é—®çš„ä¸å¸¦ä»»ä½•å‚æ•°çš„URLï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯å¯¹åº”çš„ Controllerï¼Œå°±æ˜¯æŠŠè¯·æ±‚åˆ†å‘åˆ°é‚£ä¸ªæ§åˆ¶å™¨æ¥æ‰§è¡Œç›¸åº”çš„é€»è¾‘ã€‚\n1.2. beego.Run è§£æé…ç½®æ–‡ä»¶\nbeego ä¼šè‡ªåŠ¨è§£æåœ¨ conf ç›®å½•ä¸‹é¢çš„é…ç½®æ–‡ä»¶ app.confï¼Œé€šè¿‡ä¿®æ”¹é…ç½®æ–‡ä»¶ç›¸å…³çš„å±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ï¼šå¼€å¯çš„ç«¯å£ï¼Œæ˜¯å¦å¼€å¯ sessionï¼Œåº”ç”¨åç§°ç­‰ä¿¡æ¯ã€‚\næ‰§è¡Œç”¨æˆ·çš„hookfunc\nbeegoä¼šæ‰§è¡Œç”¨æˆ·æ³¨å†Œçš„hookfuncï¼Œé»˜è®¤çš„å·²ç»å­˜åœ¨äº†æ³¨å†Œmimeï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å‡½æ•°AddAPPStartHookæ³¨å†Œè‡ªå·±çš„å¯åŠ¨å‡½æ•°ã€‚\næ˜¯å¦å¼€å¯ session\nä¼šæ ¹æ®ä¸Šé¢é…ç½®æ–‡ä»¶çš„åˆ†æä¹‹ååˆ¤æ–­æ˜¯å¦å¼€å¯ sessionï¼Œå¦‚æœå¼€å¯çš„è¯å°±åˆå§‹åŒ–å…¨å±€çš„ sessionã€‚\næ˜¯å¦ç¼–è¯‘æ¨¡æ¿\nbeego ä¼šåœ¨å¯åŠ¨çš„æ—¶å€™æ ¹æ®é…ç½®æŠŠ views ç›®å½•ä¸‹çš„æ‰€æœ‰æ¨¡æ¿è¿›è¡Œé¢„ç¼–è¯‘ï¼Œç„¶åå­˜åœ¨ map é‡Œé¢ï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆçš„æé«˜æ¨¡æ¿è¿è¡Œçš„æ•ˆç‡ï¼Œæ— éœ€è¿›è¡Œå¤šæ¬¡ç¼–è¯‘ã€‚\næ˜¯å¦å¼€å¯æ–‡æ¡£åŠŸèƒ½\næ ¹æ®EnableDocsé…ç½®åˆ¤æ–­æ˜¯å¦å¼€å¯å†…ç½®çš„æ–‡æ¡£è·¯ç”±åŠŸèƒ½\næ˜¯å¦å¯åŠ¨ç®¡ç†æ¨¡å—\nbeego ç›®å‰åšäº†ä¸€ä¸ªå¾ˆé…·çš„æ¨¡å—ï¼Œåº”ç”¨å†…ç›‘æ§æ¨¡å—ï¼Œä¼šåœ¨ 8088 ç«¯å£åšä¸€ä¸ªå†…éƒ¨ç›‘å¬ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªç«¯å£æŸ¥è¯¢åˆ° QPSã€CPUã€å†…å­˜ã€GCã€goroutineã€thread ç­‰ç»Ÿè®¡ä¿¡æ¯ã€‚\nç›‘å¬æœåŠ¡ç«¯å£\nè¿™æ˜¯æœ€åä¸€æ­¥ä¹Ÿå°±æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„è®¿é—® 8080 çœ‹åˆ°çš„ç½‘é¡µç«¯å£ï¼Œå†…éƒ¨å…¶å®è°ƒç”¨äº† ListenAndServeï¼Œå……åˆ†åˆ©ç”¨äº† goroutine çš„ä¼˜åŠ¿ï¼Œä¸€æ—¦ run èµ·æ¥ä¹‹åï¼Œæˆ‘ä»¬çš„æœåŠ¡å°±ç›‘å¬åœ¨ä¸¤ä¸ªç«¯å£äº†ï¼Œä¸€ä¸ªæœåŠ¡ç«¯å£ 8080 ä½œä¸ºå¯¹å¤–æœåŠ¡ï¼Œå¦ä¸€ä¸ª 8088 ç«¯å£å®è¡Œå¯¹å†…ç›‘æ§ã€‚\n2. controller é€»è¾‘ package controllers import ( \"github.com/astaxie/beego\" ) type MainController struct { beego.Controller } func (this *MainController) Get() { this.Data[\"Website\"] = \"beego.me\" this.Data[\"Email\"] = \"astaxie@gmail.com\" this.TplName = \"index.tpl\" } 1ã€å£°æ˜äº†ä¸€ä¸ªæ§åˆ¶å™¨ MainControllerï¼Œè¿™ä¸ªæ§åˆ¶å™¨é‡Œé¢å†…åµŒäº† beego.Controllerï¼Œå³Go çš„åµŒå…¥æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯ MainController è‡ªåŠ¨æ‹¥æœ‰äº†æ‰€æœ‰ beego.Controller çš„æ–¹æ³•ã€‚è€Œ beego.Controller æ‹¥æœ‰å¾ˆå¤šæ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬ Initã€Prepareã€Postã€Getã€Deleteã€Headç­‰æ–¹æ³•ã€‚å¯ä»¥é€šè¿‡é‡å†™çš„æ–¹å¼æ¥å®ç°è¿™äº›æ–¹æ³•ï¼Œä»¥ä¸Šä¾‹å­é‡å†™äº† Get æ–¹æ³•ã€‚\n2ã€beego æ˜¯ä¸€ä¸ª RESTful çš„æ¡†æ¶ï¼Œè¯·æ±‚é»˜è®¤æ˜¯æ‰§è¡Œå¯¹åº” req.Method çš„æ–¹æ³•ã€‚ä¾‹å¦‚æµè§ˆå™¨çš„æ˜¯ GET è¯·æ±‚ï¼Œé‚£ä¹ˆé»˜è®¤å°±ä¼šæ‰§è¡Œ MainController ä¸‹çš„ Get æ–¹æ³•ã€‚ï¼ˆç”¨æˆ·å¯ä»¥æ”¹å˜è¿™ä¸ªè¡Œä¸ºï¼Œé€šè¿‡æ³¨å†Œè‡ªå®šä¹‰çš„å‡½æ•°åï¼‰ã€‚\n3ã€è·å–æ•°æ®ï¼Œèµ‹å€¼åˆ° this.Data ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨æ¥å­˜å‚¨è¾“å‡ºæ•°æ®çš„ mapã€‚\n4ã€æ¸²æŸ“æ¨¡æ¿ï¼Œthis.TplName å°±æ˜¯éœ€è¦æ¸²æŸ“çš„æ¨¡æ¿ï¼Œè¿™é‡ŒæŒ‡å®šäº† index.tplï¼Œå¦‚æœç”¨æˆ·ä¸è®¾ç½®è¯¥å‚æ•°ï¼Œé‚£ä¹ˆé»˜è®¤ä¼šå»åˆ°æ¨¡æ¿ç›®å½•çš„ Controller/\u003cæ–¹æ³•å\u003e.tpl æŸ¥æ‰¾ï¼Œä¾‹å¦‚ä¸Šé¢çš„æ–¹æ³•ä¼šå» maincontroller/get.tpl(æ–‡ä»¶ã€æ–‡ä»¶å¤¹å¿…é¡»å°å†™)ã€‚ç”¨æˆ·è®¾ç½®äº†æ¨¡æ¿ä¹‹åç³»ç»Ÿä¼šè‡ªåŠ¨çš„è°ƒç”¨ Render å‡½æ•°ï¼ˆè¿™ä¸ªå‡½æ•°æ˜¯åœ¨ beego.Controller ä¸­å®ç°çš„ï¼‰ï¼Œæ‰€ä»¥æ— éœ€ç”¨æˆ·è‡ªå·±æ¥è°ƒç”¨æ¸²æŸ“ã€‚\n5ã€å¦‚æœä¸ä½¿ç”¨æ¨¡æ¿å¯ä»¥ç›´æ¥è¾“å‡ºï¼š\nfunc (this *MainController) Get() { this.Ctx.WriteString(\"hello\") } 3. modelé€»è¾‘ modelä¸€èˆ¬ç”¨æ¥å¤„ç†æ•°æ®åº“æ“ä½œï¼Œå¦‚æœé€»è¾‘ä¸­å­˜åœ¨å¯ä»¥å¤ç”¨çš„éƒ¨åˆ†å°±å¯ä»¥æŠ½è±¡æˆä¸€ä¸ªmodelã€‚\npackage models import ( \"loggo/utils\" \"path/filepath\" \"strconv\" \"strings\" ) var ( NotPV []string = []string{\"css\", \"js\", \"class\", \"gif\", \"jpg\", \"jpeg\", \"png\", \"bmp\", \"ico\", \"rss\", \"xml\", \"swf\"} ) const big = 0xFFFFFF func LogPV(urls string) bool { ext := filepath.Ext(urls) if ext == \"\" { return true } for _, v := range NotPV { if v == strings.ToLower(ext) { return false } } return true } 4. viewé€»è¾‘ Controllerä¸­çš„this.TplName = \"index.tpl\"ï¼Œè®¾ç½®æ˜¾ç¤ºçš„æ¨¡æ¿æ–‡ä»¶ï¼Œé»˜è®¤æ”¯æŒ tpl å’Œ html çš„åç¼€åï¼Œå¦‚æœæƒ³è®¾ç½®å…¶ä»–åç¼€ä½ å¯ä»¥è°ƒç”¨ beego.AddTemplateExt æ¥å£è®¾ç½®ã€‚beego é‡‡ç”¨äº† Go è¯­è¨€é»˜è®¤çš„æ¨¡æ¿å¼•æ“ï¼Œå’Œ Go çš„æ¨¡æ¿è¯­æ³•ä¸€æ ·ã€‚\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eBeego\u003c/title\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c/head\u003e \u003cbody\u003e \u003cheader class=\"hero-unit\" style=\"background-color:#A9F16C\"\u003e \u003cdiv class=\"container\"\u003e \u003cdiv class=\"row\"\u003e \u003cdiv class=\"hero-text\"\u003e \u003ch1\u003eWelcome to Beego!\u003c/h1\u003e \u003cp class=\"description\"\u003e Beego is a simple \u0026 powerful Go web framework which is inspired by tornado and sinatra. \u003cbr /\u003e Official website: \u003ca href=\"http://{{.Website}}\"\u003e{{.Website}}\u003c/a\u003e \u003cbr /\u003e Contact me: {{.Email}} \u003c/p\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/header\u003e \u003c/body\u003e \u003c/html\u003e Controller é‡Œé¢æŠŠæ•°æ®èµ‹å€¼ç»™äº† dataï¼ˆmap ç±»å‹ï¼‰ï¼Œç„¶ååœ¨æ¨¡æ¿ä¸­å°±ç›´æ¥é€šè¿‡ key è®¿é—® .Website å’Œ .Email ã€‚è¿™æ ·å°±åšåˆ°äº†æ•°æ®çš„è¾“å‡ºã€‚\n5. é™æ€æ–‡ä»¶ ç½‘é¡µå¾€å¾€åŒ…å«äº†å¾ˆå¤šçš„é™æ€æ–‡ä»¶ï¼ŒåŒ…æ‹¬å›¾ç‰‡ã€JSã€CSS ç­‰\nâ”œâ”€â”€ static â”‚ â”œâ”€â”€ css â”‚ â”œâ”€â”€ img â”‚ â””â”€â”€ js beego é»˜è®¤æ³¨å†Œäº† static ç›®å½•ä¸ºé™æ€å¤„ç†çš„ç›®å½•ï¼Œæ³¨å†Œæ ·å¼ï¼šURL å‰ç¼€å’Œæ˜ å°„çš„ç›®å½•ï¼ˆåœ¨/main.goæ–‡ä»¶ä¸­beego.Run()ä¹‹å‰åŠ å…¥ï¼‰ï¼š\nStaticDir[\"/static\"] = \"static\" ç”¨æˆ·å¯ä»¥è®¾ç½®å¤šä¸ªé™æ€æ–‡ä»¶å¤„ç†ç›®å½•ï¼Œä¾‹å¦‚ä½ æœ‰å¤šä¸ªæ–‡ä»¶ä¸‹è½½ç›®å½• download1ã€download2ï¼Œä½ å¯ä»¥è¿™æ ·æ˜ å°„ï¼ˆåœ¨/main.goæ–‡ä»¶ä¸­beego.Run()ä¹‹å‰åŠ å…¥ï¼‰ï¼š\nbeego.SetStaticPath(\"/down1\", \"download1\") beego.SetStaticPath(\"/down2\", \"download2\") è¿™æ ·ç”¨æˆ·è®¿é—® URL http://localhost:8080/down1/123.txt åˆ™ä¼šè¯·æ±‚ download1 ç›®å½•ä¸‹çš„ 123.txt æ–‡ä»¶ã€‚\nå‚è€ƒï¼š\nhttps://beego.me/docs/quickstart/router.md https://beego.me/docs/quickstart/controller.md https://beego.me/docs/quickstart/model.md https://beego.me/docs/quickstart/view.md https://beego.me/docs/quickstart/static.md ","categories":"","description":"","excerpt":"beegoé¡¹ç›®é€»è¾‘ 1. è·¯ç”±è®¾ç½® 1.1. beego.Router å…¥å£æ–‡ä»¶main.go\npackage main import ( â€¦","ref":"/golang-notes/web/beego/beego-project/","tags":["Golang"],"title":"Beego é¡¹ç›®é€»è¾‘"},{"body":"1. Schedulerç®€ä»‹ Schedulerè´Ÿè´£Podè°ƒåº¦ã€‚åœ¨æ•´ä¸ªç³»ç»Ÿä¸­èµ·\"æ‰¿ä¸Šå¯ä¸‹\"ä½œç”¨ï¼Œæ‰¿ä¸Šï¼šè´Ÿè´£æ¥æ”¶Controller Manageråˆ›å»ºçš„æ–°çš„Podï¼Œä¸ºå…¶é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„Nodeï¼›å¯ä¸‹ï¼šNodeä¸Šçš„kubeletæ¥ç®¡Podçš„ç”Ÿå‘½å‘¨æœŸã€‚\nSchedulerï¼š\n1ï¼‰é€šè¿‡è°ƒåº¦ç®—æ³•ä¸ºå¾…è°ƒåº¦Podåˆ—è¡¨çš„æ¯ä¸ªPodä»Nodeåˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€é€‚åˆçš„Nodeï¼Œå¹¶å°†ä¿¡æ¯å†™å…¥etcdä¸­\n2ï¼‰kubeleté€šè¿‡API Serverç›‘å¬åˆ°kubernetes Scheduleräº§ç”Ÿçš„Podç»‘å®šä¿¡æ¯ï¼Œç„¶åè·å–å¯¹åº”çš„Podæ¸…å•ï¼Œä¸‹è½½Imageï¼Œå¹¶å¯åŠ¨å®¹å™¨ã€‚\n2. è°ƒåº¦æµç¨‹ 1ã€é¢„é€‰è°ƒåº¦è¿‡ç¨‹ï¼Œå³éå†æ‰€æœ‰ç›®æ ‡Node,ç­›é€‰å‡ºç¬¦åˆè¦æ±‚çš„å€™é€‰èŠ‚ç‚¹ï¼Œkuberneteså†…ç½®äº†å¤šç§é¢„é€‰ç­–ç•¥ï¼ˆxxx Predicatesï¼‰ä¾›ç”¨æˆ·é€‰æ‹©\n2ã€ç¡®å®šæœ€ä¼˜èŠ‚ç‚¹ï¼Œåœ¨ç¬¬ä¸€æ­¥çš„åŸºç¡€ä¸Šé‡‡ç”¨ä¼˜é€‰ç­–ç•¥ï¼ˆxxx Priorityï¼‰è®¡ç®—å‡ºæ¯ä¸ªå€™é€‰èŠ‚ç‚¹çš„ç§¯åˆ†ï¼Œå–æœ€é«˜ç§¯åˆ†ã€‚\nè°ƒåº¦æµç¨‹é€šè¿‡æ’ä»¶å¼åŠ è½½çš„â€œè°ƒåº¦ç®—æ³•æä¾›è€…â€ï¼ˆAlgorithmProviderï¼‰å…·ä½“å®ç°ï¼Œä¸€ä¸ªè°ƒåº¦ç®—æ³•æä¾›è€…å°±æ˜¯åŒ…æ‹¬ä¸€ç»„é¢„é€‰ç­–ç•¥ä¸ä¸€ç»„ä¼˜é€‰ç­–ç•¥çš„ç»“æ„ä½“ã€‚\n3. é¢„é€‰ç­–ç•¥ è¯´æ˜ï¼šè¿”å›trueè¡¨ç¤ºè¯¥èŠ‚ç‚¹æ»¡è¶³è¯¥Podçš„è°ƒåº¦æ¡ä»¶ï¼›è¿”å›falseè¡¨ç¤ºè¯¥èŠ‚ç‚¹ä¸æ»¡è¶³è¯¥Podçš„è°ƒåº¦æ¡ä»¶ã€‚\n3.1. NoDiskConflict åˆ¤æ–­å¤‡é€‰Podçš„æ•°æ®å·æ˜¯å¦ä¸è¯¥Nodeä¸Šå·²å­˜åœ¨PodæŒ‚è½½çš„æ•°æ®å·å†²çªï¼Œå¦‚æœæ˜¯åˆ™è¿”å›falseï¼Œå¦åˆ™è¿”å›trueã€‚\n3.2. PodFitsResources åˆ¤æ–­å¤‡é€‰èŠ‚ç‚¹çš„èµ„æºæ˜¯å¦æ»¡è¶³å¤‡é€‰Podçš„éœ€æ±‚ï¼Œå³èŠ‚ç‚¹çš„å‰©ä½™èµ„æºæ»¡ä¸æ»¡è¶³è¯¥Podçš„èµ„æºä½¿ç”¨ã€‚\nè®¡ç®—å¤‡é€‰Podå’ŒèŠ‚ç‚¹ä¸­å·²ç”¨èµ„æºï¼ˆè¯¥èŠ‚ç‚¹æ‰€æœ‰Podçš„ä½¿ç”¨èµ„æºï¼‰çš„æ€»å’Œã€‚ è·å–å¤‡é€‰èŠ‚ç‚¹çš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹èµ„æºä¿¡æ¯ã€‚ å¦‚æœï¼ˆå¤‡é€‰Pod+èŠ‚ç‚¹å·²ç”¨èµ„æº\u003eè¯¥èŠ‚ç‚¹æ€»èµ„æºï¼‰åˆ™è¿”å›falseï¼Œå³å‰©ä½™èµ„æºä¸æ»¡è¶³è¯¥Podä½¿ç”¨ï¼›å¦åˆ™è¿”å›trueã€‚ 3.3. PodSelectorMatches åˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦åŒ…å«å¤‡é€‰Podçš„æ ‡ç­¾é€‰æ‹©å™¨æŒ‡å®šçš„æ ‡ç­¾ï¼Œå³é€šè¿‡æ ‡ç­¾æ¥é€‰æ‹©Nodeã€‚\nå¦‚æœPodä¸­æ²¡æœ‰æŒ‡å®šspec.nodeSelectorï¼Œåˆ™è¿”å›trueã€‚ å¦åˆ™è·å¾—å¤‡é€‰èŠ‚ç‚¹çš„æ ‡ç­¾ä¿¡æ¯ï¼Œåˆ¤æ–­è¯¥èŠ‚ç‚¹çš„æ ‡ç­¾ä¿¡æ¯ä¸­æ˜¯å¦åŒ…å«è¯¥Podçš„spec.nodeSelectorä¸­æŒ‡å®šçš„æ ‡ç­¾ï¼Œå¦‚æœåŒ…å«è¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚ 3.4. PodFitsHost åˆ¤æ–­å¤‡é€‰Podçš„spec.nodeNameæ‰€æŒ‡å®šçš„èŠ‚ç‚¹åç§°ä¸å¤‡é€‰èŠ‚ç‚¹åç§°æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸€è‡´è¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚\n3.5. CheckNodeLabelPresence æ£€æŸ¥å¤‡é€‰èŠ‚ç‚¹ä¸­æ˜¯å¦æœ‰Scheduleré…ç½®çš„æ ‡ç­¾ï¼Œå¦‚æœæœ‰è¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚\n3.6. CheckServiceAffinity åˆ¤æ–­å¤‡é€‰èŠ‚ç‚¹æ˜¯å¦åŒ…å«Scheduleré…ç½®çš„æ ‡ç­¾ï¼Œå¦‚æœæœ‰è¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚\n3.7. PodFitsPorts åˆ¤æ–­å¤‡é€‰Podæ‰€ç”¨çš„ç«¯å£åˆ—è¡¨ä¸­çš„ç«¯å£æ˜¯å¦åœ¨å¤‡é€‰èŠ‚ç‚¹ä¸­å·²è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨è¿”å›falseï¼Œå¦åˆ™è¿”å›trueã€‚\n4. ä¼˜é€‰ç­–ç•¥ 4.1. LeastRequestedPriority ä¼˜å…ˆä»å¤‡é€‰èŠ‚ç‚¹åˆ—è¡¨ä¸­é€‰æ‹©èµ„æºæ¶ˆè€—æœ€å°çš„èŠ‚ç‚¹ï¼ˆCPU+å†…å­˜ï¼‰ã€‚\n4.2. CalculateNodeLabelPriority ä¼˜å…ˆé€‰æ‹©å«æœ‰æŒ‡å®šLabelçš„èŠ‚ç‚¹ã€‚\n4.3. BalancedResourceAllocation ä¼˜å…ˆä»å¤‡é€‰èŠ‚ç‚¹åˆ—è¡¨ä¸­é€‰æ‹©å„é¡¹èµ„æºä½¿ç”¨ç‡æœ€å‡è¡¡çš„èŠ‚ç‚¹ã€‚\nå‚è€ƒã€ŠKubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. Schedulerç®€ä»‹ Schedulerè´Ÿè´£Podè°ƒåº¦ã€‚åœ¨æ•´ä¸ªç³»ç»Ÿä¸­èµ·\"æ‰¿ä¸Šå¯ä¸‹\"ä½œç”¨ï¼Œæ‰¿ä¸Šï¼šè´Ÿè´£æ¥æ”¶Controller â€¦","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-scheduler/","tags":["Kubernetes"],"title":"Kubernetesæ ¸å¿ƒåŸç†ï¼ˆä¸‰ï¼‰ä¹‹Scheduler"},{"body":"1. cAdvisorç®€ä»‹ â€‹ cAdvisorå¯¹Nodeæœºå™¨ä¸Šçš„èµ„æºåŠå®¹å™¨è¿›è¡Œå®æ—¶ç›‘æ§å’Œæ€§èƒ½æ•°æ®é‡‡é›†ï¼ŒåŒ…æ‹¬CPUä½¿ç”¨æƒ…å†µã€å†…å­˜ä½¿ç”¨æƒ…å†µã€ç½‘ç»œååé‡åŠæ–‡ä»¶ç³»ç»Ÿä½¿ç”¨æƒ…å†µï¼ŒcAdvisoré›†æˆåœ¨Kubeletä¸­ï¼Œå½“kubeletå¯åŠ¨æ—¶ä¼šè‡ªåŠ¨å¯åŠ¨cAdvisorï¼Œå³ä¸€ä¸ªcAdvisorä»…å¯¹ä¸€å°Nodeæœºå™¨è¿›è¡Œç›‘æ§ã€‚kubeletçš„å¯åŠ¨å‚æ•°--cadvisor-portå¯ä»¥å®šä¹‰cAdvisorå¯¹å¤–æä¾›æœåŠ¡çš„ç«¯å£ï¼Œé»˜è®¤ä¸º4194ã€‚å¯ä»¥é€šè¿‡æµè§ˆå™¨\u003cNode_IP:port\u003eè®¿é—®ã€‚é¡¹ç›®ä¸»é¡µï¼šhttp://github.com/google/cadvisorã€‚\n2. cAdvisorç»“æ„å›¾ 3. Metrics åˆ†ç±» å­—æ®µ æè¿° cpu cpu_usage_total cpu_usage_system cpu_usage_user cpu_usage_per_cpu load_average Smoothed average of number of runnable threads x 1000 memory memory_usage Memory Usage memory_working_set Working set size network rx_bytes Cumulative count of bytes received rx_errors Cumulative count of receive errors encountered tx_bytes Cumulative count of bytes transmitted tx_errors Cumulative count of transmit errors encountered filesystem fs_device Filesystem device fs_limit Filesystem limit fs_usage Filesystem usage 4. cAdvisoræºç  4.1. cAdvisorå…¥å£å‡½æ•° cadvisor.go\nfunc main() { defer glog.Flush() flag.Parse() if *versionFlag { fmt.Printf(\"cAdvisor version %s (%s)/n\", version.Info[\"version\"], version.Info[\"revision\"]) os.Exit(0) } setMaxProcs() memoryStorage, err := NewMemoryStorage() if err != nil { glog.Fatalf(\"Failed to initialize storage driver: %s\", err) } sysFs, err := sysfs.NewRealSysFs() if err != nil { glog.Fatalf(\"Failed to create a system interface: %s\", err) } collectorHttpClient := createCollectorHttpClient(*collectorCert, *collectorKey) containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, \u0026collectorHttpClient) if err != nil { glog.Fatalf(\"Failed to create a Container Manager: %s\", err) } mux := http.NewServeMux() if *enableProfiling { mux.HandleFunc(\"/debug/pprof/\", pprof.Index) mux.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline) mux.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) mux.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) } // Register all HTTP handlers. err = cadvisorhttp.RegisterHandlers(mux, containerManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm) if err != nil { glog.Fatalf(\"Failed to register HTTP handlers: %v\", err) } cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, nil) // Start the manager. if err := containerManager.Start(); err != nil { glog.Fatalf(\"Failed to start container manager: %v\", err) } // Install signal handler. installSignalHandler(containerManager) glog.Infof(\"Starting cAdvisor version: %s-%s on port %d\", version.Info[\"version\"], version.Info[\"revision\"], *argPort) addr := fmt.Sprintf(\"%s:%d\", *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux)) } æ ¸å¿ƒä»£ç ï¼š\nmemoryStorage, err := NewMemoryStorage() sysFs, err := sysfs.NewRealSysFs() #åˆ›å»ºcontainerManager containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, \u0026collectorHttpClient) #å¯åŠ¨containerManager err := containerManager.Start() 4.2. cAdvisor Clientçš„ä½¿ç”¨ import \"github.com/google/cadvisor/client\" func main(){ client, err := client.NewClient(\"http://192.168.19.30:4194/\") //http://\u003chost-ip\u003e:\u003cport\u003e/ } 4.2.1 clientå®šä¹‰ cadvisor/client/client.go\n// Client represents the base URL for a cAdvisor client. type Client struct { baseUrl string } // NewClient returns a new v1.3 client with the specified base URL. func NewClient(url string) (*Client, error) { if !strings.HasSuffix(url, \"/\") { url += \"/\" } return \u0026Client{ baseUrl: fmt.Sprintf(\"%sapi/v1.3/\", url), }, nil } 4.2.2. clientæ–¹æ³• 1ï¼‰MachineInfo\n// MachineInfo returns the JSON machine information for this client. // A non-nil error result indicates a problem with obtaining // the JSON machine information data. func (self *Client) MachineInfo() (minfo *v1.MachineInfo, err error) { u := self.machineInfoUrl() ret := new(v1.MachineInfo) if err = self.httpGetJsonData(ret, nil, u, \"machine info\"); err != nil { return } minfo = ret return } 2ï¼‰ContainerInfo\n// ContainerInfo returns the JSON container information for the specified // container and request. func (self *Client) ContainerInfo(name string, query *v1.ContainerInfoRequest) (cinfo *v1.ContainerInfo, err error) { u := self.containerInfoUrl(name) ret := new(v1.ContainerInfo) if err = self.httpGetJsonData(ret, query, u, fmt.Sprintf(\"container info for %q\", name)); err != nil { return } cinfo = ret return } 3ï¼‰DockerContainer\n// Returns the JSON container information for the specified // Docker container and request. func (self *Client) DockerContainer(name string, query *v1.ContainerInfoRequest) (cinfo v1.ContainerInfo, err error) { u := self.dockerInfoUrl(name) ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(\u0026ret, query, u, fmt.Sprintf(\"Docker container info for %q\", name)); err != nil { return } if len(ret) != 1 { err = fmt.Errorf(\"expected to only receive 1 Docker container: %+v\", ret) return } for _, cont := range ret { cinfo = cont } return } 4ï¼‰AllDockerContainers\n// Returns the JSON container information for all Docker containers. func (self *Client) AllDockerContainers(query *v1.ContainerInfoRequest) (cinfo []v1.ContainerInfo, err error) { u := self.dockerInfoUrl(\"/\") ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(\u0026ret, query, u, \"all Docker containers info\"); err != nil { return } cinfo = make([]v1.ContainerInfo, 0, len(ret)) for _, cont := range ret { cinfo = append(cinfo, cont) } return } ","categories":"","description":"","excerpt":"1. cAdvisorç®€ä»‹ â€‹ cAdvisorå¯¹Nodeæœºå™¨ä¸Šçš„èµ„æºåŠå®¹å™¨è¿›è¡Œå®æ—¶ç›‘æ§å’Œæ€§èƒ½æ•°æ®é‡‡é›†ï¼ŒåŒ…æ‹¬CPUä½¿ç”¨æƒ…å†µã€å†…å­˜ä½¿ç”¨æƒ…å†µã€ç½‘ â€¦","ref":"/kubernetes-notes/monitor/cadvisor-introduction/","tags":["Monitor"],"title":"cAdvisorä»‹ç»"},{"body":"2. æ•°æ®åº“æ“ä½œ #åˆ›å»ºæ•°æ®åº“ create database \u003cæ•°æ®åº“å\u003e #æ˜¾ç¤ºæ•°æ®åº“ show databases #åˆ é™¤æ•°æ® drop database \u003cæ•°æ®åº“å\u003e 3. æ•°æ®è¡¨æ“ä½œ 3.1. åˆ›å»ºè¡¨ create table è¡¨å( åˆ—å ç±»å‹ æ˜¯å¦å¯ä»¥ä¸ºç©ºï¼Œ åˆ—å ç±»å‹ æ˜¯å¦å¯ä»¥ä¸ºç©º )ENGINE=InnoDB DEFAULT CHARSET=utf8 é»˜è®¤å€¼ï¼Œåˆ›å»ºåˆ—æ—¶å¯ä»¥æŒ‡å®šé»˜è®¤å€¼ï¼Œå½“æ’å…¥æ•°æ®æ—¶å¦‚æœæœªä¸»åŠ¨è®¾ç½®ï¼Œåˆ™è‡ªåŠ¨æ·»åŠ é»˜è®¤å€¼ è‡ªå¢ï¼Œå¦‚æœä¸ºæŸåˆ—è®¾ç½®è‡ªå¢åˆ—ï¼Œæ’å…¥æ•°æ®æ—¶æ— éœ€è®¾ç½®æ­¤åˆ—ï¼Œé»˜è®¤å°†è‡ªå¢ï¼ˆè¡¨ä¸­åªèƒ½æœ‰ä¸€ä¸ªè‡ªå¢åˆ—ï¼‰æ³¨æ„ï¼š1ã€å¯¹äºè‡ªå¢åˆ—ï¼Œå¿…é¡»æ˜¯ç´¢å¼•ï¼ˆå«ä¸»é”®ï¼‰2ã€å¯¹äºè‡ªå¢å¯ä»¥è®¾ç½®æ­¥é•¿å’Œèµ·å§‹å€¼ ä¸»é”®ï¼Œä¸€ç§ç‰¹æ®Šçš„å”¯ä¸€ç´¢å¼•ï¼Œä¸å…è®¸æœ‰ç©ºå€¼ï¼Œå¦‚æœä¸»é”®ä½¿ç”¨å•ä¸ªåˆ—ï¼Œåˆ™å®ƒçš„å€¼å¿…é¡»å”¯ä¸€ï¼Œå¦‚æœæ˜¯å¤šåˆ—ï¼Œåˆ™å…¶ç»„åˆå¿…é¡»å”¯ä¸€ã€‚ 3.2. æŸ¥çœ‹è¡¨ show tables; # æŸ¥çœ‹æ•°æ®åº“å…¨éƒ¨è¡¨ select * from è¡¨å; # æŸ¥çœ‹è¡¨æ‰€æœ‰å†…å®¹ 3.3. åˆ é™¤è¡¨ drop table è¡¨å 3.4. æ¸…ç©ºè¡¨å†…å®¹ delete from è¡¨å truncate table è¡¨å 3.5. æŸ¥çœ‹è¡¨ç»“æ„ desc è¡¨å 3.6. ä¿®æ”¹è¡¨ åˆ—æ“ä½œ\n#æ·»åŠ åˆ— alter table è¡¨å add åˆ—å ç±»å‹ alter table è¡¨å add åˆ—å ç±»å‹ after `åˆ—å` #åˆ é™¤åˆ— alter table è¡¨å drop column åˆ—å #ä¿®æ”¹åˆ— alter table è¡¨å modify column åˆ—å ç±»å‹; -- ç±»å‹ alter table è¡¨å change åŸåˆ—å æ–°åˆ—å ç±»å‹; -- åˆ—åï¼Œç±»å‹ ä¸»é”®æ“ä½œ\n#æ·»åŠ ä¸»é”® alter table è¡¨å add primary key(åˆ—å); #åˆ é™¤ä¸»é”® alter table è¡¨å drop primary key; alter table è¡¨å modify åˆ—å int, drop primary key; #ä¿®æ”¹ä¸»é”®ï¼šå…ˆåˆ é™¤åæ·»åŠ  alter table è¡¨å drop primary key; alter table è¡¨å add primary key(åˆ—å); #æ·»åŠ å¤–é”® alter table ä»è¡¨ add constraint å¤–é”®åç§°ï¼ˆå½¢å¦‚ï¼šFKä»è¡¨ä¸»è¡¨ï¼‰ foreign key ä»è¡¨(å¤–é”®å­—æ®µ) references ä¸»è¡¨(ä¸»é”®å­—æ®µ); #åˆ é™¤å¤–é”® alter table è¡¨å drop foreign key å¤–é”®åç§° é»˜è®¤å€¼æ“ä½œ\n#ä¿®æ”¹é»˜è®¤å€¼ï¼š ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000; #åˆ é™¤é»˜è®¤å€¼ï¼š ALTER TABLE testalter_tbl ALTER i DROP DEFAULT; è°ƒæ•´è¡¨ç»“æ„å­—æ®µé¡ºåº\nalter table \u003ctable_name\u003e modify \u003cå­—æ®µ1\u003e varchar(10) after \u003cå­—æ®µ2\u003e; alter table \u003ctable_name\u003e modify id int(10) unsigned auto_increment first; ","categories":"","description":"","excerpt":"2. æ•°æ®åº“æ“ä½œ #åˆ›å»ºæ•°æ®åº“ create database \u003cæ•°æ®åº“å\u003e #æ˜¾ç¤ºæ•°æ®åº“ show databases # â€¦","ref":"/linux-notes/mysql/table-operation/","tags":["Mysql"],"title":"Mysqlå¸¸ç”¨å‘½ä»¤ä¹‹æ•°æ®è¡¨æ“ä½œ"},{"body":"1. Pod phase Podçš„phaseæ˜¯Podç”Ÿå‘½å‘¨æœŸä¸­çš„ç®€å•å®è§‚æè¿°ï¼Œå®šä¹‰åœ¨Podçš„PodStatuså¯¹è±¡çš„phaseÂ å­—æ®µä¸­ã€‚\nphaseæœ‰ä»¥ä¸‹å‡ ç§å€¼ï¼š\nçŠ¶æ€å€¼ è¯´æ˜ æŒ‚èµ·ï¼ˆPendingï¼‰ Pod å·²è¢« Kubernetes ç³»ç»Ÿæ¥å—ï¼Œä½†æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªå®¹å™¨é•œåƒå°šæœªåˆ›å»ºã€‚ç­‰å¾…æ—¶é—´åŒ…æ‹¬è°ƒåº¦ Pod çš„æ—¶é—´å’Œé€šè¿‡ç½‘ç»œä¸‹è½½é•œåƒçš„æ—¶é—´ã€‚ è¿è¡Œä¸­ï¼ˆRunningï¼‰ è¯¥ Pod å·²ç»ç»‘å®šåˆ°äº†ä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼ŒPod ä¸­æ‰€æœ‰çš„å®¹å™¨éƒ½å·²è¢«åˆ›å»ºã€‚è‡³å°‘æœ‰ä¸€ä¸ªå®¹å™¨æ­£åœ¨è¿è¡Œï¼Œæˆ–è€…æ­£å¤„äºå¯åŠ¨æˆ–é‡å¯çŠ¶æ€ã€‚ æˆåŠŸï¼ˆSucceededï¼‰ Pod ä¸­çš„æ‰€æœ‰å®¹å™¨éƒ½è¢«æˆåŠŸç»ˆæ­¢ï¼Œå¹¶ä¸”ä¸ä¼šå†é‡å¯ã€‚ å¤±è´¥ï¼ˆFailedï¼‰ Pod ä¸­çš„æ‰€æœ‰å®¹å™¨éƒ½å·²ç»ˆæ­¢äº†ï¼Œå¹¶ä¸”è‡³å°‘æœ‰ä¸€ä¸ªå®¹å™¨æ˜¯å› ä¸ºå¤±è´¥ç»ˆæ­¢ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®¹å™¨ä»¥é0çŠ¶æ€é€€å‡ºæˆ–è€…è¢«ç³»ç»Ÿç»ˆæ­¢ã€‚ æœªçŸ¥ï¼ˆUnknownï¼‰ å› ä¸ºæŸäº›åŸå› æ— æ³•å–å¾— Pod çš„çŠ¶æ€ï¼Œé€šå¸¸æ˜¯å› ä¸ºä¸ Pod æ‰€åœ¨ä¸»æœºé€šä¿¡å¤±è´¥ã€‚ 2. Pod çŠ¶æ€ Pod æœ‰ä¸€ä¸ª PodStatus å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª PodCondition æ•°ç»„ã€‚ PodConditionåŒ…å«ä»¥ä¸‹ä»¥ä¸‹å­—æ®µï¼š\nlastProbeTimeï¼šPod conditionæœ€åä¸€æ¬¡è¢«æ¢æµ‹åˆ°çš„æ—¶é—´æˆ³ã€‚ lastTransitionTimeï¼šPodæœ€åä¸€æ¬¡çŠ¶æ€è½¬å˜çš„æ—¶é—´æˆ³ã€‚ messageï¼šçŠ¶æ€è½¬åŒ–çš„ä¿¡æ¯ï¼Œä¸€èˆ¬ä¸ºæŠ¥é”™ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼šcontainers with unready status: [c-1]ã€‚ reasonï¼šæœ€åä¸€æ¬¡çŠ¶æ€å½¢æˆçš„åŸå› ï¼Œä¸€èˆ¬ä¸ºæŠ¥é”™åŸå› ï¼Œä¾‹å¦‚ï¼šContainersNotReadyã€‚ statusï¼šåŒ…å«çš„å€¼æœ‰ Trueã€False å’Œ Unknownã€‚ typeï¼šPodçŠ¶æ€çš„å‡ ç§ç±»å‹ã€‚ å…¶ä¸­typeå­—æ®µåŒ…å«ä»¥ä¸‹å‡ ä¸ªå€¼ï¼š\nPodScheduledï¼šPodå·²ç»è¢«è°ƒåº¦åˆ°è¿è¡ŒèŠ‚ç‚¹ã€‚ Readyï¼šPodå·²ç»å¯ä»¥æ¥æ”¶è¯·æ±‚æä¾›æœåŠ¡ã€‚ Initializedï¼šæ‰€æœ‰çš„init containerå·²ç»æˆåŠŸå¯åŠ¨ã€‚ Unschedulableï¼šæ— æ³•è°ƒåº¦è¯¥Podï¼Œä¾‹å¦‚èŠ‚ç‚¹èµ„æºä¸å¤Ÿã€‚ ContainersReadyï¼šPodä¸­çš„æ‰€æœ‰å®¹å™¨å·²å‡†å¤‡å°±ç»ªã€‚ 3. é‡å¯ç­–ç•¥ Podé€šè¿‡restartPolicyå­—æ®µæŒ‡å®šé‡å¯ç­–ç•¥ï¼Œé‡å¯ç­–ç•¥ç±»å‹ä¸ºï¼šAlwaysã€OnFailure å’Œ Neverï¼Œé»˜è®¤ä¸º Alwaysã€‚\nrestartPolicy ä»…æŒ‡é€šè¿‡åŒä¸€èŠ‚ç‚¹ä¸Šçš„ kubelet é‡æ–°å¯åŠ¨å®¹å™¨ã€‚\né‡å¯ç­–ç•¥ è¯´æ˜ Always å½“å®¹å™¨å¤±æ•ˆæ—¶ï¼Œç”±kubeletè‡ªåŠ¨é‡å¯è¯¥å®¹å™¨ OnFailure å½“å®¹å™¨ç»ˆæ­¢è¿è¡Œä¸”é€€å‡ºç ä¸ä¸º0æ—¶ï¼Œç”±kubeletè‡ªåŠ¨é‡å¯è¯¥å®¹å™¨ Never ä¸è®ºå®¹å™¨è¿è¡ŒçŠ¶æ€å¦‚ä½•ï¼Œkubeletéƒ½ä¸ä¼šé‡å¯è¯¥å®¹å™¨ è¯´æ˜ï¼š\nå¯ä»¥ç®¡ç†Podçš„æ§åˆ¶å™¨æœ‰Replication Controllerï¼ŒJobï¼ŒDaemonSetï¼ŒåŠkubeletï¼ˆé™æ€Podï¼‰ã€‚\nRCå’ŒDaemonSetï¼šå¿…é¡»è®¾ç½®ä¸ºAlwaysï¼Œéœ€è¦ä¿è¯è¯¥å®¹å™¨æŒç»­è¿è¡Œã€‚ Jobï¼šOnFailureæˆ–Neverï¼Œç¡®ä¿å®¹å™¨æ‰§è¡Œå®Œåä¸å†é‡å¯ã€‚ kubeletï¼šåœ¨Podå¤±æ•ˆçš„æ—¶å€™é‡å¯å®ƒï¼Œä¸è®ºRestartPolicyè®¾ç½®ä¸ºä»€ä¹ˆå€¼ï¼Œå¹¶ä¸”ä¸ä¼šå¯¹Podè¿›è¡Œå¥åº·æ£€æŸ¥ã€‚ 4. Podçš„ç”Ÿå‘½ Podçš„ç”Ÿå‘½å‘¨æœŸä¸€èˆ¬é€šè¿‡Controler\tçš„æ–¹å¼ç®¡ç†ï¼Œæ¯ç§Controlleréƒ½ä¼šåŒ…å«PodTemplateæ¥æŒ‡æ˜Podçš„ç›¸å…³å±æ€§ï¼ŒControllerå¯ä»¥è‡ªåŠ¨å¯¹podçš„å¼‚å¸¸çŠ¶æ€è¿›è¡Œé‡æ–°è°ƒåº¦å’Œæ¢å¤ï¼Œé™¤éé€šè¿‡Controllerçš„æ–¹å¼åˆ é™¤å…¶ç®¡ç†çš„Podï¼Œä¸ç„¶kuberneteså§‹ç»ˆè¿è¡Œç”¨æˆ·é¢„æœŸçŠ¶æ€çš„Podã€‚\næ§åˆ¶å™¨çš„åˆ†ç±»\nä½¿ç”¨ Jobè¿è¡Œé¢„æœŸä¼šç»ˆæ­¢çš„ Podï¼Œä¾‹å¦‚æ‰¹é‡è®¡ç®—ã€‚Job ä»…é€‚ç”¨äºé‡å¯ç­–ç•¥ä¸º OnFailure æˆ– Never çš„ Podã€‚ å¯¹é¢„æœŸä¸ä¼šç»ˆæ­¢çš„ Pod ä½¿ç”¨ ReplicationControllerã€ReplicaSetå’Œ Deploymentï¼Œä¾‹å¦‚ Web æœåŠ¡å™¨ã€‚ ReplicationController ä»…é€‚ç”¨äºå…·æœ‰ restartPolicy ä¸º Always çš„ Podã€‚ æä¾›ç‰¹å®šäºæœºå™¨çš„ç³»ç»ŸæœåŠ¡ï¼Œä½¿ç”¨ DaemonSetä¸ºæ¯å°æœºå™¨è¿è¡Œä¸€ä¸ª Pod ã€‚ å¦‚æœèŠ‚ç‚¹æ­»äº¡æˆ–ä¸é›†ç¾¤çš„å…¶ä½™éƒ¨åˆ†æ–­å¼€è¿æ¥ï¼Œåˆ™ Kubernetes å°†åº”ç”¨ä¸€ä¸ªç­–ç•¥å°†ä¸¢å¤±èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod çš„ phase è®¾ç½®ä¸º Failedã€‚\n5. PodçŠ¶æ€è½¬æ¢ å¸¸è§çš„çŠ¶æ€è½¬æ¢\nPodçš„å®¹å™¨æ•° Podå½“å‰çŠ¶æ€ å‘ç”Ÿçš„äº‹ä»¶ Podç»“æœçŠ¶æ€ RestartPolicy=Always RestartPolicy=OnFailure RestartPolicy=Never åŒ…å«ä¸€ä¸ªå®¹å™¨ Running å®¹å™¨æˆåŠŸé€€å‡º Running Succeeded Succeeded åŒ…å«ä¸€ä¸ªå®¹å™¨ Running å®¹å™¨å¤±è´¥é€€å‡º Running Running Failure åŒ…å«ä¸¤ä¸ªå®¹å™¨ Running 1ä¸ªå®¹å™¨å¤±è´¥é€€å‡º Running Running Running åŒ…å«ä¸¤ä¸ªå®¹å™¨ Running å®¹å™¨è¢«OOMæ€æ‰ Running Running Failure 5.1. å®¹å™¨è¿è¡Œæ—¶å†…å­˜è¶…å‡ºé™åˆ¶ å®¹å™¨ä»¥å¤±è´¥çŠ¶æ€ç»ˆæ­¢ã€‚ è®°å½• OOM äº‹ä»¶ã€‚ å¦‚æœrestartPolicyä¸ºï¼š Alwaysï¼šé‡å¯å®¹å™¨ï¼›Pod phase ä»ä¸º Runningã€‚ OnFailureï¼šé‡å¯å®¹å™¨ï¼›Pod phase ä»ä¸º Runningã€‚ Never: è®°å½•å¤±è´¥äº‹ä»¶ï¼›Pod phase ä»ä¸º Failedã€‚ 5.2. ç£ç›˜æ•…éšœ æ€æ‰æ‰€æœ‰å®¹å™¨ã€‚ è®°å½•é€‚å½“äº‹ä»¶ã€‚ Pod phase å˜æˆ Failedã€‚ å¦‚æœä½¿ç”¨æ§åˆ¶å™¨æ¥è¿è¡Œï¼ŒPod å°†åœ¨åˆ«å¤„é‡å»ºã€‚ 5.3. è¿è¡ŒèŠ‚ç‚¹æŒ‚æ‰ èŠ‚ç‚¹æ§åˆ¶å™¨ç­‰å¾…ç›´åˆ°è¶…æ—¶ã€‚ èŠ‚ç‚¹æ§åˆ¶å™¨å°† Pod phase è®¾ç½®ä¸º Failedã€‚ å¦‚æœæ˜¯ç”¨æ§åˆ¶å™¨æ¥è¿è¡Œï¼ŒPod å°†åœ¨åˆ«å¤„é‡å»ºã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ ","categories":"","description":"","excerpt":"1. Pod phase Podçš„phaseæ˜¯Podç”Ÿå‘½å‘¨æœŸä¸­çš„ç®€å•å®è§‚æè¿°ï¼Œå®šä¹‰åœ¨Podçš„PodStatuså¯¹è±¡çš„phaseÂ å­—æ®µä¸­ã€‚ â€¦","ref":"/kubernetes-notes/concepts/pod/pod-lifecycle/","tags":["Kubernetes"],"title":"Podç”Ÿå‘½å‘¨æœŸ"},{"body":"1. Dockerçš„æ€»æ¶æ„å›¾ dockeræ˜¯ä¸€ä¸ªC/Sæ¨¡å¼çš„æ¶æ„ï¼Œåç«¯æ˜¯ä¸€ä¸ªæ¾è€¦åˆæ¶æ„ï¼Œæ¨¡å—å„å¸å…¶èŒã€‚\nç”¨æˆ·æ˜¯ä½¿ç”¨Docker Clientä¸Docker Daemonå»ºç«‹é€šä¿¡ï¼Œå¹¶å‘é€è¯·æ±‚ç»™åè€…ã€‚ Docker Daemonä½œä¸ºDockeræ¶æ„ä¸­çš„ä¸»ä½“éƒ¨åˆ†ï¼Œé¦–å…ˆæä¾›Serverçš„åŠŸèƒ½ä½¿å…¶å¯ä»¥æ¥å—Docker Clientçš„è¯·æ±‚ï¼› Engineæ‰§è¡ŒDockerå†…éƒ¨çš„ä¸€ç³»åˆ—å·¥ä½œï¼Œæ¯ä¸€é¡¹å·¥ä½œéƒ½æ˜¯ä»¥ä¸€ä¸ªJobçš„å½¢å¼çš„å­˜åœ¨ã€‚ Jobçš„è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œå½“éœ€è¦å®¹å™¨é•œåƒæ—¶ï¼Œåˆ™ä»Docker Registryä¸­ä¸‹è½½é•œåƒï¼Œå¹¶é€šè¿‡é•œåƒç®¡ç†é©±åŠ¨graphdriverå°†ä¸‹è½½é•œåƒä»¥Graphçš„å½¢å¼å­˜å‚¨ï¼› å½“éœ€è¦ä¸ºDockeråˆ›å»ºç½‘ç»œç¯å¢ƒæ—¶ï¼Œé€šè¿‡ç½‘ç»œç®¡ç†é©±åŠ¨networkdriveråˆ›å»ºå¹¶é…ç½®Dockerå®¹å™¨ç½‘ç»œç¯å¢ƒï¼› å½“éœ€è¦é™åˆ¶Dockerå®¹å™¨è¿è¡Œèµ„æºæˆ–æ‰§è¡Œç”¨æˆ·æŒ‡ä»¤ç­‰æ“ä½œæ—¶ï¼Œåˆ™é€šè¿‡execdriveræ¥å®Œæˆã€‚ libcontaineræ˜¯ä¸€é¡¹ç‹¬ç«‹çš„å®¹å™¨ç®¡ç†åŒ…ï¼Œnetworkdriverä»¥åŠexecdriveréƒ½æ˜¯é€šè¿‡libcontaineræ¥å®ç°å…·ä½“å¯¹å®¹å™¨è¿›è¡Œçš„æ“ä½œã€‚ 2. Dockerå„æ¨¡å—ç»„ä»¶åˆ†æ 2.1. Docker Client[å‘èµ·è¯·æ±‚] Docker Clientæ˜¯å’ŒDocker Daemonå»ºç«‹é€šä¿¡çš„å®¢æˆ·ç«¯ã€‚ç”¨æˆ·ä½¿ç”¨çš„å¯æ‰§è¡Œæ–‡ä»¶ä¸ºdockerï¼ˆç±»ä¼¼å¯æ‰§è¡Œè„šæœ¬çš„å‘½ä»¤ï¼‰ï¼Œdockerå‘½ä»¤åæ¥å‚æ•°çš„å½¢å¼æ¥å®ç°ä¸€ä¸ªå®Œæ•´çš„è¯·æ±‚å‘½ä»¤ï¼ˆä¾‹å¦‚docker imagesï¼Œdockerä¸ºå‘½ä»¤ä¸å¯å˜ï¼Œimagesä¸ºå‚æ•°å¯å˜ï¼‰ã€‚ Docker Clientå¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸‰ç§æ–¹å¼å’ŒDocker Daemonå»ºç«‹é€šä¿¡ï¼štcp://host:portï¼Œunix://path_to_socketå’Œfd://socketfdã€‚ Docker Clientå‘é€å®¹å™¨ç®¡ç†è¯·æ±‚åï¼Œç”±Docker Daemonæ¥å—å¹¶å¤„ç†è¯·æ±‚ï¼Œå½“Docker Clientæ¥æ”¶åˆ°è¿”å›çš„è¯·æ±‚ç›¸åº”å¹¶ç®€å•å¤„ç†åï¼ŒDocker Clientä¸€æ¬¡å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸå°±ç»“æŸäº†ã€‚[ä¸€æ¬¡å®Œæ•´çš„è¯·æ±‚ï¼šå‘é€è¯·æ±‚â†’å¤„ç†è¯·æ±‚â†’è¿”å›ç»“æœ]ï¼Œä¸ä¼ ç»Ÿçš„C/Sæ¶æ„è¯·æ±‚æµç¨‹å¹¶æ— ä¸åŒã€‚ 2.2. Docker Daemon[åå°å®ˆæŠ¤è¿›ç¨‹] Docker Daemonçš„æ¶æ„å›¾\n2.2.1. Docker Server[è°ƒåº¦åˆ†å‘è¯·æ±‚] Docker Serverçš„æ¶æ„å›¾\nDocker Serverç›¸å½“äºC/Sæ¶æ„çš„æœåŠ¡ç«¯ã€‚åŠŸèƒ½ä¸ºæ¥å—å¹¶è°ƒåº¦åˆ†å‘Docker Clientå‘é€çš„è¯·æ±‚ã€‚æ¥å—è¯·æ±‚åï¼ŒServeré€šè¿‡è·¯ç”±ä¸åˆ†å‘è°ƒåº¦ï¼Œæ‰¾åˆ°ç›¸åº”çš„Handleræ¥æ‰§è¡Œè¯·æ±‚ã€‚ åœ¨Dockerçš„å¯åŠ¨è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åŒ…gorilla/muxï¼Œåˆ›å»ºäº†ä¸€ä¸ªmux.Routerï¼Œæä¾›è¯·æ±‚çš„è·¯ç”±åŠŸèƒ½ã€‚åœ¨Golangä¸­ï¼Œgorilla/muxæ˜¯ä¸€ä¸ªå¼ºå¤§çš„URLè·¯ç”±å™¨ä»¥åŠè°ƒåº¦åˆ†å‘å™¨ã€‚è¯¥mux.Routerä¸­æ·»åŠ äº†ä¼—å¤šçš„è·¯ç”±é¡¹ï¼Œæ¯ä¸€ä¸ªè·¯ç”±é¡¹ç”±HTTPè¯·æ±‚æ–¹æ³•ï¼ˆPUTã€POSTã€GETæˆ–DELETEï¼‰ã€URLã€Handlerä¸‰éƒ¨åˆ†ç»„æˆã€‚ åˆ›å»ºå®Œmux.Routerä¹‹åï¼ŒDockerå°†Serverçš„ç›‘å¬åœ°å€ä»¥åŠmux.Routerä½œä¸ºå‚æ•°ï¼Œåˆ›å»ºä¸€ä¸ªhttpSrv=http.Server{}ï¼Œæœ€ç»ˆæ‰§è¡ŒhttpSrv.Serve()ä¸ºè¯·æ±‚æœåŠ¡ã€‚ åœ¨Serverçš„æœåŠ¡è¿‡ç¨‹ä¸­ï¼ŒServeråœ¨listenerä¸Šæ¥å—Docker Clientçš„è®¿é—®è¯·æ±‚ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„goroutineæ¥æœåŠ¡è¯¥è¯·æ±‚ã€‚åœ¨goroutineä¸­ï¼Œé¦–å…ˆè¯»å–è¯·æ±‚å†…å®¹ï¼Œç„¶ååšè§£æå·¥ä½œï¼Œæ¥ç€æ‰¾åˆ°ç›¸åº”çš„è·¯ç”±é¡¹ï¼Œéšåè°ƒç”¨ç›¸åº”çš„Handleræ¥å¤„ç†è¯¥è¯·æ±‚ï¼Œæœ€åHandlerå¤„ç†å®Œè¯·æ±‚ä¹‹åå›å¤è¯¥è¯·æ±‚ã€‚ 2.2.2. Engine Engineæ˜¯Dockeræ¶æ„ä¸­çš„è¿è¡Œå¼•æ“ï¼ŒåŒæ—¶ä¹ŸDockerè¿è¡Œçš„æ ¸å¿ƒæ¨¡å—ã€‚å®ƒæ‰®æ¼”Docker containerå­˜å‚¨ä»“åº“çš„è§’è‰²ï¼Œå¹¶ä¸”é€šè¿‡æ‰§è¡Œjobçš„æ–¹å¼æ¥æ“çºµç®¡ç†è¿™äº›å®¹å™¨ã€‚ åœ¨Engineæ•°æ®ç»“æ„çš„è®¾è®¡ä¸å®ç°è¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸€ä¸ªhandlerå¯¹è±¡ã€‚è¯¥handlerå¯¹è±¡å­˜å‚¨çš„éƒ½æ˜¯å…³äºä¼—å¤šç‰¹å®šjobçš„handlerå¤„ç†è®¿é—®ã€‚ä¸¾ä¾‹è¯´æ˜ï¼ŒEngineçš„handlerå¯¹è±¡ä¸­æœ‰ä¸€é¡¹ä¸ºï¼š{\"create\": daemon.ContainerCreate,}ï¼Œåˆ™è¯´æ˜å½“åä¸º\"create\"çš„jobåœ¨è¿è¡Œæ—¶ï¼Œæ‰§è¡Œçš„æ˜¯daemon.ContainerCreateçš„handlerã€‚ 2.2.3. Job ä¸€ä¸ªJobå¯ä»¥è®¤ä¸ºæ˜¯Dockeræ¶æ„ä¸­Engineå†…éƒ¨æœ€åŸºæœ¬çš„å·¥ä½œæ‰§è¡Œå•å…ƒã€‚Dockerå¯ä»¥åšçš„æ¯ä¸€é¡¹å·¥ä½œï¼Œéƒ½å¯ä»¥æŠ½è±¡ä¸ºä¸€ä¸ªjobã€‚ä¾‹å¦‚ï¼šåœ¨å®¹å™¨å†…éƒ¨è¿è¡Œä¸€ä¸ªè¿›ç¨‹ï¼Œè¿™æ˜¯ä¸€ä¸ªjobï¼›åˆ›å»ºä¸€ä¸ªæ–°çš„å®¹å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªjobã€‚Docker Serverçš„è¿è¡Œè¿‡ç¨‹ä¹Ÿæ˜¯ä¸€ä¸ªjobï¼Œåä¸ºserveapiã€‚ Jobçš„è®¾è®¡è€…ï¼ŒæŠŠJobè®¾è®¡å¾—ä¸Unixè¿›ç¨‹ç›¸ä»¿ã€‚æ¯”å¦‚è¯´ï¼šJobæœ‰ä¸€ä¸ªåç§°ï¼Œæœ‰å‚æ•°ï¼Œæœ‰ç¯å¢ƒå˜é‡ï¼Œæœ‰æ ‡å‡†çš„è¾“å…¥è¾“å‡ºï¼Œæœ‰é”™è¯¯å¤„ç†ï¼Œæœ‰è¿”å›çŠ¶æ€ç­‰ã€‚ 2.3. Docker Registry[é•œåƒæ³¨å†Œä¸­å¿ƒ] Docker Registryæ˜¯ä¸€ä¸ªå­˜å‚¨å®¹å™¨é•œåƒçš„ä»“åº“ï¼ˆæ³¨å†Œä¸­å¿ƒï¼‰ï¼Œå¯ç†è§£ä¸ºäº‘ç«¯é•œåƒä»“åº“ï¼ŒæŒ‰repositoryæ¥åˆ†ç±»ï¼Œdocker pull æŒ‰ç…§[repository]:[tag]æ¥ç²¾ç¡®å®šä¹‰ä¸€ä¸ªimageã€‚ åœ¨Dockerçš„è¿è¡Œè¿‡ç¨‹ä¸­ï¼ŒDocker Daemonä¼šä¸Docker Registryé€šä¿¡ï¼Œå¹¶å®ç°æœç´¢é•œåƒã€ä¸‹è½½é•œåƒã€ä¸Šä¼ é•œåƒä¸‰ä¸ªåŠŸèƒ½ï¼Œè¿™ä¸‰ä¸ªåŠŸèƒ½å¯¹åº”çš„jobåç§°åˆ†åˆ«ä¸º\"search\"ï¼Œ\"pull\" ä¸ \"push\"ã€‚ å¯åˆ†ä¸ºå…¬æœ‰ä»“åº“ï¼ˆdocker hubï¼‰å’Œç§æœ‰ä»“åº“ã€‚ 2.4. Graph[dockerå†…éƒ¨æ•°æ®åº“] Graphçš„æ¶æ„å›¾\n2.4.1. Repository å·²ä¸‹è½½é•œåƒçš„ä¿ç®¡è€…ï¼ˆåŒ…æ‹¬ä¸‹è½½é•œåƒå’Œdockerfileæ„å»ºçš„é•œåƒï¼‰ã€‚ ä¸€ä¸ªrepositoryè¡¨ç¤ºæŸç±»é•œåƒçš„ä»“åº“ï¼ˆä¾‹å¦‚Ubuntuï¼‰ï¼ŒåŒä¸€ä¸ªrepositoryå†…çš„é•œåƒç”¨tagæ¥åŒºåˆ†ï¼ˆè¡¨ç¤ºåŒä¸€ç±»é•œåƒçš„ä¸åŒæ ‡ç­¾æˆ–ç‰ˆæœ¬ï¼‰ã€‚ä¸€ä¸ªregistryåŒ…å«å¤šä¸ªrepositoryï¼Œä¸€ä¸ªrepositoryåŒ…å«åŒç±»å‹çš„å¤šä¸ªimageã€‚ é•œåƒçš„å­˜å‚¨ç±»å‹æœ‰aufsï¼Œdevicemapper,Btrfsï¼ŒVfsç­‰ã€‚å…¶ä¸­centosç³»ç»Ÿä½¿ç”¨devicemapperçš„å­˜å‚¨ç±»å‹ã€‚ åŒæ—¶åœ¨Graphçš„æœ¬åœ°ç›®å½•ä¸­ï¼Œå…³äºæ¯ä¸€ä¸ªçš„å®¹å™¨é•œåƒï¼Œå…·ä½“å­˜å‚¨çš„ä¿¡æ¯æœ‰ï¼šè¯¥å®¹å™¨é•œåƒçš„å…ƒæ•°æ®ï¼Œå®¹å™¨é•œåƒçš„å¤§å°ä¿¡æ¯ï¼Œä»¥åŠè¯¥å®¹å™¨é•œåƒæ‰€ä»£è¡¨çš„å…·ä½“rootfsã€‚ 2.4.2. GraphDB å·²ä¸‹è½½å®¹å™¨é•œåƒä¹‹é—´å…³ç³»çš„è®°å½•è€…ã€‚ GraphDBæ˜¯ä¸€ä¸ªæ„å»ºåœ¨SQLiteä¹‹ä¸Šçš„å°å‹å›¾æ•°æ®åº“ï¼Œå®ç°äº†èŠ‚ç‚¹çš„å‘½åä»¥åŠèŠ‚ç‚¹ä¹‹é—´å…³è”å…³ç³»çš„è®°å½• 2.5. Driver[æ‰§è¡Œéƒ¨åˆ†] Driveræ˜¯Dockeræ¶æ„ä¸­çš„é©±åŠ¨æ¨¡å—ã€‚é€šè¿‡Driveré©±åŠ¨ï¼ŒDockerå¯ä»¥å®ç°å¯¹Dockerå®¹å™¨æ‰§è¡Œç¯å¢ƒçš„å®šåˆ¶ã€‚å³Graphè´Ÿè´£é•œåƒçš„å­˜å‚¨ï¼ŒDriverè´Ÿè´£å®¹å™¨çš„æ‰§è¡Œã€‚\n2.5.1. graphdriver graphdriveræ¶æ„å›¾\ngraphdriverä¸»è¦ç”¨äºå®Œæˆå®¹å™¨é•œåƒçš„ç®¡ç†ï¼ŒåŒ…æ‹¬å­˜å‚¨ä¸è·å–ã€‚ å­˜å‚¨ï¼šdocker pullä¸‹è½½çš„é•œåƒç”±graphdriverå­˜å‚¨åˆ°æœ¬åœ°çš„æŒ‡å®šç›®å½•ï¼ˆGraphä¸­ï¼‰ã€‚ è·å–ï¼šdocker runï¼ˆcreateï¼‰ç”¨é•œåƒæ¥åˆ›å»ºå®¹å™¨çš„æ—¶å€™ç”±graphdriveråˆ°æœ¬åœ°Graphä¸­è·å–é•œåƒã€‚ 2.5.2. networkdriver networkdriverçš„æ¶æ„å›¾\nnetworkdriverçš„ç”¨é€”æ˜¯å®ŒæˆDockerå®¹å™¨ç½‘ç»œç¯å¢ƒçš„é…ç½®ï¼Œå…¶ä¸­åŒ…æ‹¬ Dockerå¯åŠ¨æ—¶ä¸ºDockerç¯å¢ƒåˆ›å»ºç½‘æ¡¥ï¼› Dockerå®¹å™¨åˆ›å»ºæ—¶ä¸ºå…¶åˆ›å»ºä¸“å±è™šæ‹Ÿç½‘å¡è®¾å¤‡ï¼› Dockerå®¹å™¨åˆ†é…IPã€ç«¯å£å¹¶ä¸å®¿ä¸»æœºåšç«¯å£æ˜ å°„ï¼Œè®¾ç½®å®¹å™¨é˜²ç«å¢™ç­–ç•¥ç­‰ã€‚ 2.5.3. execdriver execdriverçš„æ¶æ„å›¾\nexecdriverä½œä¸ºDockerå®¹å™¨çš„æ‰§è¡Œé©±åŠ¨ï¼Œè´Ÿè´£åˆ›å»ºå®¹å™¨è¿è¡Œå‘½åç©ºé—´ï¼Œè´Ÿè´£å®¹å™¨èµ„æºä½¿ç”¨çš„ç»Ÿè®¡ä¸é™åˆ¶ï¼Œè´Ÿè´£å®¹å™¨å†…éƒ¨è¿›ç¨‹çš„çœŸæ­£è¿è¡Œç­‰ã€‚ ç°åœ¨execdriveré»˜è®¤ä½¿ç”¨nativeé©±åŠ¨ï¼Œä¸ä¾èµ–äºLXCã€‚ 2.6. libcontainer[å‡½æ•°åº“] libcontainerçš„æ¶æ„å›¾\nlibcontaineræ˜¯Dockeræ¶æ„ä¸­ä¸€ä¸ªä½¿ç”¨Goè¯­è¨€è®¾è®¡å®ç°çš„åº“ï¼Œè®¾è®¡åˆè¡·æ˜¯å¸Œæœ›è¯¥åº“å¯ä»¥ä¸ä¾é ä»»ä½•ä¾èµ–ï¼Œç›´æ¥è®¿é—®å†…æ ¸ä¸­ä¸å®¹å™¨ç›¸å…³çš„APIã€‚ Dockerå¯ä»¥ç›´æ¥è°ƒç”¨libcontainerï¼Œè€Œæœ€ç»ˆæ“çºµå®¹å™¨çš„namespaceã€cgroupsã€apparmorã€ç½‘ç»œè®¾å¤‡ä»¥åŠé˜²ç«å¢™è§„åˆ™ç­‰ã€‚ libcontaineræä¾›äº†ä¸€æ•´å¥—æ ‡å‡†çš„æ¥å£æ¥æ»¡è¶³ä¸Šå±‚å¯¹å®¹å™¨ç®¡ç†çš„éœ€æ±‚ã€‚æˆ–è€…è¯´ï¼Œlibcontainerå±è”½äº†Dockerä¸Šå±‚å¯¹å®¹å™¨çš„ç›´æ¥ç®¡ç†ã€‚ 2.7. docker container[æœåŠ¡äº¤ä»˜çš„æœ€ç»ˆå½¢å¼] containeræ¶æ„\nDocker containerï¼ˆDockerå®¹å™¨ï¼‰æ˜¯Dockeræ¶æ„ä¸­æœåŠ¡äº¤ä»˜çš„æœ€ç»ˆä½“ç°å½¢å¼ã€‚\nDockeræŒ‰ç…§ç”¨æˆ·çš„éœ€æ±‚ä¸æŒ‡ä»¤ï¼Œè®¢åˆ¶ç›¸åº”çš„Dockerå®¹å™¨ï¼š\nç”¨æˆ·é€šè¿‡æŒ‡å®šå®¹å™¨é•œåƒï¼Œä½¿å¾—Dockerå®¹å™¨å¯ä»¥è‡ªå®šä¹‰rootfsç­‰æ–‡ä»¶ç³»ç»Ÿï¼› ç”¨æˆ·é€šè¿‡æŒ‡å®šè®¡ç®—èµ„æºçš„é…é¢ï¼Œä½¿å¾—Dockerå®¹å™¨ä½¿ç”¨æŒ‡å®šçš„è®¡ç®—èµ„æºï¼› ç”¨æˆ·é€šè¿‡é…ç½®ç½‘ç»œåŠå…¶å®‰å…¨ç­–ç•¥ï¼Œä½¿å¾—Dockerå®¹å™¨æ‹¥æœ‰ç‹¬ç«‹ä¸”å®‰å…¨çš„ç½‘ç»œç¯å¢ƒï¼› ç”¨æˆ·é€šè¿‡æŒ‡å®šè¿è¡Œçš„å‘½ä»¤ï¼Œä½¿å¾—Dockerå®¹å™¨æ‰§è¡ŒæŒ‡å®šçš„å·¥ä½œã€‚ å‚è€ƒæ–‡ç« ï¼š\nã€ŠDockeræºç åˆ†æã€‹ ","categories":"","description":"","excerpt":"1. Dockerçš„æ€»æ¶æ„å›¾ dockeræ˜¯ä¸€ä¸ªC/Sæ¨¡å¼çš„æ¶æ„ï¼Œåç«¯æ˜¯ä¸€ä¸ªæ¾è€¦åˆæ¶æ„ï¼Œæ¨¡å—å„å¸å…¶èŒã€‚\nç”¨æˆ·æ˜¯ä½¿ç”¨Docker Client â€¦","ref":"/kubernetes-notes/runtime/docker/docker-architecture/","tags":["Docker"],"title":"Dockeræ•´ä½“æ¶æ„å›¾"},{"body":"é—®é¢˜æè¿° å†…æ ¸ç‰ˆæœ¬ï¼š 5.4.56-200.el7.x86_64\ndockeræŠ¥é”™\nMay 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.565235530+08:00\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.devmapper\" error=\"devmapper not configured\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.565525512+08:00\" level=warning msg=\"could not use snapshotter devmapper in metadata plugin\" error=\"devmapper not configured\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.574734345+08:00\" level=warning msg=\"Your kernel does not support CPU realtime scheduler\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.574792864+08:00\" level=warning msg=\"Your kernel does not support cgroup blkio weight\" May 13 16:54:26 8b26d7a8 dockerd[44352]: time=\"2021-05-13T16:54:26.574800326+08:00\" level=warning msg=\"Your kernel does not support cgroup blkio weight_device\" kubeletæŠ¥é”™\nè§£å†³ cgroupé—®é¢˜è§£å†³ï¼š\n1ã€curl https://pi-ops.oss-cn-hangzhou.aliyuncs.com/scripts/cgroupfs-mount.sh | bash\n2ã€é‡å¯è®¾å¤‡å³å¯è§£å†³\n","categories":"","description":"","excerpt":"é—®é¢˜æè¿° å†…æ ¸ç‰ˆæœ¬ï¼š 5.4.56-200.el7.x86_64\ndockeræŠ¥é”™\nMay 13 16:54:26 8b26d7a8 â€¦","ref":"/kubernetes-notes/trouble-shooting/node/cgroup-subsystem-not-mount/","tags":["é—®é¢˜æ’æŸ¥"],"title":"Cgroupå­ç³»ç»Ÿæ— æ³•æŒ‚è½½"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/cni/","tags":"","title":"CNI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/develop/csi/","tags":"","title":"CSIæ’ä»¶å¼€å‘"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦ä»¥deployment controllerä¸ºä¾‹ï¼Œåˆ†æè¯¥ç±»controllerçš„è¿è¡Œé€»è¾‘ã€‚æ­¤éƒ¨åˆ†ä»£ç ä¸»è¦ä¸ºä½äºpkg/controller/deploymentã€‚pkg/controlleréƒ¨åˆ†çš„ä»£ç åŒ…æ‹¬äº†å„ç§ç±»å‹çš„controllerçš„å…·ä½“å®ç°ã€‚\ncontroller managerçš„pkgéƒ¨åˆ†ä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\ncontroller # ä¸»è¦åŒ…å«å„ç§controllerçš„å…·ä½“å®ç° â”œâ”€â”€ apis â”œâ”€â”€ bootstrap â”œâ”€â”€ certificates â”œâ”€â”€ client_builder.go â”œâ”€â”€ cloud â”œâ”€â”€ clusterroleaggregation â”œâ”€â”€ controller_ref_manager.go â”œâ”€â”€ controller_utils.go # WaitForCacheSync â”œâ”€â”€ cronjob â”œâ”€â”€ daemon â”œâ”€â”€ deployment # deployment controller â”‚Â â”œâ”€â”€ deployment_controller.go # NewDeploymentControllerã€Runã€syncDeployment â”‚Â â”œâ”€â”€ progress.go # syncRolloutStatus â”‚Â â”œâ”€â”€ recreate.go # rolloutRecreate â”‚Â â”œâ”€â”€ rollback.go # rollback â”‚Â â”œâ”€â”€ rolling.go # rolloutRolling â”‚Â â”œâ”€â”€ sync.go â”œâ”€â”€ disruption # disruption controller â”œâ”€â”€ endpoint â”œâ”€â”€ garbagecollector â”œâ”€â”€ history â”œâ”€â”€ job â”œâ”€â”€ lookup_cache.go â”œâ”€â”€ namespace # namespace controller â”œâ”€â”€ nodeipam â”œâ”€â”€ nodelifecycle â”œâ”€â”€ podautoscaler â”œâ”€â”€ podgc â”œâ”€â”€ replicaset # replicaset controller â”œâ”€â”€ replication # replication controller â”œâ”€â”€ resourcequota â”œâ”€â”€ route â”œâ”€â”€ service # service controller â”œâ”€â”€ serviceaccount â”œâ”€â”€ statefulset # statefulset controller â””â”€â”€ volume # PersistentVolumeControllerã€AttachDetachControllerã€PVCProtectionController 1. startDeploymentController func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"}] { return nil, false, nil } dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil } startDeploymentControllerä¸»è¦è°ƒç”¨çš„å‡½æ•°ä¸ºNewDeploymentControllerå’Œå¯¹åº”çš„Runå‡½æ•°ã€‚è¯¥éƒ¨åˆ†é€»è¾‘åœ¨kubernetes/pkg/controllerä¸­ã€‚\n2. NewDeploymentController NewDeploymentControllerä¸»è¦æ„å»ºDeploymentControllerç»“æ„ä½“ã€‚\nè¯¥éƒ¨åˆ†ä¸»è¦å¤„ç†äº†ä»¥ä¸‹é€»è¾‘ï¼š\næ„å»ºå¹¶è¿è¡Œäº‹ä»¶å¤„ç†å™¨eventBroadcasterã€‚ åˆå§‹åŒ–èµ‹å€¼rsControlã€clientsetã€workqueueã€‚ æ·»åŠ dInformerã€rsInformerã€podInformerçš„ResourceEventHandlerFuncsï¼Œå…¶ä¸­ä¸»è¦ä¸ºAddFuncã€UpdateFuncã€DeleteFuncä¸‰ç±»æ–¹æ³•ã€‚ æ„é€ deploymentã€rsã€podçš„Informerçš„Listerå‡½æ•°å’ŒHasSyncedå‡½æ•°ã€‚ è°ƒç”¨syncHandlerï¼Œæ¥å®ç°syncDeploymentã€‚ 2.1. eventBroadcaster è°ƒç”¨äº‹ä»¶å¤„ç†å™¨æ¥è®°å½•deploymentç›¸å…³çš„äº‹ä»¶ã€‚\neventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) // TODO: remove the wrapper when every clients have moved to use the clientset. eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{Interface: v1core.New(client.CoreV1().RESTClient()).Events(\"\")}) 2.2. rsControl æ„é€ DeploymentControllerï¼ŒåŒ…æ‹¬clientsetã€workqueueå’ŒrsControlã€‚å…¶ä¸­rsControlæ˜¯å…·ä½“å®ç°rsé€»è¾‘çš„controllerã€‚\ndc := \u0026DeploymentController{ client: client, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } 2.3. Informer().AddEventHandler æ·»åŠ dInformerã€rsInformerã€podInformerçš„ResourceEventHandlerFuncsï¼Œå…¶ä¸­ä¸»è¦ä¸ºAddFuncã€UpdateFuncã€DeleteFuncä¸‰ç±»æ–¹æ³•ã€‚\ndInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) 2.4. Informer.Lister() è°ƒç”¨dInformerã€rsInformerå’ŒpodInformerçš„Lister()æ–¹æ³•ã€‚\ndc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() 2.5. Informer().HasSynced è°ƒç”¨Informer().HasSyncedï¼Œåˆ¤æ–­æ˜¯å¦ç¼“å­˜å®Œæˆï¼›\ndc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced 2.6. syncHandler syncHandlerå…·ä½“ä¸ºsyncDeploymentï¼ŒsyncHandlerè´Ÿè´£deploymentçš„åŒæ­¥å®ç°ã€‚\ndc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer extensionsinformers.DeploymentInformer, rsInformer extensionsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) // TODO: remove the wrapper when every clients have moved to use the clientset. eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{Interface: v1core.New(client.CoreV1().RESTClient()).Events(\"\")}) if client != nil \u0026\u0026 client.CoreV1().RESTClient().GetRateLimiter() != nil { if err := metrics.RegisterMetricAndTrackRateLimiterUsage(\"deployment_controller\", client.CoreV1().RESTClient().GetRateLimiter()); err != nil { return nil, err } } dc := \u0026DeploymentController{ client: client, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil } 3. DeploymentController.Run Runæ‰§è¡Œwatchå’Œsyncçš„æ“ä½œã€‚\n// Run begins watching and syncing. func (dc *DeploymentController) Run(workers int, stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() defer dc.queue.ShutDown() glog.Infof(\"Starting deployment controller\") defer glog.Infof(\"Shutting down deployment controller\") if !controller.WaitForCacheSync(\"deployment\", stopCh, dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) { return } for i := 0; i \u003c workers; i++ { go wait.Until(dc.worker, time.Second, stopCh) } \u003c-stopCh } 3.1. WaitForCacheSync WaitForCacheSyncä¸»è¦æ˜¯ç”¨æ¥åœ¨List-Watchæœºåˆ¶ä¸­å¯ä»¥ä¿æŒå½“å‰cacheçš„æ•°æ®ä¸etcdçš„æ•°æ®ä¸€è‡´ã€‚\n// WaitForCacheSync is a wrapper around cache.WaitForCacheSync that generates log messages // indicating that the controller identified by controllerName is waiting for syncs, followed by // either a successful or failed sync. func WaitForCacheSync(controllerName string, stopCh \u003c-chan struct{}, cacheSyncs ...cache.InformerSynced) bool { glog.Infof(\"Waiting for caches to sync for %s controller\", controllerName) if !cache.WaitForCacheSync(stopCh, cacheSyncs...) { utilruntime.HandleError(fmt.Errorf(\"Unable to sync caches for %s controller\", controllerName)) return false } glog.Infof(\"Caches are synced for %s controller\", controllerName) return true } 3.2. dc.worker workerè°ƒç”¨äº†processNextWorkItemï¼ŒprocessNextWorkItemæœ€ç»ˆè°ƒç”¨äº†syncHandlerï¼Œè€ŒsyncHandleråœ¨NewDeploymentControllerä¸­èµ‹å€¼çš„å…·ä½“å‡½æ•°ä¸ºsyncDeploymentã€‚\n// worker runs a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the syncHandler is never invoked concurrently with the same key. func (dc *DeploymentController) worker() { for dc.processNextWorkItem() { } } func (dc *DeploymentController) processNextWorkItem() bool { key, quit := dc.queue.Get() if quit { return false } defer dc.queue.Done(key) err := dc.syncHandler(key.(string)) dc.handleErr(err, key) return true } NewDeploymentControllerä¸­çš„syncHandlerèµ‹å€¼ï¼š\nfunc NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { ... dc.syncHandler = dc.syncDeployment ... } 4. syncDeployment syncDeploymentåŸºäºç»™å®šçš„keyæ‰§è¡Œsync deploymentçš„æ“ä½œã€‚\nä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡SplitMetaNamespaceKeyè·å–namespaceå’Œdeploymentå¯¹è±¡çš„nameã€‚ è°ƒç”¨Listerçš„æ¥å£è·å–çš„deploymentçš„å¯¹è±¡ã€‚ getReplicaSetsForDeploymentè·å–deploymentç®¡ç†çš„ReplicaSetå¯¹è±¡ã€‚ getPodMapForDeploymentè·å–deploymentç®¡ç†çš„podï¼ŒåŸºäºReplicaSetæ¥åˆ†ç»„ã€‚ checkPausedConditionsæ£€æŸ¥deploymentæ˜¯å¦æ˜¯pauseçŠ¶æ€å¹¶æ·»åŠ åˆé€‚çš„conditionã€‚ isScalingEventæ£€æŸ¥deploymentçš„æ›´æ–°æ˜¯å¦æ¥è‡ªäºä¸€ä¸ªscaleçš„äº‹ä»¶ï¼Œå¦‚æœæ˜¯åˆ™æ‰§è¡Œscaleçš„æ“ä½œã€‚ æ ¹æ®DeploymentStrategyTypeç±»å‹æ‰§è¡ŒrolloutRecreateæˆ–rolloutRollingã€‚ å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// syncDeployment will sync the deployment with the given key. // This function is not meant to be invoked concurrently with the same key. func (dc *DeploymentController) syncDeployment(key string) error { startTime := time.Now() glog.V(4).Infof(\"Started syncing deployment %q (%v)\", key, startTime) defer func() { glog.V(4).Infof(\"Finished syncing deployment %q (%v)\", key, time.Since(startTime)) }() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return err } deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { glog.V(2).Infof(\"Deployment %v has been deleted\", key) return nil } if err != nil { return err } // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector{} if reflect.DeepEqual(d.Spec.Selector, \u0026everything) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, \"SelectingAll\", \"This deployment is selecting all pods. A non-empty selector is required.\") if d.Status.ObservedGeneration \u003c d.Generation { d.Status.ObservedGeneration = d.Generation dc.client.ExtensionsV1beta1().Deployments(d.Namespace).UpdateStatus(d) } return nil } // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(d) if err != nil { return err } // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil { return err } if d.DeletionTimestamp != nil { return dc.syncStatusOnly(d, rsList, podMap) } // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(d); err != nil { return err } if d.Spec.Paused { return dc.sync(d, rsList, podMap) } // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won't proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if d.Spec.RollbackTo != nil { return dc.rollback(d, rsList, podMap) } scalingEvent, err := dc.isScalingEvent(d, rsList, podMap) if err != nil { return err } if scalingEvent { return dc.sync(d, rsList, podMap) } switch d.Spec.Strategy.Type { case extensions.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case extensions.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList, podMap) } return fmt.Errorf(\"unexpected deployment strategy type: %s\", d.Spec.Strategy.Type) } 4.1. Get deployment // get namespace and deployment name namespace, name, err := cache.SplitMetaNamespaceKey(key) // get deployment by name deployment, err := dc.dLister.Deployments(namespace).Get(name) 4.2. getReplicaSetsForDeployment // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(d) getReplicaSetsForDeploymentå…·ä½“ä»£ç :\n// getReplicaSetsForDeployment uses ControllerRefManager to reconcile // ControllerRef by adopting and orphaning. // It returns the list of ReplicaSets that this Deployment should manage. func (dc *DeploymentController) getReplicaSetsForDeployment(d *apps.Deployment) ([]*apps.ReplicaSet, error) { // List all ReplicaSets to find those we own but that no longer match our // selector. They will be orphaned by ClaimReplicaSets(). rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) if err != nil { return nil, err } deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, fmt.Errorf(\"deployment %s/%s has invalid label selector: %v\", d.Namespace, d.Name, err) } // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing ReplicaSets (see #42639). canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) { fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(d.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != d.UID { return nil, fmt.Errorf(\"original Deployment %v/%v is gone: got uid %v, wanted %v\", d.Namespace, d.Name, fresh.UID, d.UID) } return fresh, nil }) cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) return cm.ClaimReplicaSets(rsList) } 4.3. getPodMapForDeployment // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) getPodMapForDeploymentå…·ä½“ä»£ç ï¼š\n// getPodMapForDeployment returns the Pods managed by a Deployment. // // It returns a map from ReplicaSet UID to a list of Pods controlled by that RS, // according to the Pod's ControllerRef. func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID]*v1.PodList, error) { // Get all Pods that potentially belong to this Deployment. selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, err } pods, err := dc.podLister.Pods(d.Namespace).List(selector) if err != nil { return nil, err } // Group Pods by their controller (if it's in rsList). podMap := make(map[types.UID]*v1.PodList, len(rsList)) for _, rs := range rsList { podMap[rs.UID] = \u0026v1.PodList{} } for _, pod := range pods { // Do not ignore inactive Pods because Recreate Deployments need to verify that no // Pods from older versions are running before spinning up new Pods. controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { continue } // Only append if we care about this UID. if podList, ok := podMap[controllerRef.UID]; ok { podList.Items = append(podList.Items, *pod) } } return podMap, nil } 4.4. checkPausedConditions // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(d); err != nil { return err } if d.Spec.Paused { return dc.sync(d, rsList) } checkPausedConditionså…·ä½“ä»£ç :\n// checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition. // These conditions are needed so that we won't accidentally report lack of progress for resumed deployments // that were paused for longer than progressDeadlineSeconds. func (dc *DeploymentController) checkPausedConditions(d *apps.Deployment) error { if !deploymentutil.HasProgressDeadline(d) { return nil } cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if cond != nil \u0026\u0026 cond.Reason == deploymentutil.TimedOutReason { // If we have reported lack of progress, do not overwrite it with a paused condition. return nil } pausedCondExists := cond != nil \u0026\u0026 cond.Reason == deploymentutil.PausedDeployReason needsUpdate := false if d.Spec.Paused \u0026\u0026 !pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, \"Deployment is paused\") deploymentutil.SetDeploymentCondition(\u0026d.Status, *condition) needsUpdate = true } else if !d.Spec.Paused \u0026\u0026 pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, \"Deployment is resumed\") deploymentutil.SetDeploymentCondition(\u0026d.Status, *condition) needsUpdate = true } if !needsUpdate { return nil } var err error d, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(d) return err } 4.5. isScalingEvent scalingEvent, err := dc.isScalingEvent(d, rsList) if err != nil { return err } if scalingEvent { return dc.sync(d, rsList) } isScalingEventå…·ä½“ä»£ç :\n// isScalingEvent checks whether the provided deployment has been updated with a scaling event // by looking at the desired-replicas annotation in the active replica sets of the deployment. // // rsList should come from getReplicaSetsForDeployment(d). // podMap should come from getPodMapForDeployment(d, rsList). func (dc *DeploymentController) isScalingEvent(d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil { return false, err } allRSs := append(oldRSs, newRS) for _, rs := range controller.FilterActiveReplicaSets(allRSs) { desired, ok := deploymentutil.GetDesiredReplicasAnnotation(rs) if !ok { continue } if desired != *(d.Spec.Replicas) { return true, nil } } return false, nil } 4.6. rolloutRecreate switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) rolloutRecreateå…·ä½“ä»£ç :\n// rolloutRecreate implements the logic for recreating a replica set. func (dc *DeploymentController) rolloutRecreate(d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) error { // Don't create a new RS if not already existed, so that we avoid scaling up before scaling down. newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) // scale down old replica sets. scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus. return dc.syncRolloutStatus(allRSs, newRS, d) } // Do not process a deployment when it has old pods running. if oldPodsRunning(newRS, oldRSs, podMap) { return dc.syncRolloutStatus(allRSs, newRS, d) } // If we need to create a new RS, create it now. if newRS == nil { newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs = append(oldRSs, newRS) } // scale up new replica set. if _, err := dc.scaleUpNewReplicaSetForRecreate(newRS, d); err != nil { return err } if util.DeploymentComplete(d, \u0026d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // Sync deployment status. return dc.syncRolloutStatus(allRSs, newRS, d) } 4.7. rolloutRolling switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) } rolloutRollingå…·ä½“ä»£ç :\n// rolloutRolling implements the logic for rolling a new replica set. func (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // Scale up, if we can. scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d) if err != nil { return err } if scaledUp { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } // Scale down, if we can. scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } if deploymentutil.DeploymentComplete(d, \u0026d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // Sync deployment status return dc.syncRolloutStatus(allRSs, newRS, d) } 5. æ€»ç»“ startDeploymentControllerä¸»è¦åŒ…æ‹¬NewDeploymentControllerå’ŒDeploymentController.Runä¸¤éƒ¨åˆ†ã€‚\nNewDeploymentControllerä¸»è¦æ„å»ºDeploymentControllerç»“æ„ä½“ã€‚\nè¯¥éƒ¨åˆ†ä¸»è¦å¤„ç†äº†ä»¥ä¸‹é€»è¾‘ï¼š\næ„å»ºå¹¶è¿è¡Œäº‹ä»¶å¤„ç†å™¨eventBroadcasterã€‚ åˆå§‹åŒ–èµ‹å€¼rsControlã€clientsetã€workqueueã€‚ æ·»åŠ dInformerã€rsInformerã€podInformerçš„ResourceEventHandlerFuncsï¼Œå…¶ä¸­ä¸»è¦ä¸ºAddFuncã€UpdateFuncã€DeleteFuncä¸‰ç±»æ–¹æ³•ã€‚ æ„é€ deploymentã€rsã€podçš„Informerçš„Listerå‡½æ•°å’ŒHasSyncedå‡½æ•°ã€‚ èµ‹å€¼syncHandlerï¼Œæ¥å®ç°syncDeploymentã€‚ DeploymentController.Runä¸»è¦åŒ…å«WaitForCacheSyncå’ŒsyncDeploymentä¸¤éƒ¨åˆ†ã€‚\nsyncDeploymentåŸºäºç»™å®šçš„keyæ‰§è¡Œsync deploymentçš„æ“ä½œã€‚\nä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡SplitMetaNamespaceKeyè·å–namespaceå’Œdeploymentå¯¹è±¡çš„nameã€‚ è°ƒç”¨Listerçš„æ¥å£è·å–çš„deploymentçš„å¯¹è±¡ã€‚ getReplicaSetsForDeploymentè·å–deploymentç®¡ç†çš„ReplicaSetå¯¹è±¡ã€‚ getPodMapForDeploymentè·å–deploymentç®¡ç†çš„podï¼ŒåŸºäºReplicaSetæ¥åˆ†ç»„ã€‚ checkPausedConditionsæ£€æŸ¥deploymentæ˜¯å¦æ˜¯pauseçŠ¶æ€å¹¶æ·»åŠ åˆé€‚çš„conditionã€‚ isScalingEventæ£€æŸ¥deploymentçš„æ›´æ–°æ˜¯å¦æ¥è‡ªäºä¸€ä¸ªscaleçš„äº‹ä»¶ï¼Œå¦‚æœæ˜¯åˆ™æ‰§è¡Œscaleçš„æ“ä½œã€‚ æ ¹æ®DeploymentStrategyTypeç±»å‹æ‰§è¡ŒrolloutRecreateæˆ–rolloutRollingã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/controller/deployment/deployment_controller.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/controller/deployment/rolling.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/apps.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦ä»¥deployment controllerä¸ºä¾‹ï¼Œåˆ†æè¯¥ â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/deployment-controller/","tags":["æºç åˆ†æ"],"title":"kube-controller-manageræºç åˆ†æï¼ˆäºŒï¼‰ä¹‹ DeploymentController"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/docker/","tags":"","title":"Docker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/etcd/etcdctl/","tags":"","title":"etcdctlå‘½ä»¤å·¥å…·"},{"body":"1. glideç®€ä»‹ glideæ˜¯ä¸€ä¸ªgolangé¡¹ç›®çš„åŒ…ç®¡ç†å·¥å…·ï¼Œéå¸¸æ–¹ä¾¿å¿«æ·ï¼Œä¸€èˆ¬åªéœ€è¦2-3ä¸ªå‘½ä»¤å°±å¯ä»¥å°†goä¾èµ–åŒ…è‡ªåŠ¨ä¸‹è½½å¹¶å½’æ¡£åˆ°vendorçš„ç›®å½•ä¸­ã€‚\n2. glideå®‰è£… go get github.com/Masterminds/glide 3. glideä½¿ç”¨ #è¿›å…¥åˆ°é¡¹ç›®ç›®å½• cd /home/gopath/src/demo #glideåˆå§‹åŒ–ï¼Œåˆå§‹åŒ–é…ç½®æ–‡ä»¶glide.yaml glide init #glideåŠ è½½ä¾èµ–åŒ…ï¼Œè‡ªåŠ¨å½’æ¡£åˆ°vendorç›®å½• glide up -v 4. glideçš„é…ç½®æ–‡ä»¶ glide.yamlè®°å½•ä¾èµ–åŒ…åˆ—è¡¨ã€‚\npackage: demo import: - package: github.com/astaxie/beego version: v1.9.2 testImport: - package: github.com/smartystreets/goconvey version: 1.6.3 subpackages: - convey 5. glide-help æ›´å¤šglideçš„å‘½ä»¤å¸®åŠ©å‚è€ƒglide â€”helpã€‚\nâœ demo glide --help NAME: glide - Vendor Package Management for your Go projects. Each project should have a 'glide.yaml' file in the project directory. Files look something like this: package: github.com/Masterminds/glide imports: - package: github.com/Masterminds/cookoo version: 1.1.0 - package: github.com/kylelemons/go-gypsy subpackages: - yaml For more details on the 'glide.yaml' files see the documentation at https://glide.sh/docs/glide.yaml USAGE: glide [global options] command [command options] [arguments...] VERSION: 0.13.2-dev COMMANDS: create, init Initialize a new project, creating a glide.yaml file config-wizard, cw Wizard that makes optional suggestions to improve config in a glide.yaml file. get Install one or more packages into `vendor/` and add dependency to glide.yaml. remove, rm Remove a package from the glide.yaml file, and regenerate the lock file. import Import files from other dependency management systems. name Print the name of this project. novendor, nv List all non-vendor paths in a directory. rebuild Rebuild ('go build') the dependencies install, i Install a project's dependencies update, up Update a project's dependencies tree (Deprecated) Tree prints the dependencies of this project as a tree. list List prints all dependencies that the present code references. info Info prints information about this project cache-clear, cc Clears the Glide cache. about Learn about Glide mirror Manage mirrors help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --yaml value, -y value Set a YAML configuration file. (default: \"glide.yaml\") --quiet, -q Quiet (no info or debug messages) --debug Print debug verbose informational messages --home value The location of Glide files (default: \"/Users/meitu/.glide\") [$GLIDE_HOME] --tmp value The temp directory to use. Defaults to systems temp [$GLIDE_TMP] --no-color Turn off colored output for log messages --help, -h show help --version, -v print the version ","categories":"","description":"","excerpt":"1. glideç®€ä»‹ glideæ˜¯ä¸€ä¸ªgolangé¡¹ç›®çš„åŒ…ç®¡ç†å·¥å…·ï¼Œéå¸¸æ–¹ä¾¿å¿«æ·ï¼Œä¸€èˆ¬åªéœ€è¦2-3ä¸ªå‘½ä»¤å°±å¯ä»¥å°†goä¾èµ–åŒ…è‡ªåŠ¨ä¸‹è½½å¹¶å½’æ¡£ â€¦","ref":"/golang-notes/introduction/package/glide-usage/","tags":["Golang"],"title":"glideçš„ä½¿ç”¨"},{"body":"åœ¨golangci-lintçš„ä½¿ç”¨ä»‹ç»ä¸€æ–‡ä¸­æˆ‘ä»¬æåˆ°äº†linterå¸¸è§çš„å‡ ç§é…ç½®ï¼Œä¾‹å¦‚ï¼Œgocycloï¼ˆåœˆå¤æ‚åº¦ï¼‰ï¼Œduplï¼ˆé‡å¤ä»£ç ï¼‰ç­‰ã€‚æœ¬æ–‡ä¸»è¦ä»‹ç»è¿™å‡ ç§å¸¸è§linterçš„é—®é¢˜å¦‚ä½•ä¼˜åŒ–ã€‚\n1. gocycloï¼ˆåœˆå¤æ‚åº¦ï¼‰ åœ¨ Go ä¸­ï¼Œåœˆå¤æ‚åº¦ï¼ˆCyclomatic Complexityï¼‰ æ˜¯ä¸€ç§è¡¡é‡ä»£ç å¤æ‚åº¦çš„æŒ‡æ ‡ï¼Œè¡¨ç¤ºä»£ç ä¸­å¯èƒ½çš„ç‹¬ç«‹æ‰§è¡Œè·¯å¾„çš„æ•°é‡ã€‚å®ƒåæ˜ äº†ç¨‹åºçš„é€»è¾‘å¤æ‚åº¦ï¼Œä¹Ÿå°±æ˜¯ä»£ç æœ‰å¤šå°‘ä¸ªåˆ†æ”¯æˆ–å†³ç­–ç‚¹ï¼ˆå¦‚ ifã€forã€switch ç­‰ï¼‰ã€‚åœˆå¤æ‚åº¦è¶Šé«˜ï¼Œä»£ç çš„ç†è§£å’Œç»´æŠ¤æˆæœ¬å°±è¶Šå¤§ã€‚\n.golangci.ymlä¸­åœˆå¤æ‚åº¦çš„é…ç½®å¦‚ä¸‹ï¼š\nlinters: enabel: - gocycle linters-settings: gocyclo: # Minimal code complexity to report. # Default: 30 (but we recommend 10-20) min-complexity: 10 1.1. è®¡ç®—è§„åˆ™ åœˆå¤æ‚åº¦çš„è®¡ç®—å…¬å¼ä¸ºï¼š\nM=Eâˆ’N+2P\nå…¶ä¸­ï¼š\nEï¼šä»£ç ä¸­çš„è¾¹æ•°ï¼ˆæ§åˆ¶æµå›¾ä¸­çš„è¾¹ï¼Œè¡¨ç¤ºç¨‹åºä¸­çš„æ§åˆ¶æµè·¯å¾„ï¼Œä¾‹å¦‚ä»ä¸€æ¡è¯­å¥è·³è½¬åˆ°ä¸‹ä¸€æ¡è¯­å¥ï¼‰ã€‚ Nï¼šä»£ç ä¸­çš„èŠ‚ç‚¹æ•°ï¼ˆæ§åˆ¶æµå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œè¡¨ç¤ºç¨‹åºä¸­çš„åŸºæœ¬å—æˆ–è¯­å¥ï¼‰ã€‚ Pï¼šæ§åˆ¶æµå›¾ä¸­è¿é€šéƒ¨åˆ†çš„æ•°é‡ï¼ˆé€šå¸¸ä¸º 1ï¼‰ã€‚ ç®€å•æ¥è¯´ï¼Œåœˆå¤æ‚åº¦å¯ä»¥é€šè¿‡ç»Ÿè®¡ä»£ç ä¸­çš„ä»¥ä¸‹å†³ç­–ç‚¹æ¥ä¼°ç®—ï¼Œå…¶ä¸­åˆå§‹å€¼ä¸º1ï¼Œå³é»˜è®¤è¿”å›è·¯å¾„ï¼Œå†æ ¹æ®ä»¥ä¸‹è§„åˆ™ç´¯åŠ ï¼š\næ¯ä¸ª if æˆ– else if å¢åŠ  1ã€‚ æ¯ä¸ª for æˆ– while å¾ªç¯å¢åŠ  1ã€‚ æ¯ä¸ª caseï¼ˆä¸åŒ…æ‹¬ defaultï¼‰å¢åŠ  1ã€‚ æ¯ä¸ª \u0026\u0026 æˆ– || è¡¨è¾¾å¼å¢åŠ  1ã€‚ ä¸¾ä¾‹è¯´æ˜ï¼š\nfunc Example(a, b int) int { if a \u003e 0 { // ifè¯­å¥ +1 if b \u003e 0 { // ifè¯­å¥ +1 return a + b } else { return a - b } } return 0 åˆå§‹å€¼ä¸º1 } æ§åˆ¶æµè·¯å¾„ä¸ºï¼š\nåˆå§‹å€¼ä¸º1 2ä¸ªifè¯­å¥ + 2 å› æ­¤è¯¥å‡½æ•°çš„åœˆå¤æ‚åº¦ä¸º3ã€‚\n1.2. å¸¸è§æ ‡å‡† é€šå¸¸ï¼Œåœˆå¤æ‚åº¦ä»¥åˆç†çš„èŒƒå›´ä¸ºä½³ï¼ˆä¾‹å¦‚ï¼š10-15ï¼‰ï¼Œè¿‡ä½çš„åœˆå¤æ‚åº¦ä¼šå¯¼è‡´æ‹†åˆ†å‡½æ•°è¿‡å¤šï¼Œä»£ç å¯è¯»æ€§å˜å·®ã€‚\nä»¥ä¸‹æ˜¯åœˆå¤æ‚åº¦çš„å‚è€ƒæ ‡å‡†ï¼š\n1-10ï¼šä»£ç é€»è¾‘ç®€å•ï¼Œå¯ç»´æŠ¤æ€§é«˜ã€‚ 11-20ï¼šä»£ç é€»è¾‘è¾ƒå¤æ‚ï¼Œå¯èƒ½éœ€è¦é‡æ„ã€‚ 21+ï¼šä»£ç é€»è¾‘éå¸¸å¤æ‚ï¼Œç»´æŠ¤æˆæœ¬é«˜ï¼Œå»ºè®®æ‹†åˆ†å‡½æ•°ã€‚ 1.3. ä¼˜åŒ–åœˆå¤æ‚åº¦ æ‹†åˆ†å‡½æ•°ï¼šï¼ˆæœ€å¸¸è§çš„æ–¹å¼ï¼‰å°†å¤æ‚çš„å‡½æ•°æ‹†åˆ†æˆå¤šä¸ªå°å‡½æ•°ï¼Œæ¯ä¸ªå‡½æ•°åªå¤„ç†ä¸€ç§é€»è¾‘ã€‚ å‡å°‘åµŒå¥—ï¼šä½¿ç”¨ early return å‡å°‘åµŒå¥—å±‚çº§ã€‚ ç®€åŒ–é€»è¾‘ï¼šåˆå¹¶æ¡ä»¶è¡¨è¾¾å¼ï¼Œé¿å…é‡å¤çš„åˆ†æ”¯é€»è¾‘ã€‚ ä¼˜åŒ–åœˆå¤æ‚åº¦çš„å…³é”®åœ¨äºï¼š\nå‡å°‘åµŒå¥—ï¼Œå¢åŠ ä»£ç çš„æ‰å¹³åŒ–ã€‚ æé«˜ä»£ç çš„æ¨¡å—åŒ–å’Œå¤ç”¨æ€§ã€‚ 2. duplï¼ˆé‡å¤ä»£ç ï¼‰ Golang çš„é™æ€åˆ†æå·¥å…·ï¼ˆå¦‚ golangci-lintï¼‰èƒ½å¤Ÿæ£€æµ‹ä»£ç ä¸­çš„é‡å¤éƒ¨åˆ†ã€‚è¿™ç±»å·¥å…·é€šå¸¸ä½¿ç”¨é‡å¤ä»£ç æ£€æµ‹ç®—æ³•ï¼Œé€šè¿‡å¯¹ä»£ç å—è¿›è¡Œç‰¹å®šåˆ†æï¼Œæ‰¾åˆ°ç›¸ä¼¼æˆ–å®Œå…¨ç›¸åŒçš„ä»£ç ç‰‡æ®µã€‚\ngolangci-lintä¸­æ£€æŸ¥é‡å¤ä»£ç çš„é…ç½®å¦‚ä¸‹ï¼š\nlinters: enabel: - dupl linters-settings: dupl: # Tokens count to trigger issue. # Default: 150 threshold: 100 2.1. è®¡ç®—è§„åˆ™ Tokenizationï¼ˆä»£ç æ ‡è®°åŒ–ï¼‰\nå·¥å…·ä¼šå°†æºä»£ç åˆ†è§£ä¸ºæ ‡è®°ï¼ˆTokenï¼‰ï¼Œæ¯”å¦‚å…³é”®å­—ã€æ ‡è¯†ç¬¦ã€æ“ä½œç¬¦ç­‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå®ƒå¯ä»¥å¿½ç•¥æ³¨é‡Šå’Œæ ¼å¼å·®å¼‚ï¼Œåªå…³æ³¨ä»£ç çš„è¯­ä¹‰ã€‚\nToken æ˜¯ä»£ç çš„æœ€å°ç»„æˆå•å…ƒï¼ŒåŒ…æ‹¬ï¼š\nå…³é”®å­—ï¼šå¦‚ ifã€forã€switchã€‚ æ ‡è¯†ç¬¦ï¼šå˜é‡åã€å‡½æ•°åã€ç±»å‹åç­‰ã€‚ æ“ä½œç¬¦ï¼šå¦‚ +ã€-ã€*ã€/ ç­‰ã€‚ å¸¸é‡å€¼ï¼šå¦‚ 123ã€\"text\"ã€‚ ç•Œç¬¦ï¼šå¦‚ {ã€}ã€(ã€)ã€‚ threshold å‚æ•°çš„ä½œç”¨\nå®šä¹‰ï¼š threshold æ˜¯é‡å¤ä»£ç å—çš„æœ€å° Token æ•°ã€‚ å¦‚æœä¸¤ä¸ªä»£ç å—ä¸­æœ‰ ç›¸åŒ Token çš„æ•°é‡ \u003e= thresholdï¼Œåˆ™ä¼šè¢«è®¤ä¸ºæ˜¯é‡å¤ä»£ç ï¼Œå¹¶æŠ¥å‘Šã€‚ ç¤ºä¾‹ï¼šthreshold: 50\nå¦‚æœä¸¤ä¸ªä»£ç å—ä¸­æœ‰ 50 ä¸ªæˆ–æ›´å¤šçš„ Token æ˜¯ç›¸åŒçš„ï¼Œdupl ä¼šæŠ¥å‘Šå®ƒä»¬ä¸ºé‡å¤ä»£ç ã€‚ å°äº 50 ä¸ª Token çš„é‡å¤ä»£ç ä¸ä¼šè¢«æŠ¥å‘Šã€‚ 2.2. é…ç½®æ–¹å¼ é€šè¿‡é…ç½®thresholdçš„å¤§å°å¯ä»¥æ§åˆ¶é‡å¤ä»£ç çš„ç²’åº¦ã€‚thresholdé»˜è®¤å€¼æ˜¯150ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°ã€‚\né˜ˆå€¼è¶Šå°ï¼šè¶Šå®¹æ˜“æ£€æµ‹åˆ°å°çš„é‡å¤ä»£ç ï¼Œä½†å¯èƒ½å¯¼è‡´è¯¯æŠ¥å¢å¤šã€‚\né˜ˆå€¼è¶Šå¤§ï¼šæ£€æµ‹æ›´å®½æ¾ï¼ŒåªæŠ¥å‘Šè¾ƒå¤§çš„é‡å¤ä»£ç å—ã€‚\nå½“å‘ç°é‡å¤ä»£ç æ—¶ï¼Œgolangci-lint ä¼šè¾“å‡ºç±»ä¼¼çš„æŠ¥å‘Šï¼š\nmain.go:10-20: duplicated code found in main.go:30-40 (dupl) å¦‚æœè¦å¿½ç•¥ç‰¹å®šä»£ç ï¼Œåœ¨ä»£ç ä¸­æ·»åŠ æ³¨é‡Šï¼š\n//nolint:dupl func SomeFunction() { // é‡å¤ä»£ç å— } 2.3. ä¼˜åŒ–é‡å¤ä»£ç  æå–å‡½æ•°ï¼šå°†é‡å¤é€»è¾‘æŠ½è±¡ä¸ºé€šç”¨å‡½æ•°ã€‚ æ˜ å°„è¡¨ä»£æ›¿åˆ†æ”¯ï¼š ç”¨é”®å€¼å¯¹ç®€åŒ–æ¡ä»¶é€»è¾‘ã€‚ åˆ©ç”¨æ³›å‹ï¼š åˆå¹¶ä¸åŒç±»å‹çš„é‡å¤é€»è¾‘ã€‚ ç¤ºä¾‹1ï¼š\nå¦‚æœé‡å¤ä»£ç åŒ…å«å¤šä¸ª if-else æˆ– switchï¼Œå°è¯•ç”¨æ˜ å°„è¡¨æˆ–é…ç½®æ–‡ä»¶æ›¿ä»£ã€‚\n// Before: é‡å¤ä»£ç å— func GetDiscount(category string) float64 { switch category { case \"student\": return 0.15 case \"veteran\": return 0.2 case \"senior\": return 0.25 default: return 0.0 } } // After: ä½¿ç”¨æ˜ å°„è¡¨ func GetDiscount(category string) float64 { discounts := map[string]float64{ \"student\": 0.15, \"veteran\": 0.2, \"senior\": 0.25, } return discounts[category] } ç¤ºä¾‹2ï¼š\nä½¿ç”¨ æ³›å‹\n// Before: é‡å¤ä»£ç å— func FindMaxInt(a, b int) int { if a \u003e b { return a } return b } func FindMaxFloat(a, b float64) float64 { if a \u003e b { return a } return b } // After: ä½¿ç”¨æ³›å‹ func FindMax[T constraints.Ordered](a, b T) T { if a \u003e b { return a } return b } 3. æ€»ç»“ æœ¬æ–‡ä¸»è¦åˆ†æäº†gocycloï¼ˆåœˆå¤æ‚åº¦ï¼‰ï¼Œduplï¼ˆé‡å¤ä»£ç ï¼‰è¿™2ç§linteré…ç½®çš„æ£€æµ‹ã€é…ç½®å’Œä¼˜åŒ–æ–¹å¼ã€‚å› ä¸ºåœ¨å¤šäººå›¢é˜Ÿçš„å¼€å‘ä¸­ç»å¸¸ä¼šå› ä¸ºå¼€å‘è€…çš„æ°´å¹³ä¸ä¸€ï¼Œæ ‡å‡†ä¸ä¸€å¯¼è‡´æ— æ³•å¼€å‘å‡ºç»Ÿä¸€ä¸”è¾ƒé«˜è´¨é‡çš„ä»£ç ã€‚è™½ç„¶ä»£ç çš„è´¨é‡ä¸ä¼šä¸¥æ ¼å½±å“åŠŸèƒ½çš„è¿è¡Œï¼Œä½†å¯ä»¥ä¸ºæœªæ¥çš„å¼€å‘è€…æä¾›å¯è¯»æ€§æ›´å¼ºï¼Œæ›´æ–¹ä¾¿æ¥æ‰‹å¼€å‘çš„ä»£ç ã€‚åŒæ ·é€šè¿‡è¿™ç§æ–¹å¼ä¹Ÿèƒ½åˆæ­¥çœ‹å‡ºä¸€ä¸ªå¼€å‘è€…ä»£ç è´¨é‡æ°´å¹³çš„é«˜ä½ã€‚\nå‚è€ƒï¼š\nhttps://golangci-lint.run/usage/linters/#gocyclo https://golangci-lint.run/usage/linters/#dupl ","categories":"","description":"","excerpt":"åœ¨golangci-lintçš„ä½¿ç”¨ä»‹ç»ä¸€æ–‡ä¸­æˆ‘ä»¬æåˆ°äº†linterå¸¸è§çš„å‡ ç§é…ç½®ï¼Œä¾‹å¦‚ï¼Œgocycloï¼ˆåœˆå¤æ‚åº¦ï¼‰ï¼Œduplï¼ˆé‡å¤ä»£ç ï¼‰ç­‰ã€‚ â€¦","ref":"/golang-notes/standard/fix-golangci-lint/","tags":"","title":"golangci-linté—®é¢˜ä¼˜åŒ–"},{"body":"1. HPAç®€ä»‹ HPAå…¨ç§°HorizontalPodAutoscalerï¼Œå³podæ°´å¹³æ‰©å®¹ï¼Œå¢åŠ æˆ–å‡å°‘podçš„æ•°é‡ã€‚ç›¸å¯¹äºVPAè€Œè¨€ï¼ŒVPAæ˜¯å¢åŠ æˆ–å‡å°‘å•ä¸ªpodçš„CPUæˆ–å†…å­˜ã€‚HPAä¸»è¦ä½œç”¨äºDeploymentæˆ–Statefulsetçš„å·¥ä½œè´Ÿè½½ï¼Œæ— æ³•ä½œç”¨äºDaemonsetçš„å·¥ä½œè´Ÿè½½ã€‚\nç¤ºä¾‹å›¾ï¼š\nKubernetes å°†æ°´å¹³ Pod è‡ªåŠ¨æ‰©ç¼©å®ç°ä¸ºä¸€ä¸ªé—´æ­‡è¿è¡Œçš„æ§åˆ¶å›è·¯ï¼ˆå®ƒä¸æ˜¯ä¸€ä¸ªè¿ç»­çš„è¿‡ç¨‹ï¼‰ã€‚é—´éš”ç”± kube-controller-manager çš„ --horizontal-pod-autoscaler-sync-period å‚æ•°è®¾ç½®ï¼ˆé»˜è®¤é—´éš”ä¸º 15 ç§’ï¼‰ã€‚\nåœ¨æ¯ä¸ªæ—¶é—´æ®µå†…ï¼Œæ§åˆ¶å™¨ç®¡ç†å™¨éƒ½ä¼šæ ¹æ®æ¯ä¸ª HorizontalPodAutoscaler å®šä¹‰ä¸­æŒ‡å®šçš„æŒ‡æ ‡æŸ¥è¯¢èµ„æºåˆ©ç”¨ç‡ã€‚ æ§åˆ¶å™¨ç®¡ç†å™¨æ‰¾åˆ°ç”± scaleTargetRef å®šä¹‰çš„ç›®æ ‡èµ„æºï¼Œç„¶åæ ¹æ®ç›®æ ‡èµ„æºçš„ .spec.selector æ ‡ç­¾é€‰æ‹© Podï¼Œ å¹¶ä»èµ„æºæŒ‡æ ‡ APIï¼ˆé’ˆå¯¹æ¯ä¸ª Pod çš„èµ„æºæŒ‡æ ‡ï¼‰æˆ–è‡ªå®šä¹‰æŒ‡æ ‡è·å–æŒ‡æ ‡ APIï¼ˆé€‚ç”¨äºæ‰€æœ‰å…¶ä»–æŒ‡æ ‡ï¼‰ã€‚\nHPAä¾èµ–metrics-serveræ¥è·å–CPUå’Œå†…å­˜æ•°æ®ï¼Œä»¥ä¸‹è¯´æ˜metrics-serverçš„éƒ¨ç½²æµç¨‹ã€‚\n2. éƒ¨ç½²metrics-server ä¸‹è½½metrics-serveræ–‡ä»¶\nwget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml ä¿®æ”¹å¯åŠ¨å‚æ•°ï¼Œå¢åŠ --kubelet-insecure-tlsï¼Œå¦åˆ™ä¼šæŠ¥è·å–æ¥å£è¯ä¹¦å¤±è´¥ã€‚\nspec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # å¢åŠ è¯¥å‚æ•° åˆ›å»ºyamlæœåŠ¡\nkubectl apply -f components.yaml é€šè¿‡kubectl topæŸ¥çœ‹èµ„æºä¿¡æ¯\n# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node1 144m 0% 5762Mi 2% node2 337m 0% 5475Mi 2% node3 100m 0% 5326Mi 2% node4 302m 0% 5649Mi 2% # kubectl top pod -n prometheus NAME CPU(cores) MEMORY(bytes) alertmanager-kube-prometheus-stack-alertmanager-0 1m 30Mi kube-prometheus-stack-grafana-7688b45b4c-mvwd6 1m 225Mi kube-prometheus-stack-kube-state-metrics-5d6578867c-25xbq 1m 21Mi kube-prometheus-stack-operator-9c5fbdc68-nrn7h 1m 33Mi kube-prometheus-stack-prometheus-node-exporter-8ghd8 1m 4Mi kube-prometheus-stack-prometheus-node-exporter-brtp9 1m 4Mi kube-prometheus-stack-prometheus-node-exporter-n4kdp 1m 4Mi kube-prometheus-stack-prometheus-node-exporter-ttksv 1m 4Mi prometheus-kube-prometheus-stack-prometheus-0 8m 622Mi åŒæ—¶åœ¨k8s dashboardä¸Šä¹Ÿå¯ä»¥æŸ¥çœ‹åˆ°å®æ—¶çš„CPUå’Œå†…å­˜ä¿¡æ¯ã€‚\n3. HPAé…ç½® todo\nå‚è€ƒï¼š\nhttps://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/ https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server https://github.com/kubernetes-sigs/metrics-server https://www.qikqiak.com/post/k8s-hpa-usage/ ","categories":"","description":"","excerpt":"1. HPAç®€ä»‹ HPAå…¨ç§°HorizontalPodAutoscalerï¼Œå³podæ°´å¹³æ‰©å®¹ï¼Œå¢åŠ æˆ–å‡å°‘podçš„æ•°é‡ã€‚ç›¸å¯¹äºVPAè€Œ â€¦","ref":"/kubernetes-notes/operation/deployment/k8s-hpa-usage/","tags":["Kubernetes"],"title":"HPA[è‡ªåŠ¨æ‰©ç¼©å®¹]é…ç½®"},{"body":"1. è¯ä¹¦åˆ†ç±» æœåŠ¡å™¨è¯ä¹¦ï¼šserver certï¼Œç”¨äºå®¢æˆ·ç«¯éªŒè¯æœåŠ¡ç«¯çš„èº«ä»½ã€‚\nå®¢æˆ·ç«¯è¯ä¹¦ï¼šclient certï¼Œç”¨äºæœåŠ¡ç«¯éªŒè¯å®¢æˆ·ç«¯çš„èº«ä»½ã€‚\nå¯¹ç­‰è¯ä¹¦ï¼špeer certï¼ˆæ—¢æ˜¯server certåˆæ˜¯client certï¼‰ï¼Œç”¨æˆ·æˆå‘˜ä¹‹é—´çš„èº«ä»½éªŒè¯ï¼Œä¾‹å¦‚ etcdã€‚\n1.1. k8sé›†ç¾¤çš„è¯ä¹¦åˆ†ç±» etcdèŠ‚ç‚¹ï¼šéœ€è¦æ ‡è¯†è‡ªå·±æœåŠ¡çš„server certï¼Œä¹Ÿéœ€è¦client certä¸etcdé›†ç¾¤å…¶ä»–èŠ‚ç‚¹äº¤äº’ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªå¯¹ç­‰è¯ä¹¦ã€‚ masterèŠ‚ç‚¹ï¼šéœ€è¦æ ‡è¯† apiserveræœåŠ¡çš„server certï¼Œä¹Ÿéœ€è¦client certè¿æ¥etcdé›†ç¾¤ï¼Œä¹Ÿéœ€è¦ä¸€ä¸ªå¯¹ç­‰è¯ä¹¦ã€‚ kubeletï¼šéœ€è¦æ ‡è¯†è‡ªå·±æœåŠ¡çš„server certï¼Œä¹Ÿéœ€è¦client certè¯·æ±‚apiserverï¼Œä¹Ÿä½¿ç”¨ä¸€ä¸ªå¯¹ç­‰è¯ä¹¦ã€‚ kubectlã€kube-proxyã€calicoï¼šéœ€è¦clientè¯ä¹¦ã€‚ 2. CAè¯ä¹¦åŠç§˜é’¥ ç›®å½•ï¼š/etc/kubernetes/ssl\nåˆ†ç±» è¯ä¹¦/ç§˜é’¥ è¯´æ˜ ç»„ä»¶ ca ca-key.pem ca.pem ca.csr Kubernetes kubernetes-key.pem kubernetes.pem kubernetes.csr Admin admin-key.pem admin.pem admin.csr Kubelet kubelet.crt kubelet.key é…ç½®æ–‡ä»¶\nåˆ†ç±» è¯ä¹¦/ç§˜é’¥ è¯´æ˜ ca ca-config.json ca-csr.json Kubernetes kubernetes-csr.json Admin admin-csr.json Kube-proxy kube-proxy-csr.json 3. cfsslå·¥å…· å®‰è£…cfsslï¼š\n# ä¸‹è½½cfssl $ curl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl $ curl https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson $ curl https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfo # æ·»åŠ å¯æ‰§è¡Œæƒé™ $ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson /usr/local/bin/cfssl-certinfo 4. åˆ›å»º CA (Certificate Authority) 4.1. é…ç½®æºæ–‡ä»¶ åˆ›å»º CA é…ç½®æ–‡ä»¶\nca-config.json\ncat \u003c\u003c EOF \u003e ca-config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"876000h\" } } } } EOF å‚æ•°è¯´æ˜\nca-config.jsonï¼šå¯ä»¥å®šä¹‰å¤šä¸ª profilesï¼Œåˆ†åˆ«æŒ‡å®šä¸åŒçš„è¿‡æœŸæ—¶é—´ã€ä½¿ç”¨åœºæ™¯ç­‰å‚æ•°ï¼›åç»­åœ¨ç­¾åè¯ä¹¦æ—¶ä½¿ç”¨æŸä¸ª profileï¼› signingï¼šè¡¨ç¤ºè¯¥è¯ä¹¦å¯ç”¨äºç­¾åå…¶å®ƒè¯ä¹¦ï¼›ç”Ÿæˆçš„ ca.pem è¯ä¹¦ä¸­ CA=TRUEï¼› server authï¼šè¡¨ç¤ºclientå¯ä»¥ç”¨è¯¥ CA å¯¹serveræä¾›çš„è¯ä¹¦è¿›è¡ŒéªŒè¯ï¼› client authï¼šè¡¨ç¤ºserverå¯ä»¥ç”¨è¯¥CAå¯¹clientæä¾›çš„è¯ä¹¦è¿›è¡ŒéªŒè¯ï¼› åˆ›å»º CA è¯ä¹¦ç­¾åè¯·æ±‚\nca-csr.json\ncat \u003c\u003c EOF \u003e ca-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF å‚æ•°è¯´æ˜\nca-csr.jsonçš„å‚æ•°\nCNï¼šCommon Nameï¼Œkube-apiserver ä»è¯ä¹¦ä¸­æå–è¯¥å­—æ®µä½œä¸ºè¯·æ±‚çš„ç”¨æˆ·å (User Name)ï¼›æµè§ˆå™¨ä½¿ç”¨è¯¥å­—æ®µéªŒè¯ç½‘ç«™æ˜¯å¦åˆæ³•ï¼› namesä¸­çš„å­—æ®µï¼š\nC : countryï¼Œå›½å®¶ ST: stateï¼Œå·æˆ–çœä»½ Lï¼šlocationï¼ŒåŸå¸‚ Oï¼šorganizationï¼Œç»„ç»‡ï¼Œkube-apiserver ä»è¯ä¹¦ä¸­æå–è¯¥å­—æ®µä½œä¸ºè¯·æ±‚ç”¨æˆ·æ‰€å±çš„ç»„ (Group) OUï¼šorganization unit 4.2. æ‰§è¡Œå‘½ä»¤ cfssl gencert -initca ca-csr.json | cfssljson -bare ca è¾“å‡ºå¦‚ä¸‹ï¼š\n# cfssl gencert -initca ca-csr.json | cfssljson -bare ca 2019/12/13 14:35:52 [INFO] generating a new CA key and certificate from CSR 2019/12/13 14:35:52 [INFO] generate received request 2019/12/13 14:35:52 [INFO] received CSR 2019/12/13 14:35:52 [INFO] generating key: rsa-2048 2019/12/13 14:35:52 [INFO] encoded CSR 2019/12/13 14:35:52 [INFO] signed certificate with serial number 248379771349454958117219047414671162179070747780 ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š\n# ç”Ÿæˆæ–‡ä»¶ -rw-r--r-- 1 root root 1005 12æœˆ 13 11:32 ca.csr -rw------- 1 root root 1675 12æœˆ 13 11:32 ca-key.pem -rw-r--r-- 1 root root 1363 12æœˆ 13 11:32 ca.pem # é…ç½®æºæ–‡ä»¶ -rw-r--r-- 1 root root 293 12æœˆ 13 11:31 ca-config.json -rw-r--r-- 1 root root 210 12æœˆ 13 11:31 ca-csr.json 5. åˆ›å»º kubernetes è¯ä¹¦ 5.1. é…ç½®æºæ–‡ä»¶ åˆ›å»º kubernetes è¯ä¹¦ç­¾åè¯·æ±‚æ–‡ä»¶kubernetes-csr.jsonã€‚\ncat \u003c\u003c EOF \u003e kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"\u003cMASTER_IP\u003e\", \"\u003cMASTER_CLUSTER_IP\u003e\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [{ \"C\": \"\u003ccountry\u003e\", \"ST\": \"\u003cstate\u003e\", \"L\": \"\u003ccity\u003e\", \"O\": \"\u003corganization\u003e\", \"OU\": \"\u003corganization unit\u003e\" }] } EOF å‚æ•°è¯´æ˜ï¼š\nMASTER_IPï¼šmasterèŠ‚ç‚¹çš„IPæˆ–åŸŸå MASTER_CLUSTER_IPï¼škube-apiserver æŒ‡å®šçš„ service-cluster-ip-range ç½‘æ®µçš„ç¬¬ä¸€ä¸ªIPï¼Œä¾‹å¦‚ï¼ˆ10.254.0.1ï¼‰ã€‚ 5.2. æ‰§è¡Œå‘½ä»¤ $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes è¾“å‡ºå¦‚ä¸‹ï¼š\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 2019/12/13 14:40:28 [INFO] generate received request 2019/12/13 14:40:28 [INFO] received CSR 2019/12/13 14:40:28 [INFO] generating key: rsa-2048 2019/12/13 14:40:28 [INFO] encoded CSR 2019/12/13 14:40:28 [INFO] signed certificate with serial number 392795299385191732458211386861696542628305189374 2019/12/13 14:40:28 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š\n# ç”Ÿæˆæ–‡ä»¶ -rw-r--r-- 1 root root 1269 12æœˆ 13 14:40 kubernetes.csr -rw------- 1 root root 1679 12æœˆ 13 14:40 kubernetes-key.pem -rw-r--r-- 1 root root 1643 12æœˆ 13 14:40 kubernetes.pem # é…ç½®æºæ–‡ä»¶ -rw-r--r-- 1 root root 580 12æœˆ 13 14:40 kubernetes-csr.json 6. åˆ›å»º admin è¯ä¹¦ 6.1. é…ç½®æºæ–‡ä»¶ åˆ›å»º admin è¯ä¹¦ç­¾åè¯·æ±‚æ–‡ä»¶ admin-csr.jsonï¼š\ncat \u003c\u003c EOF \u003e admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } EOF 6.2. æ‰§è¡Œå‘½ä»¤ $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin è¾“å‡ºå¦‚ä¸‹ï¼š\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 2019/12/13 14:52:37 [INFO] generate received request 2019/12/13 14:52:37 [INFO] received CSR 2019/12/13 14:52:37 [INFO] generating key: rsa-2048 2019/12/13 14:52:37 [INFO] encoded CSR 2019/12/13 14:52:37 [INFO] signed certificate with serial number 465422983473444224050765004141217688748259757371 2019/12/13 14:52:37 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). ç”Ÿæˆæ–‡ä»¶\n# ç”Ÿæˆæ–‡ä»¶ -rw-r--r-- 1 root root 1013 12æœˆ 13 14:52 admin.csr -rw------- 1 root root 1675 12æœˆ 13 14:52 admin-key.pem -rw-r--r-- 1 root root 1407 12æœˆ 13 14:52 admin.pem # é…ç½®æºæ–‡ä»¶ -rw-r--r-- 1 root root 231 12æœˆ 13 14:49 admin-csr.json 7. åˆ›å»º kube-proxy è¯ä¹¦ 7.1. é…ç½®æºæ–‡ä»¶ åˆ›å»º kube-proxy è¯ä¹¦ç­¾åè¯·æ±‚æ–‡ä»¶ kube-proxy-csr.jsonï¼š\ncat \u003c\u003c EOF \u003e kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 7.2. æ‰§è¡Œå‘½ä»¤ $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy è¾“å‡ºå¦‚ä¸‹ï¼š\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 2019/12/13 19:37:48 [INFO] generate received request 2019/12/13 19:37:48 [INFO] received CSR 2019/12/13 19:37:48 [INFO] generating key: rsa-2048 2019/12/13 19:37:48 [INFO] encoded CSR 2019/12/13 19:37:48 [INFO] signed certificate with serial number 526712749765692443642491255093816136154324531741 2019/12/13 19:37:48 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). ç”Ÿæˆæ–‡ä»¶ï¼š\n# ç”Ÿæˆæ–‡ä»¶ -rw-r--r-- 1 root root 1009 12æœˆ 13 19:37 kube-proxy.csr -rw------- 1 root root 1675 12æœˆ 13 19:37 kube-proxy-key.pem -rw-r--r-- 1 root root 1407 12æœˆ 13 19:37 kube-proxy.pem # é…ç½®æºæ–‡ä»¶ -rw-r--r-- 1 root root 230 12æœˆ 13 19:37 kube-proxy-csr.json 8. æ ¡éªŒè¯ä¹¦ openssl x509 -noout -text -in kubernetes.pem è¾“å‡ºå¦‚ä¸‹ï¼š\n# openssl x509 -noout -text -in kubernetes.pem Certificate: Data: Version: 3 (0x2) Serial Number: 44:cd:8c:e6:a4:60:ff:3f:09:af:02:e7:68:5e:f2:0f:e6:a0:39:fe Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=ShenZhen, L=ShenZhen, O=k8s, OU=System, CN=kubernetes Validity Not Before: Dec 13 06:35:00 2019 GMT Not After : Nov 19 06:35:00 2119 GMT Subject: C=CN, ST=ShenZhen, L=ShenZhen, O=k8s, OU=System, CN=kubernetes Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:d7:91:4f:90:56:fb:ab:a9:de:c4:98:9e:d7:e6: 45:db:5a:14:9a:76:78:6a:4c:db:3c:47:3c:e7:1c: 3c:37:4e:8a:cf:9c:a1:8a:4c:51:4c:cd:45:b0:03: 87:06:b9:20:2c:3a:51:f9:21:55:1c:90:7c:f8:93: bc:6a:48:05:3d:8b:74:fd:f2:f1:e6:5e:ad:b4:a8: f6:6d:f9:63:9e:e4:b4:cc:68:9e:90:d7:ef:de:ce: c1:1d:1b:68:59:68:5e:5f:7d:5c:f3:49:4f:18:72: be:b5:c8:af:e2:8d:34:9c:d2:68:b7:8c:26:69:cc: a5:f4:ca:69:2d:d7:21:f5:19:2e:b2:b5:97:16:87: 9f:9c:fd:01:97:c2:0e:20:b4:88:27:9a:37:9a:af: 0a:cf:82:4f:26:24:cb:07:ac:8c:b1:34:20:42:22: 00:b2:b0:98:c5:53:01:fb:32:aa:15:1b:7e:39:44: ae:af:6e:c3:65:96:f6:38:7a:87:37:d0:31:63:d8: a4:15:13:f2:56:da:e6:09:45:2b:46:2c:cb:63:db: f7:ba:7f:44:0a:36:39:7c:cc:5b:42:e5:56:c7:7f: dd:64:5c:f2:4a:af:d3:a9:d1:6e:06:27:57:09:4d: db:08:62:87:66:c8:2c:36:00:41:f1:90:f6:5f:68: 20:3d Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 3D:3F:FA:B8:36:D7:FE:B1:59:BE:B1:F5:C1:5D:88:3D:BC:78:9F:87 X509v3 Authority Key Identifier: keyid:40:A2:D4:30:22:12:2E:C2:FB:A2:55:2C:CB:F0:F6:3E:4D:B8:02:03 X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.20.0.112, IP Address:172.20.0.113, IP Address:172.20.0.114, IP Address:172.20.0.115, IP Address:10.254.0.1 Signature Algorithm: sha256WithRSAEncryption 63:50:f6:2a:03:c7:35:dd:e9:10:8d:2f:b3:27:9a:64:f3:e1: 11:8a:18:1e:fa:6d:85:30:11:b4:59:a3:6c:86:cd:2b:5c:67: 17:4f:aa:0d:bb:4c:ee:c8:af:e7:3d:61:6d:03:9d:14:6f:00: 48:56:59:b5:76:13:a9:30:23:e0:b2:d2:12:64:0c:60:0d:76: ec:c6:4f:b1:bc:24:01:7a:48:c6:fd:9e:5d:68:da:b9:a1:ad: 30:7a:ba:90:e2:e3:4e:b4:92:1b:c5:f2:8c:c1:b0:3d:fc:14: d2:46:e8:f8:22:8f:d9:4d:85:4f:58:6b:0f:84:78:06:b4:b9: 92:b9:0d:bd:1d:95:e9:0d:42:d3:fd:dd:2a:59:60:3f:63:35: eb:07:25:d2:ea:0d:19:a6:f3:dc:92:8e:ee:73:04:15:5e:97: e8:da:51:c3:69:49:96:36:c7:cc:5b:e5:e5:cb:e5:ce:9f:21: 6f:6b:56:16:bf:85:ad:1c:8c:91:c1:91:0a:90:18:e2:4a:b0: 32:58:33:ef:55:8e:8f:4a:e3:0f:b8:f7:41:04:65:89:e1:1b: d8:41:28:6e:84:c3:1c:8e:a9:a0:8a:42:e4:fe:d7:fe:0e:24: dc:74:37:fa:5e:be:20:69:c5:9a:5a:e6:83:1c:0b:9e:e1:43: ef:4f:7a:37 å­—æ®µè¯´æ˜ï¼š\nç¡®è®¤ Issuer å­—æ®µçš„å†…å®¹å’Œ ca-csr.json ä¸€è‡´ï¼› ç¡®è®¤ Subject å­—æ®µçš„å†…å®¹å’Œ kubernetes-csr.json ä¸€è‡´ï¼› ç¡®è®¤ X509v3 Subject Alternative Name å­—æ®µçš„å†…å®¹å’Œ kubernetes-csr.json ä¸€è‡´ï¼› ç¡®è®¤ X509v3 Key Usageã€Extended Key Usage å­—æ®µçš„å†…å®¹å’Œ ca-config.json ä¸­ kubernetes profile ä¸€è‡´ï¼› 9. åˆ†å‘è¯ä¹¦ å°†ç”Ÿæˆçš„è¯ä¹¦å’Œç§˜é’¥æ–‡ä»¶ï¼ˆåç¼€åä¸º.pemï¼‰æ‹·è´åˆ°æ‰€æœ‰æœºå™¨çš„ /etc/kubernetes/ssl ç›®å½•ä¸‹ã€‚\nmkdir -p /etc/kubernetes/ssl cp *.pem /etc/kubernetes/ssl å‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/cluster-administration/certificates/ https://coreos.com/os/docs/latest/generate-self-signed-certificates.html https://jimmysong.io/kubernetes-handbook/practice/create-tls-and-secret-key.html ","categories":"","description":"","excerpt":"1. è¯ä¹¦åˆ†ç±» æœåŠ¡å™¨è¯ä¹¦ï¼šserver certï¼Œç”¨äºå®¢æˆ·ç«¯éªŒè¯æœåŠ¡ç«¯çš„èº«ä»½ã€‚\nå®¢æˆ·ç«¯è¯ä¹¦ï¼šclient certï¼Œç”¨äºæœåŠ¡ç«¯éªŒè¯å®¢æˆ·ç«¯çš„ â€¦","ref":"/kubernetes-notes/setup/k8s-cert/","tags":["Kubernetes"],"title":"k8sè¯ä¹¦åŠç§˜é’¥"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/multi-cluster/karmada/","tags":"","title":"Karmada"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kube-controller-manager/","tags":"","title":"kube-controller-manager"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æ https://github.com/kubernetes/kubernetes/tree/v1.12.0/pkg/kubelet éƒ¨åˆ†çš„ä»£ç ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækubeletä¸­çš„NewMainKubeletéƒ¨åˆ†ã€‚\n1. NewMainKubelet NewMainKubeletä¸»è¦ç”¨æ¥åˆå§‹åŒ–å’Œæ„é€ ä¸€ä¸ªkubeletç»“æ„ä½“ï¼Œkubeletç»“æ„ä½“å®šä¹‰å‚è€ƒ:https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go#L888\n// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, containerRuntime string, runtimeCgroups string, hostnameOverride string, nodeIP string, providerID string, cloudProvider string, certDirectory string, rootDirectory string, registerNode bool, registerWithTaints []api.Taint, allowedUnsafeSysctls []string, remoteRuntimeEndpoint string, remoteImageEndpoint string, experimentalMounterPath string, experimentalKernelMemcgNotification bool, experimentalCheckNodeCapabilitiesBeforeMount bool, experimentalNodeAllocatableIgnoreEvictionThreshold bool, minimumGCAge metav1.Duration, maxPerPodContainerCount int32, maxContainerCount int32, masterServiceNamespace string, registerSchedulable bool, nonMasqueradeCIDR string, keepTerminatedPodVolumes bool, nodeLabels map[string]string, seccompProfileRoot string, bootstrapCheckpointPath string, nodeStatusMaxImages int32) (*Kubelet, error) { ... } 1.1. PodConfig é€šè¿‡makePodSourceConfigç”ŸæˆPod configã€‚\nif kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil { return nil, err } } 1.1.1. makePodSourceConfig // makePodSourceConfig creates a config.PodConfig from the given // KubeletConfiguration or returns an error. func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, bootstrapCheckpointPath string) (*config.PodConfig, error) { ... // source of all configuration cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder) // define file config source if kubeCfg.StaticPodPath != \"\" { glog.Infof(\"Adding pod path: %v\", kubeCfg.StaticPodPath) config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource)) } // define url config source if kubeCfg.StaticPodURL != \"\" { glog.Infof(\"Adding pod url %q with HTTP header %v\", kubeCfg.StaticPodURL, manifestURLHeader) config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(kubetypes.HTTPSource)) } // Restore from the checkpoint path // NOTE: This MUST happen before creating the apiserver source // below, or the checkpoint would override the source of truth. ... if kubeDeps.KubeClient != nil { glog.Infof(\"Watching apiserver\") if updatechannel == nil { updatechannel = cfg.Channel(kubetypes.ApiserverSource) } config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, updatechannel) } return cfg, nil } 1.1.2. NewPodConfig // NewPodConfig creates an object that can merge many configuration sources into a stream // of normalized updates to a pod configuration. func NewPodConfig(mode PodConfigNotificationMode, recorder record.EventRecorder) *PodConfig { updates := make(chan kubetypes.PodUpdate, 50) storage := newPodStorage(updates, mode, recorder) podConfig := \u0026PodConfig{ pods: storage, mux: config.NewMux(storage), updates: updates, sources: sets.String{}, } return podConfig } 1.1.3. NewSourceApiserver // NewSourceApiserver creates a config source that watches and pulls from the apiserver. func NewSourceApiserver(c clientset.Interface, nodeName types.NodeName, updates chan\u003c- interface{}) { lw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), \"pods\", metav1.NamespaceAll, fields.OneTermEqualSelector(api.PodHostField, string(nodeName))) newSourceApiserverFromLW(lw, updates) } 1.2. Lister serviceListerå’ŒnodeListeråˆ†åˆ«é€šè¿‡List-Watchæœºåˆ¶ç›‘å¬serviceå’Œnodeçš„åˆ—è¡¨å˜åŒ–ã€‚\n1.2.1. serviceLister serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}) if kubeDeps.KubeClient != nil { serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything()) r := cache.NewReflector(serviceLW, \u0026v1.Service{}, serviceIndexer, 0) go r.Run(wait.NeverStop) } serviceLister := corelisters.NewServiceLister(serviceIndexer) 1.2.2. nodeLister nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{}) if kubeDeps.KubeClient != nil { fieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector() nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector) r := cache.NewReflector(nodeLW, \u0026v1.Node{}, nodeIndexer, 0) go r.Run(wait.NeverStop) } nodeInfo := \u0026predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)} 1.3. å„ç§Manager 1.3.1. containerRefManager containerRefManager := kubecontainer.NewRefManager() 1.3.2. oomWatcher oomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder) 1.3.3. dnsConfigurer clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS)) for _, ipEntry := range kubeCfg.ClusterDNS { ip := net.ParseIP(ipEntry) if ip == nil { glog.Warningf(\"Invalid clusterDNS ip '%q'\", ipEntry) } else { clusterDNS = append(clusterDNS, ip) } } ... dns.NewConfigurer(kubeDeps.Recorder, nodeRef, parsedNodeIP, clusterDNS, kubeCfg.ClusterDomain, kubeCfg.ResolverConfig), 1.3.4. secretManager \u0026 configMapManager var secretManager secret.Manager var configMapManager configmap.Manager switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy { case kubeletconfiginternal.WatchChangeDetectionStrategy: secretManager = secret.NewWatchingSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewWatchingConfigMapManager(kubeDeps.KubeClient) case kubeletconfiginternal.TTLCacheChangeDetectionStrategy: secretManager = secret.NewCachingSecretManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) configMapManager = configmap.NewCachingConfigMapManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) case kubeletconfiginternal.GetChangeDetectionStrategy: secretManager = secret.NewSimpleSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewSimpleConfigMapManager(kubeDeps.KubeClient) default: return nil, fmt.Errorf(\"unknown configmap and secret manager mode: %v\", kubeCfg.ConfigMapAndSecretChangeDetectionStrategy) } klet.secretManager = secretManager klet.configMapManager = configMapManager 1.3.5. livenessManager klet.livenessManager = proberesults.NewManager() 1.3.6. podManager // podManager is also responsible for keeping secretManager and configMapManager contents up-to-date. klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager) 1.3.7. resourceAnalyzer klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration) 1.3.8. containerGC // setup containerGC containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) if err != nil { return nil, err } klet.containerGC = containerGC klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, integer.IntMax(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod)) 1.3.9. imageManager // setup imageManager imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage) if err != nil { return nil, fmt.Errorf(\"failed to initialize image manager: %v\", err) } klet.imageManager = imageManager 1.3.10. statusManager klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) 1.3.11. probeManager klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.runner, containerRefManager, kubeDeps.Recorder) 1.3.12. tokenManager tokenManager := token.NewManager(kubeDeps.KubeClient) 1.3.13. volumePluginMgr klet.volumePluginMgr, err = NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber) if err != nil { return nil, err } if klet.enablePluginsWatcher { klet.pluginWatcher = pluginwatcher.NewWatcher(klet.getPluginsDir()) } 1.3.14. volumeManager // setup volumeManager klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.statusManager, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, klet.getPodsDir(), kubeDeps.Recorder, experimentalCheckNodeCapabilitiesBeforeMount, keepTerminatedPodVolumes) 1.3.15. evictionManager // setup eviction manager evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock) klet.evictionManager = evictionManager klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler) 1.4. containerRuntime ç›®å‰podæ‰€ä½¿ç”¨çš„runtimeåªæœ‰dockerå’Œremoteä¸¤ç§ï¼Œrktå·²ç»åºŸå¼ƒã€‚\nif containerRuntime == \"rkt\" { glog.Fatalln(\"rktnetes has been deprecated in favor of rktlet. Please see https://github.com/kubernetes-incubator/rktlet for more information.\") } å½“runtimeæ˜¯dockerçš„æ—¶å€™ï¼Œä¼šæ‰§è¡Œdockerç›¸å…³æ“ä½œã€‚\nswitch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. ... // The unix socket for kubelet \u003c-\u003e dockershim communication. ... // Create dockerLegacyService when the logging driver is not supported. ... case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\"unsupported CRI runtime: %q\", containerRuntime) } 1.4.1. NewDockerService // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil { return nil, err } if crOptions.RedirectContainerStreaming { klet.criHandler = ds } 1.4.2. NewDockerServer // The unix socket for kubelet \u003c-\u003e dockershim communication. glog.V(5).Infof(\"RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q\", remoteRuntimeEndpoint, remoteImageEndpoint) glog.V(2).Infof(\"Starting the GRPC server for the docker CRI shim.\") server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } 1.4.3. DockerServer.Start // Start starts the dockershim grpc server. func (s *DockerServer) Start() error { // Start the internal service. if err := s.service.Start(); err != nil { glog.Errorf(\"Unable to start docker service\") return err } glog.V(2).Infof(\"Start dockershim grpc server\") l, err := util.CreateListener(s.endpoint) if err != nil { return fmt.Errorf(\"failed to listen on %q: %v\", s.endpoint, err) } // Create the grpc server and register runtime and image services. s.server = grpc.NewServer( grpc.MaxRecvMsgSize(maxMsgSize), grpc.MaxSendMsgSize(maxMsgSize), ) runtimeapi.RegisterRuntimeServiceServer(s.server, s.service) runtimeapi.RegisterImageServiceServer(s.server, s.service) go func() { if err := s.server.Serve(l); err != nil { glog.Fatalf(\"Failed to serve connections: %v\", err) } }() return nil } 1.5. podWorker æ„é€ podWorkerså’ŒworkQueueã€‚\nklet.workQueue = queue.NewBasicWorkQueue(klet.clock) klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) 1.5.1. PodWorkersæ¥å£ // PodWorkers is an abstract interface for testability. type PodWorkers interface { UpdatePod(options *UpdatePodOptions) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) ForgetWorker(uid types.UID) } podWorkerä¸»è¦ç”¨æ¥å¯¹podç›¸åº”äº‹ä»¶è¿›è¡Œå¤„ç†å’ŒåŒæ­¥ï¼ŒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ–¹æ³•ï¼šUpdatePodã€ForgetNonExistingPodWorkersã€ForgetWorkerã€‚\n2. æ€»ç»“ NewMainKubeletä¸»è¦ç”¨æ¥æ„é€ kubeletç»“æ„ä½“ï¼Œå…¶ä¸­kubeleté™¤äº†åŒ…å«å¿…è¦çš„é…ç½®å’Œclientï¼ˆä¾‹å¦‚ï¼škubeClientã€csiClientç­‰ï¼‰å¤–ï¼Œæœ€ä¸»è¦çš„åŒ…å«å„ç§manageræ¥ç®¡ç†ä¸åŒçš„ä»»åŠ¡ã€‚\næ ¸å¿ƒçš„manageræœ‰ä»¥ä¸‹å‡ ç§ï¼š\noomWatcherï¼šç›‘æ§podå†…å­˜æ˜¯å¦å‘ç”ŸOOMã€‚ podManagerï¼šç®¡ç†podçš„ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬å¯¹podçš„å¢åˆ æ”¹æŸ¥æ“ä½œç­‰ã€‚ containerGCï¼šå¯¹æ­»äº¡å®¹å™¨è¿›è¡Œåƒåœ¾å›æ”¶ã€‚ imageManagerï¼šå¯¹å®¹å™¨é•œåƒè¿›è¡Œåƒåœ¾å›æ”¶ã€‚ statusManagerï¼šä¸apiserveråŒæ­¥podçŠ¶æ€ï¼ŒåŒæ—¶ä¹Ÿä½œçŠ¶æ€ç¼“å­˜ã€‚ volumeManagerï¼šå¯¹podçš„volumeè¿›è¡Œattached/detached/mounted/unmountedæ“ä½œã€‚ evictionManagerï¼šä¿è¯èŠ‚ç‚¹ç¨³å®šï¼Œå¿…è¦æ—¶å¯¹podè¿›è¡Œé©±é€ï¼ˆä¾‹å¦‚èµ„æºä¸è¶³çš„æƒ…å†µä¸‹ï¼‰ã€‚ NewMainKubeletè¿˜åŒ…å«äº†serviceListerå’ŒnodeListeræ¥ç›‘å¬serviceå’Œnodeçš„åˆ—è¡¨å˜åŒ–ã€‚\nkubeletä½¿ç”¨åˆ°çš„containerRuntimeç›®å‰ä¸»è¦æ˜¯dockerï¼Œå…¶ä¸­rktå·²åºŸå¼ƒã€‚NewMainKubeletå¯åŠ¨äº†dockershim grpc serveræ¥æ‰§è¡Œdockerç›¸å…³æ“ä½œã€‚\næ„å»ºäº†podWorkeræ¥å¯¹podç›¸å…³çš„æ›´æ–°é€»è¾‘è¿›è¡Œå¤„ç†ã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes/kubernetes/tree/v1.12.0 https://github.com/kubernetes/kubernetes/tree/v1.12.0/pkg/kubelet ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚ â€¦","ref":"/k8s-source-code-analysis/kubelet/newmainkubelet/","tags":["æºç åˆ†æ"],"title":"kubeletæºç åˆ†æï¼ˆäºŒï¼‰ä¹‹ NewMainKubelet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/openyurt/code-analysis/","tags":"","title":"OpenYurtæºç åˆ†æ"},{"body":"1. PVCæ¦‚è¿° PersistentVolumeClaimï¼ˆç®€ç§°PVCï¼‰æ˜¯ç”¨æˆ·å­˜å‚¨çš„è¯·æ±‚ï¼ŒPVCæ¶ˆè€—PVçš„èµ„æºï¼Œå¯ä»¥è¯·æ±‚ç‰¹å®šçš„å¤§å°å’Œè®¿é—®æ¨¡å¼ï¼Œéœ€è¦æŒ‡å®šå½’å±äºæŸä¸ªNamespaceï¼Œåœ¨åŒä¸€ä¸ªNamespaceçš„Podæ‰å¯ä»¥æŒ‡å®šå¯¹åº”çš„PVCã€‚\nå½“éœ€è¦ä¸åŒæ€§è´¨çš„PVæ¥æ»¡è¶³å­˜å‚¨éœ€æ±‚æ—¶ï¼Œå¯ä»¥ä½¿ç”¨StorageClassæ¥å®ç°ã€‚\næ¯ä¸ª PVC ä¸­éƒ½åŒ…å«ä¸€ä¸ª spec è§„æ ¼å­—æ®µå’Œä¸€ä¸ª status å£°æ˜çŠ¶æ€å­—æ®µã€‚\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} 2. PVCçš„å±æ€§ 2.1. accessModes å¯¹åº”å­˜å‚¨çš„è®¿é—®æ¨¡å¼ï¼Œä¾‹å¦‚ï¼šReadWriteOnceã€‚\n2.2. volumeMode å¯¹åº”å­˜å‚¨çš„æ•°æ®å·æ¨¡å¼ï¼Œä¾‹å¦‚ï¼šFilesystemã€‚\n2.3. resources å£°æ˜å¯ä»¥è¯·æ±‚ç‰¹å®šæ•°é‡çš„èµ„æºã€‚ç›¸åŒçš„èµ„æºæ¨¡å‹é€‚ç”¨äºVolumeå’ŒPVCã€‚\n2.4. selector å£°æ˜label selectorï¼Œåªæœ‰æ ‡ç­¾ä¸é€‰æ‹©å™¨åŒ¹é…çš„å·å¯ä»¥ç»‘å®šåˆ°å£°æ˜ã€‚\nmatchLabelsï¼švolume å¿…é¡»æœ‰å…·æœ‰è¯¥å€¼çš„æ ‡ç­¾ matchExpressionsï¼šæ¡ä»¶åˆ—è¡¨ï¼Œé€šè¿‡æ¡ä»¶è¡¨è¾¾å¼ç­›é€‰åŒ¹é…çš„å·ã€‚æœ‰æ•ˆçš„è¿ç®—ç¬¦åŒ…æ‹¬ Inã€NotInã€Exists å’Œ DoesNotExistã€‚ 2.5. storageClassName é€šè¿‡storageClassNameå‚æ•°æ¥æŒ‡å®šä½¿ç”¨å¯¹åº”åå­—çš„StorageClassï¼Œåªæœ‰æ‰€è¯·æ±‚çš„ç±»ä¸ PVC å…·æœ‰ç›¸åŒÂ storageClassNameÂ çš„ PV æ‰èƒ½ç»‘å®šåˆ° PVCã€‚\nPVCå¯ä»¥ä¸æŒ‡å®šstorageClassNameï¼Œæˆ–è€…å°†è¯¥å€¼è®¾ç½®ä¸ºç©ºï¼Œå¦‚æœæ‰“å¼€äº†å‡†å…¥æ§åˆ¶æ’ä»¶ï¼Œå¹¶ä¸”æŒ‡å®šä¸€ä¸ªé»˜è®¤çš„Â StorageClassï¼Œåˆ™PVCä¼šä½¿ç”¨é»˜è®¤çš„StorageClassï¼Œå¦åˆ™å°±ç»‘å®šåˆ°æ²¡æœ‰StorageClassçš„ PVä¸Šã€‚\nä¹‹å‰ä½¿ç”¨æ³¨è§£ volume.beta.kubernetes.io/storage-class è€Œä¸æ˜¯ storageClassName å±æ€§ã€‚è¿™ä¸ªæ³¨è§£ä»ç„¶æœ‰æ•ˆï¼Œä½†æ˜¯åœ¨æœªæ¥çš„ Kubernetes ç‰ˆæœ¬ä¸­ä¸ä¼šæ”¯æŒã€‚\n3. å°†PVCä½œä¸ºVolume å°†PVCä½œä¸ºPodçš„Volumeï¼ŒPVCä¸Podéœ€è¦åœ¨åŒä¸€ä¸ªå‘½åç©ºé—´ä¸‹ï¼Œå…¶å®Podçš„å£°æ˜å¦‚ä¸‹ï¼š\nkind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \"/var/www/html\" name: mypd volumes: - name: mypd persistentVolumeClaim: # ä½¿ç”¨PVC claimName: myclaim PersistentVolumes ç»‘å®šæ˜¯å”¯ä¸€çš„ï¼Œå¹¶ä¸”ç”±äº PersistentVolumeClaims æ˜¯å‘½åç©ºé—´å¯¹è±¡ï¼Œå› æ­¤åªèƒ½åœ¨ä¸€ä¸ªå‘½åç©ºé—´å†…æŒ‚è½½å…·æœ‰â€œå¤šä¸ªâ€æ¨¡å¼ï¼ˆROXã€RWXï¼‰çš„PVCã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/ ","categories":"","description":"","excerpt":"1. PVCæ¦‚è¿° PersistentVolumeClaimï¼ˆç®€ç§°PVCï¼‰æ˜¯ç”¨æˆ·å­˜å‚¨çš„è¯·æ±‚ï¼ŒPVCæ¶ˆè€—PVçš„èµ„æºï¼Œå¯ä»¥è¯·æ±‚ç‰¹å®šçš„å¤§å°å’Œè®¿é—®æ¨¡ â€¦","ref":"/kubernetes-notes/storage/volume/persistent-volume-claim/","tags":["Kubernetes"],"title":"PersistentVolumeClaim ä»‹ç»"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/pod/","tags":"","title":"Podå¯¹è±¡"},{"body":"1. éƒ¨ç½²Redisé›†ç¾¤ redisçš„å®‰è£…åŠé…ç½®å‚è€ƒ[rediséƒ¨ç½²]\næœ¬æ–‡ä»¥åˆ›å»ºä¸€ä¸»äºŒä»çš„é›†ç¾¤ä¸ºä¾‹ã€‚\n1.1 éƒ¨ç½²ä¸é…ç½® å…ˆåˆ›å»ºsentinelç›®å½•ï¼Œåœ¨è¯¥ç›®å½•ä¸‹åˆ›å»º8000ï¼Œ8001ï¼Œ8002ä¸‰ä¸ªä»¥ç«¯å£å·å‘½åçš„ç›®å½•ã€‚\nmkdir sentinel cd sentinel mkdir 8000 8001 8002 åœ¨å¯¹åº”ç«¯å£å·ç›®å½•ä¸­åˆ›å»ºredis.confçš„æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­çš„ç«¯å£å·portå‚æ•°æ”¹ä¸ºå¯¹åº”ç›®å½•çš„ç«¯å£å·ã€‚é…ç½®å¦‚ä¸‹ï¼š\n# å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼ daemonize yes # pid file pidfile /var/run/redis.pid # ç›‘å¬ç«¯å£ port 8000 # TCPæ¥æ”¶é˜Ÿåˆ—é•¿åº¦ï¼Œå—/proc/sys/net/core/somaxconnå’Œtcp_max_syn_backlogè¿™ä¸¤ä¸ªå†…æ ¸å‚æ•°çš„å½±å“ tcp-backlog 511 # ä¸€ä¸ªå®¢æˆ·ç«¯ç©ºé—²å¤šå°‘ç§’åå…³é—­è¿æ¥(0ä»£è¡¨ç¦ç”¨ï¼Œæ°¸ä¸å…³é—­) timeout 0 # å¦‚æœéé›¶ï¼Œåˆ™è®¾ç½®SO_KEEPALIVEé€‰é¡¹æ¥å‘ç©ºé—²è¿æ¥çš„å®¢æˆ·ç«¯å‘é€ACK tcp-keepalive 60 # æŒ‡å®šæœåŠ¡å™¨è°ƒè¯•ç­‰çº§ # å¯èƒ½å€¼ï¼š # debug ï¼ˆå¤§é‡ä¿¡æ¯ï¼Œå¯¹å¼€å‘/æµ‹è¯•æœ‰ç”¨ï¼‰ # verbose ï¼ˆå¾ˆå¤šç²¾ç®€çš„æœ‰ç”¨ä¿¡æ¯ï¼Œä½†æ˜¯ä¸åƒdebugç­‰çº§é‚£ä¹ˆå¤šï¼‰ # notice ï¼ˆé€‚é‡çš„ä¿¡æ¯ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä½ ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦çš„ï¼‰ # warning ï¼ˆåªæœ‰å¾ˆé‡è¦/ä¸¥é‡çš„ä¿¡æ¯ä¼šè®°å½•ä¸‹æ¥ï¼‰ loglevel notice # æŒ‡æ˜æ—¥å¿—æ–‡ä»¶å logfile \"./redis8000.log\" # è®¾ç½®æ•°æ®åº“ä¸ªæ•° databases 16 # ä¼šåœ¨æŒ‡å®šç§’æ•°å’Œæ•°æ®å˜åŒ–æ¬¡æ•°ä¹‹åæŠŠæ•°æ®åº“å†™åˆ°ç£ç›˜ä¸Š # 900ç§’ï¼ˆ15åˆ†é’Ÿï¼‰ä¹‹åï¼Œä¸”è‡³å°‘1æ¬¡å˜æ›´ # 300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰ä¹‹åï¼Œä¸”è‡³å°‘10æ¬¡å˜æ›´ # 60ç§’ä¹‹åï¼Œä¸”è‡³å°‘10000æ¬¡å˜æ›´ save 900 1 save 300 10 save 60 10000 # é»˜è®¤å¦‚æœå¼€å¯RDBå¿«ç…§(è‡³å°‘ä¸€æ¡saveæŒ‡ä»¤)å¹¶ä¸”æœ€æ–°çš„åå°ä¿å­˜å¤±è´¥ï¼ŒRediså°†ä¼šåœæ­¢æ¥å—å†™æ“ä½œ # è¿™å°†ä½¿ç”¨æˆ·çŸ¥é“æ•°æ®æ²¡æœ‰æ­£ç¡®çš„æŒä¹…åŒ–åˆ°ç¡¬ç›˜ï¼Œå¦åˆ™å¯èƒ½æ²¡äººæ³¨æ„åˆ°å¹¶ä¸”é€ æˆä¸€äº›ç¾éš¾ stop-writes-on-bgsave-error yes # å½“å¯¼å‡ºåˆ° .rdb æ•°æ®åº“æ—¶æ˜¯å¦ç”¨LZFå‹ç¼©å­—ç¬¦ä¸²å¯¹è±¡ rdbcompression yes # ç‰ˆæœ¬5çš„RDBæœ‰ä¸€ä¸ªCRC64ç®—æ³•çš„æ ¡éªŒå’Œæ”¾åœ¨äº†æ–‡ä»¶çš„æœ€åã€‚è¿™å°†ä½¿æ–‡ä»¶æ ¼å¼æ›´åŠ å¯é ã€‚ rdbchecksum yes # æŒä¹…åŒ–æ•°æ®åº“çš„æ–‡ä»¶å dbfilename dump.rdb # å·¥ä½œç›®å½• dir ./ # å½“masteræœåŠ¡è®¾ç½®äº†å¯†ç ä¿æŠ¤æ—¶ï¼ŒslaveæœåŠ¡è¿æ¥masterçš„å¯†ç  masterauth 0234kz9*l # å½“ä¸€ä¸ªslaveå¤±å»å’Œmasterçš„è¿æ¥ï¼Œæˆ–è€…åŒæ­¥æ­£åœ¨è¿›è¡Œä¸­ï¼Œslaveçš„è¡Œä¸ºå¯ä»¥æœ‰ä¸¤ç§ï¼š # # 1) å¦‚æœ slave-serve-stale-data è®¾ç½®ä¸º \"yes\" (é»˜è®¤å€¼)ï¼Œslaveä¼šç»§ç»­å“åº”å®¢æˆ·ç«¯è¯·æ±‚ï¼Œ # å¯èƒ½æ˜¯æ­£å¸¸æ•°æ®ï¼Œæˆ–è€…æ˜¯è¿‡æ—¶äº†çš„æ•°æ®ï¼Œä¹Ÿå¯èƒ½æ˜¯è¿˜æ²¡è·å¾—å€¼çš„ç©ºæ•°æ®ã€‚ # 2) å¦‚æœ slave-serve-stale-data è®¾ç½®ä¸º \"no\"ï¼Œslaveä¼šå›å¤\"æ­£åœ¨ä»masteråŒæ­¥ # ï¼ˆSYNC with master in progressï¼‰\"æ¥å¤„ç†å„ç§è¯·æ±‚ï¼Œé™¤äº† INFO å’Œ SLAVEOF å‘½ä»¤ã€‚ slave-serve-stale-data yes # ä½ å¯ä»¥é…ç½®salveå®ä¾‹æ˜¯å¦æ¥å—å†™æ“ä½œã€‚å¯å†™çš„slaveå®ä¾‹å¯èƒ½å¯¹å­˜å‚¨ä¸´æ—¶æ•°æ®æ¯”è¾ƒæœ‰ç”¨(å› ä¸ºå†™å…¥salve # çš„æ•°æ®åœ¨åŒmasteråŒæ­¥ä¹‹åå°†å¾ˆå®¹æ˜“è¢«åˆ é™¤ slave-read-only yes # æ˜¯å¦åœ¨slaveå¥—æ¥å­—å‘é€SYNCä¹‹åç¦ç”¨ TCP_NODELAYï¼Ÿ # å¦‚æœä½ é€‰æ‹©â€œyesâ€Rediså°†ä½¿ç”¨æ›´å°‘çš„TCPåŒ…å’Œå¸¦å®½æ¥å‘slaveså‘é€æ•°æ®ã€‚ä½†æ˜¯è¿™å°†ä½¿æ•°æ®ä¼ è¾“åˆ°slave # ä¸Šæœ‰å»¶è¿Ÿï¼ŒLinuxå†…æ ¸çš„é»˜è®¤é…ç½®ä¼šè¾¾åˆ°40æ¯«ç§’ # å¦‚æœä½ é€‰æ‹©äº† \"no\" æ•°æ®ä¼ è¾“åˆ°salveçš„å»¶è¿Ÿå°†ä¼šå‡å°‘ä½†è¦ä½¿ç”¨æ›´å¤šçš„å¸¦å®½ repl-disable-tcp-nodelay no # slaveçš„ä¼˜å…ˆçº§æ˜¯ä¸€ä¸ªæ•´æ•°å±•ç¤ºåœ¨Redisçš„Infoè¾“å‡ºä¸­ã€‚å¦‚æœmasterä¸å†æ­£å¸¸å·¥ä½œäº†ï¼Œå“¨å…µå°†ç”¨å®ƒæ¥ # é€‰æ‹©ä¸€ä¸ªslaveæå‡=å‡ä¸ºmasterã€‚ # ä¼˜å…ˆçº§æ•°å­—å°çš„salveä¼šä¼˜å…ˆè€ƒè™‘æå‡ä¸ºmasterï¼Œæ‰€ä»¥ä¾‹å¦‚æœ‰ä¸‰ä¸ªslaveä¼˜å…ˆçº§åˆ†åˆ«ä¸º10ï¼Œ100ï¼Œ25ï¼Œ # å“¨å…µå°†æŒ‘é€‰ä¼˜å…ˆçº§æœ€å°æ•°å­—ä¸º10çš„slaveã€‚ # 0ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„ä¼˜å…ˆçº§ï¼Œæ ‡è¯†è¿™ä¸ªslaveä¸èƒ½ä½œä¸ºmasterï¼Œæ‰€ä»¥ä¸€ä¸ªä¼˜å…ˆçº§ä¸º0çš„slaveæ°¸è¿œä¸ä¼šè¢« # å“¨å…µæŒ‘é€‰æå‡ä¸ºmaster slave-priority 100 # å¯†ç éªŒè¯ # è­¦å‘Šï¼šå› ä¸ºRediså¤ªå¿«äº†ï¼Œæ‰€ä»¥å¤–é¢çš„äººå¯ä»¥å°è¯•æ¯ç§’150kçš„å¯†ç æ¥è¯•å›¾ç ´è§£å¯†ç ã€‚è¿™æ„å‘³ç€ä½ éœ€è¦ # ä¸€ä¸ªé«˜å¼ºåº¦çš„å¯†ç ï¼Œå¦åˆ™ç ´è§£å¤ªå®¹æ˜“äº† requirepass 0234kz9*l # rediså®ä¾‹æœ€å¤§å ç”¨å†…å­˜ï¼Œä¸è¦ç”¨æ¯”è®¾ç½®çš„ä¸Šé™æ›´å¤šçš„å†…å­˜ã€‚ä¸€æ—¦å†…å­˜ä½¿ç”¨è¾¾åˆ°ä¸Šé™ï¼ŒRedisä¼šæ ¹æ®é€‰å®šçš„å›æ”¶ç­–ç•¥ï¼ˆå‚è§ï¼š # maxmemmory-policyï¼‰åˆ é™¤key maxmemory 3gb # æœ€å¤§å†…å­˜ç­–ç•¥ï¼šå¦‚æœè¾¾åˆ°å†…å­˜é™åˆ¶äº†ï¼ŒRediså¦‚ä½•é€‰æ‹©åˆ é™¤keyã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢äº”ä¸ªè¡Œä¸ºé‡Œé€‰ï¼š # volatile-lru -\u003e æ ¹æ®LRUç®—æ³•åˆ é™¤å¸¦æœ‰è¿‡æœŸæ—¶é—´çš„keyã€‚ # allkeys-lru -\u003e æ ¹æ®LRUç®—æ³•åˆ é™¤ä»»ä½•keyã€‚ # volatile-random -\u003e æ ¹æ®è¿‡æœŸè®¾ç½®æ¥éšæœºåˆ é™¤key, å…·å¤‡è¿‡æœŸæ—¶é—´çš„keyã€‚ # allkeys-\u003erandom -\u003e æ— å·®åˆ«éšæœºåˆ , ä»»ä½•ä¸€ä¸ªkeyã€‚ # volatile-ttl -\u003e æ ¹æ®æœ€è¿‘è¿‡æœŸæ—¶é—´æ¥åˆ é™¤ï¼ˆè¾…ä»¥TTLï¼‰, è¿™æ˜¯å¯¹äºæœ‰è¿‡æœŸæ—¶é—´çš„key # noeviction -\u003e è°ä¹Ÿä¸åˆ ï¼Œç›´æ¥åœ¨å†™æ“ä½œæ—¶è¿”å›é”™è¯¯ã€‚ maxmemory-policy volatile-lru # é»˜è®¤æƒ…å†µä¸‹ï¼ŒRedisæ˜¯å¼‚æ­¥çš„æŠŠæ•°æ®å¯¼å‡ºåˆ°ç£ç›˜ä¸Šã€‚è¿™ç§æ¨¡å¼åœ¨å¾ˆå¤šåº”ç”¨é‡Œå·²ç»è¶³å¤Ÿå¥½ï¼Œä½†Redisè¿›ç¨‹ # å‡ºé—®é¢˜æˆ–æ–­ç”µæ—¶å¯èƒ½é€ æˆä¸€æ®µæ—¶é—´çš„å†™æ“ä½œä¸¢å¤±(è¿™å–å†³äºé…ç½®çš„saveæŒ‡ä»¤)ã€‚ # # AOFæ˜¯ä¸€ç§æä¾›äº†æ›´å¯é çš„æ›¿ä»£æŒä¹…åŒ–æ¨¡å¼ï¼Œä¾‹å¦‚ä½¿ç”¨é»˜è®¤çš„æ•°æ®å†™å…¥æ–‡ä»¶ç­–ç•¥ï¼ˆå‚è§åé¢çš„é…ç½®ï¼‰ # åœ¨é‡åˆ°åƒæœåŠ¡å™¨æ–­ç”µæˆ–å•å†™æƒ…å†µä¸‹Redisè‡ªèº«è¿›ç¨‹å‡ºé—®é¢˜ä½†æ“ä½œç³»ç»Ÿä»æ­£å¸¸è¿è¡Œç­‰çªå‘äº‹ä»¶æ—¶ï¼ŒRedis # èƒ½åªä¸¢å¤±1ç§’çš„å†™æ“ä½œã€‚ # # AOFå’ŒRDBæŒä¹…åŒ–èƒ½åŒæ—¶å¯åŠ¨å¹¶ä¸”ä¸ä¼šæœ‰é—®é¢˜ã€‚ # å¦‚æœAOFå¼€å¯ï¼Œé‚£ä¹ˆåœ¨å¯åŠ¨æ—¶Rediså°†åŠ è½½AOFæ–‡ä»¶ï¼Œå®ƒæ›´èƒ½ä¿è¯æ•°æ®çš„å¯é æ€§ã€‚ appendonly no # aofæ–‡ä»¶å appendfilename \"appendonly.aof\" # fsync() ç³»ç»Ÿè°ƒç”¨å‘Šè¯‰æ“ä½œç³»ç»ŸæŠŠæ•°æ®å†™åˆ°ç£ç›˜ä¸Šï¼Œè€Œä¸æ˜¯ç­‰æ›´å¤šçš„æ•°æ®è¿›å…¥è¾“å‡ºç¼“å†²åŒºã€‚ # æœ‰äº›æ“ä½œç³»ç»Ÿä¼šçœŸçš„æŠŠæ•°æ®é©¬ä¸Šåˆ·åˆ°ç£ç›˜ä¸Šï¼›æœ‰äº›åˆ™ä¼šå°½å¿«å»å°è¯•è¿™ä¹ˆåšã€‚ # # Redisæ”¯æŒä¸‰ç§ä¸åŒçš„æ¨¡å¼ï¼š # # noï¼šä¸è¦ç«‹åˆ»åˆ·ï¼Œåªæœ‰åœ¨æ“ä½œç³»ç»Ÿéœ€è¦åˆ·çš„æ—¶å€™å†åˆ·ã€‚æ¯”è¾ƒå¿«ã€‚ # alwaysï¼šæ¯æ¬¡å†™æ“ä½œéƒ½ç«‹åˆ»å†™å…¥åˆ°aofæ–‡ä»¶ã€‚æ…¢ï¼Œä½†æ˜¯æœ€å®‰å…¨ã€‚ # everysecï¼šæ¯ç§’å†™ä¸€æ¬¡ã€‚æŠ˜ä¸­æ–¹æ¡ˆã€‚ appendfsync everysec # å¦‚æœAOFçš„åŒæ­¥ç­–ç•¥è®¾ç½®æˆ \"always\" æˆ–è€… \"everysec\"ï¼Œå¹¶ä¸”åå°çš„å­˜å‚¨è¿›ç¨‹ï¼ˆåå°å­˜å‚¨æˆ–å†™å…¥AOF # æ—¥å¿—ï¼‰ä¼šäº§ç”Ÿå¾ˆå¤šç£ç›˜I/Oå¼€é”€ã€‚æŸäº›Linuxçš„é…ç½®ä¸‹ä¼šä½¿Rediså› ä¸º fsync()ç³»ç»Ÿè°ƒç”¨è€Œé˜»å¡å¾ˆä¹…ã€‚ # æ³¨æ„ï¼Œç›®å‰å¯¹è¿™ä¸ªæƒ…å†µè¿˜æ²¡æœ‰å®Œç¾ä¿®æ­£ï¼Œç”šè‡³ä¸åŒçº¿ç¨‹çš„ fsync() ä¼šé˜»å¡æˆ‘ä»¬åŒæ­¥çš„write(2)è°ƒç”¨ã€‚ # # ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥ç”¨ä¸‹é¢è¿™ä¸ªé€‰é¡¹ã€‚å®ƒå¯ä»¥åœ¨ BGSAVE æˆ– BGREWRITEAOF å¤„ç†æ—¶é˜»æ­¢ä¸»è¿›ç¨‹è¿›è¡Œfsync()ã€‚ # # è¿™å°±æ„å‘³ç€å¦‚æœæœ‰å­è¿›ç¨‹åœ¨è¿›è¡Œä¿å­˜æ“ä½œï¼Œé‚£ä¹ˆRediså°±å¤„äº\"ä¸å¯åŒæ­¥\"çš„çŠ¶æ€ã€‚ # è¿™å®é™…ä¸Šæ˜¯è¯´ï¼Œåœ¨æœ€å·®çš„æƒ…å†µä¸‹å¯èƒ½ä¼šä¸¢æ‰30ç§’é’Ÿçš„æ—¥å¿—æ•°æ®ã€‚ï¼ˆé»˜è®¤Linuxè®¾å®šï¼‰ # # å¦‚æœä½ æœ‰å»¶æ—¶é—®é¢˜æŠŠè¿™ä¸ªè®¾ç½®æˆ\"yes\"ï¼Œå¦åˆ™å°±ä¿æŒ\"no\"ï¼Œè¿™æ˜¯ä¿å­˜æŒä¹…æ•°æ®çš„æœ€å®‰å…¨çš„æ–¹å¼ã€‚ no-appendfsync-on-rewrite yes # è‡ªåŠ¨é‡å†™AOFæ–‡ä»¶ auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # AOFæ–‡ä»¶å¯èƒ½åœ¨å°¾éƒ¨æ˜¯ä¸å®Œæ•´çš„ï¼ˆè¿™è·Ÿsystemå…³é—­æœ‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯mount ext4æ–‡ä»¶ç³»ç»Ÿæ—¶ # æ²¡æœ‰åŠ ä¸Šdata=orderedé€‰é¡¹ã€‚åªä¼šå‘ç”Ÿåœ¨osæ­»æ—¶ï¼Œredisè‡ªå·±æ­»ä¸ä¼šä¸å®Œæ•´ï¼‰ã€‚ # é‚£redisé‡å¯æ—¶loadè¿›å†…å­˜çš„æ—¶å€™å°±æœ‰é—®é¢˜äº†ã€‚ # å‘ç”Ÿçš„æ—¶å€™ï¼Œå¯ä»¥é€‰æ‹©rediså¯åŠ¨æŠ¥é”™ï¼Œå¹¶ä¸”é€šçŸ¥ç”¨æˆ·å’Œå†™æ—¥å¿—ï¼Œæˆ–è€…loadå°½é‡å¤šæ­£å¸¸çš„æ•°æ®ã€‚ # å¦‚æœaof-load-truncatedæ˜¯yesï¼Œä¼šè‡ªåŠ¨å‘å¸ƒä¸€ä¸ªlogç»™å®¢æˆ·ç«¯ç„¶åloadï¼ˆé»˜è®¤ï¼‰ã€‚ # å¦‚æœæ˜¯noï¼Œç”¨æˆ·å¿…é¡»æ‰‹åŠ¨redis-check-aofä¿®å¤AOFæ–‡ä»¶æ‰å¯ä»¥ã€‚ # æ³¨æ„ï¼Œå¦‚æœåœ¨è¯»å–çš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°è¿™ä¸ªaofæ˜¯æŸåçš„ï¼ŒæœåŠ¡å™¨ä¹Ÿæ˜¯ä¼šé€€å‡ºçš„ï¼Œ # è¿™ä¸ªé€‰é¡¹ä»…ä»…ç”¨äºå½“æœåŠ¡å™¨å°è¯•è¯»å–æ›´å¤šçš„æ•°æ®ä½†åˆæ‰¾ä¸åˆ°ç›¸åº”çš„æ•°æ®æ—¶ã€‚ aof-load-truncated yes # Lua è„šæœ¬çš„æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼Œæ¯«ç§’ä¸ºå•ä½ lua-time-limit 5000 # Redisæ…¢æŸ¥è¯¢æ—¥å¿—å¯ä»¥è®°å½•è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æŸ¥è¯¢ slowlog-log-slower-than 10000 # è¿™ä¸ªé•¿åº¦æ²¡æœ‰é™åˆ¶ã€‚åªæ˜¯è¦ä¸»è¦ä¼šæ¶ˆè€—å†…å­˜ã€‚ä½ å¯ä»¥é€šè¿‡ SLOWLOG RESET æ¥å›æ”¶å†…å­˜ã€‚ slowlog-max-len 128 # rediså»¶æ—¶ç›‘æ§ç³»ç»Ÿåœ¨è¿è¡Œæ—¶ä¼šé‡‡æ ·ä¸€äº›æ“ä½œï¼Œä»¥ä¾¿æ”¶é›†å¯èƒ½å¯¼è‡´å»¶æ—¶çš„æ•°æ®æ ¹æºã€‚ # é€šè¿‡ LATENCYå‘½ä»¤ å¯ä»¥æ‰“å°ä¸€äº›å›¾æ ·å’Œè·å–ä¸€äº›æŠ¥å‘Šï¼Œæ–¹ä¾¿ç›‘æ§ # è¿™ä¸ªç³»ç»Ÿä»…ä»…è®°å½•é‚£ä¸ªæ‰§è¡Œæ—¶é—´å¤§äºæˆ–ç­‰äºé¢„å®šæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰çš„æ“ä½œ, # è¿™ä¸ªé¢„å®šæ—¶é—´æ˜¯é€šè¿‡latency-monitor-thresholdé…ç½®æ¥æŒ‡å®šçš„ï¼Œ # å½“è®¾ç½®ä¸º0æ—¶ï¼Œè¿™ä¸ªç›‘æ§ç³»ç»Ÿå¤„äºåœæ­¢çŠ¶æ€ latency-monitor-threshold 0 # Redisèƒ½é€šçŸ¥ Pub/Sub å®¢æˆ·ç«¯å…³äºé”®ç©ºé—´å‘ç”Ÿçš„äº‹ä»¶ï¼Œé»˜è®¤å…³é—­ notify-keyspace-events \"\" # å½“hashåªæœ‰å°‘é‡çš„entryæ—¶ï¼Œå¹¶ä¸”æœ€å¤§çš„entryæ‰€å ç©ºé—´æ²¡æœ‰è¶…è¿‡æŒ‡å®šçš„é™åˆ¶æ—¶ï¼Œä¼šç”¨ä¸€ç§èŠ‚çœå†…å­˜çš„ # æ•°æ®ç»“æ„æ¥ç¼–ç ã€‚å¯ä»¥é€šè¿‡ä¸‹é¢çš„æŒ‡ä»¤æ¥è®¾å®šé™åˆ¶ hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # ä¸hashä¼¼ï¼Œæ•°æ®å…ƒç´ è¾ƒå°‘çš„listï¼Œå¯ä»¥ç”¨å¦ä¸€ç§æ–¹å¼æ¥ç¼–ç ä»è€ŒèŠ‚çœå¤§é‡ç©ºé—´ã€‚ # è¿™ç§ç‰¹æ®Šçš„æ–¹å¼åªæœ‰åœ¨ç¬¦åˆä¸‹é¢é™åˆ¶æ—¶æ‰å¯ä»¥ç”¨ list-max-ziplist-entries 512 list-max-ziplist-value 64 # setæœ‰ä¸€ç§ç‰¹æ®Šç¼–ç çš„æƒ…å†µï¼šå½“setæ•°æ®å…¨æ˜¯åè¿›åˆ¶64ä½æœ‰ç¬¦å·æ•´å‹æ•°å­—æ„æˆçš„å­—ç¬¦ä¸²æ—¶ã€‚ # ä¸‹é¢è¿™ä¸ªé…ç½®é¡¹å°±æ˜¯ç”¨æ¥è®¾ç½®setä½¿ç”¨è¿™ç§ç¼–ç æ¥èŠ‚çœå†…å­˜çš„æœ€å¤§é•¿åº¦ã€‚ set-max-intset-entries 512 # ä¸hashå’Œlistç›¸ä¼¼ï¼Œæœ‰åºé›†åˆä¹Ÿå¯ä»¥ç”¨ä¸€ç§ç‰¹åˆ«çš„ç¼–ç æ–¹å¼æ¥èŠ‚çœå¤§é‡ç©ºé—´ã€‚ # è¿™ç§ç¼–ç åªé€‚åˆé•¿åº¦å’Œå…ƒç´ éƒ½å°äºä¸‹é¢é™åˆ¶çš„æœ‰åºé›†åˆ zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLogç¨€ç–ç»“æ„è¡¨ç¤ºå­—èŠ‚çš„é™åˆ¶ã€‚è¯¥é™åˆ¶åŒ…æ‹¬ # 16ä¸ªå­—èŠ‚çš„å¤´ã€‚å½“HyperLogLogä½¿ç”¨ç¨€ç–ç»“æ„è¡¨ç¤º # è¿™äº›é™åˆ¶ï¼Œå®ƒä¼šè¢«è½¬æ¢æˆå¯†åº¦è¡¨ç¤ºã€‚ # å€¼å¤§äº16000æ˜¯å®Œå…¨æ²¡ç”¨çš„ï¼Œå› ä¸ºåœ¨è¯¥ç‚¹ # å¯†é›†çš„è¡¨ç¤ºæ˜¯æ›´å¤šçš„å†…å­˜æ•ˆç‡ã€‚ # å»ºè®®å€¼æ˜¯3000å·¦å³ï¼Œä»¥ä¾¿å…·æœ‰çš„å†…å­˜å¥½å¤„, å‡å°‘å†…å­˜çš„æ¶ˆè€— hll-sparse-max-bytes 3000 # å¯ç”¨å“ˆå¸Œåˆ·æ–°ï¼Œæ¯100ä¸ªCPUæ¯«ç§’ä¼šæ‹¿å‡º1ä¸ªæ¯«ç§’æ¥åˆ·æ–°Redisçš„ä¸»å“ˆå¸Œè¡¨ï¼ˆé¡¶çº§é”®å€¼æ˜ å°„è¡¨ï¼‰ activerehashing yes # å®¢æˆ·ç«¯çš„è¾“å‡ºç¼“å†²åŒºçš„é™åˆ¶ï¼Œå¯ç”¨äºå¼ºåˆ¶æ–­å¼€é‚£äº›å› ä¸ºæŸç§åŸå› ä»æœåŠ¡å™¨è¯»å–æ•°æ®çš„é€Ÿåº¦ä¸å¤Ÿå¿«çš„å®¢æˆ·ç«¯ client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # é»˜è®¤æƒ…å†µä¸‹ï¼Œâ€œhzâ€çš„è¢«è®¾å®šä¸º10ã€‚æé«˜è¯¥å€¼å°†åœ¨Redisç©ºé—²æ—¶ä½¿ç”¨æ›´å¤šçš„CPUæ—¶ï¼Œä½†åŒæ—¶å½“æœ‰å¤šä¸ªkey # åŒæ—¶åˆ°æœŸä¼šä½¿Redisçš„ååº”æ›´çµæ•ï¼Œä»¥åŠè¶…æ—¶å¯ä»¥æ›´ç²¾ç¡®åœ°å¤„ç† hz 10 # å½“ä¸€ä¸ªå­è¿›ç¨‹é‡å†™AOFæ–‡ä»¶æ—¶ï¼Œå¦‚æœå¯ç”¨ä¸‹é¢çš„é€‰é¡¹ï¼Œåˆ™æ–‡ä»¶æ¯ç”Ÿæˆ32Mæ•°æ®ä¼šè¢«åŒæ­¥ aof-rewrite-incremental-fsync yes 1.2 é…ç½®ä¸»ä»å…³ç³» 1ã€å¯åŠ¨å®ä¾‹\nä¸‰ä¸ªRediså®ä¾‹é…ç½®ç›¸åŒï¼Œåˆ†åˆ«å¯åŠ¨ä¸‰ä¸ªRediså®ä¾‹ã€‚å»ºè®®å°†redis-serverã€redis-cliã€redis-sentinelçš„äºŒè¿›åˆ¶å¤åˆ¶åˆ°/usr/local/binçš„ç›®å½•ä¸‹ã€‚\ncd 8000 redis-server redis.conf 2ã€é…ç½®ä¸»ä»å…³ç³»\nä¾‹å¦‚ï¼Œå°†8000ç«¯å£å®ä¾‹è®¾ä¸ºä¸»ï¼Œ8001å’Œ8002ç«¯å£çš„å®ä¾‹è®¾ä¸ºä»ã€‚\nåˆ™åˆ†åˆ«ç™»å½•8001å’Œ8002çš„å®ä¾‹ï¼Œæ‰§è¡Œslaveof \u003cMASTER_IP\u003e \u003cMASTER_PORT\u003eå‘½ä»¤ã€‚\nä¾‹å¦‚ï¼š\n[root@kube-node-1 8000]# redis-cli -c -p 8001 -a 0234kz9*l 127.0.0.1:8001\u003e slaveof 127.0.0.1 8000 OK 3ã€æ£€æŸ¥é›†ç¾¤çŠ¶æ€\nç™»å½•masterå’Œslaveå®ä¾‹ï¼Œæ‰§è¡Œinfo replicationæŸ¥çœ‹é›†ç¾¤çŠ¶æ€ã€‚\nMaster\n[root@kube-node-1 8000]# redis-cli -c -p 8000 -a 0234kz9*l 127.0.0.1:8000\u003e info replication # Replication role:master connected_slaves:2 slave0:ip=127.0.0.1,port=8001,state=online,offset=2853,lag=0 slave1:ip=127.0.0.1,port=8002,state=online,offset=2853,lag=0 master_replid:4f8331d5f180a4669241ab0dd97e43508abd6d8f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:2853 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:2853 Slave\n[root@kube-node-1 8000]# redis-cli -c -p 8001 -a 0234kz9*l 127.0.0.1:8001\u003e info replication # Replication role:slave master_host:127.0.0.1 master_port:8000 master_link_status:up master_last_io_seconds_ago:3 master_sync_in_progress:0 slave_repl_offset:2909 slave_priority:100 slave_read_only:1 connected_slaves:0 master_replid:4f8331d5f180a4669241ab0dd97e43508abd6d8f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:2909 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:2909 ä¹Ÿå¯ä»¥å¾€masterå†™æ•°æ®ï¼Œä»slaveè¯»å–æ•°æ®æ¥éªŒè¯ã€‚\n2. éƒ¨ç½²sentinelé›†ç¾¤ 2.1 éƒ¨ç½²ä¸é…ç½® åœ¨ä¹‹å‰åˆ›å»ºçš„sentinelç›®å½•ä¸­åœºæ™¯sentinelç«¯å£å·å‘½åçš„ç›®å½•28000ï¼Œ28001ï¼Œ28002ã€‚\ncd sentinel mkdir 28000 28001 28002 åœ¨å¯¹åº”ç«¯å£å·ç›®å½•ä¸­åˆ›å»ºredis.confçš„æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­çš„ç«¯å£å·portå‚æ•°æ”¹ä¸ºå¯¹åº”ç›®å½•çš„ç«¯å£å·ã€‚é…ç½®å¦‚ä¸‹ï¼š\nport 28000 sentinel monitor mymaster 127.0.0.1 8000 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 2.2 å¯åŠ¨sentinelå®ä¾‹ #\u0026 è¡¨ç¤ºåå°è¿è¡Œçš„æ–¹å¼ redis-sentinel sentinel.conf \u0026 2.3 æŸ¥çœ‹çŠ¶æ€ ä½¿ç”¨sentinel masterså‘½ä»¤æŸ¥çœ‹ç›‘æ§çš„masterèŠ‚ç‚¹ã€‚\n[root@kube-node-1 28000]# redis-cli -c -p 28000 -a 0234kz9*l 127.0.0.1:28000\u003e 127.0.0.1:28000\u003e ping PONG 127.0.0.1:28000\u003e 127.0.0.1:28000\u003e sentinel masters 1) 1) \"name\" 1) \"mymaster\" 2) \"ip\" 3) \"127.0.0.1\" 4) \"port\" 5) \"8000\" 6) \"runid\" 7) \"\" 8) \"flags\" 1) \"s_down,master,disconnected\" 2) \"link-pending-commands\" 3) \"0\" 4) \"link-refcount\" 5) \"1\" 6) \"last-ping-sent\" 7) \"187539\" 8) \"last-ok-ping-reply\" 9) \"187539\" 10) \"last-ping-reply\" 11) \"3943\" 12) \"s-down-time\" 13) \"127491\" 14) \"down-after-milliseconds\" 15) \"60000\" 16) \"info-refresh\" 17) \"1517346914642\" 18) \"role-reported\" 19) \"master\" 20) \"role-reported-time\" 21) \"187539\" 22) \"config-epoch\" 23) \"0\" 24) \"num-slaves\" 25) \"0\" 26) \"num-other-sentinels\" 27) \"0\" 28) \"quorum\" 29) \"2\" 30) \"failover-timeout\" 31) \"180000\" 32) \"parallel-syncs\" 33) \"1\" å‚è€ƒæ–‡ç« ï¼š\nhttps://redis.io/topics/sentinel\n","categories":"","description":"","excerpt":"1. éƒ¨ç½²Redisé›†ç¾¤ redisçš„å®‰è£…åŠé…ç½®å‚è€ƒ[rediséƒ¨ç½²]\næœ¬æ–‡ä»¥åˆ›å»ºä¸€ä¸»äºŒä»çš„é›†ç¾¤ä¸ºä¾‹ã€‚\n1.1 éƒ¨ç½²ä¸é…ç½® å…ˆåˆ› â€¦","ref":"/linux-notes/redis/redis-sentinel/","tags":["Redis"],"title":"Rediså“¨å…µæ¨¡å¼éƒ¨ç½²"},{"body":"1. shellè¿ç®—ç¬¦ Bash æ”¯æŒå¾ˆå¤šè¿ç®—ç¬¦ï¼ŒåŒ…æ‹¬ç®—æ•°è¿ç®—ç¬¦ã€å…³ç³»è¿ç®—ç¬¦ã€å¸ƒå°”è¿ç®—ç¬¦ã€å­—ç¬¦ä¸²è¿ç®—ç¬¦å’Œæ–‡ä»¶æµ‹è¯•è¿ç®—ç¬¦ã€‚ awk å’Œ exprï¼Œexpr æœ€å¸¸ç”¨\nä¾‹å¦‚ï¼Œä¸¤ä¸ªæ•°ç›¸åŠ ï¼š\n#!/bin/bash val=`expr 2 + 2` echo \"Total value : $val\" è¿è¡Œè„šæœ¬è¾“å‡ºï¼š\nTotal value : 4 ä¸¤ç‚¹æ³¨æ„ï¼š\nè¡¨è¾¾å¼å’Œè¿ç®—ç¬¦ä¹‹é—´è¦æœ‰ç©ºæ ¼ï¼Œä¾‹å¦‚ 2+2 æ˜¯ä¸å¯¹çš„ï¼Œå¿…é¡»å†™æˆ 2 + 2ï¼Œè¿™ä¸æˆ‘ä»¬ç†Ÿæ‚‰çš„å¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€ä¸ä¸€æ ·ã€‚ å®Œæ•´çš„è¡¨è¾¾å¼è¦è¢«``åŒ…å«ï¼Œæ³¨æ„è¿™ä¸ªå­—ç¬¦ä¸æ˜¯å¸¸ç”¨çš„å•å¼•å·ï¼Œåœ¨ Esc é”®ä¸‹è¾¹ã€‚ 1.1. ç®—æœ¯è¿ç®—ç¬¦ ç®—æœ¯è¿ç®—ç¬¦åˆ—è¡¨\nè¿ç®—ç¬¦ è¯´æ˜ ä¸¾ä¾‹ + åŠ æ³• `expr $a + $b` ç»“æœä¸º 30ã€‚ - å‡æ³• `expr $a - $b` ç»“æœä¸º 10ã€‚ * ä¹˜æ³• `expr $a * $b` ç»“æœä¸º 200ã€‚ / é™¤æ³• `expr $b / $a` ç»“æœä¸º 2ã€‚ % å–ä½™ `expr $b % $a` ç»“æœä¸º 0ã€‚ = èµ‹å€¼ a=$b å°†æŠŠå˜é‡ b çš„å€¼èµ‹ç»™ aã€‚ == ç›¸ç­‰ã€‚ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ•°å­—ï¼Œç›¸åŒåˆ™è¿”å› trueã€‚ [ $a == $b ] è¿”å› falseã€‚ != ä¸ç›¸ç­‰ã€‚ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ•°å­—ï¼Œä¸ç›¸åŒåˆ™è¿”å› trueã€‚ [ $a != $b ] è¿”å› trueã€‚ æ³¨æ„ï¼š\nä¹˜å·(*)å‰è¾¹å¿…é¡»åŠ åæ–œæ ()æ‰èƒ½å®ç°ä¹˜æ³•è¿ç®—\næ¡ä»¶è¡¨è¾¾å¼è¦æ”¾åœ¨æ–¹æ‹¬å·ä¹‹é—´ï¼Œå¹¶ä¸”è¦æœ‰ç©ºæ ¼ï¼Œä¾‹å¦‚ [$a==$b] æ˜¯é”™è¯¯çš„ï¼Œå¿…é¡»å†™æˆ [$a == $b ]ã€‚\n1.2. å…³ç³»è¿ç®—ç¬¦ å…³ç³»è¿ç®—ç¬¦åªæ”¯æŒæ•°å­—ï¼Œä¸æ”¯æŒå­—ç¬¦ä¸²ï¼Œé™¤éå­—ç¬¦ä¸²çš„å€¼æ˜¯æ•°å­—ã€‚\nå…³ç³»è¿ç®—ç¬¦åˆ—è¡¨\nè¿ç®—ç¬¦ è¯´æ˜ ä¸¾ä¾‹ -eq æ£€æµ‹ä¸¤ä¸ªæ•°æ˜¯å¦ç›¸ç­‰ï¼Œç›¸ç­‰è¿”å› trueã€‚ [ $a -eq $b ] è¿”å› trueã€‚ -ne æ£€æµ‹ä¸¤ä¸ªæ•°æ˜¯å¦ç›¸ç­‰ï¼Œä¸ç›¸ç­‰è¿”å› trueã€‚ [ $a -ne $b ] è¿”å› trueã€‚ -gt æ£€æµ‹å·¦è¾¹çš„æ•°æ˜¯å¦å¤§äºå³è¾¹çš„ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ $a -gt $b ] è¿”å› falseã€‚ -lt æ£€æµ‹å·¦è¾¹çš„æ•°æ˜¯å¦å°äºå³è¾¹çš„ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ $a -lt $b ] è¿”å› trueã€‚ -ge æ£€æµ‹å·¦è¾¹çš„æ•°æ˜¯å¦å¤§ç­‰äºå³è¾¹çš„ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ $a -ge $b ] è¿”å› falseã€‚ -le æ£€æµ‹å·¦è¾¹çš„æ•°æ˜¯å¦å°äºç­‰äºå³è¾¹çš„ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ $a -le $b ] è¿”å› trueã€‚ eq:equal\nne:not equal\ngt:greater than\nlt:less than\nge:greater or equal\nle:less or equal\n1.3. å¸ƒå°”è¿ç®—ç¬¦ å¸ƒå°”è¿ç®—ç¬¦åˆ—è¡¨\nè¿ç®—ç¬¦ è¯´æ˜ ä¸¾ä¾‹ ! éè¿ç®—ï¼Œè¡¨è¾¾å¼ä¸º true åˆ™è¿”å› falseï¼Œå¦åˆ™è¿”å› trueã€‚ [ ! false ] è¿”å› trueã€‚ -o æˆ–è¿ç®—ï¼Œæœ‰ä¸€ä¸ªè¡¨è¾¾å¼ä¸º true åˆ™è¿”å› trueã€‚ [ $a -lt 20 -o $b -gt 100 ] è¿”å› trueã€‚ -a ä¸è¿ç®—ï¼Œä¸¤ä¸ªè¡¨è¾¾å¼éƒ½ä¸º true æ‰è¿”å› trueã€‚ [ $a -lt 20 -a $b -gt 100 ] è¿”å› falseã€‚ -o:or æˆ– -a: and ä¸ 1.4. å­—ç¬¦ä¸²è¿ç®—ç¬¦ å­—ç¬¦ä¸²è¿ç®—ç¬¦åˆ—è¡¨\nè¿ç®—ç¬¦ è¯´æ˜ ä¸¾ä¾‹ = æ£€æµ‹ä¸¤ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ç›¸ç­‰ï¼Œç›¸ç­‰è¿”å› trueã€‚ [ $a = $b ] è¿”å› falseã€‚ != æ£€æµ‹ä¸¤ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ç›¸ç­‰ï¼Œä¸ç›¸ç­‰è¿”å› trueã€‚ [ $a != $b ] è¿”å› trueã€‚ -z æ£€æµ‹å­—ç¬¦ä¸²é•¿åº¦æ˜¯å¦ä¸º0ï¼Œä¸º0è¿”å› trueã€‚ [ -z $a ] è¿”å› falseã€‚ -n æ£€æµ‹å­—ç¬¦ä¸²é•¿åº¦æ˜¯å¦ä¸º0ï¼Œä¸ä¸º0è¿”å› trueã€‚ [ -z $a ] è¿”å› trueã€‚ str æ£€æµ‹å­—ç¬¦ä¸²æ˜¯å¦ä¸ºç©ºï¼Œä¸ä¸ºç©ºè¿”å› trueã€‚ [ $a ] è¿”å› trueã€‚ å­—ç¬¦ä¸²é•¿åº¦ï¼š\n-z:zero ---\u003etrue -n: not zero---\u003etrue 1.5. æ–‡ä»¶æµ‹è¯•è¿ç®—ç¬¦ æ–‡ä»¶æµ‹è¯•è¿ç®—ç¬¦ç”¨äºæ£€æµ‹ Unix æ–‡ä»¶çš„å„ç§å±æ€§ã€‚\næ–‡ä»¶æµ‹è¯•è¿ç®—ç¬¦åˆ—è¡¨\næ“ä½œç¬¦ è¯´æ˜ ä¸¾ä¾‹ -b file æ£€æµ‹æ–‡ä»¶æ˜¯å¦æ˜¯å—è®¾å¤‡æ–‡ä»¶ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -b $file ] è¿”å› falseã€‚ -c file æ£€æµ‹æ–‡ä»¶æ˜¯å¦æ˜¯å­—ç¬¦è®¾å¤‡æ–‡ä»¶ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -b $file ] è¿”å› falseã€‚ -d file æ£€æµ‹æ–‡ä»¶æ˜¯å¦æ˜¯ç›®å½•ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -d $file ] è¿”å› falseã€‚ -f file æ£€æµ‹æ–‡ä»¶æ˜¯å¦æ˜¯æ™®é€šæ–‡ä»¶ï¼ˆæ—¢ä¸æ˜¯ç›®å½•ï¼Œä¹Ÿä¸æ˜¯è®¾å¤‡æ–‡ä»¶ï¼‰ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -f $file ] è¿”å› trueã€‚ -g file æ£€æµ‹æ–‡ä»¶æ˜¯å¦è®¾ç½®äº† SGID ä½ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -g $file ] è¿”å› falseã€‚ -k file æ£€æµ‹æ–‡ä»¶æ˜¯å¦è®¾ç½®äº†ç²˜ç€ä½(Sticky Bit)ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -k $file ] è¿”å› falseã€‚ -p file æ£€æµ‹æ–‡ä»¶æ˜¯å¦æ˜¯å…·åç®¡é“ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -p $file ] è¿”å› falseã€‚ -u file æ£€æµ‹æ–‡ä»¶æ˜¯å¦è®¾ç½®äº† SUID ä½ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -u $file ] è¿”å› falseã€‚ -r file æ£€æµ‹æ–‡ä»¶æ˜¯å¦å¯è¯»ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -r $file ] è¿”å› trueã€‚ -w file æ£€æµ‹æ–‡ä»¶æ˜¯å¦å¯å†™ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -w $file ] è¿”å› trueã€‚ -x file æ£€æµ‹æ–‡ä»¶æ˜¯å¦å¯æ‰§è¡Œï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -x $file ] è¿”å› trueã€‚ -s file æ£€æµ‹æ–‡ä»¶æ˜¯å¦ä¸ºç©ºï¼ˆæ–‡ä»¶å¤§å°æ˜¯å¦å¤§äº0ï¼‰ï¼Œä¸ä¸ºç©ºè¿”å› trueã€‚ [ -s $file ] è¿”å› trueã€‚ -e file æ£€æµ‹æ–‡ä»¶ï¼ˆåŒ…æ‹¬ç›®å½•ï¼‰æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å› trueã€‚ [ -e $file ] è¿”å› trueã€‚ 2. shellæ³¨é‡Š ä»¥â€œ#â€å¼€å¤´çš„è¡Œå°±æ˜¯æ³¨é‡Šï¼Œä¼šè¢«è§£é‡Šå™¨å¿½ç•¥ã€‚ shé‡Œæ²¡æœ‰å¤šè¡Œæ³¨é‡Šï¼Œåªèƒ½æ¯ä¸€è¡ŒåŠ ä¸€ä¸ª#å·ã€‚\nå¦‚æœåœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œé‡åˆ°å¤§æ®µçš„ä»£ç éœ€è¦ä¸´æ—¶æ³¨é‡Šèµ·æ¥ï¼Œè¿‡ä¸€ä¼šå„¿åˆå–æ¶ˆæ³¨é‡Šï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿæ¯ä¸€è¡ŒåŠ ä¸ª#ç¬¦å·å¤ªè´¹åŠ›äº†ï¼Œå¯ä»¥æŠŠè¿™ä¸€æ®µè¦æ³¨é‡Šçš„ä»£ç ç”¨ä¸€å¯¹èŠ±æ‹¬å·æ‹¬èµ·æ¥ï¼Œå®šä¹‰æˆä¸€ä¸ªå‡½æ•°ï¼Œæ²¡æœ‰åœ°æ–¹è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼Œè¿™å—ä»£ç å°±ä¸ä¼šæ‰§è¡Œï¼Œè¾¾åˆ°äº†å’Œæ³¨é‡Šä¸€æ ·çš„æ•ˆæœã€‚\nå‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. shellè¿ç®—ç¬¦ Bash æ”¯æŒå¾ˆå¤šè¿ç®—ç¬¦ï¼ŒåŒ…æ‹¬ç®—æ•°è¿ç®—ç¬¦ã€å…³ç³»è¿ç®—ç¬¦ã€å¸ƒå°”è¿ç®—ç¬¦ã€å­—ç¬¦ä¸²è¿ç®—ç¬¦å’Œæ–‡ä»¶æµ‹è¯•è¿ç®—ç¬¦ã€‚ awk â€¦","ref":"/linux-notes/shell/shell-char/","tags":["Shell"],"title":"Shellè¿ç®—ç¬¦"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/tcpip/","tags":"","title":"TCP/IP"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/ide/vim/","tags":"","title":"VIMé…ç½®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/introduction/package/","tags":"","title":"åŒ…ç®¡ç†å·¥å…·"},{"body":"1. å®‰è£…cephfså®¢æˆ·ç«¯ æ‰€æœ‰nodeèŠ‚ç‚¹å®‰è£…cephfså®¢æˆ·ç«¯ï¼Œä¸»è¦ç”¨æ¥å’Œcephé›†ç¾¤æŒ‚è½½ä½¿ç”¨ã€‚\nyum install -y ceph-common 2. éƒ¨ç½²RBAC 2.1. ClusterRole kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] 2.2. ClusterRoleBinding kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 2.3. Role apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] 2.4. RoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner 2.5. ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs 3. éƒ¨ç½² cephfs-provisioner apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" resources: limits: cpu: 500m memory: 512Mi requests: cpu: 100m memory: 64Mi env: - name: PROVISIONER_NAME # ä¸storageclassçš„provisionerå‚æ•°ç›¸åŒ value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE # ä¸rbacçš„namespaceç›¸åŒ value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 4. éƒ¨ç½²storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cephfs-provisioner-sc provisioner: ceph.com/cephfs volumeBindingMode: WaitForFirstConsumer parameters: monitors: 192.168.27.43:6789,192.168.27.44:6789,192.168.27.45:6789 adminId: admin adminSecretName: csi-cephfs-secret adminSecretNamespace: \"kube-csi\" claimRoot: /pvc-volumes 5. éƒ¨ç½²statefulset apiVersion: apps/v1 kind: StatefulSet metadata: name: cephfs-provisioner-nginx spec: serviceName: \"nginx\" replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest #nginxçš„é•œåƒ imagePullPolicy: IfNotPresent volumeMounts: - mountPath: \"/mnt\" #å®¹å™¨é‡Œé¢çš„æŒ‚è½½ç›®å½•ï¼Œè¯¥ç›®å½•æŒ‚è½½åˆ°NFSçš„å…±äº«ç›®å½•ä¸Š name: test volumeClaimTemplates: - metadata: name: test spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 2Gi storageClassName: cephfs-provisioner-sc 6. æ—¥å¿— 6.1. cephfs-provisoner æ‰§è¡Œæ—¥å¿— I0327 07:18:19.742239 1 controller.go:987] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": started I0327 07:18:19.745239 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-22-0\", UID:\"7f6b60d5-5060-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347214256\", FieldPath:\"\"}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim \"default/test-cephfs-ngx-wait-22-0\" I0327 07:18:23.281277 1 cephfs-provisioner.go:222] successfully created CephFS share \u0026CephFSPersistentVolumeSource{Monitors:[192.168.27.43:6789 192.168.27.44:6789 192.168.27.45:6789],Path:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-7f7cb62f-5060-11e9-85c0-0adb8ef08100,User:kubernetes-dynamic-user-7f7cb69f-5060-11e9-85c0-0adb8ef08100,SecretFile:,SecretRef:\u0026SecretReference{Name:ceph-kubernetes-dynamic-user-7f7cb69f-5060-11e9-85c0-0adb8ef08100-secret,Namespace:default,},ReadOnly:false,} I0327 07:18:23.281371 1 controller.go:1087] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": volume \"pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65\" provisioned I0327 07:18:23.281415 1 controller.go:1101] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": trying to save persistentvvolume \"pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65\" I0327 07:18:23.284621 1 controller.go:1108] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": persistentvolume \"pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65\" saved I0327 07:18:23.284723 1 controller.go:1149] provision \"default/test-cephfs-ngx-wait-22-0\" class \"cephfs-provisioner-sc\": succeeded I0327 07:18:23.284810 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-22-0\", UID:\"7f6b60d5-5060-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347214256\", FieldPath:\"\"}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65 6.2. debug æ—¥å¿— I0327 08:08:11.789608 1 controller.go:987] provision \"default/test-cephfs-ngx-wait-44-0\" class \"cephfs-sc-wait\": started I0327 08:08:11.793258 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-44-0\", UID:\"81846859-5067-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347237916\", FieldPath:\"\"}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim \"default/test-cephfs-ngx-wait-44-0\" E0327 08:08:12.164705 1 cephfs-provisioner.go:158] failed to provision share \"kubernetes-dynamic-pvc-76ecdc5a-5067-11e9-9421-2a1b1be1aeef\" for \"kubernetes-dynamic-user-76ecdcee-5067-11e9-9421-2a1b1be1aeef\", err: exit status 1, output: Traceback (most recent call last): File \"/usr/local/bin/cephfs_provisioner\", line 364, in \u003cmodule\u003e main() File \"/usr/local/bin/cephfs_provisioner\", line 358, in main print cephfs.create_share(share, user, size=size) File \"/usr/local/bin/cephfs_provisioner\", line 228, in create_share volume = self.volume_client.create_volume(volume_path, size=size, namespace_isolated=not self.ceph_namespace_isolation_disabled) File \"/usr/local/bin/cephfs_provisioner\", line 112, in volume_client self._volume_client.connect(None) File \"/lib/python2.7/site-packages/ceph_volume_client.py\", line 458, in connect self.rados.connect() File \"rados.pyx\", line 895, in rados.Rados.connect (/home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos7/DIST/centos7/MACHINE_SIZE/huge/release/13.2.1/rpm/el7/BUILD/ceph-13.2.1/build/src/pybind/rados/pyrex/rados.c:9815) rados.IOError: [errno 5] error connecting to the cluster W0327 08:08:12.164908 1 controller.go:746] Retrying syncing claim \"default/test-cephfs-ngx-wait-44-0\" because failures 2 \u003c threshold 15 E0327 08:08:12.164977 1 controller.go:761] error syncing claim \"default/test-cephfs-ngx-wait-44-0\": failed to provision volume with StorageClass \"cephfs-sc-wait\": exit status 1 I0327 08:08:12.165974 1 event.go:221] Event(v1.ObjectReference{Kind:\"PersistentVolumeClaim\", Namespace:\"default\", Name:\"test-cephfs-ngx-wait-44-0\", UID:\"81846859-5067-11e9-9a9c-c81f66bcff65\", APIVersion:\"v1\", ResourceVersion:\"347237916\", FieldPath:\"\"}): type: 'Warning' reason: 'ProvisioningFailed' failed to provision volume with StorageClass \"cephfs-sc-wait\": exit status 1 å‚è€ƒ\nhttps://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs/deploy ","categories":"","description":"","excerpt":"1. å®‰è£…cephfså®¢æˆ·ç«¯ æ‰€æœ‰nodeèŠ‚ç‚¹å®‰è£…cephfså®¢æˆ·ç«¯ï¼Œä¸»è¦ç”¨æ¥å’Œcephé›†ç¾¤æŒ‚è½½ä½¿ç”¨ã€‚\nyum install -y â€¦","ref":"/kubernetes-notes/storage/csi/provisioner/cephfs-provisioner/","tags":["CSI"],"title":"éƒ¨ç½²cephfs-provisioner"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næ­¤éƒ¨åˆ†ä¸»è¦ä»‹ç»è°ƒåº¦ä¸­ä½¿ç”¨çš„å„ç§è°ƒåº¦ç®—æ³•ï¼ŒåŒ…æ‹¬è°ƒåº¦ç®—æ³•çš„æ³¨å†Œéƒ¨åˆ†ã€‚æ³¨å†Œéƒ¨åˆ†çš„ä»£ç ä¸»è¦åœ¨/pkg/scheduler/algorithmproviderä¸­ï¼Œå…·ä½“çš„é¢„é€‰ç­–ç•¥å’Œä¼˜é€‰ç­–ç•¥çš„ç®—æ³•å®ç°åœ¨/pkg/scheduler/algorithmä¸­ã€‚\n1. ApplyFeatureGates æ³¨å†Œè°ƒåº¦ç®—æ³•çš„è°ƒç”¨å…¥å£åœ¨SchedulerCommandçš„Runå‡½æ•°ä¸­ã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äº/cmd/kube-scheduler/app/server.go\n// Run runs the Scheduler. func Run(c schedulerserverconfig.CompletedConfig, stopCh \u003c-chan struct{}) error { ... // Apply algorithms based on feature gates. // TODO: make configurable? algorithmprovider.ApplyFeatureGates() ... } ApplyFeatureGatesçš„å…·ä½“å®ç°åœ¨pkg/scheduler/algorithmproviderçš„åŒ…ä¸­ã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äº/pkg/scheduler/algorithmprovider/plugins.go\n// ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { defaults.ApplyFeatureGates() } ApplyFeatureGateså…·ä½“å®ç°å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äº/pkg/scheduler/algorithmprovider/defaults/defaults.go\næ ¹æ®featureç§»é™¤éƒ¨åˆ†è°ƒåº¦ç­–ç•¥ã€‚\n// ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { if utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition) { // Remove \"CheckNodeCondition\", \"CheckNodeMemoryPressure\", \"CheckNodePIDPressurePred\" // and \"CheckNodeDiskPressure\" predicates factory.RemoveFitPredicate(predicates.CheckNodeConditionPred) factory.RemoveFitPredicate(predicates.CheckNodeMemoryPressurePred) factory.RemoveFitPredicate(predicates.CheckNodeDiskPressurePred) factory.RemoveFitPredicate(predicates.CheckNodePIDPressurePred) // Remove key \"CheckNodeCondition\", \"CheckNodeMemoryPressure\" and \"CheckNodeDiskPressure\" // from ALL algorithm provider // The key will be removed from all providers which in algorithmProviderMap[] // if you just want remove specific provider, call func RemovePredicateKeyFromAlgoProvider() factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeConditionPred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeMemoryPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeDiskPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodePIDPressurePred) // Fit is determined based on whether a pod can tolerate all of the node's taints factory.RegisterMandatoryFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints) // Fit is determined based on whether a pod can tolerate unschedulable of node factory.RegisterMandatoryFitPredicate(predicates.CheckNodeUnschedulablePred, predicates.CheckNodeUnschedulablePredicate) // Insert Key \"PodToleratesNodeTaints\" and \"CheckNodeUnschedulable\" To All Algorithm Provider // The key will insert to all providers which in algorithmProviderMap[] // if you just want insert to specific provider, call func InsertPredicateKeyToAlgoProvider() factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.PodToleratesNodeTaintsPred) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.CheckNodeUnschedulablePred) glog.Warningf(\"TaintNodesByCondition is enabled, PodToleratesNodeTaints predicate is mandatory\") } // Prioritizes nodes that satisfy pod's resource limits if utilfeature.DefaultFeatureGate.Enabled(features.ResourceLimitsPriorityFunction) { factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1) } } 2. init å½“å‡½æ•°é€»è¾‘è°ƒç”¨åˆ°algorithmprovideråŒ…æ—¶ï¼Œå°±ä¼šè‡ªåŠ¨è°ƒç”¨initçš„åˆå§‹åŒ–å‡½æ•°ï¼Œæ­¤éƒ¨åˆ†ä¸»è¦åŒ…æ‹¬å¯¹é¢„é€‰ç®—æ³•å’Œä¼˜é€‰ç®—æ³•çš„æ³¨å†Œã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äº/pkg/scheduler/algorithmprovider/defaults/defaults.go\nfunc init() { // Register functions that extract metadata used by predicates and priorities computations. factory.RegisterPredicateMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PredicateMetadataProducer { return predicates.NewPredicateMetadataFactory(args.PodLister) }) factory.RegisterPriorityMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PriorityMetadataProducer { return priorities.NewPriorityMetadataFactory(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }) registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) // IMPORTANT NOTES for predicate developers: // We are using cached predicate result for pods belonging to the same equivalence class. // So when implementing a new predicate, you are expected to check whether the result // of your predicate function can be affected by related API object change (ADD/DELETE/UPDATE). // If yes, you are expected to invalidate the cached predicate result for related API object change. // For example: // https://github.com/kubernetes/kubernetes/blob/36a218e/plugin/pkg/scheduler/factory/factory.go#L422 // Registers predicates and priorities that are not enabled by default, but user can pick when creating their // own set of priorities/predicates. // PodFitsPorts has been replaced by PodFitsHostPorts for better user understanding. // For backwards compatibility with 1.0, PodFitsPorts is registered as well. factory.RegisterFitPredicate(\"PodFitsPorts\", predicates.PodFitsHostPorts) // Fit is defined based on the absence of port conflicts. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts) // Fit is determined by resource availability. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) // Fit is determined by the presence of the Host parameter and a string match // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost) // Fit is determined by node selector query. factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) // ServiceSpreadingPriority is a priority config factory that spreads pods by minimizing // the number of pods (belonging to the same service) on the same node. // Register the factory so that it's available, but do not include it as part of the default priorities // Largely replaced by \"SelectorSpreadPriority\", but registered for backward compatibility with 1.0 factory.RegisterPriorityConfigFactory( \"ServiceSpreadingPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, algorithm.EmptyControllerLister{}, algorithm.EmptyReplicaSetLister{}, algorithm.EmptyStatefulSetLister{}) }, Weight: 1, }, ) // EqualPriority is a prioritizer function that gives an equal weight of one to all nodes // Register the priority function so that its available // but do not include it as part of the default priorities factory.RegisterPriorityFunction2(\"EqualPriority\", core.EqualPriorityMap, nil, 1) // Optional, cluster-autoscaler friendly priority function - give used nodes higher priority. factory.RegisterPriorityFunction2(\"MostRequestedPriority\", priorities.MostRequestedPriorityMap, nil, 1) factory.RegisterPriorityFunction2( \"RequestedToCapacityRatioPriority\", priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) } ä»¥ä¸‹å¯¹initä¸­çš„æ³¨å†Œè¿›è¡Œæ‹†åˆ†ä»‹ç»ã€‚\n2.1. registerAlgorithmProvider æ­¤éƒ¨åˆ†ä¸»è¦æ³¨å†Œé»˜è®¤çš„é¢„é€‰å’Œä¼˜é€‰ç­–ç•¥ã€‚\n// Register functions that extract metadata used by predicates and priorities computations. factory.RegisterPredicateMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PredicateMetadataProducer { return predicates.NewPredicateMetadataFactory(args.PodLister) }) factory.RegisterPriorityMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PriorityMetadataProducer { return priorities.NewPriorityMetadataFactory(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }) registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) registerAlgorithmProvider\næ³¨å†ŒAlgorithmProviderï¼Œå…¶ä¸­åŒ…æ‹¬DefaultProviderå’ŒClusterAutoscalerProviderã€‚\nfunc registerAlgorithmProvider(predSet, priSet sets.String) { // Registers algorithm providers. By default we use 'DefaultProvider', but user can specify one to be used // by specifying flag. factory.RegisterAlgorithmProvider(factory.DefaultProvider, predSet, priSet) // Cluster autoscaler friendly scheduling algorithm. factory.RegisterAlgorithmProvider(ClusterAutoscalerProvider, predSet, copyAndReplace(priSet, \"LeastRequestedPriority\", \"MostRequestedPriority\")) } 2.2. RegisterFitPredicate åœ¨initéƒ¨åˆ†æ³¨å†Œé¢„é€‰ç­–ç•¥å‡½æ•°ã€‚\né¢„é€‰ç­–ç•¥å¦‚ä¸‹ï¼š\nè°ƒåº¦ç­–ç•¥ å‡½æ•° æè¿° PodFitsPorts PodFitsHostPorts PodFitsPortså·²ç»è¢«PodFitsHostPortsä»£æ›¿ï¼Œæ­¤å¤„ä¸»è¦æ˜¯ä¸ºäº†å…¼å®¹æ€§ã€‚ PodFitsHostPortsPred PodFitsHostPorts åˆ¤æ–­æ˜¯å¦ä¸å®¿ä¸»æœºçš„ç«¯å£å†²çªã€‚ PodFitsResourcesPred PodFitsResources åˆ¤æ–­nodeèµ„æºæ˜¯å¦å……è¶³ã€‚ HostNamePred PodFitsHost åˆ¤æ–­podæ‰€æŒ‡å®šè°ƒåº¦çš„èŠ‚ç‚¹æ˜¯å¦æ˜¯å½“å‰çš„èŠ‚ç‚¹ã€‚ MatchNodeSelectorPred PodMatchNodeSelector åˆ¤æ–­podæŒ‡å®šçš„node selectoræ˜¯å¦åŒ¹é…å½“å‰çš„nodeã€‚ å…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// PodFitsPorts has been replaced by PodFitsHostPorts for better user understanding. // For backwards compatibility with 1.0, PodFitsPorts is registered as well. factory.RegisterFitPredicate(\"PodFitsPorts\", predicates.PodFitsHostPorts) // Fit is defined based on the absence of port conflicts. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts) // Fit is determined by resource availability. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) // Fit is determined by the presence of the Host parameter and a string match // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost) // Fit is determined by node selector query. factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) 2.3. RegisterPriorityFunction2 åœ¨initéƒ¨åˆ†æ³¨å†Œä¼˜é€‰ç­–ç•¥å‡½æ•°ã€‚\n// EqualPriority is a prioritizer function that gives an equal weight of one to all nodes // Register the priority function so that its available // but do not include it as part of the default priorities factory.RegisterPriorityFunction2(\"EqualPriority\", core.EqualPriorityMap, nil, 1) // Optional, cluster-autoscaler friendly priority function - give used nodes higher priority. factory.RegisterPriorityFunction2(\"MostRequestedPriority\", priorities.MostRequestedPriorityMap, nil, 1) factory.RegisterPriorityFunction2( \"RequestedToCapacityRatioPriority\", priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) 3. defaultPredicates æ­¤éƒ¨åˆ†ä¸ºé»˜è®¤é¢„é€‰ç­–ç•¥çš„æ³¨å†Œå‡½æ•°ã€‚\né»˜è®¤çš„é¢„é€‰ç­–ç•¥å¦‚ä¸‹ï¼š\né¢„é€‰ç­–ç•¥ å‡½æ•° æè¿° NoVolumeZoneConflictPred NewVolumeZonePredicate åˆ¤æ–­podä½¿ç”¨åˆ°çš„volumeæ˜¯å¦æœ‰èŠ‚ç‚¹çš„è¦æ±‚ã€‚ç›®å‰åªæ”¯æŒpvcã€‚ MaxEBSVolumeCountPred NewMaxPDVolumeCountPredicate åˆ¤æ–­podä½¿ç”¨EBSVolumeåœ¨è¯¥èŠ‚ç‚¹ä¸Šæ˜¯å¦å·²ç»è¾¾åˆ°ä¸Šé™äº†ã€‚ MaxGCEPDVolumeCountPred NewMaxPDVolumeCountPredicate åˆ¤æ–­podä½¿ç”¨GCEPDVolumeåœ¨è¯¥èŠ‚ç‚¹ä¸Šæ˜¯å¦å·²ç»è¾¾åˆ°ä¸Šé™äº†ã€‚ MaxAzureDiskVolumeCountPred NewMaxPDVolumeCountPredicate åˆ¤æ–­podä½¿ç”¨AzureDiskVolumeåœ¨è¯¥èŠ‚ç‚¹ä¸Šæ˜¯å¦å·²ç»è¾¾åˆ°ä¸Šé™äº†ã€‚ MaxCSIVolumeCountPred NewCSIMaxVolumeLimitPredicate åˆ¤æ–­CSIVolumeæ˜¯å¦è¾¾åˆ°ä¸Šé™äº†ã€‚ MatchInterPodAffinityPred NewPodAffinityPredicate åŒ¹é…podçš„äº²ç¼˜æ€§ã€‚ NoDiskConflictPred NoDiskConflict åˆ¤æ–­æ˜¯å¦æœ‰disk volumesçš„å†²çªã€‚ GeneralPred GeneralPredicates é€šç”¨çš„é¢„é€‰ç­–ç•¥ CheckNodeMemoryPressurePred CheckNodeMemoryPressurePredicate åˆ¤æ–­èŠ‚ç‚¹å†…å­˜æ˜¯å¦å……è¶³ã€‚ CheckNodeDiskPressurePred CheckNodeDiskPressurePredicate åˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦æœ‰ç£ç›˜å‹åŠ›ã€‚ CheckNodePIDPressurePred CheckNodePIDPressurePredicate åˆ¤æ–­èŠ‚ç‚¹ä¸Šçš„PID CheckNodeConditionPred CheckNodeConditionPredicate åˆ¤æ–­nodeæ˜¯å¦readyã€‚ PodToleratesNodeTaintsPred PodToleratesNodeTaints åˆ¤æ–­podæ˜¯å¦å¯ä»¥å®¹å¿èŠ‚ç‚¹çš„taintsã€‚ CheckVolumeBindingPred NewVolumeBindingPredicate åˆ¤æ–­æ˜¯å¦æœ‰volumeæ‹“æ‰‘çš„è¦æ±‚ã€‚ å…·ä½“ä»£ç å¦‚ä¸‹ï¼š\nfunc defaultPredicates() sets.String { return sets.NewString( // Fit is determined by volume zone requirements. factory.RegisterFitPredicateFactory( predicates.NoVolumeZoneConflictPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeZonePredicate(args.PVInfo, args.PVCInfo, args.StorageClassInfo) }, ), // Fit is determined by whether or not there would be too many AWS EBS volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxEBSVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.EBSVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), // Fit is determined by whether or not there would be too many GCE PD volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxGCEPDVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.GCEPDVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), // Fit is determined by whether or not there would be too many Azure Disk volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxAzureDiskVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.AzureDiskVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), factory.RegisterFitPredicateFactory( predicates.MaxCSIVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewCSIMaxVolumeLimitPredicate(args.PVInfo, args.PVCInfo) }, ), // Fit is determined by inter-pod affinity. factory.RegisterFitPredicateFactory( predicates.MatchInterPodAffinityPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewPodAffinityPredicate(args.NodeInfo, args.PodLister) }, ), // Fit is determined by non-conflicting disk volumes. factory.RegisterFitPredicate(predicates.NoDiskConflictPred, predicates.NoDiskConflict), // GeneralPredicates are the predicates that are enforced by all Kubernetes components // (e.g. kubelet and all schedulers) factory.RegisterFitPredicate(predicates.GeneralPred, predicates.GeneralPredicates), // Fit is determined by node memory pressure condition. factory.RegisterFitPredicate(predicates.CheckNodeMemoryPressurePred, predicates.CheckNodeMemoryPressurePredicate), // Fit is determined by node disk pressure condition. factory.RegisterFitPredicate(predicates.CheckNodeDiskPressurePred, predicates.CheckNodeDiskPressurePredicate), // Fit is determined by node pid pressure condition. factory.RegisterFitPredicate(predicates.CheckNodePIDPressurePred, predicates.CheckNodePIDPressurePredicate), // Fit is determined by node conditions: not ready, network unavailable or out of disk. factory.RegisterMandatoryFitPredicate(predicates.CheckNodeConditionPred, predicates.CheckNodeConditionPredicate), // Fit is determined based on whether a pod can tolerate all of the node's taints factory.RegisterFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints), // Fit is determined by volume topology requirements. factory.RegisterFitPredicateFactory( predicates.CheckVolumeBindingPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeBindingPredicate(args.VolumeBinder) }, ), ) } 4. defaultPriorities æ­¤éƒ¨åˆ†ä¸»è¦ä¸ºé»˜è®¤ä¼˜é€‰ç­–ç•¥çš„æ³¨å†Œå‡½æ•°ã€‚\né»˜è®¤ä¼˜é€‰ç­–ç•¥å¦‚ä¸‹ï¼š\nä¼˜é€‰ç­–ç•¥ å‡½æ•° æè¿° SelectorSpreadPriority NewSelectorSpreadPriority å±äºç›¸åŒserviceå’Œrsä¸‹çš„podå°½é‡åˆ†å¸ƒåœ¨ä¸åŒçš„nodeä¸Šã€‚ InterPodAffinityPriority NewInterPodAffinityPriority æ ¹æ®podçš„äº²ç¼˜æ€§ï¼Œå°†ç›¸åŒæ‹“æ‰‘åŸŸä¸­çš„podæ”¾åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ LeastRequestedPriority LeastRequestedPriorityMap æŒ‰æœ€å°‘è¯·æ±‚çš„åˆ©ç”¨ç‡å¯¹èŠ‚ç‚¹è¿›è¡Œä¼˜å…ˆçº§æ’åºã€‚ BalancedResourceAllocation BalancedResourceAllocationMap å®ç°èµ„æºçš„å¹³è¡¡ä½¿ç”¨ã€‚ NodePreferAvoidPodsPriority CalculateNodePreferAvoidPodsPriorityMap å°†æ­¤æƒé‡è®¾ç½®ä¸ºè¶³ä»¥è¦†ç›–æ‰€æœ‰å…¶ä»–ä¼˜å…ˆçº§å‡½æ•°ã€‚ NodeAffinityPriority CalculateNodeAffinityPriorityMap podæŒ‡å®šlabelèŠ‚ç‚¹è°ƒåº¦ï¼Œæ¥åŒ¹é…nodeäº²ç¼˜æ€§ã€‚ TaintTolerationPriority ComputeTaintTolerationPriorityMap podæœ‰è®¾ç½®tolerateå±æ€§æ¥å®¹å¿nodeçš„taintã€‚ ImageLocalityPriority ImageLocalityPriorityMap æ ¹æ®èŠ‚ç‚¹ä¸Šæ˜¯å¦æœ‰è¯¥podä½¿ç”¨åˆ°çš„é•œåƒæ‰“åˆ†ã€‚ å…·ä½“ä»£ç å®ç°å¦‚ä¸‹ï¼š\nfunc defaultPriorities() sets.String { return sets.NewString( // spreads pods by minimizing the number of pods (belonging to the same service or replication controller) on the same node. factory.RegisterPriorityConfigFactory( \"SelectorSpreadPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }, Weight: 1, }, ), // pods should be placed in the same topological domain (e.g. same node, same rack, same zone, same power domain, etc.) // as some other pods, or, conversely, should not be placed in the same topological domain as some other pods. factory.RegisterPriorityConfigFactory( \"InterPodAffinityPriority\", factory.PriorityConfigFactory{ Function: func(args factory.PluginFactoryArgs) algorithm.PriorityFunction { return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) }, Weight: 1, }, ), // Prioritize nodes by least requested utilization. factory.RegisterPriorityFunction2(\"LeastRequestedPriority\", priorities.LeastRequestedPriorityMap, nil, 1), // Prioritizes nodes to help achieve balanced resource usage factory.RegisterPriorityFunction2(\"BalancedResourceAllocation\", priorities.BalancedResourceAllocationMap, nil, 1), // Set this weight large enough to override all other priority functions. // TODO: Figure out a better way to do this, maybe at same time as fixing #24720. factory.RegisterPriorityFunction2(\"NodePreferAvoidPodsPriority\", priorities.CalculateNodePreferAvoidPodsPriorityMap, nil, 10000), // Prioritizes nodes that have labels matching NodeAffinity factory.RegisterPriorityFunction2(\"NodeAffinityPriority\", priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1), // Prioritizes nodes that marked with taint which pod can tolerate. factory.RegisterPriorityFunction2(\"TaintTolerationPriority\", priorities.ComputeTaintTolerationPriorityMap, priorities.ComputeTaintTolerationPriorityReduce, 1), // ImageLocalityPriority prioritizes nodes that have images requested by the pod present. factory.RegisterPriorityFunction2(\"ImageLocalityPriority\", priorities.ImageLocalityPriorityMap, nil, 1), ) } å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithmprovider/defaults/defaults.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næ­¤éƒ¨åˆ†ä¸»è¦ä»‹ç»è°ƒåº¦ä¸­ä½¿ç”¨çš„å„ç§è°ƒåº¦ç®—æ³•ï¼ŒåŒ…æ‹¬è°ƒåº¦ç®—æ³•çš„æ³¨å†Œéƒ¨åˆ†ã€‚æ³¨å†Œéƒ¨åˆ†çš„ â€¦","ref":"/k8s-source-code-analysis/kube-scheduler/registeralgorithmprovider/","tags":["æºç åˆ†æ"],"title":"kube-scheduleræºç åˆ†æï¼ˆäºŒï¼‰ä¹‹ è°ƒåº¦ç®—æ³•"},{"body":"æœ¬æ–‡ä¸»è¦æè¿°é‡è£…æ“ä½œç³»ç»Ÿå¦‚ä½•æ ¼å¼åŒ–è·Ÿåˆ†åŒºå’Œæ•°æ®ç›˜ä»¥åŠè®¾ç½®ç£ç›˜æŒ‚è½½é…ç½®ã€‚\n1.å°†æ“ä½œç³»ç»Ÿå†™å…¥æ ¹åˆ†åŒºè®¾å¤‡ 1ã€è§£ç»‘æ ¹åˆ†åŒºç£ç›˜ç›®å½•æŒ‚è½½\n# ä¾‹å¦‚è·Ÿåˆ†åŒºä¸‹çš„æŒ‚è½½ç›®å½•å¦‚ä¸‹ï¼š cat /proc/mounts | grep ^/dev/sda /dev/sda4 / ext4 rw,relatime,errors=remount-ro 0 0 /dev/sda3 /boot ext4 rw,relatime 0 0 /dev/sda2 /boot/efi vfat rw,relatime,fmask=0077,dmask=0077,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro 0 0 # ä¾‹å¦‚æ ¹åˆ†åŒºçš„å—è®¾å¤‡æ˜¯/dev/sdaï¼Œåˆ™è§£ç»‘è¯¥å—è®¾å¤‡ä¸‹æ‰€æœ‰æŒ‚è½½ç›®å½• cat /proc/mounts | grep ^/dev/sda | awk '{print $2}' | xargs -n1 -i umount {} 2ã€åˆ é™¤å—è®¾å¤‡åˆ†åŒº\nsfdisk --delete /dev/sda 3ã€å°†æ“ä½œç³»ç»Ÿé•œåƒå†™å…¥æ ¹åˆ†åŒºè®¾å¤‡\nqemu-img dd -f qcow2 -O raw bs=16M if=osi.qcow2 of=/dev/sda å‘½ä»¤å‚æ•°è¯´æ˜ï¼š\n-f qcow2\næŒ‡å®šè¾“å…¥æ–‡ä»¶çš„æ ¼å¼ä¸º qcow2ï¼ˆQEMU Copy-On-Write v2ï¼‰ã€‚ osi.qcow2 æ˜¯ä¸€ä¸ªè™šæ‹Ÿç£ç›˜æ–‡ä»¶ï¼ŒåŒ…å«ç³»ç»Ÿæˆ–æ•°æ®ã€‚ -O raw\næŒ‡å®šè¾“å‡ºæ ¼å¼ä¸º rawï¼Œå³ä¸åŒ…å«é¢å¤–å…ƒæ•°æ®çš„è£¸æ•°æ®æ ¼å¼ã€‚ è£¸æ•°æ®æ ¼å¼é€‚ç”¨äºç›´æ¥å†™å…¥ç‰©ç†ç£ç›˜è®¾å¤‡ã€‚ bs=16M\nè®¾ç½®å—å¤§å°ä¸º 16 MBï¼Œåœ¨æ•°æ®å¤åˆ¶è¿‡ç¨‹ä¸­ä»¥ 16 MB ä¸ºå•ä½è¿›è¡Œè¯»å†™ã€‚ è¾ƒå¤§çš„å—å¤§å°é€šå¸¸èƒ½æé«˜å†™å…¥æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹å¤§å®¹é‡æ–‡ä»¶ã€‚ if=osi.qcow2\nè¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œosi.qcow2 æ˜¯æºè™šæ‹Ÿç£ç›˜é•œåƒæ–‡ä»¶ã€‚ of=/dev/sda\nè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œ/dev/sda æ˜¯ç›®æ ‡ç‰©ç†ç£ç›˜è®¾å¤‡ã€‚ æ•°æ®å°†ç›´æ¥å†™å…¥ /dev/sdaï¼Œè¦†ç›–å…¶å†…å®¹ã€‚ 4ã€æ£€æŸ¥å¹¶ä¿®å¤æ ¹åˆ†åŒº\nå¦‚æœåˆ†åŒºè¡¨æ— é—®é¢˜ï¼Œparted ä¼šç›´æ¥æ˜¾ç¤ºåˆ†åŒºä¿¡æ¯ã€‚ å¦‚æœæ£€æµ‹åˆ°åˆ†åŒºè¡¨é”™è¯¯ï¼Œparted ä¼šè‡ªåŠ¨åº”ç”¨ä¿®å¤å¹¶è¾“å‡ºç»“æœã€‚ echo Fix | parted ---pretend-input-tty /dev/sda print 5ã€ é€šçŸ¥å†…æ ¸é‡æ–°è¯»å–åˆ†åŒºè¡¨\n# é€šçŸ¥å†…æ ¸é‡æ–°åŠ è½½æŒ‡å®šè®¾å¤‡çš„åˆ†åŒºè¡¨ï¼Œæ— éœ€é‡å¯ partprobe /dev/sda 2. æ ¹åˆ†åŒºè®¾å¤‡é‡æ–°åˆ†åŒº è·Ÿè®¾å¤‡çš„åˆ†åŒºä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªåˆ†åŒº\nswapåˆ†åŒºï¼šæ˜¯å¦éœ€è¦åšswapåˆ†åŒº\næ ¹åˆ†åŒºï¼šåœ¨swapåˆ†åŒºåšå®Œåå†åšæ ¹åˆ†åŒº\nè·Ÿè®¾å¤‡çš„dataåˆ†åŒºï¼šåœ¨æ ¹åˆ†åŒºåšå®Œåå†åšdataåˆ†åŒºã€‚\n2.1. é…ç½®swapåˆ†åŒº swapåˆ†åŒºä¸€èˆ¬ä»æ ¹åˆ†åŒºè®¾å¤‡å¤§å°ä¸­åˆ‡åˆ†å‡ºæ¥128Gä½œä¸ºswapçš„åˆ†åŒºï¼Œå‰©ä½™çš„åšæ ¹åˆ†åŒºå’Œdataåˆ†åŒºã€‚\n# å‡è®¾è·Ÿè®¾å¤‡ä¸ºsda root_device=\"sda\" swap_device_num=1 # å‡è®¾swapåˆ†åŒºçš„å¤§å°æ˜¯128Gï¼Œæ¢ç®—ä¸º131072MiB swap_size=131072 # ç§»é™¤æ ¹åˆ†åŒº echo Ignore | parted ---pretend-input-tty /dev/${root_device} rm ${swap_device_num} # è·å–æŒ‡å®šç£ç›˜è®¾å¤‡ä¸Šæœ€åä¸€æ®µç©ºé—²ç©ºé—´çš„èµ·å§‹ä½ç½®ï¼ˆå•ä½ä¸º MiBï¼‰ä»¥ä¾¿æ–°å»ºåˆ†åŒºã€‚ free_end=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') # åˆ›å»ºæŒ‡å®šå¤§å°çš„swapåˆ†åŒº swap_start=${free_end} swap_end=$((swap_start + ${swap_size})) echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${swap_start}MiB ${swap_end}MiB # æ ¼å¼åŒ–åˆ†åŒºä¸º Swap ç±»å‹ï¼Œä¾‹å¦‚ï¼šmkswap /dev/sda1 mkswap /dev/${root_device}${swap_device_num} 2.2. æ ¼å¼åŒ–æ ¹åˆ†åŒº 2.2.1. æœªæŒ‡å®šæ ¹åˆ†åŒºå¤§å° å¦‚æœæ²¡æœ‰æŒ‡å®šæ ¹åˆ†åŒºå¤§å°ï¼Œä¸€èˆ¬ä¸éœ€è¦å†åšä¸€ä¸ªdataåˆ†åŒºï¼Œè€Œæ˜¯æŠŠæ ¹åˆ†åŒºæ‰©å±•ä¸ºå‰©ä½™çš„æ‰€æœ‰ç©ºé—´ã€‚\n1ã€å¦‚æœæ ¹åˆ†åŒºç›®å½•æ²¡æœ‰æŒ‡å®šåˆ†åŒºå¤§å°ï¼Œä¸”æ²¡æœ‰åšswapåˆ†åŒºã€‚åˆ™é‡æ–°è°ƒæ•´å¤§å°ï¼Œæ‰©å±•åˆ°è®¾å¤‡çš„æ‰€æœ‰å‰©ä½™ç©ºé—´ã€‚\nroot_device=\"sda\" root_device_num=1 echo Yes | parted ---pretend-input-tty /dev/${root_device} -- resizepart ${root_device_num} 100% 2ã€å¦‚æœæ ¹åˆ†åŒºæ²¡æœ‰æŒ‡å®šåˆ†åŒºå¤§å°ï¼Œä½†æ˜¯æœ‰åšswapåˆ†åŒºã€‚\nroot_device=\"sda\" # è·å–ç©ºé—²ç©ºé—´çš„èµ·å§‹ä½ç½® root_start=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') # å°†å‰©ä½™ç©ºé—´åšä¸€ä¸ªrootåˆ†åŒº echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${root_start}MiB 100% 2.2.2. æŒ‡å®šæ ¹åˆ†åŒºå¤§å° å¦‚æœæŒ‡å®šäº†æ ¹åˆ†åŒºå¤§å°ï¼Œä¸€èˆ¬éœ€è¦å†åˆ›å»ºä¸€ä¸ªdataåˆ†åŒºï¼Œå°†dataåˆ†åŒºæ‰©å±•ä¸ºå‰©ä½™çš„æ‰€æœ‰çš„ç©ºé—´ã€‚\n1ã€å¦‚æœæ ¹åˆ†åŒºç›®å½•æœ‰æŒ‡å®šå¤§å°ï¼Œä¸”æ²¡æœ‰åšswapåˆ†åŒºã€‚åˆ™æŒ‰æŒ‡å®šå¤§å°åˆ†åŒºï¼Œä¾‹å¦‚å°†è·Ÿåˆ†åŒºå¤§å°è®¾ç½®300G\nroot_device=\"sda\" root_device_num=1 root_size=307200 # 300Gæ¢ç®—æˆå•ä½MiB # è·å–æ ¹åˆ†åŒºçš„èµ·å§‹ä½ç½®å’Œç»ˆæ­¢ä½ç½® root_start=$(parted /dev/${root_device} unit MiB print | awk '/./{end=$2} END{print end}' | sed 's/MiB//') root_end=$((1+ root_start + ${root_size})) # è°ƒæ•´è·Ÿåˆ†åŒºå¤§å° echo Yes | parted ---pretend-input-tty /dev/${root_device} -- resizepart ${root_device_num} ${root_end} 2ã€å¦‚æœæ ¹åˆ†åŒºæœ‰æŒ‡å®šå¤§å°ï¼Œä½†æ˜¯æœ‰åšswapåˆ†åŒºã€‚\nroot_device=\"sda\" root_device_num=1 root_size=307200 # 300Gæ¢ç®—æˆå•ä½MiB # è·å–æ ¹åˆ†åŒºçš„èµ·å§‹ä½ç½®å’Œç»ˆæ­¢ä½ç½® root_start=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') root_end=$((1+ root_start + ${root_size})) # åˆ›å»ºæŒ‡å®šå¤§å°çš„æ ¹åˆ†åŒº echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${root_start}MiB ${root_end}MiB 2.3. åˆ›å»ºè·Ÿè®¾å¤‡çš„dataåˆ†åŒº å¦‚æœæœ‰æŒ‡å®šéœ€è¦åˆ›å»ºè·Ÿè®¾å¤‡çš„dataåˆ†åŒºï¼Œåˆ™åœ¨åˆ›å»ºå®Œswapåˆ†åŒºå’Œæ ¹åˆ†åŒºåï¼Œç»§ç»­åˆ›å»ºdataåˆ†åŒºã€‚\n1ã€åˆ›å»ºè·Ÿè®¾å¤‡çš„dataåˆ†åŒº\nroot_device=\"sda\" # è·å–ç©ºé—²ç©ºé—´çš„èµ·å§‹ä½ç½® data_start=$(parted /dev/${root_device} unit MiB print free | grep 'Free Space'|tail -1 | awk '{print $1}' | sed 's/MiB//') # å°†å‰©ä½™ç©ºé—´åšä¸€ä¸ªdataåˆ†åŒº echo Ignore | parted ---pretend-input-tty /dev/${root_device} -- mkpart primary ${data_start}MiB 100% 2ã€æ ¼å¼åŒ–æ ¹dataåˆ†åŒºçš„æ–‡ä»¶ç³»ç»Ÿ\n# dataåˆ†åŒºçš„åºå·ä¸€èˆ¬æ˜¯åœ¨rootåˆ†åŒºçš„åºå·+1 device=\"/dev/sda2\" # æ ¼å¼åŒ–ä¸ºxfsæ–‡ä»¶ç³»ç»Ÿ mkfs.xfs -f -n ftype=1 ${device} # æ ¼å¼åŒ–ä¸ºext4æ–‡ä»¶ç³»ç»Ÿ mkfs.ext4 -F ${device} 2. æ ¼å¼åŒ–æ•°æ®ç›˜ 1ã€æ‰¾å‡ºæ•°æ®ç›˜æ‰€å¯¹åº”çš„å—è®¾å¤‡ï¼Œä¾‹å¦‚ï¼š/dev/sdb\nlsblk -J -d { \"blockdevices\": [ {\"name\":\"sda\", \"maj:min\":\"8:0\", \"rm\":false, \"size\":\"1.1T\", \"ro\":false, \"type\":\"disk\", \"mountpoint\":null}, {\"name\":\"sdb\", \"maj:min\":\"8:16\", \"rm\":false, \"size\":\"6.1T\", \"ro\":false, \"type\":\"disk\", \"mountpoint\":\"/data\"} ] } 2ã€åˆ é™¤å—è®¾å¤‡åˆ†åŒºå¹¶é‡å»ºåˆ†åŒº\n# åˆ é™¤å—è®¾å¤‡æ‰€æœ‰çš„åˆ†åŒº sfdisk --delete /dev/sdb # å°†åˆ†åŒºé‡å»ºä¸ºGPTæ ¼å¼ echo label:gpt | sfdisk /dev/sdb 3ã€ é€šçŸ¥å†…æ ¸é‡æ–°è¯»å–åˆ†åŒºè¡¨\n# é€šçŸ¥å†…æ ¸é‡æ–°åŠ è½½æŒ‡å®šè®¾å¤‡çš„åˆ†åŒºè¡¨ï¼Œæ— éœ€é‡å¯ partprobe /dev/sdb 4ã€æ ¼å¼åŒ–æ•°æ®ç›˜çš„æ–‡ä»¶ç³»ç»Ÿ\n# æ ¼å¼åŒ–ä¸ºxfsæ–‡ä»¶ç³»ç»Ÿ mkfs.xfs -f -n ftype=1 /dev/sdb # æ ¼å¼åŒ–ä¸ºext4æ–‡ä»¶ç³»ç»Ÿ mkfs.ext4 -F /dev/sdb 3. è®¾ç½®fstabç£ç›˜æŒ‚è½½ è®¾ç½®swapåˆ†åŒºç£ç›˜æŒ‚è½½\nswap_uuid=$(lsblk -f |grep swap|awk '{print $3}') echo \"UUID=${swap_uuid} swap swap defaults 0 0\" \u003e\u003e /etc/fstab è®¾ç½®æ ¹åˆ†åŒºç£ç›˜æŒ‚è½½\ntodo\n","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦æè¿°é‡è£…æ“ä½œç³»ç»Ÿå¦‚ä½•æ ¼å¼åŒ–è·Ÿåˆ†åŒºå’Œæ•°æ®ç›˜ä»¥åŠè®¾ç½®ç£ç›˜æŒ‚è½½é…ç½®ã€‚\n1.å°†æ“ä½œç³»ç»Ÿå†™å…¥æ ¹åˆ†åŒºè®¾å¤‡ 1ã€è§£ç»‘æ ¹åˆ†åŒºç£ç›˜ç›®å½•æŒ‚è½½\n# ä¾‹å¦‚è·Ÿ â€¦","ref":"/linux-notes/baremetal/format-disk/","tags":["è£¸é‡‘å±"],"title":"æ ¼å¼åŒ–ç£ç›˜åˆ†åŒº"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/","tags":"","title":"åŸºæœ¬æ¦‚å¿µ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/node/","tags":"","title":"èŠ‚ç‚¹è¿ç§»"},{"body":"å¸¸è§é•œåƒæ‹‰å–é—®é¢˜æ’æŸ¥\n1. PodçŠ¶æ€ä¸ºErrImagePullæˆ–ImagePullBackOff docker-hub-75d4dfb984-5hggg 0/1 ImagePullBackOff 0 14m 192.168.1.30 \u003cnode ip\u003e docker-hub-75d4dfb984-9r57b 0/1 ErrImagePull 0 53s 192.168.0.42 \u003cnode ip\u003e ErrImagePullï¼šè¡¨ç¤ºpodå·²ç»è°ƒåº¦åˆ°nodeèŠ‚ç‚¹ï¼Œkubeletè°ƒç”¨dockerå»æ‹‰å–é•œåƒå¤±è´¥ã€‚ ImagePullBackOffï¼šè¡¨ç¤ºkubeletæ‹‰å–é•œåƒå¤±è´¥åï¼Œä¸æ–­é‡è¯•å»æ‹‰å–ä»ç„¶å¤±è´¥ã€‚ 2. æŸ¥çœ‹podçš„äº‹ä»¶ é€šè¿‡kubectl describe pod å‘½ä»¤æŸ¥çœ‹podäº‹ä»¶ï¼Œè¯¥äº‹ä»¶çš„æŠ¥é”™ä¿¡æ¯åœ¨kubeletæˆ–dockerçš„æ—¥å¿—ä¸­ä¹Ÿä¼šæŸ¥çœ‹åˆ°ã€‚\n2.1. http: server gave HTTP response to HTTPS client å¦‚æœé‡åˆ°ä»¥ä¸‹æŠ¥é”™ï¼Œå°è¯•å°†è¯¥é•œåƒä»“åº“æ·»åŠ åˆ°dockerå¯ä¿¡ä»»çš„é•œåƒä»“åº“é…ç½®ä¸­ã€‚\nError getting v2 registry: Get https://docker.com:8080/v2/: http: server gave HTTP response to HTTPS client\" å…·ä½“æ“ä½œæ˜¯ä¿®æ”¹/etc/docker/daemon.jsonçš„insecure-registrieså‚æ•°\n#cat /etc/docker/daemon.json { ... \"insecure-registries\": [ ... \"docker.com:8080\" ], ... } 2.2. no basic auth credentials å¦‚æœé‡åˆ°no basic auth credentialsæŠ¥é”™ï¼Œè¯´æ˜kubeletè°ƒç”¨dockeræ¥å£å»æ‹‰å–é•œåƒæ—¶ï¼Œé•œåƒä»“åº“çš„è®¤è¯ä¿¡æ¯å¤±è´¥ã€‚\nNormal BackOff 18s kubelet, 192.168.1.1 Back-off pulling image \"docker.com:8080/public/2048:latest\" Warning Failed 18s kubelet, 192.168.1.1 Error: ImagePullBackOff Normal Pulling 5s (x2 over 18s) kubelet, 192.168.1.1 Pulling image \"docker.com:8080/public/2048:latest\" Warning Failed 5s (x2 over 18s) kubelet, 192.168.1.1 Failed to pull image \"docker.com:8080/public/2048:latest\": rpc error: code = Unknown desc = Error response from daemon: Get http://docker.com:8080/v2/public/2048/manifests/latest: no basic auth credentials Warning Failed 5s (x2 over 18s) kubelet, 192.168.1.1 Error: ErrImagePull å…·ä½“æ“ä½œï¼Œåœ¨æ‹‰å–é•œåƒå¤±è´¥çš„èŠ‚ç‚¹ä¸Šç™»å½•è¯¥é•œåƒä»“åº“ï¼Œè®¤è¯ä¿¡æ¯ä¼šæ›´æ–°åˆ° $HOME/.docker/config.jsonæ–‡ä»¶ä¸­ã€‚å°†è¯¥æ–‡ä»¶æ‹·è´åˆ°/var/lib/kubelet/config.jsonä¸­ã€‚\n","categories":"","description":"","excerpt":"å¸¸è§é•œåƒæ‹‰å–é—®é¢˜æ’æŸ¥\n1. PodçŠ¶æ€ä¸ºErrImagePullæˆ–ImagePullBackOff â€¦","ref":"/kubernetes-notes/trouble-shooting/pod-image-error/","tags":["é—®é¢˜æ’æŸ¥"],"title":"é•œåƒæ‹‰å–å¤±è´¥é—®é¢˜"},{"body":"1. ä¸‰è‰²æ ‡è®°æ³• goçš„åƒåœ¾å›æ”¶æœºåˆ¶æ˜¯é€šè¿‡ä¸‰è‰²æ ‡è®°æ³•æ¥å®ç°çš„ï¼Œå…¶ä¸­\né»‘è‰²ï¼šæ²¡æœ‰æŒ‡å‘ï¼ˆå¼•ç”¨ï¼‰ç™½è‰²é›†åˆä¸­çš„ä»»ä½•å¯¹è±¡ ç°è‰²ï¼šå¯èƒ½æŒ‡å‘ï¼ˆå¼•ç”¨ï¼‰ç™½è‰²é›†åˆä¸­çš„æŸäº›å¯¹è±¡ ç™½è‰²ï¼šå‰©ä¸‹çš„éœ€è¦è¢«å›æ”¶çš„å€™é€‰å¯¹è±¡ï¼Œå½“ç°è‰²é›†åˆä¸ºç©ºæ—¶ï¼Œè¡¨ç¤ºç™½è‰²é›†åˆä¸­çš„å¯¹è±¡éƒ½æ²¡æœ‰è¢«å¼•ç”¨ï¼Œé‚£ä¹ˆè¿™äº›å¯¹è±¡å°±å¯ä»¥è¢«å›æ”¶ã€‚ ä¸€ä¸ªåƒåœ¾å›æ”¶å¾ªç¯çš„æ­¥éª¤ï¼š\nå°†æ‰€æœ‰çš„å¯¹è±¡éƒ½æ”¾å…¥ç™½è‰²é›†åˆä¸­ æ‰«ææ‰€æœ‰rootså¯¹è±¡ï¼Œæ”¾å…¥ç°è‰²é›†åˆä¸­ï¼Œrootså¯¹è±¡è¡¨ç¤ºåœ¨åº”ç”¨ä¸­å¯ä»¥è¢«ç›´æ¥è®¿é—®ï¼Œä¸€èˆ¬æ˜¯å…¨å±€å˜é‡å’Œå…¶ä»–åœ¨æ ˆä¸­çš„å¯¹è±¡ã€‚ å°†ç°è‰²é›†åˆä¸­çš„æŸä¸ªå¯¹è±¡æ”¾å…¥é»‘è‰²é›†åˆï¼Œç„¶åæ‰«æè¿™ä¸ªå¯¹è±¡æœ‰å¼•åˆ°åˆ°çš„ç™½è‰²é›†åˆä¸­çš„å¯¹è±¡ï¼Œå°†é‚£äº›ç™½è‰²é›†åˆä¸­å¼•ç”¨åˆ°çš„æ‰€æœ‰å¯¹è±¡æ”¾å…¥ç°è‰²é›†åˆï¼Œä»¥æ­¤ç±»æ¨ï¼Œå°†ç°è‰²é›†åˆä¸­çš„å¯¹è±¡ä¸æ–­æ”¾å…¥é»‘è‰²é›†åˆä¸­ï¼Œç„¶åç™½è‰²é›†åˆä¸­çš„å¯¹è±¡ä¸æ–­æ”¾å…¥ç°è‰²é›†åˆä¸­ã€‚ å½“ç°è‰²é›†åˆä¸­çš„å¯¹è±¡ä¸º0ï¼Œå³éƒ½è¢«æ”¾å…¥åˆ°é»‘è‰²é›†åˆä¸­äº†ï¼Œè¡¨ç¤ºæ²¡æœ‰ä»»ä½•å¯¹è±¡ä¼šå¼•ç”¨åˆ°ç™½è‰²é›†åˆä¸­çš„å¯¹è±¡äº†ï¼Œå› ä¸ºé»‘è‰²é›†åˆå­˜æ”¾ä¸ä¼šå¼•ç”¨ç™½è‰²é›†åˆå¯¹è±¡çš„å…ƒç´ ï¼Œè€Œç°è‰²é›†åˆä¸º0ï¼Œä¹Ÿä¸å­˜åœ¨å¼•ç”¨ç™½è‰²é›†åˆå¯¹è±¡çš„å…ƒç´ ã€‚æ‰€ä»¥ç™½è‰²é›†åˆä¸­çš„å¯¹è±¡å³æ˜¯æ²¡æœ‰è¢«å¼•ç”¨çš„å¯¹è±¡ï¼Œå¯ä»¥å›æ”¶çš„å¯¹è±¡ã€‚ 2. ä¸‰è‰²æ ‡è®°å’Œå†™å±éšœ è¿™æ˜¯è®©æ ‡è®°å’Œâ½¤æˆ·ä»£ç å¹¶å‘çš„åŸºæœ¬ä¿éšœï¼ŒåŸºæœ¬åŸç†ï¼š\nèµ·åˆæ‰€æœ‰å¯¹è±¡éƒ½æ˜¯â½©â¾Šã€‚ æ‰«ææ‰¾å‡ºæ‰€æœ‰å¯è¾¾å¯¹è±¡ï¼Œæ ‡è®°ä¸ºç°â¾Šï¼Œæ”¾â¼Šå¾…å¤„ç†é˜Ÿåˆ—ã€‚ ä»é˜Ÿåˆ—æå–ç°â¾Šå¯¹è±¡ï¼Œå°†å…¶å¼•â½¤å¯¹è±¡æ ‡è®°ä¸ºç°â¾Šæ”¾â¼Šé˜Ÿåˆ—ï¼Œâ¾ƒâ¾æ ‡è®°ä¸ºâ¿Šâ¾Šã€‚ å†™å±éšœç›‘è§†å¯¹è±¡å†…å­˜ä¿®æ”¹ï¼Œé‡æ–°æ ‡â¾Šæˆ–æ”¾å›é˜Ÿåˆ—ã€‚ å½“å®Œæˆå…¨éƒ¨æ‰«æå’Œæ ‡è®°â¼¯ä½œåï¼Œå‰©ä½™ä¸æ˜¯â½©â¾Šå°±æ˜¯â¿Šâ¾Šï¼Œåˆ†åˆ«ä»£è¡¨è¦å¾…å›æ”¶å’Œæ´»è·ƒå¯¹è±¡ï¼Œæ¸…ç†æ“ä½œåªéœ€å°†â½©â¾Šå¯¹è±¡å†…å­˜æ”¶å›å³å¯ã€‚\n3. æ ‡è®°æ¸…é™¤ç®—æ³• (Mark-Sweep) æ ‡è®°-æ¸…é™¤ç®—æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ ‡è®°é˜¶æ®µå’Œæ¸…é™¤é˜¶æ®µã€‚\næ ‡è®°é˜¶æ®µçš„ä»»åŠ¡æ˜¯æ ‡è®°å‡ºæ‰€æœ‰éœ€è¦è¢«å›æ”¶çš„å¯¹è±¡ï¼Œæ¸…é™¤é˜¶æ®µå°±æ˜¯å›æ”¶è¢«æ ‡è®°çš„å¯¹è±¡æ‰€å ç”¨çš„ç©ºé—´ã€‚\nä¼˜ç‚¹æ˜¯ç®€å•ï¼Œå®¹æ˜“å®ç°ã€‚ ç¼ºç‚¹æ˜¯å®¹æ˜“äº§ç”Ÿå†…å­˜ç¢ç‰‡ï¼Œç¢ç‰‡å¤ªå¤šå¯èƒ½ä¼šå¯¼è‡´åç»­è¿‡ç¨‹ä¸­éœ€è¦ä¸ºå¤§å¯¹è±¡åˆ†é…ç©ºé—´æ—¶æ— æ³•æ‰¾åˆ°è¶³å¤Ÿçš„ç©ºé—´è€Œæå‰è§¦å‘æ–°çš„ä¸€æ¬¡åƒåœ¾æ”¶é›†åŠ¨ä½œã€‚ï¼ˆå› ä¸ºæ²¡æœ‰å¯¹ä¸åŒç”Ÿå‘½å‘¨æœŸçš„å¯¹è±¡é‡‡ç”¨ä¸åŒç®—æ³•ï¼Œæ‰€ä»¥ç¢ç‰‡å¤šï¼Œå†…å­˜å®¹æ˜“æ»¡ï¼Œgcé¢‘ç‡é«˜ï¼Œè€—æ—¶ï¼‰\nå¾…å®Œå–„\n","categories":"","description":"","excerpt":"1. ä¸‰è‰²æ ‡è®°æ³• goçš„åƒåœ¾å›æ”¶æœºåˆ¶æ˜¯é€šè¿‡ä¸‰è‰²æ ‡è®°æ³•æ¥å®ç°çš„ï¼Œå…¶ä¸­\né»‘è‰²ï¼šæ²¡æœ‰æŒ‡å‘ï¼ˆå¼•ç”¨ï¼‰ç™½è‰²é›†åˆä¸­çš„ä»»ä½•å¯¹è±¡ ç°è‰²ï¼šå¯èƒ½æŒ‡å‘ï¼ˆå¼•ç”¨ï¼‰ç™½è‰²é›† â€¦","ref":"/golang-notes/principle/garbage-collection/","tags":["Golang"],"title":"åƒåœ¾å›æ”¶"},{"body":" ä»¥ä¸‹å†…å®¹åŸºäºLinuxç³»ç»Ÿï¼Œç‰¹åˆ«ä¸ºUbuntuç³»ç»Ÿ\n1. å®‰è£…kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ ä¸‹è½½æŒ‡å®šç‰ˆæœ¬ï¼Œä¾‹å¦‚ä¸‹è½½v1.9.0ç‰ˆæœ¬\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl \u0026\u0026 chmod +x kubectl \u0026\u0026 sudo mv kubectl /usr/local/bin/ 2. å®‰è£…minikube minikubeçš„æºç åœ°å€ï¼šhttps://github.com/kubernetes/minikube\n2.1 å®‰è£…minikube ä»¥ä¸‹å‘½ä»¤ä¸ºå®‰è£…latestç‰ˆæœ¬çš„minikubeã€‚\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026\u0026 chmod +x minikube \u0026\u0026 sudo mv minikube /usr/local/bin/ å®‰è£…æŒ‡å®šç‰ˆæœ¬å¯åˆ°https://github.com/kubernetes/minikube/releasesä¸‹è½½å¯¹åº”ç‰ˆæœ¬ã€‚\nä¾‹å¦‚ï¼šä»¥ä¸‹ä¸ºå®‰è£…v0.28.2ç‰ˆæœ¬\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-linux-amd64 \u0026\u0026 chmod +x minikube \u0026\u0026 sudo mv minikube /usr/local/bin/ 2.2 minikubeå‘½ä»¤å¸®åŠ© Minikube is a CLI tool that provisions and manages single-node Kubernetes clusters optimized for development workflows. Usage: minikube [command] Available Commands: addons Modify minikube's kubernetes addons cache Add or delete an image from the local cache. completion Outputs minikube shell completion for the given shell (bash or zsh) config Modify minikube config dashboard Opens/displays the kubernetes dashboard URL for your local cluster delete Deletes a local kubernetes cluster docker-env Sets up docker env variables; similar to '$(docker-machine env)' get-k8s-versions Gets the list of Kubernetes versions available for minikube when using the localkube bootstrapper ip Retrieves the IP address of the running cluster logs Gets the logs of the running localkube instance, used for debugging minikube, not user code mount Mounts the specified directory into minikube profile Profile sets the current minikube profile service Gets the kubernetes URL(s) for the specified service in your local cluster ssh Log into or run a command on a machine with SSH; similar to 'docker-machine ssh' ssh-key Retrieve the ssh identity key path of the specified cluster start Starts a local kubernetes cluster status Gets the status of a local kubernetes cluster stop Stops a running local kubernetes cluster update-check Print current and latest version number update-context Verify the IP address of the running cluster in kubeconfig. version Print the version of minikube Flags: --alsologtostderr log to standard error as well as files -b, --bootstrapper string The name of the cluster bootstrapper that will set up the kubernetes cluster. (default \"localkube\") --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --loglevel int Log level (0 = DEBUG, 5 = FATAL) (default 1) --logtostderr log to standard error instead of files -p, --profile string The name of the minikube VM being used. This can be modified to allow for multiple minikube instances to be run independently (default \"minikube\") --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Use \"minikube [command] --help\" for more information about a command. 3. ä½¿ç”¨minikubeå®‰è£…k8sé›†ç¾¤ 3.1. minikube start å¯ä»¥ä»¥Dockerçš„æ–¹å¼è¿è¡Œk8sçš„ç»„ä»¶ï¼Œä½†éœ€è¦å…ˆå®‰è£…Docker(å¯å‚è€ƒDockerå®‰è£…)ï¼Œå¯åŠ¨å‚æ•°ä½¿ç”¨--vm-driver=noneã€‚\nminikube start --vm-driver=none ä¾‹å¦‚ï¼š\nroot@ubuntu:~# minikube start --vm-driver=none Starting local Kubernetes v1.10.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Downloading kubeadm v1.10.0 Downloading kubelet v1.10.0 ^[[DFinished Downloading kubelet v1.10.0 Finished Downloading kubeadm v1.10.0 Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. =================== WARNING: IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacks When using the none driver, the kubectl config and credentials generated will be root owned and will appear in the root home directory. You will need to move the files to the appropriate location and then set the correct permissions. An example of this is below: sudo mv /root/.kube $HOME/.kube # this will write over any previous configuration sudo chown -R $USER $HOME/.kube sudo chgrp -R $USER $HOME/.kube sudo mv /root/.minikube $HOME/.minikube # this will write over any previous configuration sudo chown -R $USER $HOME/.minikube sudo chgrp -R $USER $HOME/.minikube This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true Loading cached images from config file. å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„kubernetesé›†ç¾¤\n# æŸ¥é˜…ç‰ˆæœ¬ minikube get-k8s-versions # é€‰æ‹©ç‰ˆæœ¬å¯åŠ¨ minikube start --kubernetes-version v1.7.3 --vm-driver=none 3.2. minikube status $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 172.16.94.139 3.3. minikube stop minikube stop å‘½ä»¤å¯ä»¥ç”¨æ¥åœæ­¢é›†ç¾¤ã€‚ è¯¥å‘½ä»¤ä¼šå…³é—­ minikube è™šæ‹Ÿæœºï¼Œä½†å°†ä¿ç•™æ‰€æœ‰é›†ç¾¤çŠ¶æ€å’Œæ•°æ®ã€‚ å†æ¬¡å¯åŠ¨é›†ç¾¤å°†æ¢å¤åˆ°ä¹‹å‰çš„çŠ¶æ€ã€‚\n3.4. minikube delete minikube delete å‘½ä»¤å¯ä»¥ç”¨æ¥åˆ é™¤é›†ç¾¤ã€‚ è¯¥å‘½ä»¤å°†å…³é—­å¹¶åˆ é™¤ minikube è™šæ‹Ÿæœºã€‚æ²¡æœ‰æ•°æ®æˆ–çŠ¶æ€ä¼šè¢«ä¿å­˜ä¸‹æ¥ã€‚\n4. æŸ¥çœ‹éƒ¨ç½²ç»“æœ 4.1. éƒ¨ç½²ç»„ä»¶ root@ubuntu:~# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE pod/etcd-minikube 1/1 Running 0 38m pod/kube-addon-manager-minikube 1/1 Running 0 38m pod/kube-apiserver-minikube 1/1 Running 1 39m pod/kube-controller-manager-minikube 1/1 Running 0 38m pod/kube-dns-86f4d74b45-bdfnx 3/3 Running 0 38m pod/kube-proxy-dqdvg 1/1 Running 0 38m pod/kube-scheduler-minikube 1/1 Running 0 38m pod/kubernetes-dashboard-5498ccf677-c2gnh 1/1 Running 0 38m pod/storage-provisioner 1/1 Running 0 38m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP 38m service/kubernetes-dashboard NodePort 10.104.48.227 \u003cnone\u003e 80:30000/TCP 38m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/kube-proxy 1 1 1 1 1 \u003cnone\u003e 38m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/kube-dns 1 1 1 1 38m deployment.apps/kubernetes-dashboard 1 1 1 1 38m NAME DESIRED CURRENT READY AGE replicaset.apps/kube-dns-86f4d74b45 1 1 1 38m replicaset.apps/kubernetes-dashboard-5498ccf677 1 1 1 38m 4.2. dashboard é€šè¿‡è®¿é—®ip:portï¼Œä¾‹å¦‚ï¼šhttp://172.16.94.139:30000/ï¼Œå¯ä»¥è®¿é—®k8sçš„dashboardæ§åˆ¶å°ã€‚\n\u003cimg src=\"http://res.cloudinary.com/dqxtn0ick/image/upload/v1533695750/article/kubernetes/arch/dashboard.png\"Â width =Â \"100%\"/\u003e\n5. troubleshooting 5.1. æ²¡æœ‰å®‰è£…VirtualBox [root@minikube ~]# minikube start Starting local Kubernetes v1.10.0 cluster... Starting VM... Downloading Minikube ISO 160.27 MB / 160.27 MB [============================================] 100.00% 0s E0727 15:47:08.655647 9407 start.go:174] Error starting host: Error creating host: Error executing step: Running precreate checks. : VBoxManage not found. Make sure VirtualBox is installed and VBoxManage is in the path. Retrying. E0727 15:47:08.656994 9407 start.go:180] Error starting host: Error creating host: Error executing step: Running precreate checks. : VBoxManage not found. Make sure VirtualBox is installed and VBoxManage is in the path ================================================================================ An error has occurred. Would you like to opt in to sending anonymized crash information to minikube to help prevent future errors? To opt out of these messages, run the command: minikube config set WantReportErrorPrompt false ================================================================================ Please enter your response [Y/n]: è§£å†³æ–¹æ³•ï¼Œå…ˆå®‰è£…VirtualBoxã€‚\n5.2. æ²¡æœ‰å®‰è£…Docker [root@minikube ~]# minikube start --vm-driver=none Starting local Kubernetes v1.10.0 cluster... Starting VM... E0727 15:56:54.936706 9441 start.go:174] Error starting host: Error creating host: Error executing step: Running precreate checks. : docker cannot be found on the path for this machine. A docker installation is a requirement for using the none driver: exec: \"docker\": executable file not found in $PATH. Retrying. E0727 15:56:54.938930 9441 start.go:180] Error starting host: Error creating host: Error executing step: Running precreate checks. : docker cannot be found on the path for this machine. A docker installation is a requirement for using the none driver: exec: \"docker\": executable file not found in $PATH è§£å†³æ–¹æ³•ï¼Œå…ˆå®‰è£…Dockerã€‚\næ–‡ç« å‚è€ƒï¼š\nhttps://github.com/kubernetes/minikube\nhttps://kubernetes.io/docs/setup/minikube/\nhttps://kubernetes.io/docs/tasks/tools/install-minikube/\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl/\n","categories":"","description":"","excerpt":" ä»¥ä¸‹å†…å®¹åŸºäºLinuxç³»ç»Ÿï¼Œç‰¹åˆ«ä¸ºUbuntuç³»ç»Ÿ\n1. å®‰è£…kubectl curl -LO â€¦","ref":"/kubernetes-notes/setup/installer/install-k8s-by-minikube/","tags":["Kubernetes"],"title":"ä½¿ç”¨minikubeå®‰è£…kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/basis/","tags":"","title":"é¡ºåºç¼–ç¨‹"},{"body":" TODO\nå‚è€ƒï¼š\næ›´æ–°ï¼šç§»é™¤ Dockershim çš„å¸¸è§é—®é¢˜ | Kubernetes\nåˆ«æ…Œ: Kubernetes å’Œ Docker | Kubernetes\nå…³äº dockershim ç§»é™¤å’Œä½¿ç”¨å…¼å®¹ CRI è¿è¡Œæ—¶çš„æ–‡ç«  | Kubernetes\nKEP-2221: Removing dockershim from kubelete-dockershim\nDockershim removal feedback \u0026 issues å°† Docker Engine èŠ‚ç‚¹ä» dockershim è¿ç§»åˆ° cri-dockerd | Kubernetes\næŸ¥æ˜èŠ‚ç‚¹ä¸Šæ‰€ä½¿ç”¨çš„å®¹å™¨è¿è¡Œæ—¶ | Kubernetes\n","categories":"","description":"","excerpt":" TODO\nå‚è€ƒï¼š\næ›´æ–°ï¼šç§»é™¤ Dockershim çš„å¸¸è§é—®é¢˜ | Kubernetes\nåˆ«æ…Œ: Kubernetes å’Œ Docker â€¦","ref":"/kubernetes-notes/runtime/containerd/remove-dockershim/","tags":"","title":"ç§»é™¤Dockershim"},{"body":"Resource Quality of Service 1. èµ„æºQoSç®€ä»‹ requestå€¼è¡¨ç¤ºå®¹å™¨ä¿è¯å¯è¢«åˆ†é…åˆ°èµ„æºã€‚limitè¡¨ç¤ºå®¹å™¨å¯å…è®¸ä½¿ç”¨çš„æœ€å¤§èµ„æºã€‚Podçº§åˆ«çš„requestå’Œlimitæ˜¯å…¶æ‰€æœ‰å®¹å™¨çš„requestå’Œlimitä¹‹å’Œã€‚\n2. Requests and Limits Podå¯ä»¥æŒ‡å®šrequestå’Œlimitèµ„æºã€‚å…¶ä¸­0 \u003c= request \u003c=Node Allocatable \u0026 request \u003c= limit \u003c= Infinityã€‚è°ƒåº¦æ˜¯åŸºäºrequestè€Œä¸æ˜¯limitï¼Œå³å¦‚æœPodè¢«æˆåŠŸè°ƒåº¦ï¼Œé‚£ä¹ˆå¯ä»¥ä¿è¯Podåˆ†é…åˆ°æŒ‡å®šçš„ requestçš„èµ„æºã€‚Podä½¿ç”¨çš„èµ„æºèƒ½å¦è¶…è¿‡æŒ‡å®šçš„limitå€¼å–å†³äºè¯¥èµ„æºæ˜¯å¦å¯è¢«å‹ç¼©ã€‚\n2.1. å¯å‹ç¼©çš„èµ„æº ç›®å‰åªæ”¯æŒCPU podå¯ä»¥ä¿è¯è·å¾—å®ƒä»¬è¯·æ±‚çš„CPUæ•°é‡ï¼Œå®ƒä»¬å¯èƒ½ä¼šä¹Ÿå¯èƒ½ä¸ä¼šè·å¾—é¢å¤–çš„CPUæ—¶é—´(å–å†³äºæ­£åœ¨è¿è¡Œçš„å…¶ä»–ä½œä¸š)ã€‚å› ä¸ºç›®å‰CPUéš”ç¦»æ˜¯åœ¨å®¹å™¨çº§åˆ«è€Œä¸æ˜¯podçº§åˆ«ã€‚ 2.2. ä¸å¯å‹ç¼©çš„èµ„æº ç›®å‰åªæ”¯æŒå†…å­˜ podå°†è·å¾—å®ƒä»¬è¯·æ±‚çš„å†…å­˜æ•°é‡ï¼Œå¦‚æœè¶…è¿‡äº†å®ƒä»¬çš„å†…å­˜è¯·æ±‚ï¼Œå®ƒä»¬å¯èƒ½ä¼šè¢«æ€æ­»(å¦‚æœå…¶ä»–ä¸€äº›podéœ€è¦å†…å­˜)ï¼Œä½†å¦‚æœpodæ¶ˆè€—çš„å†…å­˜å°äºè¯·æ±‚çš„å†…å­˜ï¼Œé‚£ä¹ˆå®ƒä»¬å°†ä¸ä¼šè¢«æ€æ­»(é™¤éåœ¨ç³»ç»Ÿä»»åŠ¡æˆ–å®ˆæŠ¤è¿›ç¨‹éœ€è¦æ›´å¤šå†…å­˜çš„æƒ…å†µä¸‹)ã€‚ 3. QoS çº§åˆ« åœ¨æœºå™¨èµ„æºè¶…å–çš„æƒ…å†µä¸‹ï¼ˆlimitçš„æ€»é‡å¤§äºæœºå™¨çš„èµ„æºå®¹é‡ï¼‰ï¼Œå³CPUæˆ–å†…å­˜è€—å°½ï¼Œå°†ä¸å¾—ä¸æ€æ­»éƒ¨åˆ†ä¸é‡è¦çš„å®¹å™¨ã€‚å› æ­¤å¯¹å®¹å™¨åˆ†æˆäº†3ä¸ªQoSçš„çº§åˆ«ï¼šGuaranteed, Burstable, Best-Effortï¼Œä¸‰ä¸ªçº§åˆ«çš„ä¼˜å…ˆçº§ä¾æ¬¡é€’å‡ã€‚\nå½“CPUèµ„æºæ— æ³•æ»¡è¶³ï¼Œpodä¸ä¼šè¢«æ€æ­»å¯èƒ½è¢«çŸ­æš‚æ§åˆ¶ã€‚\nå†…å­˜æ˜¯ä¸å¯å‹ç¼©çš„èµ„æºï¼Œå½“å†…å­˜è€—å°½çš„æƒ…å†µä¸‹ï¼Œä¼šä¾æ¬¡æ€æ­»ä¼˜å…ˆçº§ä½çš„å®¹å™¨ã€‚Guaranteedçš„çº§åˆ«æœ€é«˜ï¼Œä¸ä¼šè¢«æ€æ­»ï¼Œé™¤éå®¹å™¨ä½¿ç”¨é‡è¶…è¿‡limité™å€¼æˆ–è€…èµ„æºè€—å°½ï¼Œå·²ç»æ²¡æœ‰æ›´ä½çº§åˆ«çš„å®¹å™¨å¯é©±é€ã€‚\n3.1. Guaranteed æ‰€æœ‰çš„å®¹å™¨çš„limitå€¼å’Œrequestå€¼è¢«é…ç½®ä¸”ä¸¤è€…ç›¸ç­‰ï¼ˆå¦‚æœåªé…ç½®limitæ²¡æœ‰requestï¼Œåˆ™requestå–å€¼äºlimitï¼‰ã€‚\nä¾‹å¦‚ï¼š\n# ç¤ºä¾‹1 containers: name: foo resources: limits: cpu: 10m memory: 1Gi name: bar resources: limits: cpu: 100m memory: 100Mi # ç¤ºä¾‹2 containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar resources: limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi 3.2. Burstable å¦‚æœä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨çš„limitå’Œrequestå€¼è¢«é…ç½®ä¸”ä¸¤è€…ä¸ç›¸ç­‰ã€‚\nä¾‹å¦‚ï¼š\n# ç¤ºä¾‹1 containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar # ç¤ºä¾‹2 containers: name: foo resources: limits: memory: 1Gi name: bar resources: limits: cpu: 100m # ç¤ºä¾‹3 containers: name: foo resources: requests: cpu: 10m memory: 1Gi name: bar 3.3. Best-Effort æ‰€æœ‰çš„å®¹å™¨çš„limitå’Œrequestå€¼éƒ½æ²¡æœ‰é…ç½®ã€‚\nä¾‹å¦‚ï¼š\ncontainers: name: foo resources: name: bar resources: å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md ","categories":"","description":"","excerpt":"Resource Quality of Service 1. èµ„æºQoSç®€ä»‹ requestå€¼è¡¨ç¤ºå®¹å™¨ä¿è¯å¯è¢«åˆ†é…åˆ°èµ„æºã€‚limitè¡¨ç¤ºå®¹å™¨ â€¦","ref":"/kubernetes-notes/resource/quality-of-service/","tags":["Kubernetes"],"title":"èµ„æºæœåŠ¡è´¨é‡"},{"body":"å­—ç¬¦ä¸²å¤„ç† å­—ç¬¦ä¸²æ“ä½œæ¶‰åŠçš„æ ‡å‡†åº“æœ‰stringså’Œstrconvä¸¤ä¸ªåŒ…\n1. å­—ç¬¦ä¸²æ“ä½œ å‡½æ•° è¯´æ˜ func Contains(s, substr string) bool å­—ç¬¦ä¸² s ä¸­æ˜¯å¦åŒ…å« substrï¼Œè¿”å› bool å€¼ func Join(a []string, sep string) string å­—ç¬¦ä¸²é“¾æ¥ï¼ŒæŠŠ slice a é€šè¿‡ sep é“¾æ¥èµ·æ¥ func Index(s, sep string) int åœ¨å­—ç¬¦ä¸² s ä¸­æŸ¥æ‰¾ sep æ‰€åœ¨çš„ä½ç½®ï¼Œè¿”å›ä½ç½®å€¼ï¼Œæ‰¾ä¸åˆ°è¿”å›-1 func Repeat(s string, count int) string é‡å¤ s å­—ç¬¦ä¸² count æ¬¡ï¼Œæœ€åè¿”å›é‡å¤çš„å­—ç¬¦ä¸² func Replace(s, old, new string, n int) string åœ¨ s å­—ç¬¦ä¸²ä¸­ï¼ŒæŠŠ old å­—ç¬¦ä¸²æ›¿æ¢ä¸º new å­—ç¬¦ä¸²ï¼Œn è¡¨ç¤ºæ›¿æ¢çš„æ¬¡æ•°ï¼Œå°äº 0 è¡¨ç¤ºå…¨éƒ¨æ›¿æ¢ func Split(s, sep string) []string æŠŠ s å­—ç¬¦ä¸²æŒ‰ç…§ sep åˆ†å‰²ï¼Œè¿”å› slice func Trim(s string, cutset string) string åœ¨ s å­—ç¬¦ä¸²ä¸­å»é™¤ cutset æŒ‡å®šçš„å­—ç¬¦ä¸² func Fields(s string) []string å»é™¤ s å­—ç¬¦ä¸²çš„ç©ºæ ¼ç¬¦ï¼Œå¹¶ä¸”æŒ‰ç…§ç©ºæ ¼åˆ†å‰²è¿”å› slice 2. å­—ç¬¦ä¸²è½¬æ¢ 1ã€Append ç³»åˆ—å‡½æ•°å°†æ•´æ•°ç­‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²åï¼Œæ·»åŠ åˆ°ç°æœ‰çš„å­—èŠ‚æ•°ç»„ä¸­\npackage main import ( \"fmt\" \"strconv\" ) func main() { str := make([]byte, 0, 100) str = strconv.AppendInt(str, 4567, 10) str = strconv.AppendBool(str, false) str = strconv.AppendQuote(str, \"abcdefg\") str = strconv.AppendQuoteRune(str, 'å•') fmt.Println(string(str)) } 2ã€Format ç³»åˆ—å‡½æ•°æŠŠå…¶ä»–ç±»å‹çš„è½¬æ¢ä¸ºå­—ç¬¦ä¸²\npackage main import ( \"fmt\" \"strconv\" ) func main() { a := strconv.FormatBool(false) b := strconv.FormatFloat(123.23, 'g', 12, 64) c := strconv.FormatInt(1234, 10) d := strconv.FormatUint(12345, 10) e := strconv.Itoa(1023) fmt.Println(a, b, c, d, e) } 3ã€Parse ç³»åˆ—å‡½æ•°æŠŠå­—ç¬¦ä¸²è½¬æ¢ä¸ºå…¶ä»–ç±»å‹\npackage main import ( \"fmt\" \"strconv\" ) func main() { a, err := strconv.ParseBool(\"false\") if err != nil { fmt.Println(err) } b, err := strconv.ParseFloat(\"123.23\", 64) if err != nil { fmt.Println(err) } c, err := strconv.ParseInt(\"1234\", 10, 64) if err != nil { fmt.Println(err) } d, err := strconv.ParseUint(\"12345\", 10, 64) if err != nil { fmt.Println(err) } e, err := strconv.Itoa(\"1023\") if err != nil { fmt.Println(err) } fmt.Println(a, b, c, d, e) } ","categories":"","description":"","excerpt":"å­—ç¬¦ä¸²å¤„ç† å­—ç¬¦ä¸²æ“ä½œæ¶‰åŠçš„æ ‡å‡†åº“æœ‰stringså’Œstrconvä¸¤ä¸ªåŒ…\n1. å­—ç¬¦ä¸²æ“ä½œ å‡½æ•° è¯´æ˜ func Contains(s, â€¦","ref":"/golang-notes/text/string/","tags":["Golang"],"title":"å­—ç¬¦ä¸²å¤„ç†"},{"body":"æœ¬æ–‡ä¸»è¦è¯´æ˜å¦‚ä½•ç”¨ Go å®ç°ç®€å•çš„æœ¬åœ°ç¼“å­˜ç³»ç»Ÿ SDKã€‚\n1. ç›®æ ‡ç‰¹æ€§ âœ… é€šç”¨ç¼“å­˜æ¥å£ï¼ˆSet/Get/Deleteï¼‰\nâœ… æ”¯æŒ TTLï¼ˆè¿‡æœŸè‡ªåŠ¨æ¸…é™¤ï¼‰\nâœ… å¹¶å‘å®‰å…¨ï¼ˆæ”¯æŒé«˜å¹¶å‘è¯»å†™ï¼‰\nâœ… æ˜“äºé›†æˆåˆ°ä»»æ„å¹³å°ï¼ˆå°è£…ä¸ºæ¨¡å—ï¼‰\n2. ç®€æ˜“ç‰ˆå®ç° localcache/ â”œâ”€â”€ cache.go # å¤–éƒ¨å¯ç”¨æ¥å£ â””â”€â”€ entry.go # ç¼“å­˜æ¡ç›®å®šä¹‰ âœ… 1. ç¼“å­˜æ¡ç›®å®šä¹‰ entry.go package localcache import \"time\" type entry struct { value interface{} expireAt time.Time hasExpired bool } func (e *entry) isExpired() bool { if !e.hasExpired { return false } return time.Now().After(e.expireAt) } âœ… 2. ç¼“å­˜å®ç° cache.go package localcache import ( \"sync\" \"time\" ) type Cache struct { items map[string]*entry mu sync.RWMutex ttl time.Duration quit chan struct{} } func NewCache(defaultTTL time.Duration) *Cache { c := \u0026Cache{ items: make(map[string]*entry), ttl: defaultTTL, quit: make(chan struct{}), } go c.cleanupLoop() return c } func (c *Cache) Set(key string, value interface{}, ttl ...time.Duration) { exp := c.ttl if len(ttl) \u003e 0 { exp = ttl[0] } c.mu.Lock() defer c.mu.Unlock() c.items[key] = \u0026entry{ value: value, expireAt: time.Now().Add(exp), hasExpired: exp \u003e 0, } } func (c *Cache) Get(key string) (interface{}, bool) { c.mu.RLock() defer c.mu.RUnlock() item, ok := c.items[key] if !ok || item.isExpired() { return nil, false } return item.value, true } func (c *Cache) Delete(key string) { c.mu.Lock() defer c.mu.Unlock() delete(c.items, key) } func (c *Cache) cleanupLoop() { ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for { select { case \u003c-ticker.C: c.cleanExpired() case \u003c-c.quit: return } } } func (c *Cache) cleanExpired() { c.mu.Lock() defer c.mu.Unlock() now := time.Now() for k, v := range c.items { if v.hasExpired \u0026\u0026 now.After(v.expireAt) { delete(c.items, k) } } } func (c *Cache) Close() { close(c.quit) } âœ… 3. å¦‚ä½•ä½¿ç”¨ cache := localcache.NewCache(10 * time.Minute) defer cache.Close() cache.Set(\"username\", \"huweihuang\") value, found := cache.Get(\"username\") if found { fmt.Println(\"Got:\", value) } 3. å¢å¼ºç‰ˆå®ç° åŸºäºæœ€å¤§å®¹é‡çš„ç¼“å­˜ï¼Œå¹¶ä½¿ç”¨ LRUï¼ˆLeast Recently Usedï¼Œæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ç­–ç•¥è¿›è¡Œæ·˜æ±°ã€‚\næˆ‘ä»¬åœ¨å‰é¢æä¾›çš„ Cache å®ç°åŸºç¡€ä¸Šï¼Œå¼•å…¥ åŒå‘é“¾è¡¨ + map æ¥å®ç°é«˜æ•ˆçš„ O(1) æ—¶é—´å¤æ‚åº¦çš„ LRU ç¼“å­˜æ·˜æ±°æœºåˆ¶ã€‚\nâœ… å¢å¼ºç›®æ ‡ âœ… æ”¯æŒæœ€å¤§å®¹é‡é™åˆ¶ maxEntries\nâœ… æœ€è¿‘ä½¿ç”¨çš„ç¼“å­˜é¡¹ä¿ç•™ï¼Œæœ€ä¹…æœªä½¿ç”¨çš„ä¼˜å…ˆè¢«æ·˜æ±°\nâœ… å¹¶å‘å®‰å…¨ã€æ”¯æŒ TTL æ¸…ç†\nâœ… Get / Set éƒ½æ›´æ–°è®¿é—®æ—¶é—´\nâœ… å®ç°ç»“æ„ LRUCache â”œâ”€â”€ map[string]*list.Element // å¿«é€ŸæŸ¥æ‰¾ â”œâ”€â”€ list.List // ä¿å­˜è®¿é—®é¡ºåºï¼Œå¤´æ˜¯æœ€è¿‘ä½¿ç”¨ï¼Œå°¾æ˜¯æœ€ä¹…æœªä½¿ç”¨ â””â”€â”€ entry { key, value, expireAt } âœ… LRU ç¼“å­˜ç»“æ„ä»£ç  package lru import ( \"container/list\" \"sync\" \"time\" ) type entry struct { key string value interface{} expireAt time.Time ttlSet bool } type LRUCache struct { capacity int mu sync.Mutex ll *list.List cache map[string]*list.Element ttl time.Duration } func NewLRUCache(capacity int, defaultTTL time.Duration) *LRUCache { return \u0026LRUCache{ capacity: capacity, ll: list.New(), cache: make(map[string]*list.Element), ttl: defaultTTL, } } âœ… æ ¸å¿ƒæ“ä½œé€»è¾‘ Set\nfunc (c *LRUCache) Set(key string, value interface{}, ttl ...time.Duration) { c.mu.Lock() defer c.mu.Unlock() if ele, ok := c.cache[key]; ok { // update existing en := ele.Value.(*entry) en.value = value en.expireAt = c.getExpireAt(ttl) en.ttlSet = true c.ll.MoveToFront(ele) return } // new entry en := \u0026entry{ key: key, value: value, expireAt: c.getExpireAt(ttl), ttlSet: true, } ele := c.ll.PushFront(en) c.cache[key] = ele if c.ll.Len() \u003e c.capacity { c.removeOldest() } } func (c *LRUCache) getExpireAt(ttl []time.Duration) time.Time { d := c.ttl if len(ttl) \u003e 0 { d = ttl[0] } if d == 0 { return time.Time{} } return time.Now().Add(d) } Get\nfunc (c *LRUCache) Get(key string) (interface{}, bool) { c.mu.Lock() defer c.mu.Unlock() ele, ok := c.cache[key] if !ok { return nil, false } en := ele.Value.(*entry) if en.ttlSet \u0026\u0026 !en.expireAt.IsZero() \u0026\u0026 time.Now().After(en.expireAt) { c.removeElement(ele) return nil, false } c.ll.MoveToFront(ele) return en.value, true } removeOldest \u0026 removeElement\nfunc (c *LRUCache) removeOldest() { ele := c.ll.Back() if ele != nil { c.removeElement(ele) } } func (c *LRUCache) removeElement(e *list.Element) { en := e.Value.(*entry) delete(c.cache, en.key) c.ll.Remove(e) } âœ… ä½¿ç”¨æ–¹å¼ç¤ºä¾‹ cache := lru.NewLRUCache(100, 5*time.Minute) cache.Set(\"a\", \"1\") cache.Set(\"b\", \"2\") val, ok := cache.Get(\"a\") // ä¼šå˜ä¸ºâ€œæœ€è¿‘ä½¿ç”¨â€ 4. åŸç† è¦ç†è§£è¿™ä¸ª Go å®ç°çš„ æœ¬åœ° LRU ç¼“å­˜ç³»ç»Ÿ çš„åŸç†ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸¤éƒ¨åˆ†æ¥è®²ï¼šåŸºæœ¬æ€æƒ³ å’Œ æ ¸å¿ƒæ•°æ®ç»“æ„è®¾è®¡ã€‚\nğŸ”§ 4.1. åŸºæœ¬åŸç†ï¼ˆé€»è¾‘æµç¨‹ï¼‰ ğŸ“Œ ç¼“å­˜çš„æœ¬è´¨ï¼š æŠŠçƒ­ç‚¹æ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œæé«˜è¯»å–é€Ÿåº¦ã€‚\næ¯ä¸ªç¼“å­˜é¡¹å¯èƒ½æœ‰ã€Œç”Ÿå‘½å‘¨æœŸã€ï¼ˆTTLï¼‰ã€‚\nå½“ç¼“å­˜ç©ºé—´æ»¡äº†ï¼Œéœ€è¦æ·˜æ±°æ—§çš„æ•°æ®ã€‚\nğŸ“Œ LRUï¼ˆLeast Recently Usedï¼‰ç­–ç•¥ï¼š å«ä¹‰ï¼šä¼˜å…ˆæ·˜æ±°æœ€ä¹…æ²¡æœ‰è¢«è®¿é—®è¿‡çš„ç¼“å­˜é¡¹ã€‚\né€‚ç”¨åœºæ™¯ï¼šè®¿é—®æ•°æ®å…·æœ‰å±€éƒ¨æ€§åŸåˆ™ï¼ˆè¿‘æœŸä½¿ç”¨çš„æ•°æ®å¯èƒ½ä¼šå†æ¬¡è¢«è®¿é—®ï¼‰ã€‚\nğŸ“Œ æ€ä¹ˆå®ç° LRU + TTLï¼š åŠŸèƒ½ å®ç°æ–¹æ³• O(1) è®¿é—® ç”¨ map[string]*list.Element å¿«é€ŸæŸ¥æ‰¾ key O(1) æ›´æ–°è®¿é—®é¡ºåº ç”¨ container/list åŒå‘é“¾è¡¨ï¼ˆLRU é˜Ÿåˆ—ï¼‰ O(1) æ·˜æ±°æœ€æ—§é¡¹ ä»é“¾è¡¨å°¾éƒ¨ç›´æ¥åˆ é™¤ TTL è¿‡æœŸ æ¯ä¸ª entry å­˜ expireAt æ—¶é—´æˆ³ï¼Œè®¿é—®æ—¶æ£€æŸ¥ ğŸ§  4.2. æ ¸å¿ƒæ•°æ®ç»“æ„è®¾è®¡ 1. Map + åŒå‘é“¾è¡¨ LRU é˜Ÿåˆ—ï¼ˆæœ€è¿‘è®¿é—®çš„æ”¾å¤´éƒ¨ï¼‰ï¼š [Most Recently Used] ---\u003e [Middle] ---\u003e [Least Recently Used] â†‘ â†‘ list.Front() list.Back() type LRUCache struct { ll *list.List // åŒå‘é“¾è¡¨ï¼šè®°å½•è®¿é—®é¡ºåº cache map[string]*list.Element // å¿«é€Ÿå®šä½ key -\u003e é“¾è¡¨èŠ‚ç‚¹ } 2. æ¯ä¸ªç¼“å­˜ entry åŒ…å«ä¿¡æ¯ type entry struct { key string value interface{} expireAt time.Time // TTL è¿‡æœŸæ—¶é—´ ttlSet bool // æ˜¯å¦è®¾ç½®äº† TTL } âš™ï¸ 4.3. å·¥ä½œæµç¨‹ä¸¾ä¾‹ 1. æ·»åŠ å…ƒç´  Set(\"a\", 1)ï¼š å¦‚æœ a å·²å­˜åœ¨ï¼šæ›´æ–°å€¼ï¼Œç§»åŠ¨åˆ°é“¾è¡¨å¤´éƒ¨ã€‚\nå¦‚æœä¸å­˜åœ¨ï¼š\nåˆ›å»ºæ–° entryï¼Œæ”¾åˆ°é“¾è¡¨å¤´éƒ¨ã€‚\nå¦‚æœè¶…è¿‡æœ€å¤§å®¹é‡ï¼šåˆ é™¤é“¾è¡¨å°¾éƒ¨å…ƒç´ ã€‚\n2. è®¿é—®å…ƒç´  Get(\"a\")ï¼š æ‰¾åˆ° a å¯¹åº”çš„é“¾è¡¨èŠ‚ç‚¹ã€‚\nå¦‚æœ TTL æœªè¿‡æœŸï¼Œç§»åŠ¨å®ƒåˆ°é“¾è¡¨å¤´éƒ¨ï¼ˆè¡¨ç¤ºæœ€è¿‘ä½¿ç”¨ï¼‰ã€‚\n3. æ·˜æ±°ç­–ç•¥ï¼š æ·»åŠ æ–°å…ƒç´ æ—¶ï¼Œå¦‚æœè¶…è¿‡å®¹é‡ï¼Œå°±æ‰§è¡Œï¼š removeElement(list.Back()) // æ·˜æ±°æœ€ä¹…æœªç”¨çš„ç¼“å­˜é¡¹ ğŸ›¡ï¸ 4.4. å¹¶å‘å®‰å…¨ æ‰€æœ‰æ“ä½œéƒ½ä½¿ç”¨ sync.Mutex åŠ é”ã€‚\nç¡®ä¿åœ¨å¤š goroutine ä¸‹ä¸ä¼šå‡ºç°ç«äº‰çŠ¶æ€ã€‚\nâœ… æ€»ç»“ä¸€å¥è¯ï¼š è¿™ä¸ªç¼“å­˜ç³»ç»Ÿæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªã€Œå“ˆå¸Œè¡¨ + åŒå‘é“¾è¡¨ã€çš„ç»„åˆï¼Œç”¨äºå¿«é€Ÿå®šä½ + å¿«é€Ÿç»´æŠ¤è®¿é—®é¡ºåºï¼Œå¹¶ç»“åˆ TTL ä¿è¯ç¼“å­˜ä¸è¿‡æœŸä¸”é«˜æ•ˆæ¸…ç†ã€‚\n","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦è¯´æ˜å¦‚ä½•ç”¨ Go å®ç°ç®€å•çš„æœ¬åœ°ç¼“å­˜ç³»ç»Ÿ SDKã€‚\n1. ç›®æ ‡ç‰¹æ€§ âœ… é€šç”¨ç¼“å­˜æ¥å£ï¼ˆSet/Get/Deleteï¼‰\nâœ… â€¦","ref":"/golang-notes/web/go-cache/","tags":["Golang"],"title":"Goå®ç°æœ¬åœ°ç¼“å­˜ç³»ç»Ÿ"},{"body":"æœ¬æ–‡ä¸»è¦æè¿°å¦‚ä½•é’ˆå¯¹ç‰©ç†æœºåšç¡¬ä»¶çš„raidé…ç½®ï¼Œç¡¬ä»¶raidä¸ä¾èµ–äºæ“ä½œç³»ç»Ÿï¼Œå…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œç»å¸¸åœ¨è£…æœºç³»ç»Ÿä¸­ä½¿ç”¨åˆ°ã€‚\n1. æŸ¥è¯¢raidæ§åˆ¶å™¨ä¿¡æ¯ å¯ä»¥é€šè¿‡lspci -mmå‘½ä»¤çš„è¾“å‡ºæŸ¥æ‰¾raidæ§åˆ¶å™¨ä¿¡æ¯ã€‚å®é™…æƒ…å†µå¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\nlspci -mm |egrep \"Broadcom / LSI|Adaptec\" ç¤ºä¾‹ï¼š\n# lspci -mm |egrep \"Broadcom / LSI|Adaptec\" 3b:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID SAS-3 3108 [Invader]\" -r02 -p00 \"Dell\" \"PERC H730P Mini\" lspci -mm è¾“å‡ºä¸­çš„æ¯ä¸€åˆ—å«ä¹‰å¦‚ä¸‹ï¼š\nPCI åœ°å€ï¼šè®¾å¤‡çš„æ€»çº¿å·ã€è®¾å¤‡å·å’ŒåŠŸèƒ½å·ï¼Œç”¨äºå”¯ä¸€æ ‡è¯†è®¾å¤‡ä½ç½®ã€‚ä¾‹å¦‚ï¼š3b:00.0 è®¾å¤‡ç±»å‹ï¼šè®¾å¤‡çš„åŠŸèƒ½ç±»åˆ«ï¼Œä¾‹å¦‚ RAID æ§åˆ¶å™¨ã€ç½‘ç»œæ§åˆ¶å™¨ç­‰ã€‚ä¾‹å¦‚ï¼š \"RAID bus controller\" ,\"Serial Attached SCSI controller\" åˆ¶é€ å•†ï¼šè®¾å¤‡çš„åˆ¶é€ å•†åç§°ã€‚ä¾‹å¦‚ï¼š\"Broadcom / LSI\", \"Adaptec\"ï¼Œæœ€å…³é”®çš„ä¿¡æ¯ï¼Œå†³å®šraidå‘½ä»¤ã€‚ è®¾å¤‡å‹å·ï¼šè®¾å¤‡çš„å…·ä½“äº§å“å‹å·å’Œåç§°ã€‚ä¾‹å¦‚ï¼š\"MegaRAID SAS-3 3108 [Invader]\",\"Smart Storage PQI SAS\" å­ç³»ç»Ÿåˆ¶é€ å•†ï¼šå­ç³»ç»Ÿçš„ç”Ÿäº§å‚å•†ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ã€‚ä¾‹å¦‚ï¼š\"Dell\",\"Huawei\",\"Lenovo\" å­ç³»ç»Ÿè®¾å¤‡åç§°ï¼šå­ç³»ç»Ÿçš„è®¾å¤‡åç§°ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ã€‚ä¾‹å¦‚ï¼š\"PERC H730P Mini\" ä»¥ä¸‹æ˜¯å¸¸è§çš„å‡ ä¸ªç‰©ç†æœºå‚å•†çš„è®¾å¤‡çš„raidä¿¡æ¯:\né€šè¿‡æ‰§è¡Œlspci -mm |egrep \"Broadcom / LSI|Adaptec\"å‘½ä»¤å¯ä»¥æŸ¥çœ‹ä¸åŒå‚å•†è®¾å¤‡çš„raidä¿¡æ¯ã€‚\n# DELL 18:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID SAS-3 3108 [Invader]\" -r02 -p00 \"Dell\" \"PERC H730P Adapter\" # HUAWEI 1c:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3508\" -r01 -p00 \"Huawei Technologies Co., Ltd.\" \"MegaRAID Tri-Mode SAS3508\" # XFUSION 2a:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID 12GSAS/PCIe Secure SAS39xx\" \"Broadcom / LSI\" \"MegaRAID 12GSAS/PCIe Secure SAS39xx\" # INSPUR 18:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3516\" -r01 \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3516\" # LENOVO 4b:00.0 \"RAID bus controller\" \"Broadcom / LSI\" \"MegaRAID Tri-Mode SAS3516\" -r01 \"Lenovo\" \"ThinkSystem RAID 930-16i 4GB Flash PCIe 12Gb Adapter\" # HPE 5c:00.0 \"Serial Attached SCSI controller\" \"Adaptec\" \"Smart Storage PQI SAS\" -r01 -p00 \"Hewlett-Packard Company\" \"Smart Array P408i-a SR Gen10\" # KAYTUS 31:00.0 \"Serial Attached SCSI controller\" \"Adaptec\" \"Smart Storage PQI 12G SAS/PCIe 3\" -r01 \"Inspur Electronic Information Industry Co., Ltd.\" \"PM8222-SHBA\" å¸¸è§çš„raidåˆ¶é€ å•†ï¼š\nBroadcom / LSIï¼šLSI æ˜¯çŸ¥åçš„å­˜å‚¨æ§åˆ¶å™¨åˆ¶é€ å•†ï¼Œç°å·²è¢« Broadcom æ”¶è´­ã€‚MegaRAID ç³»åˆ—æ˜¯å…¶å¹¿æ³›ä½¿ç”¨çš„ RAID æ§åˆ¶å™¨ã€‚ Adaptec: Adaptec æä¾›ä¸€ç³»åˆ— RAID æ§åˆ¶å™¨ï¼Œä¸»è¦ç”¨äºé«˜æ€§èƒ½å­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚ 2. å¸¸è§çš„ RAID å‚å•†åŠå…¶å‘½ä»¤è¡Œå·¥å…· è®¾å¤‡ç±»å‹ raidæ§åˆ¶å™¨åˆ¶é€ å•† è®¾å¤‡å‹å· Raidå‘½ä»¤ ä½¿ç”¨è¯¥raidçš„æœåŠ¡å™¨å‚å•† RAID bus controller Broadcom / LSIï¼ˆMegaRAID ç³»åˆ—ï¼‰ MegaRAID Tri-Mode SAS3508 storcli DELLï¼ŒHUAWEIï¼ŒXFUSIONï¼ŒINSPURï¼ŒLENOVO Serial Attached SCSI controller Adaptec Smart Storage PQI ssacli HPEï¼ŒKAYTUS Non-Volatile memory controller SATA controller 3. ä½¿ç”¨storcliå‘½ä»¤åˆ›å»ºraid 3.1. å®‰è£…storcli ç™»å½•https://www.broadcom.com/products/storage/raid-controllers/megaraid-9560-8iä¸‹è½½æœ€æ–°çš„storcliçš„è§£å‹åŒ…ï¼Œè§£å‹åå¯ä»¥æ‰§è¡Œrpm -ivh /tmp/StorCLIxxx.aarch64.rpmå‘½ä»¤å®‰è£…ã€‚\nå‚æ•°è¯´æ˜\n/cx = Controller ID /vx = Virtual Drive Number. /ex = Enclosure ID /sx = Slot ID 3.2. åˆ›å»ºraid åœ¨åˆ›å»ºä¹‹å‰éœ€è¦æŸ¥çœ‹å½“å‰æœ‰å‡ ä¸ªcontrollerï¼Œæœ‰å‡ å—ç‰©ç†ç£ç›˜ä»¥åŠç‰©ç†ç£ç›˜çš„EID:Sltä¿¡æ¯ï¼Œç”¨äºé€‰å®šå“ªå‡ å¿«ç‰©ç†ç£ç›˜æ¥åšæŒ‡å®šçº§åˆ«çš„raidã€‚\n# ç”±ç¬¬3ã€4ã€5å—ç‰©ç†ç£ç›˜æ¥æ„å»ºRAID5ï¼Œåˆ†é…æ‰€æœ‰ç©ºé—´çš„é€»è¾‘ç£ç›˜å‘½åtmp1ã€‚ storcli /c0 add vd type=raid5 size=all names=tmp1 drives=32:2-4 WB å‚æ•°è¯´æ˜ï¼š\nstorcli æ˜¯ Broadcom / LSI çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºç®¡ç† MegaRAID æ§åˆ¶å™¨ã€‚\n/c0 è¡¨ç¤ºç¬¬ä¸€ä¸ª RAID æ§åˆ¶å™¨ï¼ˆæ§åˆ¶å™¨ 0ï¼‰ã€‚å¦‚æœæœ‰å¤šä¸ª RAID æ§åˆ¶å™¨ï¼Œå¯ä»¥ç”¨ /c1ã€/c2 ç­‰æ¥æŒ‡å®šå…¶ä»–æ§åˆ¶å™¨ã€‚\nadd vd è¡¨ç¤ºæ·»åŠ ä¸€ä¸ªè™šæ‹Ÿé©±åŠ¨å™¨ï¼ˆVirtual Driveï¼Œç®€ç§° VDï¼‰ã€‚VD æ˜¯ RAID é˜µåˆ—çš„é€»è¾‘è¡¨ç¤ºï¼Œç”¨æˆ·é€šè¿‡å®ƒæ¥è®¿é—® RAID ä¸Šçš„æ•°æ®ã€‚\ntype=raid5 è¡¨ç¤ºè¦åˆ›å»ºçš„ RAID ç±»å‹æ˜¯ RAID 5ã€‚RAID 5 ä½¿ç”¨åˆ†å¸ƒå¼å¥‡å¶æ ¡éªŒï¼Œæä¾›äº†è¾ƒå¥½çš„è¯»å–æ€§èƒ½å’Œæ•°æ®å†—ä½™ï¼Œè‡³å°‘éœ€è¦ 3 ä¸ªç‰©ç†ç£ç›˜ã€‚\nsize=all è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ç£ç›˜ç©ºé—´æ¥åˆ›å»ºè¿™ä¸ªè™šæ‹Ÿé©±åŠ¨å™¨ã€‚å¯ä»¥æŒ‡å®šç‰¹å®šå¤§å°ï¼Œä½†æ­¤å¤„ä½¿ç”¨ all è¡¨ç¤ºä½¿ç”¨ç£ç›˜çš„å…¨éƒ¨ç©ºé—´ã€‚\nnames=tmp1 ä¸ºè™šæ‹Ÿé©±åŠ¨å™¨å‘½åä¸º tmp1ã€‚è¿™æœ‰åŠ©äºåœ¨ç³»ç»Ÿä¸­åŒºåˆ†ä¸åŒçš„è™šæ‹Ÿé©±åŠ¨å™¨ã€‚åç§°æ˜¯ç”¨æˆ·å®šä¹‰çš„ï¼Œä¾¿äºè¯†åˆ«ã€‚\ndrives=32:2-4 æˆ–**drives=32:2,3,4**\næŒ‡å®šç”¨äºåˆ›å»º RAID çš„ç‰©ç†ç£ç›˜ã€‚\n32 æ˜¯æ‰©å±•å™¨ï¼ˆenclosureï¼‰çš„ç¼–å·å³EIDã€‚å¦‚æœç£ç›˜æ˜¯ç›´æ¥è¿æ¥åˆ°æ§åˆ¶å™¨ï¼Œé€šå¸¸ç¼–å·ä¸º 0ï¼Œå¦‚æœæ˜¯é€šè¿‡æ‰©å±•å™¨è¿æ¥ï¼Œæ‰©å±•å™¨çš„ç¼–å·ä¸º 32ã€‚ 2-4 æŒ‡å®šç‰©ç†ç£ç›˜ç¼–å·å³Sltï¼Œè¡¨ç¤ºä½¿ç”¨è¯¥æ‰©å±•å™¨ä¸Šçš„ 2ã€3ã€4 å·ç£ç›˜ã€‚æ­¤å¤„çš„ RAID 5 è‡³å°‘éœ€è¦ 3 ä¸ªç£ç›˜ã€‚ WB: è¡¨ç¤ºå¯ç”¨ Write Back ç¼“å­˜ã€‚Write Back æ„å‘³ç€åœ¨å†™æ“ä½œæ—¶ï¼Œæ•°æ®é¦–å…ˆå†™å…¥ç¼“å­˜ï¼Œç„¶åè¿”å›æˆåŠŸå“åº”ï¼Œå®é™…æ•°æ®å†™å…¥ç£ç›˜å¯èƒ½ç¨åè¿›è¡Œã€‚\nWrite Back ç¼“å­˜å¯ä»¥æé«˜å†™å…¥æ€§èƒ½ï¼Œä½†éœ€è¦æœ‰å¤‡ç”¨ç”µæºï¼ˆå¦‚ç”µæ± æˆ–è¶…çº§ç”µå®¹ï¼‰æ¥ä¿è¯ç¼“å­˜æ•°æ®åœ¨æ–­ç”µæ—¶ä¸ä¼šä¸¢å¤±ã€‚ å¦ä¸€ç§æ¨¡å¼æ˜¯ Write Through (WT)ï¼Œæ•°æ®ç›´æ¥å†™å…¥ç£ç›˜ï¼Œåªæœ‰æ•°æ®æˆåŠŸå†™å…¥ç£ç›˜æ—¶æ‰è¿”å›æˆåŠŸå“åº”ï¼Œç›¸å¯¹æ›´å®‰å…¨ä½†æ€§èƒ½è¾ƒä½ã€‚ 3.3. æŸ¥çœ‹raid 3.3.1. æŸ¥çœ‹æœ‰å“ªäº›controller # storcli show System Overview : -------------------------------------------------------------------- Ctl Model Ports PDs DGs DNOpt VDs VNOpt BBU sPR DS EHS ASOs Hlth -------------------------------------------------------------------- 0 SAS3508 8 2 1 0 1 0 Opt On 1\u00262 Y 3 Opt -------------------------------------------------------------------- Ctl=Controller Index|DGs=Drive groups|VDs=Virtual drives|Fld=Failed PDs=Physical drives|DNOpt=Array NotOptimal|VNOpt=VD NotOptimal|Opt=Optimal Msng=Missing|Dgd=Degraded|NdAtn=Need Attention|Unkwn=Unknown sPR=Scheduled Patrol Read|DS=DimmerSwitch|EHS=Emergency Spare Drive Y=Yes|N=No|ASOs=Advanced Software Options|BBU=Battery backup unit/CV Hlth=Health|Safe=Safe-mode boot|CertProv-Certificate Provision mode Chrg=Charging | MsngCbl=Cable Failure ä¸Šè¿°ä¿¡æ¯å¯ä»¥çœ‹å‡ºå½“å‰æœ‰1ä¸ªcontrollerï¼Œ ç¬¬ä¸€ä¸ªcontrolleræœ‰2ä¸ªç‰©ç†ç£ç›˜ã€‚\nå¦‚æœè¦ç”¨jsonæ ¼å¼çš„æ–¹å¼æŸ¥çœ‹å¯ä»¥å¢åŠ Jå‚æ•°ã€‚\n# storcli show J { \"Controllers\": [ { \"Command Status\": { \"CLI Version\": \"007.2203.0000.0000 May 11, 2022\", \"Status Code\": 0, \"Status\": \"Success\", \"Description\": \"None\" }, \"Response Data\": { \"Number of Controllers\": 1, \"System Overview\": [ { \"Ctl\": 0, \"Model\": \"SAS3508\", \"Ports\": 8, \"PDs\": 2, \"DGs\": 1, \"DNOpt\": 0, \"VDs\": 1, \"VNOpt\": 0, \"BBU\": \"Opt\", \"sPR\": \"On\", \"DS\": \"1\u00262\", \"EHS\": \"Y\", \"ASOs\": 3, \"Hlth\": \"Opt\" } ] } } ] } 3.3.2. æŸ¥çœ‹controllerä¿¡æ¯ # storcli /c0 show TOPOLOGY : --------------------------------------------------------------------------- DG Arr Row EID:Slot DID Type State BT Size PDC PI SED DS3 FSpace TR --------------------------------------------------------------------------- 0 - - - - RAID1 Optl N 1.089 TB dflt N N dflt N N 0 0 - - - RAID1 Optl N 1.089 TB dflt N N dflt N N 0 0 0 134:0 1 DRIVE Onln N 1.089 TB dflt N N dflt - N 0 0 1 134:1 0 DRIVE Onln N 1.089 TB dflt N N dflt - N --------------------------------------------------------------------------- DG=Disk Group Index|Arr=Array Index|Row=Row Index|EID=Enclosure Device ID DID=Device ID|Type=Drive Type|Onln=Online|Rbld=Rebuild|Optl=Optimal|Dgrd=Degraded Pdgd=Partially degraded|Offln=Offline|BT=Background Task Active PDC=PD Cache|PI=Protection Info|SED=Self Encrypting Drive|Frgn=Foreign DS3=Dimmer Switch 3|dflt=Default|Msng=Missing|FSpace=Free Space Present TR=Transport Ready Virtual Drives = 1 VD LIST : ------------------------------------------------------------- DG/VD TYPE State Access Consist Cache Cac sCC Size Name ------------------------------------------------------------- 0/0 RAID1 Optl RW No RWBD - ON 1.089 TB ------------------------------------------------------------- VD=Virtual Drive| DG=Drive Group|Rec=Recovery Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled Check Consistency Physical Drives = 2 PD LIST : ---------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------- 134:0 1 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - 134:1 0 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - ---------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 134 OK 8 2 0 0 0 0 1 - SGPIO -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID Cachevault_Info : ------------------------------------ Model State Temp Mode MfgDate ------------------------------------ CVPM02 Optimal 26C - 2018/06/05 ------------------------------------ 3.3.2. æŸ¥çœ‹ç‰©ç†æœºç£ç›˜ # storcli /c0 /eall /sall show Drive Information : ---------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------- 134:0 1 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - 134:1 0 Onln 0 1.089 TB SAS HDD N N 512B ST1200MM0009 U - ---------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild 3.3.2. æŸ¥çœ‹è™šæ‹Ÿç£ç›˜ # storcli /c0 /v0 show Virtual Drives : ------------------------------------------------------------- DG/VD TYPE State Access Consist Cache Cac sCC Size Name ------------------------------------------------------------- 0/0 RAID1 Optl RW No RWBD - ON 1.089 TB ------------------------------------------------------------- VD=Virtual Drive| DG=Drive Group|Rec=Recovery Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled Check Consistency å¯ä»¥çœ‹åˆ°è™šæ‹Ÿç£ç›˜åšçš„æ˜¯RAID1çš„é…ç½®ã€‚\n3.4. åˆ é™¤raid åˆ é™¤ RAID é˜µåˆ—\n/c0 è¡¨ç¤ºæ§åˆ¶å™¨ 0ã€‚\n/v0 è¡¨ç¤ºè™šæ‹Ÿé©±åŠ¨å™¨ 0ã€‚\nstorcli /c0 /v0 delete force åˆ é™¤æ‰€æœ‰è™šæ‹Ÿé©±åŠ¨å™¨\nstorcli /c0/vall delete preservedCache åˆ é™¤ å¤–éƒ¨é˜µåˆ—ï¼ˆforeign configurationsï¼‰\nstorcli /c0/fall delete /fallï¼šfall è¡¨ç¤º æ‰€æœ‰çš„å¤–éƒ¨é˜µåˆ—ï¼ˆforeign configurationsï¼‰ã€‚å¤–éƒ¨é˜µåˆ—é€šå¸¸æ˜¯æŒ‡åœ¨ä¸åŒçš„ RAID æ§åˆ¶å™¨æˆ–ç¡¬ç›˜è¢«ç§»åŠ¨åˆ°å½“å‰ç³»ç»Ÿä¸­åï¼ŒRAID æ§åˆ¶å™¨æ£€æµ‹åˆ°çš„ç°æœ‰ RAID é…ç½®ï¼Œä½†è¿™äº›é…ç½®å°šæœªè¢«æœ¬åœ°æ§åˆ¶å™¨è®¤å¯ä¸ºæœ‰æ•ˆçš„ RAID é…ç½®ã€‚\n4. ä½¿ç”¨ssacliå‘½ä»¤åˆ›å»ºraid Todo\n3.2.1. åˆ›å»ºraid 3.2.2. æŸ¥çœ‹raid 3.2.3. åˆ é™¤raid 5. æ€»ç»“ æœ¬æ–‡ä¸»è¦æè¿°å¦‚ä½•åˆ›å»ºç¡¬ä»¶raidï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤\né€šè¿‡lspci -mm |egrep \"Broadcom / LSI|Adaptec\"å‘½ä»¤æŸ¥è¯¢å½“å‰æœºå™¨ä½¿ç”¨çš„æ˜¯å“ªä¸ªraidå‚å•†ï¼Œå¯¹åº”ä½¿ç”¨å“ªç§raidå‘½ä»¤ï¼Œä¾‹å¦‚storcliæˆ–ssacliç­‰ã€‚ æ ¹æ®é€‰ä¸­çš„å‘½ä»¤æ¸…é™¤å½“å‰çš„raidé…ç½®ã€‚ æ ¹æ®é€‰ä¸­çš„å‘½ä»¤æŸ¥è¯¢å½“å‰æœ‰å“ªäº›ç‰©ç†ç£ç›˜ï¼Œå¤§å°ï¼Œç£ç›˜ç¼–å·ï¼Œæ§åˆ¶å™¨æœ‰å“ªäº›å¹¶è®°å½•ä¸‹æ¥ã€‚ æ ¹æ®ç”¨æˆ·é…ç½®é€‰æ‹©åšå“ªç§raidï¼ˆraidçº§åˆ«ï¼‰ï¼Œä»¥åŠé€‰æ‹©å“ªäº›ç£ç›˜ç”¨æ¥åšraidï¼ˆé€šè¿‡ç£ç›˜ç¼–å·å³EID:Sltï¼‰ å‚è€ƒï¼š\nstorcli ssacli ","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦æè¿°å¦‚ä½•é’ˆå¯¹ç‰©ç†æœºåšç¡¬ä»¶çš„raidé…ç½®ï¼Œç¡¬ä»¶raidä¸ä¾èµ–äºæ“ä½œç³»ç»Ÿï¼Œå…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œç»å¸¸åœ¨è£…æœºç³»ç»Ÿä¸­ä½¿ç”¨åˆ°ã€‚\n1. æŸ¥è¯¢raidæ§ â€¦","ref":"/linux-notes/disk/make-hardware-raid/","tags":["disk"],"title":"åˆ›å»ºç¡¬ä»¶Raid"},{"body":"1. ä»‹ç» ç½‘ç«™çš„ SSL/TLS åŠ å¯†ä¼šä¸ºæ‚¨çš„ç”¨æˆ·å¸¦æ¥æ›´é å‰çš„æœç´¢æ’åå’Œæ›´å‡ºè‰²çš„å®‰å…¨æ€§ã€‚ä½†æ˜¯æœ€å¤§éšœç¢æ˜¯è¯ä¹¦è·å–æˆæœ¬é«˜æ˜‚å’Œæ‰€æ¶‰äººå·¥æµç¨‹ç¹çã€‚\nLetâ€™s Encrypt æ˜¯ä¸€å®¶å…è´¹ã€å¼€æ”¾ã€è‡ªåŠ¨åŒ–çš„è¯ä¹¦é¢å‘æœºæ„ (CA)ã€‚æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ Letâ€™s Encrypt å®¢æˆ·ç«¯ç”Ÿæˆè¯ä¹¦ï¼Œä»¥åŠå¦‚ä½•è‡ªåŠ¨é…ç½® NGINX å¼€æºç‰ˆå’Œ NGINX Plus ä»¥ä½¿ç”¨è¿™äº›è¯ä¹¦ã€‚\n2. å®‰è£…certbot apt-get update sudo apt-get install certbot apt-get install python3-certbot-nginx 3. ä¸ºåŸŸåç”Ÿæˆè¯ä¹¦ æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä¼šç”Ÿæˆä¸€ä¸ª90å¤©åˆ°æœŸçš„è¯ä¹¦æ–‡ä»¶ã€‚\nsudo certbot --nginx -d www.example.com ä»¥ä¸Šå‘½ä»¤ä¼šåœ¨/etc/letsencrypt/live/ç”Ÿæˆè¯ä¹¦æ–‡ä»¶ã€‚\ncd /etc/letsencrypt/live/www.example.com ls README cert.pem chain.pem fullchain.pem privkey.pem å¦‚æœé…ç½®æˆåŠŸä¼šç”Ÿæˆä»¥ä¸‹ä¿¡æ¯ï¼š\nCongratulations! You have successfully enabled https://example.com and https://www.example.com ------------------------------------------------------------------------------------- IMPORTANT NOTES: Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/example.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/example.com//privkey.pem Your cert will expire on 2017-12-12. å¹¶ä¸”certbotä¼šè‡ªåŠ¨ä¸ºdomainâ€‘name.confæ–‡ä»¶è‡ªè¡Œä¿®æ”¹è¯ä¹¦è·¯å¾„ã€‚\nserver { listen 80 default_server; listen [::]:80 default_server; root /var/www/html; server_name example.com www.example.com; listen 443 ssl; # managed by Certbot # RSA certificate ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot # Redirect non-https traffic to https if ($scheme != \"https\") { return 301 https://$host$request_uri; } # managed by Certbot } 3. é‡å¯Nginx nginx -t \u0026\u0026 nginx -s reload 4. è‡ªåŠ¨æ›´æ–°è¯ä¹¦ Letâ€™s Encrypt è¯ä¹¦å°†åœ¨ 90 å¤©ååˆ°æœŸï¼Œ å› æ­¤è®¾ç½®å®šæ—¶ä»»åŠ¡è‡ªåŠ¨æ›´æ–°è¯ä¹¦ã€‚\ncrontab -e # å°†ä»¥ä¸‹ä¿¡æ¯å†™å…¥åˆ°crontabæ–‡ä»¶ä¸­ 0 12 * * * /usr/bin/certbot renew --quiet å‚è€ƒï¼š\næ›´æ–°ï¼šä¸º NGINX é…ç½®å…è´¹çš„ Letâ€™s Encrypt SSL/TLS è¯ä¹¦ ","categories":"","description":"","excerpt":"1. ä»‹ç» ç½‘ç«™çš„ SSL/TLS åŠ å¯†ä¼šä¸ºæ‚¨çš„ç”¨æˆ·å¸¦æ¥æ›´é å‰çš„æœç´¢æ’åå’Œæ›´å‡ºè‰²çš„å®‰å…¨æ€§ã€‚ä½†æ˜¯æœ€å¤§éšœç¢æ˜¯è¯ä¹¦è·å–æˆæœ¬é«˜æ˜‚å’Œæ‰€æ¶‰äººå·¥æµç¨‹ç¹çã€‚ â€¦","ref":"/linux-notes/nginx/config-ssl-for-nginx/","tags":["Nginx"],"title":"é…ç½®Nginxå…è´¹è¯ä¹¦"},{"body":"1. Installation # mac brew install wrk 2. Usage $ wrk --help Usage: wrk \u003coptions\u003e \u003curl\u003e Options: -c, --connections \u003cN\u003e Connections to keep open # è·ŸæœåŠ¡å™¨å»ºç«‹å¹¶ä¿æŒçš„TCPè¿æ¥æ•°é‡ -d, --duration \u003cT\u003e Duration of test # å‹æµ‹æ—¶é—´ -t, --threads \u003cN\u003e Number of threads to use # ä½¿ç”¨å¤šå°‘ä¸ªçº¿ç¨‹è¿›è¡Œå‹æµ‹ -s, --script \u003cS\u003e Load Lua script file # æŒ‡å®šLuaè„šæœ¬è·¯å¾„ -H, --header \u003cH\u003e Add header to request # ä¸ºæ¯ä¸€ä¸ªHTTPè¯·æ±‚æ·»åŠ HTTPå¤´ --latency Print latency statistics # åœ¨å‹æµ‹ç»“æŸåï¼Œæ‰“å°å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯ --timeout \u003cT\u003e Socket/request timeout # è¶…æ—¶æ—¶é—´ -v, --version Print version details # æ‰“å°æ­£åœ¨ä½¿ç”¨çš„wrkçš„è¯¦ç»†ç‰ˆæœ¬ä¿¡æ¯ Numeric arguments may include a SI unit (1k, 1M, 1G) # ä»£è¡¨æ•°å­—å‚æ•°ï¼Œæ”¯æŒå›½é™…å•ä½ (1k, 1M, 1G) Time arguments may include a time unit (2s, 2m, 2h) # ä»£è¡¨æ—¶é—´å‚æ•°ï¼Œæ”¯æŒæ—¶é—´å•ä½ (2s, 2m, 2h) å‚æ•°è®¾ç½®è¯´æ˜ï¼š\nä¸€èˆ¬è®¾ç½®çº¿ç¨‹æ•°tï¼Œå¹¶å‘æ•°cï¼Œå‹æµ‹æ—¶é—´dï¼Œ--latencyå››ä¸ªé€šç”¨çš„å‚æ•°ã€‚\nçº¿ç¨‹æ•°ï¼šä¸€èˆ¬è®¾ç½®ä¸ºå‹æµ‹æœºå™¨CPUæ ¸æ•°çš„2-4å€ï¼Œè¿‡å¤§ä¼šå¯¼è‡´çº¿ç¨‹åˆ‡æ¢é¢‘ç¹ï¼Œæ•ˆæœä¸‹é™ã€‚\nå¹¶å‘æ•°ï¼šæ ¹æ®å‹æµ‹ç»“æœï¼ŒåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°ä½¿å¾—å‹æµ‹è¾¾åˆ°ç“¶é¢ˆã€‚\nç¤ºä¾‹ï¼š\nwrk -t12 -c400 -d30s --latency http://www.baidu.com 3. å‹æµ‹ç»“æœ # wrk -t12 -c400 -d30s --latency http://www.baidu.com Running 30s test @ http://www.baidu.com 12 threads and 400 connections ï¼ˆå¹³å‡å€¼ï¼‰ ï¼ˆæ ‡å‡†å·®ï¼‰ ï¼ˆæœ€å¤§å€¼ï¼‰ï¼ˆæ­£è´Ÿä¸€ä¸ªæ ‡å‡†å·®æ‰€å æ¯”ä¾‹ï¼‰ Thread Stats Avg Stdev Max +/- Stdev ï¼ˆå»¶è¿Ÿï¼‰ Latency 568.16ms 250.70ms 2.00s 83.49% (æ¯ç§’è¯·æ±‚æ•°) Req/Sec 28.26 14.99 90.00 65.71% Latency Distribution ï¼ˆå»¶è¿Ÿåˆ†å¸ƒï¼‰ 50% 530.91ms 75% 585.73ms 90% 691.64ms 99% 1.78s 9842 requests in 30.10s, 99.03MB read (30.10så†…å¤„ç†äº†9842ä¸ªè¯·æ±‚ï¼Œè€—è´¹æµé‡99.03MB) Socket errors: connect 158, read 0, write 0, timeout 580 (å‘ç”Ÿé”™è¯¯æ•°) Requests/sec: 327.00 (QPS ,å³å¹³å‡æ¯ç§’å¤„ç†è¯·æ±‚æ•°) Transfer/sec: 3.29MB (å¹³å‡æ¯ç§’æµé‡) 4. Luaè„šæœ¬å®šåˆ¶å‹æµ‹å‚æ•° ä¾‹å¦‚ï¼šå‹æµ‹postè¯·æ±‚ï¼Œéœ€è¦è®¾ç½®æŒ‡å®šå‚æ•°ã€‚\nå†™å…¥ä»¥ä¸‹luaè„šæœ¬ï¼Œlogin.lua\nwrk.method = \"POST\" wrk.body = '{\"username\":\"xxx\",\"password\":\"xxx\"}' wrk.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\" å‘èµ·å‹æµ‹è¯·æ±‚ï¼š\nwrk -t12 -c1000 -d30s --latency http://127.0.0.1:8081/login -s login.lua å‚è€ƒï¼š\nGitHub - wg/wrk: Modern HTTP benchmarking tool\nhttps://www.cnblogs.com/quanxiaoha/p/10661650.html\n","categories":"","description":"","excerpt":"1. Installation # mac brew install wrk 2. Usage $ wrk --help Usage: â€¦","ref":"/linux-notes/network/wrk-usage/","tags":"","title":"wrkçš„ä½¿ç”¨"},{"body":"1. ç¤¾åŒºè¯´æ˜ 1.1. Community membership Role Responsibilities Requirements Defined by Member Active contributor in the community Sponsored by 2 reviewers and multiple contributions to the project Kubernetes GitHub org member Reviewer Review contributions from other members History of review and authorship in a subproject OWNERSÂ file reviewer entry Approver Contributions acceptance approval Highly experienced active reviewer and contributor to a subproject OWNERSÂ file approver entry Subproject owner Set direction and priorities for a subproject Demonstrated responsibility and excellent technical judgement for the subproject sigs.yamlÂ subprojectÂ OWNERSÂ fileÂ ownersÂ entry 1.2. ç¤¾åŒºæ´»åŠ¨æ—¥å† Community Calendar | Kubernetes Contributors\n1.3. åŠ å…¥k8s slack ç‚¹å‡» https://communityinviter.com/apps/kubernetes/community\n1.4. ç‰¹åˆ«å…´è¶£å°ç»„ï¼ˆSIGï¼‰ åˆ—è¡¨ï¼š https://github.com/kubernetes/community/blob/master/sig-list.md\n2. ç¼–è¯‘k8sä»“åº“ å‚è€ƒï¼š\nBuilding Kubernetes https://github.com/kubernetes/community/blob/master/contributors/devel/development.md#building-kubernetes 2.1. ç¼–è¯‘äºŒè¿›åˆ¶ 2.1.1. åŸºäºdockeræ„å»ºå®¹å™¨ç¼–è¯‘ã€‚ è¯¥æ–¹å¼ä¸ºå®˜æ–¹é•œåƒåŠäºŒè¿›åˆ¶æ–‡ä»¶çš„æ„å»ºæ–¹å¼ã€‚\næ„å»ºé•œåƒ(å¤§å°ï¼š5.97GB)ä¸ºï¼š kube-build:build-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0\ngit clone https://github.com/kubernetes/kubernetes.git cd kubernetes build/run.sh make # æ„å»ºå…¨éƒ¨ #æŒ‡å®šæ¨¡å—æ„å»º build/run.sh make kubeadm è¾“å‡ºå¦‚ä¸‹ï¼š\n# build/run.sh make +++ [0804 18:39:11] Verifying Prerequisites.... +++ [0804 18:39:16] Building Docker image kube-build:build-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0 +++ [0804 18:40:49] Creating data container kube-build-data-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0 +++ [0804 18:40:50] Syncing sources to container +++ [0804 18:40:58] Output from this container will be rsynced out upon completion. Set KUBE_RUN_COPY_OUTPUT=n to disable. +++ [0804 18:40:58] Running build command... go: downloading go.uber.org/automaxprocs v1.5.2 +++ [0804 18:41:04] Setting GOMAXPROCS: 8 Go version: go version go1.20.6 linux/amd64 +++ [0804 18:41:04] Building go targets for linux/amd64 k8s.io/kubernetes/cmd/kube-proxy (static) k8s.io/kubernetes/cmd/kube-apiserver (static) k8s.io/kubernetes/cmd/kube-controller-manager (static) k8s.io/kubernetes/cmd/kubelet (non-static) k8s.io/kubernetes/cmd/kubeadm (static) k8s.io/kubernetes/cmd/kube-scheduler (static) k8s.io/component-base/logs/kube-log-runner (static) k8s.io/kube-aggregator (static) k8s.io/apiextensions-apiserver (static) k8s.io/kubernetes/cluster/gce/gci/mounter (non-static) k8s.io/kubernetes/cmd/kubectl (static) k8s.io/kubernetes/cmd/kubectl-convert (static) github.com/onsi/ginkgo/v2/ginkgo (non-static) k8s.io/kubernetes/test/e2e/e2e.test (test) k8s.io/kubernetes/test/conformance/image/go-runner (non-static) k8s.io/kubernetes/cmd/kubemark (static) github.com/onsi/ginkgo/v2/ginkgo (non-static) k8s.io/kubernetes/test/e2e_node/e2e_node.test (test) Env for linux/amd64: GOOS=linux GOARCH=amd64 GOROOT=/usr/local/go CGO_ENABLED= CC= Coverage is disabled. Coverage is disabled. +++ [0804 18:48:17] Placing binaries +++ [0804 18:48:25] Syncing out of container äº§ç‰©æ–‡ä»¶åœ¨_outputç›®å½•ä¸Šã€‚\nkubernetes/_output# tree . |-- dockerized | |-- bin | | `-- linux | | `-- amd64 | | |-- apiextensions-apiserver | | |-- e2e_node.test | | |-- e2e.test | | |-- ginkgo | | |-- go-runner | | |-- kubeadm | | |-- kube-aggregator | | |-- kube-apiserver | | |-- kube-controller-manager | | |-- kubectl | | |-- kubectl-convert | | |-- kubelet | | |-- kube-log-runner | | |-- kubemark | | |-- kube-proxy | | |-- kube-scheduler | | |-- mounter | | `-- ncpu | `-- go `-- images `-- kube-build:build-8faa8d3cb7-5-v1.27.0-go1.20.6-bullseye.0 |-- Dockerfile |-- localtime |-- rsyncd.password `-- rsyncd.sh 2.1.2. åŸºäºæ„å»ºæœºç¯å¢ƒç¼–è¯‘ git clone https://github.com/kubernetes/kubernetes.git cd kubernetes # æ„å»ºå…¨éƒ¨äºŒè¿›åˆ¶ make # æ„å»ºæŒ‡å®šäºŒè¿›åˆ¶ make WHAT=cmd/kubeadm è¾“å‡ºå¦‚ä¸‹ï¼š\n# make WHAT=cmd/kubeadm go version go1.20.6 linux/amd64 +++ [0804 19:30:55] Setting GOMAXPROCS: 8 +++ [0804 19:30:56] Building go targets for linux/amd64 k8s.io/kubernetes/cmd/kubeadm (static) 2.2. ç¼–è¯‘é•œåƒ git clone https://github.com/kubernetes/kubernetes cd kubernetes make quick-release 3. å¦‚ä½•ç»™k8sæäº¤PR å‚è€ƒï¼š\nhttps://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md\nhttps://github.com/kubernetes/community/blob/master/contributors/guide/first-contribution.md\nHere is the bot commands documentation.\ntesting guide\nå‚è€ƒï¼š\nhttps://github.com/kubernetes/community/\nhttps://github.com/kubernetes/community/tree/master/contributors/guide\nhttps://github.com/kubernetes/community/blob/master/contributors/guide/first-contribution.md\nissues labeled as a good first issue\n","categories":"","description":"","excerpt":"1. ç¤¾åŒºè¯´æ˜ 1.1. Community membership Role Responsibilities Requirements â€¦","ref":"/kubernetes-notes/develop/develop-k8s/","tags":["Kubernetes"],"title":"k8sç¤¾åŒºå¼€å‘æŒ‡å—"},{"body":"æœ¬æ–‡ä»‹ç»å¦‚ä½•é€šè¿‡kubectlè¿›å…¥èŠ‚ç‚¹çš„shellç¯å¢ƒã€‚\n1. å®‰è£…krew node-shell 1.1. å®‰è£…krew ( set -x; cd \"$(mktemp -d)\" \u0026\u0026 OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" \u0026\u0026 ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" \u0026\u0026 KREW=\"krew-${OS}_${ARCH}\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" \u0026\u0026 tar zxvf \"${KREW}.tar.gz\" \u0026\u0026 ./\"${KREW}\" install krew ) åœ¨~/.bashrcæˆ–~/.zshrcæ·»åŠ ä»¥ä¸‹å‘½ä»¤\nexport PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" 1.2. å®‰è£…node-shell node-shellçš„ä»£ç å‚è€ƒï¼škubectl-node-shell/kubectl-node_shell at master Â· kvaps/kubectl-node-shell Â· GitHub\nkubectl krew install node-shell ç¤ºä¾‹ï¼š\n# kubectl krew install node-shell Updated the local copy of plugin index. Installing plugin: node-shell Installed plugin: node-shell \\ | Use this plugin: | kubectl node-shell | Documentation: | https://github.com/kvaps/kubectl-node-shell | Caveats: | \\ | | You need to be allowed to start privileged pods in the cluster | / / WARNING: You installed plugin \"node-shell\" from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. 2. è¿›å…¥èŠ‚ç‚¹çš„shell 2.1. ç™»å½•node åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ç‰¹æƒå®¹å™¨ï¼Œç™»å½•å®¹å™¨å³ç™»å½•node shellã€‚\nkubectl node-shell \u003cnode-name\u003e ç¤ºä¾‹ï¼š\n# kubectl node-shell node1 spawning \"nsenter-9yqytp\" on \"node1\" If you don't see a command prompt, try pressing enter. groups: cannot find name for group ID 11 To run a command as administrator (user \"root\"), use \"sudo \u003ccommand\u003e\". See \"man sudo_root\" for details. root@node1:/# 2.2. é€€å‡ºnode é€€å‡ºå®¹å™¨ï¼Œå®¹å™¨ä¼šè¢«è‡ªåŠ¨åˆ é™¤ã€‚\n# exit logout pod default/nsenter-9yqytp terminated (Error) pod \"nsenter-9yqytp\" deleted 3. åŸç† å®¹å™¨æ˜¯å¼±éš”ç¦»ï¼Œå…±äº«èŠ‚ç‚¹çš„å†…æ ¸ï¼Œé€šè¿‡cgroupå’Œnamespaceæ¥å®ç°è¿›ç¨‹çº§åˆ«çš„éš”ç¦»ã€‚é‚£ä¹ˆé€šè¿‡åœ¨ç‰¹æƒå®¹å™¨é‡Œæ‰§è¡Œnsenterçš„å‘½ä»¤ï¼Œåˆ™å¯ä»¥é€šè¿‡ç™»å½•ç‰¹æƒå®¹å™¨æ¥å®ç°ç™»å½•nodeçš„shellç¯å¢ƒã€‚\nåˆ›å»ºä¸€ä¸ªç‰¹æƒå®¹å™¨ï¼Œè¿›å…¥node shellçš„å‘½ä»¤ä¸ºï¼š\nnsenter --target 1 --mount --uts --ipc --net --pid -- bash -l è¿›å…¥ node shell çš„æƒé™ï¼š\nhostPID: trueÂ å…±äº« host çš„ pid\nhostNetwork: trueÂ å…±äº« host çš„ç½‘ç»œ\nprivileged: true: PSP æƒé™ç­–ç•¥æ˜¯Â privileged, å³å®Œå…¨æ— é™åˆ¶ã€‚\n3.1. Pod.yaml apiVersion: v1 kind: Pod metadata: labels: run: nsenter-9yqytp name: nsenter-9yqytp namespace: default spec: containers: - command: - nsenter - --target - \"1\" - --mount - --uts - --ipc - --net - --pid - -- - bash - -l image: docker.io/library/alpine imagePullPolicy: Always name: nsenter resources: limits: cpu: 100m memory: 256Mi requests: cpu: 100m memory: 256Mi securityContext: privileged: true stdin: true stdinOnce: true tty: true volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-4ktlf readOnly: true enableServiceLinks: true hostNetwork: true hostPID: true nodeName: node1 preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Never schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoExecute operator: Exists volumes: - name: kube-api-access-4ktlf projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace åˆ›å»ºå®Œå®¹å™¨åï¼Œç›´æ¥ç™»å½•å®¹å™¨å³å¯ç™»å½•èŠ‚ç‚¹çš„shell\nkubectl exec -it nsenter-9yqytp bash å‚è€ƒï¼š\nå¦‚ä½•é€šè¿‡ kubectl è¿›å…¥ node shell - ä¸œé£å¾®é¸£æŠ€æœ¯åšå®¢\nGitHub - kvaps/kubectl-node-shell: Exec into node via kubectl\nhttps://krew.sigs.k8s.io/docs/user-guide/setup/install/\n","categories":"","description":"","excerpt":"æœ¬æ–‡ä»‹ç»å¦‚ä½•é€šè¿‡kubectlè¿›å…¥èŠ‚ç‚¹çš„shellç¯å¢ƒã€‚\n1. å®‰è£…krew node-shell 1.1. å®‰è£…krew ( set â€¦","ref":"/kubernetes-notes/operation/kubectl/kubectl-node-shell/","tags":["Kubernetes"],"title":"kubectlè¿›å…¥node shell"},{"body":" æœ¬æ–‡ä¸»è¦è®°å½•apisixç›¸å…³ç»„ä»¶é»˜è®¤é…ç½®ï¼Œä¾¿äºæŸ¥é˜…ã€‚\n1. apisixé…ç½® å‚è€ƒï¼šapisix/config-default.yaml at master Â· apache/apisix Â· GitHub\n# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # PLEASE DO NOT UPDATE THIS FILE! # If you want to set the specified configuration value, you can set the new # value in the conf/config.yaml file. # apisix: # node_listen: 9080 # APISIX listening port node_listen: # This style support multiple ports - 9080 # - port: 9081 # enable_http2: true # If not set, the default value is `false`. # - ip: 127.0.0.2 # Specific IP, If not set, the default value is `0.0.0.0`. # port: 9082 # enable_http2: true enable_admin: true enable_admin_cors: true # Admin API support CORS response headers. enable_dev_mode: false # Sets nginx worker_processes to 1 if set to true enable_reuseport: true # Enable nginx SO_REUSEPORT switch if set to true. show_upstream_status_in_response_header: false # when true all upstream status write to `X-APISIX-Upstream-Status` otherwise only 5xx code enable_ipv6: true config_center: etcd # etcd: use etcd to store the config value # yaml: fetch the config value from local yaml file `/your_path/conf/apisix.yaml` #proxy_protocol: # Proxy Protocol configuration #listen_http_port: 9181 # The port with proxy protocol for http, it differs from node_listen and admin_listen. # This port can only receive http request with proxy protocol, but node_listen \u0026 admin_listen # can only receive http request. If you enable proxy protocol, you must use this port to # receive http request with proxy protocol #listen_https_port: 9182 # The port with proxy protocol for https #enable_tcp_pp: true # Enable the proxy protocol for tcp proxy, it works for stream_proxy.tcp option #enable_tcp_pp_to_upstream: true # Enables the proxy protocol to the upstream server enable_server_tokens: true # Whether the APISIX version number should be shown in Server header. # It's enabled by default. # configurations to load third party code and/or override the builtin one. extra_lua_path: \"\" # extend lua_package_path to load third party code extra_lua_cpath: \"\" # extend lua_package_cpath to load third party code #lua_module_hook: \"my_project.my_hook\" # the hook module which will be used to inject third party code into APISIX proxy_cache: # Proxy Caching configuration cache_ttl: 10s # The default caching time in disk if the upstream does not specify the cache time zones: # The parameters of a cache - name: disk_cache_one # The name of the cache, administrator can specify # which cache to use by name in the admin api (disk|memory) memory_size: 50m # The size of shared memory, it's used to store the cache index for # disk strategy, store cache content for memory strategy (disk|memory) disk_size: 1G # The size of disk, it's used to store the cache data (disk) disk_path: /tmp/disk_cache_one # The path to store the cache data (disk) cache_levels: 1:2 # The hierarchy levels of a cache (disk) #- name: disk_cache_two # memory_size: 50m # disk_size: 1G # disk_path: \"/tmp/disk_cache_two\" # cache_levels: \"1:2\" - name: memory_cache memory_size: 50m allow_admin: # http://nginx.org/en/docs/http/ngx_http_access_module.html#allow - 127.0.0.0/24 # If we don't set any IP list, then any IP access is allowed by default. #- \"::/64\" #admin_listen: # use a separate port # ip: 127.0.0.1 # Specific IP, if not set, the default value is `0.0.0.0`. # port: 9180 #https_admin: true # enable HTTPS when use a separate port for Admin API. # Admin API will use conf/apisix_admin_api.crt and conf/apisix_admin_api.key as certificate. admin_api_mtls: # Depends on `admin_listen` and `https_admin`. admin_ssl_cert: \"\" # Path of your self-signed server side cert. admin_ssl_cert_key: \"\" # Path of your self-signed server side key. admin_ssl_ca_cert: \"\" # Path of your self-signed ca cert.The CA is used to sign all admin api callers' certificates. admin_api_version: v3 # The version of admin api, latest version is v3. # Default token when use API to call for Admin API. # *NOTE*: Highly recommended to modify this value to protect APISIX's Admin API. # Disabling this configuration item means that the Admin API does not # require any authentication. admin_key: - name: admin key: edd1c9f034335f136f87ad84b625c8f1 role: admin # admin: manage all configuration data # viewer: only can view configuration data - name: viewer key: 4054f7cf07e344346cd3f287985e76a2 role: viewer delete_uri_tail_slash: false # delete the '/' at the end of the URI # The URI normalization in servlet is a little different from the RFC's. # See https://github.com/jakartaee/servlet/blob/master/spec/src/main/asciidoc/servlet-spec-body.adoc#352-uri-path-canonicalization, # which is used under Tomcat. # Turn this option on if you want to be compatible with servlet when matching URI path. normalize_uri_like_servlet: false router: http: radixtree_uri # radixtree_uri: match route by uri(base on radixtree) # radixtree_host_uri: match route by host + uri(base on radixtree) # radixtree_uri_with_parameter: like radixtree_uri but match uri with parameters, # see https://github.com/api7/lua-resty-radixtree/#parameters-in-path for # more details. ssl: radixtree_sni # radixtree_sni: match route by SNI(base on radixtree) #stream_proxy: # TCP/UDP proxy # only: true # use stream proxy only, don't enable HTTP stuff # tcp: # TCP proxy port list # - addr: 9100 # tls: true # - addr: \"127.0.0.1:9101\" # udp: # UDP proxy port list # - 9200 # - \"127.0.0.1:9201\" #dns_resolver: # If not set, read from `/etc/resolv.conf` # - 1.1.1.1 # - 8.8.8.8 #dns_resolver_valid: 30 # if given, override the TTL of the valid records. The unit is second. resolver_timeout: 5 # resolver timeout enable_resolv_search_opt: true # enable search option in resolv.conf ssl: enable: true listen: # APISIX listening port in https. - port: 9443 enable_http2: true # - ip: 127.0.0.3 # Specific IP, If not set, the default value is `0.0.0.0`. # port: 9445 # enable_http2: true #ssl_trusted_certificate: /path/to/ca-cert # Specifies a file path with trusted CA certificates in the PEM format # used to verify the certificate when APISIX needs to do SSL/TLS handshaking # with external services (e.g. etcd) ssl_protocols: TLSv1.2 TLSv1.3 ssl_ciphers: ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl_session_tickets: false # disable ssl_session_tickets by default for 'ssl_session_tickets' would make Perfect Forward Secrecy useless. # ref: https://github.com/mozilla/server-side-tls/issues/135 key_encrypt_salt: edd1c9f0985e76a2 # If not set, will save origin ssl key into etcd. # If set this, must be a string of length 16. And it will encrypt ssl key with AES-128-CBC # !!! So do not change it after saving your ssl, it can't decrypt the ssl keys have be saved if you change !! #fallback_sni: \"my.default.domain\" # If set this, when the client doesn't send SNI during handshake, the fallback SNI will be used instead enable_control: true #control: # ip: 127.0.0.1 # port: 9090 disable_sync_configuration_during_start: false # safe exit. Remove this once the feature is stable nginx_config: # config for render the template to generate nginx.conf #user: root # specifies the execution user of the worker process. # the \"user\" directive makes sense only if the master process runs with super-user privileges. # if you're not root user,the default is current user. error_log: logs/error.log error_log_level: warn # warn,error worker_processes: auto # if you want use multiple cores in container, you can inject the number of cpu as environment variable \"APISIX_WORKER_PROCESSES\" enable_cpu_affinity: true # enable cpu affinity, this is just work well only on physical machine worker_rlimit_nofile: 20480 # the number of files a worker process can open, should be larger than worker_connections worker_shutdown_timeout: 240s # timeout for a graceful shutdown of worker processes max_pending_timers: 16384 # increase it if you see \"too many pending timers\" error max_running_timers: 4096 # increase it if you see \"lua_max_running_timers are not enough\" error event: worker_connections: 10620 #envs: # allow to get a list of environment variables # - TEST_ENV meta: lua_shared_dict: prometheus-metrics: 15m stream: enable_access_log: false # enable access log or not, default false access_log: logs/access_stream.log access_log_format: \"$remote_addr [$time_local] $protocol $status $bytes_sent $bytes_received $session_time\" # create your custom log format by visiting http://nginx.org/en/docs/varindex.html access_log_format_escape: default # allows setting json or default characters escaping in variables lua_shared_dict: etcd-cluster-health-check-stream: 10m lrucache-lock-stream: 10m plugin-limit-conn-stream: 10m # As user can add arbitrary configurations in the snippet, # it is user's responsibility to check the configurations # don't conflict with APISIX. main_configuration_snippet: | # Add custom Nginx main configuration to nginx.conf. # The configuration should be well indented! http_configuration_snippet: | # Add custom Nginx http configuration to nginx.conf. # The configuration should be well indented! http_server_configuration_snippet: | # Add custom Nginx http server configuration to nginx.conf. # The configuration should be well indented! http_server_location_configuration_snippet: | # Add custom Nginx http server location configuration to nginx.conf. # The configuration should be well indented! http_admin_configuration_snippet: | # Add custom Nginx admin server configuration to nginx.conf. # The configuration should be well indented! http_end_configuration_snippet: | # Add custom Nginx http end configuration to nginx.conf. # The configuration should be well indented! stream_configuration_snippet: | # Add custom Nginx stream configuration to nginx.conf. # The configuration should be well indented! http: enable_access_log: true # enable access log or not, default true access_log: logs/access.log access_log_format: \"$remote_addr - $remote_user [$time_local] $http_host \\\"$request\\\" $status $body_bytes_sent $request_time \\\"$http_referer\\\" \\\"$http_user_agent\\\" $upstream_addr $upstream_status $upstream_response_time \\\"$upstream_scheme://$upstream_host$upstream_uri\\\"\" access_log_format_escape: default # allows setting json or default characters escaping in variables keepalive_timeout: 60s # timeout during which a keep-alive client connection will stay open on the server side. client_header_timeout: 60s # timeout for reading client request header, then 408 (Request Time-out) error is returned to the client client_body_timeout: 60s # timeout for reading client request body, then 408 (Request Time-out) error is returned to the client client_max_body_size: 0 # The maximum allowed size of the client request body. # If exceeded, the 413 (Request Entity Too Large) error is returned to the client. # Note that unlike Nginx, we don't limit the body size by default. send_timeout: 10s # timeout for transmitting a response to the client.then the connection is closed underscores_in_headers: \"on\" # default enables the use of underscores in client request header fields real_ip_header: X-Real-IP # http://nginx.org/en/docs/http/ngx_http_realip_module.html#real_ip_header real_ip_recursive: \"off\" # http://nginx.org/en/docs/http/ngx_http_realip_module.html#real_ip_recursive real_ip_from: # http://nginx.org/en/docs/http/ngx_http_realip_module.html#set_real_ip_from - 127.0.0.1 - \"unix:\" #custom_lua_shared_dict: # add custom shared cache to nginx.conf # ipc_shared_dict: 100m # custom shared cache, format: `cache-key: cache-size` # Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) # when establishing a connection with the proxied HTTPS server. proxy_ssl_server_name: true upstream: keepalive: 320 # Sets the maximum number of idle keepalive connections to upstream servers that are preserved in the cache of each worker process. # When this number is exceeded, the least recently used connections are closed. keepalive_requests: 1000 # Sets the maximum number of requests that can be served through one keepalive connection. # After the maximum number of requests is made, the connection is closed. keepalive_timeout: 60s # Sets a timeout during which an idle keepalive connection to an upstream server will stay open. charset: utf-8 # Adds the specified charset to the \"Content-Type\" response header field, see # http://nginx.org/en/docs/http/ngx_http_charset_module.html#charset variables_hash_max_size: 2048 # Sets the maximum size of the variables hash table. lua_shared_dict: internal-status: 10m plugin-limit-req: 10m plugin-limit-count: 10m prometheus-metrics: 10m plugin-limit-conn: 10m upstream-healthcheck: 10m worker-events: 10m lrucache-lock: 10m balancer-ewma: 10m balancer-ewma-locks: 10m balancer-ewma-last-touched-at: 10m plugin-limit-count-redis-cluster-slot-lock: 1m tracing_buffer: 10m plugin-api-breaker: 10m etcd-cluster-health-check: 10m discovery: 1m jwks: 1m introspection: 10m access-tokens: 1m ext-plugin: 1m kubernetes: 1m tars: 1m etcd: host: # it's possible to define multiple etcd hosts addresses of the same etcd cluster. - \"http://127.0.0.1:2379\" # multiple etcd address, if your etcd cluster enables TLS, please use https scheme, # e.g. https://127.0.0.1:2379. prefix: /apisix # apisix configurations prefix #timeout: 30 # 30 seconds #resync_delay: 5 # when sync failed and a rest is needed, resync after the configured seconds plus 50% random jitter #health_check_timeout: 10 # etcd retry the unhealthy nodes after the configured seconds startup_retry: 2 # the number of retry to etcd during the startup, default to 2 #user: root # root username for etcd #password: 5tHkHhYkjr6cQY # root password for etcd tls: # To enable etcd client certificate you need to build APISIX-Base, see # https://apisix.apache.org/docs/apisix/FAQ#how-do-i-build-the-apisix-base-environment #cert: /path/to/cert # path of certificate used by the etcd client #key: /path/to/key # path of key used by the etcd client verify: true # whether to verify the etcd endpoint certificate when setup a TLS connection to etcd, # the default value is true, e.g. the certificate will be verified strictly. #sni: # the SNI for etcd TLS requests. If missed, the host part of the URL will be used. # HashiCorp Vault storage backend for sensitive data retrieval. The config shows an example of what APISIX expects if you # wish to integrate Vault for secret (sensetive string, public private keys etc.) retrieval. APISIX communicates with Vault # server HTTP APIs. By default, APISIX doesn't need this configuration. # vault: # host: \"http://0.0.0.0:8200\" # The host address where the vault server is running. # timeout: 10 # request timeout 30 seconds # token: root # Authentication token to access Vault HTTP APIs # prefix: kv/apisix # APISIX supports vault kv engine v1, where sensitive data are being stored # and retrieved through vault HTTP APIs. enabling a prefix allows you to better enforcement of # policies, generate limited scoped tokens and tightly control the data that can be accessed # from APISIX. #discovery: # service discovery center # dns: # servers: # - \"127.0.0.1:8600\" # use the real address of your dns server # eureka: # host: # it's possible to define multiple eureka hosts addresses of the same eureka cluster. # - \"http://127.0.0.1:8761\" # prefix: /eureka/ # fetch_interval: 30 # default 30s # weight: 100 # default weight for node # timeout: # connect: 2000 # default 2000ms # send: 2000 # default 2000ms # read: 5000 # default 5000ms # nacos: # host: # - \"http://${username}:${password}@${host1}:${port1}\" # prefix: \"/nacos/v1/\" # fetch_interval: 30 # default 30 sec # weight: 100 # default 100 # timeout: # connect: 2000 # default 2000 ms # send: 2000 # default 2000 ms # read: 5000 # default 5000 ms # consul_kv: # servers: # - \"http://127.0.0.1:8500\" # - \"http://127.0.0.1:8600\" # prefix: \"upstreams\" # skip_keys: # if you need to skip special keys # - \"upstreams/unused_api/\" # timeout: # connect: 2000 # default 2000 ms # read: 2000 # default 2000 ms # wait: 60 # default 60 sec # weight: 1 # default 1 # fetch_interval: 3 # default 3 sec, only take effect for keepalive: false way # keepalive: true # default true, use the long pull way to query consul servers # default_server: # you can define default server when missing hit # host: \"127.0.0.1\" # port: 20999 # metadata: # fail_timeout: 1 # default 1 ms # weight: 1 # default 1 # max_fails: 1 # default 1 # dump: # if you need, when registered nodes updated can dump into file # path: \"logs/consul_kv.dump\" # expire: 2592000 # unit sec, here is 30 day # kubernetes: # service: # schema: https #apiserver schema, options [http, https], default https # host: ${KUBERNETES_SERVICE_HOST} #apiserver host, options [ipv4, ipv6, domain, environment variable], default ${KUBERNETES_SERVICE_HOST} # port: ${KUBERNETES_SERVICE_PORT} #apiserver port, options [port number, environment variable], default ${KUBERNETES_SERVICE_PORT} # client: # # serviceaccount token or path of serviceaccount token_file # token_file: ${KUBERNETES_CLIENT_TOKEN_FILE} # # token: |- # # eyJhbGciOiJSUzI1NiIsImtpZCI6Ikx5ME1DNWdnbmhQNkZCNlZYMXBsT3pYU3BBS2swYzBPSkN3ZnBESGpkUEEif # # 6Ikx5ME1DNWdnbmhQNkZCNlZYMXBsT3pYU3BBS2swYzBPSkN3ZnBESGpkUEEifeyJhbGciOiJSUzI1NiIsImtpZCI # # kubernetes discovery plugin support use namespace_selector # # you can use one of [equal, not_equal, match, not_match] filter namespace # namespace_selector: # # only save endpoints with namespace equal default # equal: default # # only save endpoints with namespace not equal default # #not_equal: default # # only save endpoints with namespace match one of [default, ^my-[a-z]+$] # #match: # #- default # #- ^my-[a-z]+$ # # only save endpoints with namespace not match one of [default, ^my-[a-z]+$ ] # #not_match: # #- default # #- ^my-[a-z]+$ # # kubernetes discovery plugin support use label_selector # # for the expression of label_selector, please refer to https://kubernetes.io/docs/concepts/overview/working-with-objects/labels # label_selector: |- # first=\"a\",second=\"b\" graphql: max_size: 1048576 # the maximum size limitation of graphql in bytes, default 1MiB #ext-plugin: #cmd: [\"ls\", \"-l\"] plugins: # plugin list (sorted by priority) - real-ip # priority: 23000 - client-control # priority: 22000 - proxy-control # priority: 21990 - request-id # priority: 12015 - zipkin # priority: 12011 #- skywalking # priority: 12010 #- opentelemetry # priority: 12009 - ext-plugin-pre-req # priority: 12000 - fault-injection # priority: 11000 - mocking # priority: 10900 - serverless-pre-function # priority: 10000 #- batch-requests # priority: 4010 - cors # priority: 4000 - ip-restriction # priority: 3000 - ua-restriction # priority: 2999 - referer-restriction # priority: 2990 - csrf # priority: 2980 - uri-blocker # priority: 2900 - request-validation # priority: 2800 - openid-connect # priority: 2599 - authz-casbin # priority: 2560 - authz-casdoor # priority: 2559 - wolf-rbac # priority: 2555 - ldap-auth # priority: 2540 - hmac-auth # priority: 2530 - basic-auth # priority: 2520 - jwt-auth # priority: 2510 - key-auth # priority: 2500 - consumer-restriction # priority: 2400 - forward-auth # priority: 2002 - opa # priority: 2001 - authz-keycloak # priority: 2000 #- error-log-logger # priority: 1091 - proxy-mirror # priority: 1010 - proxy-cache # priority: 1009 - proxy-rewrite # priority: 1008 - workflow # priority: 1006 - api-breaker # priority: 1005 - limit-conn # priority: 1003 - limit-count # priority: 1002 - limit-req # priority: 1001 #- node-status # priority: 1000 - gzip # priority: 995 - server-info # priority: 990 - traffic-split # priority: 966 - redirect # priority: 900 - response-rewrite # priority: 899 - kafka-proxy # priority: 508 #- dubbo-proxy # priority: 507 - grpc-transcode # priority: 506 - grpc-web # priority: 505 - public-api # priority: 501 - prometheus # priority: 500 - datadog # priority: 495 - echo # priority: 412 - loggly # priority: 411 - http-logger # priority: 410 - splunk-hec-logging # priority: 409 - skywalking-logger # priority: 408 - google-cloud-logging # priority: 407 - sls-logger # priority: 406 - tcp-logger # priority: 405 - kafka-logger # priority: 403 - rocketmq-logger # priority: 402 - syslog # priority: 401 - udp-logger # priority: 400 - file-logger # priority: 399 - clickhouse-logger # priority: 398 - tencent-cloud-cls # priority: 397 #- log-rotate # priority: 100 # \u003c- recommend to use priority (0, 100) for your custom plugins - example-plugin # priority: 0 - aws-lambda # priority: -1899 - azure-functions # priority: -1900 - openwhisk # priority: -1901 - serverless-post-function # priority: -2000 - ext-plugin-post-req # priority: -3000 - ext-plugin-post-resp # priority: -4000 stream_plugins: # sorted by priority - ip-restriction # priority: 3000 - limit-conn # priority: 1003 - mqtt-proxy # priority: 1000 #- prometheus # priority: 500 - syslog # priority: 401 # \u003c- recommend to use priority (0, 100) for your custom plugins #wasm: #plugins: #- name: wasm_log #priority: 7999 #file: t/wasm/log/main.go.wasm #xrpc: #protocols: #- name: pingpong plugin_attr: log-rotate: interval: 3600 # rotate interval (unit: second) max_kept: 168 # max number of log files will be kept max_size: -1 # max size bytes of log files to be rotated, size check would be skipped with a value less than 0 enable_compression: false # enable log file compression(gzip) or not, default false skywalking: service_name: APISIX service_instance_name: APISIX Instance Name endpoint_addr: http://127.0.0.1:12800 opentelemetry: trace_id_source: x-request-id resource: service.name: APISIX collector: address: 127.0.0.1:4318 request_timeout: 3 request_headers: Authorization: token batch_span_processor: drop_on_queue_full: false max_queue_size: 1024 batch_timeout: 2 inactive_timeout: 1 max_export_batch_size: 16 prometheus: export_uri: /apisix/prometheus/metrics metric_prefix: apisix_ enable_export_server: true export_addr: ip: 127.0.0.1 port: 9091 #metrics: # http_status: # # extra labels from nginx variables # extra_labels: # # the label name doesn't need to be the same as variable name # # below labels are only examples, you could add any valid variables as you need # - upstream_addr: $upstream_addr # - upstream_status: $upstream_status # http_latency: # extra_labels: # - upstream_addr: $upstream_addr # bandwidth: # extra_labels: # - upstream_addr: $upstream_addr server-info: report_ttl: 60 # live time for server info in etcd (unit: second) dubbo-proxy: upstream_multiplex_count: 32 request-id: snowflake: enable: false snowflake_epoc: 1609459200000 # the starting timestamp is expressed in milliseconds data_machine_bits: 12 # data machine bit, maximum 31, because Lua cannot do bit operations greater than 31 sequence_bits: 10 # each machine generates a maximum of (1 \u003c\u003c sequence_bits) serial numbers per millisecond data_machine_ttl: 30 # live time for data_machine in etcd (unit: second) data_machine_interval: 10 # lease renewal interval in etcd (unit: second) proxy-mirror: timeout: # proxy timeout in mirrored sub-request connect: 60s read: 60s send: 60s # redirect: # https_port: 8443 # the default port for use by HTTP redirects to HTTPS #deployment: # role: traditional # role_traditional: # config_provider: etcd # etcd: # host: # it's possible to define multiple etcd hosts addresses of the same etcd cluster. # - \"http://127.0.0.1:2379\" # multiple etcd address, if your etcd cluster enables TLS, please use https scheme, # # e.g. https://127.0.0.1:2379. # prefix: /apisix # configuration prefix in etcd # timeout: 30 # 30 seconds 2. apisix-ingress-controller å‚è€ƒï¼šapisix-ingress-controller/config-default.yaml at master Â· apache/apisix-ingress-controller Â· GitHub\n# Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # log options log_level: \"info\" # the error log level, default is info, optional values are: # debug # info # warn # error # panic # fatal log_output: \"stderr\" # the output file path of error log, default is stderr, when # the file path is \"stderr\" or \"stdout\", logs are marshalled # plainly, which is more readable for human; otherwise logs # are marshalled in JSON format, which can be parsed by # programs easily. log_rotate_output_path: \"\" # rotate output path, the logs will be written in this file log_rotation_max_size: 100 # rotate max size, max size in megabytes of log file before it get rotated. It defaults to 100 log_rotation_max_age: 0 # rotate max age, max age of old log files to retain log_rotation_max_backups: 0 # rotate max backups, max numbers of old log files to retain cert_file: \"/etc/webhook/certs/cert.pem\" # the TLS certificate file path. key_file: \"/etc/webhook/certs/key.pem\" # the TLS key file path. http_listen: \":8080\" # the HTTP Server listen address, default is \":8080\" https_listen: \":8443\" # the HTTPS Server listen address, default is \":8443\" ingress_publish_service: \"\" # the controller will use the Endpoint of this Service to # update the status information of the Ingress resource. # The format is \"namespace/svc-name\" to solve the situation that # the data plane and the controller are not deployed in the same namespace. ingress_status_address: [] # when there is no available information on the Service # used for publishing on the data plane, # the static address provided here will be # used to update the status information of Ingress. # When ingress-publish-service is specified at the same time, ingress-status-address is preferred. # For example, no available LB exists in the bare metal environment. enable_profiling: true # enable profiling via web interfaces # host:port/debug/pprof, default is true. apisix-resource-sync-interval: \"300s\" # Default interval for synchronizing Kubernetes resources to APISIX # Kubernetes related configurations. kubernetes: kubeconfig: \"\" # the Kubernetes configuration file path, default is # \"\", so the in-cluster configuration will be used. resync_interval: \"6h\" # how long should apisix-ingress-controller # re-synchronizes with Kubernetes, default is 6h, # and the minimal resync interval is 30s. app_namespaces: [\"*\"] # namespace list that controller will watch for resources, # by default all namespaces (represented by \"*\") are watched. # The `app_namespace` is deprecated, using `namespace_selector` instead since version 1.4.0 namespace_selector: [\"\"] # namespace_selector represent basis for selecting managed namespaces. # the field is support since version 1.4.0 # For example, \"apisix.ingress=watching\", so ingress will watching the namespaces which labels \"apisix.ingress=watching\" election_id: \"ingress-apisix-leader\" # the election id for the controller leader campaign, # only the leader will watch and delivery resource changes, # other instances (as candidates) stand by. ingress_class: \"apisix\" # the class of an Ingress object is set using the field # IngressClassName in Kubernetes clusters version v1.18.0 # or higher or the annotation \"kubernetes.io/ingress.class\" # (deprecated). ingress_version: \"networking/v1\" # the supported ingress api group version, can be \"networking/v1beta1\" # , \"networking/v1\" (for Kubernetes version v1.19.0 or higher), and # \"extensions/v1beta1\", default is \"networking/v1\". watch_endpointslices: false # whether to watch EndpointSlices rather than Endpoints. apisix_route_version: \"apisix.apache.org/v2\" # the supported apisixroute api group version. # the latest version is \"apisix.apache.org/v2\". enable_gateway_api: false # whether to enable support for Gateway API. # Note: This feature is currently under development and may not work as expected. # It is not recommended to use it in a production environment. # Before we announce support for it to reach Beta level or GA. api_version: apisix.apache.org/v2 # the default value of API version is \"apisix.apache.org/v2\", support \"apisix.apache.org/v2beta3\" and \"apisix.apache.org/v2\". # APISIX related configurations. apisix: default_cluster_base_url: \"http://127.0.0.1:9080/apisix/admin\" # The base url of admin api / manager api # of the default APISIX cluster default_cluster_admin_key: \"\" # the admin key used for the authentication of admin api / manager api in the # default APISIX cluster, by default this field is unset. default_cluster_name: \"default\" # name of the default APISIX cluster. 3. apisix-dashboard å‚è€ƒï¼šapisix-dashboard/conf.yaml at master Â· apache/apisix-dashboard Â· GitHub\n# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # yamllint disable rule:comments-indentation conf: listen: # host: 127.0.0.1 # the address on which the `Manager API` should listen. # The default value is 0.0.0.0, if want to specify, please enable it. # This value accepts IPv4, IPv6, and hostname. port: 9000 # The port on which the `Manager API` should listen. # ssl: # host: 127.0.0.1 # the address on which the `Manager API` should listen for HTTPS. # The default value is 0.0.0.0, if want to specify, please enable it. # port: 9001 # The port on which the `Manager API` should listen for HTTPS. # cert: \"/tmp/cert/example.crt\" # Path of your SSL cert. # key: \"/tmp/cert/example.key\" # Path of your SSL key. allow_list: # If we don't set any IP list, then any IP access is allowed by default. - 127.0.0.1 # The rules are checked in sequence until the first match is found. - ::1 # In this example, access is allowed only for IPv4 network 127.0.0.1, and for IPv6 network ::1. # It also support CIDR like 192.168.1.0/24 and 2001:0db8::/32 etcd: endpoints: # supports defining multiple etcd host addresses for an etcd cluster - 127.0.0.1:2379 # yamllint disable rule:comments-indentation # etcd basic auth info # username: \"root\" # ignore etcd username if not enable etcd auth # password: \"123456\" # ignore etcd password if not enable etcd auth mtls: key_file: \"\" # Path of your self-signed client side key cert_file: \"\" # Path of your self-signed client side cert ca_file: \"\" # Path of your self-signed ca cert, the CA is used to sign callers' certificates # prefix: /apisix # apisix config's prefix in etcd, /apisix by default log: error_log: level: warn # supports levels, lower to higher: debug, info, warn, error, panic, fatal file_path: logs/error.log # supports relative path, absolute path, standard output # such as: logs/error.log, /tmp/logs/error.log, /dev/stdout, /dev/stderr # such as absolute path on Windows: winfile:///C:\\error.log access_log: file_path: logs/access.log # supports relative path, absolute path, standard output # such as: logs/access.log, /tmp/logs/access.log, /dev/stdout, /dev/stderr # such as absolute path on Windows: winfile:///C:\\access.log # log example: 2020-12-09T16:38:09.039+0800 INFO filter/logging.go:46 /apisix/admin/routes/r1 {\"status\": 401, \"host\": \"127.0.0.1:9000\", \"query\": \"asdfsafd=adf\u0026a=a\", \"requestId\": \"3d50ecb8-758c-46d1-af5b-cd9d1c820156\", \"latency\": 0, \"remoteIP\": \"127.0.0.1\", \"method\": \"PUT\", \"errs\": []} max_cpu: 0 # supports tweaking with the number of OS threads are going to be used for parallelism. Default value: 0 [will use max number of available cpu cores considering hyperthreading (if any)]. If the value is negative, is will not touch the existing parallelism profile. # security: # access_control_allow_origin: \"http://httpbin.org\" # access_control_allow_credentials: true # support using custom cors configration # access_control_allow_headers: \"Authorization\" # access_control-allow_methods: \"*\" # x_frame_options: \"deny\" # content_security_policy: \"default-src 'self'; script-src 'self' 'unsafe-eval' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; frame-src xx.xx.xx.xx:3000\" # You can set frame-src to provide content for your grafana panel. authentication: secret: secret # secret for jwt token generation. # NOTE: Highly recommended to modify this value to protect `manager api`. # if it's default value, when `manager api` start, it will generate a random string to replace it. expire_time: 3600 # jwt token expire time, in second users: # yamllint enable rule:comments-indentation - username: admin # username and password for login `manager api` password: admin - username: user password: user plugins: # plugin list (sorted in alphabetical order) - api-breaker - authz-keycloak - basic-auth - batch-requests - consumer-restriction - cors # - dubbo-proxy - echo # - error-log-logger # - example-plugin - fault-injection - grpc-transcode - hmac-auth - http-logger - ip-restriction - jwt-auth - kafka-logger - key-auth - limit-conn - limit-count - limit-req # - log-rotate # - node-status - openid-connect - prometheus - proxy-cache - proxy-mirror - proxy-rewrite - redirect - referer-restriction - request-id - request-validation - response-rewrite - serverless-post-function - serverless-pre-function # - skywalking - sls-logger - syslog - tcp-logger - udp-logger - uri-blocker - wolf-rbac - zipkin - server-info - traffic-split ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦è®°å½•apisixç›¸å…³ç»„ä»¶é»˜è®¤é…ç½®ï¼Œä¾¿äºæŸ¥é˜…ã€‚\n1. apisixé…ç½® å‚è€ƒï¼šapisix/config-default.yaml â€¦","ref":"/kubernetes-notes/network/gateway/apisix-config/","tags":["ApiSix"],"title":"APISIXé…ç½®"},{"body":"å‡½æ•° 1. å‡½æ•°å®šä¹‰ä¸è°ƒç”¨ //1ã€å‡½æ•°ç»„æˆï¼šå…³é”®å­—func ,å‡½æ•°åï¼Œå‚æ•°åˆ—è¡¨ï¼Œè¿”å›å€¼ï¼Œå‡½æ•°ä½“ï¼Œè¿”å›è¯­å¥ //å…ˆåç§°åç±»å‹ func å‡½æ•°å(å‚æ•°åˆ—è¡¨)(è¿”å›å€¼åˆ—è¡¨){ //å‚æ•°åˆ—è¡¨å’Œè¿”å›å€¼åˆ—è¡¨ä»¥å˜é‡å£°æ˜çš„å½¢å¼ï¼Œå¦‚æœå•è¿”å›å€¼å¯ä»¥ç›´æ¥åŠ ç±»å‹ å‡½æ•°ä½“ return //è¿”å›è¯­å¥ } //ä¾‹å­ func Add(a,b int)(ret int,err error){ //å‡½æ•°ä½“ return //returnè¯­å¥ } //2ã€å‡½æ•°è°ƒç”¨ //å…ˆå¯¼å…¥å‡½æ•°æ‰€åœ¨çš„åŒ…ï¼Œç›´æ¥è°ƒç”¨å‡½æ•° import \"mymath\" sum,err:=mymath.Add(1,2) //å¤šè¿”å›å€¼å’Œé”™è¯¯å¤„ç†æœºåˆ¶ //å¯è§æ€§ï¼ŒåŒ…æ‹¬å‡½æ•°ã€ç±»å‹ã€å˜é‡ //æœ¬åŒ…å†…å¯è§(private)ï¼šå°å†™å­—æ¯å¼€å¤´ //åŒ…å¤–å¯è§(public)ï¼šå¤§å†™å­—æ¯å¼€å¤´ 2. ä¸å®šå‚æ•° //1ã€ä¸å®šå‚æ•°çš„ç±»å‹ func myfunc(args ...int){ //...typeä¸å®šå‚æ•°çš„ç±»å‹ï¼Œå¿…é¡»æ˜¯æœ€åä¸€ä¸ªå‚æ•°ï¼Œæœ¬è´¨æ˜¯åˆ‡ç‰‡ for _,arg:=range args{ fmt.Println(arg) } } //å‡½æ•°è°ƒç”¨,ä¼ å‚å¯ä»¥é€‰æ‹©å¤šä¸ªï¼Œä¸ªæ•°ä¸å®š myfunc(1,2,3) myfunc(1,2,3,4,5) //2ã€ä¸å®šå‚æ•°çš„ä¼ é€’ï¼Œå‡å¦‚æœ‰ä¸ªå˜å‚å‡½æ•°myfunc2(args ...int) func myfunc1(args ...int){ //æŒ‰åŸæ ·ä¼ é€’ myfunc2(args...) //ä¼ é€’åˆ‡ç‰‡ myfunc2(args[1:]...) } //3ã€ä»»æ„ç±»å‹çš„ä¸å®šå‚æ•°ï¼Œä½¿ç”¨interface{}ä½œä¸ºæŒ‡å®šç±»å‹ func Printf(format string,args ...interface{}){ //æ­¤ä¸ºæ ‡å‡†åº“ä¸­fmt.Printf()å‡½æ•°çš„åŸå‹ //å‡½æ•°ä½“ } 3. å¤šè¿”å›å€¼ //å¤šè¿”å›å€¼ func (file *File) Read(b []byte) (n int,err error) //ä½¿ç”¨ä¸‹åˆ’çº¿\"_\"æ¥ä¸¢å¼ƒè¿”å›å€¼ n,_:=f.Read(buf) 4. åŒ¿åå‡½æ•° åŒ¿åå‡½æ•°ï¼šä¸å¸¦å‡½æ•°åçš„å‡½æ•°ï¼Œå¯ä»¥åƒå˜é‡ä¸€æ ·è¢«ä¼ é€’ã€‚\nfunc(a,b int,z float32) bool{ //æ²¡æœ‰å‡½æ•°å return a*b\u003cint(z) } f:=func(x,y int) int{ return x+y } 5. é—­åŒ… 5.1. é—­åŒ…çš„æ¦‚å¿µ é—­åŒ…æ˜¯å¯ä»¥åŒ…å«è‡ªç”±å˜é‡ï¼ˆæœªç»‘å®šåˆ°ç‰¹å®šçš„å¯¹è±¡ï¼‰çš„ä»£ç å—ï¼Œè¿™äº›å˜é‡ä¸åœ¨ä»£ç å—å†…æˆ–å…¨å±€ä¸Šä¸‹æ–‡ä¸­å®šä¹‰ï¼Œè€Œåœ¨å®šä¹‰ä»£ç å—çš„ç¯å¢ƒä¸­å®šä¹‰ã€‚è¦æ‰§è¡Œçš„ä»£ç å—ä¸ºè‡ªç”±å˜é‡æä¾›ç»‘å®šçš„è®¡ç®—ç¯å¢ƒï¼ˆä½œç”¨åŸŸï¼‰ã€‚\n5.2. é—­åŒ…çš„ä»·å€¼ é—­åŒ…çš„ä»·å€¼åœ¨äºå¯ä»¥ä½œä¸ºä¸€ä¸ªå˜é‡å¯¹è±¡æ¥è¿›è¡Œä¼ é€’å’Œè¿”å›ã€‚å³å¯ä»¥æŠŠå‡½æ•°æœ¬èº«çœ‹ä½œæ˜¯ä¸€ä¸ªå˜é‡ã€‚\n5.3. Goä¸­çš„é—­åŒ… Goé—­åŒ…æ˜¯æŒ‡å¼•ç”¨äº†å‡½æ•°å¤–çš„å˜é‡çš„ä¸€ç§å‡½æ•°ï¼Œè¿™æ ·è¯¥å‡½æ•°å°±è¢«ç»‘å®šåœ¨æŸä¸ªå˜é‡ä¸Šï¼Œåªè¦é—­åŒ…è¿˜è¢«ä½¿ç”¨åˆ™å¼•ç”¨çš„å˜é‡ä¼šä¸€ç›´å­˜åœ¨ã€‚\nGoçš„åŒ¿åå‡½æ•°æ˜¯ä¸€ä¸ªé—­åŒ…ï¼ŒGoé—­åŒ…å¸¸ç”¨åœ¨goå’Œdeferå…³é”®å­—ä¸­ã€‚\n5.4. é—­åŒ…çš„å‘ åœ¨for rangeä¸­goroutineçš„æ–¹å¼ä½¿ç”¨é—­åŒ…ï¼Œå¦‚æœæ²¡æœ‰ç»™åŒ¿åå‡½æ•°ä¼ å…¥ä¸€ä¸ªå˜é‡ï¼Œæˆ–æ–°å»ºä¸€ä¸ªå˜é‡å­˜å‚¨è¿­ä»£çš„å˜é‡ï¼Œé‚£ä¹ˆgoroutineæ‰§è¡Œçš„ç»“æœä¼šæ˜¯æœ€åä¸€ä¸ªè¿­ä»£å˜é‡çš„ç»“æœï¼Œè€Œä¸æ˜¯æ¯ä¸ªè¿­ä»£å˜é‡çš„ç»“æœã€‚è¿™æ˜¯å› ä¸ºå¦‚æœæ²¡æœ‰é€šè¿‡ä¸€ä¸ªå˜é‡æ¥æ‹·è´è¿­ä»£å˜é‡ï¼Œé‚£ä¹ˆé—­åŒ…å› ä¸ºç»‘å®šäº†å˜é‡ï¼Œå½“æ¯ä¸ªgroutineè¿è¡Œæ—¶ï¼Œè¿­ä»£å˜é‡å¯èƒ½è¢«æ›´æ”¹ã€‚\nç¤ºä¾‹å¦‚ä¸‹ï¼š\n// false, print 3 3 3 values := []int{1,2,3} for _, val := range values { go func() { fmt.Println(val) }() } // true, print 1 2 3 for _, val := range values { go func(val interface{}) { fmt.Println(val) }(val) } 5.5 é—­åŒ…çš„ç¤ºä¾‹ closure.go\npackage main import ( \"fmt\" ) func main() { var j int = 5 a := func() func() { var i int = 10 return func() { fmt.Printf(\"i, j: %d, %d\\n\", i, j) } }() a() j *= 2 a() } 5.6 é—­åŒ…çš„å‚è€ƒé“¾æ¥ https://tour.golang.org/moretypes/25 https://golang.org/doc/faq#closures_and_goroutines https://github.com/golang/go/wiki/CommonMistakes å‚è€ƒï¼š\nã€ŠGoè¯­è¨€ç¼–ç¨‹ã€‹ ","categories":"","description":"","excerpt":"å‡½æ•° 1. å‡½æ•°å®šä¹‰ä¸è°ƒç”¨ //1ã€å‡½æ•°ç»„æˆï¼šå…³é”®å­—func ,å‡½æ•°åï¼Œå‚æ•°åˆ—è¡¨ï¼Œè¿”å›å€¼ï¼Œå‡½æ•°ä½“ï¼Œè¿”å›è¯­å¥ //å…ˆåç§°åç±»å‹ func å‡½æ•° â€¦","ref":"/golang-notes/basis/functions/","tags":["Golang"],"title":"å‡½æ•°ä¸é—­åŒ…"},{"body":"1. èµ„æºè§†å›¾éš”ç¦» å®¹å™¨ä¸­çš„æ‰§è¡Œtopã€freeç­‰å‘½ä»¤å±•ç¤ºå‡ºæ¥çš„CPUï¼Œå†…å­˜ç­‰ä¿¡æ¯æ˜¯ä»/procç›®å½•ä¸­çš„ç›¸å…³æ–‡ä»¶é‡Œè¯»å–å‡ºæ¥çš„ã€‚è€Œå®¹å™¨å¹¶æ²¡æœ‰å¯¹/procï¼Œ/sysç­‰æ–‡ä»¶ç³»ç»Ÿåšéš”ç¦»ï¼Œå› æ­¤å®¹å™¨ä¸­è¯»å–å‡ºæ¥çš„CPUå’Œå†…å­˜çš„ä¿¡æ¯æ˜¯å®¿ä¸»æœºçš„ä¿¡æ¯ï¼Œä¸å®¹å™¨å®é™…åˆ†é…å’Œé™åˆ¶çš„èµ„æºé‡ä¸åŒã€‚\n/proc/cpuinfo /proc/diskstats /proc/meminfo /proc/stat /proc/swaps /proc/uptime ä¸ºäº†å®ç°è®©å®¹å™¨å†…éƒ¨çš„èµ„æºè§†å›¾æ›´åƒè™šæ‹Ÿæœºï¼Œä½¿å¾—åº”ç”¨ç¨‹åºå¯ä»¥æ‹¿åˆ°çœŸå®çš„CPUå’Œå†…å­˜ä¿¡æ¯ï¼Œå°±éœ€è¦é€šè¿‡æ–‡ä»¶æŒ‚è½½çš„æ–¹å¼å°†cgroupçš„çœŸå®çš„å®¹å™¨èµ„æºä¿¡æ¯æŒ‚è½½åˆ°å®¹å™¨å†…/procä¸‹çš„æ–‡ä»¶ï¼Œä½¿å¾—å®¹å™¨å†…æ‰§è¡Œtopã€freeç­‰å‘½ä»¤æ—¶å¯ä»¥æ‹¿åˆ°çœŸå®çš„CPUå’Œå†…å­˜ä¿¡æ¯ã€‚\n2. Lxcfsç®€ä»‹ lxcfsæ˜¯ä¸€ä¸ªFUSEæ–‡ä»¶ç³»ç»Ÿï¼Œä½¿å¾—Linuxå®¹å™¨çš„æ–‡ä»¶ç³»ç»Ÿæ›´åƒè™šæ‹Ÿæœºã€‚lxcfsæ˜¯ä¸€ä¸ªå¸¸é©»è¿›ç¨‹è¿è¡Œåœ¨å®¿ä¸»æœºä¸Šï¼Œä»è€Œæ¥è‡ªåŠ¨ç»´æŠ¤å®¿ä¸»æœºcgroupä¸­å®¹å™¨çš„çœŸå®èµ„æºä¿¡æ¯ä¸å®¹å™¨å†…/procä¸‹æ–‡ä»¶çš„æ˜ å°„å…³ç³»ã€‚\nlxcfsçš„å‘½ä»¤ä¿¡æ¯å¦‚ä¸‹ï¼š\n#/usr/local/bin/lxcfs -h Usage: lxcfs [-f|-d] -u -l -n [-p pidfile] mountpoint -f running foreground by default; -d enable debug output -l use loadavg -u no swap Default pidfile is /run/lxcfs.pid lxcfs -h lxcfsçš„æºç ï¼šhttps://github.com/lxc/lxcfs\n3. LxcfsåŸç† lxcfså®ç°çš„åŸºæœ¬åŸç†æ˜¯é€šè¿‡æ–‡ä»¶æŒ‚è½½çš„æ–¹å¼ï¼ŒæŠŠcgroupä¸­å®¹å™¨ç›¸å…³çš„ä¿¡æ¯è¯»å–å‡ºæ¥ï¼Œå­˜å‚¨åˆ°lxcfsç›¸å…³çš„ç›®å½•ä¸‹ï¼Œå¹¶å°†ç›¸å…³ç›®å½•æ˜ å°„åˆ°å®¹å™¨å†…çš„/procç›®å½•ä¸‹ï¼Œä»è€Œä½¿å¾—å®¹å™¨å†…æ‰§è¡Œtop,freeç­‰å‘½ä»¤æ—¶æ‹¿åˆ°çš„/procä¸‹çš„æ•°æ®æ˜¯çœŸå®çš„cgroupåˆ†é…ç»™å®¹å™¨çš„CPUå’Œå†…å­˜æ•°æ®ã€‚\nåŸç†å›¾\næ˜ å°„ç›®å½•\nç±»åˆ« å®¹å™¨å†…ç›®å½• å®¿ä¸»æœºlxcfsç›®å½• cpu /proc/cpuinfo /var/lib/lxcfs/{container_id}/proc/cpuinfo å†…å­˜ /proc/meminfo /var/lib/lxcfs/{container_id}/proc/meminfo /proc/diskstats /var/lib/lxcfs/{container_id}/proc/diskstats /proc/stat /var/lib/lxcfs/{container_id}/proc/stat /proc/swaps /var/lib/lxcfs/{container_id}/proc/swaps /proc/uptime /var/lib/lxcfs/{container_id}/proc/uptime /proc/loadavg /var/lib/lxcfs/{container_id}/proc/loadavg /sys/devices/system/cpu/online /var/lib/lxcfs/{container_id}/sys/devices/system/cpu/online 4. ä½¿ç”¨æ–¹å¼ 4.1. å®‰è£…lxcfs ç¯å¢ƒå‡†å¤‡\nyum install -y fuse fuse-lib fuse-devel æºç ç¼–è¯‘å®‰è£…\ngit clone git://github.com/lxc/lxcfs cd lxcfs ./bootstrap.sh ./configure make make install æˆ–è€…é€šè¿‡rpmåŒ…å®‰è£…\nwget https://copr-be.cloud.fedoraproject.org/results/ganto/lxc3/epel-7-x86_64/01041891-lxcfs/lxcfs-3.1.2-0.2.el7.x86_64.rpm; rpm -ivh lxcfs-3.1.2-0.2.el7.x86_64.rpm --force --nodeps æŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸ\nlxcfs -h 4.2. è¿è¡Œlxcfs è¿è¡Œlxcfsä¸»è¦æ‰§è¡Œä¸¤æ¡å‘½ä»¤ã€‚\nsudo mkdir -p /var/lib/lxcfs sudo lxcfs /var/lib/lxcfs å¯ä»¥é€šè¿‡systemdè¿è¡Œã€‚\nlxcfs.serviceæ–‡ä»¶ï¼š\ncat \u003e /usr/lib/systemd/system/lxcfs.service \u003c\u003cEOF [Unit] Description=lxcfs [Service] ExecStart=/usr/bin/lxcfs -f /var/lib/lxcfs Restart=on-failure #ExecReload=/bin/kill -s SIGHUP $MAINPID [Install] WantedBy=multi-user.target EOF è¿è¡Œå‘½ä»¤\nsystemctl daemon-reload \u0026\u0026 systemctl enable lxcfs \u0026\u0026 systemctl start lxcfs \u0026\u0026 systemctl status lxcfs 4.3. æŒ‚è½½å®¹å™¨å†…/procä¸‹çš„æ–‡ä»¶ç›®å½• docker run -it --rm -m 256m --cpus 2 \\ -v /var/lib/lxcfs/proc/cpuinfo:/proc/cpuinfo:rw \\ -v /var/lib/lxcfs/proc/diskstats:/proc/diskstats:rw \\ -v /var/lib/lxcfs/proc/meminfo:/proc/meminfo:rw \\ -v /var/lib/lxcfs/proc/stat:/proc/stat:rw \\ -v /var/lib/lxcfs/proc/swaps:/proc/swaps:rw \\ -v /var/lib/lxcfs/proc/uptime:/proc/uptime:rw \\ nginx:latest /bin/sh 4.4. éªŒè¯å®¹å™¨å†…CPUå’Œå†…å­˜ # cpu grep -c processor /proc/cpuinfo cat /proc/cpuinfo # memory free -g cat /proc/meminfo 5. ä½¿ç”¨k8sé›†ç¾¤éƒ¨ç½² ä½¿ç”¨k8sé›†ç¾¤éƒ¨ç½²ä¸systemdéƒ¨ç½²æ–¹å¼åŒç†ï¼Œéœ€è¦è§£å†³2ä¸ªé—®é¢˜ï¼š\nåœ¨æ¯ä¸ªnodeèŠ‚ç‚¹ä¸Šéƒ¨ç½²lxcfså¸¸é©»è¿›ç¨‹ï¼Œlxcfséœ€è¦é€šè¿‡é•œåƒæ¥è¿è¡Œï¼Œå¯ä»¥é€šè¿‡daemonsetæ¥éƒ¨ç½²ã€‚ å®ç°å°†lxcfsç»´æŠ¤çš„ç›®å½•è‡ªåŠ¨æŒ‚è½½åˆ°podå†…çš„/procç›®å½•ã€‚ å…·ä½“å¯å‚è€ƒï¼šhttps://github.com/denverdino/lxcfs-admission-webhook\n5.1. lxcfs-image Dockerfile\nFROM centos:7 as build RUN yum -y update RUN yum -y install fuse-devel pam-devel wget install gcc automake autoconf libtool make ENV LXCFS_VERSION 3.1.2 RUN wget https://linuxcontainers.org/downloads/lxcfs/lxcfs-$LXCFS_VERSION.tar.gz \u0026\u0026 \\ mkdir /lxcfs \u0026\u0026 tar xzvf lxcfs-$LXCFS_VERSION.tar.gz -C /lxcfs --strip-components=1 \u0026\u0026 \\ cd /lxcfs \u0026\u0026 ./configure \u0026\u0026 make FROM centos:7 STOPSIGNAL SIGINT COPY --from=build /lxcfs/lxcfs /usr/local/bin/lxcfs COPY --from=build /lxcfs/.libs/liblxcfs.so /usr/local/lib/lxcfs/liblxcfs.so COPY --from=build /lxcfs/lxcfs /lxcfs/lxcfs COPY --from=build /lxcfs/.libs/liblxcfs.so /lxcfs/liblxcfs.so COPY --from=build /usr/lib64/libfuse.so.2.9.2 /usr/lib64/libfuse.so.2.9.2 COPY --from=build /usr/lib64/libulockmgr.so.1.0.1 /usr/lib64/libulockmgr.so.1.0.1 RUN ln -s /usr/lib64/libfuse.so.2.9.2 /usr/lib64/libfuse.so.2 \u0026\u0026 \\ ln -s /usr/lib64/libulockmgr.so.1.0.1 /usr/lib64/libulockmgr.so.1 COPY start.sh / CMD [\"/start.sh\"] star.sh\n#!/bin/bash # Cleanup nsenter -m/proc/1/ns/mnt fusermount -u /var/lib/lxcfs 2\u003e /dev/null || true nsenter -m/proc/1/ns/mnt [ -L /etc/mtab ] || \\ sed -i \"/^lxcfs \\/var\\/lib\\/lxcfs fuse.lxcfs/d\" /etc/mtab # Prepare mkdir -p /usr/local/lib/lxcfs /var/lib/lxcfs # Update lxcfs cp -f /lxcfs/lxcfs /usr/local/bin/lxcfs cp -f /lxcfs/liblxcfs.so /usr/local/lib/lxcfs/liblxcfs.so # Mount exec nsenter -m/proc/1/ns/mnt /usr/local/bin/lxcfs /var/lib/lxcfs/ 5.2. daemonset lxcfs-daemonset.yaml\napiVersion: apps/v1 kind: DaemonSet metadata: name: lxcfs labels: app: lxcfs spec: selector: matchLabels: app: lxcfs template: metadata: labels: app: lxcfs spec: hostPID: true tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: lxcfs image: registry.cn-hangzhou.aliyuncs.com/denverdino/lxcfs:3.1.2 imagePullPolicy: Always securityContext: privileged: true volumeMounts: - name: cgroup mountPath: /sys/fs/cgroup - name: lxcfs mountPath: /var/lib/lxcfs mountPropagation: Bidirectional - name: usr-local mountPath: /usr/local volumes: - name: cgroup hostPath: path: /sys/fs/cgroup - name: usr-local hostPath: path: /usr/local - name: lxcfs hostPath: path: /var/lib/lxcfs type: DirectoryOrCreate 5.3. lxcfs-admission-webhook lxcfs-admission-webhookå®ç°äº†ä¸€ä¸ªåŠ¨æ€çš„å‡†å…¥webhookï¼Œæ›´å‡†ç¡®çš„è®²æ˜¯å®ç°äº†ä¸€ä¸ªä¿®æ”¹æ€§è´¨çš„webhookï¼Œå³ç›‘å¬podçš„åˆ›å»ºï¼Œç„¶åå¯¹podæ‰§è¡Œpatchçš„æ“ä½œï¼Œä»è€Œå°†lxcfsä¸å®¹å™¨å†…çš„ç›®å½•æ˜ å°„å…³ç³»æ¤å…¥åˆ°podåˆ›å»ºçš„yamlä¸­ä»è€Œå®ç°è‡ªåŠ¨æŒ‚è½½ã€‚\ndeployment\napiVersion: apps/v1 kind: Deployment metadata: name: lxcfs-admission-webhook-deployment labels: app: lxcfs-admission-webhook spec: replicas: 1 selector: matchLabels: app: lxcfs-admission-webhook template: metadata: labels: app: lxcfs-admission-webhook spec: containers: - name: lxcfs-admission-webhook image: registry.cn-hangzhou.aliyuncs.com/denverdino/lxcfs-admission-webhook:v1 imagePullPolicy: IfNotPresent args: - -tlsCertFile=/etc/webhook/certs/cert.pem - -tlsKeyFile=/etc/webhook/certs/key.pem - -alsologtostderr - -v=4 - 2\u003e\u00261 volumeMounts: - name: webhook-certs mountPath: /etc/webhook/certs readOnly: true volumes: - name: webhook-certs secret: secretName: lxcfs-admission-webhook-certs å…·ä½“éƒ¨ç½²å‚è€ƒ:install.sh\n#!/bin/bash ./deployment/webhook-create-signed-cert.sh kubectl get secret lxcfs-admission-webhook-certs kubectl create -f deployment/deployment.yaml kubectl create -f deployment/service.yaml cat ./deployment/mutatingwebhook.yaml | ./deployment/webhook-patch-ca-bundle.sh \u003e ./deployment/mutatingwebhook-ca-bundle.yaml kubectl create -f deployment/mutatingwebhook-ca-bundle.yaml æ‰§è¡Œå‘½ä»¤\n/deployment/install.sh å‚è€ƒï¼š\nhttps://github.com/lxc/lxcfs https://linuxcontainers.org/lxcfs/ https://github.com/denverdino/lxcfs-admission-webhook https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/ ","categories":"","description":"","excerpt":"1. èµ„æºè§†å›¾éš”ç¦» å®¹å™¨ä¸­çš„æ‰§è¡Œtopã€freeç­‰å‘½ä»¤å±•ç¤ºå‡ºæ¥çš„CPUï¼Œå†…å­˜ç­‰ä¿¡æ¯æ˜¯ä»/procç›®å½•ä¸­çš„ç›¸å…³æ–‡ä»¶é‡Œè¯»å–å‡ºæ¥çš„ã€‚è€Œå®¹å™¨å¹¶æ²¡æœ‰ â€¦","ref":"/kubernetes-notes/resource/lxcfs/lxcfs/","tags":["Kubernetes"],"title":"Lxcfsèµ„æºè§†å›¾éš”ç¦»"},{"body":"1. Linuxæ–‡ä»¶ç®¡ç† Linuxä¸­çš„æ‰€æœ‰æ•°æ®éƒ½è¢«ä¿å­˜åœ¨æ–‡ä»¶ä¸­ï¼Œæ‰€æœ‰çš„æ–‡ä»¶è¢«åˆ†é…åˆ°ä¸åŒçš„ç›®å½•ã€‚ç›®å½•æ˜¯ä¸€ç§ç±»ä¼¼äºæ ‘çš„ç»“æ„ï¼Œç§°ä¸ºæ–‡ä»¶ç³»ç»Ÿã€‚\n1.1. æ–‡ä»¶ç±»å‹ 1ã€æ™®é€šæ–‡ä»¶\næ™®é€šæ–‡ä»¶æ˜¯ä»¥å­—èŠ‚ä¸ºå•ä½çš„æ•°æ®æµï¼ŒåŒ…æ‹¬æ–‡æœ¬æ–‡ä»¶ã€æºç æ–‡ä»¶ã€å¯æ‰§è¡Œæ–‡ä»¶ç­‰ã€‚æ–‡æœ¬å’ŒäºŒè¿›åˆ¶å¯¹Linuxæ¥è¯´å¹¶æ— åŒºåˆ«ï¼Œå¯¹æ™®é€šæ–‡ä»¶çš„è§£é‡Šç”±å¤„ç†è¯¥æ–‡ä»¶çš„åº”ç”¨ç¨‹åºè¿›è¡Œã€‚\n2ã€ç›®å½•\nç›®å½•å¯ä»¥åŒ…å«æ™®é€šæ–‡ä»¶å’Œç‰¹æ®Šæ–‡ä»¶ï¼Œç›®å½•ç›¸å½“äºWindowså’ŒMac OSä¸­çš„æ–‡ä»¶å¤¹ã€‚\n3ã€è®¾å¤‡æ–‡ä»¶\nLinux ä¸å¤–éƒ¨è®¾å¤‡ï¼ˆä¾‹å¦‚å…‰é©±ï¼Œæ‰“å°æœºï¼Œç»ˆç«¯ï¼Œmodernç­‰ï¼‰æ˜¯é€šè¿‡ä¸€ç§è¢«ç§°ä¸ºè®¾å¤‡æ–‡ä»¶çš„æ–‡ä»¶æ¥è¿›è¡Œé€šä¿¡ã€‚Linux è¾“å…¥è¾“å‡ºåˆ°å¤–éƒ¨è®¾å¤‡çš„æ–¹å¼å’Œè¾“å…¥è¾“å‡ºåˆ°ä¸€ä¸ªæ–‡ä»¶çš„æ–¹å¼æ˜¯ç›¸åŒçš„ã€‚Linux å’Œä¸€ä¸ªå¤–éƒ¨è®¾å¤‡é€šè®¯ä¹‹å‰ï¼Œè¿™ä¸ªè®¾å¤‡å¿…é¡»é¦–å…ˆè¦æœ‰ä¸€ä¸ªè®¾å¤‡æ–‡ä»¶å­˜åœ¨ã€‚\nè®¾å¤‡æ–‡ä»¶å’Œæ™®é€šæ–‡ä»¶ä¸ä¸€æ ·ï¼Œè®¾å¤‡æ–‡ä»¶ä¸­å¹¶ä¸åŒ…å«ä»»ä½•æ•°æ®ã€‚\nè®¾å¤‡æ–‡ä»¶æœ‰ä¸¤ç§ç±»å‹ï¼šå­—ç¬¦è®¾å¤‡æ–‡ä»¶å’Œå—è®¾å¤‡æ–‡ä»¶ã€‚\nå­—ç¬¦è®¾å¤‡æ–‡ä»¶ä»¥å­—æ¯\"c\"å¼€å¤´ã€‚å­—ç¬¦è®¾å¤‡æ–‡ä»¶å‘è®¾å¤‡ä¼ é€æ•°æ®æ—¶ï¼Œä¸€æ¬¡ä¼ é€ä¸€ä¸ªå­—ç¬¦ã€‚å…¸å‹çš„é€šè¿‡å­—ç¬¦ä¼ é€æ•°æ®çš„è®¾å¤‡æœ‰ç»ˆç«¯ã€æ‰“å°æœºã€ç»˜å›¾ä»ªã€modernç­‰ã€‚å­—ç¬¦è®¾å¤‡æ–‡ä»¶æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º\"raw\"è®¾å¤‡æ–‡ä»¶ã€‚ å—è®¾å¤‡æ–‡ä»¶ä»¥å­—æ¯\"b\"å¼€å¤´ã€‚å—è®¾å¤‡æ–‡ä»¶å‘è®¾å¤‡ä¼ é€æ•°æ®æ—¶ï¼Œå…ˆä»å†…å­˜ä¸­çš„bufferä¸­è¯»æˆ–å†™æ•°æ®ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¼ é€æ•°æ®åˆ°ç‰©ç†ç£ç›˜ã€‚ç£ç›˜å’ŒCD-ROMSæ—¢å¯ä»¥ä½¿ç”¨å­—ç¬¦è®¾å¤‡æ–‡ä»¶ä¹Ÿå¯ä»¥ä½¿ç”¨å—è®¾å¤‡æ–‡ä»¶ã€‚ 1.2. æ–‡ä»¶å±æ€§ å¯ä»¥ä½¿ç”¨ls -alæ¥æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨ã€‚\n[root@www ~]# ls -al total 156 drwxr-x--- 4 root root 4096 Sep 8 14:06 . # å½“å‰ç›®å½• drwxr-xr-x 23 root root 4096 Sep 8 14:21 .. # çˆ¶ç›®å½• -rw------- 1 root root 1474 Sep 4 18:27 anaconda-ks.cfg -rw------- 1 root root 199 Sep 8 17:14 .bash_history -rw-r--r-- 1 root root 24 Jan 6 2007 .bash_logout -rw-r--r-- 1 root root 191 Jan 6 2007 .bash_profile -rw-r--r-- 1 root root 176 Jan 6 2007 .bashrc -rw-r--r-- 1 root root 100 Jan 6 2007 .cshrc drwx------ 3 root root 4096 Sep 5 10:37 .gconf drwx------ 2 root root 4096 Sep 5 14:09 .gconfd -rw-r--r-- 1 root root 42304 Sep 4 18:26 install.log -rw-r--r-- 1 root root 5661 Sep 4 18:25 install.log.syslog [ 1 ] [ 2 ][ 3 ][ 4 ] [ 5 ] [ 6 ] [ 7 ] [ æƒé™ ][æ–‡ä»¶æ•°][æ‰€æœ‰è€…] [ç”¨æˆ·ç»„][æ–‡ä»¶å®¹é‡][ ä¿®æ”¹æ—¥æœŸ ] [ æ–‡ä»¶å ] æ¯åˆ—å«ä¹‰è¯´æ˜ï¼š\nç¬¬ä¸€åˆ—ï¼šæ–‡ä»¶ç±»å‹ã€‚ ç¬¬äºŒåˆ—ï¼šè¡¨ç¤ºæ–‡ä»¶ä¸ªæ•°ã€‚å¦‚æœæ˜¯æ–‡ä»¶ï¼Œé‚£ä¹ˆå°±æ˜¯1ï¼›å¦‚æœæ˜¯ç›®å½•ï¼Œé‚£ä¹ˆå°±æ˜¯è¯¥ç›®å½•ä¸­æ–‡ä»¶çš„æ•°ç›®ã€‚ ç¬¬ä¸‰åˆ—ï¼šæ–‡ä»¶çš„æ‰€æœ‰è€…ï¼Œå³æ–‡ä»¶çš„åˆ›å»ºè€…ã€‚ ç¬¬å››åˆ—ï¼šæ–‡ä»¶æ‰€æœ‰è€…æ‰€åœ¨çš„ç”¨æˆ·ç»„ã€‚åœ¨Linuxä¸­ï¼Œæ¯ä¸ªç”¨æˆ·éƒ½éš¶å±äºä¸€ä¸ªç”¨æˆ·ç»„ã€‚ ç¬¬äº”åˆ—ï¼šæ–‡ä»¶å¤§å°ï¼ˆä»¥å­—èŠ‚è®¡ï¼‰ã€‚ ç¬¬å…­åˆ—ï¼šæ–‡ä»¶è¢«åˆ›å»ºæˆ–ä¸Šæ¬¡è¢«ä¿®æ”¹çš„æ—¶é—´ã€‚ ç¬¬ä¸ƒåˆ—ï¼šæ–‡ä»¶åæˆ–ç›®å½•åã€‚ æ–‡ä»¶ç±»å‹å­—ç¬¦\nå‰ç¼€ æè¿° - æ™®é€šæ–‡ä»¶ã€‚å¦‚æ–‡æœ¬æ–‡ä»¶ã€äºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶ã€æºä»£ç ç­‰ã€‚ b å—è®¾å¤‡æ–‡ä»¶ã€‚ç¡¬ç›˜å¯ä»¥ä½¿ç”¨å—è®¾å¤‡æ–‡ä»¶ã€‚ c å­—ç¬¦è®¾å¤‡æ–‡ä»¶ã€‚ç¡¬ç›˜ä¹Ÿå¯ä»¥ä½¿ç”¨å­—ç¬¦è®¾å¤‡æ–‡ä»¶ã€‚ d ç›®å½•æ–‡ä»¶ã€‚ç›®å½•å¯ä»¥åŒ…å«æ–‡ä»¶å’Œå…¶ä»–ç›®å½•ã€‚ l ç¬¦å·é“¾æ¥ï¼ˆè½¯é“¾æ¥ï¼‰ã€‚å¯ä»¥é“¾æ¥ä»»ä½•æ™®é€šæ–‡ä»¶ï¼Œç±»ä¼¼äº Windows ä¸­çš„å¿«æ·æ–¹å¼ã€‚ p å…·åç®¡é“ã€‚ç®¡é“æ˜¯è¿›ç¨‹é—´çš„ä¸€ç§é€šä¿¡æœºåˆ¶ã€‚ s ç”¨äºè¿›ç¨‹é—´é€šä¿¡çš„å¥—æ¥å­—ã€‚ éšè—æ–‡ä»¶\néšè—æ–‡ä»¶çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸ºè‹±æ–‡å¥å·æˆ–ç‚¹å·(.)ï¼ŒLinuxç¨‹åºï¼ˆåŒ…æ‹¬Shellï¼‰é€šå¸¸ä½¿ç”¨éšè—æ–‡ä»¶æ¥ä¿å­˜é…ç½®ä¿¡æ¯ã€‚å¯ä»¥é€šè¿‡ls -aæ¥æŸ¥çœ‹æ‰€æœ‰æ–‡ä»¶ï¼Œå³åŒ…å«éšè—æ–‡ä»¶ã€‚\nå¸¸è§çš„éšè—æ–‡ä»¶ï¼š .profileï¼šBourne shell (sh) åˆå§‹åŒ–è„šæœ¬ .kshrcï¼šKorn shell (ksh) åˆå§‹åŒ–è„šæœ¬ .cshrcï¼šC shell (csh) åˆå§‹åŒ–è„šæœ¬ .rhostsï¼šRemote shell (rsh) é…ç½®æ–‡ä»¶\n1.3. æ–‡ä»¶çš„æ“ä½œ æ“ä½œ å‘½ä»¤ åˆ›å»º touch filename ç¼–è¾‘ vi filename æŸ¥çœ‹ cat filename å¤åˆ¶ cp filename copyfile é‡å‘½å mv filename newfile åˆ é™¤ rm filename filename2 ç»Ÿè®¡è¯æ•° wc filename 1.4. æ ‡å‡†çš„Linuxæµ ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ¯ä¸ªLinuxç¨‹åºè¿è¡Œæ—¶éƒ½ä¼šåˆ›å»ºä¸‰ä¸ªæ–‡ä»¶æµï¼ˆä¸‰ä¸ªæ–‡ä»¶ï¼‰ï¼š\næ ‡å‡†è¾“å…¥æµ(stdin)ï¼šstdinçš„æ–‡ä»¶æè¿°ç¬¦ä¸º0ï¼ŒLinuxç¨‹åºé»˜è®¤ä»stdinè¯»å–æ•°æ®ã€‚ æ ‡å‡†è¾“å‡ºæµ(stdout)ï¼šstdout çš„æ–‡ä»¶æè¿°ç¬¦ä¸º1ï¼ŒLinuxç¨‹åºé»˜è®¤å‘stdoutè¾“å‡ºæ•°æ®ã€‚ æ ‡å‡†é”™è¯¯æµ(stderr)ï¼šstderrçš„æ–‡ä»¶æè¿°ç¬¦ä¸º2ï¼ŒLinuxç¨‹åºä¼šå‘stderræµä¸­å†™å…¥é”™è¯¯ä¿¡æ¯ã€‚ 2. æ–‡ä»¶æƒé™å’Œè®¿é—®æ¨¡å¼ 2.1. æŸ¥çœ‹æ–‡ä»¶æƒé™ Linuxæ¯ä¸ªæ–‡ä»¶éƒ½æœ‰ä¸‰ç±»æƒé™ï¼š\næ‰€æœ‰è€…æƒé™(user)ï¼šæ–‡ä»¶æ‰€æœ‰è€…èƒ½å¤Ÿè¿›è¡Œçš„æ“ä½œ ç»„æƒé™(group)ï¼šæ–‡ä»¶æ‰€å±ç”¨æˆ·ç»„èƒ½å¤Ÿè¿›è¡Œçš„æ“ä½œ å¤–éƒ¨æƒé™ï¼ˆotherï¼‰ï¼šå…¶ä»–ç”¨æˆ·å¯ä»¥è¿›è¡Œçš„æ“ä½œã€‚ é€šè¿‡ls -lçš„å‘½ä»¤å¯ä»¥æŸ¥çœ‹æ–‡ä»¶æƒé™ä¿¡æ¯ã€‚\n$ls -l /home/amrood -rwxr-xr-- 1 amrood users 1024 Nov 2 00:10 myfile drwxr-xr--- 1 amrood users 1024 Nov 2 00:10 mydir ç¬¬ä¸€åˆ—-rwxr-xr-- åŒ…å«äº†æ–‡ä»¶æˆ–ç›®å½•çš„æƒé™ã€‚\né™¤äº†ç¬¬ä¸€ä¸ªå­—ç¬¦-æˆ–dåˆ†åˆ«ç”¨æ¥è¡¨ç¤ºæ–‡ä»¶æˆ–ç›®å½•å¤–ï¼Œå…¶ä»–çš„ä¹ä¸ªå­—ç¬¦å¯ä»¥åˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«å¯¹åº”æ‰€æœ‰è€…æƒé™ï¼Œç”¨æˆ·ç»„æƒé™ï¼Œå…¶ä»–ç”¨æˆ·æƒé™ï¼Œå³-|user|group|otherã€‚\næ¯ç»„çš„æƒé™åˆå¯åˆ†ä¸ºä¸‰ç±»ï¼š\nè¯»å–ï¼ˆrï¼‰ï¼Œå¯¹åº”æƒé™æ•°å­—4\nå†™å…¥ï¼ˆwï¼‰ï¼Œå¯¹åº”æƒé™æ•°å­—2\næ‰§è¡Œï¼ˆxï¼‰ï¼Œå¯¹åº”æƒé™æ•°å­—1\nä½¿ç”¨æ•°å­—è¡¨ç¤ºæƒé™ï¼š\næ•°å­— è¯´æ˜ æƒé™ 0 æ²¡æœ‰ä»»ä½•æƒé™ --- 1 æ‰§è¡Œæƒé™ --x 2 å†™å…¥æƒé™ -w- 3 æ‰§è¡Œæƒé™å’Œå†™å…¥æƒé™ï¼š1 (æ‰§è¡Œ) + 2 (å†™å…¥) = 3 -wx 4 è¯»å–æƒé™ r-- 5 è¯»å–å’Œæ‰§è¡Œæƒé™ï¼š4 (è¯»å–) + 1 (æ‰§è¡Œ) = 5 r-x 6 è¯»å–å’Œå†™å…¥æƒé™ï¼š4 (è¯»å–) + 2 (å†™å…¥) = 6 rw- 7 æ‰€æœ‰æƒé™: 4 (è¯»å–) + 2 (å†™å…¥) + 1 (æ‰§è¡Œ) = 7 rwx 2.2. è®¿é—®æ¨¡å¼ 2.2.1. æ–‡ä»¶è®¿é—®æ¨¡å¼ åŸºæœ¬çš„æƒé™æœ‰è¯»å–(r)ã€å†™å…¥(w)å’Œæ‰§è¡Œ(x)ï¼š\nè¯»å–ï¼šç”¨æˆ·èƒ½å¤Ÿè¯»å–æ–‡ä»¶ä¿¡æ¯ï¼ŒæŸ¥çœ‹æ–‡ä»¶å†…å®¹ã€‚ å†™å…¥ï¼šç”¨æˆ·å¯ä»¥ç¼–è¾‘æ–‡ä»¶ï¼Œå¯ä»¥å‘æ–‡ä»¶å†™å…¥å†…å®¹ï¼Œä¹Ÿå¯ä»¥åˆ é™¤æ–‡ä»¶å†…å®¹ã€‚ æ‰§è¡Œï¼šç”¨æˆ·å¯ä»¥å°†æ–‡ä»¶ä½œä¸ºç¨‹åºæ¥è¿è¡Œã€‚ 2.2.2. ç›®å½•è®¿é—®æ¨¡å¼ ç›®å½•çš„è®¿é—®æ¨¡å¼å’Œæ–‡ä»¶ç±»ä¼¼ï¼Œä½†æ˜¯ç¨æœ‰ä¸åŒï¼š\nè¯»å–ï¼šç”¨æˆ·å¯ä»¥æŸ¥çœ‹ç›®å½•ä¸­çš„æ–‡ä»¶ å†™å…¥ï¼šç”¨æˆ·å¯ä»¥åœ¨å½“å‰ç›®å½•ä¸­åˆ é™¤æ–‡ä»¶æˆ–åˆ›å»ºæ–‡ä»¶ æ‰§è¡Œï¼šæ‰§è¡Œæƒé™èµ‹äºˆç”¨æˆ·éå†ç›®å½•çš„æƒåˆ©ï¼Œä¾‹å¦‚æ‰§è¡Œ cd å’Œ ls å‘½ä»¤ã€‚ 2.3. æƒé™çš„æ“ä½œ 2.3.1. chmod chmod (change mode) å‘½ä»¤æ¥æ”¹å˜æ–‡ä»¶æˆ–ç›®å½•çš„è®¿é—®æƒé™ï¼Œæƒé™å¯ä»¥ä½¿ç”¨ç¬¦å·æˆ–æ•°å­—æ¥è¡¨ç¤ºã€‚\n1ã€é€šè¿‡ç¬¦å·æ–¹å¼\nå¯ä»¥ä½¿ç”¨ç¬¦å·æ¥æ”¹å˜æ–‡ä»¶æˆ–ç›®å½•çš„æƒé™ï¼Œä½ å¯ä»¥å¢åŠ (+)å’Œåˆ é™¤(-)æƒé™ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šç‰¹å®šæƒé™(=)ã€‚\næŒ‡å®šæƒé™èŒƒå›´\nu (user)ï¼šæ‰€æœ‰è€…æƒé™ g(group)ï¼šæ‰€å±ç”¨æˆ·ç»„æƒé™ o(other)ï¼šå…¶ä»–ç”¨æˆ·æƒé™ ç¬¦å· è¯´æ˜ + ä¸ºæ–‡ä»¶æˆ–ç›®å½•å¢åŠ æƒé™ - åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•çš„æƒé™ = è®¾ç½®æŒ‡å®šçš„æƒé™ ç¤ºä¾‹\n# æŸ¥çœ‹æƒé™ $ls -l testfile -rwxrwxr-- 1 amrood users 1024 Nov 2 00:10 testfile # å¢åŠ æƒé™ $chmod o+wx testfile $ls -l testfile -rwxrwxrwx 1 amrood users 1024 Nov 2 00:10 testfile # åˆ é™¤æƒé™ $chmod u-x testfile $ls -l testfile -rw-rwxrwx 1 amrood users 1024 Nov 2 00:10 testfile # æŒ‡å®šæƒé™ $chmod g=rx testfile $ls -l testfile -rw-r-xrwx 1 amrood users 1024 Nov 2 00:10 testfile # åŒæ—¶ä½¿ç”¨å¤šä¸ªç¬¦å· $chmod o+wx,u-x,g=rx testfile $ls -l testfile -rw-r-xrwx 1 amrood users 1024 Nov 2 00:10 testfile 2ã€é€šè¿‡æ•°å­—æƒé™æ–¹å¼\næ•°å­—æƒé™ä¾ç…§2.1çš„æƒé™è¯´æ˜ã€‚\nç¤ºä¾‹\n$ls -l testfile -rwxrwxr-- 1 amrood users 1024 Nov 2 00:10 testfile $ chmod 755 testfile $ls -l testfile -rwxr-xr-x 1 amrood users 1024 Nov 2 00:10 testfile 2.3.2. chown chown å‘½ä»¤æ˜¯\"change owner\"çš„ç¼©å†™ï¼Œç”¨æ¥æ”¹å˜æ–‡ä»¶çš„æ‰€æœ‰è€…ã€‚\n# userå¯ä»¥æ˜¯ç”¨æˆ·åæˆ–ç”¨æˆ·ID $ chown user filelist # ä¾‹å¦‚ï¼š $ chown amrood testfile è¶…çº§ç”¨æˆ· root å¯ä»¥ä¸å—é™åˆ¶çš„æ›´æ”¹æ–‡ä»¶çš„æ‰€æœ‰è€…å’Œç”¨æˆ·ç»„ï¼Œä½†æ˜¯æ™®é€šç”¨æˆ·åªèƒ½æ›´æ”¹æ‰€æœ‰è€…æ˜¯è‡ªå·±çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚\n2.3.3. chgrp chgrp å‘½ä»¤æ˜¯\"change group\"çš„ç¼©å†™ï¼Œç”¨æ¥æ”¹å˜æ–‡ä»¶æ‰€åœ¨çš„ç¾¤ç»„ã€‚\n# groupå¯ä»¥æ˜¯ç”¨æˆ·ç»„åæˆ–ç”¨æˆ·ç»„ID $ chgrp group filelist # ä¾‹å¦‚ï¼š $ chgrp special testfile 2.4. SUIDå’ŒSGIDä½ åœ¨Linuxä¸­ï¼Œä¸€äº›ç¨‹åºéœ€è¦ç‰¹æ®Šæƒé™æ‰èƒ½å®Œæˆç”¨æˆ·æŒ‡å®šçš„æ“ä½œã€‚ä¾‹å¦‚å¯†ç æ–‡ä»¶/etc/shadowã€‚\nLinux é€šè¿‡ç»™ç¨‹åºè®¾ç½®SUID(Set User ID)å’ŒSGID(Set Group ID)ä½æ¥èµ‹äºˆæ™®é€šç”¨æˆ·ç‰¹æ®Šæƒé™ã€‚å½“æˆ‘ä»¬è¿è¡Œä¸€ä¸ªå¸¦æœ‰SUIDä½çš„ç¨‹åºæ—¶ï¼Œå°±ä¼šç»§æ‰¿è¯¥ç¨‹åºæ‰€æœ‰è€…çš„æƒé™ï¼›å¦‚æœç¨‹åºä¸å¸¦SUIDä½ï¼Œåˆ™ä¼šæ ¹æ®ç¨‹åºä½¿ç”¨è€…çš„æƒé™æ¥è¿è¡Œã€‚\nä¾‹å¦‚ï¼š\n$ ls -l /usr/bin/passwd -r-sr-xr-x 1 root bin 19031 Feb 7 13:47 /usr/bin/passwd* ä¸Šé¢ç¬¬ä¸€åˆ—ç¬¬å››ä¸ªå­—ç¬¦ä¸æ˜¯'x'æˆ–'-'ï¼Œè€Œæ˜¯'s'ï¼Œè¯´æ˜ /usr/bin/passwd æ–‡ä»¶è®¾ç½®äº†SUIDä½ï¼Œè¿™æ—¶æ™®é€šç”¨æˆ·ä¼šä»¥rootç”¨æˆ·çš„æƒé™æ¥æ‰§è¡Œpasswdç¨‹åºã€‚\nå°å†™å­—æ¯'s'è¯´æ˜æ–‡ä»¶æ‰€æœ‰è€…æœ‰æ‰§è¡Œæƒé™(x)ï¼Œå¤§å†™å­—æ¯'S'è¯´æ˜ç¨‹åºæ‰€æœ‰è€…æ²¡æœ‰æ‰§è¡Œæƒé™(x)ã€‚\nä¸ºä¸€ä¸ªç›®å½•è®¾ç½®SUIDå’ŒSGIDä½å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ï¼š\n$ chmod ug+s dirname $ ls -l drwsr-sr-x 2 root root 4096 Jun 19 06:45 dirname ","categories":"","description":"","excerpt":"1. Linuxæ–‡ä»¶ç®¡ç† Linuxä¸­çš„æ‰€æœ‰æ•°æ®éƒ½è¢«ä¿å­˜åœ¨æ–‡ä»¶ä¸­ï¼Œæ‰€æœ‰çš„æ–‡ä»¶è¢«åˆ†é…åˆ°ä¸åŒçš„ç›®å½•ã€‚ç›®å½•æ˜¯ä¸€ç§ç±»ä¼¼äºæ ‘çš„ç»“æ„ï¼Œç§°ä¸ºæ–‡ä»¶ç³»ç»Ÿã€‚ â€¦","ref":"/linux-notes/file/linux-file-permission/","tags":["Linux"],"title":"Linuxæ–‡ä»¶æƒé™"},{"body":"1. Git commitè§„èŒƒ 1.1. æ ¼å¼ \u003ctype\u003e(\u003cscope\u003e): \u003csubject\u003e ç¤ºä¾‹ï¼š\nfix(ngRepeat): fix trackBy function being invoked with incorrect scope 1.2. type ä¸»è¦çš„æäº¤ç±»å‹å¦‚ä¸‹ï¼š\nType è¯´æ˜ å¤‡æ³¨ feat æäº¤æ–°åŠŸèƒ½ å¸¸ç”¨ fix ä¿®å¤bug å¸¸ç”¨ docs ä¿®æ”¹æ–‡æ¡£ style ä¿®æ”¹æ ¼å¼ï¼Œä¾‹å¦‚æ ¼å¼åŒ–ä»£ç ï¼Œç©ºæ ¼ï¼Œæ‹¼å†™é”™è¯¯ç­‰ refactor é‡æ„ä»£ç ï¼Œæ²¡æœ‰æ·»åŠ æ–°åŠŸèƒ½ä¹Ÿæ²¡æœ‰ä¿®å¤bug test æ·»åŠ æˆ–ä¿®æ”¹æµ‹è¯•ç”¨ä¾‹ perf ä»£ç æ€§èƒ½è°ƒä¼˜ chore ä¿®æ”¹æ„å»ºå·¥å…·ã€æ„å»ºæµç¨‹ã€æ›´æ–°ä¾èµ–åº“ã€æ–‡æ¡£ç”Ÿæˆé€»è¾‘ ä¾‹å¦‚vendoråŒ… 1.3. scope è¡¨ç¤ºæ­¤æ¬¡commitæ¶‰åŠçš„æ–‡ä»¶èŒƒå›´ï¼Œå¯ä»¥ä½¿ç”¨*æ¥è¡¨ç¤ºæ¶‰åŠå¤šä¸ªèŒƒå›´ã€‚\n1.4. subject æè¿°æ­¤æ¬¡commitæ¶‰åŠçš„ä¿®æ”¹å†…å®¹ã€‚\nä½¿ç”¨ç¥ˆä½¿å¥ï¼ˆåŠ¨è¯å¼€å¤´ï¼‰ã€åŠ¨å®¾çŸ­è¯­ã€‚ ç¬¬ä¸€ä¸ªå­—æ¯ä¸è¦å¤§å†™ã€‚ ä¸è¦ä»¥.å¥å·ç»“å°¾ã€‚ 2. Git commitå·¥å…· å®‰è£…commitizenå’Œcz-conventional-changelogã€‚\nnpm install -g commitizen cz-conventional-changelog echo '{ \"path\": \"cz-conventional-changelog\" }' \u003e ~/.czrc ä½¿ç”¨cz-cli\n$ git cz cz-cli@4.0.3, cz-conventional-changelog@3.0.1 ? Select the type of change that you're committing: (Use arrow keys) â¯ feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests (Move up and down to reveal more choices) å‚è€ƒï¼š\nhttps://github.com/angular/angular.js/blob/master/DEVELOPERS.md#-git-commit-guidelines https://juejin.im/post/5afc5242f265da0b7f44bee4 commitizen/cz-cli commitizen/cz-conventional-changelog ","categories":"","description":"","excerpt":"1. Git commitè§„èŒƒ 1.1. æ ¼å¼ \u003ctype\u003e(\u003cscope\u003e): \u003csubject\u003e ç¤ºä¾‹ï¼š\nfix(ngRepeat): â€¦","ref":"/linux-notes/git/git-commit-msg/","tags":["Git"],"title":"Git commitè§„èŒƒ"},{"body":"è¯¦ç»†é…ç½®è¯´æ˜ keepalivedåªæœ‰ä¸€ä¸ªé…ç½®æ–‡ä»¶/etc/keepalived/keepalived.confã€‚\né‡Œé¢ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé…ç½®åŒºåŸŸï¼Œåˆ†åˆ«æ˜¯:\nglobal_defs static_ipaddress static_routes vrrp_script vrrp_instance virtual_server 1. global_defsåŒºåŸŸ ä¸»è¦æ˜¯é…ç½®æ•…éšœå‘ç”Ÿæ—¶çš„é€šçŸ¥å¯¹è±¡ä»¥åŠæœºå™¨æ ‡è¯†ã€‚\nglobal_defs { notification_email { # notification_email æ•…éšœå‘ç”Ÿæ—¶ç»™è°å‘é‚®ä»¶é€šçŸ¥ a@abc.com b@abc.com ... } notification_email_from alert@abc.com # notification_email_from é€šçŸ¥é‚®ä»¶ä»å“ªä¸ªåœ°å€å‘å‡º smtp_server smtp.abc.com # smpt_server é€šçŸ¥é‚®ä»¶çš„smtpåœ°å€ smtp_connect_timeout 30 # smtp_connect_timeout è¿æ¥smtpæœåŠ¡å™¨çš„è¶…æ—¶æ—¶é—´ enable_traps # enable_traps å¼€å¯SNMPé™·é˜±ï¼ˆSimple Network Management Protocolï¼‰ router_id host163 # router_id æ ‡è¯†æœ¬èŠ‚ç‚¹çš„å­—æ¡ä¸²ï¼Œé€šå¸¸ä¸ºhostnameï¼Œä½†ä¸ä¸€å®šéå¾—æ˜¯hostnameã€‚æ•…éšœå‘ç”Ÿæ—¶ï¼Œé‚®ä»¶é€šçŸ¥ä¼šç”¨åˆ°ã€‚ } 2. static_ipaddresså’Œstatic_routesåŒºåŸŸ[å¯å¿½ç•¥] static_ipaddresså’Œstatic_routesåŒºåŸŸé…ç½®çš„æ˜¯æ˜¯æœ¬èŠ‚ç‚¹çš„IPå’Œè·¯ç”±ä¿¡æ¯ã€‚å¦‚æœä½ çš„æœºå™¨ä¸Šå·²ç»é…ç½®äº†IPå’Œè·¯ç”±ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªåŒºåŸŸå¯ä»¥ä¸ç”¨é…ç½®ã€‚å…¶å®ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä½ çš„æœºå™¨éƒ½ä¼šæœ‰IPåœ°å€å’Œè·¯ç”±ä¿¡æ¯çš„ï¼Œå› æ­¤æ²¡å¿…è¦å†åœ¨è¿™ä¸¤ä¸ªåŒºåŸŸé…ç½®ã€‚\nstatic_ipaddress { 10.210.214.163/24 brd 10.210.214.255 dev eth0 ... } static_routes { 10.0.0.0/8 via 10.210.214.1 dev eth0 ... } 3. vrrp_scriptåŒºåŸŸ ç”¨æ¥åšå¥åº·æ£€æŸ¥çš„ï¼Œå½“æ—¶æ£€æŸ¥å¤±è´¥æ—¶ä¼šå°†vrrp_instanceçš„priorityå‡å°‘ç›¸åº”çš„å€¼ã€‚\nvrrp_script chk_http_port {Â scriptÂ \"\u003c/dev/tcp/127.0.0.1/80\"Â #ä¸€å¥æŒ‡ä»¤æˆ–è€…ä¸€ä¸ªè„šæœ¬æ–‡ä»¶ï¼Œéœ€è¿”å›0(æˆåŠŸ)æˆ–é0(å¤±è´¥)ï¼Œkeepalivedä»¥æ­¤ä¸ºä¾æ®åˆ¤æ–­å…¶ç›‘æ§çš„æœåŠ¡çŠ¶æ€ã€‚ interval 1Â #å¥åº·æ£€æŸ¥å‘¨æœŸ weight -10 # ä¼˜å…ˆçº§å˜åŒ–å¹…åº¦ï¼Œå¦‚æœscriptä¸­çš„æŒ‡ä»¤æ‰§è¡Œå¤±è´¥ï¼Œé‚£ä¹ˆç›¸åº”çš„vrrp_instanceçš„ä¼˜å…ˆçº§ä¼šå‡å°‘10ä¸ªç‚¹ã€‚ } 4. vrrp_instanceå’Œvrrp_sync_groupåŒºåŸŸ vrrp_instanceç”¨æ¥å®šä¹‰å¯¹å¤–æä¾›æœåŠ¡çš„VIPåŒºåŸŸåŠå…¶ç›¸å…³å±æ€§ã€‚\nvrrp_rsync_groupç”¨æ¥å®šä¹‰vrrp_intanceç»„ï¼Œä½¿å¾—è¿™ä¸ªç»„å†…æˆå‘˜åŠ¨ä½œä¸€è‡´ã€‚\nvrrp_sync_group VG_1 {Â #ç›‘æ§å¤šä¸ªç½‘æ®µçš„å®ä¾‹ group { inside_network # name of vrrp_instance (below) outside_networkÂ # One for each moveable IP. ... } notify_masterÂ /path/to_master.sh # notify_masterè¡¨ç¤ºåˆ‡æ¢ä¸ºä¸»æœºæ‰§è¡Œçš„è„šæœ¬ notify_backupÂ /path/to_backup.sh # notify_backupè¡¨ç¤ºåˆ‡æ¢ä¸ºå¤‡æœºå¸ˆçš„è„šæœ¬ notify_faultÂ \"/path/fault.sh VG_1\"Â # notify_faultè¡¨ç¤ºå‡ºé”™æ—¶æ‰§è¡Œçš„è„šæœ¬ notifyÂ /path/notify.shÂ # notifyè¡¨ç¤ºä»»ä½•ä¸€çŠ¶æ€åˆ‡æ¢æ—¶éƒ½ä¼šè°ƒç”¨è¯¥è„šæœ¬ï¼Œä¸”åœ¨ä»¥ä¸Šä¸‰ä¸ªè„šæœ¬æ‰§è¡Œå®Œæˆä¹‹åè¿›è¡Œè°ƒç”¨ smtp_alertÂ # smtp_alert è¡¨ç¤ºæ˜¯å¦å¼€å¯é‚®ä»¶é€šçŸ¥ï¼ˆç”¨å…¨å±€åŒºåŸŸçš„é‚®ä»¶è®¾ç½®æ¥å‘é€šçŸ¥ï¼‰ } vrrp_instance VI_1 { state MASTERÂ # state MASTERæˆ–BACKUPï¼Œå½“å…¶ä»–èŠ‚ç‚¹keepalivedå¯åŠ¨æ—¶ä¼šå°†priorityæ¯”è¾ƒå¤§çš„èŠ‚ç‚¹é€‰ä¸¾ä¸ºMASTERï¼Œå› æ­¤è¯¥é¡¹å…¶å®æ²¡æœ‰å®è´¨ç”¨é€”ã€‚ interface eth0Â # interface èŠ‚ç‚¹å›ºæœ‰IPï¼ˆéVIPï¼‰çš„ç½‘å¡ï¼Œç”¨æ¥å‘VRRPåŒ… use_vmac dont_track_primaryÂ # use_vmac æ˜¯å¦ä½¿ç”¨VRRPçš„è™šæ‹ŸMACåœ°å€ï¼Œdont_track_primary å¿½ç•¥VRRPç½‘å¡é”™è¯¯ï¼ˆé»˜è®¤æœªè®¾ç½®ï¼‰ track_interface {# track_interface ç›‘æ§ä»¥ä¸‹ç½‘å¡ï¼Œå¦‚æœä»»ä½•ä¸€ä¸ªä¸é€šå°±ä¼šåˆ‡æ¢åˆ°FALTçŠ¶æ€ã€‚ï¼ˆå¯é€‰é¡¹ï¼‰ eth0 eth1 } #mcast_src_ip ä¿®æ”¹vrrpç»„æ’­åŒ…çš„æºåœ°å€ï¼Œé»˜è®¤æºåœ°å€ä¸ºmasterçš„IP mcast_src_ip lvs_sync_daemon_interface eth1Â #lvs_sync_daemon_interface ç»‘å®šlvs syncdçš„ç½‘å¡ garp_master_delay 10Â # garp_master_delay å½“åˆ‡ä¸ºä¸»çŠ¶æ€åå¤šä¹…æ›´æ–°ARPç¼“å­˜ï¼Œé»˜è®¤5ç§’ virtual_router_id 1 # virtual_router_id å–å€¼åœ¨0-255ä¹‹é—´ï¼Œç”¨æ¥åŒºåˆ†å¤šä¸ªinstanceçš„VRRPç»„æ’­ï¼Œ åŒä¸€ç½‘æ®µä¸­virtual_router_idçš„å€¼ä¸èƒ½é‡å¤ï¼Œå¦åˆ™ä¼šå‡ºé”™ priority 100Â #priorityç”¨æ¥é€‰ä¸¾masterçš„ï¼Œæ ¹æ®æœåŠ¡æ˜¯å¦å¯ç”¨ï¼Œä»¥weightçš„å¹…åº¦æ¥è°ƒæ•´èŠ‚ç‚¹çš„priorityï¼Œä»è€Œé€‰å–priorityé«˜çš„ä¸ºmasterï¼Œè¯¥é¡¹å–å€¼èŒƒå›´æ˜¯1-255ï¼ˆåœ¨æ­¤èŒƒå›´ä¹‹å¤–ä¼šè¢«è¯†åˆ«æˆé»˜è®¤å€¼100ï¼‰ advert_int 1Â # advert_int å‘VRRPåŒ…çš„æ—¶é—´é—´éš”ï¼Œå³å¤šä¹…è¿›è¡Œä¸€æ¬¡masteré€‰ä¸¾ï¼ˆå¯ä»¥è®¤ä¸ºæ˜¯å¥åº·æŸ¥æ£€æ—¶é—´é—´éš”ï¼‰ authentication {Â # authentication è®¤è¯åŒºåŸŸï¼Œè®¤è¯ç±»å‹æœ‰PASSå’ŒHAï¼ˆIPSECï¼‰ï¼Œæ¨èä½¿ç”¨PASSï¼ˆå¯†ç åªè¯†åˆ«å‰8ä½ï¼‰ auth_type PASSÂ #è®¤è¯æ–¹å¼ auth_pass 12345678Â #è®¤è¯å¯†ç  } virtual_ipaddress {Â # è®¾ç½®vip 10.210.214.253/24Â brd 10.210.214.255 dev eth0 192.168.1.11/24Â brd 192.168.1.255 dev eth1 } virtual_routes {Â # virtual_routes è™šæ‹Ÿè·¯ç”±ï¼Œå½“IPæ¼‚è¿‡æ¥ä¹‹åéœ€è¦æ·»åŠ çš„è·¯ç”±ä¿¡æ¯ 172.16.0.0/12Â via 10.210.214.1 192.168.1.0/24Â via 192.168.1.1 dev eth1 default via 202.102.152.1 } track_script { chk_http_port } nopreemptÂ # nopreempt å…è®¸ä¸€ä¸ªpriorityæ¯”è¾ƒä½çš„èŠ‚ç‚¹ä½œä¸ºmasterï¼Œå³ä½¿æœ‰priorityæ›´é«˜çš„èŠ‚ç‚¹å¯åŠ¨ preempt_delay 300Â # preempt_delay masterå¯åŠ¨å¤šä¹…ä¹‹åè¿›è¡Œæ¥ç®¡èµ„æºï¼ˆVIP/Routeä¿¡æ¯ç­‰ï¼‰ï¼Œå¹¶ææ˜¯æ²¡æœ‰nopreempté€‰é¡¹ debug notify_master| notify_backup| notify_fault| notify| smtp_alert } 5. virtual_server_groupå’Œvirtual_serveråŒºåŸŸ virtual_server_groupä¸€èˆ¬åœ¨è¶…å¤§å‹çš„LVSä¸­ç”¨åˆ°ï¼Œä¸€èˆ¬LVSç”¨ä¸åˆ°è¿™ä¸œè¥¿ã€‚\nvirtual_server IP Port { delay_loop # delay_loop å»¶è¿Ÿè½®è¯¢æ—¶é—´ï¼ˆå•ä½ç§’ï¼‰ lb_algo rr|wrr|lc|wlc|lblc|sh|dhÂ # lb_algo åç«¯è°ƒè¯•ç®—æ³•ï¼ˆload balancing algorithmï¼‰ lb_kind NAT|DR|TUNÂ # lb_kind LVSè°ƒåº¦ç±»å‹NAT/DR/TUN persistence_timeout #ä¼šè¯ä¿æŒæ—¶é—´ persistence_granularityÂ #lvsä¼šè¯ä¿æŒç²’åº¦Â protocol TCPÂ #ä½¿ç”¨çš„åè®® ha_suspend virtualhost # virtualhost ç”¨æ¥ç»™HTTP_GETå’ŒSSL_GETé…ç½®è¯·æ±‚headerçš„ alphaÂ omega quorum hysteresis quorum_up| quorum_down| sorry_serverÂ #å¤‡ç”¨æœºï¼Œæ‰€æœ‰realserverå¤±æ•ˆåå¯ç”¨Â real_server{ # real_server çœŸæ­£æä¾›æœåŠ¡çš„æœåŠ¡å™¨ weight 1Â # é»˜è®¤ä¸º1,0ä¸ºå¤±æ•ˆ inhibit_on_failureÂ #åœ¨æœåŠ¡å™¨å¥åº·æ£€æŸ¥å¤±æ•ˆæ—¶ï¼Œå°†å…¶è®¾ä¸º0ï¼Œè€Œä¸æ˜¯ç›´æ¥ä»ipvsä¸­åˆ é™¤ notify_up| # real serverå®•æ‰æ—¶æ‰§è¡Œçš„è„šæœ¬ notify_down| # real serverå¯åŠ¨æ—¶æ‰§è¡Œçš„è„šæœ¬ # HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|MISC_CHECK TCP_CHECK { connect_timeout 3Â #è¿æ¥è¶…æ—¶æ—¶é—´ nb_get_retry 3Â #é‡è¿æ¬¡æ•° delay_before_retry 3Â #é‡è¿é—´éš”æ—¶é—´ connect_port 23Â #å¥åº·æ£€æŸ¥çš„ç«¯å£çš„ç«¯å£ bindto } HTTP_GET|SSL_GET { url {# æ£€æŸ¥urlï¼Œå¯ä»¥æŒ‡å®šå¤šä¸ª path # path è¯·æ±‚real serserverä¸Šçš„è·¯å¾„ digest # ç”¨genhashç®—å‡ºçš„æ‘˜è¦ä¿¡æ¯ status_code # æ£€æŸ¥çš„httpçŠ¶æ€ç  } connect_port # connect_port å¥åº·æ£€æŸ¥ï¼Œå¦‚æœç«¯å£é€šåˆ™è®¤ä¸ºæœåŠ¡å™¨æ­£å¸¸ connect_timeout # è¶…æ—¶æ—¶é•¿ nb_get_retry # é‡è¯•æ¬¡æ•° delay_before_retry #Â ä¸‹æ¬¡é‡è¯•çš„æ—¶é—´å»¶è¿Ÿ } SMTP_CHECK { host { connect_ip connect_portÂ #é»˜è®¤æ£€æŸ¥25ç«¯å£ bindto } connect_timeout 5 retry 3 delay_before_retry 2 helo_name |Â #smtp heloè¯·æ±‚å‘½ä»¤å‚æ•°ï¼Œå¯é€‰ } MISC_CHECK { misc_path |Â #å¤–éƒ¨è„šæœ¬è·¯å¾„ misc_timeoutÂ #è„šæœ¬æ‰§è¡Œè¶…æ—¶æ—¶é—´ misc_dynamicÂ #å¦‚è®¾ç½®è¯¥é¡¹ï¼Œåˆ™é€€å‡ºçŠ¶æ€ç ä¼šç”¨æ¥åŠ¨æ€è°ƒæ•´æœåŠ¡å™¨çš„æƒé‡ï¼Œè¿”å›0 æ­£å¸¸ï¼Œä¸ä¿®æ”¹ï¼›è¿”å›1ï¼Œ #æ£€æŸ¥å¤±è´¥ï¼Œæƒé‡æ”¹ä¸º0ï¼›è¿”å›2-255ï¼Œæ­£å¸¸ï¼Œæƒé‡è®¾ç½®ä¸ºï¼šè¿”å›çŠ¶æ€ç -2 } } } ","categories":"","description":"","excerpt":"è¯¦ç»†é…ç½®è¯´æ˜ keepalivedåªæœ‰ä¸€ä¸ªé…ç½®æ–‡ä»¶/etc/keepalived/keepalived.confã€‚\né‡Œé¢ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé…ç½® â€¦","ref":"/linux-notes/keepalived/keepalived-conf/","tags":["Keepalived"],"title":"Keepalivedé…ç½®è¯¦è§£"},{"body":"1. å®‰è£…tmux # linux yum install -y tmux # mac brew install tmux 2. tmuxå¸¸ç”¨å‘½ä»¤ 2.1. è¿›å…¥tmux tmux 2.2. é€€å‡ºtmuxï¼Œç¨‹åºåå°è¿è¡Œ æŒ‰ctrl + b è¿›å…¥æ§åˆ¶å°ï¼Œå†æŒ‰ d 2.3. é‡å›ä¸Šæ¬¡tmuxçª—å£ ctrl + a 2.4. ç»“æŸtmuxçª—å£è¿è¡Œçš„è¿›ç¨‹ ctrl + d 3. æ›´å¤šå¿«æ·é”®è¯´æ˜ ç±»åˆ« å¿«æ·é”® è¯´æ˜ è¿›å…¥æ§åˆ¶å° Ctrl+b æ¿€æ´»æ§åˆ¶å°ï¼›æ­¤æ—¶ä»¥ä¸‹æŒ‰é”®ç”Ÿæ•ˆ ç³»ç»Ÿæ“ä½œ ? åˆ—å‡ºæ‰€æœ‰å¿«æ·é”®ï¼›æŒ‰qè¿”å› d è„±ç¦»å½“å‰ä¼šè¯ï¼›è¿™æ ·å¯ä»¥æš‚æ—¶è¿”å›Shellç•Œé¢ï¼Œè¾“å…¥tmux attachèƒ½å¤Ÿé‡æ–°è¿›å…¥ä¹‹å‰çš„ä¼šè¯ D é€‰æ‹©è¦è„±ç¦»çš„ä¼šè¯ï¼›åœ¨åŒæ—¶å¼€å¯äº†å¤šä¸ªä¼šè¯æ—¶ä½¿ç”¨ Ctrl+z æŒ‚èµ·å½“å‰ä¼šè¯ r å¼ºåˆ¶é‡ç»˜æœªè„±ç¦»çš„ä¼šè¯ s é€‰æ‹©å¹¶åˆ‡æ¢ä¼šè¯ï¼›åœ¨åŒæ—¶å¼€å¯äº†å¤šä¸ªä¼šè¯æ—¶ä½¿ç”¨ : è¿›å…¥å‘½ä»¤è¡Œæ¨¡å¼ï¼›æ­¤æ—¶å¯ä»¥è¾“å…¥æ”¯æŒçš„å‘½ä»¤ï¼Œä¾‹å¦‚kill-serverå¯ä»¥å…³é—­æœåŠ¡å™¨ [ è¿›å…¥å¤åˆ¶æ¨¡å¼ï¼›æ­¤æ—¶çš„æ“ä½œä¸vi/emacsç›¸åŒï¼ŒæŒ‰q/Escé€€å‡º ~ åˆ—å‡ºæç¤ºä¿¡æ¯ç¼“å­˜ï¼›å…¶ä¸­åŒ…å«äº†ä¹‹å‰tmuxè¿”å›çš„å„ç§æç¤ºä¿¡æ¯ çª—å£æ“ä½œ c åˆ›å»ºæ–°çª—å£ \u0026 å…³é—­å½“å‰çª—å£ æ•°å­—é”® åˆ‡æ¢è‡³æŒ‡å®šçª—å£ p åˆ‡æ¢è‡³ä¸Šä¸€çª—å£ n åˆ‡æ¢è‡³ä¸‹ä¸€çª—å£ l åœ¨å‰åä¸¤ä¸ªçª—å£é—´äº’ç›¸åˆ‡æ¢ w é€šè¿‡çª—å£åˆ—è¡¨åˆ‡æ¢çª—å£ , é‡å‘½åå½“å‰çª—å£ï¼›è¿™æ ·ä¾¿äºè¯†åˆ« . ä¿®æ”¹å½“å‰çª—å£ç¼–å·ï¼›ç›¸å½“äºçª—å£é‡æ–°æ’åº f åœ¨æ‰€æœ‰çª—å£ä¸­æŸ¥æ‰¾æŒ‡å®šæ–‡æœ¬ é¢æ¿æ“ä½œ â€ å°†å½“å‰é¢æ¿å¹³åˆ†ä¸ºä¸Šä¸‹ä¸¤å— % å°†å½“å‰é¢æ¿å¹³åˆ†ä¸ºå·¦å³ä¸¤å— x å…³é—­å½“å‰é¢æ¿ ! å°†å½“å‰é¢æ¿ç½®äºæ–°çª—å£ï¼›å³æ–°å»ºä¸€ä¸ªçª—å£ï¼Œå…¶ä¸­ä»…åŒ…å«å½“å‰é¢æ¿ Ctrl+æ–¹å‘é”® ä»¥1ä¸ªå•å…ƒæ ¼ä¸ºå•ä½ç§»åŠ¨è¾¹ç¼˜ä»¥è°ƒæ•´å½“å‰é¢æ¿å¤§å° Alt+æ–¹å‘é”® ä»¥5ä¸ªå•å…ƒæ ¼ä¸ºå•ä½ç§»åŠ¨è¾¹ç¼˜ä»¥è°ƒæ•´å½“å‰é¢æ¿å¤§å° Space åœ¨é¢„ç½®çš„é¢æ¿å¸ƒå±€ä¸­å¾ªç¯åˆ‡æ¢ï¼›ä¾æ¬¡åŒ…æ‹¬even-horizontalã€even-verticalã€main-horizontalã€main-verticalã€tiled q æ˜¾ç¤ºé¢æ¿ç¼–å· o åœ¨å½“å‰çª—å£ä¸­é€‰æ‹©ä¸‹ä¸€é¢æ¿ æ–¹å‘é”® ç§»åŠ¨å…‰æ ‡ä»¥é€‰æ‹©é¢æ¿ { å‘å‰ç½®æ¢å½“å‰é¢æ¿ } å‘åç½®æ¢å½“å‰é¢æ¿ Alt+o é€†æ—¶é’ˆæ—‹è½¬å½“å‰çª—å£çš„é¢æ¿ Ctrl+o é¡ºæ—¶é’ˆæ—‹è½¬å½“å‰çª—å£çš„é¢æ¿ å¿«æ·é”®åˆ—è¡¨åŸæ–‡ï¼šhttps://blog.csdn.net/hcx25909/article/details/7602935\n","categories":"","description":"","excerpt":"1. å®‰è£…tmux # linux yum install -y tmux # mac brew install tmux 2. tmuxå¸¸ â€¦","ref":"/linux-notes/keymap/tmux-keymap/","tags":["å¿«æ·é”®"],"title":"tmuxå¿«æ·é”®"},{"body":"1. UDPåè®®æ¦‚è¿° UDP:User Datagram Protocolçš„ç¼©å†™ï¼Œæä¾›é¢å‘æ— è¿æ¥çš„é€šä¿¡æœåŠ¡ï¼Œåœ¨åº”ç”¨ç¨‹åºå‘æ¥æ•°æ®æ”¶åˆ°é‚£ä¸€åˆ»åˆ™ç«‹å³åŸæ ·å‘é€åˆ°ç½‘ç»œä¸Šã€‚å³ä½¿å‡ºç°ä¸¢åŒ…ä¹Ÿä¸è´Ÿè´£é‡å‘ï¼ŒåŒ…å‡ºç°ä¹±åºä¹Ÿä¸èƒ½çº æ­£ã€‚\nUDPå¯ä»¥éšæ—¶å‘é€æ•°æ®ï¼Œæœ¬èº«å¤„ç†ç®€å•é«˜æ•ˆï¼Œä½†ä¸å…·å¤‡å¯é æ€§ï¼Œé€‚åˆä»¥ä¸‹åœºæ™¯ï¼š\nåŒ…æ€»é‡è¾ƒå°‘çš„é€šä¿¡ï¼ˆDNSã€SNMPç­‰ï¼‰ è§†é¢‘ã€éŸ³é¢‘ç­‰å¤šåª’ä½“é€šä¿¡ï¼ˆå³ä½¿é€šä¿¡ï¼‰ é™å®šäºLANç­‰ç‰¹å®šç½‘ç»œä¸­çš„åº”ç”¨é€šä¿¡ å¹¿æ’­é€šä¿¡ï¼ˆå¹¿æ’­ã€å¤šæ’­ï¼‰ ","categories":"","description":"","excerpt":"1. UDPåè®®æ¦‚è¿° UDP:User Datagram Protocolçš„ç¼©å†™ï¼Œæä¾›é¢å‘æ— è¿æ¥çš„é€šä¿¡æœåŠ¡ï¼Œåœ¨åº”ç”¨ç¨‹åºå‘æ¥æ•°æ®æ”¶åˆ°é‚£ä¸€åˆ»åˆ™ç«‹ â€¦","ref":"/linux-notes/tcpip/udp/","tags":["TCPIP"],"title":"UDPåè®®"},{"body":"1. ä½¿ç”¨å…¥é—¨ beego çš„æ—¥å¿—å¤„ç†æ˜¯åŸºäº logs æ¨¡å—æ­å»ºçš„ï¼Œå†…ç½®äº†ä¸€ä¸ªå˜é‡ BeeLoggerï¼Œé»˜è®¤å·²ç»æ˜¯ logs.BeeLogger ç±»å‹ï¼Œåˆå§‹åŒ–äº† consoleï¼Œä¹Ÿå°±æ˜¯é»˜è®¤è¾“å‡ºåˆ° consoleã€‚\nbeego.Emergency(\"this is emergency\") beego.Alert(\"this is alert\") beego.Critical(\"this is critical\") beego.Error(\"this is error\") beego.Warning(\"this is warning\") beego.Notice(\"this is notice\") beego.Informational(\"this is informational\") beego.Debug(\"this is debug\") 2. è®¾ç½®è¾“å‡º æˆ‘ä»¬çš„ç¨‹åºå¾€å¾€æœŸæœ›æŠŠä¿¡æ¯è¾“å‡ºåˆ° log ä¸­ï¼Œç°åœ¨è®¾ç½®è¾“å‡ºåˆ°æ–‡ä»¶å¾ˆæ–¹ä¾¿ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\nbeego.SetLogger(\"file\", `{\"filename\":\"logs/test.log\"}`) è¿™ä¸ªé»˜è®¤æƒ…å†µå°±ä¼šåŒæ—¶è¾“å‡ºåˆ°ä¸¤ä¸ªåœ°æ–¹ï¼Œä¸€ä¸ª consoleï¼Œä¸€ä¸ª fileï¼Œå¦‚æœåªæƒ³è¾“å‡ºåˆ°æ–‡ä»¶ï¼Œå°±éœ€è¦è°ƒç”¨åˆ é™¤æ“ä½œï¼š\nbeego.BeeLogger.DelLogger(\"console\") 3. è®¾ç½®çº§åˆ« LevelEmergency LevelAlert LevelCritical LevelError LevelWarning LevelNotice LevelInformational LevelDebug çº§åˆ«ä¾æ¬¡é™ä½ï¼Œé»˜è®¤å…¨éƒ¨æ‰“å°ï¼Œä½†æ˜¯ä¸€èˆ¬æˆ‘ä»¬åœ¨éƒ¨ç½²ç¯å¢ƒï¼Œå¯ä»¥é€šè¿‡è®¾ç½®çº§åˆ«è®¾ç½®æ—¥å¿—çº§åˆ«ï¼š\nbeego.SetLevel(beego.LevelInformational) 4. è¾“å‡ºæ–‡ä»¶åå’Œè¡Œå· æ—¥å¿—é»˜è®¤ä¸è¾“å‡ºè°ƒç”¨çš„æ–‡ä»¶åå’Œæ–‡ä»¶è¡Œå·,å¦‚æœä½ æœŸæœ›è¾“å‡ºè°ƒç”¨çš„æ–‡ä»¶åå’Œæ–‡ä»¶è¡Œå·,å¯ä»¥å¦‚ä¸‹è®¾ç½®\nbeego.SetLogFuncCall(true) å¼€å¯ä¼ å…¥å‚æ•° true, å…³é—­ä¼ å…¥å‚æ•° false, é»˜è®¤æ˜¯å…³é—­çš„ã€‚\n5. beego/logsæ¨¡å—çš„ä½¿ç”¨ æ˜¯ä¸€ä¸ªç”¨æ¥å¤„ç†æ—¥å¿—çš„åº“ï¼Œç›®å‰æ”¯æŒçš„å¼•æ“æœ‰ fileã€consoleã€netã€smtpï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼è¿›è¡Œå®‰è£…ï¼š\ngo get github.com/astaxie/beego/logs 5.1. é€šç”¨æ–¹å¼ é¦–å…ˆå¼•å…¥åŒ…ï¼š\nimport ( \"github.com/astaxie/beego/logs\" ) ç„¶åæ·»åŠ è¾“å‡ºå¼•æ“ï¼ˆlog æ”¯æŒåŒæ—¶è¾“å‡ºåˆ°å¤šä¸ªå¼•æ“ï¼‰ï¼Œè¿™é‡Œæˆ‘ä»¬ä»¥ console ä¸ºä¾‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¼•æ“åï¼ˆåŒ…æ‹¬ï¼šconsoleã€fileã€connã€smtpã€esã€multifileï¼‰\nlogs.SetLogger(\"console\") æ·»åŠ è¾“å‡ºå¼•æ“ä¹Ÿæ”¯æŒç¬¬äºŒä¸ªå‚æ•°,ç”¨æ¥è¡¨ç¤ºé…ç½®ä¿¡æ¯ï¼Œè¯¦ç»†çš„é…ç½®è¯·çœ‹ä¸‹é¢ä»‹ç»ï¼š\nlogs.SetLogger(logs.AdapterFile,`{\"filename\":\"project.log\",\"level\":7,\"maxlines\":0,\"maxsize\":0,\"daily\":true,\"maxdays\":10}`) ç¤ºä¾‹ï¼š\npackage main import ( \"github.com/astaxie/beego/logs\" ) func main() { //an official log.Logger l := logs.GetLogger() l.Println(\"this is a message of http\") //an official log.Logger with prefix ORM logs.GetLogger(\"ORM\").Println(\"this is a message of orm\") logs.Debug(\"my book is bought in the year of \", 2016) logs.Info(\"this %s cat is %v years old\", \"yellow\", 3) logs.Warn(\"json is a type of kv like\", map[string]int{\"key\": 2016}) logs.Error(1024, \"is a very\", \"good game\") logs.Critical(\"oh,crash\") } 5.2. è¾“å‡ºæ–‡ä»¶åå’Œè¡Œå· æ—¥å¿—é»˜è®¤ä¸è¾“å‡ºè°ƒç”¨çš„æ–‡ä»¶åå’Œæ–‡ä»¶è¡Œå·,å¦‚æœä½ æœŸæœ›è¾“å‡ºè°ƒç”¨çš„æ–‡ä»¶åå’Œæ–‡ä»¶è¡Œå·,å¯ä»¥å¦‚ä¸‹è®¾ç½®\nlogs.EnableFuncCallDepth(true) å¼€å¯ä¼ å…¥å‚æ•° true,å…³é—­ä¼ å…¥å‚æ•° false,é»˜è®¤æ˜¯å…³é—­çš„.\nå¦‚æœä½ çš„åº”ç”¨è‡ªå·±å°è£…äº†è°ƒç”¨ log åŒ…,é‚£ä¹ˆéœ€è¦è®¾ç½® SetLogFuncCallDepth,é»˜è®¤æ˜¯ 2,ä¹Ÿå°±æ˜¯ç›´æ¥è°ƒç”¨çš„å±‚çº§,å¦‚æœä½ å°è£…äº†å¤šå±‚,é‚£ä¹ˆéœ€è¦æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´.\nlogs.SetLogFuncCallDepth(3) 5.3. å¼‚æ­¥è¾“å‡ºæ—¥å¿— ä¸ºäº†æå‡æ€§èƒ½, å¯ä»¥è®¾ç½®å¼‚æ­¥è¾“å‡º:\nlogs.Async() å¼‚æ­¥è¾“å‡ºå…è®¸è®¾ç½®ç¼“å†² chan çš„å¤§å°\nlogs.Async(1e3) 5.4. å¼•æ“é…ç½® console\nå¯ä»¥è®¾ç½®è¾“å‡ºçš„çº§åˆ«ï¼Œæˆ–è€…ä¸è®¾ç½®ä¿æŒé»˜è®¤ï¼Œé»˜è®¤è¾“å‡ºåˆ° os.Stdoutï¼š\nlogs.SetLogger(logs.AdapterConsole, `{\"level\":1}`) file\nè®¾ç½®çš„ä¾‹å­å¦‚ä¸‹æ‰€ç¤ºï¼š\nlogs.SetLogger(logs.AdapterFile, `{\"filename\":\"test.log\"}`) ä¸»è¦çš„å‚æ•°å¦‚ä¸‹è¯´æ˜ï¼š\nfilename ä¿å­˜çš„æ–‡ä»¶å maxlines æ¯ä¸ªæ–‡ä»¶ä¿å­˜çš„æœ€å¤§è¡Œæ•°ï¼Œé»˜è®¤å€¼ 1000000 maxsize æ¯ä¸ªæ–‡ä»¶ä¿å­˜çš„æœ€å¤§å°ºå¯¸ï¼Œé»˜è®¤å€¼æ˜¯ 1 \u003c\u003c 28, //256 MB daily æ˜¯å¦æŒ‰ç…§æ¯å¤© logrotateï¼Œé»˜è®¤æ˜¯ true maxdays æ–‡ä»¶æœ€å¤šä¿å­˜å¤šå°‘å¤©ï¼Œé»˜è®¤ä¿å­˜ 7 å¤© rotate æ˜¯å¦å¼€å¯ logrotateï¼Œé»˜è®¤æ˜¯ true level æ—¥å¿—ä¿å­˜çš„æ—¶å€™çš„çº§åˆ«ï¼Œé»˜è®¤æ˜¯ Trace çº§åˆ« perm æ—¥å¿—æ–‡ä»¶æƒé™ multifile\nè®¾ç½®çš„ä¾‹å­å¦‚ä¸‹æ‰€ç¤ºï¼š\nlogs.SetLogger(logs.AdapterMultiFile, ``{\"filename\":\"test.log\",\"separate\":[\"emergency\", \"alert\", \"critical\", \"error\", \"warning\", \"notice\", \"info\", \"debug\"]}``) ä¸»è¦çš„å‚æ•°å¦‚ä¸‹è¯´æ˜(é™¤ separate å¤–,å‡ä¸fileç›¸åŒ)ï¼š\nfilename ä¿å­˜çš„æ–‡ä»¶å maxlines æ¯ä¸ªæ–‡ä»¶ä¿å­˜çš„æœ€å¤§è¡Œæ•°ï¼Œé»˜è®¤å€¼ 1000000 maxsize æ¯ä¸ªæ–‡ä»¶ä¿å­˜çš„æœ€å¤§å°ºå¯¸ï¼Œé»˜è®¤å€¼æ˜¯ 1 \u003c\u003c 28, //256 MB daily æ˜¯å¦æŒ‰ç…§æ¯å¤© logrotateï¼Œé»˜è®¤æ˜¯ true maxdays æ–‡ä»¶æœ€å¤šä¿å­˜å¤šå°‘å¤©ï¼Œé»˜è®¤ä¿å­˜ 7 å¤© rotate æ˜¯å¦å¼€å¯ logrotateï¼Œé»˜è®¤æ˜¯ true level æ—¥å¿—ä¿å­˜çš„æ—¶å€™çš„çº§åˆ«ï¼Œé»˜è®¤æ˜¯ Trace çº§åˆ« perm æ—¥å¿—æ–‡ä»¶æƒé™ separate éœ€è¦å•ç‹¬å†™å…¥æ–‡ä»¶çš„æ—¥å¿—çº§åˆ«,è®¾ç½®åå‘½åç±»ä¼¼ test.error.log conn\nç½‘ç»œè¾“å‡ºï¼Œè®¾ç½®çš„ä¾‹å­å¦‚ä¸‹æ‰€ç¤ºï¼š\nlogs.SetLogger(logs.AdapterConn, `{\"net\":\"tcp\",\"addr\":\":7020\"}`) ä¸»è¦çš„å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\nreconnectOnMsg æ˜¯å¦æ¯æ¬¡é“¾æ¥éƒ½é‡æ–°æ‰“å¼€é“¾æ¥ï¼Œé»˜è®¤æ˜¯ false reconnect æ˜¯å¦è‡ªåŠ¨é‡æ–°é“¾æ¥åœ°å€ï¼Œé»˜è®¤æ˜¯ false net å‘å¼€ç½‘ç»œé“¾æ¥çš„æ–¹å¼ï¼Œå¯ä»¥ä½¿ç”¨ tcpã€unixã€udp ç­‰ addr ç½‘ç»œé“¾æ¥çš„åœ°å€ level æ—¥å¿—ä¿å­˜çš„æ—¶å€™çš„çº§åˆ«ï¼Œé»˜è®¤æ˜¯ Trace çº§åˆ« smtp\né‚®ä»¶å‘é€ï¼Œè®¾ç½®çš„ä¾‹å­å¦‚ä¸‹æ‰€ç¤ºï¼š\nlogs.SetLogger(logs.AdapterMail, `{\"username\":\"beegotest@gmail.com\",\"password\":\"xxxxxxxx\",\"host\":\"smtp.gmail.com:587\",\"sendTos\":[\"xiemengjun@gmail.com\"]}`) ä¸»è¦çš„å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\nusername smtp éªŒè¯çš„ç”¨æˆ·å password smtp éªŒè¯å¯†ç  host å‘é€çš„é‚®ç®±åœ°å€ sendTos é‚®ä»¶éœ€è¦å‘é€çš„äººï¼Œæ”¯æŒå¤šä¸ª subject å‘é€é‚®ä»¶çš„æ ‡é¢˜ï¼Œé»˜è®¤æ˜¯ Diagnostic message from server level æ—¥å¿—å‘é€çš„çº§åˆ«ï¼Œé»˜è®¤æ˜¯ Trace çº§åˆ« ElasticSearch\nè¾“å‡ºåˆ° ElasticSearch:\nlogs.SetLogger(logs.AdapterEs, `{\"dsn\":\"http://localhost:9200/\",\"level\":1}` å‚è€ƒæ–‡ç« ï¼š\nhttps://beego.me/docs/mvc/controller/logs.md https://beego.me/docs/module/logs.md ","categories":"","description":"","excerpt":"1. ä½¿ç”¨å…¥é—¨ beego çš„æ—¥å¿—å¤„ç†æ˜¯åŸºäº logs æ¨¡å—æ­å»ºçš„ï¼Œå†…ç½®äº†ä¸€ä¸ªå˜é‡ BeeLoggerï¼Œ â€¦","ref":"/golang-notes/web/beego/beego-log/","tags":["Golang"],"title":"Beego æ—¥å¿—å¤„ç†"},{"body":"1. govendorç®€ä»‹ golangå·¥ç¨‹çš„ä¾èµ–åŒ…ç»å¸¸ä½¿ç”¨go get çš„æ–¹å¼æ¥è·å–ï¼Œä¾‹å¦‚ï¼šgo get github.com/kardianos/govendor ï¼Œä¼šå°†ä¾èµ–åŒ…ä¸‹è½½åˆ°GOPATHçš„è·¯å¾„ä¸‹ã€‚\nå¸¸ç”¨çš„ä¾èµ–åŒ…ç®¡ç†å·¥å…·æœ‰godepï¼Œgovendorç­‰ï¼Œåœ¨Golang1.5ä¹‹åï¼ŒGoæä¾›äº† GO15VENDOREXPERIMENT ç¯å¢ƒå˜é‡ï¼Œç”¨äºå°†go buildæ—¶çš„åº”ç”¨è·¯å¾„æœç´¢è°ƒæ•´æˆä¸º å½“å‰é¡¹ç›®ç›®å½•/vendor ç›®å½•æ–¹å¼ã€‚é€šè¿‡è¿™ç§å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ç±»ä¼¼äº godep æ–¹å¼çš„é¡¹ç›®ä¾èµ–ç®¡ç†ã€‚\n2. å®‰è£…ä¸ä½¿ç”¨ 2.1. å®‰è£… go get -u -v github.com/kardianos/govendor 2.2. ä½¿ç”¨ #è¿›å…¥åˆ°é¡¹ç›®ç›®å½• cd /home/gopath/src/mytool #åˆå§‹åŒ–vendorç›®å½• govendor init #æŸ¥çœ‹vendorç›®å½• [root@CC54425A mytool]# ls commands main.go vendor mytool_test.sh #è¿›å…¥vendorç›®å½• cd vendor #å°†GOPATHä¸­æœ¬å·¥ç¨‹ä½¿ç”¨åˆ°çš„ä¾èµ–åŒ…è‡ªåŠ¨ç§»åŠ¨åˆ°vendorç›®å½•ä¸­ #è¯´æ˜ï¼šå¦‚æœæœ¬åœ°GOPATHæ²¡æœ‰ä¾èµ–åŒ…ï¼Œå…ˆgo getç›¸åº”çš„ä¾èµ–åŒ… govendor add +external #é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ GO15VENDOREXPERIMENT=1 ä½¿ç”¨vendoræ–‡ä»¶å¤¹æ„å»ºæ–‡ä»¶ã€‚ #å¯ä»¥é€‰æ‹© export GO15VENDOREXPERIMENT=1 æˆ– GO15VENDOREXPERIMENT=1 go build æ‰§è¡Œç¼–è¯‘ export GO15VENDOREXPERIMENT=1 2.3. è¯´æ˜ govendoråªæ˜¯ç”¨æ¥ç®¡ç†é¡¹ç›®çš„ä¾èµ–åŒ…ï¼Œå¦‚æœGOPATHä¸­æœ¬èº«æ²¡æœ‰é¡¹ç›®çš„ä¾èµ–åŒ…ï¼Œåˆ™éœ€è¦é€šè¿‡go getå…ˆä¸‹è½½åˆ°GOPATHä¸­ï¼Œå†é€šè¿‡govendor add +external æ‹·è´åˆ°vendorç›®å½•ä¸­ã€‚\n3. govendorçš„é…ç½®æ–‡ä»¶ vendor.jsonè®°å½•ä¾èµ–åŒ…åˆ—è¡¨ã€‚\n{ \"comment\": \"\", \"ignore\": \"test\", \"package\": [ { \"checksumSHA1\": \"uGalSICR4r7354vvKuNnh7Y/R/4=\", \"path\": \"github.com/urfave/cli\", \"revision\": \"b99aa811b4c1dd84cc6bccb8499c82c72098085a\", \"revisionTime\": \"2017-08-04T09:34:15Z\" } ], \"rootPath\": \"mytool\" } 4. govendorä½¿ç”¨å‘½ä»¤ [root@CC54425A mytool]# govendor govendor (v1.0.8): record dependencies and copy into vendor folder -govendor-licenses Show govendor's licenses. -version Show govendor version -cpuprofile 'file' Writes a CPU profile to 'file' for debugging. -memprofile 'file' Writes a heap profile to 'file' for debugging. Sub-Commands init Create the \"vendor\" folder and the \"vendor.json\" file. list List and filter existing dependencies and packages. add Add packages from $GOPATH. update Update packages from $GOPATH. remove Remove packages from the vendor folder. status Lists any packages missing, out-of-date, or modified locally. fetch Add new or update vendor folder packages from remote repository. sync Pull packages into vendor folder from remote repository with revisions from vendor.json file. migrate Move packages from a legacy tool to the vendor folder with metadata. get Like \"go get\" but copies dependencies into a \"vendor\" folder. license List discovered licenses for the given status or import paths. shell Run a \"shell\" to make multiple sub-commands more efficient for large projects. go tool commands that are wrapped: \"+status\" package selection may be used with them fmt, build, install, clean, test, vet, generate, tool Status Types +local (l) packages in your project +external (e) referenced packages in GOPATH but not in current project +vendor (v) packages in the vendor folder +std (s) packages in the standard library +excluded (x) external packages explicitly excluded from vendoring +unused (u) packages in the vendor folder, but unused +missing (m) referenced packages but not found +program (p) package is a main package +outside +external +missing +all +all packages Status can be referenced by their initial letters. Package specifier \u003cpath\u003e[::\u003corigin\u003e][{/...|/^}][@[\u003cversion-spec\u003e]] Ignoring files with build tags, or excluding packages from being vendored: The \"vendor.json\" file contains a string field named \"ignore\". It may contain a space separated list of build tags to ignore when listing and copying files. This list may also contain package prefixes (containing a \"/\", possibly as last character) to exclude when copying files in the vendor folder. If \"foo/\" appears in this field, then package \"foo\" and all its sub-packages (\"foo/bar\", â€¦) will be excluded (but package \"bar/foo\" will not). By default the init command adds the \"test\" tag to the ignore list. If using go1.5, ensure GO15VENDOREXPERIMENT=1 is set. ","categories":"","description":"","excerpt":"1. govendorç®€ä»‹ golangå·¥ç¨‹çš„ä¾èµ–åŒ…ç»å¸¸ä½¿ç”¨go get çš„æ–¹å¼æ¥è·å–ï¼Œä¾‹å¦‚ï¼šgo get â€¦","ref":"/golang-notes/introduction/package/govendor-usage/","tags":["Golang"],"title":"govendorçš„ä½¿ç”¨"},{"body":"1. heapsterç®€ä»‹ Heapsteræ˜¯å®¹å™¨é›†ç¾¤ç›‘æ§å’Œæ€§èƒ½åˆ†æå·¥å…·ï¼Œå¤©ç„¶çš„æ”¯æŒKuberneteså’ŒCoreOSã€‚ Kubernetesæœ‰ä¸ªå‡ºåçš„ç›‘æ§agentâ€”cAdvisorã€‚åœ¨æ¯ä¸ªkubernetes Nodeä¸Šéƒ½ä¼šè¿è¡ŒcAdvisorï¼Œå®ƒä¼šæ”¶é›†æœ¬æœºä»¥åŠå®¹å™¨çš„ç›‘æ§æ•°æ®(cpu,memory,filesystem,network,uptime)ã€‚\n2. heapsteréƒ¨ç½²ä¸é…ç½® 2.1. æ³¨æ„äº‹é¡¹ éœ€åŒæ­¥éƒ¨ç½²æœºå™¨å’Œè¢«é‡‡é›†æœºå™¨çš„æ—¶é—´ï¼šntpdate time.windows.com\nåŠ å…¥å®šæ—¶ä»»åŠ¡ï¼Œå®šæœŸåŒæ­¥æ—¶é—´\ncrontab â€“e\n30 5 * * * /usr/sbin/ntpdate time.windows.com //æ¯å¤©æ—©æ™¨5ç‚¹åŠæ‰§è¡Œ\n2.2. å®¹å™¨å¼éƒ¨ç½² #æ‹‰å–é•œåƒ docker pull heapster:latest #è¿è¡Œå®¹å™¨ docker run -d -p 8082:8082 --net=host heapster:latest --source=kubernetes:http://\u003ck8s-server-ip\u003e:8080?inClusterConfig=false\\\u0026useServiceAccount=false --sink=influxdb:http://\u003cinfluxdb-ip\u003e:8086?db=\u003ck8s_env_zone\u003e 2.3. é…ç½®è¯´æ˜ å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£\n2.3.1. â€“source â€“source: æŒ‡å®šæ•°æ®è·å–æºã€‚è¿™é‡ŒæŒ‡å®škube-apiserverå³å¯ã€‚ åç¼€å‚æ•°ï¼š inClusterConfig: kubeletPort: æŒ‡å®škubeletçš„ä½¿ç”¨ç«¯å£ï¼Œé»˜è®¤10255 kubeletHttps: æ˜¯å¦ä½¿ç”¨httpså»è¿æ¥kubelets(é»˜è®¤ï¼šfalse) apiVersion: æŒ‡å®šK8Sçš„apiversion insecure: æ˜¯å¦ä½¿ç”¨å®‰å…¨è¯ä¹¦(é»˜è®¤ï¼šfalse) auth: å®‰å…¨è®¤è¯ useServiceAccount: æ˜¯å¦ä½¿ç”¨K8Sçš„å®‰å…¨ä»¤ç‰Œ\n2.3.2. â€“sink â€“sink: æŒ‡å®šåç«¯æ•°æ®å­˜å‚¨ã€‚è¿™é‡ŒæŒ‡å®šinfluxdbæ•°æ®åº“ã€‚ åç¼€å‚æ•°ï¼š user: InfluxDBç”¨æˆ· pw: InfluxDBå¯†ç  db: æ•°æ®åº“å secure: å®‰å…¨è¿æ¥åˆ°InfluxDB(é»˜è®¤ï¼šfalse) withfieldsï¼š ä½¿ç”¨InfluxDB fields(é»˜è®¤ï¼šfalse)ã€‚\n3. Metrics åˆ†ç±» Metric Name Description å¤‡æ³¨ cpu cpu/limit CPU hard limit in millicores. CPUä¸Šé™ cpu/node_capacity Cpu capacity of a node. NodeèŠ‚ç‚¹çš„CPUå®¹é‡ cpu/node_allocatable Cpu allocatable of a node. NodeèŠ‚ç‚¹å¯åˆ†é…çš„CPU cpu/node_reservation Share of cpu that is reserved on the node allocatable. cpu/node_utilization CPU utilization as a share of node allocatable. cpu/request CPU request (the guaranteed amount of resources) in millicores. cpu/usage Cumulative CPU usage on all cores. CPUæ€»ä½¿ç”¨é‡ cpu/usage_rate CPU usage on all cores in millicores. filesystem filesystem/usage Total number of bytes consumed on a filesystem. æ–‡ä»¶ç³»ç»Ÿçš„ä½¿ç”¨é‡ filesystem/limit The total size of filesystem in bytes. æ–‡ä»¶ç³»ç»Ÿçš„ä½¿ç”¨ä¸Šé™ filesystem/available The number of available bytes remaining in a the filesystem å¯ç”¨çš„æ–‡ä»¶ç³»ç»Ÿå®¹é‡ filesystem/inodes The number of available inodes in a the filesystem filesystem/inodes_free The number of free inodes remaining in a the filesystem memory memory/limit Memory hard limit in bytes. å†…å­˜ä¸Šé™ memory/major_page_faults Number of major page faults. memory/major_page_faults_rate Number of major page faults per second. memory/node_capacity Memory capacity of a node. memory/node_allocatable Memory allocatable of a node. memory/node_reservation Share of memory that is reserved on the node allocatable. memory/node_utilization Memory utilization as a share of memory allocatable. memory/page_faults Number of page faults. memory/page_faults_rate Number of page faults per second. memory/request Memory request (the guaranteed amount of resources) in bytes. memory/usage Total memory usage. memory/cache Cache memory usage. memory/rss RSS memory usage. memory/working_set Total working set usage. Working set is the memory being used and not easily dropped by the kernel. network network/rx Cumulative number of bytes received over the network. network/rx_errors Cumulative number of errors while receiving over the network. network/rx_errors_rate Number of errors while receiving over the network per second. network/rx_rate Number of bytes received over the network per second. network/tx Cumulative number of bytes sent over the network network/tx_errors Cumulative number of errors while sending over the network network/tx_errors_rate Number of errors while sending over the network network/tx_rate Number of bytes sent over the network per second. uptime Number of milliseconds since the container was started. - 4. Labels Label Name Description pod_id Unique ID of a Pod pod_name User-provided name of a Pod pod_namespace The namespace of a Pod container_base_image Base image for the container container_name User-provided name of the container or full cgroup name for system containers host_id Cloud-provider specified or user specified Identifier of a node hostname Hostname where the container ran labels Comma-separated(Default) list of user-provided labels. Format is 'key:value' namespace_id UID of the namespace of a Pod resource_id A unique identifier used to differentiate multiple metrics of the same type. e.x. Fs partitions under filesystem/usage 5. heapster API è§å®˜æ–¹æ–‡æ¡£ï¼šhttps://github.com/kubernetes/heapster/blob/master/docs/model.md\n","categories":"","description":"","excerpt":"1. heapsterç®€ä»‹ Heapsteræ˜¯å®¹å™¨é›†ç¾¤ç›‘æ§å’Œæ€§èƒ½åˆ†æå·¥å…·ï¼Œå¤©ç„¶çš„æ”¯æŒKuberneteså’ŒCoreOSã€‚ â€¦","ref":"/kubernetes-notes/monitor/heapster-introduction/","tags":["Monitor"],"title":"Heapsterä»‹ç»"},{"body":"4. è¡¨å†…å®¹æ“ä½œ 4.1. å¢ insert into è¡¨ (åˆ—å,åˆ—å...) values (å€¼,å€¼,...) insert into è¡¨ (åˆ—å,åˆ—å...) values (å€¼,å€¼,...),(å€¼,å€¼,å€¼...) insert into è¡¨ (åˆ—å,åˆ—å...) select (åˆ—å,åˆ—å...) from è¡¨ ä¾‹ï¼š insert into tab1(name,email) values('zhangyanlin','zhangyanlin8851@163.com') 4.2. åˆ  delete from è¡¨ # åˆ é™¤è¡¨é‡Œå…¨éƒ¨æ•°æ® delete from è¡¨ where idï¼1 and nameï¼'zhangyanlin' # åˆ é™¤ID =1 å’Œname='zhangyanlin' é‚£ä¸€è¡Œæ•°æ® 4.3. æ”¹ update è¡¨ set name ï¼ 'zhangyanlin' where id\u003e1 4.4. æŸ¥ select * from è¡¨ select * from è¡¨ where id \u003e 1 select nid,name,gender as gg from è¡¨ where id \u003e 1 4.5. æ¡ä»¶åˆ¤æ–­ 4.5.1. where select * from \u003ctable\u003e where id \u003e1 and name!='huwh' and num =12; select * from \u003ctable\u003e where id between 5 and 6; select * from \u003ctable\u003e where id in (11,22,33); select * from \u003ctable\u003e where id not in (11,22,33); select * from \u003ctable\u003e where id in (select nid from \u003ctable\u003e) 4.5.2. é€šé…ç¬¦like select * from \u003ctable\u003e where name like 'hu%'; #huå¼€å¤´ select * from \u003ctable\u003e where name like 'hu_' #huå¼€å¤´åæ¥ä¸€ä¸ªå­—ç¬¦ 4.5.3. é™åˆ¶limit select * from \u003ctable\u003e limit 5; #å‰5è¡Œ select * from \u003ctable\u003e limit 4,5 #ä»ç¬¬å››è¡Œå¼€å§‹çš„5è¡Œ select * from \u003ctable\u003e limit 5 offset 4;#ä»ç¬¬å››è¡Œå¼€å§‹çš„5è¡Œ 4.5.4. æ’åºascï¼Œdesc select * from \u003ctable\u003e order by åˆ— asc; #è·Ÿæ®â€œåˆ—â€ä»å°åˆ°å¤§æ’åºï¼ˆä¸æŒ‡å®šé»˜è®¤ä¸ºä»å°åˆ°å¤§æ’åºï¼‰ select * from \u003ctable\u003e order by åˆ— desc; #æ ¹æ®â€œåˆ—â€ä»å¤§åˆ°å°æ’åº select * from \u003ctable\u003e order by åˆ—1 desc,åˆ—2 asc; #æ ¹æ®â€œåˆ—1â€ä»å¤§åˆ°å°æ’åºï¼Œå¦‚æœç›¸åŒåˆ™æŒ‰â€œåˆ—2â€ä»å°åˆ°å¤§æ’åº 4.5.5. åˆ†ç»„group by group by å¿…é¡»åœ¨whereä¹‹åï¼Œorder byä¹‹å‰ã€‚\nselect num,from \u003ctable\u003e group by num; select num,nid from \u003ctable\u003e group by num,nid; select num from \u003ctable\u003e where nid \u003e 10 group by num,nid order nid desc; select num,nid,count(*),sum(score),max(score) from \u003ctable\u003e group by num; select num from \u003ctable\u003e group by num having max(id) \u003e 10; select num from \u003ctable\u003e group by num; ","categories":"","description":"","excerpt":"4. è¡¨å†…å®¹æ“ä½œ 4.1. å¢ insert into è¡¨ (åˆ—å,åˆ—å...) values (å€¼,å€¼,...) insert into  â€¦","ref":"/linux-notes/mysql/curd-commands/","tags":["Mysql"],"title":"Mysqlå¸¸ç”¨å‘½ä»¤ä¹‹è¡¨å†…å®¹æ“ä½œ"},{"body":"Podå¥åº·æ£€æŸ¥ Podçš„å¥åº·çŠ¶æ€ç”±ä¸¤ç±»æ¢é’ˆæ¥æ£€æŸ¥ï¼šLivenessProbeå’ŒReadinessProbeã€‚\n1. æ¢é’ˆç±»å‹ 1. livenessProbe(å­˜æ´»æ¢é’ˆ)\nè¡¨æ˜å®¹å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œã€‚ å¦‚æœå­˜æ´»æ¢æµ‹å¤±è´¥ï¼Œåˆ™ kubelet ä¼šæ€æ­»å®¹å™¨ï¼Œå¹¶ä¸”å®¹å™¨å°†å—åˆ°å…¶ é‡å¯ç­–ç•¥çš„å½±å“ã€‚ å¦‚æœå®¹å™¨ä¸æä¾›å­˜æ´»æ¢é’ˆï¼Œåˆ™é»˜è®¤çŠ¶æ€ä¸º Successã€‚ 2. readinessProbe(å°±ç»ªæ¢é’ˆ)\nè¡¨æ˜å®¹å™¨æ˜¯å¦å¯ä»¥æ­£å¸¸æ¥å—è¯·æ±‚ã€‚ å¦‚æœå°±ç»ªæ¢æµ‹å¤±è´¥ï¼Œç«¯ç‚¹æ§åˆ¶å™¨å°†ä»ä¸ Pod åŒ¹é…çš„æ‰€æœ‰ Service çš„ç«¯ç‚¹ä¸­åˆ é™¤è¯¥ Pod çš„ IP åœ°å€ã€‚ åˆå§‹å»¶è¿Ÿä¹‹å‰çš„å°±ç»ªçŠ¶æ€é»˜è®¤ä¸º Failureã€‚ å¦‚æœå®¹å™¨ä¸æä¾›å°±ç»ªæ¢é’ˆï¼Œåˆ™é»˜è®¤çŠ¶æ€ä¸º Successã€‚ 2. æ¢é’ˆä½¿ç”¨åœºæ™¯ å¦‚æœå®¹å™¨å¼‚å¸¸å¯ä»¥è‡ªåŠ¨å´©æºƒï¼Œåˆ™ä¸ä¸€å®šè¦ä½¿ç”¨æ¢é’ˆï¼Œå¯ä»¥ç”±Podçš„restartPolicyæ‰§è¡Œé‡å¯æ“ä½œã€‚ å­˜æ´»æ¢é’ˆé€‚ç”¨äºå¸Œæœ›å®¹å™¨æ¢æµ‹å¤±è´¥åè¢«æ€æ­»å¹¶é‡æ–°å¯åŠ¨ï¼Œéœ€è¦æŒ‡å®šrestartPolicy ä¸º Always æˆ– OnFailureã€‚ å°±ç»ªæ¢é’ˆé€‚ç”¨äºå¸Œæœ›Podåœ¨ä¸èƒ½æ­£å¸¸æ¥æ”¶æµé‡çš„æ—¶å€™è¢«å‰”é™¤ï¼Œå¹¶ä¸”åœ¨å°±ç»ªæ¢é’ˆæ¢æµ‹æˆåŠŸåæ‰æ¥æ”¶æµé‡ã€‚ æ¢é’ˆæ˜¯kubeletå¯¹å®¹å™¨æ‰§è¡Œå®šæœŸçš„è¯Šæ–­ï¼Œä¸»è¦é€šè¿‡è°ƒç”¨å®¹å™¨é…ç½®çš„å››ç±»Handlerå®ç°ï¼š\nHandlerçš„ç±»å‹ï¼š\nExecActionï¼šåœ¨å®¹å™¨å†…æ‰§è¡ŒæŒ‡å®šå‘½ä»¤ã€‚å¦‚æœå‘½ä»¤é€€å‡ºæ—¶è¿”å›ç ä¸º 0 åˆ™è®¤ä¸ºè¯Šæ–­æˆåŠŸã€‚ TCPSocketActionï¼šå¯¹æŒ‡å®šç«¯å£ä¸Šçš„å®¹å™¨çš„ IP åœ°å€è¿›è¡Œ TCP æ£€æŸ¥ã€‚å¦‚æœç«¯å£æ‰“å¼€ï¼Œåˆ™è¯Šæ–­è¢«è®¤ä¸ºæ˜¯æˆåŠŸçš„ã€‚ HTTPGetActionï¼šå¯¹æŒ‡å®šçš„ç«¯å£å’Œè·¯å¾„ä¸Šçš„å®¹å™¨çš„ IP åœ°å€æ‰§è¡Œ HTTP Get è¯·æ±‚ã€‚å¦‚æœå“åº”çš„çŠ¶æ€ç å¤§äºç­‰äº200 ä¸”å°äº 400ï¼Œåˆ™è¯Šæ–­è¢«è®¤ä¸ºæ˜¯æˆåŠŸçš„ã€‚ GRPCActionï¼šè°ƒç”¨GRPCæ¥å£æ¥åˆ¤æ–­æœåŠ¡æ˜¯å¦å¥åº·ã€‚Â å¦‚æœå“åº”çš„çŠ¶æ€æ˜¯ \"SERVING\"ï¼Œåˆ™è®¤ä¸ºè¯Šæ–­æˆåŠŸã€‚ æ¢æµ‹ç»“æœä¸ºä»¥ä¸‹ä¸‰ç§ä¹‹ä¸€ï¼š\næˆåŠŸï¼šå®¹å™¨é€šè¿‡äº†è¯Šæ–­ã€‚ å¤±è´¥ï¼šå®¹å™¨æœªé€šè¿‡è¯Šæ–­ã€‚ æœªçŸ¥ï¼šè¯Šæ–­å¤±è´¥ï¼Œå› æ­¤ä¸ä¼šé‡‡å–ä»»ä½•è¡ŒåŠ¨ã€‚ 3. æ¢é’ˆçš„é…ç½® æ¢é’ˆé…ç½®åœ¨podçš„containerç»“æ„ä½“ä¸‹ï¼ŒlivenessProbeå’ŒreadinessProbeå‚æ•°åŸºæœ¬ä¸€è‡´ã€‚\n3.1. æ¢é’ˆé€šç”¨å‚æ•° å¸¸ç”¨çš„å‚æ•°ä¸ºtimeoutSecondsã€periodSecondsã€periodSecondsï¼Œå³æ¥å£è¶…æ—¶æ—¶é—´ï¼Œé‡è¯•é¢‘ç‡ï¼Œé‡è¯•æ¬¡æ•°ä¸‰ä¸ªå€¼ã€‚\ninitialDelaySecondsï¼šå¯åŠ¨å®¹å™¨åé¦–æ¬¡è¿›è¡Œå¥åº·æ£€æŸ¥çš„ç­‰å¾…æ—¶é—´ï¼Œå•ä½ä¸ºç§’ï¼Œé»˜è®¤å€¼ä¸º0ã€‚ timeoutSeconds:å¥åº·æ£€æŸ¥æ¥å£è¶…æ—¶å“åº”çš„æ—¶é—´ï¼Œé»˜è®¤ä¸º1ç§’ï¼Œæœ€å°å€¼ä¸º1ç§’ã€‚ periodSecondsï¼šé‡è¯•çš„é¢‘ç‡ï¼Œé»˜è®¤å€¼ä¸º10ç§’ï¼Œå³10ç§’é‡è¯•ä¸€æ¬¡ï¼Œæœ€å°å€¼æ˜¯1ç§’ï¼Œå»ºè®®å¯ä»¥è®¾ç½®ä¸º3-5ç§’ã€‚ failureThresholdï¼šå¤±è´¥é‡è¯•çš„æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º3ï¼Œæœ€å°å€¼ä¸º1ã€‚ successThresholdï¼šæœ€å°æ¢æµ‹æˆåŠŸæ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º1ï¼Œä¸€èˆ¬ä¸è®¾ç½®ã€‚ é™¤äº†ä»¥ä¸Šçš„é€šç”¨å‚æ•°å¤–ï¼ŒlivenessProbeå’ŒreadinessProbeå‚æ•°åŸºæœ¬ä¸€è‡´ã€‚ä»¥ä¸‹ä»¥readinessProbeä¸ºä¾‹è¯´æ˜æ¢é’ˆçš„ä½¿ç”¨æ–¹å¼ã€‚\n3.2. ReadinessProbeä¸‰ç§å®ç°æ–¹å¼ 3.2.1. HTTPGetAction é€šè¿‡å®¹å™¨çš„IPåœ°å€ã€ç«¯å£å·åŠè·¯å¾„è°ƒç”¨HTTP Getæ–¹æ³•ï¼Œå¦‚æœå“åº”çš„çŠ¶æ€ç å¤§äºç­‰äº200ä¸”å°äºç­‰äº400ï¼Œåˆ™è®¤ä¸ºå®¹å™¨å¥åº·ã€‚\napiVersion: v1 kind: Pod metadata: name: pod-with-healthcheck spec: containers: - name: nginx image: nginx ports: - containnerPort: 80 readinessProbe: httpGet: path: /_status/healthz port: 80 scheme: HTTP initialDelaySeconds: 1 periodSeconds: 5 timeoutSeconds: 5 3.2.2. TCPSocketAction é€šè¿‡å®¹å™¨IPåœ°å€å’Œç«¯å£å·æ‰§è¡ŒTCPæ£€æŸ¥ï¼Œå¦‚æœèƒ½å¤Ÿå»ºç«‹TCPè¿æ¥ï¼Œåˆ™è¡¨æ˜å®¹å™¨å¥åº·ã€‚\napiVersion: v1 kind: Pod metadata: name: pod-with-healthcheck spec: containers: - name: nginx image: nginx ports: - containnerPort: 80 readinessProbe: tcpSocket: port: 80 initialDelaySeconds: 1 timeoutSeconds: 5 3.2.3. ExecAction åœ¨ä¸€ä¸ªå®¹å™¨å†…éƒ¨æ‰§è¡Œä¸€ä¸ªå‘½ä»¤ï¼Œå¦‚æœè¯¥å‘½ä»¤çŠ¶æ€è¿”å›å€¼ä¸º0ï¼Œåˆ™è¡¨æ˜å®¹å™¨å¥åº·ã€‚\napiVersion: v1 kind: Pod metadata: name: readiness-exec spec: containers: - name: readiness image: tomcagcr.io/google_containers/busybox args: - /bin/sh - -c - echo ok \u003e /tmp/health;sleep 10;rm -fr /tmp/health;sleep 600 readinessreadinessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 1 timeoutSeconds: 5 4. æ¢é’ˆç›¸å…³æºç  æ¢é’ˆé…ç½®åœ¨podçš„containerç»“æ„ä½“ä¸‹ï¼š\n// å­˜æ´»æ¢é’ˆ LivenessProbe *Probe `json:\"livenessProbe,omitempty\" protobuf:\"bytes,10,opt,name=livenessProbe\"` // å°±ç»ªæ¢é’ˆ ReadinessProbe *Probe `json:\"readinessProbe,omitempty\" protobuf:\"bytes,11,opt,name=readinessProbe\"` 4.1. Probeæºç  type Probe struct { // The action taken to determine the health of a container ProbeHandler `json:\",inline\" protobuf:\"bytes,1,opt,name=handler\"` // Number of seconds after the container has started before liveness probes are initiated. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes // +optional InitialDelaySeconds int32 `json:\"initialDelaySeconds,omitempty\" protobuf:\"varint,2,opt,name=initialDelaySeconds\"` // Number of seconds after which the probe times out. // Defaults to 1 second. Minimum value is 1. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes // +optional TimeoutSeconds int32 `json:\"timeoutSeconds,omitempty\" protobuf:\"varint,3,opt,name=timeoutSeconds\"` // How often (in seconds) to perform the probe. // Default to 10 seconds. Minimum value is 1. // +optional PeriodSeconds int32 `json:\"periodSeconds,omitempty\" protobuf:\"varint,4,opt,name=periodSeconds\"` // Minimum consecutive successes for the probe to be considered successful after having failed. // Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1. // +optional SuccessThreshold int32 `json:\"successThreshold,omitempty\" protobuf:\"varint,5,opt,name=successThreshold\"` // Minimum consecutive failures for the probe to be considered failed after having succeeded. // Defaults to 3. Minimum value is 1. // +optional FailureThreshold int32 `json:\"failureThreshold,omitempty\" protobuf:\"varint,6,opt,name=failureThreshold\"` // Optional duration in seconds the pod needs to terminate gracefully upon probe failure. // The grace period is the duration in seconds after the processes running in the pod are sent // a termination signal and the time when the processes are forcibly halted with a kill signal. // Set this value longer than the expected cleanup time for your process. // If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this // value overrides the value provided by the pod spec. // Value must be non-negative integer. The value zero indicates stop immediately via // the kill signal (no opportunity to shut down). // This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate. // Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset. // +optional TerminationGracePeriodSeconds *int64 `json:\"terminationGracePeriodSeconds,omitempty\" protobuf:\"varint,7,opt,name=terminationGracePeriodSeconds\"` } 4.2. ProbeHandleræºç  // ProbeHandler defines a specific action that should be taken in a probe. // One and only one of the fields must be specified. type ProbeHandler struct { // Exec specifies the action to take. // +optional Exec *ExecAction `json:\"exec,omitempty\" protobuf:\"bytes,1,opt,name=exec\"` // HTTPGet specifies the http request to perform. // +optional HTTPGet *HTTPGetAction `json:\"httpGet,omitempty\" protobuf:\"bytes,2,opt,name=httpGet\"` // TCPSocket specifies an action involving a TCP port. // +optional TCPSocket *TCPSocketAction `json:\"tcpSocket,omitempty\" protobuf:\"bytes,3,opt,name=tcpSocket\"` // GRPC specifies an action involving a GRPC port. // This is a beta field and requires enabling GRPCContainerProbe feature gate. // +featureGate=GRPCContainerProbe // +optional GRPC *GRPCAction `json:\"grpc,omitempty\" protobuf:\"bytes,4,opt,name=grpc\"` } 4.3. ProbeAction 4.3.1. HTTPGetAction // HTTPHeader describes a custom header to be used in HTTP probes type HTTPHeader struct { // The header field name Name string `json:\"name\" protobuf:\"bytes,1,opt,name=name\"` // The header field value Value string `json:\"value\" protobuf:\"bytes,2,opt,name=value\"` } // HTTPGetAction describes an action based on HTTP Get requests. type HTTPGetAction struct { // Path to access on the HTTP server. // +optional Path string `json:\"path,omitempty\" protobuf:\"bytes,1,opt,name=path\"` // Name or number of the port to access on the container. // Number must be in the range 1 to 65534. // Name must be an IANA_SVC_NAME. Port intstr.IntOrString `json:\"port\" protobuf:\"bytes,2,opt,name=port\"` // Host name to connect to, defaults to the pod IP. You probably want to set // \"Host\" in httpHeaders instead. // +optional Host string `json:\"host,omitempty\" protobuf:\"bytes,3,opt,name=host\"` // Scheme to use for connecting to the host. // Defaults to HTTP. // +optional Scheme URIScheme `json:\"scheme,omitempty\" protobuf:\"bytes,4,opt,name=scheme,casttype=URIScheme\"` // Custom headers to set in the request. HTTP allows repeated headers. // +optional HTTPHeaders []HTTPHeader `json:\"httpHeaders,omitempty\" protobuf:\"bytes,5,rep,name=httpHeaders\"` } // URIScheme identifies the scheme used for connection to a host for Get actions // +enum type URIScheme string const ( // URISchemeHTTP means that the scheme used will be http:// URISchemeHTTP URIScheme = \"HTTP\" // URISchemeHTTPS means that the scheme used will be https:// URISchemeHTTPS URIScheme = \"HTTPS\" ) 4.3.2. TCPSocketAction // TCPSocketAction describes an action based on opening a socket type TCPSocketAction struct { // Number or name of the port to access on the container. // Number must be in the range 1 to 65534. // Name must be an IANA_SVC_NAME. Port intstr.IntOrString `json:\"port\" protobuf:\"bytes,1,opt,name=port\"` // Optional: Host name to connect to, defaults to the pod IP. // +optional Host string `json:\"host,omitempty\" protobuf:\"bytes,2,opt,name=host\"` } 4.3.3. ExecAction // ExecAction describes a \"run in container\" action. type ExecAction struct { // Command is the command line to execute inside the container, the working directory for the // command is root ('/') in the container's filesystem. The command is simply exec'd, it is // not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use // a shell, you need to explicitly call out to that shell. // Exit status of 0 is treated as live/healthy and non-zero is unhealthy. // +optional Command []string `json:\"command,omitempty\" protobuf:\"bytes,1,rep,name=command\"` } 4.3.4. GRPCAction type GRPCAction struct { // Port number of the gRPC service. Number must be in the range 1 to 65534. Port int32 `json:\"port\" protobuf:\"bytes,1,opt,name=port\"` // Service is the name of the service to place in the gRPC HealthCheckRequest // (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md). // // If this is not specified, the default behavior is defined by gRPC. // +optional // +default=\"\" Service *string `json:\"service\" protobuf:\"bytes,2,opt,name=service\"` } å‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ é…ç½®å­˜æ´»ã€å°±ç»ªå’Œå¯åŠ¨æ¢é’ˆ | Kubernetes https://mp.weixin.qq.com/s/ApD8D0_UAPftUjw-0Txcyw ã€ŠKubernetesæƒå¨æŒ‡å—ã€‹ ","categories":"","description":"","excerpt":"Podå¥åº·æ£€æŸ¥ Podçš„å¥åº·çŠ¶æ€ç”±ä¸¤ç±»æ¢é’ˆæ¥æ£€æŸ¥ï¼šLivenessProbeå’ŒReadinessProbeã€‚\n1. æ¢é’ˆç±»å‹ 1. â€¦","ref":"/kubernetes-notes/concepts/pod/pod-probe/","tags":["Pod"],"title":"Podå¥åº·æ£€æŸ¥"},{"body":"1. ETCDèµ„æºç±»å‹ There are three types of resources in etcd\npermission resources: users and roles in the user store key-value resources: key-value pairs in the key-value store settings resources: security settings, auth settings, and dynamic etcd cluster settings (election/heartbeat) 2. æƒé™èµ„æº Usersï¼šuserç”¨æ¥è®¾ç½®èº«ä»½è®¤è¯ï¼ˆuserï¼špasswdï¼‰ï¼Œä¸€ä¸ªç”¨æˆ·å¯ä»¥æ‹¥æœ‰å¤šä¸ªè§’è‰²ï¼Œæ¯ä¸ªè§’è‰²è¢«åˆ†é…ä¸€å®šçš„æƒé™ï¼ˆåªè¯»ã€åªå†™ã€å¯è¯»å†™ï¼‰ï¼Œç”¨æˆ·åˆ†ä¸ºrootç”¨æˆ·å’Œérootç”¨æˆ·ã€‚\nRolesï¼šè§’è‰²ç”¨æ¥å…³è”æƒé™ï¼Œè§’è‰²ä¸»è¦ä¸‰ç±»ï¼šrootè§’è‰²ã€‚é»˜è®¤åˆ›å»ºrootç”¨æˆ·æ—¶å³åˆ›å»ºäº†rootè§’è‰²ï¼Œè¯¥è§’è‰²æ‹¥æœ‰æ‰€æœ‰æƒé™ï¼›guestè§’è‰²ï¼Œé»˜è®¤è‡ªåŠ¨åˆ›å»ºï¼Œä¸»è¦ç”¨äºéè®¤è¯ä½¿ç”¨ã€‚æ™®é€šè§’è‰²ï¼Œç”±rootç”¨æˆ·åˆ›å»ºè§’è‰²ï¼Œå¹¶åˆ†é…æŒ‡å®šæƒé™ã€‚\næ³¨æ„ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•éªŒè¯æ–¹å¼ï¼Œå³æ²¡æ˜¾ç¤ºæŒ‡å®šä»¥ä»€ä¹ˆç”¨æˆ·è¿›è¡Œè®¿é—®ï¼Œé‚£ä¹ˆé»˜è®¤ä¼šè®¾å®šä¸º guest è§’è‰²ã€‚é»˜è®¤æƒ…å†µä¸‹ guest ä¹Ÿæ˜¯å…·æœ‰å…¨å±€è®¿é—®æƒé™çš„ã€‚å¦‚æœä¸å¸Œæœ›æœªæˆæƒå°±è·å–æˆ–ä¿®æ”¹etcdçš„æ•°æ®ï¼Œåˆ™å¯æ”¶å›guestè§’è‰²çš„æƒé™æˆ–åˆ é™¤è¯¥è§’è‰²ï¼Œetcdctl role revoke ã€‚\nPermissions:æƒé™åˆ†ä¸ºåªè¯»ã€åªå†™ã€å¯è¯»å†™ä¸‰ç§æƒé™ï¼Œæƒé™å³å¯¹æŒ‡å®šç›®å½•æˆ–keyçš„è¯»å†™æƒé™ã€‚\n3. ETCDè®¿é—®æ§åˆ¶ 3.1. è®¿é—®æ§åˆ¶ç›¸å…³å‘½ä»¤ NAME: etcdctl - A simple command line client for etcd. USAGE: etcdctl [global options] command [command options] [arguments...] VERSION: 2.2.0 COMMANDS: user user add, grant and revoke subcommands role role add, grant and revoke subcommands auth overall auth controls GLOBAL OPTIONS: --peers, -C a comma-delimited list of machine addresses in the cluster (default: \"http://127.0.0.1:4001,http://127.0.0.1:2379\") --endpoint a comma-delimited list of machine addresses in the cluster (default: \"http://127.0.0.1:4001,http://127.0.0.1:2379\") --cert-file identify HTTPS client using this SSL certificate file --key-file identify HTTPS client using this SSL key file --ca-file verify certificates of HTTPS-enabled servers using this CA bundle --username, -u provide username[:password] and prompt if password is not supplied. --timeout '1s' connection timeout per request 3.2. userç›¸å…³å‘½ä»¤ [root@localhost etcd]# etcdctl user --help NAME: etcdctl user - user add, grant and revoke subcommands USAGE: etcdctl user command [command options] [arguments...] COMMANDS: add add a new user for the etcd cluster get get details for a user list list all current users remove remove a user for the etcd cluster grant grant roles to an etcd user revoke revoke roles for an etcd user passwd change password for a user help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.2.1. æ·»åŠ rootç”¨æˆ·å¹¶è®¾ç½®å¯†ç  etcdctl --endpoints http://172.16.22.36:2379 user add root\n3.2.2. æ·»åŠ érootç”¨æˆ·å¹¶è®¾ç½®å¯†ç  etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user add huwh\n3.2.3. æŸ¥çœ‹å½“å‰æ‰€æœ‰ç”¨æˆ· etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user list\n3.2.4. å°†ç”¨æˆ·æ·»åŠ åˆ°å¯¹åº”è§’è‰² etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user grant --roles test1 phpor\n3.2.5. æŸ¥çœ‹ç”¨æˆ·æ‹¥æœ‰å“ªäº›è§’è‰² etcdctl --endpoints http://172.16.22.36:2379 --username root:123 user get phpor\n3.3. roleç›¸å…³å‘½ä»¤ [root@localhost etcd]# etcdctl role --help NAME: etcdctl role - role add, grant and revoke subcommands USAGE: etcdctl role command [command options] [arguments...] COMMANDS: add add a new role for the etcd cluster get get details for a role list list all roles remove remove a role from the etcd cluster grant grant path matches to an etcd role revoke revoke path matches for an etcd role help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.3.1. æ·»åŠ è§’è‰² etcdctl --endpoints http://172.16.22.36:2379 --username root:2379 role add test1\n3.3.2. æŸ¥çœ‹æ‰€æœ‰è§’è‰² etcdctl --endpoints http://172.16.22.36:2379 --username root:123 role list\n3.3.3. ç»™è§’è‰²åˆ†é…æƒé™ [root@localhost etcd]# etcdctl role grant --help NAME: grant - grant path matches to an etcd role USAGE: command grant [command options] [arguments...] OPTIONS: --path Path granted for the role to access --read Grant read-only access --write Grant write-only access --readwrite Grant read-write access 1ã€åªåŒ…å«ç›®å½• etcdctl --endpoints http://172.16.22.36:2379 --username root:123 role grant --readwrite --path /test1 test1\n2ã€åŒ…æ‹¬ç›®å½•å’Œå­ç›®å½•æˆ–æ–‡ä»¶ etcdctl --endpoints http://172.16.22.36:2379 --username root:123 role grant --readwrite --path /test1/* test1\n3.3.4. æŸ¥çœ‹è§’è‰²æ‰€æ‹¥æœ‰çš„æƒé™ etcdctl --endpoints http://172.16.22.36:2379 --username root:2379 role get test1\n3.4. authç›¸å…³æ“ä½œ [root@localhost etcd]# etcdctl auth --help NAME: etcdctl auth - overall auth controls USAGE: etcdctl auth command [command options] [arguments...] COMMANDS: enable enable auth access controls disable disable auth access controls help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.4.1. å¼€å¯è®¤è¯ etcdctl --endpoints http://172.16.22.36:2379 auth enable\n4. è®¿é—®æ§åˆ¶è®¾ç½®æ­¥éª¤ é¡ºåº æ­¥éª¤ å‘½ä»¤ 1 æ·»åŠ rootç”¨æˆ· etcdctl --endpoints http://: user add root 2 å¼€å¯è®¤è¯ etcdctl --endpoints http://: auth enable 3 æ·»åŠ érootç”¨æˆ· etcdctl --endpoints http://: â€“username root: user add 4 æ·»åŠ è§’è‰² etcdctl --endpoints http://: â€“username root: role add 5 ç»™è§’è‰²æˆæƒï¼ˆåªè¯»ã€åªå†™ã€å¯è¯»å†™ï¼‰ etcdctl --endpoints http://: â€“username root: role grant --readwrite --path 6 ç»™ç”¨æˆ·åˆ†é…è§’è‰²ï¼ˆå³åˆ†é…äº†è§’è‰²å¯¹åº”çš„æƒé™ï¼‰ etcdctl --endpoints http://: â€“username root: user grant --roles 5. è®¿é—®è®¤è¯çš„APIè°ƒç”¨ æ›´å¤šå‚è€ƒ\nhttps://coreos.com/etcd/docs/latest/v2/auth_api.html\nhttps://coreos.com/etcd/docs/latest/v2/authentication.html\n","categories":"","description":"","excerpt":"1. ETCDèµ„æºç±»å‹ There are three types of resources in etcd\npermission â€¦","ref":"/kubernetes-notes/etcd/etcd-auth-and-security/","tags":["Etcd"],"title":"Etcdè®¿é—®æ§åˆ¶"},{"body":"1. åŸºæœ¬æ¦‚å¿µ 1.1. image layerï¼ˆé•œåƒå±‚ï¼‰ é•œåƒå¯ä»¥çœ‹æˆæ˜¯ç”±å¤šä¸ªé•œåƒå±‚å åŠ èµ·æ¥çš„ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œé•œåƒå±‚ä¹Ÿå¯ä»¥ç®€å•ç†è§£ä¸ºä¸€ä¸ªåŸºæœ¬çš„é•œåƒï¼Œè€Œæ¯ä¸ªé•œåƒå±‚ä¹‹é—´é€šè¿‡æŒ‡é’ˆçš„å½¢å¼è¿›è¡Œå åŠ ã€‚\næ ¹æ®ä¸Šå›¾ï¼Œé•œåƒå±‚çš„ä¸»è¦ç»„æˆéƒ¨åˆ†åŒ…æ‹¬é•œåƒå±‚idï¼Œé•œåƒå±‚æŒ‡é’ˆã€æŒ‡å‘çˆ¶å±‚ã€‘ï¼Œå…ƒæ•°æ®ã€layer metadataã€‘åŒ…å«äº†dockeræ„å»ºå’Œè¿è¡Œçš„ä¿¡æ¯è¿˜æœ‰çˆ¶å±‚çš„å±‚æ¬¡ä¿¡æ¯ã€‚\nåªè¯»å±‚å’Œè¯»å†™å±‚ã€top layerã€‘çš„ç»„æˆéƒ¨åˆ†åŸºæœ¬ä¸€è‡´ã€‚åŒæ—¶è¯»å†™å±‚å¯ä»¥è½¬æ¢æˆåªè¯»å±‚ã€docker commitæ“ä½œå®ç°ã€‘\n1.2. imageï¼ˆé•œåƒï¼‰---ã€åªè¯»å±‚çš„é›†åˆã€‘ 1ã€é•œåƒæ˜¯ä¸€å †åªè¯»å±‚çš„ç»Ÿä¸€è§†è§’ï¼Œé™¤äº†æœ€åº•å±‚æ²¡æœ‰æŒ‡å‘å¤–ï¼Œæ¯ä¸€å±‚éƒ½æŒ‡å‘å®ƒçš„çˆ¶å±‚ï¼Œç»Ÿä¸€æ–‡ä»¶ç³»ç»Ÿï¼ˆunion file systemï¼‰æŠ€æœ¯èƒ½å¤Ÿå°†ä¸åŒçš„å±‚æ•´åˆæˆä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œä¸ºè¿™äº›å±‚æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§’ï¼Œè¿™æ ·å°±éšè—äº†å¤šå±‚çš„å­˜åœ¨ï¼Œåœ¨ç”¨æˆ·çš„è§’åº¦çœ‹æ¥ï¼Œåªå­˜åœ¨ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿã€‚è€Œæ¯ä¸€å±‚éƒ½æ˜¯ä¸å¯å†™çš„ï¼Œå°±æ˜¯åªè¯»å±‚ã€‚\n1.3. containerï¼ˆå®¹å™¨ï¼‰---ã€ä¸€å±‚è¯»å†™å±‚+å¤šå±‚åªè¯»å±‚ã€‘ 1ã€å®¹å™¨å’Œé•œåƒçš„åŒºåˆ«åœ¨äºå®¹å™¨çš„æœ€ä¸Šé¢ä¸€å±‚æ˜¯è¯»å†™å±‚ã€top layerã€‘ï¼Œè€Œè¿™è¾¹å¹¶æ²¡æœ‰åŒºåˆ†å®¹å™¨æ˜¯å¦åœ¨è¿è¡Œã€‚è¿è¡ŒçŠ¶æ€çš„å®¹å™¨ã€running containerã€‘å³ä¸€ä¸ªå¯è¯»å†™çš„æ–‡ä»¶ç³»ç»Ÿã€é™æ€å®¹å™¨ã€‘+éš”ç¦»çš„è¿›ç¨‹ç©ºé—´å’Œå…¶ä¸­çš„è¿›ç¨‹ã€‚\néš”ç¦»çš„è¿›ç¨‹ç©ºé—´ä¸­çš„è¿›ç¨‹å¯ä»¥å¯¹è¯¥è¯»å†™å±‚è¿›è¡Œå¢åˆ æ”¹ï¼Œå…¶è¿è¡ŒçŠ¶æ€å®¹å™¨çš„è¿›ç¨‹æ“ä½œéƒ½ä½œç”¨åœ¨è¯¥è¯»å†™å±‚ä¸Šã€‚æ¯ä¸ªå®¹å™¨åªèƒ½æœ‰ä¸€ä¸ªè¿›ç¨‹éš”ç¦»ç©ºé—´ã€‚\n2. Dockerå¸¸ç”¨å‘½ä»¤åŸç†å›¾æ¦‚è§ˆï¼š 3. Dockerå¸¸ç”¨å‘½ä»¤è¯´æ˜ 3.1. æ ‡è¯†è¯´æ˜ 3.1.1. image---ï¼ˆç»Ÿä¸€åªè¯»æ–‡ä»¶ç³»ç»Ÿï¼‰ 3.1.2. é™æ€å®¹å™¨ã€æœªè¿è¡Œçš„å®¹å™¨ã€‘---ï¼ˆç»Ÿä¸€å¯è¯»å†™æ–‡ä»¶ç³»ç»Ÿï¼‰ 3.1.3. åŠ¨æ€å®¹å™¨ã€running containerã€‘---ï¼ˆè¿›ç¨‹ç©ºé—´ï¼ˆåŒ…æ‹¬è¿›ç¨‹ï¼‰+ç»Ÿä¸€å¯è¯»å†™æ–‡ä»¶ç³»ç»Ÿï¼‰ 3.2. å‘½ä»¤è¯´æ˜ 3.2.1. dockerç”Ÿå‘½å‘¨æœŸç›¸å…³å‘½ä»¤: 3.2.1.1. docker create {image-id} å³ä¸ºåªè¯»æ–‡ä»¶ç³»ç»Ÿæ·»åŠ ä¸€å±‚å¯è¯»å†™å±‚ã€top layerã€‘ï¼Œç”Ÿæˆå¯è¯»å†™æ–‡ä»¶ç³»ç»Ÿï¼Œè¯¥å‘½ä»¤çŠ¶æ€ä¸‹å®¹å™¨ä¸ºé™æ€å®¹å™¨ï¼Œå¹¶æ²¡æœ‰è¿è¡Œã€‚\n3.2.1.2. docker startï¼ˆrestartï¼‰ {container-id} docker stopå³ä¸ºdocker startçš„é€†è¿‡ç¨‹\nå³ä¸ºå¯è¯»å†™æ–‡ä»¶ç³»ç»Ÿæ·»åŠ ä¸€ä¸ªè¿›ç¨‹ç©ºé—´ã€åŒ…æ‹¬è¿›ç¨‹ã€‘ï¼Œç”ŸæˆåŠ¨æ€å®¹å™¨ã€running containerã€‘\n3.2.1.3. docker run {image-id} docker run=docker create+docker start\nç±»ä¼¼æµç¨‹å¦‚ä¸‹ ï¼š\n3.2.1.4. docker stop {container-id} å‘è¿è¡Œçš„å®¹å™¨ä¸­å‘ä¸€ä¸ªSIGTERMçš„ä¿¡å·ï¼Œç„¶ååœæ­¢æ‰€æœ‰çš„è¿›ç¨‹ã€‚å³ä¸ºdocker startçš„é€†è¿‡ç¨‹ã€‚\n3.2.1.5. docker kill {container-id} docker killå‘å®¹å™¨å‘é€ä¸å‹å¥½çš„SIGKILLçš„ä¿¡å·ï¼Œç›¸å½“äºå¿«é€Ÿå¼ºåˆ¶å…³é—­å®¹å™¨ï¼Œä¸docker stopçš„åŒºåˆ«åœ¨äºdocker stopæ˜¯æ­£å¸¸å…³é—­ï¼Œå…ˆå‘SIGTERMä¿¡å·ï¼Œæ¸…ç†è¿›ç¨‹ï¼Œå†å‘SIGKILLä¿¡å·é€€å‡ºã€‚\n3.2.1.6. docker pause {container-id} docker unpauseä¸ºé€†è¿‡ç¨‹---æ¯”è¾ƒå°‘ä½¿ç”¨\næš‚åœå®¹å™¨ä¸­çš„æ‰€æœ‰è¿›ç¨‹ï¼Œä½¿ç”¨cgroupçš„freezeré¡ºåºæš‚åœå®¹å™¨é‡Œçš„æ‰€æœ‰è¿›ç¨‹ï¼Œdocker unpauseä¸ºé€†è¿‡ç¨‹å³æ¢å¤æ‰€æœ‰è¿›ç¨‹ã€‚æ¯”è¾ƒå°‘ä½¿ç”¨ã€‚\n3.2.1.7. docker commit {container-id} æŠŠå®¹å™¨çš„å¯è¯»å†™å±‚è½¬åŒ–æˆåªè¯»å±‚ï¼Œå³ä»å®¹å™¨çŠ¶æ€ã€å¯è¯»å†™æ–‡ä»¶ç³»ç»Ÿã€‘å˜ä¸ºé•œåƒçŠ¶æ€ã€åªè¯»æ–‡ä»¶ç³»ç»Ÿã€‘ï¼Œå¯ç†è§£ä¸ºã€å›ºåŒ–ã€‘ã€‚\n3.2.1.8. docker build docker build=docker runã€è¿è¡Œå®¹å™¨ã€‘+ã€è¿›ç¨‹ä¿®æ”¹æ•°æ®ã€‘+docker commitã€å›ºåŒ–æ•°æ®ã€‘ï¼Œä¸æ–­å¾ªç¯ç›´è‡³ç”Ÿæˆæ‰€éœ€é•œåƒã€‚\nå¾ªç¯ä¸€æ¬¡ä¾¿ä¼šå½¢æˆæ–°çš„å±‚ï¼ˆé•œåƒï¼‰ã€åŸé•œåƒå±‚+å·²å›ºåŒ–çš„å¯è¯»å†™å±‚ã€‘\ndocker build ä¸€èˆ¬ä½œç”¨åœ¨dockerfileæ–‡ä»¶ä¸Šã€‚\n3.2.2. dockeræŸ¥è¯¢ç±»å‘½ä»¤ æŸ¥è¯¢å¯¹è±¡ï¼šâ‘ imageï¼Œâ‘¡containerï¼Œâ‘¢image/containerä¸­çš„æ•°æ®ï¼Œâ‘£ç³»ç»Ÿä¿¡æ¯[å®¹å™¨æ•°ï¼Œé•œåƒæ•°åŠå…¶ä»–]\n3.2.2.1. Image 1ã€docker images docker images åˆ—å‡ºå½“å‰é•œåƒã€ä»¥é¡¶å±‚é•œåƒidæ¥è¡¨ç¤ºæ•´ä¸ªå®Œæ•´é•œåƒã€‘ï¼Œæ¯ä¸ªé¡¶å±‚é•œåƒä¸‹é¢éšè—å¤šä¸ªé•œåƒå±‚ã€‚\n2ã€docker images -a docker images -aåˆ—å‡ºæ‰€æœ‰é•œåƒå±‚ã€æ’åºä»¥æ¯ä¸ªé¡¶å±‚é•œåƒidä¸ºé¦–åæ¥è¯¥é•œåƒä¸‹çš„æ‰€æœ‰é•œåƒå±‚ã€‘ï¼Œä¾æ¬¡åˆ—å‡ºæ¯ä¸ªé•œåƒçš„æ‰€æœ‰é•œåƒå±‚ã€‚\n3ã€docker history {image-id} docker history åˆ—å‡ºè¯¥é•œåƒidä¸‹çš„æ‰€æœ‰å†å²é•œåƒã€‚\n3.2.2.2. Container 1ã€docker ps åˆ—å‡ºæ‰€æœ‰è¿è¡Œçš„å®¹å™¨ã€running containerã€‘\n2ã€docker ps -a åˆ—å‡ºæ‰€æœ‰å®¹å™¨ï¼ŒåŒ…æ‹¬é™æ€å®¹å™¨ã€æœªè¿è¡Œçš„å®¹å™¨ã€‘å’ŒåŠ¨æ€å®¹å™¨ã€running containerã€‘\n3.2.2.3. Info 1ã€docker inspect {container-id} or {image-id} æå–å‡ºå®¹å™¨æˆ–é•œåƒæœ€é¡¶å±‚çš„å…ƒæ•°æ®ã€‚\n2ã€docker info æ˜¾ç¤º Docker ç³»ç»Ÿä¿¡æ¯ï¼ŒåŒ…æ‹¬é•œåƒå’Œå®¹å™¨æ•°ã€‚\n3.2.3. dockeræ“ä½œç±»å‘½ä»¤ï¼š 3.2.3.1. docker rm {container-id} docker rmä¼šç§»é™¤é•œåƒï¼Œè¯¥å‘½ä»¤åªèƒ½å¯¹é™æ€å®¹å™¨ã€éè¿è¡ŒçŠ¶æ€ã€‘è¿›è¡Œæ“ä½œã€‚\né€šè¿‡docker rm -f {container-id}çš„-f ï¼ˆforceï¼‰å‚æ•°å¯ä»¥å¼ºåˆ¶åˆ é™¤è¿è¡ŒçŠ¶æ€çš„å®¹å™¨ã€running containerã€‘ã€‚\n3.2.3.2. docker rmi {image-id} 3.2.3.3. docker exec {running-container-id} docker execä¼šåœ¨è¿è¡ŒçŠ¶æ€çš„å®¹å™¨ä¸­æ‰§è¡Œä¸€ä¸ªæ–°çš„è¿›ç¨‹ã€‚\n3.2.3.4. docker export {container-id} docker exportå‘½ä»¤åˆ›å»ºä¸€ä¸ªtaræ–‡ä»¶ï¼Œå¹¶ä¸”ç§»é™¤äº†å…ƒæ•°æ®å’Œä¸å¿…è¦çš„å±‚ï¼Œå°†å¤šä¸ªå±‚æ•´åˆæˆäº†ä¸€ä¸ªå±‚ï¼Œåªä¿å­˜äº†å½“å‰ç»Ÿä¸€è§†è§’çœ‹åˆ°çš„å†…å®¹ã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttp://merrigrove.blogspot.com/2015/10/visualizing-docker-containers-and-images.html ","categories":"","description":"","excerpt":"1. åŸºæœ¬æ¦‚å¿µ 1.1. image layerï¼ˆé•œåƒå±‚ï¼‰ é•œåƒå¯ä»¥çœ‹æˆæ˜¯ç”±å¤šä¸ªé•œåƒå±‚å åŠ èµ·æ¥çš„ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œé•œåƒå±‚ä¹Ÿå¯ä»¥ç®€å•ç†è§£ä¸ºä¸€ä¸ªåŸºæœ¬ â€¦","ref":"/kubernetes-notes/runtime/docker/docker-commands-principle/","tags":["Docker"],"title":"Dockerå¸¸ç”¨å‘½ä»¤åŸç†å›¾"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/flannel/","tags":"","title":"Flannel"},{"body":"1. FlexVolumeä»‹ç» Flexvolumeæä¾›äº†ä¸€ç§æ‰©å±•k8så­˜å‚¨æ’ä»¶çš„æ–¹å¼ï¼Œç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰è‡ªå·±çš„å­˜å‚¨æ’ä»¶ã€‚ç±»ä¼¼çš„åŠŸèƒ½çš„å®ç°è¿˜æœ‰CSIçš„æ–¹å¼ã€‚Flexvolumeåœ¨k8s 1.8+ä»¥ä¸Šç‰ˆæœ¬æä¾›GAåŠŸèƒ½ç‰ˆæœ¬ã€‚\n2. ä½¿ç”¨æ–¹å¼ åœ¨æ¯ä¸ªnodeèŠ‚ç‚¹å®‰è£…å­˜å‚¨æ’ä»¶äºŒè¿›åˆ¶ï¼Œè¯¥äºŒè¿›åˆ¶å®ç°flexvolumeçš„ç›¸å…³æ¥å£ï¼Œé»˜è®¤å­˜å‚¨æ’ä»¶çš„å­˜æ”¾è·¯å¾„ä¸º/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\u003cvendor~driver\u003e/\u003cdriver\u003eã€‚\nå…¶ä¸­vendor~driverçš„åå­—éœ€è¦å’Œpodä¸­flexVolume.driverçš„å­—æ®µåå­—åŒ¹é…ï¼Œè¯¥å­—æ®µåå­—é€šè¿‡/æ›¿æ¢~ã€‚\nä¾‹å¦‚ï¼š\npath:/usr/libexec/kubernetes/kubelet-plugins/volume/exec/foo~cifs/cifs\npodä¸­flexVolume.driver:foo/cifs\n3. FlexVolumeæ¥å£ èŠ‚ç‚¹ä¸Šçš„å­˜å‚¨æ’ä»¶éœ€è¦å®ç°ä»¥ä¸‹çš„æ¥å£ã€‚\n3.1. init \u003cdriver executable\u003e init 3.2. attach \u003cdriver executable\u003e attach \u003cjson options\u003e \u003cnode name\u003e 3.3. detach \u003cdriver executable\u003e detach \u003cmount device\u003e \u003cnode name\u003e 3.4. waitforattach \u003cdriver executable\u003e waitforattach \u003cmount device\u003e \u003cjson options\u003e 3.5. isattached \u003cdriver executable\u003e isattached \u003cjson options\u003e \u003cnode name\u003e 3.6. mountdevice \u003cdriver executable\u003e mountdevice \u003cmount dir\u003e \u003cmount device\u003e \u003cjson options\u003e 3.7. unmountdevice \u003cdriver executable\u003e unmountdevice \u003cmount device\u003e 3.8. mount \u003cdriver executable\u003e mount \u003cmount dir\u003e \u003cjson options\u003e 3.9. unmount \u003cdriver executable\u003e unmount \u003cmount dir\u003e 3.10. æ’ä»¶è¾“å‡º { \"status\": \"\u003cSuccess/Failure/Not supported\u003e\", \"message\": \"\u003cReason for success/failure\u003e\", \"device\": \"\u003cPath to the device attached. This field is valid only for attach \u0026 waitforattach call-outs\u003e\" \"volumeName\": \"\u003cCluster wide unique name of the volume. Valid only for getvolumename call-out\u003e\" \"attached\": \u003cTrue/False (Return true if volume is attached on the node. Valid only for isattached call-out)\u003e \"capabilities\": \u003cOnly included as part of the Init response\u003e { \"attach\": \u003cTrue/False (Return true if the driver implements attach and detach)\u003e } } 4. ç¤ºä¾‹ 4.1. podçš„yamlæ–‡ä»¶å†…å®¹ nginx-nfs.yaml\nç›¸å…³å‚æ•°ä¸ºflexVolume.driverç­‰ã€‚\napiVersion: v1 kind: Pod metadata: name: nginx-nfs namespace: default spec: containers: - name: nginx-nfs image: nginx volumeMounts: - name: test mountPath: /data ports: - containerPort: 80 volumes: - name: test flexVolume: driver: \"k8s/nfs\" fsType: \"nfs\" options: server: \"172.16.0.25\" share: \"dws_nas_scratch\" 4.2. æ’ä»¶è„šæœ¬ nfsè„šæœ¬å®ç°äº†flexvolumeçš„æ¥å£ã€‚\n/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfsã€‚\n#!/bin/bash # Copyright 2015 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Notes: # - Please install \"jq\" package before using this driver. usage() { err \"Invalid usage. Usage: \" err \"\\t$0 init\" err \"\\t$0 mount \u003cmount dir\u003e \u003cjson params\u003e\" err \"\\t$0 unmount \u003cmount dir\u003e\" exit 1 } err() { echo -ne $* 1\u003e\u00262 } log() { echo -ne $* \u003e\u00261 } ismounted() { MOUNT=`findmnt -n ${MNTPATH} 2\u003e/dev/null | cut -d' ' -f1` if [ \"${MOUNT}\" == \"${MNTPATH}\" ]; then echo \"1\" else echo \"0\" fi } domount() { MNTPATH=$1 NFS_SERVER=$(echo $2 | jq -r '.server') SHARE=$(echo $2 | jq -r '.share') if [ $(ismounted) -eq 1 ] ; then log '{\"status\": \"Success\"}' exit 0 fi mkdir -p ${MNTPATH} \u0026\u003e /dev/null mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} \u0026\u003e /dev/null if [ $? -ne 0 ]; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\\\"}\" exit 1 fi log '{\"status\": \"Success\"}' exit 0 } unmount() { MNTPATH=$1 if [ $(ismounted) -eq 0 ] ; then log '{\"status\": \"Success\"}' exit 0 fi umount ${MNTPATH} \u0026\u003e /dev/null if [ $? -ne 0 ]; then err \"{ \\\"status\\\": \\\"Failed\\\", \\\"message\\\": \\\"Failed to unmount volume at ${MNTPATH}\\\"}\" exit 1 fi log '{\"status\": \"Success\"}' exit 0 } op=$1 if ! command -v jq \u003e/dev/null 2\u003e\u00261; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"'jq' binary not found. Please install jq package before using this driver\\\"}\" exit 1 fi if [ \"$op\" = \"init\" ]; then log '{\"status\": \"Success\", \"capabilities\": {\"attach\": false}}' exit 0 fi if [ $# -lt 2 ]; then usage fi shift case \"$op\" in mount) domount $* ;; unmount) unmount $* ;; *) log '{\"status\": \"Not supported\"}' exit 0 esac exit 1 å‚è€ƒï¼š\nhttps://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nginx-nfs.yaml https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nfs ","categories":"","description":"","excerpt":"1. FlexVolumeä»‹ç» Flexvolumeæä¾›äº†ä¸€ç§æ‰©å±•k8så­˜å‚¨æ’ä»¶çš„æ–¹å¼ï¼Œç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰è‡ªå·±çš„å­˜å‚¨æ’ä»¶ã€‚ç±»ä¼¼çš„åŠŸèƒ½çš„å®ç°è¿˜ â€¦","ref":"/kubernetes-notes/storage/csi/flexvolume/","tags":["CSI"],"title":"FlexVolumeä»‹ç»"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æk8sä¸­å„ä¸ªæ ¸å¿ƒç»„ä»¶ç»å¸¸ä½¿ç”¨åˆ°çš„Informeræœºåˆ¶(å³List-Watch)ã€‚è¯¥éƒ¨åˆ†çš„ä»£ç ä¸»è¦ä½äºclient-goè¿™ä¸ªç¬¬ä¸‰æ–¹åŒ…ä¸­ã€‚\næ­¤éƒ¨åˆ†çš„é€»è¾‘ä¸»è¦ä½äº/vendor/k8s.io/client-go/tools/cacheåŒ…ä¸­ï¼Œä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\ncache â”œâ”€â”€ controller.go # åŒ…å«ï¼šConfigã€Runã€processLoopã€NewInformerã€NewIndexerInformer â”œâ”€â”€ delta_fifo.go # åŒ…å«ï¼šNewDeltaFIFOã€DeltaFIFOã€AddIfNotPresent â”œâ”€â”€ expiration_cache.go â”œâ”€â”€ expiration_cache_fakes.go â”œâ”€â”€ fake_custom_store.go â”œâ”€â”€ fifo.go # åŒ…å«ï¼šQueueã€FIFOã€NewFIFO â”œâ”€â”€ heap.go â”œâ”€â”€ index.go # åŒ…å«ï¼šIndexerã€MetaNamespaceIndexFunc â”œâ”€â”€ listers.go â”œâ”€â”€ listwatch.go # åŒ…å«ï¼šListerWatcherã€ListWatchã€Listã€Watch â”œâ”€â”€ mutation_cache.go â”œâ”€â”€ mutation_detector.go â”œâ”€â”€ reflector.go # åŒ…å«ï¼šReflectorã€NewReflectorã€Runã€ListAndWatch â”œâ”€â”€ reflector_metrics.go â”œâ”€â”€ shared_informer.go # åŒ…å«ï¼šNewSharedInformerã€WaitForCacheSyncã€Runã€HasSynced â”œâ”€â”€ store.go # åŒ…å«ï¼šStoreã€MetaNamespaceKeyFuncã€SplitMetaNamespaceKey â”œâ”€â”€ testing â”‚Â â”œâ”€â”€ fake_controller_source.go â”œâ”€â”€ thread_safe_store.go # åŒ…å«ï¼šThreadSafeStoreã€threadSafeMap â”œâ”€â”€ undelta_store.go 0. åŸç†ç¤ºæ„å›¾ ç¤ºæ„å›¾1ï¼š\nç¤ºæ„å›¾2ï¼š\n0.1. client-goç»„ä»¶ Reflectorï¼šreflectorç”¨æ¥watchç‰¹å®šçš„k8s APIèµ„æºã€‚å…·ä½“çš„å®ç°æ˜¯é€šè¿‡ListAndWatchçš„æ–¹æ³•ï¼Œwatchå¯ä»¥æ˜¯k8så†…å»ºçš„èµ„æºæˆ–è€…æ˜¯è‡ªå®šä¹‰çš„èµ„æºã€‚å½“reflectoré€šè¿‡watch APIæ¥æ”¶åˆ°æœ‰å…³æ–°èµ„æºå®ä¾‹å­˜åœ¨çš„é€šçŸ¥æ—¶ï¼Œå®ƒä½¿ç”¨ç›¸åº”çš„åˆ—è¡¨APIè·å–æ–°åˆ›å»ºçš„å¯¹è±¡ï¼Œå¹¶å°†å…¶æ”¾å…¥watchHandlerå‡½æ•°å†…çš„Delta Fifoé˜Ÿåˆ—ä¸­ã€‚\nInformerï¼šinformerä»Delta Fifoé˜Ÿåˆ—ä¸­å¼¹å‡ºå¯¹è±¡ã€‚æ‰§è¡Œæ­¤æ“ä½œçš„åŠŸèƒ½æ˜¯processLoopã€‚base controllerçš„ä½œç”¨æ˜¯ä¿å­˜å¯¹è±¡ä»¥ä¾›ä»¥åæ£€ç´¢ï¼Œå¹¶è°ƒç”¨æˆ‘ä»¬çš„æ§åˆ¶å™¨å°†å¯¹è±¡ä¼ é€’ç»™å®ƒã€‚\nIndexerï¼šç´¢å¼•å™¨æä¾›å¯¹è±¡çš„ç´¢å¼•åŠŸèƒ½ã€‚å…¸å‹çš„ç´¢å¼•ç”¨ä¾‹æ˜¯åŸºäºå¯¹è±¡æ ‡ç­¾åˆ›å»ºç´¢å¼•ã€‚ Indexerå¯ä»¥æ ¹æ®å¤šä¸ªç´¢å¼•å‡½æ•°ç»´æŠ¤ç´¢å¼•ã€‚Indexerä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ•°æ®å­˜å‚¨æ¥å­˜å‚¨å¯¹è±¡åŠå…¶é”®ã€‚ åœ¨Storeä¸­å®šä¹‰äº†ä¸€ä¸ªåä¸ºMetaNamespaceKeyFuncçš„é»˜è®¤å‡½æ•°ï¼Œè¯¥å‡½æ•°ç”Ÿæˆå¯¹è±¡çš„é”®ä½œä¸ºè¯¥å¯¹è±¡çš„\u003cnamespace\u003e / \u003cname\u003eç»„åˆã€‚\n0.2. è‡ªå®šä¹‰controllerç»„ä»¶ Informer referenceï¼šæŒ‡çš„æ˜¯Informerå®ä¾‹çš„å¼•ç”¨ï¼Œå®šä¹‰å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰èµ„æºå¯¹è±¡ã€‚ è‡ªå®šä¹‰æ§åˆ¶å™¨ä»£ç éœ€è¦åˆ›å»ºå¯¹åº”çš„Informerã€‚\nIndexer reference: è‡ªå®šä¹‰æ§åˆ¶å™¨å¯¹Indexerå®ä¾‹çš„å¼•ç”¨ã€‚è‡ªå®šä¹‰æ§åˆ¶å™¨éœ€è¦åˆ›å»ºå¯¹åº”çš„Indexserã€‚\nclient-goä¸­æä¾›NewIndexerInformerå‡½æ•°å¯ä»¥åˆ›å»ºInformer å’Œ Indexerã€‚\nResource Event Handlersï¼šèµ„æºäº‹ä»¶å›è°ƒå‡½æ•°ï¼Œå½“å®ƒæƒ³è¦å°†å¯¹è±¡ä¼ é€’ç»™æ§åˆ¶å™¨æ—¶ï¼Œå®ƒå°†è¢«è°ƒç”¨ã€‚ ç¼–å†™è¿™äº›å‡½æ•°çš„å…¸å‹æ¨¡å¼æ˜¯è·å–è°ƒåº¦å¯¹è±¡çš„keyï¼Œå¹¶å°†è¯¥keyæ’å…¥å·¥ä½œé˜Ÿåˆ—ä»¥è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚\nWork queueï¼šä»»åŠ¡é˜Ÿåˆ—ã€‚ ç¼–å†™èµ„æºäº‹ä»¶å¤„ç†ç¨‹åºå‡½æ•°ä»¥æå–ä¼ é€’çš„å¯¹è±¡çš„keyå¹¶å°†å…¶æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—ã€‚\nProcess Itemï¼šå¤„ç†ä»»åŠ¡é˜Ÿåˆ—ä¸­å¯¹è±¡çš„å‡½æ•°ï¼Œ è¿™äº›å‡½æ•°é€šå¸¸ä½¿ç”¨Indexerå¼•ç”¨æˆ–ListingåŒ…è£…å™¨æ¥é‡è¯•ä¸è¯¥keyå¯¹åº”çš„å¯¹è±¡ã€‚\n1. sharedInformerFactory.Start åœ¨controller-managerçš„Runå‡½æ•°éƒ¨åˆ†è°ƒç”¨äº†InformerFactory.Startçš„æ–¹æ³•ã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äº/cmd/kube-controller-manager/app/controllermanager.go\n// Run runs the KubeControllerManagerOptions. This should never exit. func Run(c *config.CompletedConfig, stopCh \u003c-chan struct{}) error { ... controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) ... } InformerFactoryæ˜¯ä¸€ä¸ªSharedInformerFactoryçš„æ¥å£ï¼Œæ¥å£å®šä¹‰å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºvendor/k8s.io/client-go/informers/internalinterfaces/factory_interfaces.go\n// SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh \u003c-chan struct{}) InformerFor(obj runtime.Object, newFunc NewInformerFunc) cache.SharedIndexInformer } Startæ–¹æ³•åˆå§‹åŒ–å„ç§ç±»å‹çš„informerï¼Œå¹¶ä¸”æ¯ä¸ªç±»å‹èµ·äº†ä¸ªinformer.Runçš„goroutineã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äºvendor/k8s.io/client-go/informers/factory.go\n// Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh \u003c-chan struct{}) { f.lock.Lock() defer f.lock.Unlock() for informerType, informer := range f.informers { if !f.startedInformers[informerType] { go informer.Run(stopCh) f.startedInformers[informerType] = true } } } 2. sharedIndexInformer.Run æ­¤éƒ¨åˆ†çš„ä»£ç ä½äº/vendor/k8s.io/client-go/tools/cache/shared_informer.go\nfunc (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) cfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) } 2.1. NewDeltaFIFO DeltaFIFOæ˜¯ä¸€ä¸ªå¯¹è±¡å˜åŒ–çš„å­˜å‚¨é˜Ÿåˆ—ï¼Œä¾æ®å…ˆè¿›å…ˆå‡ºçš„åŸåˆ™ï¼Œprocessçš„å‡½æ•°æ¥æ”¶è¯¥é˜Ÿåˆ—çš„Popæ–¹æ³•çš„è¾“å‡ºå¯¹è±¡æ¥å¤„ç†ç›¸å…³åŠŸèƒ½ã€‚\nfifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) 2.2. Config æ„é€ controllerçš„é…ç½®æ–‡ä»¶ï¼Œæ„é€ processï¼Œå³HandleDeltasï¼Œè¯¥å‡½æ•°ä¸ºåé¢ä½¿ç”¨åˆ°çš„processå‡½æ•°ã€‚\ncfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } 2.3. controller è°ƒç”¨New(cfg)ï¼Œæ„å»ºsharedIndexInformerçš„controllerã€‚\nfunc() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() 2.4. cacheMutationDetector.Run è°ƒç”¨s.cacheMutationDetector.Runï¼Œæ£€æŸ¥ç¼“å­˜å¯¹è±¡æ˜¯å¦å˜åŒ–ã€‚\nwg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) defaultCacheMutationDetector.Run\nfunc (d *defaultCacheMutationDetector) Run(stopCh \u003c-chan struct{}) { // we DON'T want protection from panics. If we're running this code, we want to die for { d.CompareObjects() select { case \u003c-stopCh: return case \u003c-time.After(d.period): } } } CompareObjects\nfunc (d *defaultCacheMutationDetector) CompareObjects() { d.lock.Lock() defer d.lock.Unlock() altered := false for i, obj := range d.cachedObjs { if !reflect.DeepEqual(obj.cached, obj.copied) { fmt.Printf(\"CACHE %s[%d] ALTERED!\\n%v\\n\", d.name, i, diff.ObjectDiff(obj.cached, obj.copied)) altered = true } } if altered { msg := fmt.Sprintf(\"cache %s modified\", d.name) if d.failureFunc != nil { d.failureFunc(msg) return } panic(msg) } } 2.5. processor.run è°ƒç”¨s.processor.runï¼Œå°†è°ƒç”¨sharedProcessor.runï¼Œä¼šè°ƒç”¨Listener.runå’ŒListener.pop,æ‰§è¡Œå¤„ç†queueçš„å‡½æ•°ã€‚\nwg.StartWithChannel(processorStopCh, s.processor.run) sharedProcessor.Run\nfunc (p *sharedProcessor) run(stopCh \u003c-chan struct{}) { func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } }() \u003c-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } è¯¥éƒ¨åˆ†é€»è¾‘å¾…åé¢åˆ†æã€‚\n2.6. controller.Run è°ƒç”¨s.controller.Runï¼Œæ„å»ºReflectorï¼Œè¿›è¡Œå¯¹etcdçš„ç¼“å­˜\ndefer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) controller.Run\næ­¤éƒ¨åˆ†ä»£ç ä½äº/vendor/k8s.io/client-go/tools/cache/controller.go\n// Run begins processing items, and will continue until a value is sent down stopCh. // It's an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() go func() { \u003c-stopCh c.config.Queue.Close() }() r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.clock = c.clock c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group defer wg.Wait() wg.StartWithChannel(stopCh, r.Run) wait.Until(c.processLoop, time.Second, stopCh) } æ ¸å¿ƒä»£ç ï¼š\n// æ„å»ºReflector r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) // è¿è¡ŒReflector wg.StartWithChannel(stopCh, r.Run) // æ‰§è¡ŒprocessLoop wait.Until(c.processLoop, time.Second, stopCh) 3. Reflector 3.1. Reflector Reflectorçš„ä¸»è¦ä½œç”¨æ˜¯watchæŒ‡å®šçš„k8sèµ„æºï¼Œå¹¶å°†å˜åŒ–åŒæ­¥åˆ°æœ¬åœ°æ˜¯storeä¸­ã€‚Reflectoråªä¼šæ”¾ç½®æŒ‡å®šçš„expectedTypeç±»å‹çš„èµ„æºåˆ°storeä¸­ï¼Œé™¤éexpectedTypeä¸ºnilã€‚å¦‚æœresyncPeriodä¸ä¸ºé›¶ï¼Œé‚£ä¹ˆReflectorä¸ºä»¥resyncPeriodä¸ºå‘¨æœŸå®šæœŸæ‰§è¡Œlistçš„æ“ä½œï¼Œè¿™æ ·å°±å¯ä»¥ä½¿ç”¨Reflectoræ¥å®šæœŸå¤„ç†æ‰€æœ‰çš„å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥é€æ­¥å¤„ç†å˜åŒ–çš„å¯¹è±¡ã€‚\nå¸¸ç”¨å±æ€§è¯´æ˜ï¼š\nexpectedTypeï¼šæœŸæœ›æ”¾å…¥ç¼“å­˜storeçš„èµ„æºç±»å‹ã€‚ storeï¼šwatchçš„èµ„æºå¯¹åº”çš„æœ¬åœ°ç¼“å­˜ã€‚ listerWatcherï¼šlistå’Œwatchçš„æ¥å£ã€‚ periodï¼šwatchçš„å‘¨æœŸï¼Œé»˜è®¤ä¸º1ç§’ã€‚ resyncPeriodï¼šresyncçš„å‘¨æœŸï¼Œå½“éé›¶çš„æ—¶å€™ï¼Œä¼šæŒ‰è¯¥å‘¨æœŸæ‰§è¡Œlistã€‚ lastSyncResourceVersionï¼šæœ€æ–°ä¸€æ¬¡çœ‹åˆ°çš„èµ„æºçš„ç‰ˆæœ¬å·ï¼Œä¸»è¦åœ¨watchæ—¶å€™ä½¿ç”¨ã€‚ // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { // name identifies this reflector. By default it will be a file:line if possible. name string // metrics tracks basic metric information about the reflector metrics *reflectorMetrics // The type of object we expect to place in the store. expectedType reflect.Type // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // period controls timing between one watch ending and // the beginning of the next one. period time.Duration resyncPeriod time.Duration ShouldResync func() bool // clock allows tests to manipulate time clock clock.Clock // lastSyncResourceVersion is the resource version token last // observed when doing a sync with the underlying store // it is thread safe, but not synchronized with the underlying store lastSyncResourceVersion string // lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion lastSyncResourceVersionMutex sync.RWMutex } 3.2. NewReflector NewReflectorä¸»è¦ç”¨æ¥æ„å»ºReflectorçš„ç»“æ„ä½“ã€‚\næ­¤éƒ¨åˆ†çš„ä»£ç ä½äº/vendor/k8s.io/client-go/tools/cache/reflector.go\n// NewReflector creates a new Reflector object which will keep the given store up to // date with the server's contents for the given resource. Reflector promises to // only put things in the store that have the type of expectedType, unless expectedType // is nil. If resyncPeriod is non-zero, then lists will be executed after every // resyncPeriod, so that you can use reflectors to periodically process everything as // well as incrementally processing the things that change. func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(getDefaultReflectorName(internalPackages...), lw, expectedType, store, resyncPeriod) } // reflectorDisambiguator is used to disambiguate started reflectors. // initialized to an unstable value to ensure meaning isn't attributed to the suffix. var reflectorDisambiguator = int64(time.Now().UnixNano() % 12345) // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(\u0026reflectorDisambiguator, 1) r := \u0026Reflector{ name: name, // we need this to be unique per process (some names are still the same)but obvious who it belongs to metrics: newReflectorMetrics(makeValidPromethusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: \u0026clock.RealClock{}, } return r } 3.3. Reflector.Run Reflector.Runä¸»è¦æ‰§è¡Œäº†ListAndWatchçš„æ–¹æ³•ã€‚\n// Run starts a watch and handles watch events. Will restart the watch if it is closed. // Run will exit when stopCh is closed. func (r *Reflector) Run(stopCh \u003c-chan struct{}) { glog.V(3).Infof(\"Starting reflector %v (%s) from %s\", r.expectedType, r.resyncPeriod, r.name) wait.Until(func() { if err := r.ListAndWatch(stopCh); err != nil { utilruntime.HandleError(err) } }, r.period, stopCh) } 3.4. ListAndWatch ListAndWatchç¬¬ä¸€æ¬¡ä¼šåˆ—å‡ºæ‰€æœ‰çš„å¯¹è±¡ï¼Œå¹¶è·å–èµ„æºå¯¹è±¡çš„ç‰ˆæœ¬å·ï¼Œç„¶åwatchèµ„æºå¯¹è±¡çš„ç‰ˆæœ¬å·æ¥æŸ¥çœ‹æ˜¯å¦æœ‰è¢«å˜æ›´ã€‚é¦–å…ˆä¼šå°†èµ„æºç‰ˆæœ¬å·è®¾ç½®ä¸º0ï¼Œlist()å¯èƒ½ä¼šå¯¼è‡´æœ¬åœ°çš„ç¼“å­˜ç›¸å¯¹äºetcdé‡Œé¢çš„å†…å®¹å­˜åœ¨å»¶è¿Ÿï¼ŒReflectorä¼šé€šè¿‡watchçš„æ–¹æ³•å°†å»¶è¿Ÿçš„éƒ¨åˆ†è¡¥å……ä¸Šï¼Œä½¿å¾—æœ¬åœ°çš„ç¼“å­˜æ•°æ®ä¸etcdçš„æ•°æ®ä¿æŒä¸€è‡´ã€‚\n3.4.1. List // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { glog.V(3).Infof(\"Listing and watching %v from %s\", r.expectedType, r.name) var resourceVersion string // Explicitly set \"0\" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1.ListOptions{ResourceVersion: \"0\"} r.metrics.numberOfLists.Inc() start := r.clock.Now() list, err := r.listerWatcher.List(options) if err != nil { return fmt.Errorf(\"%s: Failed to list %v: %v\", r.name, r.expectedType, err) } r.metrics.listDuration.Observe(time.Since(start).Seconds()) listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v: %v\", r.name, list, err) } resourceVersion = listMetaInterface.GetResourceVersion() items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v (%v)\", r.name, list, err) } r.metrics.numberOfItemsInList.Observe(float64(len(items))) if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err) } r.setLastSyncResourceVersion(resourceVersion) ... } é¦–å…ˆå°†èµ„æºçš„ç‰ˆæœ¬å·è®¾ç½®ä¸º0ï¼Œç„¶åè°ƒç”¨listerWatcher.List(options)ï¼Œåˆ—å‡ºæ‰€æœ‰listçš„å†…å®¹ã€‚\n// ç‰ˆæœ¬å·è®¾ç½®ä¸º0 options := metav1.ListOptions{ResourceVersion: \"0\"} // listæ¥å£ list, err := r.listerWatcher.List(options) è·å–èµ„æºç‰ˆæœ¬å·ï¼Œå¹¶å°†listçš„å†…å®¹æå–æˆå¯¹è±¡åˆ—è¡¨ã€‚\n// è·å–ç‰ˆæœ¬å· resourceVersion = listMetaInterface.GetResourceVersion() // å°†listçš„å†…å®¹æå–æˆå¯¹è±¡åˆ—è¡¨ items, err := meta.ExtractList(list) å°†listä¸­å¯¹è±¡åˆ—è¡¨çš„å†…å®¹å’Œç‰ˆæœ¬å·å­˜å‚¨åˆ°æœ¬åœ°çš„ç¼“å­˜storeä¸­ï¼Œå¹¶å…¨é‡æ›¿æ¢å·²æœ‰çš„storeçš„å†…å®¹ã€‚\nerr := r.syncWith(items, resourceVersion) syncWithè°ƒç”¨äº†storeçš„Replaceçš„æ–¹æ³•æ¥æ›¿æ¢åŸæ¥storeä¸­çš„æ•°æ®ã€‚\n// syncWith replaces the store's items with the given list. func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error { found := make([]interface{}, 0, len(items)) for _, item := range items { found = append(found, item) } return r.store.Replace(found, resourceVersion) } Store.Replaceæ–¹æ³•å®šä¹‰å¦‚ä¸‹ï¼š\ntype Store interface { ... // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. Replace([]interface{}, string) error ... } æœ€åè®¾ç½®æœ€æ–°çš„èµ„æºç‰ˆæœ¬å·ã€‚\nr.setLastSyncResourceVersion(resourceVersion) setLastSyncResourceVersion:\nfunc (r *Reflector) setLastSyncResourceVersion(v string) { r.lastSyncResourceVersionMutex.Lock() defer r.lastSyncResourceVersionMutex.Unlock() r.lastSyncResourceVersion = v rv, err := strconv.Atoi(v) if err == nil { r.metrics.lastResourceVersion.Set(float64(rv)) } } 3.4.2. store.Resync resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) go func() { resyncCh, cleanup := r.resyncChan() defer func() { cleanup() // Call the last one written into cleanup }() for { select { case \u003c-resyncCh: case \u003c-stopCh: return case \u003c-cancelCh: return } if r.ShouldResync == nil || r.ShouldResync() { glog.V(4).Infof(\"%s: forcing resync\", r.name) if err := r.store.Resync(); err != nil { resyncerrc \u003c- err return } } cleanup() resyncCh, cleanup = r.resyncChan() } }() æ ¸å¿ƒä»£ç ï¼š\nerr := r.store.Resync() storeçš„å…·ä½“å¯¹è±¡ä¸ºDeltaFIFOï¼Œå³è°ƒç”¨DeltaFIFO.Resync\n// Resync will send a sync event for each item func (f *DeltaFIFO) Resync() error { f.lock.Lock() defer f.lock.Unlock() if f.knownObjects == nil { return nil } keys := f.knownObjects.ListKeys() for _, k := range keys { if err := f.syncKeyLocked(k); err != nil { return err } } return nil } 3.4.3. Watch for { // give the stopCh a chance to stop the loop, even in case of continue statements further down on errors select { case \u003c-stopCh: return nil default: } timemoutseconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0)) options = metav1.ListOptions{ ResourceVersion: resourceVersion, // We want to avoid situations of hanging watchers. Stop any wachers that do not // receive any events within the timeout window. TimeoutSeconds: \u0026timemoutseconds, } r.metrics.numberOfWatches.Inc() w, err := r.listerWatcher.Watch(options) if err != nil { switch err { case io.EOF: // watch closed normally case io.ErrUnexpectedEOF: glog.V(1).Infof(\"%s: Watch for %v closed with unexpected EOF: %v\", r.name, r.expectedType, err) default: utilruntime.HandleError(fmt.Errorf(\"%s: Failed to watch %v: %v\", r.name, r.expectedType, err)) } // If this is \"connection refused\" error, it means that most likely apiserver is not responsive. // It doesn't make sense to re-list all objects because most likely we will be able to restart // watch where we ended. // If that's the case wait and resend watch request. if urlError, ok := err.(*url.Error); ok { if opError, ok := urlError.Err.(*net.OpError); ok { if errno, ok := opError.Err.(syscall.Errno); ok \u0026\u0026 errno == syscall.ECONNREFUSED { time.Sleep(time.Second) continue } } } return nil } if err := r.watchHandler(w, \u0026resourceVersion, resyncerrc, stopCh); err != nil { if err != errorStopRequested { glog.Warningf(\"%s: watch of %v ended with: %v\", r.name, r.expectedType, err) } return nil } } è®¾ç½®watchçš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º5åˆ†é’Ÿã€‚\ntimemoutseconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0)) options = metav1.ListOptions{ ResourceVersion: resourceVersion, // We want to avoid situations of hanging watchers. Stop any wachers that do not // receive any events within the timeout window. TimeoutSeconds: \u0026timemoutseconds, } æ‰§è¡ŒlisterWatcher.Watch(options)ã€‚\nw, err := r.listerWatcher.Watch(options) æ‰§è¡ŒwatchHandlerã€‚\nerr := r.watchHandler(w, \u0026resourceVersion, resyncerrc, stopCh) 3.4.4. watchHandler watchHandlerä¸»è¦æ˜¯é€šè¿‡watchçš„æ–¹å¼ä¿è¯å½“å‰çš„èµ„æºç‰ˆæœ¬æ˜¯æœ€æ–°çš„ã€‚\n// watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh \u003c-chan struct{}) error { start := r.clock.Now() eventCount := 0 // Stopping the watcher should be idempotent and if we return from this function there's no way // we're coming back in with the same watch interface. defer w.Stop() // update metrics defer func() { r.metrics.numberOfItemsInWatch.Observe(float64(eventCount)) r.metrics.watchDuration.Observe(time.Since(start).Seconds()) }() loop: for { select { case \u003c-stopCh: return errorStopRequested case err := \u003c-errc: return err case event, ok := \u003c-w.ResultChan(): if !ok { break loop } if event.Type == watch.Error { return apierrs.FromObject(event.Object) } if e, a := r.expectedType, reflect.TypeOf(event.Object); e != nil \u0026\u0026 e != a { utilruntime.HandleError(fmt.Errorf(\"%s: expected type %v, but watch event object had type %v\", r.name, e, a)) continue } meta, err := meta.Accessor(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) continue } newResourceVersion := meta.GetResourceVersion() switch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) eventCount++ } } watchDuration := r.clock.Now().Sub(start) if watchDuration \u003c 1*time.Second \u0026\u0026 eventCount == 0 { r.metrics.numberOfShortWatches.Inc() return fmt.Errorf(\"very short watch: %s: Unexpected watch close - watch lasted less than a second and no items received\", r.name) } glog.V(4).Infof(\"%s: Watch close - %v total %v items received\", r.name, r.expectedType, eventCount) return nil } è·å–watchæ¥å£ä¸­çš„äº‹ä»¶çš„channelï¼Œæ¥è·å–äº‹ä»¶çš„å†…å®¹ã€‚\nfor { select { ... case event, ok := \u003c-w.ResultChan(): ... } å½“è·å¾—æ·»åŠ ã€æ›´æ–°ã€åˆ é™¤çš„äº‹ä»¶æ—¶ï¼Œå°†å¯¹åº”çš„å¯¹è±¡æ›´æ–°åˆ°æœ¬åœ°ç¼“å­˜storeä¸­ã€‚\nswitch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } æ›´æ–°å½“å‰çš„æœ€æ–°ç‰ˆæœ¬å·ã€‚\nnewResourceVersion := meta.GetResourceVersion() *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) é€šè¿‡å¯¹Reflectoræ¨¡å—çš„åˆ†æï¼Œå¯ä»¥çœ‹åˆ°å¤šæ¬¡ä½¿ç”¨åˆ°æœ¬åœ°ç¼“å­˜storeæ¨¡å—ï¼Œè€Œstoreçš„æ•°æ®ç”±DeltaFIFOèµ‹å€¼è€Œæ¥ï¼Œä»¥ä¸‹é’ˆå¯¹DeltaFIFOå’Œstoreåšåˆ†æã€‚\n4. DeltaFIFO DeltaFIFOç”±NewDeltaFIFOåˆå§‹åŒ–ï¼Œå¹¶èµ‹å€¼ç»™config.Queueã€‚\nfunc (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) cfg := \u0026Config{ Queue: fifo, ... } ... } 4.1. NewDeltaFIFO // NewDeltaFIFO returns a Store which can be used process changes to items. // // keyFunc is used to figure out what key an object should have. (It's // exposed in the returned DeltaFIFO's KeyOf() method, with bonus features.) // // 'compressor' may compress as many or as few items as it wants // (including returning an empty slice), but it should do what it // does quickly since it is called while the queue is locked. // 'compressor' may be nil if you don't want any delta compression. // // 'keyLister' is expected to return a list of keys that the consumer of // this queue \"knows about\". It is used to decide which items are missing // when Replace() is called; 'Deleted' deltas are produced for these items. // It may be nil if you don't need to detect all deletions. // TODO: consider merging keyLister with this object, tracking a list of // \"known\" keys when Pop() is called. Have to think about how that // affects error retrying. // TODO(lavalamp): I believe there is a possible race only when using an // external known object source that the above TODO would // fix. // // Also see the comment on DeltaFIFO. func NewDeltaFIFO(keyFunc KeyFunc, compressor DeltaCompressor, knownObjects KeyListerGetter) *DeltaFIFO { f := \u0026DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: keyFunc, deltaCompressor: compressor, knownObjects: knownObjects, } f.cond.L = \u0026f.lock return f } controller.Runçš„éƒ¨åˆ†è°ƒç”¨äº†NewReflectorã€‚\nfunc (c *controller) Run(stopCh \u003c-chan struct{}) { ... r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) ... } NewReflectoræ„é€ å‡½æ•°ï¼Œå°†c.config.Queueèµ‹å€¼ç»™Reflector.storeçš„å±æ€§ã€‚\nfunc NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(getDefaultReflectorName(internalPackages...), lw, expectedType, store, resyncPeriod) } // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(\u0026reflectorDisambiguator, 1) r := \u0026Reflector{ name: name, // we need this to be unique per process (some names are still the same)but obvious who it belongs to metrics: newReflectorMetrics(makeValidPromethusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: \u0026clock.RealClock{}, } return r } 4.2. DeltaFIFO DeltaFIFOæ˜¯ä¸€ä¸ªç”Ÿäº§è€…ä¸æ¶ˆè´¹è€…çš„é˜Ÿåˆ—ï¼Œå…¶ä¸­Reflectoræ˜¯ç”Ÿäº§è€…ï¼Œæ¶ˆè´¹è€…è°ƒç”¨Pop()çš„æ–¹æ³•ã€‚\nDeltaFIFOä¸»è¦ç”¨åœ¨ä»¥ä¸‹åœºæ™¯ï¼š\nå¸Œæœ›å¯¹è±¡å˜æ›´æœ€å¤šå¤„ç†ä¸€æ¬¡ å¤„ç†å¯¹è±¡æ—¶ï¼Œå¸Œæœ›æŸ¥çœ‹è‡ªä¸Šæ¬¡å¤„ç†å¯¹è±¡ä»¥æ¥å‘ç”Ÿçš„æ‰€æœ‰äº‹æƒ… è¦å¤„ç†å¯¹è±¡çš„åˆ é™¤ å¸Œæœ›å®šæœŸé‡æ–°å¤„ç†å¯¹è±¡ // DeltaFIFO is like FIFO, but allows you to process deletes. // // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // // DeltaFIFO solves this use case: // * You want to process every object change (delta) at most once. // * When you process an object, you want to see everything // that's happened to it since you last processed it. // * You want to process the deletion of objects. // * You might want to periodically reprocess objects. // // DeltaFIFO's Pop(), Get(), and GetByKey() methods return // interface{} to satisfy the Store/Queue interfaces, but it // will always return an object of type Deltas. // // A note on threading: If you call Pop() in parallel from multiple // threads, you could end up with multiple threads processing slightly // different versions of the same object. // // A note on the KeyLister used by the DeltaFIFO: It's main purpose is // to list keys that are \"known\", for the purpose of figuring out which // items have been deleted when Replace() or Delete() are called. The deleted // object will be included in the DeleteFinalStateUnknown markers. These objects // could be stale. // // You may provide a function to compress deltas (e.g., represent a // series of Updates as a single Update). type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. lock sync.RWMutex cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. items map[string]Deltas queue []string // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // deltaCompressor tells us how to combine two or more // deltas. It may be nil. deltaCompressor DeltaCompressor // knownObjects list keys that are \"known\", for the // purpose of figuring out which items have been deleted // when Replace() or Delete() is called. knownObjects KeyListerGetter // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRED operations. closed bool closedLock sync.Mutex } 4.3. Queue \u0026 Store DeltaFIFOçš„ç±»å‹æ˜¯Queueæ¥å£ï¼ŒReflector.storeæ˜¯Storeæ¥å£ï¼ŒQueueæ¥å£æ˜¯ä¸€ä¸ªå­˜å‚¨é˜Ÿåˆ—ï¼ŒProcessçš„æ–¹æ³•æ‰§è¡ŒQueue.Popå‡ºæ¥çš„æ•°æ®å¯¹è±¡ï¼Œ\n// Queue is exactly like a Store, but has a Pop() method too. type Queue interface { Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // Return true if the first batch of items has been popped HasSynced() bool // Close queue Close() } 5. store Storeæ˜¯ä¸€ä¸ªé€šç”¨çš„å­˜å‚¨æ¥å£ï¼ŒReflectoré€šè¿‡watch serverçš„æ–¹å¼æ›´æ–°æ•°æ®åˆ°storeä¸­ï¼Œstoreç»™Reflectoræä¾›æœ¬åœ°çš„ç¼“å­˜ï¼Œè®©Reflectorå¯ä»¥åƒæ¶ˆæ¯é˜Ÿåˆ—ä¸€æ ·çš„å·¥ä½œã€‚\nStoreå®ç°çš„æ˜¯ä¸€ç§å¯ä»¥å‡†ç¡®çš„å†™å…¥å¯¹è±¡å’Œè·å–å¯¹è±¡çš„æœºåˆ¶ã€‚\n// Store is a generic object storage interface. Reflector knows how to watch a server // and update a store. A generic store is provided, which allows Reflector to be used // as a local caching system, and an LRU store, which allows Reflector to work like a // queue of items yet to be processed. // // Store makes no assumptions about stored object identity; it is the responsibility // of a Store implementation to provide a mechanism to correctly key objects and to // define the contract for obtaining objects by some arbitrary key type. type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. Replace([]interface{}, string) error Resync() error } å…¶ä¸­Replaceæ–¹æ³•ä¼šåˆ é™¤åŸæ¥storeä¸­çš„å†…å®¹ï¼Œå¹¶å°†æ–°å¢çš„listçš„å†…å®¹å­˜å…¥storeä¸­ï¼Œå³å®Œå…¨æ›¿æ¢æ•°æ®ã€‚\n6.1. cache cacheå®ç°äº†storeçš„æ¥å£ï¼Œè€Œcacheçš„å…·ä½“å®ç°åˆæ˜¯è°ƒç”¨ThreadSafeStoreæ¥å£æ¥å®ç°åŠŸèƒ½çš„ã€‚\ncacheçš„åŠŸèƒ½ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç‚¹ï¼š\né€šè¿‡keyFuncè®¡ç®—å¯¹è±¡çš„key è°ƒç”¨ThreadSafeStorageæ¥å£çš„æ–¹æ³• // cache responsibilities are limited to: //\t1. Computing keys for objects via keyFunc // 2. Invoking methods of a ThreadSafeStorage interface type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } å…¶ä¸­ListAndWatchä¸»è¦ç”¨åˆ°ä»¥ä¸‹çš„æ–¹æ³•ï¼š\ncache.Replace\n// Replace will delete the contents of 'c', using instead the given list. // 'c' takes ownership of the list, you should not reference the list again // after calling this function. func (c *cache) Replace(list []interface{}, resourceVersion string) error { items := map[string]interface{}{} for _, item := range list { key, err := c.keyFunc(item) if err != nil { return KeyError{item, err} } items[key] = item } c.cacheStorage.Replace(items, resourceVersion) return nil } cache.Add\n// Add inserts an item into the cache. func (c *cache) Add(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Add(key, obj) return nil } cache.Update\n// Update sets an item in the cache to its updated state. func (c *cache) Update(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Update(key, obj) return nil } cache.Delete\n// Delete removes an item from the cache. func (c *cache) Delete(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Delete(key) return nil } 6.2. ThreadSafeStore cacheçš„å…·ä½“æ˜¯è°ƒç”¨ThreadSafeStoreæ¥å®ç°çš„ã€‚\n// ThreadSafeStore is an interface that allows concurrent access to a storage backend. // TL;DR caveats: you must not modify anything returned by Get or List as it will break // the indexing feature in addition to not being thread safe. // // The guarantees of thread safety provided by List/Get are only valid if the caller // treats returned items as read-only. For example, a pointer inserted in the store // through `Add` will be returned as is by `Get`. Multiple clients might invoke `Get` // on the same key and modify the pointer in a non-thread-safe way. Also note that // modifying objects stored by the indexers (if any) will *not* automatically lead // to a re-index. So it's not a good idea to directly modify the objects returned by // Get/List, in general. type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error Resync() error } threadSafeMap\n// threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} // indexers maps a name to an IndexFunc indexers Indexers // indices maps a name to an Index indices Indices } 6. processLoop func (c *controller) Run(stopCh \u003c-chan struct{}) { ... wait.Until(c.processLoop, time.Second, stopCh) } åœ¨controller.Runæ–¹æ³•ä¸­ä¼šè°ƒç”¨processLoopï¼Œä»¥ä¸‹åˆ†æprocessLoopçš„å¤„ç†é€»è¾‘ã€‚\n// processLoop drains the work queue. // TODO: Consider doing the processing in parallel. This will require a little thought // to make sure that we don't end up processing the same object multiple times // concurrently. // // TODO: Plumb through the stopCh here (and down to the queue) so that this can // actually exit when the controller is stopped. Or just give up on this stuff // ever being stoppable. Converting this whole package to use Context would // also be helpful. func (c *controller) processLoop() { for { obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == FIFOClosedError { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } processLoopä¸»è¦å¤„ç†ä»»åŠ¡é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œå…¶ä¸­å¤„ç†é€»è¾‘æ˜¯è°ƒç”¨å…·ä½“çš„ProcessFuncå‡½æ•°æ¥å®ç°ï¼Œæ ¸å¿ƒä»£ç ä¸ºï¼š\nobj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) 5.1. DeltaFIFO.Pop Popä¼šé˜»å¡ä½ç›´åˆ°é˜Ÿåˆ—é‡Œé¢æ·»åŠ äº†æ–°çš„å¯¹è±¡ï¼Œå¦‚æœæœ‰å¤šä¸ªå¯¹è±¡ï¼ŒæŒ‰ç…§å…ˆè¿›å…ˆå‡ºçš„åŸåˆ™å¤„ç†ï¼Œå¦‚æœæŸä¸ªå¯¹è±¡æ²¡æœ‰å¤„ç†æˆåŠŸä¼šé‡æ–°è¢«åŠ å…¥è¯¥é˜Ÿåˆ—ä¸­ã€‚\nPopä¸­ä¼šè°ƒç”¨å…·ä½“çš„processå‡½æ•°æ¥å¤„ç†å¯¹è±¡ã€‚\n// Pop blocks until an item is added to the queue, and then returns it. If // multiple items are ready, they are returned in the order in which they were // added/updated. The item is removed from the queue (and the store) before it // is returned, so if you don't successfully process it, you need to add it back // with AddIfNotPresent(). // process function is called under lock, so it is safe update data structures // in it that need to be in sync with the queue (e.g. knownKeys). The PopProcessFunc // may return an instance of ErrRequeue with a nested error to indicate the current // item should be requeued (equivalent to calling AddIfNotPresent under the lock). // // Pop returns a 'Deltas', which has a complete list of all the things // that happened to the object (deltas) while it was sitting in the queue. func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.IsClosed() { return nil, FIFOClosedError } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] item, ok := f.items[id] if f.initialPopulationCount \u003e 0 { f.initialPopulationCount-- } if !ok { // Item may have been deleted subsequently. continue } delete(f.items, id) err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } } æ ¸å¿ƒä»£ç ï¼š\nfor { ... item, ok := f.items[id] ... err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } 5.2. HandleDeltas cfg := \u0026Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } å…¶ä¸­processå‡½æ•°å°±æ˜¯åœ¨sharedIndexInformer.Runæ–¹æ³•ä¸­ï¼Œç»™config.Processèµ‹å€¼çš„HandleDeltaså‡½æ•°ã€‚\nfunc (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026\u0026 exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } æ ¸å¿ƒä»£ç ï¼š\nswitch d.Type { case Sync, Added, Updated: ... if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026\u0026 exists { ... s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { ... s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: ... s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } æ ¹æ®ä¸åŒçš„ç±»å‹ï¼Œè°ƒç”¨processor.distributeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å¯¹è±¡åŠ å…¥processorListenerçš„channelä¸­ã€‚\n5.3. sharedProcessor.distribute func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { listener.add(obj) } } else { for _, listener := range p.listeners { listener.add(obj) } } } processorListener.add:\nfunc (p *processorListener) add(notification interface{}) { p.addCh \u003c- notification } ç»¼åˆä»¥ä¸Šçš„åˆ†æï¼Œå¯ä»¥çœ‹å‡ºprocessLoopé€šè¿‡è°ƒç”¨HandleDeltasï¼Œå†è°ƒç”¨distributeï¼ŒprocessorListener.addæœ€ç»ˆå°†ä¸åŒæ›´æ–°ç±»å‹çš„å¯¹è±¡åŠ å…¥processorListenerçš„channelä¸­ï¼Œä¾›processorListener.Runä½¿ç”¨ã€‚ä»¥ä¸‹åˆ†æprocessorListener.Runçš„éƒ¨åˆ†ã€‚\n7. processor processorçš„ä¸»è¦åŠŸèƒ½å°±æ˜¯è®°å½•äº†æ‰€æœ‰çš„å›è°ƒå‡½æ•°å®ä¾‹(å³ ResourceEventHandler å®ä¾‹)ï¼Œå¹¶è´Ÿè´£è§¦å‘è¿™äº›å‡½æ•°ã€‚åœ¨sharedIndexInformer.Runéƒ¨åˆ†ä¼šè°ƒç”¨processor.runã€‚\næµç¨‹ï¼š\nlistenserçš„addå‡½æ•°è´Ÿè´£å°†notifyè£…è¿›pendingNotificationsã€‚ popå‡½æ•°å–å‡ºpendingNotificationsçš„ç¬¬ä¸€ä¸ªnofify,è¾“å‡ºåˆ°nextCh channelã€‚ runå‡½æ•°åˆ™è´Ÿè´£å–å‡ºnotifyï¼Œç„¶åæ ¹æ®notifyçš„ç±»å‹(å¢åŠ ã€åˆ é™¤ã€æ›´æ–°)è§¦å‘ç›¸åº”çš„å¤„ç†å‡½æ•°ï¼Œè¿™äº›å‡½æ•°æ˜¯åœ¨ä¸åŒçš„NewXxxcontrollerå®ç°ä¸­æ³¨å†Œçš„ã€‚ func (s *sharedIndexInformer) Run(stopCh \u003c-chan struct{}) { ... wg.StartWithChannel(processorStopCh, s.processor.run) ... } 7.1. sharedProcessor.Run func (p *sharedProcessor) run(stopCh \u003c-chan struct{}) { func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } }() \u003c-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } 7.1.1. listener.pop popå‡½æ•°å–å‡ºpendingNotificationsçš„ç¬¬ä¸€ä¸ªnofify,è¾“å‡ºåˆ°nextCh channelã€‚\nfunc (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop var nextCh chan\u003c- interface{} var notification interface{} for { select { case nextCh \u003c- notification: // Notification dispatched var ok bool notification, ok = p.pendingNotifications.ReadOne() if !ok { // Nothing to pop nextCh = nil // Disable this select case } case notificationToAdd, ok := \u003c-p.addCh: if !ok { return } if notification == nil { // No notification to pop (and pendingNotifications is empty) // Optimize the case - skip adding to pendingNotifications notification = notificationToAdd nextCh = p.nextCh } else { // There is already a notification waiting to be dispatched p.pendingNotifications.WriteOne(notificationToAdd) } } } } 7.1.2. listener.run listener.runéƒ¨åˆ†æ ¹æ®ä¸åŒçš„æ›´æ–°ç±»å‹è°ƒç”¨ä¸åŒçš„å¤„ç†å‡½æ•°ã€‚\nfunc (p *processorListener) run() { defer utilruntime.HandleCrash() for next := range p.nextCh { switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\"unrecognized notification: %#v\", next)) } } } å…¶ä¸­å…·ä½“çš„å®ç°å‡½æ•°handleræ˜¯åœ¨NewDeploymentControllerï¼ˆå…¶ä»–ä¸åŒç±»å‹çš„controllerç±»ä¼¼ï¼‰ä¸­èµ‹å€¼çš„ï¼Œè€Œè¯¥handleræ˜¯ä¸€ä¸ªæ¥å£ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n// ResourceEventHandler can handle notifications for events that happen to a // resource. The events are informational only, so you can't return an // error. // * OnAdd is called when an object is added. // * OnUpdate is called when an object is modified. Note that oldObj is the // last known state of the object-- it is possible that several changes // were combined together, so you can't use this to see every single // change. OnUpdate is also called when a re-list happens, and it will // get called even if nothing changed. This is useful for periodically // evaluating or syncing something. // * OnDelete will get the final state of the item if it is known, otherwise // it will get an object of type DeletedFinalStateUnknown. This can // happen if the watch is closed and misses the delete event and we don't // notice the deletion until the subsequent re-list. type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } 7.2. ResourceEventHandler ä»¥ä¸‹ä»¥DeploymentControllerçš„å¤„ç†é€»è¾‘ä¸ºä¾‹ã€‚\nåœ¨NewDeploymentControlleréƒ¨åˆ†ä¼šæ³¨å†Œdeploymentçš„äº‹ä»¶å‡½æ•°ï¼Œä»¥ä¸‹æ³¨å†Œäº†ä¸‰ç§ç±»å‹çš„äº‹ä»¶å‡½æ•°ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šdInformerã€rsInformerå’ŒpodInformerã€‚\n// NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer extensionsinformers.DeploymentInformer, rsInformer extensionsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { ... dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) ... } 7.2.1. addDeployment ä»¥ä¸‹ä»¥addDeploymentä¸ºä¾‹ï¼ŒaddDeploymentä¸»è¦æ˜¯å°†å¯¹è±¡åŠ å…¥åˆ°enqueueDeploymentçš„é˜Ÿåˆ—ä¸­ã€‚\nfunc (dc *DeploymentController) addDeployment(obj interface{}) { d := obj.(*extensions.Deployment) glog.V(4).Infof(\"Adding deployment %s\", d.Name) dc.enqueueDeployment(d) } enqueueDeploymentçš„å®šä¹‰\ntype DeploymentController struct { ... enqueueDeployment func(deployment *extensions.Deployment) ... } å°†dc.enqueueèµ‹å€¼ç»™dc.enqueueDeployment\ndc.enqueueDeployment = dc.enqueue dc.enqueueè°ƒç”¨äº†dc.queue.Add(key)\nfunc (dc *DeploymentController) enqueue(deployment *extensions.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf(\"Couldn't get key for object %#v: %v\", deployment, err)) return } dc.queue.Add(key) } dc.queueä¸»è¦è®°å½•äº†éœ€è¦è¢«åŒæ­¥çš„deploymentçš„å¯¹è±¡ï¼Œä¾›syncDeploymentä½¿ç”¨ã€‚\ndc := \u0026DeploymentController{ ... queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } NewNamedRateLimitingQueue\nfunc NewNamedRateLimitingQueue(rateLimiter RateLimiter, name string) RateLimitingInterface { return \u0026rateLimitingType{ DelayingInterface: NewNamedDelayingQueue(name), rateLimiter: rateLimiter, } } é€šè¿‡ä»¥ä¸Šåˆ†æï¼Œå¯ä»¥çœ‹å‡ºprocessorè®°å½•äº†ä¸åŒç±»ä¼¼çš„äº‹ä»¶å‡½æ•°ï¼Œå…¶ä¸­äº‹ä»¶å‡½æ•°åœ¨NewXxxControlleræ„é€ å‡½æ•°éƒ¨åˆ†æ³¨å†Œï¼Œå…·ä½“äº‹ä»¶å‡½æ•°çš„å¤„ç†ï¼Œä¸€èˆ¬æ˜¯å°†éœ€è¦å¤„ç†çš„å¯¹è±¡åŠ å…¥å¯¹åº”çš„controllerçš„ä»»åŠ¡é˜Ÿåˆ—ä¸­ï¼Œç„¶åç”±ç±»ä¼¼syncDeploymentçš„åŒæ­¥å‡½æ•°æ¥ç»´æŒæœŸæœ›çŠ¶æ€çš„åŒæ­¥é€»è¾‘ã€‚\n8. æ€»ç»“ æœ¬æ–‡åˆ†æçš„éƒ¨åˆ†ä¸»è¦æ˜¯k8sçš„informeræœºåˆ¶ï¼Œå³List-Watchæœºåˆ¶ã€‚\n8.1. Reflector Reflectorçš„ä¸»è¦ä½œç”¨æ˜¯watchæŒ‡å®šçš„k8sèµ„æºï¼Œå¹¶å°†å˜åŒ–åŒæ­¥åˆ°æœ¬åœ°æ˜¯storeä¸­ã€‚Reflectoråªä¼šæ”¾ç½®æŒ‡å®šçš„expectedTypeç±»å‹çš„èµ„æºåˆ°storeä¸­ï¼Œé™¤éexpectedTypeä¸ºnilã€‚å¦‚æœresyncPeriodä¸ä¸ºé›¶ï¼Œé‚£ä¹ˆReflectorä¸ºä»¥resyncPeriodä¸ºå‘¨æœŸå®šæœŸæ‰§è¡Œlistçš„æ“ä½œï¼Œè¿™æ ·å°±å¯ä»¥ä½¿ç”¨Reflectoræ¥å®šæœŸå¤„ç†æ‰€æœ‰çš„å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥é€æ­¥å¤„ç†å˜åŒ–çš„å¯¹è±¡ã€‚\n8.2. ListAndWatch ListAndWatchç¬¬ä¸€æ¬¡ä¼šåˆ—å‡ºæ‰€æœ‰çš„å¯¹è±¡ï¼Œå¹¶è·å–èµ„æºå¯¹è±¡çš„ç‰ˆæœ¬å·ï¼Œç„¶åwatchèµ„æºå¯¹è±¡çš„ç‰ˆæœ¬å·æ¥æŸ¥çœ‹æ˜¯å¦æœ‰è¢«å˜æ›´ã€‚é¦–å…ˆä¼šå°†èµ„æºç‰ˆæœ¬å·è®¾ç½®ä¸º0ï¼Œlist()å¯èƒ½ä¼šå¯¼è‡´æœ¬åœ°çš„ç¼“å­˜ç›¸å¯¹äºetcdé‡Œé¢çš„å†…å®¹å­˜åœ¨å»¶è¿Ÿï¼ŒReflectorä¼šé€šè¿‡watchçš„æ–¹æ³•å°†å»¶è¿Ÿçš„éƒ¨åˆ†è¡¥å……ä¸Šï¼Œä½¿å¾—æœ¬åœ°çš„ç¼“å­˜æ•°æ®ä¸etcdçš„æ•°æ®ä¿æŒä¸€è‡´ã€‚\n8.3. DeltaFIFO DeltaFIFOæ˜¯ä¸€ä¸ªç”Ÿäº§è€…ä¸æ¶ˆè´¹è€…çš„é˜Ÿåˆ—ï¼Œå…¶ä¸­Reflectoræ˜¯ç”Ÿäº§è€…ï¼Œæ¶ˆè´¹è€…è°ƒç”¨Pop()çš„æ–¹æ³•ã€‚\nDeltaFIFOä¸»è¦ç”¨åœ¨ä»¥ä¸‹åœºæ™¯ï¼š\nå¸Œæœ›å¯¹è±¡å˜æ›´æœ€å¤šå¤„ç†ä¸€æ¬¡ å¤„ç†å¯¹è±¡æ—¶ï¼Œå¸Œæœ›æŸ¥çœ‹è‡ªä¸Šæ¬¡å¤„ç†å¯¹è±¡ä»¥æ¥å‘ç”Ÿçš„æ‰€æœ‰äº‹æƒ… è¦å¤„ç†å¯¹è±¡çš„åˆ é™¤ å¸Œæœ›å®šæœŸé‡æ–°å¤„ç†å¯¹è±¡ 8.4. store Storeæ˜¯ä¸€ä¸ªé€šç”¨çš„å­˜å‚¨æ¥å£ï¼ŒReflectoré€šè¿‡watch serverçš„æ–¹å¼æ›´æ–°æ•°æ®åˆ°storeä¸­ï¼Œstoreç»™Reflectoræä¾›æœ¬åœ°çš„ç¼“å­˜ï¼Œè®©Reflectorå¯ä»¥åƒæ¶ˆæ¯é˜Ÿåˆ—ä¸€æ ·çš„å·¥ä½œã€‚\nStoreå®ç°çš„æ˜¯ä¸€ç§å¯ä»¥å‡†ç¡®çš„å†™å…¥å¯¹è±¡å’Œè·å–å¯¹è±¡çš„æœºåˆ¶ã€‚\n8.5. processor processorçš„ä¸»è¦åŠŸèƒ½å°±æ˜¯è®°å½•äº†æ‰€æœ‰çš„å›è°ƒå‡½æ•°å®ä¾‹(å³ ResourceEventHandler å®ä¾‹)ï¼Œå¹¶è´Ÿè´£è§¦å‘è¿™äº›å‡½æ•°ã€‚åœ¨sharedIndexInformer.Runéƒ¨åˆ†ä¼šè°ƒç”¨processor.runã€‚\næµç¨‹ï¼š\nlistenserçš„addå‡½æ•°è´Ÿè´£å°†notifyè£…è¿›pendingNotificationsã€‚ popå‡½æ•°å–å‡ºpendingNotificationsçš„ç¬¬ä¸€ä¸ªnofify,è¾“å‡ºåˆ°nextCh channelã€‚ runå‡½æ•°åˆ™è´Ÿè´£å–å‡ºnotifyï¼Œç„¶åæ ¹æ®notifyçš„ç±»å‹(å¢åŠ ã€åˆ é™¤ã€æ›´æ–°)è§¦å‘ç›¸åº”çš„å¤„ç†å‡½æ•°ï¼Œè¿™äº›å‡½æ•°æ˜¯åœ¨ä¸åŒçš„NewXxxcontrollerå®ç°ä¸­æ³¨å†Œçš„ã€‚ processorè®°å½•äº†ä¸åŒç±»ä¼¼çš„äº‹ä»¶å‡½æ•°ï¼Œå…¶ä¸­äº‹ä»¶å‡½æ•°åœ¨NewXxxControlleræ„é€ å‡½æ•°éƒ¨åˆ†æ³¨å†Œï¼Œå…·ä½“äº‹ä»¶å‡½æ•°çš„å¤„ç†ï¼Œä¸€èˆ¬æ˜¯å°†éœ€è¦å¤„ç†çš„å¯¹è±¡åŠ å…¥å¯¹åº”çš„controllerçš„ä»»åŠ¡é˜Ÿåˆ—ä¸­ï¼Œç„¶åç”±ç±»ä¼¼syncDeploymentçš„åŒæ­¥å‡½æ•°æ¥ç»´æŒæœŸæœ›çŠ¶æ€çš„åŒæ­¥é€»è¾‘ã€‚\n8.6. ä¸»è¦æ­¥éª¤ åœ¨controller-managerçš„Runå‡½æ•°éƒ¨åˆ†è°ƒç”¨äº†InformerFactory.Startçš„æ–¹æ³•ï¼ŒStartæ–¹æ³•åˆå§‹åŒ–å„ç§ç±»å‹çš„informerï¼Œå¹¶ä¸”æ¯ä¸ªç±»å‹èµ·äº†ä¸ªinformer.Runçš„goroutineã€‚ informer.Runçš„éƒ¨åˆ†å…ˆç”Ÿæˆä¸€ä¸ªDeltaFIFOçš„é˜Ÿåˆ—æ¥å­˜å‚¨å¯¹è±¡å˜åŒ–çš„æ•°æ®ã€‚ç„¶åè°ƒç”¨processor.Runå’Œcontroller.Runå‡½æ•°ã€‚ controller.Runå‡½æ•°ä¼šç”Ÿæˆä¸€ä¸ªReflectorï¼ŒReflectorçš„ä¸»è¦ä½œç”¨æ˜¯watchæŒ‡å®šçš„k8sèµ„æºï¼Œå¹¶å°†å˜åŒ–åŒæ­¥åˆ°æœ¬åœ°æ˜¯storeä¸­ã€‚Reflectorä»¥resyncPeriodä¸ºå‘¨æœŸå®šæœŸæ‰§è¡Œlistçš„æ“ä½œï¼Œè¿™æ ·å°±å¯ä»¥ä½¿ç”¨Reflectoræ¥å®šæœŸå¤„ç†æ‰€æœ‰çš„å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥é€æ­¥å¤„ç†å˜åŒ–çš„å¯¹è±¡ã€‚ Reflectoræ¥ç€æ‰§è¡ŒListAndWatchå‡½æ•°ï¼ŒListAndWatchç¬¬ä¸€æ¬¡ä¼šåˆ—å‡ºæ‰€æœ‰çš„å¯¹è±¡ï¼Œå¹¶è·å–èµ„æºå¯¹è±¡çš„ç‰ˆæœ¬å·ï¼Œç„¶åwatchèµ„æºå¯¹è±¡çš„ç‰ˆæœ¬å·æ¥æŸ¥çœ‹æ˜¯å¦æœ‰è¢«å˜æ›´ã€‚é¦–å…ˆä¼šå°†èµ„æºç‰ˆæœ¬å·è®¾ç½®ä¸º0ï¼Œlist()å¯èƒ½ä¼šå¯¼è‡´æœ¬åœ°çš„ç¼“å­˜ç›¸å¯¹äºetcdé‡Œé¢çš„å†…å®¹å­˜åœ¨å»¶è¿Ÿï¼ŒReflectorä¼šé€šè¿‡watchçš„æ–¹æ³•å°†å»¶è¿Ÿçš„éƒ¨åˆ†è¡¥å……ä¸Šï¼Œä½¿å¾—æœ¬åœ°çš„ç¼“å­˜æ•°æ®ä¸etcdçš„æ•°æ®ä¿æŒä¸€è‡´ã€‚ controller.Runå‡½æ•°è¿˜ä¼šè°ƒç”¨processLoopå‡½æ•°ï¼ŒprocessLoopé€šè¿‡è°ƒç”¨HandleDeltasï¼Œå†è°ƒç”¨distributeï¼ŒprocessorListener.addæœ€ç»ˆå°†ä¸åŒæ›´æ–°ç±»å‹çš„å¯¹è±¡åŠ å…¥processorListenerçš„channelä¸­ï¼Œä¾›processorListener.Runä½¿ç”¨ã€‚ processorçš„ä¸»è¦åŠŸèƒ½å°±æ˜¯è®°å½•äº†æ‰€æœ‰çš„å›è°ƒå‡½æ•°å®ä¾‹(å³ ResourceEventHandler å®ä¾‹)ï¼Œå¹¶è´Ÿè´£è§¦å‘è¿™äº›å‡½æ•°ã€‚processorè®°å½•äº†ä¸åŒç±»å‹çš„äº‹ä»¶å‡½æ•°ï¼Œå…¶ä¸­äº‹ä»¶å‡½æ•°åœ¨NewXxxControlleræ„é€ å‡½æ•°éƒ¨åˆ†æ³¨å†Œï¼Œå…·ä½“äº‹ä»¶å‡½æ•°çš„å¤„ç†ï¼Œä¸€èˆ¬æ˜¯å°†éœ€è¦å¤„ç†çš„å¯¹è±¡åŠ å…¥å¯¹åº”çš„controllerçš„ä»»åŠ¡é˜Ÿåˆ—ä¸­ï¼Œç„¶åç”±ç±»ä¼¼syncDeploymentçš„åŒæ­¥å‡½æ•°æ¥ç»´æŒæœŸæœ›çŠ¶æ€çš„åŒæ­¥é€»è¾‘ã€‚ å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes/client-go/tree/master/tools/cache\nhttps://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md\nhttps://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go\n","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æk8sä¸­å„ä¸ªæ ¸å¿ƒç»„ä»¶ç»å¸¸ä½¿ç”¨åˆ°çš„Informeræœºåˆ¶( â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/informer/","tags":["æºç åˆ†æ"],"title":"kube-controller-manageræºç åˆ†æï¼ˆä¸‰ï¼‰ä¹‹ Informeræœºåˆ¶"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/kata/","tags":"","title":"Kata Container"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kube-scheduler/","tags":"","title":"kube-scheduler"},{"body":"é€šè¿‡kubeadmæ­å»ºçš„é›†ç¾¤é»˜è®¤çš„è¯ä¹¦æ—¶é—´æ˜¯1å¹´ï¼ˆç”±äºå®˜æ–¹æœŸæœ›æ¯å¹´æ›´æ–°ä¸€æ¬¡k8sçš„ç‰ˆæœ¬ï¼Œåœ¨æ›´æ–°çš„æ—¶å€™ä¼šé»˜è®¤æ›´æ–°è¯ä¹¦ï¼‰ï¼Œå½“ä½ æ‰§è¡Œå‘½ä»¤å‡ºç°ä»¥ä¸‹æŠ¥é”™ï¼Œè¯´æ˜ä½ çš„è¯ä¹¦å·²ç»åˆ°æœŸäº†ï¼Œåˆ™éœ€è¦æ‰‹åŠ¨æ›´æ–°è¯ä¹¦ã€‚\n# kubectl get node Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2023-08-03T18:06:23+08:00 is after 2023-07-04T06:30:54Z # æˆ–è€…å‡ºç°ä»¥ä¸‹æŠ¥é”™ You must be logged in to the server(unauthorized) ä»¥ä¸‹è¯´æ˜æ‰‹åŠ¨æ›´æ–°è¯ä¹¦çš„æµç¨‹ã€‚\nå…·ä½“å¯ä»¥å‚è€ƒï¼šä½¿ç”¨ kubeadm è¿›è¡Œè¯ä¹¦ç®¡ç† | Kubernetes\n1. æ£€æŸ¥è¯ä¹¦æ˜¯å¦è¿‡æœŸ kubeadm certs check-expiration ä¼šè¾“å‡ºä»¥ä¸‹çš„å†…å®¹ï¼š\nCERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Dec 30, 2020 23:36 UTC 364d no apiserver Dec 30, 2020 23:36 UTC 364d ca no apiserver-etcd-client Dec 30, 2020 23:36 UTC 364d etcd-ca no apiserver-kubelet-client Dec 30, 2020 23:36 UTC 364d ca no controller-manager.conf Dec 30, 2020 23:36 UTC 364d no etcd-healthcheck-client Dec 30, 2020 23:36 UTC 364d etcd-ca no etcd-peer Dec 30, 2020 23:36 UTC 364d etcd-ca no etcd-server Dec 30, 2020 23:36 UTC 364d etcd-ca no front-proxy-client Dec 30, 2020 23:36 UTC 364d front-proxy-ca no scheduler.conf Dec 30, 2020 23:36 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Dec 28, 2029 23:36 UTC 9y no etcd-ca Dec 28, 2029 23:36 UTC 9y no front-proxy-ca Dec 28, 2029 23:36 UTC 9y no 2. æ‰‹åŠ¨æ›´æ–°è¿‡æœŸçš„è¯ä¹¦ åˆ†åˆ«åœ¨masterèŠ‚ç‚¹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n2.1. å¤‡ä»½/etc/kubernetesç›®å½• cp -fr /etc/kubernetes /etc/kubernetes.bak cp -fr ~/.kube ~/.kube.bak 2.2. æ‰§è¡Œæ›´æ–°è¯ä¹¦å‘½ä»¤ kubeadm certs renew all è¾“å‡ºå¦‚ä¸‹ï¼š\n# kubeadm certs renew all [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [renew] Error reading configuration from the Cluster. Falling back to default configuration certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates. 2.3. é‡å¯k8sç»„ä»¶ å…ˆé‡å¯etcd\næ³¨æ„äº‹é¡¹ï¼šéœ€è¦å°†ä¸‰ä¸ªmasterèŠ‚ç‚¹çš„è¯ä¹¦éƒ½é‡æ–°æ›´æ–°åï¼Œç„¶åä¸‰ä¸ªmasterçš„etcdæœåŠ¡ä¸€èµ·é‡å¯ï¼Œä½¿å¾—etcdé›†ç¾¤ä½¿ç”¨æ–°çš„è¯ä¹¦å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œå¦åˆ™ä¼šå¯¼è‡´kube-apiserverä¹Ÿå¯åŠ¨å¤±è´¥ã€‚\ncrictl ps |grep \"etcd\"|awk '{print $1}'|xargs crictl stop å†é‡å¯kube-apiserverã€kube-controllerã€kube-schedulerå®¹å™¨ã€‚\ncrictl ps |egrep \"kube-apiserver|kube-scheduler|kube-controller\"|awk '{print $1}'|xargs crictl stop 2.4. æ›´æ–°é»˜è®¤çš„kubeconfigæ–‡ä»¶ cp -fr /etc/kubernetes/admin.conf $HOME/.kube/config 2.5. é…ç½®kubeletè¯ä¹¦è½®è½¬ ç”±äºkubeleté»˜è®¤æ”¯æŒè¯ä¹¦è½®è½¬ï¼Œå½“è¯ä¹¦è¿‡æœŸæ—¶ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆæ–°çš„å¯†é’¥ï¼Œå¹¶ä» Kubernetes API ç”³è¯·æ–°çš„è¯ä¹¦ã€‚å¯ä»¥æŸ¥çœ‹kubeletçš„é…ç½®æ£€æŸ¥æ˜¯å¦å·²ç»å¼€å¯ã€‚\n# cat /var/lib/kubelet/config.yaml |grep rotate rotateCertificates: true 3. ä¿®æ”¹kubeadmæºç è¯ä¹¦æ—¶é—´ ç”±äºç¤¾åŒºä¸å…è®¸ç”¨æˆ·é…ç½®è¶…è¿‡1å¹´çš„è¯ä¹¦ï¼Œå› æ­¤è‡ªå®šä¹‰è¯ä¹¦æ—¶é—´çš„å‚æ•°ä¸è¢«å…è®¸å¼€å‘ã€‚\nç›¸å…³issueå¦‚ä¸‹ï¼š\nhttps://github.com/kubernetes/kubernetes/issues/119350 å¦‚æœè¦å®ç°è‡ªå®šä¹‰å‚æ•°è®¾ç½®è¯ä¹¦æ—¶é—´ï¼Œå¯å‚è€ƒä¸€ä¸‹prï¼š\nhttps://github.com/kubernetes/kubernetes/pull/100907/files å¦‚æœéœ€è¦ä¿®æ”¹kubeadmæºç è¯ä¹¦å¯ä»¥å‚è€ƒå¦‚ä¸‹ä»£ç ä¿®æ”¹ã€‚\nkubeadmä¸­è·Ÿè¯ä¹¦ç›¸å…³çš„ä»£ç æœ‰ï¼š\n3.1. caæ–‡ä»¶çš„æœ‰æ•ˆæœŸï¼ˆé»˜è®¤ä¸º10å¹´ï¼‰ ä»£ç æ–‡ä»¶ï¼š./staging/src/k8s.io/client-go/util/cert/cert.go ä¸­ NewSelfSignedCACert å‡½æ•°çš„NotAfterå­—æ®µ\nä»£ç å¦‚ä¸‹ï¼š\n// NewSelfSignedCACert creates a CA certificate func NewSelfSignedCACert(cfg Config, key crypto.Signer) (*x509.Certificate, error) { now := time.Now() // returns a uniform random value in [0, max-1), then add 1 to serial to make it a uniform random value in [1, max). serial, err := cryptorand.Int(cryptorand.Reader, new(big.Int).SetInt64(math.MaxInt64-1)) if err != nil { return nil, err } serial = new(big.Int).Add(serial, big.NewInt(1)) notBefore := now.UTC() if !cfg.NotBefore.IsZero() { notBefore = cfg.NotBefore.UTC() } tmpl := x509.Certificate{ SerialNumber: serial, Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, DNSNames: []string{cfg.CommonName}, NotBefore: notBefore, NotAfter: now.Add(duration365d * 10).UTC(), # é»˜è®¤ä¸º10å¹´ KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, IsCA: true, } certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, \u0026tmpl, \u0026tmpl, key.Public(), key) if err != nil { return nil, err } return x509.ParseCertificate(certDERBytes) } 3.2. è¯ä¹¦æ–‡ä»¶çš„æœ‰æ•ˆæœŸï¼ˆé»˜è®¤ä¸º1å¹´ï¼‰ ä»£ç æ–‡ä»¶ï¼šcmd/kubeadm/app/util/pkiutil/pki_helpers.goä¸­ NewSignedCert å‡½æ•°çš„ notAfter å­—æ®µ\nå¸¸é‡å‚æ•°kubeadmconstants.CertificateValidity ï¼š /cmd/kubeadm/app/constants/constants.go ä»£ç å¦‚ä¸‹ï¼š\n// NewSignedCert creates a signed certificate using the given CA certificate and key func NewSignedCert(cfg *CertConfig, key crypto.Signer, caCert *x509.Certificate, caKey crypto.Signer, isCA bool) (*x509.Certificate, error) { // returns a uniform random value in [0, max-1), then add 1 to serial to make it a uniform random value in [1, max). serial, err := cryptorand.Int(cryptorand.Reader, new(big.Int).SetInt64(math.MaxInt64-1)) if err != nil { return nil, err } serial = new(big.Int).Add(serial, big.NewInt(1)) if len(cfg.CommonName) == 0 { return nil, errors.New(\"must specify a CommonName\") } keyUsage := x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature if isCA { keyUsage |= x509.KeyUsageCertSign } RemoveDuplicateAltNames(\u0026cfg.AltNames) # æ­¤å¤„å¼•ç”¨äº†ä¸€ä¸ªå¸¸é‡ notAfter := time.Now().Add(kubeadmconstants.CertificateValidity).UTC() if cfg.NotAfter != nil { notAfter = *cfg.NotAfter } certTmpl := x509.Certificate{ Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, DNSNames: cfg.AltNames.DNSNames, IPAddresses: cfg.AltNames.IPs, SerialNumber: serial, NotBefore: caCert.NotBefore, NotAfter: notAfter, KeyUsage: keyUsage, ExtKeyUsage: cfg.Usages, BasicConstraintsValid: true, IsCA: isCA, } certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, \u0026certTmpl, caCert, key.Public(), caKey) if err != nil { return nil, err } return x509.ParseCertificate(certDERBytes) } å…¶ä¸­å¸¸é‡æ–‡ä»¶ä¸ºï¼š\n/cmd/kubeadm/app/constants/constants.go ä»£ç å¦‚ä¸‹ï¼š\n# å¸¸é‡é»˜è®¤è¯ä¹¦ä¸º1å¹´ã€‚ // CertificateValidity defines the validity for all the signed certificates generated by kubeadm CertificateValidity = time.Hour * 24 * 365 å¯ä»¥ä¿®æ”¹æ­¤å¤„å¸¸é‡çš„å€¼10å¹´ï¼Œä¾‹å¦‚ï¼š\n# å¸¸é‡é»˜è®¤è¯ä¹¦ä¸º1å¹´ã€‚ // CertificateValidity defines the validity for all the signed certificates generated by kubeadm CertificateValidity = time.Hour * 24 * 365 * 10 ä¿®æ”¹æºç åï¼Œå°±å¯ä»¥é‡æ–°ç¼–è¯‘kubeadmäºŒè¿›åˆ¶ã€‚ç”Ÿæˆ10å¹´çš„è¯ä¹¦æ–‡ä»¶ã€‚\nå‚è€ƒï¼š\nä½¿ç”¨ kubeadm è¿›è¡Œè¯ä¹¦ç®¡ç† | Kubernetes\nKubernetes v1.25 ç¼–è¯‘ kubeadm ä¿®æ”¹è¯ä¹¦æœ‰æ•ˆæœŸåˆ° 100 å¹´\n","categories":"","description":"","excerpt":"é€šè¿‡kubeadmæ­å»ºçš„é›†ç¾¤é»˜è®¤çš„è¯ä¹¦æ—¶é—´æ˜¯1å¹´ï¼ˆç”±äºå®˜æ–¹æœŸæœ›æ¯å¹´æ›´æ–°ä¸€æ¬¡k8sçš„ç‰ˆæœ¬ï¼Œåœ¨æ›´æ–°çš„æ—¶å€™ä¼šé»˜è®¤æ›´æ–°è¯ä¹¦ï¼‰ï¼Œå½“ä½ æ‰§è¡Œå‘½ä»¤å‡ºç°ä»¥ä¸‹æŠ¥ â€¦","ref":"/kubernetes-notes/setup/kubeadm-certs/","tags":["kubeadm"],"title":"kubeadmç®¡ç†è¯ä¹¦"},{"body":"é—®é¢˜æè¿° pvc terminating pvcåœ¨åˆ é™¤æ—¶ï¼Œå¡åœ¨terminatingä¸­ã€‚\nè§£å†³æ–¹æ³• kubectl patch pvc {PVC_NAME} -p '{\"metadata\":{\"finalizers\":null}}' ","categories":"","description":"","excerpt":"é—®é¢˜æè¿° pvc terminating pvcåœ¨åˆ é™¤æ—¶ï¼Œå¡åœ¨terminatingä¸­ã€‚\nè§£å†³æ–¹æ³• kubectl patch pvc â€¦","ref":"/kubernetes-notes/trouble-shooting/pvc-terminating/","tags":["é—®é¢˜æ’æŸ¥"],"title":"PVC Terminating"},{"body":" ä»¥ä¸‹ä¸ºredis.confçš„æ–‡ä»¶çš„ä¸­æ–‡æè¿°ï¼Œæ•´ç†äºç½‘ç»œ\n# Redis é…ç½®æ–‡ä»¶ç¤ºä¾‹ # æ³¨æ„å•ä½: å½“éœ€è¦é…ç½®å†…å­˜å¤§å°æ—¶, å¯èƒ½éœ€è¦æŒ‡å®šåƒ1k,5GB,4Mç­‰å¸¸è§æ ¼å¼ # # 1k =\u003e 1000 bytes # 1kb =\u003e 1024 bytes # 1m =\u003e 1000000 bytes # 1mb =\u003e 1024*1024 bytes # 1g =\u003e 1000000000 bytes # 1gb =\u003e 1024*1024*1024 bytes # # å•ä½æ˜¯å¯¹å¤§å°å†™ä¸æ•æ„Ÿçš„ 1GB 1Gb 1gB æ˜¯ç›¸åŒçš„ã€‚ INCLUDES ################################## INCLUDES ################################### # å¯ä»¥åœ¨è¿™é‡ŒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå…¶ä»–çš„é…ç½®æ–‡ä»¶ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªé€‚ç”¨äºæ‰€æœ‰RedisæœåŠ¡å™¨çš„æ ‡å‡†é…ç½®æ¨¡æ¿ # ä½†ä¹Ÿéœ€è¦ä¸€äº›æ¯ä¸ªæœåŠ¡å™¨è‡ªå®šä¹‰çš„è®¾ç½®ï¼Œè¿™ä¸ªåŠŸèƒ½å°†å¾ˆæœ‰ç”¨ã€‚è¢«åŒ…å«çš„é…ç½®æ–‡ä»¶ä¹Ÿå¯ä»¥åŒ…å«å…¶ä»–é…ç½®æ–‡ä»¶ï¼Œ # æ‰€ä»¥éœ€è¦è°¨æ…çš„ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚ # # æ³¨æ„â€œinclueâ€é€‰é¡¹ä¸èƒ½è¢«adminæˆ–Rediså“¨å…µçš„\"CONFIG REWRITE\"å‘½ä»¤é‡å†™ã€‚ # å› ä¸ºRedisæ€»æ˜¯ä½¿ç”¨æœ€åè§£æçš„é…ç½®è¡Œæœ€ä¸ºé…ç½®æŒ‡ä»¤çš„å€¼, ä½ æœ€å¥½åœ¨è¿™ä¸ªæ–‡ä»¶çš„å¼€å¤´é…ç½®includesæ¥ # é¿å…å®ƒåœ¨è¿è¡Œæ—¶é‡å†™é…ç½®ã€‚ # å¦‚æœç›¸åä½ æƒ³ç”¨includesçš„é…ç½®è¦†ç›–åŸæ¥çš„é…ç½®ï¼Œä½ æœ€å¥½åœ¨è¯¥æ–‡ä»¶çš„æœ€åä½¿ç”¨include # # include /path/to/local.conf # include /path/to/other.conf GENERAL ################################ GENERAL ##################################### # é»˜è®¤Rdisä¸ä¼šä½œä¸ºå®ˆæŠ¤è¿›ç¨‹è¿è¡Œã€‚å¦‚æœéœ€è¦çš„è¯é…ç½®æˆ'yes' # æ³¨æ„é…ç½®æˆå®ˆæŠ¤è¿›ç¨‹åRedisä¼šå°†è¿›ç¨‹å·å†™å…¥æ–‡ä»¶/var/run/redis.pid daemonize no # å½“ä»¥å®ˆæŠ¤è¿›ç¨‹æ–¹å¼è¿è¡Œæ—¶ï¼Œé»˜è®¤Redisä¼šæŠŠè¿›ç¨‹IDå†™åˆ° /var/run/redis.pidã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹è·¯å¾„ã€‚ pidfile /var/run/redis.pid # æ¥å—è¿æ¥çš„ç‰¹å®šç«¯å£ï¼Œé»˜è®¤æ˜¯6379 # å¦‚æœç«¯å£è®¾ç½®ä¸º0ï¼ŒRediså°±ä¸ä¼šç›‘å¬TCPå¥—æ¥å­—ã€‚ port 6379 # TCP listen() backlog. # # åœ¨é«˜å¹¶å‘ç¯å¢ƒä¸‹ä½ éœ€è¦ä¸€ä¸ªé«˜backlogå€¼æ¥é¿å…æ…¢å®¢æˆ·ç«¯è¿æ¥é—®é¢˜ã€‚æ³¨æ„Linuxå†…æ ¸é»˜é»˜åœ°å°†è¿™ä¸ªå€¼å‡å° # åˆ°/proc/sys/net/core/somaxconnçš„å€¼ï¼Œæ‰€ä»¥éœ€è¦ç¡®è®¤å¢å¤§somaxconnå’Œtcp_max_syn_backlog # ä¸¤ä¸ªå€¼æ¥è¾¾åˆ°æƒ³è¦çš„æ•ˆæœã€‚ tcp-backlog 511 # é»˜è®¤Redisç›‘å¬æœåŠ¡å™¨ä¸Šæ‰€æœ‰å¯ç”¨ç½‘ç»œæ¥å£çš„è¿æ¥ã€‚å¯ä»¥ç”¨\"bind\"é…ç½®æŒ‡ä»¤è·Ÿä¸€ä¸ªæˆ–å¤šä¸ªipåœ°å€æ¥å®ç° # ç›‘å¬ä¸€ä¸ªæˆ–å¤šä¸ªç½‘ç»œæ¥å£ # # ç¤ºä¾‹: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 # æŒ‡å®šç”¨æ¥ç›‘å¬Unixå¥—å¥—æ¥å­—çš„è·¯å¾„ã€‚æ²¡æœ‰é»˜è®¤å€¼ï¼Œ æ‰€ä»¥åœ¨æ²¡æœ‰æŒ‡å®šçš„æƒ…å†µä¸‹Redisä¸ä¼šç›‘å¬Unixå¥—æ¥å­— # # unixsocket /tmp/redis.sock # unixsocketperm 755 # ä¸€ä¸ªå®¢æˆ·ç«¯ç©ºé—²å¤šå°‘ç§’åå…³é—­è¿æ¥ã€‚(0ä»£è¡¨ç¦ç”¨ï¼Œæ°¸ä¸å…³é—­) timeout 0 # TCP keepalive. # # å¦‚æœéé›¶ï¼Œåˆ™è®¾ç½®SO_KEEPALIVEé€‰é¡¹æ¥å‘ç©ºé—²è¿æ¥çš„å®¢æˆ·ç«¯å‘é€ACKï¼Œç”±äºä»¥ä¸‹ä¸¤ä¸ªåŸå› è¿™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼š # # 1ï¼‰èƒ½å¤Ÿæ£€æµ‹æ— å“åº”çš„å¯¹ç«¯ # 2ï¼‰è®©è¯¥è¿æ¥ä¸­é—´çš„ç½‘ç»œè®¾å¤‡çŸ¥é“è¿™ä¸ªè¿æ¥è¿˜å­˜æ´» # # åœ¨Linuxä¸Šï¼Œè¿™ä¸ªæŒ‡å®šçš„å€¼(å•ä½ï¼šç§’)å°±æ˜¯å‘é€ACKçš„æ—¶é—´é—´éš”ã€‚ # æ³¨æ„ï¼šè¦å…³é—­è¿™ä¸ªè¿æ¥éœ€è¦ä¸¤å€çš„è¿™ä¸ªæ—¶é—´å€¼ã€‚ # åœ¨å…¶ä»–å†…æ ¸ä¸Šè¿™ä¸ªæ—¶é—´é—´éš”ç”±å†…æ ¸é…ç½®å†³å®š # # è¿™ä¸ªé€‰é¡¹çš„ä¸€ä¸ªåˆç†å€¼æ˜¯60ç§’ tcp-keepalive 0 # æŒ‡å®šæœåŠ¡å™¨è°ƒè¯•ç­‰çº§ # å¯èƒ½å€¼ï¼š # debug ï¼ˆå¤§é‡ä¿¡æ¯ï¼Œå¯¹å¼€å‘/æµ‹è¯•æœ‰ç”¨ï¼‰ # verbose ï¼ˆå¾ˆå¤šç²¾ç®€çš„æœ‰ç”¨ä¿¡æ¯ï¼Œä½†æ˜¯ä¸åƒdebugç­‰çº§é‚£ä¹ˆå¤šï¼‰ # notice ï¼ˆé€‚é‡çš„ä¿¡æ¯ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä½ ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦çš„ï¼‰ # warning ï¼ˆåªæœ‰å¾ˆé‡è¦/ä¸¥é‡çš„ä¿¡æ¯ä¼šè®°å½•ä¸‹æ¥ï¼‰ loglevel notice # æŒ‡æ˜æ—¥å¿—æ–‡ä»¶åã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨\"stdout\"æ¥å¼ºåˆ¶è®©RedisæŠŠæ—¥å¿—ä¿¡æ¯å†™åˆ°æ ‡å‡†è¾“å‡ºä¸Šã€‚ # æ³¨æ„:å¦‚æœRedisä»¥å®ˆæŠ¤è¿›ç¨‹æ–¹å¼è¿è¡Œï¼Œè€Œè®¾ç½®æ—¥å¿—æ˜¾ç¤ºåˆ°æ ‡å‡†è¾“å‡ºçš„è¯ï¼Œæ—¥å¿—ä¼šå‘é€åˆ°/dev/null logfile \"\" # è¦ä½¿ç”¨ç³»ç»Ÿæ—¥å¿—è®°å½•å™¨ï¼Œåªè¦è®¾ç½® \"syslog-enabled\" ä¸º \"yes\" å°±å¯ä»¥äº†ã€‚ # ç„¶åæ ¹æ®éœ€è¦è®¾ç½®å…¶ä»–ä¸€äº›syslogå‚æ•°å°±å¯ä»¥äº†ã€‚ # syslog-enabled no # æŒ‡æ˜syslogèº«ä»½ # syslog-ident redis # æŒ‡æ˜syslogçš„è®¾å¤‡ã€‚å¿…é¡»æ˜¯useræˆ–LOCAL0 ~ LOCAL7ä¹‹ä¸€ã€‚ # syslog-facility local0 # è®¾ç½®æ•°æ®åº“ä¸ªæ•°ã€‚é»˜è®¤æ•°æ®åº“æ˜¯ DB 0ï¼Œ # å¯ä»¥é€šè¿‡select \u003cdbid\u003e (0 \u003c= dbid \u003c= 'databases' - 1 ï¼‰æ¥ä¸ºæ¯ä¸ªè¿æ¥ä½¿ç”¨ä¸åŒçš„æ•°æ®åº“ã€‚ databases 16 SNAPSHOTTING ################################ SNAPSHOTTING ################################ # # æŠŠæ•°æ®åº“å­˜åˆ°ç£ç›˜ä¸Š: # # save \u003cseconds\u003e \u003cchanges\u003e # # ä¼šåœ¨æŒ‡å®šç§’æ•°å’Œæ•°æ®å˜åŒ–æ¬¡æ•°ä¹‹åæŠŠæ•°æ®åº“å†™åˆ°ç£ç›˜ä¸Šã€‚ # # ä¸‹é¢çš„ä¾‹å­å°†ä¼šè¿›è¡ŒæŠŠæ•°æ®å†™å…¥ç£ç›˜çš„æ“ä½œ: # 900ç§’ï¼ˆ15åˆ†é’Ÿï¼‰ä¹‹åï¼Œä¸”è‡³å°‘1æ¬¡å˜æ›´ # 300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰ä¹‹åï¼Œä¸”è‡³å°‘10æ¬¡å˜æ›´ # 60ç§’ä¹‹åï¼Œä¸”è‡³å°‘10000æ¬¡å˜æ›´ # # æ³¨æ„ï¼šä½ è¦æƒ³ä¸å†™ç£ç›˜çš„è¯å°±æŠŠæ‰€æœ‰ \"save\" è®¾ç½®æ³¨é‡Šæ‰å°±è¡Œäº†ã€‚ # # é€šè¿‡æ·»åŠ ä¸€æ¡å¸¦ç©ºå­—ç¬¦ä¸²å‚æ•°çš„saveæŒ‡ä»¤ä¹Ÿèƒ½ç§»é™¤ä¹‹å‰æ‰€æœ‰é…ç½®çš„saveæŒ‡ä»¤ # åƒä¸‹é¢çš„ä¾‹å­ï¼š # save \"\" save 900 1 save 300 10 save 60 10000 # é»˜è®¤å¦‚æœå¼€å¯RDBå¿«ç…§(è‡³å°‘ä¸€æ¡saveæŒ‡ä»¤)å¹¶ä¸”æœ€æ–°çš„åå°ä¿å­˜å¤±è´¥ï¼ŒRediså°†ä¼šåœæ­¢æ¥å—å†™æ“ä½œ # è¿™å°†ä½¿ç”¨æˆ·çŸ¥é“æ•°æ®æ²¡æœ‰æ­£ç¡®çš„æŒä¹…åŒ–åˆ°ç¡¬ç›˜ï¼Œå¦åˆ™å¯èƒ½æ²¡äººæ³¨æ„åˆ°å¹¶ä¸”é€ æˆä¸€äº›ç¾éš¾ã€‚ # # å¦‚æœåå°ä¿å­˜è¿›ç¨‹èƒ½é‡æ–°å¼€å§‹å·¥ä½œï¼ŒRediså°†è‡ªåŠ¨å…è®¸å†™æ“ä½œ # # ç„¶è€Œå¦‚æœä½ å·²ç»éƒ¨ç½²äº†é€‚å½“çš„RedisæœåŠ¡å™¨å’ŒæŒä¹…åŒ–çš„ç›‘æ§ï¼Œä½ å¯èƒ½æƒ³å…³æ‰è¿™ä¸ªåŠŸèƒ½ä»¥ä¾¿äºå³ä½¿æ˜¯ # ç¡¬ç›˜ï¼Œæƒé™ç­‰å‡ºé—®é¢˜äº†Redisä¹Ÿèƒ½å¤Ÿåƒå¹³æ—¶ä¸€æ ·æ­£å¸¸å·¥ä½œï¼Œ stop-writes-on-bgsave-error yes # å½“å¯¼å‡ºåˆ° .rdb æ•°æ®åº“æ—¶æ˜¯å¦ç”¨LZFå‹ç¼©å­—ç¬¦ä¸²å¯¹è±¡ï¼Ÿ # é»˜è®¤è®¾ç½®ä¸º \"yes\"ï¼Œå› ä¸ºå‡ ä¹åœ¨ä»»ä½•æƒ…å†µä¸‹å®ƒéƒ½æ˜¯ä¸é”™çš„ã€‚ # å¦‚æœä½ æƒ³èŠ‚çœCPUçš„è¯ä½ å¯ä»¥æŠŠè¿™ä¸ªè®¾ç½®ä¸º \"no\"ï¼Œä½†æ˜¯å¦‚æœä½ æœ‰å¯å‹ç¼©çš„keyå’Œvalueçš„è¯ï¼Œ # é‚£æ•°æ®æ–‡ä»¶å°±ä¼šæ›´å¤§äº†ã€‚ rdbcompression yes # å› ä¸ºç‰ˆæœ¬5çš„RDBæœ‰ä¸€ä¸ªCRC64ç®—æ³•çš„æ ¡éªŒå’Œæ”¾åœ¨äº†æ–‡ä»¶çš„æœ€åã€‚è¿™å°†ä½¿æ–‡ä»¶æ ¼å¼æ›´åŠ å¯é ä½†åœ¨ # ç”Ÿäº§å’ŒåŠ è½½RDBæ–‡ä»¶æ—¶ï¼Œè¿™æœ‰ä¸€ä¸ªæ€§èƒ½æ¶ˆè€—(å¤§çº¦10%)ï¼Œæ‰€ä»¥ä½ å¯ä»¥å…³æ‰å®ƒæ¥è·å–æœ€å¥½çš„æ€§èƒ½ã€‚ # # ç”Ÿæˆçš„å…³é—­æ ¡éªŒçš„RDBæ–‡ä»¶æœ‰ä¸€ä¸ª0çš„æ ¡éªŒå’Œï¼Œå®ƒå°†å‘Šè¯‰åŠ è½½ä»£ç è·³è¿‡æ£€æŸ¥ rdbchecksum yes # æŒä¹…åŒ–æ•°æ®åº“çš„æ–‡ä»¶å dbfilename dump.rdb # å·¥ä½œç›®å½• # # æ•°æ®åº“ä¼šå†™åˆ°è¿™ä¸ªç›®å½•ä¸‹ï¼Œæ–‡ä»¶åå°±æ˜¯ä¸Šé¢çš„ \"dbfilename\" çš„å€¼ã€‚ # # ç´¯åŠ æ–‡ä»¶ä¹Ÿæ”¾è¿™é‡Œã€‚ # # æ³¨æ„ä½ è¿™é‡ŒæŒ‡å®šçš„å¿…é¡»æ˜¯ç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶åã€‚ dir ./ REPLICATION ################################# REPLICATION ################################# # ä¸»ä»åŒæ­¥ã€‚é€šè¿‡ slaveof æŒ‡ä»¤æ¥å®ç°Rediså®ä¾‹çš„å¤‡ä»½ã€‚ # æ³¨æ„ï¼Œè¿™é‡Œæ˜¯æœ¬åœ°ä»è¿œç«¯å¤åˆ¶æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœ¬åœ°å¯ä»¥æœ‰ä¸åŒçš„æ•°æ®åº“æ–‡ä»¶ã€ç»‘å®šä¸åŒçš„IPã€ç›‘å¬ # ä¸åŒçš„ç«¯å£ã€‚ # # slaveof \u003cmasterip\u003e \u003cmasterport\u003e # å¦‚æœmasterè®¾ç½®äº†å¯†ç ä¿æŠ¤ï¼ˆé€šè¿‡ \"requirepass\" é€‰é¡¹æ¥é…ç½®ï¼‰ï¼Œé‚£ä¹ˆslaveåœ¨å¼€å§‹åŒæ­¥ä¹‹å‰å¿…é¡» # è¿›è¡Œèº«ä»½éªŒè¯ï¼Œå¦åˆ™å®ƒçš„åŒæ­¥è¯·æ±‚ä¼šè¢«æ‹’ç»ã€‚ # # masterauth \u003cmaster-password\u003e # å½“ä¸€ä¸ªslaveå¤±å»å’Œmasterçš„è¿æ¥ï¼Œæˆ–è€…åŒæ­¥æ­£åœ¨è¿›è¡Œä¸­ï¼Œslaveçš„è¡Œä¸ºæœ‰ä¸¤ç§å¯èƒ½ï¼š # # 1) å¦‚æœ slave-serve-stale-data è®¾ç½®ä¸º \"yes\" (é»˜è®¤å€¼)ï¼Œslaveä¼šç»§ç»­å“åº”å®¢æˆ·ç«¯è¯·æ±‚ï¼Œ # å¯èƒ½æ˜¯æ­£å¸¸æ•°æ®ï¼Œä¹Ÿå¯èƒ½æ˜¯è¿˜æ²¡è·å¾—å€¼çš„ç©ºæ•°æ®ã€‚ # 2) å¦‚æœ slave-serve-stale-data è®¾ç½®ä¸º \"no\"ï¼Œslaveä¼šå›å¤\"æ­£åœ¨ä»masteråŒæ­¥ # ï¼ˆSYNC with master in progressï¼‰\"æ¥å¤„ç†å„ç§è¯·æ±‚ï¼Œé™¤äº† INFO å’Œ SLAVEOF å‘½ä»¤ã€‚ # slave-serve-stale-data yes # ä½ å¯ä»¥é…ç½®salveå®ä¾‹æ˜¯å¦æ¥å—å†™æ“ä½œã€‚å¯å†™çš„slaveå®ä¾‹å¯èƒ½å¯¹å­˜å‚¨ä¸´æ—¶æ•°æ®æ¯”è¾ƒæœ‰ç”¨(å› ä¸ºå†™å…¥salve # çš„æ•°æ®åœ¨åŒmasteråŒæ­¥ä¹‹åå°†å¾ˆå®¹è¢«åˆ é™¤)ï¼Œä½†æ˜¯å¦‚æœå®¢æˆ·ç«¯ç”±äºé…ç½®é”™è¯¯åœ¨å†™å…¥æ—¶ä¹Ÿå¯èƒ½äº§ç”Ÿä¸€äº›é—®é¢˜ã€‚ # # ä»Redis2.6é»˜è®¤æ‰€æœ‰çš„slaveä¸ºåªè¯» # # æ³¨æ„:åªè¯»çš„slaveä¸æ˜¯ä¸ºäº†æš´éœ²ç»™äº’è”ç½‘ä¸Šä¸å¯ä¿¡çš„å®¢æˆ·ç«¯è€Œè®¾è®¡çš„ã€‚å®ƒåªæ˜¯ä¸€ä¸ªé˜²æ­¢å®ä¾‹è¯¯ç”¨çš„ä¿æŠ¤å±‚ã€‚ # ä¸€ä¸ªåªè¯»çš„slaveæ”¯æŒæ‰€æœ‰çš„ç®¡ç†å‘½ä»¤æ¯”å¦‚config,debugç­‰ã€‚ä¸ºäº†é™åˆ¶ä½ å¯ä»¥ç”¨'rename-command'æ¥ # éšè—æ‰€æœ‰çš„ç®¡ç†å’Œå±é™©å‘½ä»¤æ¥å¢å¼ºåªè¯»slaveçš„å®‰å…¨æ€§ slave-read-only yes # slaveæ ¹æ®æŒ‡å®šçš„æ—¶é—´é—´éš”å‘masterå‘é€pingè¯·æ±‚ã€‚ # æ—¶é—´é—´éš”å¯ä»¥é€šè¿‡ repl_ping_slave_period æ¥è®¾ç½®ã€‚ # é»˜è®¤10ç§’ã€‚ # # repl-ping-slave-period 10 # ä»¥ä¸‹é€‰é¡¹è®¾ç½®åŒæ­¥çš„è¶…æ—¶æ—¶é—´ # # 1ï¼‰slaveåœ¨ä¸master SYNCæœŸé—´æœ‰å¤§é‡æ•°æ®ä¼ è¾“ï¼Œé€ æˆè¶…æ—¶ # 2ï¼‰åœ¨slaveè§’åº¦ï¼Œmasterè¶…æ—¶ï¼ŒåŒ…æ‹¬æ•°æ®ã€pingç­‰ # 3ï¼‰åœ¨masterè§’åº¦ï¼Œslaveè¶…æ—¶ï¼Œå½“masterå‘é€REPLCONF ACK pings # # ç¡®ä¿è¿™ä¸ªå€¼å¤§äºæŒ‡å®šçš„repl-ping-slave-periodï¼Œå¦åˆ™åœ¨ä¸»ä»é—´æµé‡ä¸é«˜æ—¶æ¯æ¬¡éƒ½ä¼šæ£€æµ‹åˆ°è¶…æ—¶ # # repl-timeout 60 # æ˜¯å¦åœ¨slaveå¥—æ¥å­—å‘é€SYNCä¹‹åç¦ç”¨ TCP_NODELAY ï¼Ÿ # # å¦‚æœä½ é€‰æ‹©â€œyesâ€Rediså°†ä½¿ç”¨æ›´å°‘çš„TCPåŒ…å’Œå¸¦å®½æ¥å‘slaveså‘é€æ•°æ®ã€‚ä½†æ˜¯è¿™å°†ä½¿æ•°æ®ä¼ è¾“åˆ°slave # ä¸Šæœ‰å»¶è¿Ÿï¼ŒLinuxå†…æ ¸çš„é»˜è®¤é…ç½®ä¼šè¾¾åˆ°40æ¯«ç§’ # # å¦‚æœä½ é€‰æ‹©äº† \"no\" æ•°æ®ä¼ è¾“åˆ°salveçš„å»¶è¿Ÿå°†ä¼šå‡å°‘ä½†è¦ä½¿ç”¨æ›´å¤šçš„å¸¦å®½ # # é»˜è®¤æˆ‘ä»¬ä¼šä¸ºä½å»¶è¿Ÿåšä¼˜åŒ–ï¼Œä½†é«˜æµé‡æƒ…å†µæˆ–ä¸»ä»ä¹‹é—´çš„è·³æ•°è¿‡å¤šæ—¶ï¼ŒæŠŠè¿™ä¸ªé€‰é¡¹è®¾ç½®ä¸ºâ€œyesâ€ # æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ã€‚ repl-disable-tcp-nodelay no # è®¾ç½®æ•°æ®å¤‡ä»½çš„backlogå¤§å°ã€‚backlogæ˜¯ä¸€ä¸ªslaveåœ¨ä¸€æ®µæ—¶é—´å†…æ–­å¼€è¿æ¥æ—¶è®°å½•salveæ•°æ®çš„ç¼“å†²ï¼Œ # æ‰€ä»¥ä¸€ä¸ªslaveåœ¨é‡æ–°è¿æ¥æ—¶ï¼Œä¸å¿…è¦å…¨é‡çš„åŒæ­¥ï¼Œè€Œæ˜¯ä¸€ä¸ªå¢é‡åŒæ­¥å°±è¶³å¤Ÿäº†ï¼Œå°†åœ¨æ–­å¼€è¿æ¥çš„è¿™æ®µ # æ—¶é—´å†…slaveä¸¢å¤±çš„éƒ¨åˆ†æ•°æ®ä¼ é€ç»™å®ƒã€‚ # # åŒæ­¥çš„backlogè¶Šå¤§ï¼Œslaveèƒ½å¤Ÿè¿›è¡Œå¢é‡åŒæ­¥å¹¶ä¸”å…è®¸æ–­å¼€è¿æ¥çš„æ—¶é—´å°±è¶Šé•¿ã€‚ # # backlogåªåˆ†é…ä¸€æ¬¡å¹¶ä¸”è‡³å°‘éœ€è¦ä¸€ä¸ªslaveè¿æ¥ # # repl-backlog-size 1mb # å½“masteråœ¨ä¸€æ®µæ—¶é—´å†…ä¸å†ä¸ä»»ä½•slaveè¿æ¥ï¼Œbacklogå°†ä¼šé‡Šæ”¾ã€‚ä»¥ä¸‹é€‰é¡¹é…ç½®äº†ä»æœ€åä¸€ä¸ª # slaveæ–­å¼€å¼€å§‹è®¡æ—¶å¤šå°‘ç§’åï¼Œbacklogç¼“å†²å°†ä¼šé‡Šæ”¾ã€‚ # # 0è¡¨ç¤ºæ°¸ä¸é‡Šæ”¾backlog # # repl-backlog-ttl 3600 # slaveçš„ä¼˜å…ˆçº§æ˜¯ä¸€ä¸ªæ•´æ•°å±•ç¤ºåœ¨Redisçš„Infoè¾“å‡ºä¸­ã€‚å¦‚æœmasterä¸å†æ­£å¸¸å·¥ä½œäº†ï¼Œå“¨å…µå°†ç”¨å®ƒæ¥ # é€‰æ‹©ä¸€ä¸ªslaveæå‡=å‡ä¸ºmasterã€‚ # # ä¼˜å…ˆçº§æ•°å­—å°çš„salveä¼šä¼˜å…ˆè€ƒè™‘æå‡ä¸ºmasterï¼Œæ‰€ä»¥ä¾‹å¦‚æœ‰ä¸‰ä¸ªslaveä¼˜å…ˆçº§åˆ†åˆ«ä¸º10ï¼Œ100ï¼Œ25ï¼Œ # å“¨å…µå°†æŒ‘é€‰ä¼˜å…ˆçº§æœ€å°æ•°å­—ä¸º10çš„slaveã€‚ # # 0ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„ä¼˜å…ˆçº§ï¼Œæ ‡è¯†è¿™ä¸ªslaveä¸èƒ½ä½œä¸ºmasterï¼Œæ‰€ä»¥ä¸€ä¸ªä¼˜å…ˆçº§ä¸º0çš„slaveæ°¸è¿œä¸ä¼šè¢« # å“¨å…µæŒ‘é€‰æå‡ä¸ºmaster # # é»˜è®¤ä¼˜å…ˆçº§ä¸º100 slave-priority 100 # å¦‚æœmasterå°‘äºNä¸ªå»¶æ—¶å°äºç­‰äºMç§’çš„å·²è¿æ¥slaveï¼Œå°±å¯ä»¥åœæ­¢æ¥æ”¶å†™æ“ä½œã€‚ # # Nä¸ªslaveéœ€è¦æ˜¯â€œonelineâ€çŠ¶æ€ # # å»¶æ—¶æ˜¯ä»¥ç§’ä¸ºå•ä½ï¼Œå¹¶ä¸”å¿…é¡»å°äºç­‰äºæŒ‡å®šå€¼ï¼Œæ˜¯ä»æœ€åä¸€ä¸ªä»slaveæ¥æ”¶åˆ°çš„pingï¼ˆé€šå¸¸æ¯ç§’å‘é€ï¼‰ # å¼€å§‹è®¡æ•°ã€‚ # # This option does not GUARANTEES that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough slaves # are available, to the specified number of seconds. # # ä¾‹å¦‚è‡³å°‘éœ€è¦3ä¸ªå»¶æ—¶å°äºç­‰äº10ç§’çš„slaveç”¨ä¸‹é¢çš„æŒ‡ä»¤ï¼š # # min-slaves-to-write 3 # min-slaves-max-lag 10 # # ä¸¤è€…ä¹‹ä¸€è®¾ç½®ä¸º0å°†ç¦ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚ # # é»˜è®¤ min-slaves-to-write å€¼æ˜¯0ï¼ˆè¯¥åŠŸèƒ½ç¦ç”¨ï¼‰å¹¶ä¸” min-slaves-max-lag å€¼æ˜¯10ã€‚ SECURITY ################################## SECURITY ################################### # è¦æ±‚å®¢æˆ·ç«¯åœ¨å¤„ç†ä»»ä½•å‘½ä»¤æ—¶éƒ½è¦éªŒè¯èº«ä»½å’Œå¯†ç ã€‚ # è¿™ä¸ªåŠŸèƒ½åœ¨æœ‰ä½ ä¸ä¿¡ä»»çš„å…¶å®ƒå®¢æˆ·ç«¯èƒ½å¤Ÿè®¿é—®redisæœåŠ¡å™¨çš„ç¯å¢ƒé‡Œéå¸¸æœ‰ç”¨ã€‚ # # ä¸ºäº†å‘åå…¼å®¹çš„è¯è¿™æ®µåº”è¯¥æ³¨é‡Šæ‰ã€‚è€Œä¸”å¤§å¤šæ•°äººä¸éœ€è¦èº«ä»½éªŒè¯(ä¾‹å¦‚:å®ƒä»¬è¿è¡Œåœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Š) # # è­¦å‘Šï¼šå› ä¸ºRediså¤ªå¿«äº†ï¼Œæ‰€ä»¥å¤–é¢çš„äººå¯ä»¥å°è¯•æ¯ç§’150kçš„å¯†ç æ¥è¯•å›¾ç ´è§£å¯†ç ã€‚è¿™æ„å‘³ç€ä½ éœ€è¦ # ä¸€ä¸ªé«˜å¼ºåº¦çš„å¯†ç ï¼Œå¦åˆ™ç ´è§£å¤ªå®¹æ˜“äº†ã€‚ # # requirepass foobared # å‘½ä»¤é‡å‘½å # # åœ¨å…±äº«ç¯å¢ƒä¸‹ï¼Œå¯ä»¥ä¸ºå±é™©å‘½ä»¤æ”¹å˜åå­—ã€‚æ¯”å¦‚ï¼Œä½ å¯ä»¥ä¸º CONFIG æ”¹ä¸ªå…¶ä»–ä¸å¤ªå®¹æ˜“çŒœåˆ°çš„åå­—ï¼Œ # è¿™æ ·å†…éƒ¨çš„å·¥å…·ä»ç„¶å¯ä»¥ä½¿ç”¨ï¼Œè€Œæ™®é€šçš„å®¢æˆ·ç«¯å°†ä¸è¡Œã€‚ # # ä¾‹å¦‚ï¼š # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # ä¹Ÿå¯ä»¥é€šè¿‡æ”¹åä¸ºç©ºå­—ç¬¦ä¸²æ¥å®Œå…¨ç¦ç”¨ä¸€ä¸ªå‘½ä»¤ # # rename-command CONFIG \"\" # # è¯·æ³¨æ„ï¼šæ”¹å˜å‘½ä»¤åå­—è¢«è®°å½•åˆ°AOFæ–‡ä»¶æˆ–è¢«ä¼ é€åˆ°ä»æœåŠ¡å™¨å¯èƒ½äº§ç”Ÿé—®é¢˜ã€‚ LIMITS ################################### LIMITS #################################### # è®¾ç½®æœ€å¤šåŒæ—¶è¿æ¥çš„å®¢æˆ·ç«¯æ•°é‡ã€‚é»˜è®¤è¿™ä¸ªé™åˆ¶æ˜¯10000ä¸ªå®¢æˆ·ç«¯ï¼Œç„¶è€Œå¦‚æœRedisæœåŠ¡å™¨ä¸èƒ½é…ç½® # å¤„ç†æ–‡ä»¶çš„é™åˆ¶æ•°æ¥æ»¡è¶³æŒ‡å®šçš„å€¼ï¼Œé‚£ä¹ˆæœ€å¤§çš„å®¢æˆ·ç«¯è¿æ¥æ•°å°±è¢«è®¾ç½®æˆå½“å‰æ–‡ä»¶é™åˆ¶æ•°å‡32ï¼ˆå›  # ä¸ºRedisæœåŠ¡å™¨ä¿ç•™äº†ä¸€äº›æ–‡ä»¶æè¿°ç¬¦ä½œä¸ºå†…éƒ¨ä½¿ç”¨ï¼‰ # # ä¸€æ—¦è¾¾åˆ°è¿™ä¸ªé™åˆ¶ï¼ŒRedisä¼šå…³é—­æ‰€æœ‰æ–°è¿æ¥å¹¶å‘é€é”™è¯¯'max number of clients reached' # # maxclients 10000 # ä¸è¦ç”¨æ¯”è®¾ç½®çš„ä¸Šé™æ›´å¤šçš„å†…å­˜ã€‚ä¸€æ—¦å†…å­˜ä½¿ç”¨è¾¾åˆ°ä¸Šé™ï¼ŒRedisä¼šæ ¹æ®é€‰å®šçš„å›æ”¶ç­–ç•¥ï¼ˆå‚è§ï¼š # maxmemmory-policyï¼‰åˆ é™¤key # # å¦‚æœå› ä¸ºåˆ é™¤ç­–ç•¥Redisæ— æ³•åˆ é™¤keyï¼Œæˆ–è€…ç­–ç•¥è®¾ç½®ä¸º \"noeviction\"ï¼ŒRedisä¼šå›å¤éœ€è¦æ›´ # å¤šå†…å­˜çš„é”™è¯¯ä¿¡æ¯ç»™å‘½ä»¤ã€‚ä¾‹å¦‚ï¼ŒSET,LPUSHç­‰ç­‰ï¼Œä½†æ˜¯ä¼šç»§ç»­å“åº”åƒGetè¿™æ ·çš„åªè¯»å‘½ä»¤ã€‚ # # åœ¨ä½¿ç”¨Redisä½œä¸ºLRUç¼“å­˜ï¼Œæˆ–è€…ä¸ºå®ä¾‹è®¾ç½®äº†ç¡¬æ€§å†…å­˜é™åˆ¶çš„æ—¶å€™ï¼ˆä½¿ç”¨ \"noeviction\" ç­–ç•¥ï¼‰ # çš„æ—¶å€™ï¼Œè¿™ä¸ªé€‰é¡¹é€šå¸¸äº‹å¾ˆæœ‰ç”¨çš„ã€‚ # # è­¦å‘Šï¼šå½“æœ‰å¤šä¸ªslaveè¿ä¸Šè¾¾åˆ°å†…å­˜ä¸Šé™çš„å®ä¾‹æ—¶ï¼Œmasterä¸ºåŒæ­¥slaveçš„è¾“å‡ºç¼“å†²åŒºæ‰€éœ€ # å†…å­˜ä¸è®¡ç®—åœ¨ä½¿ç”¨å†…å­˜ä¸­ã€‚è¿™æ ·å½“é©±é€keyæ—¶ï¼Œå°±ä¸ä¼šå› ç½‘ç»œé—®é¢˜ / é‡æ–°åŒæ­¥äº‹ä»¶è§¦å‘é©±é€key # çš„å¾ªç¯ï¼Œåè¿‡æ¥slavesçš„è¾“å‡ºç¼“å†²åŒºå……æ»¡äº†keyè¢«é©±é€çš„DELå‘½ä»¤ï¼Œè¿™å°†è§¦å‘åˆ é™¤æ›´å¤šçš„keyï¼Œ # ç›´åˆ°è¿™ä¸ªæ•°æ®åº“å®Œå…¨è¢«æ¸…ç©ºä¸ºæ­¢ # # æ€»ä¹‹...å¦‚æœä½ éœ€è¦é™„åŠ å¤šä¸ªslaveï¼Œå»ºè®®ä½ è®¾ç½®ä¸€ä¸ªç¨å°maxmemoryé™åˆ¶ï¼Œè¿™æ ·ç³»ç»Ÿå°±ä¼šæœ‰ç©ºé—² # çš„å†…å­˜ä½œä¸ºslaveçš„è¾“å‡ºç¼“å­˜åŒº(ä½†æ˜¯å¦‚æœæœ€å¤§å†…å­˜ç­–ç•¥è®¾ç½®ä¸º\"noeviction\"çš„è¯å°±æ²¡å¿…è¦äº†) # # maxmemory \u003cbytes\u003e # æœ€å¤§å†…å­˜ç­–ç•¥ï¼šå¦‚æœè¾¾åˆ°å†…å­˜é™åˆ¶äº†ï¼ŒRediså¦‚ä½•é€‰æ‹©åˆ é™¤keyã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢äº”ä¸ªè¡Œä¸ºé‡Œé€‰ï¼š # # volatile-lru -\u003e æ ¹æ®LRUç®—æ³•ç”Ÿæˆçš„è¿‡æœŸæ—¶é—´æ¥åˆ é™¤ã€‚ # allkeys-lru -\u003e æ ¹æ®LRUç®—æ³•åˆ é™¤ä»»ä½•keyã€‚ # volatile-random -\u003e æ ¹æ®è¿‡æœŸè®¾ç½®æ¥éšæœºåˆ é™¤keyã€‚ # allkeys-\u003erandom -\u003e æ— å·®åˆ«éšæœºåˆ ã€‚ # volatile-ttl -\u003e æ ¹æ®æœ€è¿‘è¿‡æœŸæ—¶é—´æ¥åˆ é™¤ï¼ˆè¾…ä»¥TTLï¼‰ # noeviction -\u003e è°ä¹Ÿä¸åˆ ï¼Œç›´æ¥åœ¨å†™æ“ä½œæ—¶è¿”å›é”™è¯¯ã€‚ # # æ³¨æ„ï¼šå¯¹æ‰€æœ‰ç­–ç•¥æ¥è¯´ï¼Œå¦‚æœRedisæ‰¾ä¸åˆ°åˆé€‚çš„å¯ä»¥åˆ é™¤çš„keyéƒ½ä¼šåœ¨å†™æ“ä½œæ—¶è¿”å›ä¸€ä¸ªé”™è¯¯ã€‚ # # ç›®å‰ä¸ºæ­¢æ¶‰åŠçš„å‘½ä»¤ï¼šset setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # é»˜è®¤å€¼å¦‚ä¸‹ï¼š # # maxmemory-policy volatile-lru # LRUå’Œæœ€å°TTLç®—æ³•çš„å®ç°éƒ½ä¸æ˜¯å¾ˆç²¾ç¡®ï¼Œä½†æ˜¯å¾ˆæ¥è¿‘ï¼ˆä¸ºäº†çœå†…å­˜ï¼‰ï¼Œæ‰€ä»¥ä½ å¯ä»¥ç”¨æ ·æœ¬é‡åšæ£€æµ‹ã€‚ # ä¾‹å¦‚ï¼šé»˜è®¤Redisä¼šæ£€æŸ¥3ä¸ªkeyç„¶åå–æœ€æ—§çš„é‚£ä¸ªï¼Œä½ å¯ä»¥é€šè¿‡ä¸‹é¢çš„é…ç½®æŒ‡ä»¤æ¥è®¾ç½®æ ·æœ¬çš„ä¸ªæ•°ã€‚ # # maxmemory-samples 3 APPEND ONLY MODE ############################## APPEND ONLY MODE ############################### # é»˜è®¤æƒ…å†µä¸‹ï¼ŒRedisæ˜¯å¼‚æ­¥çš„æŠŠæ•°æ®å¯¼å‡ºåˆ°ç£ç›˜ä¸Šã€‚è¿™ç§æ¨¡å¼åœ¨å¾ˆå¤šåº”ç”¨é‡Œå·²ç»è¶³å¤Ÿå¥½ï¼Œä½†Redisè¿›ç¨‹ # å‡ºé—®é¢˜æˆ–æ–­ç”µæ—¶å¯èƒ½é€ æˆä¸€æ®µæ—¶é—´çš„å†™æ“ä½œä¸¢å¤±(è¿™å–å†³äºé…ç½®çš„saveæŒ‡ä»¤)ã€‚ # # AOFæ˜¯ä¸€ç§æä¾›äº†æ›´å¯é çš„æ›¿ä»£æŒä¹…åŒ–æ¨¡å¼ï¼Œä¾‹å¦‚ä½¿ç”¨é»˜è®¤çš„æ•°æ®å†™å…¥æ–‡ä»¶ç­–ç•¥ï¼ˆå‚è§åé¢çš„é…ç½®ï¼‰ # åœ¨é‡åˆ°åƒæœåŠ¡å™¨æ–­ç”µæˆ–å•å†™æƒ…å†µä¸‹Redisè‡ªèº«è¿›ç¨‹å‡ºé—®é¢˜ä½†æ“ä½œç³»ç»Ÿä»æ­£å¸¸è¿è¡Œç­‰çªå‘äº‹ä»¶æ—¶ï¼ŒRedis # èƒ½åªä¸¢å¤±1ç§’çš„å†™æ“ä½œã€‚ # # AOFå’ŒRDBæŒä¹…åŒ–èƒ½åŒæ—¶å¯åŠ¨å¹¶ä¸”ä¸ä¼šæœ‰é—®é¢˜ã€‚ # å¦‚æœAOFå¼€å¯ï¼Œé‚£ä¹ˆåœ¨å¯åŠ¨æ—¶Rediså°†åŠ è½½AOFæ–‡ä»¶ï¼Œå®ƒæ›´èƒ½ä¿è¯æ•°æ®çš„å¯é æ€§ã€‚ # # è¯·æŸ¥çœ‹ http://redis.io/topics/persistence æ¥è·å–æ›´å¤šä¿¡æ¯. appendonly no # çº¯ç´¯åŠ æ–‡ä»¶åå­—ï¼ˆé»˜è®¤ï¼š\"appendonly.aof\"ï¼‰ appendfilename \"appendonly.aof\" # fsync() ç³»ç»Ÿè°ƒç”¨å‘Šè¯‰æ“ä½œç³»ç»ŸæŠŠæ•°æ®å†™åˆ°ç£ç›˜ä¸Šï¼Œè€Œä¸æ˜¯ç­‰æ›´å¤šçš„æ•°æ®è¿›å…¥è¾“å‡ºç¼“å†²åŒºã€‚ # æœ‰äº›æ“ä½œç³»ç»Ÿä¼šçœŸçš„æŠŠæ•°æ®é©¬ä¸Šåˆ·åˆ°ç£ç›˜ä¸Šï¼›æœ‰äº›åˆ™ä¼šå°½å¿«å»å°è¯•è¿™ä¹ˆåšã€‚ # # Redisæ”¯æŒä¸‰ç§ä¸åŒçš„æ¨¡å¼ï¼š # # noï¼šä¸è¦ç«‹åˆ»åˆ·ï¼Œåªæœ‰åœ¨æ“ä½œç³»ç»Ÿéœ€è¦åˆ·çš„æ—¶å€™å†åˆ·ã€‚æ¯”è¾ƒå¿«ã€‚ # alwaysï¼šæ¯æ¬¡å†™æ“ä½œéƒ½ç«‹åˆ»å†™å…¥åˆ°aofæ–‡ä»¶ã€‚æ…¢ï¼Œä½†æ˜¯æœ€å®‰å…¨ã€‚ # everysecï¼šæ¯ç§’å†™ä¸€æ¬¡ã€‚æŠ˜ä¸­æ–¹æ¡ˆã€‚ # # é»˜è®¤çš„ \"everysec\" é€šå¸¸æ¥è¯´èƒ½åœ¨é€Ÿåº¦å’Œæ•°æ®å®‰å…¨æ€§ä¹‹é—´å–å¾—æ¯”è¾ƒå¥½çš„å¹³è¡¡ã€‚æ ¹æ®ä½ çš„ç†è§£æ¥ # å†³å®šï¼Œå¦‚æœä½ èƒ½æ”¾å®½è¯¥é…ç½®ä¸º\"no\" æ¥è·å–æ›´å¥½çš„æ€§èƒ½(ä½†å¦‚æœä½ èƒ½å¿å—ä¸€äº›æ•°æ®ä¸¢å¤±ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ # é»˜è®¤çš„å¿«ç…§æŒä¹…åŒ–æ¨¡å¼)ï¼Œæˆ–è€…ç›¸åï¼Œç”¨â€œalwaysâ€ä¼šæ¯”è¾ƒæ…¢ä½†æ¯”everysecè¦æ›´å®‰å…¨ã€‚ # # è¯·æŸ¥çœ‹ä¸‹é¢çš„æ–‡ç« æ¥è·å–æ›´å¤šçš„ç»†èŠ‚ # http://antirez.com/post/redis-persistence-demystified.html # # å¦‚æœä¸èƒ½ç¡®å®šï¼Œå°±ç”¨ \"everysec\" # appendfsync always appendfsync everysec # appendfsync no # å¦‚æœAOFçš„åŒæ­¥ç­–ç•¥è®¾ç½®æˆ \"always\" æˆ–è€… \"everysec\"ï¼Œå¹¶ä¸”åå°çš„å­˜å‚¨è¿›ç¨‹ï¼ˆåå°å­˜å‚¨æˆ–å†™å…¥AOF # æ—¥å¿—ï¼‰ä¼šäº§ç”Ÿå¾ˆå¤šç£ç›˜I/Oå¼€é”€ã€‚æŸäº›Linuxçš„é…ç½®ä¸‹ä¼šä½¿Rediså› ä¸º fsync()ç³»ç»Ÿè°ƒç”¨è€Œé˜»å¡å¾ˆä¹…ã€‚ # æ³¨æ„ï¼Œç›®å‰å¯¹è¿™ä¸ªæƒ…å†µè¿˜æ²¡æœ‰å®Œç¾ä¿®æ­£ï¼Œç”šè‡³ä¸åŒçº¿ç¨‹çš„ fsync() ä¼šé˜»å¡æˆ‘ä»¬åŒæ­¥çš„write(2)è°ƒç”¨ã€‚ # # ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥ç”¨ä¸‹é¢è¿™ä¸ªé€‰é¡¹ã€‚å®ƒå¯ä»¥åœ¨ BGSAVE æˆ– BGREWRITEAOF å¤„ç†æ—¶é˜»æ­¢fsync()ã€‚ # # è¿™å°±æ„å‘³ç€å¦‚æœæœ‰å­è¿›ç¨‹åœ¨è¿›è¡Œä¿å­˜æ“ä½œï¼Œé‚£ä¹ˆRediså°±å¤„äº\"ä¸å¯åŒæ­¥\"çš„çŠ¶æ€ã€‚ # è¿™å®é™…ä¸Šæ˜¯è¯´ï¼Œåœ¨æœ€å·®çš„æƒ…å†µä¸‹å¯èƒ½ä¼šä¸¢æ‰30ç§’é’Ÿçš„æ—¥å¿—æ•°æ®ã€‚ï¼ˆé»˜è®¤Linuxè®¾å®šï¼‰ # # å¦‚æœæŠŠè¿™ä¸ªè®¾ç½®æˆ\"yes\"å¸¦æ¥äº†å»¶è¿Ÿé—®é¢˜ï¼Œå°±ä¿æŒ\"no\"ï¼Œè¿™æ˜¯ä¿å­˜æŒä¹…æ•°æ®çš„æœ€å®‰å…¨çš„æ–¹å¼ã€‚ no-appendfsync-on-rewrite no # è‡ªåŠ¨é‡å†™AOFæ–‡ä»¶ # å¦‚æœAOFæ—¥å¿—æ–‡ä»¶å¢å¤§åˆ°æŒ‡å®šç™¾åˆ†æ¯”ï¼ŒRedisèƒ½å¤Ÿé€šè¿‡ BGREWRITEAOF è‡ªåŠ¨é‡å†™AOFæ—¥å¿—æ–‡ä»¶ã€‚ # # å·¥ä½œåŸç†ï¼šRedisè®°ä½ä¸Šæ¬¡é‡å†™æ—¶AOFæ–‡ä»¶çš„å¤§å°ï¼ˆå¦‚æœé‡å¯åè¿˜æ²¡æœ‰å†™æ“ä½œï¼Œå°±ç›´æ¥ç”¨å¯åŠ¨æ—¶çš„AOFå¤§å°ï¼‰ # # è¿™ä¸ªåŸºå‡†å¤§å°å’Œå½“å‰å¤§å°åšæ¯”è¾ƒã€‚å¦‚æœå½“å‰å¤§å°è¶…è¿‡æŒ‡å®šæ¯”ä¾‹ï¼Œå°±ä¼šè§¦å‘é‡å†™æ“ä½œã€‚ä½ è¿˜éœ€è¦æŒ‡å®šè¢«é‡å†™ # æ—¥å¿—çš„æœ€å°å°ºå¯¸ï¼Œè¿™æ ·é¿å…äº†è¾¾åˆ°æŒ‡å®šç™¾åˆ†æ¯”ä½†å°ºå¯¸ä»ç„¶å¾ˆå°çš„æƒ…å†µè¿˜è¦é‡å†™ã€‚ # # æŒ‡å®šç™¾åˆ†æ¯”ä¸º0ä¼šç¦ç”¨AOFè‡ªåŠ¨é‡å†™ç‰¹æ€§ã€‚ auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb ################################ LUA SCRIPTING ############################### # Lua è„šæœ¬çš„æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼Œæ¯«ç§’ä¸ºå•ä½ # # å¦‚æœè¾¾åˆ°äº†æœ€å¤§çš„æ‰§è¡Œæ—¶é—´ï¼ŒRediså°†è¦è®°å½•åœ¨è¾¾åˆ°æœ€å¤§å…è®¸æ—¶é—´ä¹‹åä¸€ä¸ªè„šæœ¬ä»ç„¶åœ¨æ‰§è¡Œï¼Œå¹¶ä¸”å°† # å¼€å§‹å¯¹æŸ¥è¯¢è¿›è¡Œé”™è¯¯å“åº”ã€‚ # # å½“ä¸€ä¸ªé•¿æ—¶é—´è¿è¡Œçš„è„šæœ¬è¶…è¿‡äº†æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼Œåªæœ‰ SCRIPT KILL å’Œ SHUTDOWN NOSAVE ä¸¤ä¸ª # å‘½ä»¤å¯ç”¨ã€‚ç¬¬ä¸€ä¸ªå¯ä»¥ç”¨äºåœæ­¢ä¸€ä¸ªè¿˜æ²¡æœ‰è°ƒç”¨å†™å‘½åçš„è„šæœ¬ã€‚ç¬¬äºŒä¸ªæ˜¯å…³é—­æœåŠ¡å™¨å”¯ä¸€æ–¹å¼ï¼Œå½“ # å†™å‘½ä»¤å·²ç»é€šè¿‡è„šæœ¬å¼€å§‹æ‰§è¡Œï¼Œå¹¶ä¸”ç”¨æˆ·ä¸æƒ³ç­‰åˆ°è„šæœ¬çš„è‡ªç„¶ç»ˆæ­¢ã€‚ # # è®¾ç½®æˆ0æˆ–è€…è´Ÿå€¼è¡¨ç¤ºä¸é™åˆ¶æ‰§è¡Œæ—¶é—´å¹¶ä¸”æ²¡æœ‰ä»»ä½•è­¦å‘Š lua-time-limit 5000 SLOW LOG ################################## SLOW LOG ################################### # Redisæ…¢æŸ¥è¯¢æ—¥å¿—å¯ä»¥è®°å½•è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æŸ¥è¯¢ã€‚è¿è¡Œæ—¶é—´ä¸åŒ…æ‹¬å„ç§I/Oæ—¶é—´ï¼Œä¾‹å¦‚ï¼šè¿æ¥å®¢æˆ·ç«¯ï¼Œ # å‘é€å“åº”æ•°æ®ç­‰ï¼Œè€Œåªè®¡ç®—å‘½ä»¤æ‰§è¡Œçš„å®é™…æ—¶é—´ï¼ˆè¿™åªæ˜¯çº¿ç¨‹é˜»å¡è€Œæ— æ³•åŒæ—¶ä¸ºå…¶ä»–è¯·æ±‚æœåŠ¡çš„å‘½ä»¤æ‰§ # è¡Œé˜¶æ®µï¼‰ # # ä½ å¯ä»¥ä¸ºæ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®ä¸¤ä¸ªå‚æ•°:ä¸€ä¸ªæŒ‡æ˜Redisçš„è¶…æ—¶æ—¶é—´(å•ä½ä¸ºå¾®ç§’)æ¥è®°å½•è¶…è¿‡è¿™ä¸ªæ—¶é—´çš„å‘½ä»¤ # å¦ä¸€ä¸ªæ˜¯æ…¢æŸ¥è¯¢æ—¥å¿—é•¿åº¦ã€‚å½“ä¸€ä¸ªæ–°çš„å‘½ä»¤è¢«å†™è¿›æ—¥å¿—çš„æ—¶å€™ï¼Œæœ€è€çš„é‚£ä¸ªè®°å½•ä»é˜Ÿåˆ—ä¸­ç§»é™¤ã€‚ # # ä¸‹é¢çš„æ—¶é—´å•ä½æ˜¯å¾®ç§’ï¼Œæ‰€ä»¥1000000å°±æ˜¯1ç§’ã€‚æ³¨æ„ï¼Œè´Ÿæ•°æ—¶é—´ä¼šç¦ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œè€Œ0åˆ™ä¼šå¼ºåˆ¶è®°å½• # æ‰€æœ‰å‘½ä»¤ã€‚ slowlog-log-slower-than 10000 # è¿™ä¸ªé•¿åº¦æ²¡æœ‰é™åˆ¶ã€‚åªæ˜¯è¦ä¸»è¦ä¼šæ¶ˆè€—å†…å­˜ã€‚ä½ å¯ä»¥é€šè¿‡ SLOWLOG RESET æ¥å›æ”¶å†…å­˜ã€‚ slowlog-max-len 128 Event notification ############################# Event notification ############################## # Redis èƒ½é€šçŸ¥ Pub/Sub å®¢æˆ·ç«¯å…³äºé”®ç©ºé—´å‘ç”Ÿçš„äº‹ä»¶ # è¿™ä¸ªåŠŸèƒ½æ–‡æ¡£ä½äºhttp://redis.io/topics/keyspace-events # # ä¾‹å¦‚ï¼šå¦‚æœé”®ç©ºé—´äº‹ä»¶é€šçŸ¥è¢«å¼€å¯ï¼Œå¹¶ä¸”å®¢æˆ·ç«¯å¯¹ 0 å·æ•°æ®åº“çš„é”® foo æ‰§è¡Œ DEL å‘½ä»¤æ—¶ï¼Œå°†é€šè¿‡ # Pub/Subå‘å¸ƒä¸¤æ¡æ¶ˆæ¯ï¼š # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # å¯ä»¥åœ¨ä¸‹è¡¨ä¸­é€‰æ‹©Redisè¦é€šçŸ¥çš„äº‹ä»¶ç±»å‹ã€‚äº‹ä»¶ç±»å‹ç”±å•ä¸ªå­—ç¬¦æ¥æ ‡è¯†ï¼š # # K é”®ç©ºé—´é€šçŸ¥ï¼Œä»¥__keyspace@\u003cdb\u003e__ä¸ºå‰ç¼€ # E é”®äº‹ä»¶é€šçŸ¥ï¼Œä»¥__keysevent@\u003cdb\u003e__ä¸ºå‰ç¼€ # g DEL , EXPIRE , RENAME ç­‰ç±»å‹æ— å…³çš„é€šç”¨å‘½ä»¤çš„é€šçŸ¥, ... # $ Stringå‘½ä»¤ # l Listå‘½ä»¤ # s Setå‘½ä»¤ # h Hashå‘½ä»¤ # z æœ‰åºé›†åˆå‘½ä»¤ # x è¿‡æœŸäº‹ä»¶ï¼ˆæ¯æ¬¡keyè¿‡æœŸæ—¶ç”Ÿæˆï¼‰ # e é©±é€äº‹ä»¶ï¼ˆå½“keyåœ¨å†…å­˜æ»¡äº†è¢«æ¸…é™¤æ—¶ç”Ÿæˆï¼‰ # A g$lshzxeçš„åˆ«åï¼Œå› æ­¤â€AKEâ€æ„å‘³ç€æ‰€æœ‰çš„äº‹ä»¶ # # notify-keyspace-events å¸¦ä¸€ä¸ªç”±0åˆ°å¤šä¸ªå­—ç¬¦ç»„æˆçš„å­—ç¬¦ä¸²å‚æ•°ã€‚ç©ºå­—ç¬¦ä¸²æ„æ€æ˜¯é€šçŸ¥è¢«ç¦ç”¨ã€‚ # # ä¾‹å­ï¼šå¯ç”¨Listå’Œé€šç”¨äº‹ä»¶é€šçŸ¥ï¼š # notify-keyspace-events Elg # # ä¾‹å­2ï¼šä¸ºäº†è·å–è¿‡æœŸkeyçš„é€šçŸ¥è®¢é˜…åå­—ä¸º __keyevent@__:expired çš„é¢‘é“ï¼Œç”¨ä»¥ä¸‹é…ç½® # notify-keyspace-events Ex # # é»˜è®¤æ‰€ç”¨çš„é€šçŸ¥è¢«ç¦ç”¨ï¼Œå› ä¸ºç”¨æˆ·é€šå¸¸ä¸éœ€è¦è¯¥ç‰¹æ€§ï¼Œå¹¶ä¸”è¯¥ç‰¹æ€§ä¼šæœ‰æ€§èƒ½æŸè€—ã€‚ # æ³¨æ„å¦‚æœä½ ä¸æŒ‡å®šè‡³å°‘Kæˆ–Eä¹‹ä¸€ï¼Œä¸ä¼šå‘é€ä»»ä½•äº‹ä»¶ã€‚ notify-keyspace-events \"\" ADVANCED CONFIG # å½“hashåªæœ‰å°‘é‡çš„entryæ—¶ï¼Œå¹¶ä¸”æœ€å¤§çš„entryæ‰€å ç©ºé—´æ²¡æœ‰è¶…è¿‡æŒ‡å®šçš„é™åˆ¶æ—¶ï¼Œä¼šç”¨ä¸€ç§èŠ‚çœå†…å­˜çš„ # æ•°æ®ç»“æ„æ¥ç¼–ç ã€‚å¯ä»¥é€šè¿‡ä¸‹é¢çš„æŒ‡ä»¤æ¥è®¾å®šé™åˆ¶ hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # ä¸hashä¼¼ï¼Œæ•°æ®å…ƒç´ è¾ƒå°‘çš„listï¼Œå¯ä»¥ç”¨å¦ä¸€ç§æ–¹å¼æ¥ç¼–ç ä»è€ŒèŠ‚çœå¤§é‡ç©ºé—´ã€‚ # è¿™ç§ç‰¹æ®Šçš„æ–¹å¼åªæœ‰åœ¨ç¬¦åˆä¸‹é¢é™åˆ¶æ—¶æ‰å¯ä»¥ç”¨ï¼š list-max-ziplist-entries 512 list-max-ziplist-value 64 # setæœ‰ä¸€ç§ç‰¹æ®Šç¼–ç çš„æƒ…å†µï¼šå½“setæ•°æ®å…¨æ˜¯åè¿›åˆ¶64ä½æœ‰ç¬¦å·æ•´å‹æ•°å­—æ„æˆçš„å­—ç¬¦ä¸²æ—¶ã€‚ # ä¸‹é¢è¿™ä¸ªé…ç½®é¡¹å°±æ˜¯ç”¨æ¥è®¾ç½®setä½¿ç”¨è¿™ç§ç¼–ç æ¥èŠ‚çœå†…å­˜çš„æœ€å¤§é•¿åº¦ã€‚ set-max-intset-entries 512 # ä¸hashå’Œlistç›¸ä¼¼ï¼Œæœ‰åºé›†åˆä¹Ÿå¯ä»¥ç”¨ä¸€ç§ç‰¹åˆ«çš„ç¼–ç æ–¹å¼æ¥èŠ‚çœå¤§é‡ç©ºé—´ã€‚ # è¿™ç§ç¼–ç åªé€‚åˆé•¿åº¦å’Œå…ƒç´ éƒ½å°äºä¸‹é¢é™åˆ¶çš„æœ‰åºé›†åˆï¼š zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # å¯ç”¨å“ˆå¸Œåˆ·æ–°ï¼Œæ¯100ä¸ªCPUæ¯«ç§’ä¼šæ‹¿å‡º1ä¸ªæ¯«ç§’æ¥åˆ·æ–°Redisçš„ä¸»å“ˆå¸Œè¡¨ï¼ˆé¡¶çº§é”®å€¼æ˜ å°„è¡¨ï¼‰ã€‚ # redisæ‰€ç”¨çš„å“ˆå¸Œè¡¨å®ç°ï¼ˆè§dict.cï¼‰é‡‡ç”¨å»¶è¿Ÿå“ˆå¸Œåˆ·æ–°æœºåˆ¶ï¼šä½ å¯¹ä¸€ä¸ªå“ˆå¸Œè¡¨æ“ä½œè¶Šå¤šï¼Œå“ˆå¸Œåˆ·æ–° # æ“ä½œå°±è¶Šé¢‘ç¹ï¼›åä¹‹ï¼Œå¦‚æœæœåŠ¡å™¨æ˜¯ç©ºé—²çš„ï¼Œé‚£ä¹ˆå“ˆå¸Œåˆ·æ–°å°±ä¸ä¼šå®Œæˆï¼Œå“ˆå¸Œè¡¨å°±ä¼šå ç”¨æ›´å¤šçš„ä¸€äº› # å†…å­˜è€Œå·²ã€‚ # # é»˜è®¤æ˜¯æ¯ç§’é’Ÿè¿›è¡Œ10æ¬¡å“ˆå¸Œè¡¨åˆ·æ–°ï¼Œç”¨æ¥åˆ·æ–°å­—å…¸ï¼Œç„¶åå°½å¿«é‡Šæ”¾å†…å­˜ã€‚ # # å»ºè®®ï¼š # å¦‚æœä½ å¯¹å»¶è¿Ÿæ¯”è¾ƒåœ¨æ„ï¼Œä¸èƒ½å¤Ÿæ¥å—Redisæ—¶ä¸æ—¶çš„å¯¹è¯·æ±‚æœ‰2æ¯«ç§’çš„å»¶è¿Ÿçš„è¯ï¼Œå°±ç”¨ # \"activerehashing no\"ï¼Œå¦‚æœä¸å¤ªåœ¨æ„å»¶è¿Ÿè€Œå¸Œæœ›å°½å¿«é‡Šæ”¾å†…å­˜å°±è®¾ç½®\"activerehashing yes\" activerehashing yes # å®¢æˆ·ç«¯çš„è¾“å‡ºç¼“å†²åŒºçš„é™åˆ¶ï¼Œå¯ç”¨äºå¼ºåˆ¶æ–­å¼€é‚£äº›å› ä¸ºæŸç§åŸå› ä»æœåŠ¡å™¨è¯»å–æ•°æ®çš„é€Ÿåº¦ä¸å¤Ÿå¿«çš„å®¢æˆ·ç«¯ï¼Œ # ï¼ˆä¸€ä¸ªå¸¸è§çš„åŸå› æ˜¯ä¸€ä¸ªå‘å¸ƒ/è®¢é˜…å®¢æˆ·ç«¯æ¶ˆè´¹æ¶ˆæ¯çš„é€Ÿåº¦æ— æ³•èµ¶ä¸Šç”Ÿäº§å®ƒä»¬çš„é€Ÿåº¦ï¼‰ # # å¯ä»¥å¯¹ä¸‰ç§ä¸åŒçš„å®¢æˆ·ç«¯è®¾ç½®ä¸åŒçš„é™åˆ¶ï¼š # normal -\u003e æ­£å¸¸å®¢æˆ·ç«¯ # slave -\u003e slaveå’Œ MONITOR å®¢æˆ·ç«¯ # pubsub -\u003e è‡³å°‘è®¢é˜…äº†ä¸€ä¸ªpubsub channelæˆ–patternçš„å®¢æˆ·ç«¯ # # ä¸‹é¢æ˜¯æ¯ä¸ªclient-output-buffer-limitè¯­æ³•: # client-output-buffer-limit \u003cclass\u003e\u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e # ä¸€æ—¦è¾¾åˆ°ç¡¬é™åˆ¶å®¢æˆ·ç«¯ä¼šç«‹å³è¢«æ–­å¼€ï¼Œæˆ–è€…è¾¾åˆ°è½¯é™åˆ¶å¹¶æŒç»­è¾¾åˆ°æŒ‡å®šçš„ç§’æ•°ï¼ˆè¿ç»­çš„ï¼‰ã€‚ # ä¾‹å¦‚ï¼Œå¦‚æœç¡¬é™åˆ¶ä¸º32å…†å­—èŠ‚å’Œè½¯é™åˆ¶ä¸º16å…†å­—èŠ‚/10ç§’ï¼Œå®¢æˆ·ç«¯å°†ä¼šç«‹å³æ–­å¼€ # å¦‚æœè¾“å‡ºç¼“å†²åŒºçš„å¤§å°è¾¾åˆ°32å…†å­—èŠ‚ï¼Œæˆ–å®¢æˆ·ç«¯è¾¾åˆ°16å…†å­—èŠ‚å¹¶è¿ç»­è¶…è¿‡äº†é™åˆ¶10ç§’ï¼Œå°±å°†æ–­å¼€è¿æ¥ã€‚ # # é»˜è®¤normalå®¢æˆ·ç«¯ä¸åšé™åˆ¶ï¼Œå› ä¸ºä»–ä»¬åœ¨ä¸ä¸»åŠ¨è¯·æ±‚æ—¶ä¸æ¥æ”¶æ•°æ®ï¼ˆä»¥æ¨çš„æ–¹å¼ï¼‰ï¼Œåªæœ‰å¼‚æ­¥å®¢æˆ·ç«¯ # å¯èƒ½ä¼šå‡ºç°è¯·æ±‚æ•°æ®çš„é€Ÿåº¦æ¯”å®ƒå¯ä»¥è¯»å–çš„é€Ÿåº¦å¿«çš„åœºæ™¯ã€‚ # # pubsubå’Œslaveå®¢æˆ·ç«¯ä¼šæœ‰ä¸€ä¸ªé»˜è®¤å€¼ï¼Œå› ä¸ºè®¢é˜…è€…å’Œslavesä»¥æ¨çš„æ–¹å¼æ¥æ¥æ”¶æ•°æ® # # æŠŠç¡¬é™åˆ¶å’Œè½¯é™åˆ¶éƒ½è®¾ç½®ä¸º0æ¥ç¦ç”¨è¯¥åŠŸèƒ½ client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Redisè°ƒç”¨å†…éƒ¨å‡½æ•°æ¥æ‰§è¡Œè®¸å¤šåå°ä»»åŠ¡ï¼Œå¦‚å…³é—­å®¢æˆ·ç«¯è¶…æ—¶çš„è¿æ¥ï¼Œæ¸…é™¤æœªè¢«è¯·æ±‚è¿‡çš„è¿‡æœŸKeyç­‰ç­‰ã€‚ # # ä¸æ˜¯æ‰€æœ‰çš„ä»»åŠ¡éƒ½ä»¥ç›¸åŒçš„é¢‘ç‡æ‰§è¡Œï¼Œä½†Redisä¾ç…§æŒ‡å®šçš„â€œhzâ€å€¼æ¥æ‰§è¡Œæ£€æŸ¥ä»»åŠ¡ã€‚ # # é»˜è®¤æƒ…å†µä¸‹ï¼Œâ€œhzâ€çš„è¢«è®¾å®šä¸º10ã€‚æé«˜è¯¥å€¼å°†åœ¨Redisç©ºé—²æ—¶ä½¿ç”¨æ›´å¤šçš„CPUæ—¶ï¼Œä½†åŒæ—¶å½“æœ‰å¤šä¸ªkey # åŒæ—¶åˆ°æœŸä¼šä½¿Redisçš„ååº”æ›´çµæ•ï¼Œä»¥åŠè¶…æ—¶å¯ä»¥æ›´ç²¾ç¡®åœ°å¤„ç†ã€‚ # # èŒƒå›´æ˜¯1åˆ°500ä¹‹é—´ï¼Œä½†æ˜¯å€¼è¶…è¿‡100é€šå¸¸ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ # å¤§å¤šæ•°ç”¨æˆ·åº”è¯¥ä½¿ç”¨10è¿™ä¸ªé»˜è®¤å€¼ï¼Œåªæœ‰åœ¨éå¸¸ä½çš„å»¶è¿Ÿè¦æ±‚æ—¶æœ‰å¿…è¦æé«˜åˆ°100ã€‚ hz 10 # å½“ä¸€ä¸ªå­è¿›ç¨‹é‡å†™AOFæ–‡ä»¶æ—¶ï¼Œå¦‚æœå¯ç”¨ä¸‹é¢çš„é€‰é¡¹ï¼Œåˆ™æ–‡ä»¶æ¯ç”Ÿæˆ32Mæ•°æ®ä¼šè¢«åŒæ­¥ã€‚ä¸ºäº†å¢é‡å¼çš„ # å†™å…¥ç¡¬ç›˜å¹¶ä¸”é¿å…å¤§çš„å»¶è¿Ÿé«˜å³°è¿™ä¸ªæŒ‡ä»¤æ˜¯éå¸¸æœ‰ç”¨çš„ aof-rewrite-incremental-fsync yes ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä¸ºredis.confçš„æ–‡ä»¶çš„ä¸­æ–‡æè¿°ï¼Œæ•´ç†äºç½‘ç»œ\n# Redis é…ç½®æ–‡ä»¶ç¤ºä¾‹ # æ³¨æ„å•ä½: å½“éœ€è¦é…ç½®å†…å­˜å¤§å°æ—¶, å¯èƒ½éœ€è¦æŒ‡å®š â€¦","ref":"/linux-notes/redis/redis-conf-cn/","tags":["Redis"],"title":"Redisé…ç½®è¯¦è§£ï¼ˆä¸­æ–‡ç‰ˆï¼‰"},{"body":"é—®é¢˜æè¿° å½“ä½¿ç”¨runc 1.1.3çš„ç‰ˆæœ¬æ—¶ï¼Œå¦‚æœæ‰§è¡Œsystemctl daemon-reloadåï¼Œé€šè¿‡execè¿›å…¥å®¹å™¨åˆ™ä¼šè§¦å‘ä»¥ä¸‹é”™è¯¯ï¼Œæ— æ³•è¿›å…¥å®¹å™¨ã€‚\nFATA[0000] execing command in container: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec \"33d05b4f71a2da69c8c77cc3f7e61451814eb150edd15d0a3153b57a862126d4\": OCI runtime exec failed: exec failed: unable to start container process: open /dev/pts/0: operation not permitted: unknown åŸå›  è¿™ä¸ªæ˜¯runc 1.1.3ç‰ˆæœ¬èµ·å­˜åœ¨çš„ä¸€ä¸ªbugï¼Œrunc1.1.2çš„ç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œç¤¾åŒºåœ¨runc 1.1.4çš„ç‰ˆæœ¬ä¸­ä¿®å¤äº†è¿™ä¸ªbugã€‚ç”±äº runc v1.1.3 ä¸­ä¸å†æ·»åŠ Â DeviceAllow=char-pts rwmÂ è§„åˆ™äº†ï¼Œå½“æ‰§è¡ŒÂ systemctl daemon-reloadÂ åï¼Œ ä¼šå¯¼è‡´é‡æ–°åº”ç”¨ systemd çš„è§„åˆ™ï¼Œè¿›è€Œå¯¼è‡´è¿™æ¡è§„åˆ™çš„ç¼ºå¤±ã€‚\nè§£å†³æ–¹æ¡ˆ å‡çº§runcåˆ°1.1.4çš„ç‰ˆæœ¬ï¼Œå¹¶ä¸”æ‰€æœ‰é€šè¿‡runc1.1.3åˆ›å»ºçš„podéƒ½éœ€è¦é‡å»ºæ‰èƒ½ç”Ÿæ•ˆã€‚\nå‚è€ƒï¼š\nhttps://github.com/containerd/containerd/issues/7219#issuecomment-1225358826\nhttps://github.com/opencontainers/runc/issues/3551\nRelease runc 1.1.4 -- \"If you look for perfection, you'll never be content.\" Â· opencontainers/runc Â· GitHub\n","categories":"","description":"","excerpt":"é—®é¢˜æè¿° å½“ä½¿ç”¨runc 1.1.3çš„ç‰ˆæœ¬æ—¶ï¼Œå¦‚æœæ‰§è¡Œsystemctl daemon-reloadåï¼Œé€šè¿‡execè¿›å…¥å®¹å™¨åˆ™ä¼šè§¦å‘ä»¥ä¸‹é”™ â€¦","ref":"/kubernetes-notes/trouble-shooting/node/runc-1.1.3-exec-failed/","tags":["é—®é¢˜æ’æŸ¥"],"title":"runc-v1.1.3-exec-failed"},{"body":"1. å­—ç¬¦ä¸² å­—ç¬¦ä¸²æ˜¯shellç¼–ç¨‹ä¸­æœ€å¸¸ç”¨æœ€æœ‰ç”¨çš„æ•°æ®ç±»å‹ï¼ˆé™¤äº†æ•°å­—å’Œå­—ç¬¦ä¸²ï¼Œä¹Ÿæ²¡å•¥å…¶å®ƒç±»å‹å¥½ç”¨äº†ï¼‰ï¼Œå­—ç¬¦ä¸²å¯ä»¥ç”¨å•å¼•å·ï¼Œä¹Ÿå¯ä»¥ç”¨åŒå¼•å·ï¼Œä¹Ÿå¯ä»¥ä¸ç”¨å¼•å·ã€‚å•åŒå¼•å·çš„åŒºåˆ«è·ŸPHPç±»ä¼¼ã€‚\n1.1. å•å¼•å· str='this is a string' å•å¼•å·å­—ç¬¦ä¸²çš„é™åˆ¶ï¼š\nå•å¼•å·é‡Œçš„ä»»ä½•å­—ç¬¦éƒ½ä¼šåŸæ ·è¾“å‡ºï¼Œå•å¼•å·å­—ç¬¦ä¸²ä¸­çš„å˜é‡æ˜¯æ— æ•ˆçš„ï¼› å•å¼•å·å­—ä¸²ä¸­ä¸èƒ½å‡ºç°å•å¼•å·ï¼ˆå¯¹å•å¼•å·ä½¿ç”¨è½¬ä¹‰ç¬¦åä¹Ÿä¸è¡Œï¼‰ã€‚ 1.2. åŒå¼•å· your_name='qinjx' str=\"Hello, I know your are \\\"$your_name\\\"! \\n\" åŒå¼•å·çš„ä¼˜ç‚¹ï¼š\nåŒå¼•å·é‡Œå¯ä»¥æœ‰å˜é‡ åŒå¼•å·é‡Œå¯ä»¥å‡ºç°è½¬ä¹‰å­—ç¬¦ 1.3. æ‹¼æ¥å­—ç¬¦ä¸² your_name=\"qinjx\" greeting=\"hello, \"$your_name\" !\" greeting_1=\"hello, ${your_name} !\" echo $greeting $greeting_1 1.4. è·å–å­—ç¬¦ä¸²é•¿åº¦ string=\"abcd\" echo ${#string} #è¾“å‡º 4 1.5. æå–å­å­—ç¬¦ä¸² string=\"alibaba is a great company\" echo ${string:1:4} #è¾“å‡ºliba 1.6. æŸ¥æ‰¾å­å­—ç¬¦ä¸² string=\"alibaba is a great company\" echo `expr index \"$string\" is` 2. æ•°ç»„ bashæ”¯æŒä¸€ç»´æ•°ç»„ï¼ˆä¸æ”¯æŒå¤šç»´æ•°ç»„ï¼‰ï¼Œå¹¶ä¸”æ²¡æœ‰é™å®šæ•°ç»„çš„å¤§å°ã€‚ç±»ä¼¼ä¸Cè¯­è¨€ï¼Œæ•°ç»„å…ƒç´ çš„ä¸‹æ ‡ç”±0å¼€å§‹ç¼–å·ã€‚è·å–æ•°ç»„ä¸­çš„å…ƒç´ è¦åˆ©ç”¨ä¸‹æ ‡ï¼Œä¸‹æ ‡å¯ä»¥æ˜¯æ•´æ•°æˆ–ç®—æœ¯è¡¨è¾¾å¼ï¼Œå…¶å€¼åº”å¤§äºæˆ–ç­‰äº0ã€‚\n2.1. å®šä¹‰æ•°ç»„ åœ¨Shellä¸­ï¼Œç”¨æ‹¬å·æ¥è¡¨ç¤ºæ•°ç»„ï¼Œæ•°ç»„å…ƒç´ ç”¨â€œç©ºæ ¼â€ç¬¦å·åˆ†å‰²å¼€ã€‚\nå®šä¹‰æ•°ç»„çš„ä¸€èˆ¬å½¢å¼ä¸ºï¼š\narray_name=(value1 ... valuen) ä¾‹å¦‚ï¼š\narray_name=(value0 value1 value2 value3) æˆ–è€…\narray_name=( value0 value1 value2 value3 ) è¿˜å¯ä»¥å•ç‹¬å®šä¹‰æ•°ç»„çš„å„ä¸ªåˆ†é‡ï¼š\narray_name[0]=value0 array_name[1]=value1 array_name[2]=value2 å¯ä»¥ä¸ä½¿ç”¨è¿ç»­çš„ä¸‹æ ‡ï¼Œè€Œä¸”ä¸‹æ ‡çš„èŒƒå›´æ²¡æœ‰é™åˆ¶ã€‚\n2.2. è¯»å–æ•°ç»„ è¯»å–æ•°ç»„å…ƒç´ å€¼çš„ä¸€èˆ¬æ ¼å¼æ˜¯ï¼š\n${array_name[index]} ä¾‹å¦‚ï¼š\nvaluen=${array_name[2]} ä½¿ç”¨@ æˆ– * å¯ä»¥è·å–æ•°ç»„ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œä¾‹å¦‚ï¼š\n${array_name[*]} ${array_name[@]} 2.3. è·å–æ•°ç»„çš„é•¿åº¦ è·å–æ•°ç»„é•¿åº¦çš„æ–¹æ³•ä¸è·å–å­—ç¬¦ä¸²é•¿åº¦çš„æ–¹æ³•ç›¸åŒï¼Œä¾‹å¦‚ï¼š\n# å–å¾—æ•°ç»„å…ƒç´ çš„ä¸ªæ•° length=${#array_name[@]} # æˆ–è€… length=${#array_name[*]} # å–å¾—æ•°ç»„å•ä¸ªå…ƒç´ çš„é•¿åº¦ lengthn=${#array_name[n]} å‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. å­—ç¬¦ä¸² å­—ç¬¦ä¸²æ˜¯shellç¼–ç¨‹ä¸­æœ€å¸¸ç”¨æœ€æœ‰ç”¨çš„æ•°æ®ç±»å‹ï¼ˆé™¤äº†æ•°å­—å’Œå­—ç¬¦ä¸²ï¼Œä¹Ÿæ²¡å•¥å…¶å®ƒç±»å‹å¥½ç”¨äº†ï¼‰ï¼Œå­—ç¬¦ä¸²å¯ä»¥ç”¨å•å¼•å·ï¼Œä¹Ÿå¯ä»¥ç”¨åŒå¼•å·ï¼Œ â€¦","ref":"/linux-notes/shell/shell-array/","tags":["Shell"],"title":"Shellæ•°ç»„"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æstartKubeletï¼Œå…¶ä¸­ä¸»è¦æ˜¯kubelet.Runéƒ¨åˆ†ï¼Œè¯¥éƒ¨åˆ†çš„å†…å®¹ä¸»è¦æ˜¯åˆå§‹åŒ–å¹¶è¿è¡Œä¸€äº›managerã€‚å¯¹äºkubeletæ‰€åŒ…å«çš„å„ç§managerçš„æ‰§è¡Œé€»è¾‘å’Œpodçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†é€»è¾‘å¾…åç»­æ–‡ç« åˆ†æã€‚\nåç»­çš„æ–‡ç« ä¸»è¦ä¼šåˆ†ç±»åˆ†æpkg/kubeletéƒ¨åˆ†çš„ä»£ç å®ç°ã€‚\nkubeletçš„pkgä»£ç ç›®å½•ç»“æ„ï¼š\nkubelet â”œâ”€â”€ apis # å®šä¹‰ä¸€äº›ç›¸å…³æ¥å£ â”œâ”€â”€ cadvisor # cadvisor â”œâ”€â”€ cm # ContainerManagerã€cpu mangerã€cgroup manager â”œâ”€â”€ config â”œâ”€â”€ configmap # configmap manager â”œâ”€â”€ container # Runtimeã€ImageService â”œâ”€â”€ dockershim # dockerçš„ç›¸å…³è°ƒç”¨ â”œâ”€â”€ eviction # eviction manager â”œâ”€â”€ images # image manager â”œâ”€â”€ kubeletconfig â”œâ”€â”€ kuberuntime # æ ¸å¿ƒï¼škubeGenericRuntimeManagerã€runtimeå®¹å™¨çš„ç›¸å…³æ“ä½œ â”œâ”€â”€ lifecycle â”œâ”€â”€ mountpod â”œâ”€â”€ network # pod dns â”œâ”€â”€ nodelease â”œâ”€â”€ nodestatus # MachineInfoã€èŠ‚ç‚¹ç›¸å…³ä¿¡æ¯ â”œâ”€â”€ pleg # PodLifecycleEventGenerator â”œâ”€â”€ pod # æ ¸å¿ƒï¼špod managerã€mirror pod â”œâ”€â”€ preemption â”œâ”€â”€ qos # èµ„æºæœåŠ¡è´¨é‡ï¼Œä¸è¿‡æš‚æ—¶å†…å®¹å¾ˆå°‘ â”œâ”€â”€ remote # RemoteRuntimeService â”œâ”€â”€ server â”œâ”€â”€ stats # StatsProvider â”œâ”€â”€ status # status manager â”œâ”€â”€ types # PodUpdateã€PodOperation â”œâ”€â”€ volumemanager # VolumeManager â”œâ”€â”€ kubelet.go # æ ¸å¿ƒ: SyncHandlerã€kubeletçš„å¤§éƒ¨åˆ†æ“ä½œ â”œâ”€â”€ kubelet_getters.go # å„ç§getæ“ä½œï¼Œä¾‹å¦‚è·å–ç›¸å…³ç›®å½•ï¼šgetRootDirã€getPodsDirã€getPluginsDir â”œâ”€â”€ kubelet_network.go # â”œâ”€â”€ kubelet_network_linux.go â”œâ”€â”€ kubelet_node_status.go # registerWithAPIServerã€initialNodeã€syncNodeStatus â”œâ”€â”€ kubelet_pods.go # æ ¸å¿ƒï¼špodçš„å¢åˆ æ”¹æŸ¥ç­‰ç›¸å…³æ“ä½œã€podKillerã€ â”œâ”€â”€ kubelet_resources.go â”œâ”€â”€ kubelet_volumes.go # ListVolumesForPodã€cleanupOrphanedPodDirs â”œâ”€â”€ oom_watcher.go # OOMWatcher â”œâ”€â”€ pod_container_deletor.go â”œâ”€â”€ pod_workers.go # æ ¸å¿ƒï¼šPodWorkersã€UpdatePodOptionsã€syncPodOptionsã€managePodLoop â”œâ”€â”€ runonce.go # RunOnce â”œâ”€â”€ runtime.go ... 1. startKubelet startKubeletçš„å‡½æ•°ä½äºcmd/kubelet/app/server.goï¼Œå¯åŠ¨å¹¶è¿è¡Œä¸€ä¸ªkubeletï¼Œè¿è¡Œkubeletçš„é€»è¾‘ä»£ç ä½äºpkg/kubelet/kubelet.goã€‚\nä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\nè¿è¡Œä¸€ä¸ªkubeletï¼Œæ‰§è¡Œkubeletä¸­å„ç§managerçš„ç›¸å…³é€»è¾‘ã€‚ è¿è¡Œkubelet serverå¯åŠ¨ç›‘å¬æœåŠ¡ã€‚ æ­¤éƒ¨åˆ†ä»£ç ä½äºcmd/kubelet/app/server.go\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) // start the kubelet server if enableServer { go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) } if kubeCfg.ReadOnlyPort \u003e 0 { go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } } 2. Kubelet.Run Kubelet.Runæ–¹æ³•ä¸»è¦å°†NewMainKubeletæ„é€ çš„å„ç§managerè¿è¡Œèµ·æ¥ï¼Œè®©å„ç§manageræ‰§è¡Œç›¸åº”çš„åŠŸèƒ½ï¼Œå¤§éƒ¨åˆ†managerä¸ºå¸¸é©»è¿›ç¨‹çš„æ–¹å¼è¿è¡Œã€‚\nKubelet.Runå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kubelet.go\n// Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u003c-chan kubetypes.PodUpdate) { if kl.logServer == nil { kl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\"))) } if kl.kubeClient == nil { glog.Warning(\"No api server defined - no node status update will be sent.\") } // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) glog.Fatal(err) } // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil { // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { go kl.nodeLeaseController.Run(wait.NeverStop) } } go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Start loop to sync iptables util rules if kl.makeIPTablesUtilChains { go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) } // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) // Start component sync loops. kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { go kl.runtimeClassManager.Run(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl) } ä»¥ä¸‹å¯¹Kubelet.Runåˆ†æ®µè¿›è¡Œåˆ†æã€‚\n3. initializeModules initializeModulesåŒ…å«äº†imageManagerã€serverCertificateManagerã€oomWatcherå’ŒresourceAnalyzerã€‚\nä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\nåˆ›å»ºæ–‡ä»¶ç³»ç»Ÿç›®å½•ï¼ŒåŒ…æ‹¬kubeletçš„rootç›®å½•ã€podsçš„ç›®å½•ã€pluginsçš„ç›®å½•å’Œå®¹å™¨æ—¥å¿—ç›®å½•ã€‚ å¯åŠ¨imageManagerã€serverCertificateManagerã€oomWatcherã€resourceAnalyzerã€‚ å„ç§managerçš„è¯´æ˜å¦‚ä¸‹ï¼š\nimageManagerï¼šè´Ÿè´£é•œåƒåƒåœ¾å›æ”¶ã€‚ serverCertificateManagerï¼šè´Ÿè´£å¤„ç†è¯ä¹¦ã€‚ oomWatcherï¼šç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œæ˜¯å¦å‘ç”Ÿå†…å­˜è€—å°½ã€‚ resourceAnalyzerï¼šç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µã€‚ å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kubelet.go\n// initializeModules will initialize internal modules that do not require the container runtime to be up. // Note that the modules here must not depend on modules that are not initialized here. func (kl *Kubelet) initializeModules() error { // Prometheus metrics. metrics.Register(kl.runtimeCache, collectors.NewVolumeStatsCollector(kl)) // Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { glog.Errorf(\"Failed to create directory %q: %v\", ContainerLogsDir, err) } } // Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } // Start out of memory watcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\"Failed to start OOM watcher %v\", err) } // Start resource analyzer kl.resourceAnalyzer.Start() return nil } 3.1. setupDataDirs initializeModuleså…ˆåˆ›å»ºç›¸å…³ç›®å½•ã€‚\nå…·ä½“ç›®å½•å¦‚ä¸‹ï¼š\nContainerLogsDirï¼šç›®å½•ä¸º/var/log/containersã€‚ rootDirectoryï¼šç”±å‚æ•°ä¼ å…¥ï¼Œä¸€èˆ¬ä¸º/var/lib/kubeletã€‚ PodsDirï¼šç›®å½•ä¸º{rootDirectory}/podsã€‚ PluginsDirï¼šç›®å½•ä¸º{rootDirectory}/pluginsã€‚ initializeModulesä¸­setupDataDirsçš„ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š\n// Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { glog.Errorf(\"Failed to create directory %q: %v\", ContainerLogsDir, err) } } setupDataDirsä»£ç å¦‚ä¸‹\n// setupDataDirs creates: // 1. the root directory // 2. the pods directory // 3. the plugins directory func (kl *Kubelet) setupDataDirs() error { kl.rootDirectory = path.Clean(kl.rootDirectory) if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil { return fmt.Errorf(\"error creating root directory: %v\", err) } if err := kl.mounter.MakeRShared(kl.getRootDir()); err != nil { return fmt.Errorf(\"error configuring root directory: %v\", err) } if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil { return fmt.Errorf(\"error creating pods directory: %v\", err) } if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil { return fmt.Errorf(\"error creating plugins directory: %v\", err) } return nil } 3.2. manager initializeModulesä¸­çš„managerå¦‚ä¸‹ï¼š\n// Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } // Start out of memory watcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\"Failed to start OOM watcher %v\", err) } // Start resource analyzer kl.resourceAnalyzer.Start() 4. è¿è¡Œå„ç§manager 4.1. volumeManager volumeManagerä¸»è¦è¿è¡Œä¸€ç»„å¼‚æ­¥å¾ªç¯ï¼Œæ ¹æ®åœ¨æ­¤èŠ‚ç‚¹ä¸Šå®‰æ’çš„podè°ƒæ•´å“ªäº›volumeéœ€è¦attached/detached/mounted/unmountedã€‚\n// Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) volumeManager.Runå®ç°ä»£ç å¦‚ä¸‹ï¼š\nfunc (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) { defer runtime.HandleCrash() go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh) glog.V(2).Infof(\"The desired_state_of_world populator starts\") glog.Infof(\"Starting Kubelet Volume Manager\") go vm.reconciler.Run(stopCh) metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr) \u003c-stopCh glog.Infof(\"Shutting down Kubelet Volume Manager\") } 4.2. syncNodeStatus syncNodeStatusé€šè¿‡goroutineçš„æ–¹å¼å®šæœŸæ‰§è¡Œï¼Œå®ƒå°†èŠ‚ç‚¹çš„çŠ¶æ€åŒæ­¥ç»™masterï¼Œå¿…è¦çš„æ—¶å€™æ³¨å†Œkubeletã€‚\nif kl.kubeClient != nil { // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { go kl.nodeLeaseController.Run(wait.NeverStop) } } 4.3. updateRuntimeUp updateRuntimeUpè°ƒç”¨å®¹å™¨è¿è¡Œæ—¶çŠ¶æ€å›è°ƒï¼Œåœ¨å®¹å™¨è¿è¡Œæ—¶é¦–æ¬¡å¯åŠ¨æ—¶åˆå§‹åŒ–è¿è¡Œæ—¶ç›¸å…³æ¨¡å—ï¼Œå¦‚æœçŠ¶æ€æ£€æŸ¥å¤±è´¥åˆ™è¿”å›é”™è¯¯ã€‚ å¦‚æœçŠ¶æ€æ£€æŸ¥æ­£å¸¸ï¼Œåœ¨kubelet runtimeStateä¸­æ›´æ–°å®¹å™¨è¿è¡Œæ—¶çš„æ­£å¸¸è¿è¡Œæ—¶é—´ã€‚\ngo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) 4.4. syncNetworkUtil é€šè¿‡å¾ªç¯çš„æ–¹å¼åŒæ­¥iptablesçš„è§„åˆ™ï¼Œä¸è¿‡å½“å‰ä»£ç å¹¶æ²¡æœ‰æ‰§è¡Œä»»ä½•æ“ä½œã€‚\n// Start loop to sync iptables util rules if kl.makeIPTablesUtilChains { go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) } 4.5. podKiller ä½†podæ²¡æœ‰è¢«podworkeræ­£ç¡®å¤„ç†çš„æ—¶å€™ï¼Œå¯åŠ¨ä¸€ä¸ªgoroutineè´Ÿè´£æ€æ­»podã€‚\n// Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) podKillerä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kubelet_pods.go\n// podKiller launches a goroutine to kill a pod received from the channel if // another goroutine isn't already in action. func (kl *Kubelet) podKiller() { killing := sets.NewString() // guard for the killing set lock := sync.Mutex{} for podPair := range kl.podKillingCh { runningPod := podPair.RunningPod apiPod := podPair.APIPod lock.Lock() exists := killing.Has(string(runningPod.ID)) if !exists { killing.Insert(string(runningPod.ID)) } lock.Unlock() if !exists { go func(apiPod *v1.Pod, runningPod *kubecontainer.Pod) { glog.V(2).Infof(\"Killing unwanted pod %q\", runningPod.Name) err := kl.killPod(apiPod, runningPod, nil, nil) if err != nil { glog.Errorf(\"Failed killing the pod %q: %v\", runningPod.Name, err) } lock.Lock() killing.Delete(string(runningPod.ID)) lock.Unlock() }(apiPod, runningPod) } } } 4.6. statusManager ä½¿ç”¨apiserveråŒæ­¥podsçŠ¶æ€; ä¹Ÿç”¨ä½œçŠ¶æ€ç¼“å­˜ã€‚\n// Start component sync loops. kl.statusManager.Start() statusManager.Startçš„å®ç°ä»£ç å¦‚ä¸‹ï¼š\nfunc (m *manager) Start() { // Don't start the status manager if we don't have a client. This will happen // on the master, where the kubelet is responsible for bootstrapping the pods // of the master components. if m.kubeClient == nil { glog.Infof(\"Kubernetes client is nil, not starting status manager.\") return } glog.Info(\"Starting to sync pod status with apiserver\") syncTicker := time.Tick(syncPeriod) // syncPod and syncBatch share the same go routine to avoid sync races. go wait.Forever(func() { select { case syncRequest := \u003c-m.podStatusChannel: glog.V(5).Infof(\"Status Manager: syncing pod: %q, with status: (%d, %v) from podStatusChannel\", syncRequest.podUID, syncRequest.status.version, syncRequest.status.status) m.syncPod(syncRequest.podUID, syncRequest.status) case \u003c-syncTicker: m.syncBatch() } }, 0) } 4.7. probeManager å¤„ç†å®¹å™¨æ¢é’ˆ\nkl.probeManager.Start() 4.8. runtimeClassManager // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { go kl.runtimeClassManager.Run(wait.NeverStop) } 4.9. PodLifecycleEventGenerator // Start the pod lifecycle event generator. kl.pleg.Start() PodLifecycleEventGeneratoræ˜¯ä¸€ä¸ªpodç”Ÿå‘½å‘¨æœŸæ—¶é—´ç”Ÿæˆå™¨æ¥å£ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n// PodLifecycleEventGenerator contains functions for generating pod life cycle events. type PodLifecycleEventGenerator interface { Start() Watch() chan *PodLifecycleEvent Healthy() (bool, error) } startæ–¹æ³•å…·ä½“å®ç°å¦‚ä¸‹ï¼š\n// Start spawns a goroutine to relist periodically. func (g *GenericPLEG) Start() { go wait.Until(g.relist, g.relistPeriod, wait.NeverStop) } 4.10. syncLoop æœ€åè°ƒç”¨syncLoopæ¥æ‰§è¡ŒåŒæ­¥å˜åŒ–å˜æ›´çš„å¾ªç¯ã€‚\nkl.syncLoop(updates, kl) 5. syncLoop syncLoopæ˜¯å¤„ç†å˜åŒ–çš„å¾ªç¯ã€‚ å®ƒç›‘å¬æ¥è‡ªä¸‰ç§channelï¼ˆfileï¼Œapiserverå’Œhttpï¼‰çš„æ›´æ”¹ã€‚ å¯¹äºçœ‹åˆ°çš„ä»»ä½•æ–°æ›´æ”¹ï¼Œå°†é’ˆå¯¹æ‰€éœ€çŠ¶æ€å’Œè¿è¡ŒçŠ¶æ€è¿è¡ŒåŒæ­¥ã€‚ å¦‚æœæ²¡æœ‰çœ‹åˆ°é…ç½®çš„å˜åŒ–ï¼Œå°†åœ¨æ¯ä¸ªåŒæ­¥é¢‘ç‡ç§’åŒæ­¥æœ€åå·²çŸ¥çš„æ‰€éœ€çŠ¶æ€ã€‚\n// syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates \u003c-chan kubetypes.PodUpdate, handler SyncHandler) { glog.Info(\"Starting kubelet main sync loop.\") // The resyncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync'd. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 { glog.Infof(\"skipping pod synchronization - %v\", rs) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } å…¶ä¸­è°ƒç”¨äº†syncLoopIterationçš„å‡½æ•°æ¥æ‰§è¡Œæ›´å…·ä½“çš„ç›‘æ§podå˜åŒ–çš„å¾ªç¯ã€‚syncLoopIterationä»£ç é€»è¾‘å¾…åç»­å•ç‹¬åˆ†æã€‚\n6. æ€»ç»“ 6.1. åŸºæœ¬æµç¨‹ Kubelet.Runä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\nåˆå§‹åŒ–æ¨¡å—ï¼Œå…¶å®å°±æ˜¯è¿è¡ŒimageManagerã€serverCertificateManagerã€oomWatcherã€resourceAnalyzerã€‚ è¿è¡Œå„ç§managerï¼Œå¤§éƒ¨åˆ†ä»¥å¸¸é©»goroutineçš„æ–¹å¼è¿è¡Œï¼Œå…¶ä¸­åŒ…æ‹¬volumeManagerã€statusManagerç­‰ã€‚ æ‰§è¡Œå¤„ç†å˜æ›´çš„å¾ªç¯å‡½æ•°syncLoopï¼Œå¯¹podçš„ç”Ÿå‘½å‘¨æœŸè¿›è¡Œç®¡ç†ã€‚ syncLoopï¼š\nsyncLoopå‡½æ•°ï¼Œå¯¹podçš„ç”Ÿå‘½å‘¨æœŸè¿›è¡Œç®¡ç†ï¼Œå…¶ä¸­syncLoopè°ƒç”¨äº†syncLoopIterationå‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®podUpdateçš„ä¿¡æ¯ï¼Œé’ˆå¯¹ä¸åŒçš„æ“ä½œï¼Œç”±SyncHandleræ¥æ‰§è¡Œpodçš„å¢åˆ æ”¹æŸ¥ç­‰ç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†ï¼Œå…¶ä¸­çš„syncHandleråŒ…æ‹¬HandlePodSyncså’ŒHandlePodCleanupsç­‰ã€‚è¯¥éƒ¨åˆ†é€»è¾‘å¾…åç»­æ–‡ç« å…·ä½“åˆ†æã€‚\n6.2. Manager ä»¥ä¸‹ä»‹ç»kubeletè¿è¡Œæ—¶æ¶‰åŠåˆ°çš„managerçš„å†…å®¹ã€‚\nmanager è¯´æ˜ imageManager è´Ÿè´£é•œåƒåƒåœ¾å›æ”¶ serverCertificateManager è´Ÿè´£å¤„ç†è¯ä¹¦ oomWatcher ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œæ˜¯å¦å‘ç”Ÿå†…å­˜è€—å°½å³OOM resourceAnalyzer ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ volumeManager å¯¹podæ‰§è¡Œattached/detached/mounted/unmountedæ“ä½œ statusManager ä½¿ç”¨apiserveråŒæ­¥podsçŠ¶æ€; ä¹Ÿç”¨ä½œçŠ¶æ€ç¼“å­˜ probeManager å¤„ç†å®¹å™¨æ¢é’ˆ runtimeClassManager åŒæ­¥RuntimeClasses podKiller è´Ÿè´£æ€æ­»pod å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kubelet/app/server.go\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go\n","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æstartKubeletï¼Œå…¶ä¸­ä¸»è¦æ˜¯kubelet.Runéƒ¨ â€¦","ref":"/k8s-source-code-analysis/kubelet/startkubelet/","tags":["æºç åˆ†æ"],"title":"kubeletæºç åˆ†æï¼ˆä¸‰ï¼‰ä¹‹ RunKubelet"},{"body":"1. StorageClassæ¦‚è¿° StorageClassæä¾›äº†ä¸€ç§æè¿°å­˜å‚¨ç±»ï¼ˆclassï¼‰çš„æ–¹æ³•ï¼Œä¸åŒçš„classå¯èƒ½ä¼šæ˜ å°„åˆ°ä¸åŒçš„æœåŠ¡è´¨é‡ç­‰çº§å’Œå¤‡ä»½ç­–ç•¥æˆ–å…¶ä»–ç­–ç•¥ç­‰ã€‚\nStorageClassÂ å¯¹è±¡ä¸­åŒ…å«Â provisionerã€parametersÂ å’ŒÂ reclaimPolicyÂ å­—æ®µï¼Œå½“éœ€è¦åŠ¨æ€åˆ†é…Â PersistentVolumeÂ æ—¶ä¼šä½¿ç”¨åˆ°ã€‚å½“åˆ›å»ºÂ StorageClassÂ å¯¹è±¡æ—¶ï¼Œè®¾ç½®åç§°å’Œå…¶ä»–å‚æ•°ï¼Œä¸€æ—¦åˆ›å»ºäº†å¯¹è±¡å°±ä¸èƒ½å†å¯¹å…¶æ›´æ–°ã€‚ä¹Ÿå¯ä»¥ä¸ºæ²¡æœ‰ç”³è¯·ç»‘å®šåˆ°ç‰¹å®š class çš„ PVC æŒ‡å®šä¸€ä¸ªé»˜è®¤çš„Â StorageClassÂ ã€‚\nStorageClasså¯¹è±¡æ–‡ä»¶\nkind: StorageClass apiVersion: storage.k8s.io/v3 metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug 2. StorageClassçš„å±æ€§ 2.1. Provisionerï¼ˆå­˜å‚¨åˆ†é…å™¨ï¼‰ Storage class æœ‰ä¸€ä¸ªåˆ†é…å™¨ï¼ˆprovisionerï¼‰ï¼Œç”¨æ¥å†³å®šä½¿ç”¨å“ªä¸ªå·æ’ä»¶åˆ†é… PVï¼Œè¯¥å­—æ®µå¿…é¡»æŒ‡å®šã€‚å¯ä»¥æŒ‡å®šå†…éƒ¨åˆ†é…å™¨ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šå¤–éƒ¨åˆ†é…å™¨ã€‚å¤–éƒ¨åˆ†é…å™¨çš„ä»£ç åœ°å€ä¸ºï¼šÂ kubernetes-incubator/external-storageï¼Œå…¶ä¸­åŒ…æ‹¬NFSå’ŒCephç­‰ã€‚\n2.2. Reclaim Policyï¼ˆå›æ”¶ç­–ç•¥ï¼‰ å¯ä»¥é€šè¿‡reclaimPolicyå­—æ®µæŒ‡å®šåˆ›å»ºçš„Persistent Volumeçš„å›æ”¶ç­–ç•¥ï¼Œå›æ”¶ç­–ç•¥åŒ…æ‹¬ï¼šDeleteÂ æˆ–è€…Â Retainï¼Œæ²¡æœ‰æŒ‡å®šé»˜è®¤ä¸ºDeleteã€‚\n2.3. Mount Optionsï¼ˆæŒ‚è½½é€‰é¡¹ï¼‰ ç”± storage class åŠ¨æ€åˆ›å»ºçš„ Persistent Volume å°†ä½¿ç”¨ class ä¸­Â mountOptionsÂ å­—æ®µæŒ‡å®šçš„æŒ‚è½½é€‰é¡¹ã€‚\n2.4. å‚æ•° Storage class å…·æœ‰æè¿°å±äº storage class å·çš„å‚æ•°ã€‚å–å†³äºåˆ†é…å™¨ï¼Œå¯ä»¥æ¥å—ä¸åŒçš„å‚æ•°ã€‚ å½“å‚æ•°è¢«çœç•¥æ—¶ï¼Œä¼šä½¿ç”¨é»˜è®¤å€¼ã€‚\nä¾‹å¦‚ä»¥ä¸‹ä½¿ç”¨Ceph RBD\nkind: StorageClass apiVersion: storage.k8s.io/v3 metadata: name: fast provisioner: kubernetes.io/rbd parameters: monitors: 30.36.353.305:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" å¯¹åº”çš„å‚æ•°è¯´æ˜\nmonitorsï¼šCeph monitorï¼Œé€—å·åˆ†éš”ã€‚è¯¥å‚æ•°æ˜¯å¿…éœ€çš„ã€‚\nadminIdï¼šCeph å®¢æˆ·ç«¯ IDï¼Œç”¨äºåœ¨æ± ï¼ˆceph poolï¼‰ä¸­åˆ›å»ºæ˜ åƒã€‚ é»˜è®¤æ˜¯ â€œadminâ€ã€‚\nadminSecretNamespaceï¼šadminSecret çš„ namespaceã€‚é»˜è®¤æ˜¯ â€œdefaultâ€ã€‚\nadminSecretï¼šadminId çš„ Secret åç§°ã€‚è¯¥å‚æ•°æ˜¯å¿…éœ€çš„ã€‚ æä¾›çš„ secret å¿…é¡»æœ‰å€¼ä¸º â€œkubernetes.io/rbdâ€ çš„ type å‚æ•°ã€‚\npool: Ceph RBD æ± . é»˜è®¤æ˜¯ â€œrbdâ€ã€‚\nuserIdï¼šCeph å®¢æˆ·ç«¯ IDï¼Œç”¨äºæ˜ å°„ RBD é•œåƒï¼ˆRBD imageï¼‰ã€‚é»˜è®¤ä¸ adminId ç›¸åŒã€‚\nuserSecretNameï¼šç”¨äºæ˜ å°„ RBD é•œåƒçš„ userId çš„ Ceph Secret çš„åå­—ã€‚ å®ƒå¿…é¡»ä¸ PVC å­˜åœ¨äºç›¸åŒçš„ namespace ä¸­ã€‚è¯¥å‚æ•°æ˜¯å¿…éœ€çš„ã€‚ æä¾›çš„ secret å¿…é¡»å…·æœ‰å€¼ä¸º â€œkubernetes.io/rbdâ€ çš„ type å‚æ•°ï¼Œä¾‹å¦‚ä»¥è¿™æ ·çš„æ–¹å¼åˆ›å»ºï¼š\nkubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" \\ --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \\ --namespace=kube-system fsTypeï¼šKubernetes æ”¯æŒçš„ fsTypeã€‚é»˜è®¤ï¼š\"ext4\"ã€‚\nimageFormatï¼šCeph RBD é•œåƒæ ¼å¼ï¼Œâ€1â€ æˆ–è€… â€œ2â€ã€‚é»˜è®¤å€¼æ˜¯ â€œ1â€ã€‚\nimageFeaturesï¼šè¿™ä¸ªå‚æ•°æ˜¯å¯é€‰çš„ï¼Œåªèƒ½åœ¨ä½ å°† imageFormat è®¾ç½®ä¸º â€œ2â€ æ‰ä½¿ç”¨ã€‚ ç›®å‰æ”¯æŒçš„åŠŸèƒ½åªæ˜¯ layeringã€‚ é»˜è®¤æ˜¯ â€œâ€œï¼Œæ²¡æœ‰åŠŸèƒ½æ‰“å¼€ã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/storage/storage-classes/ ","categories":"","description":"","excerpt":"1. StorageClassæ¦‚è¿° StorageClassæä¾›äº†ä¸€ç§æè¿°å­˜å‚¨ç±»ï¼ˆclassï¼‰çš„æ–¹æ³•ï¼Œä¸åŒçš„classå¯èƒ½ä¼šæ˜ å°„åˆ°ä¸åŒçš„æœåŠ¡ â€¦","ref":"/kubernetes-notes/storage/volume/storage-class/","tags":["Kubernetes"],"title":"StorageClass ä»‹ç»"},{"body":"scheduleOne ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æ/pkg/scheduler/ä¸­è°ƒåº¦çš„åŸºæœ¬æµç¨‹ã€‚å…·ä½“çš„é¢„é€‰è°ƒåº¦é€»è¾‘ã€ä¼˜é€‰è°ƒåº¦é€»è¾‘ã€èŠ‚ç‚¹æŠ¢å é€»è¾‘å¾…åç»­å†ç‹¬ç«‹åˆ†æã€‚\nschedulerçš„pkgä»£ç ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\nscheduler â”œâ”€â”€ algorithm # ä¸»è¦åŒ…å«è°ƒåº¦çš„ç®—æ³• â”‚Â â”œâ”€â”€ predicates # é¢„é€‰çš„ç­–ç•¥ â”‚Â â”œâ”€â”€ priorities # ä¼˜é€‰çš„ç­–ç•¥ â”‚Â â”œâ”€â”€ scheduler_interface.go # ScheduleAlgorithmã€SchedulerExtenderæ¥å£å®šä¹‰ â”‚Â â”œâ”€â”€ types.go # ä½¿ç”¨åˆ°çš„typeçš„å®šä¹‰ â”œâ”€â”€ algorithmprovider â”‚Â â”œâ”€â”€ defaults â”‚Â â”‚Â â”œâ”€â”€ defaults.go # é»˜è®¤ç®—æ³•çš„åˆå§‹åŒ–æ“ä½œï¼ŒåŒ…æ‹¬é¢„é€‰å’Œä¼˜é€‰ç­–ç•¥ â”œâ”€â”€ cache # schedulerè°ƒåº¦ä½¿ç”¨åˆ°çš„cache â”‚Â â”œâ”€â”€ cache.go # schedulerCache â”‚Â â”œâ”€â”€ interface.go â”‚Â â”œâ”€â”€ node_info.go â”‚Â â”œâ”€â”€ node_tree.go â”œâ”€â”€ core # è°ƒåº¦é€»è¾‘çš„æ ¸å¿ƒä»£ç  â”‚Â â”œâ”€â”€ equivalence â”‚Â â”‚Â â”œâ”€â”€ eqivalence.go # å­˜å‚¨ç›¸åŒpodçš„è°ƒåº¦ç»“æœç¼“å­˜ï¼Œä¸»è¦ç»™é¢„é€‰ç­–ç•¥ä½¿ç”¨ â”‚Â â”œâ”€â”€ extender.go â”‚Â â”œâ”€â”€ generic_scheduler.go # genericScheduler,ä¸»è¦åŒ…å«é»˜è®¤è°ƒåº¦å™¨çš„è°ƒåº¦é€»è¾‘ â”‚Â â”œâ”€â”€ scheduling_queue.go # è°ƒåº¦ä½¿ç”¨åˆ°çš„é˜Ÿåˆ—ï¼Œä¸»è¦ç”¨æ¥å­˜å‚¨éœ€è¦è¢«è°ƒåº¦çš„pod â”œâ”€â”€ factory â”‚Â â”œâ”€â”€ factory.go # ä¸»è¦åŒ…æ‹¬NewConfigFactoryã€NewPodInformerï¼Œç›‘å¬podäº‹ä»¶æ¥æ›´æ–°è°ƒåº¦é˜Ÿåˆ— â”œâ”€â”€ metrics â”‚Â â””â”€â”€ metrics.go # ä¸»è¦ç»™prometheusä½¿ç”¨ â”œâ”€â”€ scheduler.go # pkgéƒ¨åˆ†çš„Runå…¥å£(æ ¸å¿ƒä»£ç )ï¼Œä¸»è¦åŒ…å«Runã€scheduleOneã€scheduleã€preemptç­‰å‡½æ•° â””â”€â”€ volumebinder â””â”€â”€ volume_binder.go # volume bind 1. Scheduler.Run æ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/scheduler.go\næ­¤å¤„ä¸ºå…·ä½“è°ƒåº¦é€»è¾‘çš„å…¥å£ã€‚\n// Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 2. Scheduler.scheduleOne æ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/scheduler.go\nscheduleOneä¸»è¦ä¸ºå•ä¸ªpodé€‰æ‹©ä¸€ä¸ªé€‚åˆçš„èŠ‚ç‚¹ï¼Œä¸ºè°ƒåº¦é€»è¾‘çš„æ ¸å¿ƒå‡½æ•°ã€‚\nå¯¹å•ä¸ªpodè¿›è¡Œè°ƒåº¦çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡podQueueçš„å¾…è°ƒåº¦é˜Ÿåˆ—ä¸­å¼¹å‡ºéœ€è¦è°ƒåº¦çš„podã€‚ é€šè¿‡å…·ä½“çš„è°ƒåº¦ç®—æ³•ä¸ºè¯¥podé€‰å‡ºåˆé€‚çš„èŠ‚ç‚¹ï¼Œå…¶ä¸­è°ƒåº¦ç®—æ³•å°±åŒ…æ‹¬é¢„é€‰å’Œä¼˜é€‰ä¸¤æ­¥ç­–ç•¥ã€‚ å¦‚æœä¸Šè¿°è°ƒåº¦å¤±è´¥ï¼Œåˆ™ä¼šå°è¯•æŠ¢å æœºåˆ¶ï¼Œå°†ä¼˜å…ˆçº§ä½çš„podå‰”é™¤ï¼Œè®©ä¼˜å…ˆçº§é«˜çš„podè°ƒåº¦æˆåŠŸã€‚ å°†è¯¥podå’Œé€‰å®šçš„èŠ‚ç‚¹è¿›è¡Œå‡æ€§ç»‘å®šï¼Œå­˜å…¥scheduler cacheä¸­ï¼Œæ–¹ä¾¿å…·ä½“ç»‘å®šæ“ä½œå¯ä»¥å¼‚æ­¥è¿›è¡Œã€‚ å®é™…æ‰§è¡Œç»‘å®šæ“ä½œï¼Œå°†nodeçš„åå­—æ·»åŠ åˆ°podçš„èŠ‚ç‚¹ç›¸å…³å±æ€§ä¸­ã€‚ å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. func (sched *Scheduler) scheduleOne() { pod := sched.config.NextPod() if pod.DeletionTimestamp != nil { sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) glog.V(3).Infof(\"Skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) return } glog.V(3).Infof(\"Attempting to schedule pod: %v/%v\", pod.Namespace, pod.Name) // Synchronously attempt to find a fit for the pod. start := time.Now() suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInMicroseconds(start)) // Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPod := pod.DeepCopy() // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then allBound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumedPod' if volume binding is required. allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) if err != nil { return } // assume modifies `assumedPod` by setting NodeName=suggestedHost err = sched.assume(assumedPod, suggestedHost) if err != nil { return } // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod if !allBound { err = sched.bindVolumes(assumedPod) if err != nil { return } } err := sched.bind(assumedPod, \u0026v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) metrics.E2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start)) if err != nil { glog.Errorf(\"Internal error binding pod: (%v)\", err) } }() } ä»¥ä¸‹å¯¹é‡è¦ä»£ç åˆ†åˆ«è¿›è¡Œåˆ†æã€‚\n3. config.NextPod é€šè¿‡podQueueçš„æ–¹å¼å­˜å‚¨å¾…è°ƒåº¦çš„podé˜Ÿåˆ—ï¼ŒNextPodæ‹¿å‡ºä¸‹ä¸€ä¸ªéœ€è¦è¢«è°ƒåº¦çš„podã€‚\npod := sched.config.NextPod() if pod.DeletionTimestamp != nil { sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) glog.V(3).Infof(\"Skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) return } glog.V(3).Infof(\"Attempting to schedule pod: %v/%v\", pod.Namespace, pod.Name) NextPodçš„å…·ä½“å‡½æ•°åœ¨factory.goçš„CreateFromKeyå‡½æ•°ä¸­å®šä¹‰ï¼Œå¦‚ä¸‹ï¼š\nfunc (c *configFactory) CreateFromKeys(predicateKeys, priorityKeys sets.String, extenders []algorithm.SchedulerExtender) (*scheduler.Config, error) { ... return \u0026scheduler.Config{ ... NextPod: func() *v1.Pod { return c.getNextPod() } ... } 3.1. getNextPod é€šè¿‡ä¸€ä¸ªpodQueueæ¥å­˜å‚¨éœ€è¦è°ƒåº¦çš„podçš„é˜Ÿåˆ—ï¼Œé€šè¿‡é˜Ÿåˆ—Popçš„æ–¹å¼å¼¹å‡ºéœ€è¦è¢«è°ƒåº¦çš„podã€‚\nfunc (c *configFactory) getNextPod() *v1.Pod { pod, err := c.podQueue.Pop() if err == nil { glog.V(4).Infof(\"About to try and schedule pod %v/%v\", pod.Namespace, pod.Name) return pod } glog.Errorf(\"Error while retrieving next pod from scheduling queue: %v\", err) return nil } 4. Scheduler.schedule æ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/scheduler.go\næ­¤éƒ¨åˆ†ä¸ºè°ƒåº¦é€»è¾‘çš„æ ¸å¿ƒï¼Œé€šè¿‡ä¸åŒçš„ç®—æ³•ä¸ºå…·ä½“çš„podé€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„èŠ‚ç‚¹ã€‚\n// Synchronously attempt to find a fit for the pod. start := time.Now() suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } scheduleé€šè¿‡è°ƒåº¦ç®—æ³•è¿”å›ä¸€ä¸ªæœ€ä¼˜çš„èŠ‚ç‚¹ã€‚\n// schedule implements the scheduling algorithm and returns the suggested host. func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) { host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil { pod = pod.DeepCopy() sched.config.Error(pod, err) sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"%v\", err) sched.config.PodConditionUpdater.Update(pod, \u0026v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: v1.PodReasonUnschedulable, Message: err.Error(), }) return \"\", err } return host, err } 4.1. ScheduleAlgorithm ScheduleAlgorithmæ˜¯ä¸€ä¸ªè°ƒåº¦ç®—æ³•çš„æ¥å£ï¼Œä¸»è¦çš„å®ç°ä½“æ˜¯genericSchedulerï¼Œåç»­åˆ†ægenericScheduler.Scheduleã€‚\nScheduleAlgorithmæ¥å£å®šä¹‰å¦‚ä¸‹ï¼š\n// ScheduleAlgorithm is an interface implemented by things that know how to schedule pods // onto machines. type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt receives scheduling errors for a pod and tries to create room for // the pod by preempting lower priority pods if possible. // It returns the node where preemption happened, a list of preempted pods, a // list of pods whose nominated node name should be removed, and error if any. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) // Predicates() returns a pointer to a map of predicate functions. This is // exposed for testing. Predicates() map[string]FitPredicate // Prioritizers returns a slice of priority config. This is exposed for // testing. Prioritizers() []PriorityConfig } 5. genericScheduler.Schedule æ­¤éƒ¨åˆ†ä»£ç ä½äº/pkg/scheduler/core/generic_scheduler.go\ngenericScheduler.Scheduleå®ç°äº†åŸºæœ¬çš„è°ƒåº¦é€»è¾‘ï¼ŒåŸºäºç»™å®šéœ€è¦è°ƒåº¦çš„podå’Œnodeåˆ—è¡¨ï¼Œå¦‚æœæ‰§è¡ŒæˆåŠŸè¿”å›è°ƒåº¦çš„èŠ‚ç‚¹çš„åå­—ï¼Œå¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œåˆ™è¿”å›é”™è¯¯å’ŒåŸå› ã€‚ä¸»è¦é€šè¿‡é¢„é€‰å’Œä¼˜é€‰ä¸¤æ­¥æ“ä½œå®Œæˆè°ƒåº¦çš„é€»è¾‘ã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nå¯¹podåšåŸºæœ¬æ€§æ£€æŸ¥ï¼Œç›®å‰ä¸»è¦æ˜¯å¯¹pvcçš„æ£€æŸ¥ã€‚ é€šè¿‡findNodesThatFité¢„é€‰ç­–ç•¥é€‰å‡ºæ»¡è¶³è°ƒåº¦æ¡ä»¶çš„nodeåˆ—è¡¨ã€‚ é€šè¿‡PrioritizeNodesä¼˜é€‰ç­–ç•¥ç»™é¢„é€‰çš„nodeåˆ—è¡¨ä¸­çš„nodeè¿›è¡Œæ‰“åˆ†ã€‚ åœ¨æ‰“åˆ†çš„nodeåˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªåˆ†æ•°æœ€é«˜çš„nodeä½œä¸ºè°ƒåº¦çš„èŠ‚ç‚¹ã€‚ å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { trace := utiltrace.New(fmt.Sprintf(\"Scheduling %s/%s\", pod.Namespace, pod.Name)) defer trace.LogIfLong(100 * time.Millisecond) if err := podPassesBasicChecks(pod, g.pvcLister); err != nil { return \"\", err } nodes, err := nodeLister.List() if err != nil { return \"\", err } if len(nodes) == 0 { return \"\", ErrNoNodesAvailable } // Used for all fit and priority funcs. err = g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return \"\", err } trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", \u0026FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) trace.Step(\"Selecting host\") return g.selectHost(priorityList) } 5.1. podPassesBasicChecks podPassesBasicChecksä¸»è¦åšä¸€ä¸‹åŸºæœ¬æ€§æ£€æŸ¥ï¼Œç›®å‰ä¸»è¦æ˜¯å¯¹pvcçš„æ£€æŸ¥ã€‚\nif err := podPassesBasicChecks(pod, g.pvcLister); err != nil { return \"\", err } podPassesBasicCheckså…·ä½“å®ç°å¦‚ä¸‹ï¼š\n// podPassesBasicChecks makes sanity checks on the pod if it can be scheduled. func podPassesBasicChecks(pod *v1.Pod, pvcLister corelisters.PersistentVolumeClaimLister) error { // Check PVCs used by the pod namespace := pod.Namespace manifest := \u0026(pod.Spec) for i := range manifest.Volumes { volume := \u0026manifest.Volumes[i] if volume.PersistentVolumeClaim == nil { // Volume is not a PVC, ignore continue } pvcName := volume.PersistentVolumeClaim.ClaimName pvc, err := pvcLister.PersistentVolumeClaims(namespace).Get(pvcName) if err != nil { // The error has already enough context (\"persistentvolumeclaim \"myclaim\" not found\") return err } if pvc.DeletionTimestamp != nil { return fmt.Errorf(\"persistentvolumeclaim %q is being deleted\", pvc.Name) } } return nil } 5.2. findNodesThatFit é¢„é€‰ï¼Œé€šè¿‡é¢„é€‰å‡½æ•°æ¥åˆ¤æ–­æ¯ä¸ªèŠ‚ç‚¹æ˜¯å¦é€‚åˆè¢«è¯¥Podè°ƒåº¦ã€‚\nå…·ä½“çš„findNodesThatFitä»£ç å®ç°ç»†èŠ‚å¾…åç»­æ–‡ç« ç‹¬ç«‹åˆ†æã€‚\ngenericScheduler.Scheduleä¸­å¯¹findNodesThatFitçš„è°ƒç”¨è¿‡ç¨‹å¦‚ä¸‹ï¼š\ntrace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", \u0026FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) 5.3. PrioritizeNodes ä¼˜é€‰ï¼Œä»æ»¡è¶³çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å‡ºæœ€ä¼˜çš„èŠ‚ç‚¹ã€‚\nå…·ä½“æ“ä½œå¦‚ä¸‹ï¼š\nPrioritizeNodesé€šè¿‡å¹¶è¡Œè¿è¡Œå„ä¸ªä¼˜å…ˆçº§å‡½æ•°æ¥å¯¹èŠ‚ç‚¹è¿›è¡Œä¼˜å…ˆçº§æ’åºã€‚ æ¯ä¸ªä¼˜å…ˆçº§å‡½æ•°ä¼šç»™èŠ‚ç‚¹æ‰“åˆ†ï¼Œæ‰“åˆ†èŒƒå›´ä¸º0-10åˆ†ã€‚ 0 è¡¨ç¤ºä¼˜å…ˆçº§æœ€ä½çš„èŠ‚ç‚¹ï¼Œ10è¡¨ç¤ºä¼˜å…ˆçº§æœ€é«˜çš„èŠ‚ç‚¹ã€‚ æ¯ä¸ªä¼˜å…ˆçº§å‡½æ•°ä¹Ÿæœ‰å„è‡ªçš„æƒé‡ã€‚ ä¼˜å…ˆçº§å‡½æ•°è¿”å›çš„èŠ‚ç‚¹åˆ†æ•°ä¹˜ä»¥æƒé‡ä»¥è·å¾—åŠ æƒåˆ†æ•°ã€‚ æœ€åç»„åˆï¼ˆæ·»åŠ ï¼‰æ‰€æœ‰åˆ†æ•°ä»¥è·å¾—æ‰€æœ‰èŠ‚ç‚¹çš„æ€»åŠ æƒåˆ†æ•°ã€‚ å…·ä½“PrioritizeNodesçš„å®ç°é€»è¾‘å¾…åç»­æ–‡ç« ç‹¬ç«‹åˆ†æã€‚\ngenericScheduler.Scheduleä¸­å¯¹PrioritizeNodesçš„è°ƒç”¨è¿‡ç¨‹å¦‚ä¸‹ï¼š\ntrace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) 5.4. selectHost scheduleråœ¨æœ€åä¼šä»priorityListä¸­é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚\ntrace.Step(\"Selecting host\") return g.selectHost(priorityList) selectHostè·å–ä¼˜å…ˆçº§çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œç„¶åä»åˆ†æ•°æœ€é«˜çš„èŠ‚ç‚¹ä»¥å¾ªç¯æ–¹å¼é€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹ã€‚\nå…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// selectHost takes a prioritized list of nodes and then picks one // in a round-robin manner from the nodes that had the highest score. func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) { if len(priorityList) == 0 { return \"\", fmt.Errorf(\"empty priorityList\") } maxScores := findMaxScores(priorityList) ix := int(g.lastNodeIndex % uint64(len(maxScores))) g.lastNodeIndex++ return priorityList[maxScores[ix]].Host, nil } 5.4.1. findMaxScores findMaxScoresè¿”å›priorityListä¸­å…·æœ‰æœ€é«˜Scoreçš„èŠ‚ç‚¹çš„ç´¢å¼•ã€‚\n// findMaxScores returns the indexes of nodes in the \"priorityList\" that has the highest \"Score\". func findMaxScores(priorityList schedulerapi.HostPriorityList) []int { maxScoreIndexes := make([]int, 0, len(priorityList)/2) maxScore := priorityList[0].Score for i, hp := range priorityList { if hp.Score \u003e maxScore { maxScore = hp.Score maxScoreIndexes = maxScoreIndexes[:0] maxScoreIndexes = append(maxScoreIndexes, i) } else if hp.Score == maxScore { maxScoreIndexes = append(maxScoreIndexes, i) } } return maxScoreIndexes } 6. Scheduler.preempt å¦‚æœpodåœ¨é¢„é€‰å’Œä¼˜é€‰è°ƒåº¦ä¸­å¤±è´¥ï¼Œåˆ™æ‰§è¡ŒæŠ¢å æ“ä½œã€‚æŠ¢å ä¸»è¦æ˜¯å°†ä½ä¼˜å…ˆçº§çš„podçš„èµ„æºç©ºé—´è…¾å‡ºç»™å¾…è°ƒåº¦çš„é«˜ä¼˜å…ˆçº§çš„podã€‚\nå…·ä½“Scheduler.preemptçš„å®ç°é€»è¾‘å¾…åç»­æ–‡ç« ç‹¬ç«‹åˆ†æã€‚\nsuggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } 7. Scheduler.assume å°†è¯¥podå’Œé€‰å®šçš„èŠ‚ç‚¹è¿›è¡Œå‡æ€§ç»‘å®šï¼Œå­˜å…¥scheduler cacheä¸­ï¼Œæ–¹ä¾¿å¯ä»¥ç»§ç»­æ‰§è¡Œè°ƒåº¦é€»è¾‘ï¼Œè€Œä¸éœ€è¦ç­‰å¾…ç»‘å®šæ“ä½œçš„å‘ç”Ÿï¼Œå…·ä½“ç»‘å®šæ“ä½œå¯ä»¥å¼‚æ­¥è¿›è¡Œã€‚\n// Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPod := pod.DeepCopy() // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then allBound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumedPod' if volume binding is required. allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) if err != nil { return } // assume modifies `assumedPod` by setting NodeName=suggestedHost err = sched.assume(assumedPod, suggestedHost) if err != nil { return } å¦‚æœå‡æ€§ç»‘å®šæˆåŠŸåˆ™å‘é€è¯·æ±‚ç»™apiserverï¼Œå¦‚æœå¤±è´¥åˆ™schedulerä¼šç«‹å³é‡Šæ”¾å·²åˆ†é…ç»™å‡æ€§ç»‘å®šçš„podçš„èµ„æºã€‚\nassumeæ–¹æ³•çš„å…·ä½“å®ç°ï¼š\n// assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous. // assume modifies `assumed`. func (sched *Scheduler) assume(assumed *v1.Pod, host string) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed.Spec.NodeName = host // NOTE: Because the scheduler uses snapshots of SchedulerCache and the live // version of Ecache, updates must be written to SchedulerCache before // invalidating Ecache. if err := sched.config.SchedulerCache.AssumePod(assumed); err != nil { glog.Errorf(\"scheduler cache AssumePod failed: %v\", err) // This is most probably result of a BUG in retrying logic. // We report an error here so that pod scheduling can be retried. // This relies on the fact that Error will check if the pod has been bound // to a node and if so will not add it back to the unscheduled pods queue // (otherwise this would cause an infinite loop). sched.config.Error(assumed, err) sched.config.Recorder.Eventf(assumed, v1.EventTypeWarning, \"FailedScheduling\", \"AssumePod failed: %v\", err) sched.config.PodConditionUpdater.Update(assumed, \u0026v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: \"SchedulerError\", Message: err.Error(), }) return err } // Optimistically assume that the binding will succeed, so we need to invalidate affected // predicates in equivalence cache. // If the binding fails, these invalidated item will not break anything. if sched.config.Ecache != nil { sched.config.Ecache.InvalidateCachedPredicateItemForPodAdd(assumed, host) } return nil } 8. Scheduler.bind å¼‚æ­¥çš„æ–¹å¼ç»™podç»‘å®šåˆ°å…·ä½“çš„è°ƒåº¦èŠ‚ç‚¹ä¸Šã€‚\n// bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod if !allBound { err = sched.bindVolumes(assumedPod) if err != nil { return } } err := sched.bind(assumedPod, \u0026v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) metrics.E2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start)) if err != nil { glog.Errorf(\"Internal error binding pod: (%v)\", err) } }() bindå…·ä½“å®ç°å¦‚ä¸‹ï¼š\n// bind binds a pod to a given node defined in a binding object. We expect this to run asynchronously, so we // handle binding metrics internally. func (sched *Scheduler) bind(assumed *v1.Pod, b *v1.Binding) error { bindingStart := time.Now() // If binding succeeded then PodScheduled condition will be updated in apiserver so that // it's atomic with setting host. err := sched.config.GetBinder(assumed).Bind(b) if err := sched.config.SchedulerCache.FinishBinding(assumed); err != nil { glog.Errorf(\"scheduler cache FinishBinding failed: %v\", err) } if err != nil { glog.V(1).Infof(\"Failed to bind pod: %v/%v\", assumed.Namespace, assumed.Name) if err := sched.config.SchedulerCache.ForgetPod(assumed); err != nil { glog.Errorf(\"scheduler cache ForgetPod failed: %v\", err) } sched.config.Error(assumed, err) sched.config.Recorder.Eventf(assumed, v1.EventTypeWarning, \"FailedScheduling\", \"Binding rejected: %v\", err) sched.config.PodConditionUpdater.Update(assumed, \u0026v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: \"BindingRejected\", }) return err } metrics.BindingLatency.Observe(metrics.SinceInMicroseconds(bindingStart)) metrics.SchedulingLatency.WithLabelValues(metrics.Binding).Observe(metrics.SinceInSeconds(bindingStart)) sched.config.Recorder.Eventf(assumed, v1.EventTypeNormal, \"Scheduled\", \"Successfully assigned %v/%v to %v\", assumed.Namespace, assumed.Name, b.Target.Name) return nil } 9. æ€»ç»“ æœ¬æ–‡ä¸»è¦åˆ†æäº†å•ä¸ªpodçš„è°ƒåº¦è¿‡ç¨‹ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡podQueueçš„å¾…è°ƒåº¦é˜Ÿåˆ—ä¸­å¼¹å‡ºéœ€è¦è°ƒåº¦çš„podã€‚ é€šè¿‡å…·ä½“çš„è°ƒåº¦ç®—æ³•ä¸ºè¯¥podé€‰å‡ºåˆé€‚çš„èŠ‚ç‚¹ï¼Œå…¶ä¸­è°ƒåº¦ç®—æ³•å°±åŒ…æ‹¬é¢„é€‰å’Œä¼˜é€‰ä¸¤æ­¥ç­–ç•¥ã€‚ å¦‚æœä¸Šè¿°è°ƒåº¦å¤±è´¥ï¼Œåˆ™ä¼šå°è¯•æŠ¢å æœºåˆ¶ï¼Œå°†ä¼˜å…ˆçº§ä½çš„podå‰”é™¤ï¼Œè®©ä¼˜å…ˆçº§é«˜çš„podè°ƒåº¦æˆåŠŸã€‚ å°†è¯¥podå’Œé€‰å®šçš„èŠ‚ç‚¹è¿›è¡Œå‡æ€§ç»‘å®šï¼Œå­˜å…¥scheduler cacheä¸­ï¼Œæ–¹ä¾¿å…·ä½“ç»‘å®šæ“ä½œå¯ä»¥å¼‚æ­¥è¿›è¡Œã€‚ å®é™…æ‰§è¡Œç»‘å®šæ“ä½œï¼Œå°†nodeçš„åå­—æ·»åŠ åˆ°podçš„èŠ‚ç‚¹ç›¸å…³å±æ€§ä¸­ã€‚ å…¶ä¸­æ ¸å¿ƒçš„éƒ¨åˆ†ä¸ºé€šè¿‡å…·ä½“çš„è°ƒåº¦ç®—æ³•é€‰å‡ºè°ƒåº¦èŠ‚ç‚¹çš„è¿‡ç¨‹ï¼Œå³genericScheduler.Scheduleçš„å®ç°éƒ¨åˆ†ã€‚è¯¥éƒ¨åˆ†åŒ…æ‹¬é¢„é€‰å’Œä¼˜é€‰ä¸¤ä¸ªéƒ¨åˆ†ã€‚\ngenericScheduler.Scheduleè°ƒåº¦çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nå¯¹podåšåŸºæœ¬æ€§æ£€æŸ¥ï¼Œç›®å‰ä¸»è¦æ˜¯å¯¹pvcçš„æ£€æŸ¥ã€‚ é€šè¿‡findNodesThatFité¢„é€‰ç­–ç•¥é€‰å‡ºæ»¡è¶³è°ƒåº¦æ¡ä»¶çš„nodeåˆ—è¡¨ã€‚ é€šè¿‡PrioritizeNodesä¼˜é€‰ç­–ç•¥ç»™é¢„é€‰çš„nodeåˆ—è¡¨ä¸­çš„nodeè¿›è¡Œæ‰“åˆ†ã€‚ åœ¨æ‰“åˆ†çš„nodeåˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªåˆ†æ•°æœ€é«˜çš„nodeä½œä¸ºè°ƒåº¦çš„èŠ‚ç‚¹ã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go ","categories":"","description":"","excerpt":"scheduleOne ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æ/pkg/scheduler/ä¸­è°ƒåº¦çš„åŸº â€¦","ref":"/k8s-source-code-analysis/kube-scheduler/scheduleone/","tags":["æºç åˆ†æ"],"title":"kube-scheduleræºç åˆ†æï¼ˆä¸‰ï¼‰ä¹‹ è°ƒåº¦æµç¨‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/principle/","tags":"","title":"æ ¸å¿ƒåŸç†"},{"body":"åç«¯å¼€å‘æŠ€èƒ½æ ‘ å›¾ç‰‡æ¥æºäºç½‘ç»œ\nå‚è€ƒï¼š\nhttps://github.com/xingshaocheng/architect-awesome ","categories":"","description":"","excerpt":"åç«¯å¼€å‘æŠ€èƒ½æ ‘ å›¾ç‰‡æ¥æºäºç½‘ç»œ\nå‚è€ƒï¼š\nhttps://github.com/xingshaocheng/architect-awesome â€¦","ref":"/golang-notes/summary/skill-tree/","tags":["Golang"],"title":"åç«¯å¼€å‘æŠ€èƒ½æ ‘"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/registry/","tags":"","title":"é•œåƒä»“åº“"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/oop/","tags":"","title":"é¢å‘å¯¹è±¡ç¼–ç¨‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/concepts/configmap/","tags":"","title":"é…ç½®"},{"body":"channelçš„åŸç† channelçš„ä½œç”¨æ˜¯è§£å†³goroutineä¹‹é—´çš„é€šä¿¡é—®é¢˜ã€‚ä¸è¦é€šè¿‡å…±äº«å†…å­˜æ¥é€šä¿¡ï¼Œè€Œåº”è¯¥é€šè¿‡é€šä¿¡æ¥å…±äº«å†…å­˜ã€‚\n1. channelçš„ç‰¹æ€§ goroutineå®‰å…¨ æä¾›FIFOè¯­ä¹‰(buffered channelæä¾›)ï¼Œç¼“å†²å¤§å° åœ¨ä¸åŒçš„goroutineä¹‹é—´å­˜å‚¨å’Œä¼ è¾“å€¼ å¯ä»¥è®©goroutine block/unblock 2. channelçš„æ•°æ®ç»“æ„ type hchan struct { qcount uint // å½“å‰é˜Ÿåˆ—ä¸­å‰©ä½™å…ƒç´ ä¸ªæ•° dataqsiz uint // ç¯å½¢é˜Ÿåˆ—é•¿åº¦ï¼Œå³å¯ä»¥å­˜æ”¾çš„å…ƒç´ ä¸ªæ•° buf unsafe.Pointer // ç¯å½¢é˜Ÿåˆ—æŒ‡é’ˆ elemsize uint16 // æ¯ä¸ªå…ƒç´ çš„å¤§å° closed uint32 // æ ‡è¯†å…³é—­çŠ¶æ€ elemtype *_type // å…ƒç´ ç±»å‹ sendx uint // é˜Ÿåˆ—ä¸‹æ ‡ï¼ŒæŒ‡ç¤ºå…ƒç´ å†™å…¥æ—¶å­˜æ”¾åˆ°é˜Ÿåˆ—ä¸­çš„ä½ç½® recvx uint // é˜Ÿåˆ—ä¸‹æ ‡ï¼ŒæŒ‡ç¤ºå…ƒç´ ä»é˜Ÿåˆ—çš„è¯¥ä½ç½®è¯»å‡º recvq waitq // ç­‰å¾…è¯»æ¶ˆæ¯çš„goroutineé˜Ÿåˆ— sendq waitq // ç­‰å¾…å†™æ¶ˆæ¯çš„goroutineé˜Ÿåˆ— lock mutex // äº’æ–¥é”ï¼Œchanä¸å…è®¸å¹¶å‘è¯»å†™ } hchanç»´æŠ¤äº†ä¸¤ä¸ªé“¾è¡¨ï¼Œrecvqæ˜¯å› è¯»è¿™ä¸ªchanè€Œé˜»å¡çš„Gï¼Œsendqåˆ™æ˜¯å› å†™è¿™ä¸ªchanè€Œé˜»å¡çš„Gã€‚waitqé˜Ÿåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„æ•°æ®ç»“æ„ä¸ºsudogï¼Œå…¶ä¸­elemç”¨äºä¿å­˜æ•°æ®ã€‚\n3. åˆ›å»ºchannel makeå‡½æ•°åœ¨åˆ›å»ºchannelçš„æ—¶å€™ä¼šåœ¨è¯¥è¿›ç¨‹çš„heapåŒºç”³è¯·ä¸€å—å†…å­˜ï¼Œåˆ›å»ºä¸€ä¸ªhchanç»“æ„ä½“ï¼Œè¿”å›æ‰§è¡Œè¯¥å†…å­˜çš„æŒ‡é’ˆï¼Œæ‰€ä»¥è·å–çš„çš„chå˜é‡æœ¬èº«å°±æ˜¯ä¸€ä¸ªæŒ‡é’ˆï¼Œåœ¨å‡½æ•°ä¹‹é—´ä¼ é€’çš„æ—¶å€™æ˜¯åŒä¸€ä¸ªchannelã€‚\nhchanç»“æ„ä½“ä½¿ç”¨ä¸€ä¸ªç¯å½¢é˜Ÿåˆ—æ¥ä¿å­˜groutineä¹‹é—´ä¼ é€’çš„æ•°æ®(å¦‚æœæ˜¯ç¼“å­˜channelçš„è¯)ï¼Œä½¿ç”¨ä¸¤ä¸ªlistä¿å­˜åƒè¯¥chanå‘é€å’Œä»æ”¹chanæ¥æ”¶æ•°æ®çš„goroutineï¼Œè¿˜æœ‰ä¸€ä¸ªmutexæ¥ä¿è¯æ“ä½œè¿™äº›ç»“æ„çš„å®‰å…¨ã€‚\n4. å†™å…¥channel recvqå­˜æ”¾è¯»å–channelé˜»å¡çš„Gï¼Œæ­¤æ—¶è¯´æ˜channelé‡Œé¢æ²¡æœ‰æ•°æ®ã€‚sendqå­˜æ”¾å†™å…¥channelé˜»å¡çš„Gï¼Œæ­¤æ—¶è¯´æ˜channelå·²ç»æ»¡äº†ã€‚\nå¦‚æœç­‰å¾…æ¥æ”¶é˜Ÿåˆ—recvqä¸ä¸ºç©ºï¼Œè¯´æ˜ç¼“å†²åŒºä¸­æ²¡æœ‰æ•°æ®æˆ–è€…æ²¡æœ‰ç¼“å†²åŒºï¼Œæ­¤æ—¶ç›´æ¥ä»recvqå–å‡ºG,å¹¶æŠŠæ•°æ®å†™å…¥ï¼Œæœ€åæŠŠè¯¥recvqçš„Gå”¤é†’ï¼Œç»“æŸå‘é€è¿‡ç¨‹ï¼› å¦‚æœç¼“å†²åŒºä¸­æœ‰ç©ºä½™ä½ç½®ï¼Œå°†æ•°æ®å†™å…¥ç¼“å†²åŒºï¼Œç»“æŸå‘é€è¿‡ç¨‹ï¼› å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œå°†å¾…å‘é€æ•°æ®å†™å…¥Gï¼Œå°†å½“å‰GåŠ å…¥sendqï¼Œè¿›å…¥ç¡çœ ï¼ˆé˜»å¡çŠ¶æ€ï¼‰ï¼Œç­‰å¾…è¢«è¯»goroutineå”¤é†’ï¼› 5. è¯»å‡ºchannel å¦‚æœç­‰å¾…å‘é€é˜Ÿåˆ—sendqä¸ä¸ºç©ºï¼Œä¸”æ²¡æœ‰ç¼“å†²åŒºï¼Œç›´æ¥ä»sendqä¸­å–å‡ºGï¼ŒæŠŠGä¸­æ•°æ®è¯»å‡ºï¼Œæœ€åæŠŠGå”¤é†’ï¼Œç»“æŸè¯»å–è¿‡ç¨‹ï¼› å¦‚æœç­‰å¾…å‘é€é˜Ÿåˆ—sendqä¸ä¸ºç©ºï¼Œæ­¤æ—¶è¯´æ˜ç¼“å†²åŒºå·²æ»¡ï¼Œä»ç¼“å†²åŒºä¸­é¦–éƒ¨è¯»å‡ºæ•°æ®ï¼ŒæŠŠGä¸­æ•°æ®å†™å…¥ç¼“å†²åŒºå°¾éƒ¨ï¼ŒæŠŠGå”¤é†’ï¼Œç»“æŸè¯»å–è¿‡ç¨‹ï¼› å¦‚æœç­‰å¾…å‘é€é˜Ÿåˆ—sendqä¸ºç©ºï¼Œè¯´æ˜ç¼“å†²åŒºä¸­æœ‰æ•°æ®ï¼Œåˆ™ä»ç¼“å†²åŒºå–å‡ºæ•°æ®ï¼Œç»“æŸè¯»å–è¿‡ç¨‹ï¼› å¦‚æœchannelè¯»å–ä¸åˆ°æ•°æ®ï¼Œå°†å½“å‰goroutineåŠ å…¥recvqï¼Œè¿›å…¥ç¡çœ ï¼ˆé˜»å¡çŠ¶æ€ï¼‰ï¼Œç­‰å¾…è¢«å†™goroutineå”¤é†’ï¼› 6. å…³é—­channel å°† c.closed è®¾ç½®ä¸º 1\nå”¤é†’ recvq é˜Ÿåˆ—é‡Œé¢çš„é˜»å¡ goroutine\nå”¤é†’ sendq é˜Ÿåˆ—é‡Œé¢çš„é˜»å¡ goroutine\n7. é˜»å¡ å½“G1å‘bufå·²ç»æ»¡äº†çš„chå‘é€æ•°æ®çš„æ—¶å€™ï¼Œå½“runtineæ£€æµ‹åˆ°å¯¹åº”çš„hchançš„bufå·²ç»æ»¡äº†ï¼Œä¼šé€šçŸ¥è°ƒåº¦å™¨ï¼Œè°ƒåº¦å™¨ä¼šå°†G1çš„çŠ¶æ€è®¾ç½®ä¸ºwaiting, ç§»é™¤ä¸çº¿ç¨‹Mçš„è”ç³»ï¼Œç„¶åä»Pçš„runqueueä¸­é€‰æ‹©ä¸€ä¸ªgoroutineåœ¨çº¿ç¨‹Mä¸­æ‰§è¡Œï¼Œæ­¤æ—¶G1å°±æ˜¯é˜»å¡çŠ¶æ€ï¼Œä½†æ˜¯ä¸æ˜¯æ“ä½œç³»ç»Ÿçš„çº¿ç¨‹é˜»å¡ï¼Œæ‰€ä»¥è¿™ä¸ªæ—¶å€™åªç”¨æ¶ˆè€—å°‘é‡çš„èµ„æºã€‚\n8. å”¤é†’ å½“G1å˜ä¸ºwaitingçŠ¶æ€åï¼Œä¼šåˆ›å»ºä¸€ä¸ªä»£è¡¨è‡ªå·±çš„sudogçš„ç»“æ„ï¼Œç„¶åæ”¾åˆ°sendqè¿™ä¸ªlistä¸­ï¼Œsudogç»“æ„ä¸­ä¿å­˜äº†channelç›¸å…³çš„å˜é‡çš„æŒ‡é’ˆã€‚å½“G2ä»chä¸­æ¥æ”¶ä¸€ä¸ªæ•°æ®æ—¶ï¼Œä¼šé€šçŸ¥è°ƒåº¦å™¨ï¼Œè®¾ç½®G1çš„çŠ¶æ€ä¸ºrunnableï¼Œç„¶åå°†åŠ å…¥Pçš„runqueueé‡Œï¼Œç­‰å¾…çº¿ç¨‹æ‰§è¡Œ.\n","categories":"","description":"","excerpt":"channelçš„åŸç† channelçš„ä½œç”¨æ˜¯è§£å†³goroutineä¹‹é—´çš„é€šä¿¡é—®é¢˜ã€‚ä¸è¦é€šè¿‡å…±äº«å†…å­˜æ¥é€šä¿¡ï¼Œè€Œåº”è¯¥é€šè¿‡é€šä¿¡æ¥å…±äº«å†…å­˜ã€‚\n1. â€¦","ref":"/golang-notes/principle/understand-channel/","tags":["Golang"],"title":"æ·±å…¥ç†è§£Channel"},{"body":"1. å®‰è£…kind On mac or linux\ncurl -Lo ./kind \"https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-$(uname)-amd64\" chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind 2. åˆ›å»ºk8sé›†ç¾¤ $ kind create cluster Creating cluster \"kind\" ... âœ“ Ensuring node image (kindest/node:v1.17.0) ğŸ–¼ âœ“ Preparing nodes ğŸ“¦ âœ“ Writing configuration ğŸ“œ âœ“ Starting control-plane ğŸ•¹ï¸ âœ“ Installing CNI ğŸ”Œ âœ“ Installing StorageClass ğŸ’¾ Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Not sure what to do next? ğŸ˜… Check out https://kind.sigs.k8s.io/docs/user/quick-start/ æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯\n$ kubectl cluster-info --context kind-kind Kubernetes master is running at https://127.0.0.1:32768 KubeDNS is running at https://127.0.0.1:32768/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy æŸ¥çœ‹node\n$ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-control-plane Ready master 35h v1.17.0 172.17.0.2 \u003cnone\u003e Ubuntu 19.10 3.10.107-1-tlinux2_kvm_guest-0049 containerd://1.3.2 æŸ¥çœ‹pod\n$ kubectl get po --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-6955765f44-lqk9v 1/1 Running 0 35h 10.244.0.4 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system coredns-6955765f44-zpsmc 1/1 Running 0 35h 10.244.0.3 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system etcd-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kindnet-8mt7d 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-5w25s 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-kind-control-plane 1/1 Running 0 35h 172.17.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e local-path-storage local-path-provisioner-7745554f7f-dckzr 1/1 Running 0 35h 10.244.0.2 kind-control-plane \u003cnone\u003e \u003cnone\u003e docker ps\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 93b291f99dd4 kindest/node:v1.17.0 \"/usr/local/bin/entrâ€¦\" 2 minutes ago Up 2 minutes 127.0.0.1:32768-\u003e6443/tcp kind-control-plane 3. kindest/nodeå®¹å™¨å†…è¿›ç¨‹ $ docker exec -it 93b291f99dd4 bash root@kind-control-plane:/# ps auxw USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.0 19512 7480 ? Ss 03:18 0:00 /sbin/init root 105 0.0 0.0 26396 7344 ? S\u003cs 03:18 0:00 /lib/systemd/systemd-journald root 141 2.3 0.3 2374736 51564 ? Ssl 03:18 0:06 /usr/local/bin/containerd root 325 0.0 0.0 112540 5036 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 3f415d609e15ef12b9f53557891c311c156b912d5a326544a25c8b29cfa9d366 -address /run/containerd/containerd.sock root 346 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 370 0.0 0.0 112540 5108 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 1e1f3eed09f701fb621325e7b9e96d1c3de60ebd3bd64e0aec376e9490cf0e57 -address /run/containerd/containerd.sock root 397 0.0 0.0 112540 4684 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c5e451089a1a5b3dfb2cc68ee27ac7d414285be55ecfdc5bd59180fbfbc7df2e -address /run/containerd/containerd.sock root 424 0.0 0.0 112540 4924 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 81e35f29ac8c2dda344125a10e3791be7ccf788a88f1efbc3397fa319f02881f -address /run/containerd/containerd.sock root 443 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 458 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 465 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 548 0.7 0.1 145500 27724 ? Ssl 03:18 0:02 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true root 589 1.0 0.3 159536 54384 ? Ssl 03:18 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.cr root 613 3.8 1.6 445780 273484 ? Ssl 03:18 0:10 kube-apiserver --advertise-address=172.17.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/ root 660 1.4 0.2 10613604 37448 ? Ssl 03:18 0:04 etcd --advertise-client-urls=https://172.17.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://172.17.0.2:2380 --initial-cluster=kind-control-plane=https://172. root 718 1.3 0.3 2084848 52772 ? Ssl 03:18 0:03 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail root 876 0.0 0.0 112540 5084 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id adfbea8fec5ac6986407291f5bfc5aecead176954e5dabbe1517b98dd77bf78b -address /run/containerd/containerd.sock root 893 0.0 0.0 112540 4796 ? Sl 03:18 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 53bdce023626b60ffaa0548b5888e457dc9c3bc45c7808a385dd0f63dcc90327 -address /run/containerd/containerd.sock root 924 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 931 0.0 0.0 1012 4 ? Ss 03:18 0:00 /pause root 1000 0.0 0.0 127616 11100 ? Ssl 03:18 0:00 /bin/kindnetd root 1017 0.0 0.1 141060 19420 ? Ssl 03:18 0:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=kind-control-plane root 1066 0.0 0.0 0 0 ? Z 03:18 0:00 [iptables-nft-sa] \u003cdefunct\u003e root 1080 0.0 0.0 0 0 ? Z 03:18 0:00 [iptables-nft-sa] \u003cdefunct\u003e root 1241 0.0 0.0 112540 5156 ? Sl 03:19 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 5cbd7bbe186cf5847786c7a03aa4c6f82e6c805d0a189f0f3e8fb1750594260d -address /run/containerd/containerd.sock root 1262 0.0 0.0 1012 4 ? Ss 03:19 0:00 /pause root 1303 0.1 0.0 134372 14088 ? Ssl 03:19 0:00 local-path-provisioner --debug start --helper-image k8s.gcr.io/debian-base:v2.0.0 --config /etc/config/config.json root 1411 0.0 0.0 112540 4876 ? Sl 03:19 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 196b440345cb5ef47a6c31222323d35bbfef85d1d79c149ec0e3a6e22022a5f0 -address /run/containerd/containerd.sock root 1437 0.0 0.0 1012 4 ? Ss 03:19 0:00 /pause root 1450 0.0 0.0 112540 4380 ? Sl 03:19 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id de7bdf052083978c78708383f842567d4fb38adff22a56792437a4de82425afe -address /run/containerd/containerd.sock root 1480 0.0 0.0 1012 4 ? Ss 03:19 0:00 /pause root 1530 0.1 0.1 144324 19056 ? Ssl 03:19 0:00 /coredns -conf /etc/coredns/Corefile root 1531 0.1 0.1 144580 19204 ? Ssl 03:19 0:00 /coredns -conf /etc/coredns/Corefile å‚è€ƒï¼š\nhttps://github.com/kubernetes-sigs/kind https://kind.sigs.k8s.io/docs/user/quick-start/ ","categories":"","description":"","excerpt":"1. å®‰è£…kind On mac or linux\ncurl -Lo ./kind â€¦","ref":"/kubernetes-notes/setup/installer/install-k8s-by-kind/","tags":["Kubernetes"],"title":"ä½¿ç”¨kindå®‰è£…kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/network/","tags":"","title":"ç½‘ç»œ"},{"body":"1. vlanæ˜¯ä»€ä¹ˆ VLANï¼ˆVirtual Local Area Networkï¼‰å³è™šæ‹Ÿå±€åŸŸç½‘ï¼Œæ˜¯å°†ä¸€ä¸ªç‰©ç†çš„LANåœ¨é€»è¾‘ä¸Šåˆ’åˆ†æˆå¤šä¸ªå¹¿æ’­åŸŸçš„é€šä¿¡æŠ€æœ¯ã€‚æ¯ä¸ªVLANæ˜¯ä¸€ä¸ªå¹¿æ’­åŸŸï¼ŒVLANå†…çš„ä¸»æœºé—´å¯ä»¥ç›´æ¥é€šä¿¡ï¼Œè€ŒVLANé—´åˆ™ä¸èƒ½ç›´æ¥äº’é€šã€‚è¿™æ ·ï¼Œå¹¿æ’­æŠ¥æ–‡å°±è¢«é™åˆ¶åœ¨ä¸€ä¸ªVLANå†…ã€‚\n2. ä¸ºä»€ä¹ˆéœ€è¦vlan æ²¡æœ‰vlanå‰ï¼Œå¹¿æ’­æŠ¥æ–‡ä¼šè¢«å‘é€åˆ°è¾ƒå¤§çš„å¹¿æ’­åŸŸï¼Œä¸»æœºæ•°é‡è¾ƒå¤šæ—¶å€™ä¼šé€ æˆå¹¿æ’­æ³›æ»¥ï¼Œæ€§èƒ½ä½ä¸‹ï¼Œç½‘ç»œä¸å¯ç”¨ç­‰é—®é¢˜ã€‚å¦‚æœæŠŠä¸€ä¸ªLANç½‘åˆ†æˆå¤šä¸ªé€»è¾‘LANç½‘ï¼Œæ¯ä¸ªé€»è¾‘LANç½‘é€šè¿‡idè¿›è¡Œæ ‡è¯†ï¼Œç›¸åŒé€»è¾‘LANå†…çš„ä¸»æœºå¯ä»¥ç›´æ¥é€šä¿¡ï¼Œä¸åŒé€»è¾‘LANç½‘å†…çš„ä¸»æœºä¸èƒ½ç›´æ¥é€šä¿¡ï¼Œé‚£ä¹ˆå¹¿æ’­æŠ¥æ–‡å°±é™åˆ¶åœ¨ä¸€ä¸ªé€»è¾‘LANç½‘å†…ï¼Œè€Œè¿™ä¸ªé€»è¾‘LANå°±æ˜¯æ‰€è°“çš„VLANã€‚\nç”±æ­¤å¯è§ï¼ŒVLANæœ‰ä»¥ä¸‹çš„ä¼˜ç‚¹ï¼š\né™åˆ¶å¹¿æ’­åŸŸï¼šèŠ‚çœäº†å¸¦å®½ï¼Œæé«˜ç½‘ç»œå¤„ç†æ•ˆç‡ã€‚ å¢åŠ å®‰å…¨æ€§ï¼šä¸åŒVLANäº’ç›¸éš”ç¦»ï¼Œå¢åŠ å®‰å…¨æ€§ã€‚ çµæ´»æ„å»ºå±€åŸŸç½‘ï¼šå¯ä»¥æ–¹ä¾¿çš„æ„å»ºå®‰å…¨çš„å±€åŸŸç½‘ã€‚ 3. vlan IDåŠvlan tag ä¸ºäº†è®©äº¤æ¢æœºè¯†åˆ«ä¸åŒvlançš„æŠ¥æ–‡ï¼Œåˆ™éœ€è¦é€šè¿‡æŸç§æ ‡è¯†æ¥åŒºåˆ†ï¼Œå› æ­¤åœ¨æŠ¥æ–‡ä¸­åŠ äº†vlan tagçš„å­—æ®µï¼Œå…¶ä¸­åŒ…æ‹¬vlan idçš„ä¿¡æ¯ï¼Œ\nVlan id(ç®€ç§°VID)ï¼Œå–å€¼èŒƒå›´ä¸º0-4095,0å’Œ4095ä¸ºä¿ç•™å­—æ®µï¼Œå› æ­¤VLAN IDçš„æœ‰æ•ˆèŒƒå›´ä¸º1-4094ã€‚\näº¤æ¢æœºå†…éƒ¨å¤„ç†çš„æ•°æ®å¸§éƒ½å¸¦æœ‰VLANæ ‡ç­¾ã€‚è€Œäº¤æ¢æœºè¿æ¥çš„éƒ¨åˆ†è®¾å¤‡ï¼ˆå¦‚ç”¨æˆ·ä¸»æœºã€æœåŠ¡å™¨ï¼‰åªä¼šæ”¶å‘ä¸å¸¦VLAN tagçš„ä¼ ç»Ÿä»¥å¤ªç½‘æ•°æ®å¸§ã€‚å› æ­¤ï¼Œè¦ä¸è¿™äº›è®¾å¤‡äº¤äº’ï¼Œå°±éœ€è¦äº¤æ¢æœºçš„æ¥å£èƒ½å¤Ÿè¯†åˆ«ä¼ ç»Ÿä»¥å¤ªç½‘æ•°æ®å¸§ï¼Œå¹¶åœ¨æ”¶å‘æ—¶ç»™å¸§æ·»åŠ ã€å‰¥é™¤VLANæ ‡ç­¾ã€‚æ·»åŠ ä»€ä¹ˆVLANæ ‡ç­¾ï¼Œç”±æ¥å£ä¸Šçš„ç¼ºçœVLANï¼ˆPort Default VLAN IDï¼ŒPVIDï¼‰å†³å®šã€‚\n4. vlançš„æ¥å£ç±»å‹ ç°ç½‘ä¸­å±äºåŒä¸€ä¸ªVLANçš„ç”¨æˆ·å¯èƒ½ä¼šè¢«è¿æ¥åœ¨ä¸åŒçš„äº¤æ¢æœºä¸Šï¼Œä¸”è·¨è¶Šäº¤æ¢æœºçš„VLANå¯èƒ½ä¸æ­¢ä¸€ä¸ªï¼Œå¦‚æœéœ€è¦ç”¨æˆ·é—´çš„äº’é€šï¼Œå°±éœ€è¦äº¤æ¢æœºé—´çš„æ¥å£èƒ½å¤ŸåŒæ—¶è¯†åˆ«å’Œå‘é€å¤šä¸ªVLANçš„æ•°æ®å¸§ã€‚æ ¹æ®æ¥å£è¿æ¥å¯¹è±¡ä»¥åŠå¯¹æ”¶å‘æ•°æ®å¸§å¤„ç†çš„ä¸åŒï¼Œå½“å‰æœ‰VLANçš„å¤šç§æ¥å£ç±»å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„è¿æ¥å’Œç»„ç½‘ã€‚\nå¸¸è§çš„VLANæ¥å£ç±»å‹æœ‰ä¸‰ç§ï¼ŒåŒ…æ‹¬ï¼šAccessã€Trunkå’ŒHybridã€‚\n4.1. Accessæ¥å£ï¼ˆæ¥å…¥VLANï¼‰ Accessæ¥å£ä¸€èˆ¬ç”¨äºå’Œä¸èƒ½è¯†åˆ«Tagçš„ç”¨æˆ·ç»ˆç«¯ï¼ˆå¦‚ç”¨æˆ·ä¸»æœºã€æœåŠ¡å™¨ï¼‰ç›¸è¿ï¼Œæˆ–è€…ä¸éœ€è¦åŒºåˆ†ä¸åŒVLANæˆå‘˜æ—¶ä½¿ç”¨ã€‚\nå®šä¹‰ï¼šAccess VLANæ¥å£æ˜¯ä¸€ç§ç”¨äºè¿æ¥ç»ˆç«¯è®¾å¤‡ï¼ˆå¦‚è®¡ç®—æœºã€æ‰“å°æœºç­‰ï¼‰çš„æ¥å£ï¼Œæ¯ä¸ªæ¥å£åªå±äºä¸€ä¸ªVLANã€‚\nä½¿ç”¨åœºæ™¯ï¼šé€‚ç”¨äºæ¥å…¥å±‚äº¤æ¢æœºç«¯å£ï¼Œæ¯ä¸ªç«¯å£è¿æ¥ä¸€ä¸ªç»ˆç«¯è®¾å¤‡ï¼Œåªèƒ½ä¼ è¾“å•ä¸ªVLANçš„æµé‡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåŠå…¬åŒºåŸŸå†…çš„è®¡ç®—æœºéƒ½è¿æ¥åˆ°Access VLANï¼Œä»¥ä¾¿è¿™äº›è®¡ç®—æœºèƒ½å¤Ÿäº’ç›¸é€šä¿¡ã€‚\n4.2. Trunkæ¥å£ï¼ˆå¹²çº¿VLANï¼‰ Trunkæ¥å£ä¸€èˆ¬ç”¨äºè¿æ¥äº¤æ¢æœºã€è·¯ç”±å™¨ã€APä»¥åŠå¯åŒæ—¶æ”¶å‘Taggedå¸§å’ŒUntaggedå¸§çš„è¯­éŸ³ç»ˆç«¯ã€‚å®ƒå¯ä»¥å…è®¸å¤šä¸ªVLANçš„å¸§å¸¦Tagé€šè¿‡ï¼Œä½†åªå…è®¸å±äºç¼ºçœVLANçš„å¸§ä»è¯¥ç±»æ¥å£ä¸Šå‘å‡ºæ—¶ä¸å¸¦Tagï¼ˆå³å‰¥é™¤Tagï¼‰ã€‚\nå®šä¹‰ï¼šTrunk VLANæ¥å£ç”¨äºåœ¨äº¤æ¢æœºä¹‹é—´ä¼ è¾“å¤šä¸ªVLANçš„æµé‡ã€‚Trunkç«¯å£èƒ½å¤Ÿæ ‡è®°ï¼ˆtaggingï¼‰VLANä¿¡æ¯ï¼Œä»¥ä¾¿åŒºåˆ†ä¸åŒçš„VLANæµé‡ã€‚ ä½¿ç”¨åœºæ™¯ï¼šé€‚ç”¨äºäº¤æ¢æœºä¹‹é—´æˆ–äº¤æ¢æœºä¸è·¯ç”±å™¨ä¹‹é—´çš„è¿æ¥ï¼Œç”¨äºä¼ è¾“å¤šVLANæµé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•°æ®ä¸­å¿ƒå†…ï¼Œéœ€è¦é€šè¿‡Trunkç«¯å£å°†å¤šä¸ªVLANçš„æ•°æ®ä¼ è¾“åˆ°æ ¸å¿ƒäº¤æ¢æœºã€‚\n4.3. Hybridæ¥å£ï¼ˆæ··åˆVLANï¼‰ Hybridæ¥å£æ—¢å¯ä»¥ç”¨äºè¿æ¥ä¸èƒ½è¯†åˆ«Tagçš„ç”¨æˆ·ç»ˆç«¯ï¼ˆå¦‚ç”¨æˆ·ä¸»æœºã€æœåŠ¡å™¨ï¼‰å’Œç½‘ç»œè®¾å¤‡ï¼ˆå¦‚Hubï¼‰ï¼Œä¹Ÿå¯ä»¥ç”¨äºè¿æ¥äº¤æ¢æœºã€è·¯ç”±å™¨ä»¥åŠå¯åŒæ—¶æ”¶å‘Taggedå¸§å’ŒUntaggedå¸§çš„è¯­éŸ³ç»ˆç«¯ã€APã€‚å®ƒå¯ä»¥å…è®¸å¤šä¸ªVLANçš„å¸§å¸¦Tagé€šè¿‡ï¼Œä¸”å…è®¸ä»è¯¥ç±»æ¥å£å‘å‡ºçš„å¸§æ ¹æ®éœ€è¦é…ç½®æŸäº›VLANçš„å¸§å¸¦Tagï¼ˆå³ä¸å‰¥é™¤Tagï¼‰ã€æŸäº›VLANçš„å¸§ä¸å¸¦Tagï¼ˆå³å‰¥é™¤Tagï¼‰ã€‚\nå®šä¹‰ï¼šHybrid VLANæ¥å£å¯ä»¥åŒæ—¶å¤„ç†Access VLANå’ŒTrunk VLANçš„æµé‡ã€‚å®ƒå¯ä»¥å°†æœªæ ‡è®°çš„æµé‡ä½œä¸ºAccess VLANæµé‡å¤„ç†ï¼Œå¹¶å°†æ ‡è®°çš„æµé‡ä½œä¸ºTrunk VLANæµé‡å¤„ç†ã€‚\nä½¿ç”¨åœºæ™¯ï¼šé€‚ç”¨äºéœ€è¦çµæ´»é…ç½®çš„ç¯å¢ƒï¼Œæ—¢éœ€è¦å¤„ç†æ¥è‡ªç»ˆç«¯è®¾å¤‡çš„å•ä¸€VLANæµé‡ï¼Œåˆéœ€è¦å¤„ç†æ¥è‡ªäº¤æ¢æœºçš„å¤šVLANæµé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªç½‘ç»œä¸­ï¼Œéœ€è¦åŒæ—¶è¿æ¥è®¡ç®—æœºå’Œå…¶ä»–äº¤æ¢æœºçš„ç«¯å£å¯ä»¥é…ç½®ä¸ºHybrid VLANã€‚\nå‚è€ƒï¼š\nhttps://mp.weixin.qq.com/s/5wH9QbBTGKpYaolRbMijMA https://www.wpgdadatong.com.cn/blog/detail/71784 ","categories":"","description":"","excerpt":"1. vlanæ˜¯ä»€ä¹ˆ VLANï¼ˆVirtual Local Area Networkï¼‰å³è™šæ‹Ÿå±€åŸŸç½‘ï¼Œæ˜¯å°†ä¸€ä¸ªç‰©ç†çš„LANåœ¨é€»è¾‘ä¸Šåˆ’åˆ†æˆå¤šä¸ªå¹¿ â€¦","ref":"/linux-notes/network/vlan/","tags":["network"],"title":"VLANä»‹ç»"},{"body":"1. éƒ¨ç½²dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml é•œåƒï¼š kubernetesui/dashboard:v2.5.0\né»˜è®¤ç«¯å£ï¼š8443\nç™»å½•é¡µé¢éœ€è¦å¡«å…¥tokenæˆ–kubeconfig\n2. ç™»å½•dashboard 2.1. åˆ›å»ºè¶…çº§ç®¡ç†å‘˜ å‚è€ƒï¼šdashboard/creating-sample-user\nåˆ›å»ºdashboard-adminuser.yamlæ–‡ä»¶å¦‚ä¸‹ï¼š\nk8s 1.24+ç‰ˆæœ¬éœ€è¦è‡ªè¡Œåˆ›å»ºsecretç»‘å®šserviceaccount\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: name: admin-user-secret namespace: kubernetes-dashboard annotations: kubernetes.io/service-account.name: \"admin-user\" type: kubernetes.io/service-account-token åˆ›å»ºserviceaccountå’ŒClusterRoleBindingï¼Œç»‘å®šcluster-adminçš„è¶…çº§ç®¡ç†å‘˜çš„æƒé™ã€‚\nkubectl apply -f dashboard-adminuser.yaml åˆ›å»ºç”¨æˆ·token\nkubectl -n kubernetes-dashboard create token admin-user --duration 8760h æˆ–è€…é€šè¿‡secretæŸ¥è¯¢token\nkubectl get secret admin-user-secret -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d ç§»é™¤è´¦å·\nkubectl -n kubernetes-dashboard delete serviceaccount admin-user kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user 2.2. åˆ›å»ºNamespaceç®¡ç†å‘˜ 1ã€åˆ›å»ºè§’è‰²æƒé™ï¼ˆroleï¼‰\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: \u003cnamespace\u003e name: \u003cnamespace\u003e-admin-role rules: - apiGroups: - '*' resources: - '*' verbs: - '*' 2ã€åˆ›å»ºç”¨æˆ·è´¦å·ï¼ˆServiceAccountï¼‰\napiVersion: v1 kind: ServiceAccount metadata: name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e åˆ›å»ºsecret å¯è‡ªåŠ¨ç”Ÿæˆtoken\napiVersion: v1 kind: Secret metadata: name: ${SecretName} namespace: ${ServiceAccountNS} annotations: kubernetes.io/service-account.name: \"${ServiceAccountName}\" type: kubernetes.io/service-account-token 3ã€åˆ›å»ºè§’è‰²ç»‘å®šå…³ç³»\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: \u003cnamespace\u003e-admin-role subjects: - kind: ServiceAccount name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e 4ã€ç”Ÿæˆtoken\nkubectl -n \u003cnamespace\u003e create token \u003cServiceAccount\u003e --duration 8760h æˆ–è€…é€šè¿‡ä¸Šè¿°secretä¸­çš„tokenè·å¾—\nkubectl get secret ${SecretName} -n ${ServiceAccountNS} -o jsonpath={\".data.token\"} | base64 -d 2.3. åˆ›å»ºåªè¯»è´¦æˆ· é›†ç¾¤é»˜è®¤æä¾›äº†å‡ ç§å‘½åç©ºé—´çº§åˆ«çš„æƒé™ï¼Œåˆ†åˆ«è®¾ç½®ClusterRole: [admin, edit, view], å°†æˆæƒè®¾ç½®ä¸ºClusterRoleä¸ºviewå³å¯ã€‚\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: view subjects: - kind: ServiceAccount name: \u003cnamespace\u003e-admin-user namespace: \u003cnamespace\u003e 3. é›†æˆSSOç™»å½• ç¤¾åŒºæä¾›äº†æ·»åŠ Authorization headerçš„æ–¹å¼æ¥é›†æˆè‡ªå®šä¹‰çš„SSOç™»å½•ã€‚å³åœ¨HTTPè¯·æ±‚ä¸­å¢åŠ Header: Authorization: Bearer \u003ctoken\u003eã€‚è¯¥æ“ä½œå¯ä»¥é€šè¿‡apisixæˆ–Nginxç­‰æ’ä»¶æ³¨å…¥Headerã€‚\nå‚è€ƒï¼š\néƒ¨ç½²å’Œè®¿é—® Kubernetes ä»ªè¡¨æ¿ï¼ˆDashboardï¼‰ | Kubernetes\ndashboard/creating-sample-user.md at master Â· kubernetes/dashboard Â· GitHub\ndashboard/docs/user/access-control at master Â· kubernetes/dashboard Â· GitHub\n","categories":"","description":"","excerpt":"1. éƒ¨ç½²dashboard kubectl apply -f â€¦","ref":"/kubernetes-notes/setup/installer/install-dashboard/","tags":["Kubernetes"],"title":"å®‰è£…k8s dashboard"},{"body":"é”™è¯¯å¤„ç† 1. erroræ¥å£ //å®šä¹‰erroræ¥å£ type error interface{ Error() string } //è°ƒç”¨erroræ¥å£ func Foo(param int) (n int,err error){ //... } n,err:=Foo(0) if err!=nil{ //é”™è¯¯å¤„ç† }else{ //ä½¿ç”¨è¿”å›å€¼ } 2. defer[å»¶è¿Ÿå‡½æ•°] è¯­æ³•ï¼š\ndefer function_name() 1ï¼‰deferåœ¨å£°æ˜æ—¶ä¸ä¼šæ‰§è¡Œï¼Œè€Œæ˜¯æ¨è¿Ÿæ‰§è¡Œï¼Œåœ¨returnæ‰§è¡Œå‰ï¼Œå€’åºæ‰§è¡Œdefer[å…ˆè¿›åå‡º]ï¼Œä¸€èˆ¬ç”¨äºé‡Šæ”¾èµ„æºï¼Œæ¸…ç†æ•°æ®ï¼Œè®°å½•æ—¥å¿—ï¼Œå¼‚å¸¸å¤„ç†ç­‰ã€‚\n2ï¼‰deferæœ‰ä¸€ä¸ªç‰¹æ€§ï¼šå³ä½¿å‡½æ•°æŠ›å‡ºå¼‚å¸¸ï¼Œdeferä»ä¼šè¢«æ‰§è¡Œï¼Œè¿™æ ·ä¸ä¼šå‡ºç°ç¨‹åºé”™è¯¯å¯¼è‡´èµ„æºä¸è¢«é‡Šæ”¾ï¼Œæˆ–è€…å› ä¸ºç¬¬ä¸‰æ–¹åŒ…çš„å¼‚å¸¸å¯¼è‡´ç¨‹åºå´©æºƒã€‚\n3ï¼‰ä¸€èˆ¬ç”¨äºæ‰“å¼€æ–‡ä»¶åé‡Šæ”¾èµ„æºçš„æ“ä½œï¼Œæ¯”å¦‚æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ï¼Œæœ€åæ€»æ˜¯è¦å…³é—­çš„ã€‚è€Œåœ¨æ‰“å¼€å’Œå…³é—­ä¹‹é—´ï¼Œä¼šæœ‰è¯¸å¤šçš„å¤„ç†ï¼Œå¯èƒ½ä¼šæœ‰è¯¸å¤šçš„if-elseã€æ ¹æ®ä¸åŒçš„æƒ…å†µéœ€è¦æå‰è¿”å›\nf, = os.open(filename) defer f.close() do_something() if (condition_a) {return} do_something_again() if (condition_b) {return} do_further_things() 4ï¼‰deferç¤ºä¾‹\npackage main import \"fmt\" func deferTest(number int) int { defer func() { number++ fmt.Println(\"three:\", number) }() defer func() { number++ fmt.Println(\"two:\", number) }() defer func() { number++ fmt.Println(\"one:\", number) }() return number } func main() { fmt.Println(\"å‡½æ•°è¿”å›å€¼ï¼š\", deferTest(0)) } /* one: 1 two: 2 three: 3 å‡½æ•°è¿”å›å€¼ï¼š 0 */ 3. panic()å’Œrecover() Goä¸­ä½¿ç”¨å†…ç½®å‡½æ•°panic()å’Œrecover()æ¥å¤„ç†ç¨‹åºä¸­çš„é”™è¯¯ã€‚\nfunc panic(interface{}) func recover() interface{} 3.1. panic() å½“å‡½æ•°æ‰§è¡Œè§¦å‘äº†panic()å‡½æ•°æ—¶ï¼Œå¦‚æœæ²¡æœ‰ä½¿ç”¨åˆ°deferå…³é”®å­—ï¼Œå‡½æ•°æ‰§è¡Œæµç¨‹ä¼šè¢«ç«‹å³ç»ˆæ­¢ï¼›å¦‚æœä½¿ç”¨äº†deferå…³é”®å­—ï¼Œåˆ™ä¼šé€å±‚æ‰§è¡Œdeferè¯­å¥ï¼Œç›´åˆ°æ‰€æœ‰çš„å‡½æ•°è¢«ç»ˆæ­¢ã€‚\né”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬panicå‡½æ•°ä¼ å…¥çš„å‚æ•°ï¼Œå°†ä¼šè¢«æŠ¥å‘Šå‡ºæ¥ã€‚\nç¤ºä¾‹å¦‚ä¸‹ï¼š\npanic(404) panic(\"network broken\") panic(Error(\"file not exists\")) 3.2. recover() recover()å‡½æ•°ç”¨äºç»ˆæ­¢é”™è¯¯å¤„ç†æµç¨‹ã€‚ä¸€èˆ¬ä½¿ç”¨åœ¨deferå…³é”®å­—åä»¥æœ‰æ•ˆæˆªå–é”™è¯¯å¤„ç†æµç¨‹ã€‚å¦‚æœæ²¡æœ‰åœ¨å‘ç”Ÿå¼‚å¸¸çš„goroutineä¸­æ˜ç¡®è°ƒç”¨æ¢å¤è¿‡ç¨‹ï¼ˆä½¿ç”¨recoverå…³é”®å­—ï¼‰ ï¼Œä¼šå¯¼è‡´è¯¥goroutineæ‰€å±çš„è¿›ç¨‹æ‰“å°å¼‚å¸¸ä¿¡æ¯åç›´æ¥é€€å‡ºã€‚\nç¤ºä¾‹å¦‚ä¸‹ï¼š\ndefer func() { if r := recover(); r != nil { log.Printf(\"Runtime error caught: %v\", r) } }() foo() æ— è®ºfoo()ä¸­æ˜¯å¦è§¦å‘äº†é”™è¯¯å¤„ç†æµç¨‹ï¼Œè¯¥åŒ¿ådeferå‡½æ•°éƒ½å°†åœ¨å‡½æ•°é€€å‡ºæ—¶å¾—åˆ°æ‰§è¡Œã€‚å‡å¦‚foo()ä¸­è§¦å‘äº†é”™è¯¯å¤„ç†æµç¨‹ï¼Œ recover()å‡½æ•°æ‰§è¡Œå°†ä½¿å¾—è¯¥é”™è¯¯å¤„ç†è¿‡ç¨‹ç»ˆæ­¢ã€‚å¦‚æœé”™è¯¯å¤„ç†æµç¨‹è¢«è§¦å‘æ—¶ï¼Œç¨‹åºä¼ ç»™panicå‡½æ•°çš„å‚æ•°ä¸ä¸ºnilï¼Œåˆ™è¯¥å‡½æ•°è¿˜ä¼šæ‰“å°è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ã€‚\nå‚è€ƒï¼š\nã€ŠGoè¯­è¨€ç¼–ç¨‹ã€‹ ","categories":"","description":"","excerpt":"é”™è¯¯å¤„ç† 1. erroræ¥å£ //å®šä¹‰erroræ¥å£ type error interface{ Error() string } //è°ƒ â€¦","ref":"/golang-notes/basis/errors/","tags":["Golang"],"title":"é”™è¯¯å¤„ç†"},{"body":" ä»¥ä¸‹ç”±zsh-plugin-gitå†…å®¹è½¬è½½è¿‡æ¥ï¼Œä»¥å¤‡æŸ¥è¯¢ä½¿ç”¨ã€‚\ngit plugin The git plugin provides many aliases and a few useful functions.\nTo use it, add git to the plugins array in your zshrc file:\nplugins=(... git) Aliases å¸¸ç”¨ Alias Command g git ga git add gaa git add --all gcmsg git commit -m -------------------- ------------------------------------------------------------ ggp git push origin $(current_branch) ggf git push --force origin $(current_branch) gp git push gl git pull ggl git pull origin $(current_branch) -------------------- ------------------------------------------------------------ gco git checkout gcb git checkout -b gcm git checkout master gb git branch gba git branch -a gcf git config --list gd git diff å®Œæ•´åˆ—è¡¨ Alias Command g git ga git add gaa git add --all gapa git add --patch gau git add --update gav git add --verbose gap git apply gb git branch gba git branch -a gbd git branch -d gbda - gbD git branch -D gbl git blame -b -w gbnm git branch --no-merged gbr git branch --remote gbs git bisect gbsb git bisect bad gbsg git bisect good gbsr git bisect reset gbss git bisect start gc git commit -v gc! git commit -v --amend gcn! git commit -v --no-edit --amend gca git commit -v -a gca! git commit -v -a --amend gcan! git commit -v -a --no-edit --amend gcans! git commit -v -a -s --no-edit --amend gcam git commit -a -m gcsm git commit -s -m gcb git checkout -b gcf git config --list gcl git clone --recurse-submodules gclean git clean -id gpristine git reset --hard \u0026\u0026 git clean -dfx gcm git checkout master gcd git checkout develop gcmsg git commit -m gco git checkout gcount git shortlog -sn gcp git cherry-pick gcpa git cherry-pick --abort gcpc git cherry-pick --continue gcs git commit -S gd git diff gdca git diff --cached gdcw git diff --cached --word-diff gdct git describe --tags $(git rev-list --tags --max-count=1) gds git diff --staged gdt git diff-tree --no-commit-id --name-only -r gdv - gdw git diff --word-diff gf git fetch gfa git fetch --all --prune gfg - gfo git fetch origin gg git gui citool gga git gui citool --amend ggf git push --force origin $(current_branch) ggfl git push --force-with-lease origin $(current_branch) ggl git pull origin $(current_branch) ggp git push origin $(current_branch) ggpnp ggl \u0026\u0026 ggp ggpull git pull origin \"$(git_current_branch)\" ggpur ggu ggpush git push origin \"$(git_current_branch)\" ggsup git branch --set-upstream-to=origin/$(git_current_branch) ggu git pull --rebase origin $(current_branch) gpsup git push --set-upstream origin $(git_current_branch) ghh git help gignore git update-index --assume-unchanged gignored - git-svn-dcommit-push git svn dcommit \u0026\u0026 git push github master:svntrunk gk gitk --all --branches gke gitk --all $(git log -g --pretty=%h) gl git pull glg git log --stat glgp git log --stat -p glgg git log --graph glgga git log --graph --decorate --all glgm git log --graph --max-count=10 glo git log --oneline --decorate glol git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' glols git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --stat glod git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%ad) %C(bold blue)\u003c%an\u003e%Creset' glods git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%ad) %C(bold blue)\u003c%an\u003e%Creset' --date=short glola git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --all glog git log --oneline --decorate --graph gloga git log --oneline --decorate --graph --all glp _git_log_prettily gm git merge gmom git merge origin/master gmt git mergetool --no-prompt gmtvim git mergetool --no-prompt --tool=vimdiff gmum git merge upstream/master gma git merge --abort gp git push gpd git push --dry-run gpf git push --force-with-lease gpf! git push --force gpoat git push origin --all \u0026\u0026 git push origin --tags gpu git push upstream gpv git push -v gr git remote gra git remote add grb git rebase grba git rebase --abort grbc git rebase --continue grbd git rebase develop grbi git rebase -i grbm git rebase master grbs git rebase --skip grh git reset grhh git reset --hard groh git reset origin/$(git_current_branch) --hard grm git rm grmc git rm --cached grmv git remote rename grrm git remote remove grset git remote set-url grt - gru git reset -- grup git remote update grv git remote -v gsb git status -sb gsd git svn dcommit gsh git show gsi git submodule init gsps git show --pretty=short --show-signature gsr git svn rebase gss git status -s gst git status gsta git stash push gsta git stash save gstaa git stash apply gstc git stash clear gstd git stash drop gstl git stash list gstp git stash pop gsts git stash show --text gstall git stash --all gsu git submodule update gts git tag -s gtv - gtl gtl(){ git tag --sort=-v:refname -n -l ${1}* }; noglob gtl gunignore git update-index --no-assume-unchanged gunwip - gup git pull --rebase gupv git pull --rebase -v gupa git pull --rebase --autostash gupav git pull --rebase --autostash -v glum git pull upstream master gwch git whatchanged -p --abbrev-commit --pretty=medium gwip git add -A; git rm $(git ls-files --deleted) 2\u003e /dev/null; git commit --no-verify --no-gpg-sign -m \"--wip-- [skip ci]\" Deprecated These are aliases that have been removed, renamed, or otherwise modified in a way that may, or may not, receive further support.\nAlias Command Modification gap git add --patch new alias gapa gcl git config --list new alias gcf gdc git diff --cached new alias gdca gdt git difftool no replacement ggpull git pull origin $(current_branch) new alias ggl (ggpull still exists for now though) ggpur git pull --rebase origin $(current_branch) new alias ggu (ggpur still exists for now though) ggpush git push origin $(current_branch) new alias ggp (ggpush still exists for now though) gk gitk --all --branches now aliased to gitk --all --branches glg git log --stat --max-count = 10 now aliased to git log --stat --color glgg git log --graph --max-count = 10 now aliased to git log --graph --color gwc git whatchanged -p --abbrev-commit --pretty = medium new alias gwch Functions Current Command Description current_branch Return the name of the current branch git_current_user_name Returns the user.name config value git_current_user_email Returns the user.email config value Work in Progress (WIP) These features allow to pause a branch development and switch to another one (\"Work in Progress\", or wip). When you want to go back to work, just unwip it.\nCommand Description work_in_progress Echoes a warning if the current branch is a wip gwip Commit wip branch gunwip Uncommit wip branch Deprecated Command Description Reason current_repository Return the names of the current remotes Didn't work properly. Use git remote -v instead (grv alias) å‚è€ƒ\nhttps://github.com/robbyrussell/oh-my-zsh/tree/master/plugins/git ","categories":"","description":"","excerpt":" ä»¥ä¸‹ç”±zsh-plugin-gitå†…å®¹è½¬è½½è¿‡æ¥ï¼Œä»¥å¤‡æŸ¥è¯¢ä½¿ç”¨ã€‚\ngit plugin The git plugin provides â€¦","ref":"/linux-notes/git/git-alias-zsh/","tags":["Git"],"title":"Gitå‘½ä»¤åˆ«å"},{"body":" æœ¬æ–‡ç”±ç½‘ç»œæ–‡ç« æ•´ç†å¤‡ä»½ã€‚\niterm2 rzä¸szçš„åŠŸèƒ½ æœ¬æ–‡ä¸»è¦ä»‹ç»macç¯å¢ƒä¸‹ä½¿ç”¨iterm2çš„rz szåŠŸèƒ½çš„å®‰è£…æµç¨‹ã€‚\n1. å®‰è£…lrzsz brew install lrzsz 2. å®‰è£…æ‰§è¡Œè„šæœ¬ å°†iterm2-send-zmodem.shå’Œiterm2-recv-zmodem.shä¿å­˜åˆ°/usr/local/binç›®å½•ä¸‹ã€‚\niterm2-send-zmodem.sh\n#!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' \u003e /dev/null 2\u003e\u00261 \u0026\u0026 NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=$(osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") else FILE=$(osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else /usr/local/bin/sz \"$FILE\" --escape --binary --bufsize 4096 sleep 1 echo echo \\# Received \"$FILE\" fi iterm2-recv-zmodem.sh\n#!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' \u003e /dev/null 2\u003e\u00261 \u0026\u0026 NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=$(osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") else FILE=$(osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"\u0026(quoted form of POSIX path of thefile as Unicode text)\u0026\\\"\\\")\") fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else cd \"$FILE\" /usr/local/bin/rz --rename --escape --binary --bufsize 4096 sleep 1 echo echo echo \\# Sent \\-\\\u003e $FILE fi 3. èµ‹äºˆè¿™ä¸¤ä¸ªæ–‡ä»¶å¯æ‰§è¡Œæƒé™ chmod 777 /usr/local/bin/iterm2-* 4. è®¾ç½®Iterm2çš„Tirggerç‰¹æ€§ è®¾ç½®Iterm2çš„Tirggerç‰¹æ€§ï¼Œprofiles-\u003edefault-\u003eeditProfiles-\u003eAdvancedä¸­çš„Tirgger\næ·»åŠ ä¸¤æ¡triggerï¼Œåˆ†åˆ«è®¾ç½® Regular expressionï¼ŒActionï¼ŒParametersï¼ŒInstantå¦‚ä¸‹ï¼š\nRegular expression: rz waiting to receive.\\*\\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checked Regular expression: \\*\\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked ç¤ºä¾‹å›¾ï¼š\n5. ä½¿ç”¨ ä¸Šä¼ æ–‡ä»¶ï¼šrz ä¸‹è½½æ–‡ä»¶ï¼šsz + file å‚è€ƒï¼š\nhttps://www.robberphex.com/use-zmodem-at-macos/ https://github.com/RobberPhex/iterm2-zmodem ","categories":"","description":"","excerpt":" æœ¬æ–‡ç”±ç½‘ç»œæ–‡ç« æ•´ç†å¤‡ä»½ã€‚\niterm2 rzä¸szçš„åŠŸèƒ½ æœ¬æ–‡ä¸»è¦ä»‹ç»macç¯å¢ƒä¸‹ä½¿ç”¨iterm2çš„rz szåŠŸèƒ½çš„å®‰è£…æµç¨‹ã€‚\n1. å®‰ â€¦","ref":"/linux-notes/keymap/iterm2-rzsz/","tags":["å¿«æ·é”®"],"title":"iterm2 rzä¸szçš„ä½¿ç”¨"},{"body":"1. webåŠç½‘ç»œåŸºç¡€ 1.1. é€šè¿‡HTTPè®¿é—®web[C/S] 1.2. TCP/IPå››å±‚æ¨¡å‹ 1.2.1. æ•°æ®åŒ…çš„å°è£… 1.3. TCP/IPåè®®æ— 1.3.1. è´Ÿè´£ä¼ è¾“çš„IPåè®® ä½¿ç”¨ARPåè®®å‡­å€ŸMACåœ°å€é€šä¿¡\n1.3.2. ç¡®ä¿å¯é çš„TCPåè®® 1.3.3. è´Ÿè´£åŸŸåè§£æçš„DNSæœåŠ¡ 1.3.4. å„åè®®ä¸HTTPçš„å…³ç³» 1.4. URIä¸URL URI(Uniform Resource Identifier):ç»Ÿä¸€èµ„æºæ ‡è¯†ç¬¦ URL(Uniform Resource Locator):ç»Ÿä¸€èµ„æºå®šä½ç¬¦ï¼›URLæ˜¯URIçš„å­é›† 1.4.1. URIçš„æ ¼å¼ å­—æ®µ è¯´æ˜ åè®® http/https ç™»å½•ä¿¡æ¯ï¼ˆè®¤è¯ï¼‰ user:pass@(ä¸€èˆ¬æ²¡æœ‰) æœåŠ¡å™¨åœ°å€ åŸŸåæˆ–IP æœåŠ¡å™¨ç«¯å£å· æœåŠ¡ç«¯å£å·ï¼Œçœç•¥åˆ™å–é»˜è®¤ç«¯å£å· å¸¦å±‚æ¬¡çš„æ–‡ä»¶è·¯å¾„ æŒ‡å®šæœåŠ¡å™¨ä¸Šçš„æ–‡ä»¶è·¯å¾„æ¥å®šä½ç‰¹æŒ‡çš„èµ„æº æŸ¥è¯¢å­—ç¬¦ä¸² ä½¿ç”¨æŸ¥è¯¢å­—ç¬¦ä¸²ä¼ å…¥å‚æ•° ç‰‡æ®µæ ‡è¯†ç¬¦ æ ‡è®°ä»¥è·å–èµ„æºä¸­çš„å­èµ„æºï¼ˆæ–‡æ¡£å†…çš„æŸä¸ªä½ç½®ï¼‰ 1.4.2. URIçš„ç¤ºä¾‹ 2. HTTPåè®® 2.1. é€šè¿‡è¯·æ±‚å’Œå“åº”çš„äº¤æ¢è¾¾æˆé€šä¿¡ 2.1.1. è¯·æ±‚æŠ¥æ–‡ 2.1.2. å“åº”æŠ¥æ–‡ 2.2. HTTPè¯·æ±‚æ–¹æ³• 2.2.1. GET:è·å–èµ„æº 2.2.2. POST:ä¼ è¾“å®ä½“ä¸»ä½“ 2.2.3. PUT:ä¼ è¾“æ–‡ä»¶ PUTæ–¹æ³•ç”¨æ¥ä¼ è¾“æ–‡ä»¶ï¼ŒåƒFTPåè®®ä¸€æ ·ï¼Œè¦æ±‚åœ¨è¯·æ±‚æŠ¥æ–‡çš„ä¸»ä½“ä¸­åŒ…å«æ–‡ä»¶å†…å®¹ï¼Œç„¶åä¿å­˜åˆ°è¯·æ±‚URIæŒ‡å®šçš„ä½ç½®ã€‚\nå› ä¸ºè‡ªèº«ä¸å¸¦éªŒè¯æœºåˆ¶ï¼Œæœ‰å®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸€èˆ¬ä¸é‡‡ç”¨ã€‚è‹¥é…åˆéªŒè¯æœºåˆ¶æˆ–è€…RESTæ ‡å‡†åˆ™å¯ä½¿ç”¨ã€‚\n2.2.4. HEAD:è·å–æŠ¥æ–‡å¤´éƒ¨ HEADå’ŒGETä¸€æ ·ä½†ä¸è¿”å›æŠ¥æ–‡ä¸»ä½“éƒ¨åˆ†ï¼Œç”¨äºç¡®è®¤URIçš„æœ‰æ•ˆæ€§åŠèµ„æºçš„æ›´æ–°æ—¶é—´ç­‰ã€‚\n2.2.5. DELETE:åˆ é™¤æ–‡ä»¶ DELETEä¸PUTä½œç”¨ç›¸åï¼Œä½†ä¸å¸¦å®‰å…¨éªŒè¯æœºåˆ¶ä¸€èˆ¬ä¸é‡‡ç”¨ã€‚\n2.2.6. OPTIONS:è¯¢é—®æ”¯æŒçš„æ–¹æ³• OPTIONSç”¨æ¥æŸ¥è¯¢é’ˆå¯¹è¯·æ±‚URIæŒ‡å®šçš„èµ„æºæ”¯æŒçš„æ–¹æ³•\n2.2.7. TRACE:è¿½è¸ªè·¯å¾„ TRACEç”¨æ¥æŸ¥è¯¢å‘é€å‡ºå»çš„è¯·æ±‚æ˜¯æ€æ ·è¢«åŠ å·¥ä¿®æ”¹/ç¯¡æ”¹çš„ï¼Œå› ä¸ºæ˜“å¼•å‘XSTï¼ˆè·¨ç«™è¿½è¸ªï¼‰æ”»å‡»ï¼Œä¸€èˆ¬ä¸ä½¿ç”¨ã€‚\n2.2.8. CONNECT:è¦æ±‚ç”¨éš§é“åè®®è¿æ¥ä»£ç† CONNECTè¦æ±‚åœ¨ä¸ä»£ç†æœåŠ¡å™¨é€šä¿¡æ—¶å»ºç«‹éš§é“ï¼Œå®ç°ç”¨éš§é“åè®®è¿›è¡ŒTCPé€šä¿¡ã€‚ä¸»è¦ä½¿ç”¨SSLï¼ˆSource Sockets Layer:å®‰å…¨å¥—æ¥å­—ï¼‰å’ŒTLSï¼ˆTransport Layer Security:ä¼ è¾“å±‚å®‰å…¨ï¼‰åè®®æŠŠé€šä¿¡å†…å®¹åŠ å¯†åç»ç½‘ç»œéš§é“ä¼ è¾“ã€‚\næ–¹æ³•æ ¼å¼å¦‚ä¸‹ï¼š\n2.3. æŒä¹…è¿æ¥ 2.3.1. keep-alive ä¸ºè§£å†³æ¯è¿›è¡Œä¸€æ¬¡HTTPé€šä¿¡å°±è¦æ–­å¼€ä¸€æ¬¡TCPè¿æ¥ï¼Œå¢åŠ äº†é€šä¿¡é‡çš„å¼€é”€ï¼ŒHTTP/1.1é€šè¿‡keep-aliveæŒä¹…è¿æ¥ï¼Œåªè¦ä»»æ„ä¸€ç«¯æ²¡æœ‰æ˜ç¡®æå‡ºæ–­å¼€è¿æ¥ï¼Œåˆ™ä¿æŒTCPè¿æ¥çŠ¶æ€ã€‚\næŒä¹…è¿æ¥å‡å°‘äº†TCPè¿æ¥çš„é‡å¤å»ºç«‹å’Œæ–­å¼€æ‰€é€ æˆçš„é¢å¤–å¼€é”€ï¼Œå‡è½»äº†æœåŠ¡å™¨ç«¯çš„è´Ÿè½½ã€‚\n2.3.2. ç®¡çº¿åŒ– æŒç»­è¿æ¥ä½¿å¾—å¤šæ•°è¯·æ±‚ä»¥ç®¡çº¿åŒ–ï¼ˆpipeliningï¼‰æ–¹å¼å‘é€æˆä¸ºå¯èƒ½ã€‚ç®¡çº¿åŒ–å³åŒæ—¶å¹¶è¡Œå‘é€å¤šä¸ªè¯·æ±‚ï¼Œè€Œä¸éœ€è¦ä¸€ä¸ªæ¥ä¸€ä¸ªç­‰å¾…å“åº”ã€‚ç®¡çº¿åŒ–æŠ€æœ¯æ¯”æŒç»­è¿æ¥é€Ÿåº¦å¿«ï¼Œè¯·æ±‚æ•°è¶Šå¤šè¶Šæ˜æ˜¾ã€‚\n2.3.3. ä½¿ç”¨cookieçš„çŠ¶æ€ç®¡ç† HTTPæ˜¯æ— çŠ¶æ€åè®®ï¼Œä¸å¯¹ä¹‹å‰å‘ç”Ÿè¿‡çš„è¯·æ±‚å’Œå“åº”çš„çŠ¶æ€è¿›è¡Œç®¡ç†ï¼Œå³æ— æ³•æ ¹æ®ä¹‹å‰çš„çŠ¶æ€è¿›è¡Œæœ¬æ¬¡çš„è¯·æ±‚å¤„ç†ã€‚æ— çŠ¶æ€åè®®çš„ä¼˜ç‚¹åœ¨äºä¸å¿…ä¿å­˜çŠ¶æ€ï¼Œå‡å°‘æœåŠ¡å™¨CPUåŠå†…å­˜èµ„æºçš„æ¶ˆè€—ã€‚\ncookieæŠ€æœ¯é€šè¿‡åœ¨è¯·æ±‚å’Œå“åº”æŠ¥æ–‡ä¸­å†™å…¥cookieä¿¡æ¯æ¥æ§åˆ¶å®¢æˆ·ç«¯çš„çŠ¶æ€ã€‚cookieä¼šæ ¹æ®ä»æœåŠ¡ç«¯å‘é€çš„å“åº”æŠ¥æ–‡å†…çš„ä¸€ä¸ªå«åšSet-Cookieçš„é¦–éƒ¨å­—æ®µé€šçŸ¥å®¢æˆ·ç«¯ä¿å­˜Cookieï¼›å½“å®¢æˆ·ç«¯å†å¾€æœåŠ¡ç«¯å‘é€è¯·æ±‚æ—¶ï¼Œå®¢æˆ·ç«¯è‡ªåŠ¨åœ¨è¯·æ±‚æŠ¥æ–‡ä¸­åŠ å…¥Cookieå€¼åå‘é€å‡ºå»ã€‚æœåŠ¡å™¨å‘ç°Cookieåä¼šæ£€æŸ¥ä»å“ªä¸ªå®¢æˆ·ç«¯å‘é€æ¥çš„è¿æ¥è¯·æ±‚ï¼Œå¯¹æ¯”æœåŠ¡å™¨ä¸Šçš„è®°å½•ï¼Œæœ€åå¾—åˆ°ä¹‹å‰çš„çŠ¶æ€ä¿¡æ¯ã€‚\n3. HTTPæŠ¥æ–‡ 3.1. HTTPæŠ¥æ–‡ ç”¨äºHTTPåè®®äº¤äº’çš„ä¿¡æ¯è¢«ç§°ä¸ºHTTPæŠ¥æ–‡ï¼Œå®¢æˆ·ç«¯çš„HTTPæŠ¥æ–‡å«åšè¯·æ±‚æŠ¥æ–‡ï¼ŒæœåŠ¡ç«¯çš„å«åšå“åº”æŠ¥æ–‡ã€‚æŠ¥æ–‡å¤§è‡´åˆ†ä¸ºæŠ¥æ–‡é¦–éƒ¨å’ŒæŠ¥æ–‡ä¸»ä½“ï¼Œä½†å¹¶ä¸ä¸€å®šè¦æœ‰æŠ¥æ–‡ä¸»ä½“ã€‚\n3.2. æŠ¥æ–‡ç»“æ„ å­—æ®µ è¯´æ˜ è¯·æ±‚è¡Œ è¯·æ±‚æ–¹æ³•ï¼Œè¯·æ±‚URIå’ŒHTTPç‰ˆæœ¬ çŠ¶æ€è¡Œ å“åº”ç»“æœçš„çŠ¶æ€ç ï¼ŒåŸå› çŸ­è¯­å’ŒHTTPç‰ˆæœ¬ é¦–éƒ¨å­—æ®µ è¯·æ±‚å’Œå“åº”çš„å„ç§æ¡ä»¶å’Œå±æ€§çš„å„ç±»é¦–éƒ¨ï¼šé€šç”¨é¦–éƒ¨ã€è¯·æ±‚é¦–éƒ¨ã€å“åº”é¦–éƒ¨ã€å®ä½“é¦–éƒ¨ å…¶ä»– HTTPçš„RFCé‡Œæœªå®šä¹‰çš„é¦–éƒ¨ï¼ˆCookieç­‰ï¼‰ 3.3. ç¼–ç æå‡ä¼ è¾“é€Ÿç‡ HTTPåœ¨ä¼ è¾“æ•°æ®æ—¶å¯ä»¥æŒ‰ç…§æ•°æ®åŸè²Œç›´æ¥ä¼ è¾“ä¹Ÿå¯ä»¥åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­ç¼–ç æå‡ä¼ è¾“é€Ÿç‡ï¼›é€šè¿‡ç¼–ç å¯ä»¥å¤„ç†å¤§é‡è¯·æ±‚ä½†ä¼šæ¶ˆè€—æ›´å¤šçš„CPUç­‰èµ„æºã€‚\n3.3.1. æŠ¥æ–‡ä¸»ä½“å’Œå®ä½“ä¸»ä½“çš„å·®å¼‚ æŠ¥æ–‡ï¼šæ˜¯HTTPé€šä¿¡ä¸­çš„åŸºæœ¬å•ä½ï¼Œç”±8ä½ç»„å­—èŠ‚æµç»„æˆï¼Œé€šè¿‡HTTPé€šä¿¡ä¼ è¾“ã€‚ å®ä½“ï¼šä½œä¸ºè¯·æ±‚æˆ–å“åº”çš„æœ‰æ•ˆè½½è·æ•°æ®è¢«ä¼ è¾“ï¼Œå…¶å†…å®¹ç”±å®ä½“é¦–éƒ¨å’Œå®ä½“ä¸»ä½“ç»„æˆã€‚ é€šå¸¸æŠ¥æ–‡ä¸»ä½“ç­‰äºå®ä½“ä¸»ä½“ï¼Œä½†å½“ä¼ è¾“ä¸­è¿›è¡Œç¼–ç æ—¶ï¼Œå®ä½“ä¸»ä½“çš„å†…å®¹å‘ç”Ÿå˜åŒ–æ‰ä¼šä¸æŠ¥æ–‡ä¸»ä½“äº§ç”Ÿå·®å¼‚ã€‚\n3.3.2. å‹ç¼©ä¼ è¾“çš„å†…å®¹ç¼–ç  HTTPä¸­çš„å†…å®¹ç¼–ç æŒ‡æ˜åº”ç”¨åœ¨å®ä½“å†…å®¹ä¸Šçš„ç¼–ç æ ¼å¼ï¼Œå¹¶ä¿æŒå®ä½“ä¿¡æ¯åŸæ ·å‹ç¼©ï¼Œå†…å®¹ç¼–ç åçš„å®ä½“ç”±å®¢æˆ·ç«¯æ¥æ”¶å¹¶è´Ÿè´£è§£ç ã€‚\nå¸¸ç”¨çš„å†…å®¹ç¼–ç ï¼š\ngzip(GNU ZIP) compress(UNIXç³»ç»Ÿçš„æ ‡å‡†å‹ç¼©) deflate(zlib) identity(ä¸è¿›è¡Œç¼–ç ) 3.3.3. åˆ†å—ä¼ è¾“ç¼–ç  åˆ†å—ä¼ è¾“ç¼–ç ä¼šå°†å®ä½“ä¸»ä½“åˆ†æˆå¤šä¸ªå—ï¼Œæ¯ä¸€å—éƒ½ä¼šç”¨åå…­è¿›åˆ¶æ¥æ ‡è®°å¿«çš„å¤§å°ï¼Œè€Œå®ä½“çš„æœ€åä¸€å—ä¼šä½¿ç”¨â€œ0ï¼ˆCR+LFï¼‰â€æ¥æ ‡è®°ã€‚\nç”±æ¥æ”¶çš„å®¢æˆ·ç«¯è´Ÿè´£è§£ç ï¼Œå›å¤åˆ°ç¼–ç å‰çš„å®ä½“ä¸»ä½“ã€‚\n3.4. å‘é€å¤šç§æ•°æ®çš„å¤šéƒ¨åˆ†å¯¹è±¡é›†åˆ HTTPä¸­çš„å¤šéƒ¨åˆ†å¯¹è±¡é›†åˆå³å‘é€ä¸€ä»½æŠ¥æ–‡ä¸»ä½“å†…å¯å«æœ‰å¤šç±»å‹å®ä½“ï¼Œé€šå¸¸æ˜¯å›¾ç‰‡æˆ–æ–‡æœ¬æ–‡ä»¶ä¸Šä¼ ç­‰ã€‚\nå¤šéƒ¨åˆ†å¯¹è±¡é›†åˆåŒ…å«çš„å¯¹è±¡ï¼š\nmultipart/form-data:åœ¨webè¡¨å•æ–‡ä»¶ä¸Šä¼ æ—¶ä½¿ç”¨ multipart/byterangesï¼šçŠ¶æ€ç 206å“åº”æŠ¥æ–‡åŒ…å«äº†å¤šä¸ªèŒƒå›´çš„å†…å®¹æ—¶ä½¿ç”¨ 3.5. è·å–éƒ¨åˆ†å†…å®¹çš„èŒƒå›´è¯·æ±‚ æŒ‡å®šèŒƒå›´å‘é€çš„è¯·æ±‚å«åšèŒƒå›´è¯·æ±‚ï¼Œå¯¹äºä¸€ä»½10000å­—èŠ‚å¤§å°çš„èµ„æºï¼Œå¦‚æœä½¿ç”¨èŒƒå›´è¯·æ±‚ï¼Œå¯ä»¥åªè¯·æ±‚5001-10000å­—èŠ‚å†…çš„èµ„æºã€‚\næ‰§è¡ŒèŒƒå›´è¯·æ±‚æ—¶ï¼Œä¼šç”¨åˆ°é¦–éƒ¨å­—æ®µRangeæ¥æŒ‡å®šèµ„æºçš„byteèŒƒå›´\n3.6. å†…å®¹åå•†è¿”å›æœ€åˆé€‚çš„å†…å®¹ å†…å®¹åå•†æœºåˆ¶æ˜¯æŒ‡å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯å°±å“åº”çš„èµ„æºå†…å®¹è¿›è¡Œäº¤æ¶‰ï¼Œç„¶åæä¾›ç»™å®¢æˆ·ç«¯æœ€åˆé€‚çš„èµ„æºã€‚å†…å®¹åå•†ä¼šä»¥å“åº”èµ„æºçš„è¯­è¨€ã€ç¼–ç æ–¹å¼ç­‰ä½œä¸ºåˆ¤æ–­çš„åŸºå‡†ã€‚\nå†…å®¹åå•†ç±»å‹ï¼š\næœåŠ¡å™¨é©±åŠ¨åå•† å®¢æˆ·ç«¯é©±åŠ¨åå•† é€æ˜åå•† 4. HTTPçŠ¶æ€ç  çŠ¶æ€ç å³æœåŠ¡å™¨è¿”å›çš„è¯·æ±‚ç»“æœã€‚\nçŠ¶æ€ç  ç±»å‹ è¯´æ˜ 1xx Informational(ä¿¡æ¯æ€§çŠ¶æ€ç ) æ¥æ”¶çš„è¯·æ±‚æ­£åœ¨å¤„ç† 2xx Success(æˆåŠŸ) è¯·æ±‚æ­£å¸¸å¤„ç†å®Œæ¯• 3xx Redirection(é‡å®šå‘) éœ€è¦è¿›è¡Œé™„åŠ æ“ä½œä»¥å®Œæˆè¯·æ±‚ 4xx Client Error(å®¢æˆ·ç«¯é”™è¯¯) æœåŠ¡å™¨æ— æ³•å¤„ç†è¯·æ±‚ 5xx Server Error(æœåŠ¡ç«¯é”™è¯¯) æœåŠ¡å™¨å¤„ç†è¯·æ±‚å‡ºé”™ 4.1. 2XXæˆåŠŸ 4.1.1. 200 OK 4.1.2. 204 No Content è¡¨ç¤ºè¯·æ±‚å·²æˆåŠŸå¤„ç†ï¼Œä½†åœ¨è¿”å›çš„å“åº”æŠ¥æ–‡ä¸­ä¸å«å®ä½“çš„ä¸»ä½“éƒ¨åˆ†ã€‚\n4.1.3. 206 Partial Content è¯¥çŠ¶æ€ç è¡¨ç¤ºå®¢æˆ·ç«¯è¿›è¡Œäº†èŒƒå›´è¯·æ±‚ï¼ŒæœåŠ¡å™¨æˆåŠŸæ‰§è¡Œäº†è¿™éƒ¨åˆ†çš„GETè¯·æ±‚ã€‚å“åº”æŠ¥æ–‡ä¸­åŒ…å«ç”±Content-RangeæŒ‡å®šèŒƒå›´çš„å®ä½“å†…å®¹ã€‚\n4.2. 3XX é‡å®šå‘ 4.2.1. 301 Moved Permanently æ°¸ä¹…æ€§é‡å®šå‘ï¼Œè¡¨ç¤ºèµ„æºå·²è¢«åˆ†é…äº†æ–°çš„URIï¼Œä»¥ååº”ä½¿ç”¨æ–°çš„URIã€‚\n4.2.2. 302 Found ä¸´æ—¶æ€§é‡å®šå‘ï¼Œè¡¨ç¤ºè¯·æ±‚çš„èµ„æºå·²è¢«åˆ†é…äº†æ–°çš„URIï¼Œä½†æ˜¯ä¸´æ—¶æ€§çš„ã€‚\n4.2.3. 303 See Other è¡¨ç¤ºç”±äºè¯·æ±‚çš„èµ„æºå­˜åœ¨å¦ä¸€ä¸ªURIï¼Œåº”ä½¿ç”¨GETæ–¹æ³•é‡å®šå‘è·å–è¯·æ±‚çš„èµ„æºã€‚\n4.2.4. 304 Not Modified è¡¨ç¤ºå®¢æˆ·ç«¯å‘é€é™„å¸¦æ¡ä»¶çš„è¯·æ±‚æ—¶ï¼ˆGETä¸­çš„If-Modified-Sinceç­‰é¦–éƒ¨ï¼‰ï¼ŒæœåŠ¡å™¨å…è®¸è®¿é—®èµ„æºï¼Œä½†æœªæ»¡è¶³é™„å¸¦æ¡ä»¶å› æ­¤ç›´æ¥è¿”å›304ï¼ˆæœåŠ¡å™¨çš„èµ„æºæœªæ”¹å˜ï¼Œå¯ç›´æ¥ä½¿ç”¨å®¢æˆ·ç«¯æœªè¿‡æœŸçš„ç¼“å­˜ï¼‰ï¼Œä¸åŒ…å«ä»»ä½•å“åº”çš„ä¸»ä½“éƒ¨åˆ†ã€‚\n4.2.5. 307 Temporary Redirect ä¸´æ—¶é‡å®šå‘ï¼Œè¯¥çŠ¶æ€ä¸302æœ‰ç›¸åŒçš„å«ä¹‰ã€‚\n4.3. 4XX å®¢æˆ·ç«¯é”™è¯¯ 4.3.1. 400 Bad Request è¡¨ç¤ºè¯·æ±‚æŠ¥æ–‡ä¸­å­˜åœ¨è¯­æ³•é”™è¯¯ï¼Œéœ€ä¿®æ”¹å†…å®¹é‡æ–°å‘é€è¯·æ±‚ã€‚\n4.3.2. 401 Unauthorized è¡¨ç¤ºéœ€è¦é€šè¿‡HTTPè®¤è¯ã€‚\n4.3.3. 403 Forbidden è¡¨ç¤ºè¯·æ±‚è¢«æœåŠ¡å™¨æ‹’ç»ï¼Œæœªè·å¾—è®¿é—®æˆæƒã€‚\n4.3.4. 404 No Found è¡¨æ˜æœåŠ¡å™¨ä¸Šæ‰¾ä¸åˆ°è¯·æ±‚çš„èµ„æºï¼Œä¹Ÿå¯ä»¥åœ¨æœåŠ¡å™¨æ‹’ç»è¯·æ±‚ä¸”ä¸æƒ³è¯´æ˜ç†ç”±æ—¶ä½¿ç”¨ã€‚\n4.4. 5XX æœåŠ¡å™¨é”™è¯¯ 4.4.1. 500 Internal Server Error è¡¨æ˜æœåŠ¡å™¨åœ¨æ‰§è¡Œè¯·æ±‚æ—¶å‘ç”Ÿäº†é”™è¯¯ï¼Œä¹Ÿå¯èƒ½æ˜¯Webåº”ç”¨å­˜åœ¨bugæˆ–ä¸´æ—¶æ•…éšœç­‰ã€‚\n4.4.2. 503 Service Unavailable è¡¨æ˜æœåŠ¡å™¨æš‚æ—¶å¤„äºè¶…è´Ÿè·æˆ–æ­£åœ¨è¿›è¡Œåœæœºç»´æŠ¤ï¼Œç°åœ¨ä¸èƒ½å¤„ç†è¯·æ±‚ã€‚\nå‚è€ƒï¼š\nã€Šå›¾è§£HTTPã€‹ ","categories":"","description":"","excerpt":"1. webåŠç½‘ç»œåŸºç¡€ 1.1. é€šè¿‡HTTPè®¿é—®web[C/S] 1.2. TCP/IPå››å±‚æ¨¡å‹ 1.2.1. æ•°æ®åŒ…çš„å°è£… 1.3. â€¦","ref":"/linux-notes/tcpip/http/","tags":["TCPIP"],"title":"HTTPåè®®"},{"body":"1. InfluxDBç®€ä»‹ InfluxDBæ˜¯ä¸€ä¸ªå½“ä¸‹æ¯”è¾ƒæµè¡Œçš„æ—¶åºæ•°æ®åº“ï¼ŒInfluxDBä½¿ç”¨ Go è¯­è¨€ç¼–å†™ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–ï¼Œå®‰è£…é…ç½®éå¸¸æ–¹ä¾¿ï¼Œé€‚åˆæ„å»ºå¤§å‹åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç›‘æ§ç³»ç»Ÿã€‚\nä¸»è¦ç‰¹è‰²åŠŸèƒ½ï¼š\n1ï¼‰åŸºäºæ—¶é—´åºåˆ—ï¼Œæ”¯æŒä¸æ—¶é—´æœ‰å…³çš„ç›¸å…³å‡½æ•°ï¼ˆå¦‚æœ€å¤§ï¼Œæœ€å°ï¼Œæ±‚å’Œç­‰ï¼‰\n2ï¼‰å¯åº¦é‡æ€§ï¼šä½ å¯ä»¥å®æ—¶å¯¹å¤§é‡æ•°æ®è¿›è¡Œè®¡ç®—\n3ï¼‰åŸºäºäº‹ä»¶ï¼šå®ƒæ”¯æŒä»»æ„çš„äº‹ä»¶æ•°æ®\n2. InfluxDBå®‰è£… 1ï¼‰å®‰è£… wget https://dl.influxdata.com/influxdb/releases/influxdb-0.13.0.x86_64.rpm\nyum localinstall influxdb-0.13.0.armhf.rpm\n2ï¼‰å¯åŠ¨ service influxdb start\n3ï¼‰è®¿é—® http://æœåŠ¡å™¨IP:8083\n4ï¼‰docker imageæ–¹å¼å®‰è£… docker pull influxdb\ndocker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --volume=/opt/data/influxdb:/data --name influxsrv influxdb:latest\n3. InfluxDBçš„åŸºæœ¬æ¦‚å¿µ 3.1. ä¸ä¼ ç»Ÿæ•°æ®åº“ä¸­çš„åè¯åšæ¯”è¾ƒ influxDBä¸­çš„åè¯ ä¼ ç»Ÿæ•°æ®åº“ä¸­çš„æ¦‚å¿µ database æ•°æ®åº“ measurement æ•°æ®åº“ä¸­çš„è¡¨ points è¡¨é‡Œé¢çš„ä¸€è¡Œæ•°æ® 3.2. InfluxDBä¸­ç‹¬æœ‰çš„æ¦‚å¿µ 3.2.1. Point Pointç”±æ—¶é—´æˆ³ï¼ˆtimeï¼‰ã€æ•°æ®ï¼ˆfieldï¼‰ã€æ ‡ç­¾ï¼ˆtagsï¼‰ç»„æˆã€‚\nPointç›¸å½“äºä¼ ç»Ÿæ•°æ®åº“é‡Œçš„ä¸€è¡Œæ•°æ®ï¼Œå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\nPointå±æ€§ ä¼ ç»Ÿæ•°æ®åº“ä¸­çš„æ¦‚å¿µ time æ¯ä¸ªæ•°æ®è®°å½•æ—¶é—´ï¼Œæ˜¯æ•°æ®åº“ä¸­çš„ä¸»ç´¢å¼•(ä¼šè‡ªåŠ¨ç”Ÿæˆ) fields å„ç§è®°å½•å€¼ï¼ˆæ²¡æœ‰ç´¢å¼•çš„å±æ€§ï¼‰ä¹Ÿå°±æ˜¯è®°å½•çš„å€¼ï¼šæ¸©åº¦ï¼Œ æ¹¿åº¦ tags å„ç§æœ‰ç´¢å¼•çš„å±æ€§ï¼šåœ°åŒºï¼Œæµ·æ‹” 3.2.2. series æ‰€æœ‰åœ¨æ•°æ®åº“ä¸­çš„æ•°æ®ï¼Œéƒ½éœ€è¦é€šè¿‡å›¾è¡¨æ¥å±•ç¤ºï¼Œè€Œè¿™ä¸ªseriesè¡¨ç¤ºè¿™ä¸ªè¡¨é‡Œé¢çš„æ•°æ®ï¼Œå¯ä»¥åœ¨å›¾è¡¨ä¸Šç”»æˆå‡ æ¡çº¿ï¼šé€šè¿‡tagsæ’åˆ—ç»„åˆç®—å‡ºæ¥\nshow series from cpu\n4. InfluxDBçš„åŸºæœ¬æ“ä½œ InfluxDBæä¾›ä¸‰ç§æ“ä½œæ–¹å¼ï¼š\n1ï¼‰å®¢æˆ·ç«¯å‘½ä»¤è¡Œæ–¹å¼\n2ï¼‰HTTP APIæ¥å£\n3ï¼‰å„è¯­è¨€APIåº“\n4.1. InfluxDBæ•°æ®åº“æ“ä½œ æ“ä½œ å‘½ä»¤ æ˜¾ç¤ºæ•°æ®åº“ show databases åˆ›å»ºæ•°æ®åº“ create database db_name åˆ é™¤æ•°æ®åº“ drop database db_name ä½¿ç”¨æŸä¸ªæ•°æ®åº“ use db_name 4.2. InfluxDBæ•°æ®è¡¨æ“ä½œ æ“ä½œ å‘½ä»¤ è¯´æ˜ æ˜¾ç¤ºæ‰€æœ‰è¡¨ SHOW MEASUREMENTS åˆ›å»ºæ•°æ®è¡¨ insert table_name,hostname=server01 value=442221834240i 1435362189575692182 å…¶ä¸­ disk_free å°±æ˜¯è¡¨åï¼Œhostnameæ˜¯ç´¢å¼•ï¼Œvalue=xxæ˜¯è®°å½•å€¼ï¼Œè®°å½•å€¼å¯ä»¥æœ‰å¤šä¸ªï¼Œæœ€åæ˜¯æŒ‡å®šçš„æ—¶é—´ åˆ é™¤æ•°æ®è¡¨ drop measurement table_name æŸ¥çœ‹è¡¨å†…å®¹ select * from table_name æŸ¥çœ‹series show series from table_name seriesè¡¨ç¤ºè¿™ä¸ªè¡¨é‡Œé¢çš„æ•°æ®ï¼Œå¯ä»¥åœ¨å›¾è¡¨ä¸Šç”»æˆå‡ æ¡çº¿ï¼Œseriesä¸»è¦é€šè¿‡tagsæ’åˆ—ç»„åˆç®—å‡ºæ¥ ","categories":"","description":"","excerpt":"1. InfluxDBç®€ä»‹ InfluxDBæ˜¯ä¸€ä¸ªå½“ä¸‹æ¯”è¾ƒæµè¡Œçš„æ—¶åºæ•°æ®åº“ï¼ŒInfluxDBä½¿ç”¨ Go è¯­è¨€ç¼–å†™ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–ï¼Œå®‰è£…é…ç½®éå¸¸ â€¦","ref":"/kubernetes-notes/monitor/influxdb-introduction/","tags":["Monitor"],"title":"Influxdbä»‹ç»"},{"body":"Pod Volume åŒä¸€ä¸ªPodä¸­çš„å¤šä¸ªå®¹å™¨å¯ä»¥å…±äº«Podçº§åˆ«çš„å­˜å‚¨å·Volume,Volumeå¯ä»¥å®šä¹‰ä¸ºå„ç§ç±»å‹ï¼Œå¤šä¸ªå®¹å™¨å„è‡ªè¿›è¡ŒæŒ‚è½½ï¼Œå°†Podçš„VolumeæŒ‚è½½ä¸ºå®¹å™¨å†…éƒ¨éœ€è¦çš„ç›®å½•ã€‚\nä¾‹å¦‚ï¼šPodçº§åˆ«çš„Volume:\"app-logs\",ç”¨äºtomcatå‘å…¶ä¸­å†™æ—¥å¿—æ–‡ä»¶ï¼Œbusyboxè¯»æ—¥å¿—æ–‡ä»¶ã€‚\npod-volumes-applogs.yaml\napiVersion: v1 kind: Pod metadata: name: volume-pod spec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080 volumeMounts: - name: app-logs mountPath: /usr/local/tomcat/logs - name: busybox image: busybox command: [\"sh\",\"-c\",\"tailf /logs/catalina*.log\"] volumeMounts: - name: app-logs mountPath: /logs volumes: - name: app-logs emptuDir: {} æŸ¥çœ‹æ—¥å¿—\nkubectl logs \u003cpod_name\u003e -c \u003ccontainer_name\u003e kubectl exec -it \u003cpod_name\u003e -c \u003ccontainer_name\u003e â€“ tail /usr/local/tomcat/logs/catalina.xx.log å‚è€ƒæ–‡ç« \nã€ŠKubernetesæƒå¨æŒ‡å—ã€‹ ","categories":"","description":"","excerpt":"Pod Volume åŒä¸€ä¸ªPodä¸­çš„å¤šä¸ªå®¹å™¨å¯ä»¥å…±äº«Podçº§åˆ«çš„å­˜å‚¨å·Volume,Volumeå¯ä»¥å®šä¹‰ä¸ºå„ç§ç±»å‹ï¼Œå¤šä¸ªå®¹å™¨å„è‡ªè¿›è¡ŒæŒ‚è½½ï¼Œ â€¦","ref":"/kubernetes-notes/concepts/pod/pod-volume/","tags":["Kubernetes"],"title":"Podå­˜å‚¨å·"},{"body":"1. Etcdé…ç½®å‚æ•° / # etcd --help usage: etcd [flags] start an etcd server etcd --version show the version of etcd etcd -h | --help show the help information about etcd etcd --config-file path to the server configuration file etcd gateway run the stateless pass-through etcd TCP connection forwarding proxy etcd grpc-proxy run the stateless etcd v3 gRPC L7 reverse proxy 1.1. member flags member flags: --name 'default' human-readable name for this member. --data-dir '${name}.etcd' path to the data directory. --wal-dir '' path to the dedicated wal directory. --snapshot-count '100000' number of committed transactions to trigger a snapshot to disk. --heartbeat-interval '100' time (in milliseconds) of a heartbeat interval. --election-timeout '1000' time (in milliseconds) for an election to timeout. See tuning documentation for details. --initial-election-tick-advance 'true' whether to fast-forward initial election ticks on boot for faster election. --listen-peer-urls 'http://localhost:2380' list of URLs to listen on for peer traffic. --listen-client-urls 'http://localhost:2379' list of URLs to listen on for client traffic. --max-snapshots '5' maximum number of snapshot files to retain (0 is unlimited). --max-wals '5' maximum number of wal files to retain (0 is unlimited). --cors '' comma-separated whitelist of origins for CORS (cross-origin resource sharing). --quota-backend-bytes '0' raise alarms when backend size exceeds the given quota (0 defaults to low space quota). --max-txn-ops '128' maximum number of operations permitted in a transaction. --max-request-bytes '1572864' maximum client request size in bytes the server will accept. --grpc-keepalive-min-time '5s' minimum duration interval that a client should wait before pinging server. --grpc-keepalive-interval '2h' frequency duration of server-to-client ping to check if a connection is alive (0 to disable). --grpc-keepalive-timeout '20s' additional duration of wait before closing a non-responsive connection (0 to disable). 1.2. clustering flags clustering flags: --initial-advertise-peer-urls 'http://localhost:2380' list of this member's peer URLs to advertise to the rest of the cluster. --initial-cluster 'default=http://localhost:2380' initial cluster configuration for bootstrapping. --initial-cluster-state 'new' initial cluster state ('new' or 'existing'). --initial-cluster-token 'etcd-cluster' initial cluster token for the etcd cluster during bootstrap. Specifying this can protect you from unintended cross-cluster interaction when running multiple clusters. --advertise-client-urls 'http://localhost:2379' list of this member's client URLs to advertise to the public. The client URLs advertised should be accessible to machines that talk to etcd cluster. etcd client libraries parse these URLs to connect to the cluster. --discovery '' discovery URL used to bootstrap the cluster. --discovery-fallback 'proxy' expected behavior ('exit' or 'proxy') when discovery services fails. \"proxy\" supports v2 API only. --discovery-proxy '' HTTP proxy to use for traffic to discovery service. --discovery-srv '' dns srv domain used to bootstrap the cluster. --strict-reconfig-check 'true' reject reconfiguration requests that would cause quorum loss. --auto-compaction-retention '0' auto compaction retention length. 0 means disable auto compaction. --auto-compaction-mode 'periodic' interpret 'auto-compaction-retention' one of: periodic|revision. 'periodic' for duration based retention, defaulting to hours if no time unit is provided (e.g. '5m'). 'revision' for revision number based retention. --enable-v2 'true' Accept etcd V2 client requests. 1.3. proxy flags proxy flags: \"proxy\" supports v2 API only. --proxy 'off' proxy mode setting ('off', 'readonly' or 'on'). --proxy-failure-wait 5000 time (in milliseconds) an endpoint will be held in a failed state. --proxy-refresh-interval 30000 time (in milliseconds) of the endpoints refresh interval. --proxy-dial-timeout 1000 time (in milliseconds) for a dial to timeout. --proxy-write-timeout 5000 time (in milliseconds) for a write to timeout. --proxy-read-timeout 0 time (in milliseconds) for a read to timeout. 1.4. security flags security flags: --ca-file '' [DEPRECATED] path to the client server TLS CA file. '-ca-file ca.crt' could be replaced by '-trusted-ca-file ca.crt -client-cert-auth' and etcd will perform the same. --cert-file '' path to the client server TLS cert file. --key-file '' path to the client server TLS key file. --client-cert-auth 'false' enable client cert authentication. --client-crl-file '' path to the client certificate revocation list file. --trusted-ca-file '' path to the client server TLS trusted CA cert file. --auto-tls 'false' client TLS using generated certificates. --peer-ca-file '' [DEPRECATED] path to the peer server TLS CA file. '-peer-ca-file ca.crt' could be replaced by '-peer-trusted-ca-file ca.crt -peer-client-cert-auth' and etcd will perform the same. --peer-cert-file '' path to the peer server TLS cert file. --peer-key-file '' path to the peer server TLS key file. --peer-client-cert-auth 'false' enable peer client cert authentication. --peer-trusted-ca-file '' path to the peer server TLS trusted CA file. --peer-auto-tls 'false' peer TLS using self-generated certificates if --peer-key-file and --peer-cert-file are not provided. --peer-crl-file '' path to the peer certificate revocation list file. 1.5. logging flags logging flags --debug 'false' enable debug-level logging for etcd. --log-package-levels '' specify a particular log level for each etcd package (eg: 'etcdmain=CRITICAL,etcdserver=DEBUG'). --log-output 'default' specify 'stdout' or 'stderr' to skip journald logging even when running under systemd. 1.6. unsafe flags unsafe flags: Please be CAUTIOUS when using unsafe flags because it will break the guarantees given by the consensus protocol. --force-new-cluster 'false' force to create a new one-member cluster. 1.7. profiling flags profiling flags: --enable-pprof 'false' Enable runtime profiling data via HTTP server. Address is at client URL + \"/debug/pprof/\" --metrics 'basic' Set level of detail for exported metrics, specify 'extensive' to include histogram metrics. --listen-metrics-urls '' List of URLs to listen on for metrics. 1.8. auth flags auth flags: --auth-token 'simple' Specify a v3 authentication token type and its options ('simple' or 'jwt'). 1.9. experimental flags experimental flags: --experimental-initial-corrupt-check 'false' enable to check data corruption before serving any client/peer traffic. --experimental-corrupt-check-time '0s' duration of time between cluster corruption check passes. --experimental-enable-v2v3 '' serve v2 requests through the v3 backend under a given prefix. ","categories":"","description":"","excerpt":"1. Etcdé…ç½®å‚æ•° / # etcd --help usage: etcd [flags] start an etcd server â€¦","ref":"/kubernetes-notes/etcd/etcd-setup-flags/","tags":["Etcd"],"title":"Etcdå¯åŠ¨é…ç½®å‚æ•°"},{"body":"1. Dockerfileçš„è¯´æ˜ dockerfileæŒ‡ä»¤å¿½ç•¥å¤§å°å†™ï¼Œå»ºè®®å¤§å†™ï¼Œ#ä½œä¸ºæ³¨é‡Šï¼Œæ¯è¡Œåªæ”¯æŒä¸€æ¡æŒ‡ä»¤ï¼ŒæŒ‡ä»¤å¯ä»¥å¸¦å¤šä¸ªå‚æ•°ã€‚\ndockerfileæŒ‡ä»¤åˆ†ä¸ºæ„å»ºæŒ‡ä»¤å’Œè®¾ç½®æŒ‡ä»¤ã€‚\næ„å»ºæŒ‡ä»¤ï¼šç”¨äºæ„å»ºimageï¼Œå…¶æŒ‡å®šçš„æ“ä½œä¸ä¼šåœ¨è¿è¡Œimageçš„å®¹å™¨ä¸­æ‰§è¡Œã€‚ è®¾ç½®æŒ‡ä»¤ï¼šç”¨äºè®¾ç½®imageçš„å±æ€§ï¼Œå…¶æŒ‡å®šçš„æ“ä½œä¼šåœ¨è¿è¡Œimageçš„å®¹å™¨ä¸­æ‰§è¡Œã€‚ 2. DockerfileæŒ‡ä»¤è¯´æ˜ 2.1. FROMï¼ˆæŒ‡å®šåŸºç¡€é•œåƒï¼‰[æ„å»ºæŒ‡ä»¤] è¯¥å‘½ä»¤ç”¨æ¥æŒ‡å®šåŸºç¡€é•œåƒï¼Œåœ¨åŸºç¡€é•œåƒçš„åŸºç¡€ä¸Šä¿®æ”¹æ•°æ®ä»è€Œæ„å»ºæ–°çš„é•œåƒã€‚åŸºç¡€é•œåƒå¯ä»¥æ˜¯æœ¬åœ°ä»“åº“ä¹Ÿå¯ä»¥æ˜¯è¿œç¨‹ä»“åº“ã€‚\næŒ‡ä»¤æœ‰ä¸¤ç§æ ¼å¼ï¼š\nFROM image ã€é»˜è®¤ä¸ºlatestç‰ˆæœ¬ã€‘ FROM image:tag ã€æŒ‡å®šç‰ˆæœ¬ã€‘ 2.2. MAINTAINERï¼ˆé•œåƒåˆ›å»ºè€…ä¿¡æ¯ï¼‰[æ„å»ºæŒ‡ä»¤] å°†é•œåƒåˆ¶ä½œè€…ï¼ˆç»´æŠ¤è€…ï¼‰çš„ä¿¡æ¯å†™å…¥imageä¸­ï¼Œæ‰§è¡Œdocker inspectæ—¶ä¼šè¾“å‡ºè¯¥ä¿¡æ¯ã€‚\næ ¼å¼ï¼šMAINTAINER name\nMAINTAINERå‘½ä»¤å·²åºŸå¼ƒï¼Œå¯ä½¿ç”¨maintainer labelçš„æ–¹å¼ã€‚\nLABEL maintainer=\"SvenDowideit@home.org.au\" 2.3. RUNï¼ˆå®‰è£…è½¯ä»¶ç”¨ï¼‰[æ„å»ºæŒ‡ä»¤] RUNå¯ä»¥è¿è¡Œä»»ä½•è¢«åŸºç¡€é•œåƒæ”¯æŒçš„å‘½ä»¤ï¼ˆå³åœ¨åŸºç¡€é•œåƒä¸Šæ‰§è¡Œä¸€ä¸ªè¿›ç¨‹ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨å¤šæ¡RUNæŒ‡ä»¤ï¼ŒæŒ‡ä»¤è¾ƒé•¿å¯ä»¥ä½¿ç”¨\\æ¥æ¢è¡Œã€‚\næŒ‡ä»¤æœ‰ä¸¤ç§æ ¼å¼ï¼š\nRUN command (the command is run in a shell - /bin/sh -c) RUN [\"executable\", \"param1\", \"param2\" ... ] (exec form) æŒ‡å®šä½¿ç”¨å…¶ä»–ç»ˆç«¯å®ç°ï¼Œä½¿ç”¨execæ‰§è¡Œã€‚ ä¾‹å­ï¼šRUN[\"/bin/bash\",\"-c\",\"echo hello\"] 2.4. CMDï¼ˆè®¾ç½®containerå¯åŠ¨æ—¶æ‰§è¡Œçš„æ“ä½œï¼‰[è®¾ç½®æŒ‡ä»¤] ç”¨äºå®¹å™¨å¯åŠ¨æ—¶çš„æŒ‡å®šæ“ä½œï¼Œå¯ä»¥æ˜¯è‡ªå®šä¹‰è„šæœ¬æˆ–å‘½ä»¤ï¼Œåªæ‰§è¡Œä¸€æ¬¡ï¼Œå¤šä¸ªé»˜è®¤æ‰§è¡Œæœ€åä¸€ä¸ªã€‚\næŒ‡ä»¤æœ‰ä¸‰ç§æ ¼å¼ï¼š\nCMD [\"executable\",\"param1\",\"param2\"] (like an exec, this is the preferred form) è¿è¡Œä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶å¹¶æä¾›å‚æ•°ã€‚ CMD command param1 param2 (as a shell) ç›´æ¥æ‰§è¡Œshellå‘½ä»¤ï¼Œé»˜è®¤ä»¥/bin/sh -cæ‰§è¡Œã€‚ CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT) å’ŒENTRYPOINTé…åˆä½¿ç”¨ï¼Œåªä½œä¸ºå®Œæ•´å‘½ä»¤çš„å‚æ•°éƒ¨åˆ†ã€‚ 2.5. ENTRYPOINTï¼ˆè®¾ç½®containerå¯åŠ¨æ—¶æ‰§è¡Œçš„æ“ä½œï¼‰[è®¾ç½®æŒ‡ä»¤] æŒ‡å®šå®¹å™¨å¯åŠ¨æ—¶æ‰§è¡Œçš„å‘½ä»¤ï¼Œè‹¥å¤šæ¬¡è®¾ç½®åªæ‰§è¡Œæœ€åä¸€æ¬¡ã€‚\nENTRYPOINTç¿»è¯‘ä¸ºâ€œè¿›å…¥ç‚¹â€ï¼Œå®ƒçš„åŠŸèƒ½å¯ä»¥è®©å®¹å™¨è¡¨ç°å¾—åƒä¸€ä¸ªå¯æ‰§è¡Œç¨‹åºä¸€æ ·ã€‚\nä¾‹å­ï¼šENTRYPOINT [\"/bin/echo\"] ï¼Œé‚£ä¹ˆdocker buildå‡ºæ¥çš„é•œåƒä»¥åçš„å®¹å™¨åŠŸèƒ½å°±åƒä¸€ä¸ª/bin/echoç¨‹åºï¼Œdocker run -it imageecho â€œthis is a testâ€ï¼Œå°±ä¼šè¾“å‡ºå¯¹åº”çš„å­—ç¬¦ä¸²ã€‚è¿™ä¸ªimageechoé•œåƒå¯¹åº”çš„å®¹å™¨è¡¨ç°å‡ºæ¥çš„åŠŸèƒ½å°±åƒä¸€ä¸ªechoç¨‹åºä¸€æ ·ã€‚\næŒ‡ä»¤æœ‰ä¸¤ç§æ ¼å¼ï¼š\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] (like an exec, the preferred form)\nå’ŒCMDé…åˆä½¿ç”¨ï¼ŒCMDåˆ™ä½œä¸ºå®Œæ•´å‘½ä»¤çš„å‚æ•°éƒ¨åˆ†ï¼ŒENTRYPOINTä»¥JSONæ ¼å¼æŒ‡å®šæ‰§è¡Œçš„å‘½ä»¤éƒ¨åˆ†ã€‚CMDå¯ä»¥ä¸ºENTRYPOINTæä¾›å¯å˜å‚æ•°ï¼Œä¸éœ€è¦å˜åŠ¨çš„å‚æ•°å¯ä»¥å†™åœ¨ENTRYPOINTé‡Œé¢ã€‚\nä¾‹å­ï¼š\nENTRYPOINT [\"/usr/bin/ls\",\"-a\"]\nCMD [\"-l\"]\nENTRYPOINT command param1 param2 (as a shell)\nç‹¬è‡ªä½¿ç”¨ï¼Œå³å’ŒCMDç±»ä¼¼ï¼Œå¦‚æœCMDä¹Ÿæ˜¯ä¸ªå®Œæ•´å‘½ä»¤[CMD command param1 param2 (as a shell) ]ï¼Œé‚£ä¹ˆä¼šç›¸äº’è¦†ç›–ï¼Œåªæ‰§è¡Œæœ€åä¸€ä¸ªCMDæˆ–ENTRYPOINTã€‚ ä¾‹å­ï¼šENTRYPOINT ls -l 2.6. USERï¼ˆè®¾ç½®containerå®¹å™¨å¯åŠ¨çš„ç™»å½•ç”¨æˆ·ï¼‰[è®¾ç½®æŒ‡ä»¤] è®¾ç½®å¯åŠ¨å®¹å™¨çš„ç”¨æˆ·ï¼Œé»˜è®¤ä¸ºrootç”¨æˆ·ã€‚\næ ¼å¼ï¼šUSER daemon\n2.7. EXPOSEï¼ˆæŒ‡å®šå®¹å™¨éœ€è¦æ˜ å°„åˆ°å®¿ä¸»æœºçš„ç«¯å£ï¼‰[è®¾ç½®æŒ‡ä»¤] è¯¥æŒ‡ä»¤ä¼šå°†å®¹å™¨ä¸­çš„ç«¯å£æ˜ å°„ä¸ºå®¿ä¸»æœºä¸­çš„ç«¯å£[ç¡®ä¿å®¿ä¸»æœºçš„ç«¯å£å·æ²¡æœ‰è¢«ä½¿ç”¨]ã€‚é€šè¿‡å®¿ä¸»æœºIPå’Œæ˜ å°„åçš„ç«¯å£å³å¯è®¿é—®å®¹å™¨[é¿å…æ¯æ¬¡è¿è¡Œå®¹å™¨æ—¶IPéšæœºç”Ÿæˆä¸å›ºå®šçš„é—®é¢˜]ã€‚å‰ææ˜¯EXPOSEè®¾ç½®æ˜ å°„ç«¯å£ï¼Œè¿è¡Œå®¹å™¨æ—¶åŠ ä¸Š-på‚æ•°æŒ‡å®šEXPOSEè®¾ç½®çš„ç«¯å£ã€‚EXPOSEå¯ä»¥è®¾ç½®å¤šä¸ªç«¯å£å·ï¼Œç›¸åº”åœ°è¿è¡Œå®¹å™¨é…å¥—å¤šæ¬¡ä½¿ç”¨-på‚æ•°ã€‚å¯ä»¥é€šè¿‡docker port +å®¹å™¨éœ€è¦æ˜ å°„çš„ç«¯å£å·å’Œå®¹å™¨IDæ¥å‚è€ƒå®¿ä¸»æœºçš„æ˜ å°„ç«¯å£ã€‚\næ ¼å¼ï¼šEXPOSE port [port...]\n2.8. ENVï¼ˆç”¨äºè®¾ç½®ç¯å¢ƒå˜é‡ï¼‰[æ„å»ºæŒ‡ä»¤] åœ¨imageä¸­è®¾ç½®ç¯å¢ƒå˜é‡[ä»¥é”®å€¼å¯¹çš„å½¢å¼]ï¼Œè®¾ç½®ä¹‹åRUNå‘½ä»¤å¯ä»¥ä½¿ç”¨è¯¥ç¯å¢ƒå˜é‡ï¼Œåœ¨å®¹å™¨å¯åŠ¨åä¹Ÿå¯ä»¥é€šè¿‡docker inspectæŸ¥çœ‹ç¯å¢ƒå˜é‡æˆ–è€…é€šè¿‡ docker run --env key=valueè®¾ç½®æˆ–ä¿®æ”¹ç¯å¢ƒå˜é‡ã€‚\næ ¼å¼ï¼šENV key value\nä¾‹å­ï¼šENV JAVA_HOME /path/to/java/dirent\n2.9. ARGï¼ˆç”¨äºè®¾ç½®å˜é‡ï¼‰[æ„å»ºæŒ‡ä»¤] ARGå®šä¹‰ä¸€ä¸ªé»˜è®¤å‚æ•°ï¼Œå¯ä»¥åœ¨dockerfileä¸­å¼•ç”¨ã€‚æ„å»ºé˜¶æ®µå¯ä»¥é€šè¿‡docker build --build-arg =å‚æ•°å‘dockerfileæ–‡ä»¶ä¸­ä¼ å…¥å‚æ•°ã€‚\nARG \u003carg_name\u003e[=\u003cdefault value\u003e] # å¯ä»¥æ­é…ENVä½¿ç”¨ ENV env_name ${arg_name} ç¤ºä¾‹ï¼š\ndocker build --build-arg user=what_user . 2.10. ADDï¼ˆä»srcå¤åˆ¶æ–‡ä»¶åˆ°containerçš„destè·¯å¾„ï¼‰[æ„å»ºæŒ‡ä»¤] å¤åˆ¶æŒ‡å®šçš„srcåˆ°å®¹å™¨ä¸­çš„destï¼Œå…¶ä¸­srcæ˜¯ç›¸å¯¹è¢«æ„å»ºçš„æºç›®å½•çš„ç›¸å¯¹è·¯å¾„ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•çš„è·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªè¿œç¨‹çš„æ–‡ä»¶urlã€‚dest æ˜¯containerä¸­çš„ç»å¯¹è·¯å¾„ã€‚æ‰€æœ‰æ‹·è´åˆ°containerä¸­çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹æƒé™ä¸º0755ï¼Œuidå’Œgidä¸º0ã€‚\nå¦‚æœsrcæ˜¯ä¸€ä¸ªç›®å½•ï¼Œé‚£ä¹ˆä¼šå°†è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶æ·»åŠ åˆ°containerä¸­ï¼Œä¸åŒ…æ‹¬ç›®å½•ï¼› å¦‚æœsrcæ–‡ä»¶æ˜¯å¯è¯†åˆ«çš„å‹ç¼©æ ¼å¼ï¼Œåˆ™dockerä¼šå¸®å¿™è§£å‹ç¼©ï¼ˆæ³¨æ„å‹ç¼©æ ¼å¼ï¼‰ï¼› å¦‚æœsrcæ˜¯æ–‡ä»¶ä¸”destä¸­ä¸ä½¿ç”¨æ–œæ ç»“æŸï¼Œåˆ™ä¼šå°†destè§†ä¸ºæ–‡ä»¶ï¼Œsrcçš„å†…å®¹ä¼šå†™å…¥destï¼› å¦‚æœsrcæ˜¯æ–‡ä»¶ä¸”destä¸­ä½¿ç”¨æ–œæ ç»“æŸï¼Œåˆ™ä¼šsrcæ–‡ä»¶æ‹·è´åˆ°destç›®å½•ä¸‹ã€‚ æ ¼å¼ï¼šADD src dest\nä¸ºé¿å… ADDå‘½ä»¤å¸¦æ¥çš„æœªçŸ¥é£é™©å’Œå¤æ‚æ€§ï¼Œå¯ä»¥ä½¿ç”¨COPYå‘½ä»¤æ›¿ä»£ADDå‘½ä»¤\n2.11. COPYï¼ˆå¤åˆ¶æ–‡ä»¶ï¼‰ å¤åˆ¶æœ¬åœ°ä¸»æœºçš„srcä¸ºå®¹å™¨ä¸­çš„destï¼Œç›®æ ‡è·¯å¾„ä¸å­˜åœ¨æ—¶ä¼šè‡ªåŠ¨åˆ›å»ºã€‚\næ ¼å¼ï¼šCOPY src dest\n2.12. VOLUMEï¼ˆæŒ‡å®šæŒ‚è½½ç‚¹ï¼‰[è®¾ç½®æŒ‡ä»¤] åˆ›å»ºä¸€ä¸ªå¯ä»¥ä»æœ¬åœ°ä¸»æœºæˆ–å…¶ä»–å®¹å™¨æŒ‚è½½çš„æŒ‚è½½ç‚¹ï¼Œä½¿å®¹å™¨ä¸­çš„ä¸€ä¸ªç›®å½•å…·æœ‰æŒä¹…åŒ–å­˜å‚¨æ•°æ®çš„åŠŸèƒ½ï¼Œè¯¥ç›®å½•å¯ä»¥è¢«å®¹å™¨æœ¬èº«ä½¿ç”¨ä¹Ÿå¯ä»¥è¢«å…¶ä»–å®¹å™¨ä½¿ç”¨ã€‚\næ ¼å¼ï¼šVOLUME [\"mountpoint\"]\nå…¶ä»–å®¹å™¨ä½¿ç”¨å…±äº«æ•°æ®å·ï¼šdocker run -t -i -rm -volumes-from container1 image2 bash [container1ä¸ºç¬¬ä¸€ä¸ªå®¹å™¨çš„IDï¼Œimage2ä¸ºç¬¬äºŒä¸ªå®¹å™¨è¿è¡Œimageçš„åå­—ã€‚]\n2.13. WORKDIRï¼ˆåˆ‡æ¢ç›®å½•ï¼‰[è®¾ç½®æŒ‡ä»¤] ç›¸å½“äºcdå‘½ä»¤ï¼Œå¯ä»¥å¤šæ¬¡åˆ‡æ¢ç›®å½•ï¼Œä¸ºRUN,CMD,ENTRYPOINTé…ç½®å·¥ä½œç›®å½•ã€‚å¯ä»¥ä½¿ç”¨å¤šä¸ªWORKDIRçš„å‘½ä»¤ï¼Œåç»­å‘½ä»¤å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„åˆ™æ˜¯åœ¨ä¸Šä¸€çº§è·¯å¾„çš„åŸºç¡€ä¸Šæ‰§è¡Œ[ç±»ä¼¼cdçš„åŠŸèƒ½]ã€‚\næ ¼å¼ï¼šWORKDIR /path/to/workdir\n2.14. ONBUILDï¼ˆåœ¨å­é•œåƒä¸­æ‰§è¡Œï¼‰ å½“æ‰€åˆ›å»ºçš„é•œåƒä½œä¸ºå…¶ä»–æ–°åˆ›å»ºé•œåƒçš„åŸºç¡€é•œåƒæ—¶æ‰§è¡Œçš„æ“ä½œå‘½ä»¤ï¼Œå³åœ¨åˆ›å»ºæœ¬é•œåƒæ—¶ä¸è¿è¡Œï¼Œå½“ä½œä¸ºåˆ«äººçš„åŸºç¡€é•œåƒæ—¶å†åœ¨æ„å»ºæ—¶è¿è¡Œï¼ˆå¯è®¤ä¸ºåŸºç¡€é•œåƒä¸ºçˆ¶é•œåƒï¼Œè€Œè¯¥å‘½ä»¤å³åœ¨å®ƒçš„å­é•œåƒæ„å»ºæ—¶è¿è¡Œï¼Œç›¸å½“äºåœ¨å­é•œåƒæ„å»ºæ—¶å¤šåŠ äº†ä¸€äº›å‘½ä»¤ï¼‰ã€‚\næ ¼å¼ï¼šONBUILD Dockerfileå…³é”®å­—\n3. dockerfileç¤ºä¾‹ æœ€ä½³å®è·µ\né•œåƒå¯ä»¥åˆ†ä¸ºä¸‰å±‚ï¼šç³»ç»ŸåŸºç¡€é•œåƒã€ä¸šåŠ¡åŸºç¡€é•œåƒã€ä¸šåŠ¡é•œåƒã€‚ å°½é‡å°†ä¸å˜çš„é•œåƒæ“ä½œæ”¾dockerfileå‰é¢ã€‚ ä¸€ç±»RUNå‘½ä»¤æ“ä½œå¯ä»¥é€šè¿‡\\å’Œ\u0026\u0026æ–¹å¼ç»„åˆæˆä¸€æ¡RUNå‘½ä»¤ã€‚ dockerfileå°½é‡æ¸…æ™°ç®€æ´ã€‚ æ–‡ä»¶ç›®å½•\n./ |-- Dockerfile |-- docker-entrypoint.sh |-- dumb-init |-- conf # é…ç½®æ–‡ä»¶è·¯å¾„ | `-- app_conf.py |-- pkg # å®‰è£…åŒ…è·¯å¾„ | `-- install.tar.gz |-- run.sh # å¯åŠ¨è„šæœ¬ dockerfileç¤ºä¾‹\nFROM centos:latest LABEL maintainer=\"xxx@xxx.com\" ARG APP=appname ENV APP ${APP} # copy and install app COPY conf/app_conf.py /usr/local/app/app_conf/app_conf.py COPY pkg/${APP}-*-install.tar.gz /data/${APP}-install.tar.gz RUN mkdir -p /data/${APP} \\ \u0026\u0026 tar -zxvf /data/${APP}-install.tar.gz -C /data/${APP} \\ \u0026\u0026 cd /data/${APP}/${APP}* \\ \u0026\u0026 ./install.sh WORKDIR /usr/local/app/ # init COPY dumb-init /usr/bin/dumb-init COPY docker-entrypoint.sh /docker-entrypoint.sh ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\",\"/docker-entrypoint.sh\"] COPY run.sh /run.sh RUN chmod +x /run.sh CMD [\"/run.sh\"] 4. docker build æŒ‡å®šdockerfileæ–‡ä»¶æ„å»º\né»˜è®¤ä¸æŒ‡å®šdockerfileæ–‡ä»¶åï¼Œåˆ™è¯»å–æŒ‡å®šè·¯å¾„çš„Dockerfile\ndocker build -t \u003cimage_name\u003e -f \u003cdockerfile_name\u003e \u003cdockerfile_path\u003e docker build --help\ndocker build --help Usage:\tdocker build [OPTIONS] PATH | URL | - Build an image from a Dockerfile Options: --add-host list Add a custom host-to-IP mapping (host:ip) --build-arg list Set build-time variables --cache-from strings Images to consider as cache sources --cgroup-parent string Optional parent cgroup for the container --compress Compress the build context using gzip --cpu-period int Limit the CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit the CPU CFS (Completely Fair Scheduler) quota -c, --cpu-shares int CPU shares (relative weight) --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) --disable-content-trust Skip image verification (default true) -f, --file string Name of the Dockerfile (Default is 'PATH/Dockerfile') --force-rm Always remove intermediate containers --iidfile string Write the image ID to the file --isolation string Container isolation technology --label list Set metadata for an image -m, --memory bytes Memory limit --memory-swap bytes Swap limit equal to memory plus swap: '-1' to enable unlimited swap --network string Set the networking mode for the RUN instructions during build (default \"default\") --no-cache Do not use cache when building the image --pull Always attempt to pull a newer version of the image -q, --quiet Suppress the build output and print image ID on success --rm Remove intermediate containers after a successful build (default true) --security-opt strings Security options --shm-size bytes Size of /dev/shm -t, --tag list Name and optionally a tag in the 'name:tag' format --target string Set the target build stage to build. --ulimit ulimit Ulimit options (default []) å‚è€ƒï¼š\nhttps://docs.docker.com/engine/reference/builder/ ","categories":"","description":"","excerpt":"1. Dockerfileçš„è¯´æ˜ dockerfileæŒ‡ä»¤å¿½ç•¥å¤§å°å†™ï¼Œå»ºè®®å¤§å†™ï¼Œ#ä½œä¸ºæ³¨é‡Šï¼Œæ¯è¡Œåªæ”¯æŒä¸€æ¡æŒ‡ä»¤ï¼ŒæŒ‡ä»¤å¯ä»¥å¸¦å¤šä¸ªå‚æ•°ã€‚ â€¦","ref":"/kubernetes-notes/runtime/docker/dockerfile-usage/","tags":["Docker"],"title":"Dockerfileä½¿ç”¨è¯´æ˜"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/calico/","tags":"","title":"Calico"},{"body":"é—®é¢˜ configmapå‡ºç°å¤šè¡Œæ–‡æœ¬æ— æ³•æ­£å¸¸æ˜¾ç¤ºæ¢è¡Œæ ¼å¼ï¼Œè€Œæ˜¯ä»¥\\nè¿æ¥æ–‡æœ¬ï¼ŒæŸ¥çœ‹å’Œç¼–è¾‘æ—¶å¯è¯»æ€§å¾ˆå·®ã€‚\napiVersion: v1 data: config.yaml: \"# log options\\nlog_level: \\\"info\\\"\\nlog_output: \\\"stderr\\\"\\ncert_file: \\\"/etc/webhook/certs/cert.pem\\\"\\nkey_file: \\\"/etc/webhook/certs/key.pem\\\"\\nhttp_listen: \\\":8080\\\"\\nhttps_listen: \\\":8443\\\"\\ningress_publish_service: \\nenable_profiling: true\\nkubernetes:\\n kubeconfig: \\\"\\\"\\n resync_interval: \\\"6h\\\"\\n app_namespaces:\\n \\ - \\\"*\\\"\\n namespace_selector:\\n - \\\"\\\"\\n election_id: \\\"ingress-apisix-leader\\\"\\n \\ ingress_class: \\\"ph-apisix\\\"\\n ingress_version: \\\"networking/v1\\\"\\n watch_endpointslices: false\\n apisix_route_version: \\\"apisix.apache.org/v2beta3\\\"\\n enable_gateway_api: false\\napisix:\\n default_cluster_base_url: http://apisix-admin.apisix.svc.cluster.local:9180/apisix/admin\\n \\ default_cluster_admin_key: \\\"edd1c9f034335f136f87ad84b625c8f1\\\"\\n default_cluster_name: \\\"default\\\"\" kind: ConfigMap è§£å†³æ–¹æ¡ˆ å¦‚æœè¦ä¿æŒå¤šè¡Œè¾“å…¥å’Œè¾“å‡ºçš„æ ¼å¼ï¼Œåˆ™éœ€è¦ç¬¦åˆä»¥ä¸‹æƒ…å†µï¼š\næ–‡æœ¬ä¸è¦ä»¥ç©ºæ ¼ç»“å°¾ ä¸è¦æ¢è¡Œå‰å†å¸¦ä¸ªç©ºæ ¼ ä¸è¦åœ¨æ–‡æœ¬ä¸­æ·»åŠ ä¸å¯è§ç‰¹æ®Šå­—ç¬¦ å°†æ–‡æœ¬æ‹·è´å¹¶æ ¼å¼åŒ–yamlæ–‡æœ¬ã€‚å¯ä½¿ç”¨åœ¨çº¿æ ¼å¼åŒ–å·¥å…·ï¼šYAMLåœ¨çº¿æ ¼å¼åŒ–ã€‚\nå°†æ ¼å¼åŒ–çš„æ–‡æœ¬æ‹·è´åˆ°configmapæ–‡ä»¶ï¼Œå¹¶æ£€æŸ¥ä¸Šè¿°ä¸‰ä¸ªé—®é¢˜ã€‚ä¸€èˆ¬æ˜¯å› ä»¥ç©ºæ ¼ç»“å°¾å¯¼è‡´ï¼Œæœç´¢ç©ºæ ¼å¹¶å»é™¤è¡Œæœ«çš„ç©ºæ ¼ã€‚\napiVersion: v1 data: config.yaml: |- # log options log_level: \"info\" log_output: \"stderr\" cert_file: \"/etc/webhook/certs/cert.pem\" key_file: \"/etc/webhook/certs/key.pem\" http_listen: \":8080\" https_listen: \":8443\" ingress_publish_service: enable_profiling: true å‚è€ƒï¼š\nhttps://kennylong.io/fix-yaml-multi-line-format/ ","categories":"","description":"","excerpt":"é—®é¢˜ configmapå‡ºç°å¤šè¡Œæ–‡æœ¬æ— æ³•æ­£å¸¸æ˜¾ç¤ºæ¢è¡Œæ ¼å¼ï¼Œè€Œæ˜¯ä»¥\\nè¿æ¥æ–‡æœ¬ï¼ŒæŸ¥çœ‹å’Œç¼–è¾‘æ—¶å¯è¯»æ€§å¾ˆå·®ã€‚\napiVersion: v1 â€¦","ref":"/kubernetes-notes/trouble-shooting/configmap-yaml-format/","tags":["é—®é¢˜æ’æŸ¥"],"title":"ConfigMapå¤šè¡Œæ ¼å¼"},{"body":"Dynamic Volume Provisioning Dynamic volume provisioningå…è®¸ç”¨æˆ·æŒ‰éœ€è‡ªåŠ¨åˆ›å»ºå­˜å‚¨å·ï¼Œè¿™ç§æ–¹å¼å¯ä»¥è®©ç”¨æˆ·ä¸éœ€è¦å…³å¿ƒå­˜å‚¨çš„å¤æ‚æ€§å’Œå·®åˆ«ï¼Œåˆå¯ä»¥é€‰æ‹©ä¸åŒçš„å­˜å‚¨ç±»å‹ã€‚\n1. å¼€å¯Dynamic Provisioning éœ€è¦å…ˆæå‰åˆ›å»ºStorageClasså¯¹è±¡ï¼ŒStorageClassä¸­å®šä¹‰äº†ä½¿ç”¨å“ªä¸ªprovisionerï¼Œå¹¶ä¸”åœ¨provisionerè¢«è°ƒç”¨æ—¶ä¼ å…¥å“ªäº›å‚æ•°ï¼Œå…·ä½“å¯å‚è€ƒStorageClassä»‹ç»ã€‚\nä¾‹å¦‚ï¼š\nç£ç›˜ç±»å­˜å‚¨ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/gce-pd parameters: type: pd-standard SSDç±»å­˜å‚¨ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd 2. ä½¿ç”¨Dynamic Provisioning åˆ›å»ºä¸€ä¸ªPVCå¯¹è±¡ï¼Œå¹¶ä¸”åœ¨å…¶ä¸­storageClassNameå­—æ®µæŒ‡æ˜éœ€è¦ç”¨åˆ°çš„StorageClassçš„åç§°ï¼Œä¾‹å¦‚ï¼š\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: fast resources: requests: storage: 30Gi å½“ä½¿ç”¨åˆ°PVCçš„æ—¶å€™ä¼šè‡ªåŠ¨åˆ›å»ºå¯¹åº”çš„å¤–éƒ¨å­˜å‚¨ï¼Œå½“PVCè¢«åˆ é™¤çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨é”€æ¯ï¼ˆæˆ–å¤‡ä»½ï¼‰å¤–éƒ¨å­˜å‚¨ã€‚\n3. é»˜è®¤çš„StorageClass å½“æ²¡æœ‰å¯¹åº”çš„StorageClassé…ç½®æ—¶ï¼Œå¯ä»¥è®¾å®šé»˜è®¤çš„StorageClassï¼Œéœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\nåœ¨API Serverå¼€å¯DefaultStorageClassÂ admission controllerÂ ã€‚ è®¾ç½®é»˜è®¤çš„StorageClasså¯¹è±¡ã€‚ å¯ä»¥é€šè¿‡æ·»åŠ storageclass.kubernetes.io/is-default-classæ³¨è§£çš„æ–¹å¼è®¾ç½®æŸä¸ªStorageClassä¸ºé»˜è®¤çš„StorageClassã€‚å½“ç”¨æˆ·åˆ›å»ºäº†ä¸€ä¸ªPersistentVolumeClaimï¼Œä½†æ²¡æœ‰æŒ‡å®šstorageClassNameçš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨å°†è¯¥PVCçš„storageClassNameæŒ‡å‘é»˜è®¤çš„StorageClassã€‚\nå‚è€ƒæ–‡ç« ï¼š\nhttps://kubernetes.io/docs/concepts/storage/dynamic-provisioning/ ","categories":"","description":"","excerpt":"Dynamic Volume Provisioning Dynamic volume provisioningå…è®¸ç”¨æˆ·æŒ‰éœ€è‡ªåŠ¨åˆ›å»ºå­˜å‚¨å·ï¼Œè¿™ â€¦","ref":"/kubernetes-notes/storage/volume/dynamic-provisioning/","tags":["Kubernetes"],"title":"Dynamic Volume Provisioning ä»‹ç»"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/gpu/","tags":"","title":"GPU"},{"body":"é—®é¢˜æè¿° å®¹å™¨å¯åŠ¨æŠ¥é”™ï¼šincrease the mlock limitï¼ŒåŸå› æ˜¯ulimit mlockå€¼æ¯”è¾ƒå°ï¼Œéœ€è¦å°†ulimitå€¼è°ƒå¤§ã€‚\næŠ¥é”™å¦‚ä¸‹ï¼š\nruntime: mlock of signal stack failed: 12 runtime: increase the mlock limit (ulimit -l) or runtime: update your kernel to 5.3.15+, 5.4.2+, or 5.5+ fatal error: mlock failed runtime stack: runtime.throw(0x1a7729f, 0xc) /usr/local/go/src/runtime/panic.go:1112 +0x72 runtime.mlockGsignal(0xc000702300) /usr/local/go/src/runtime/os_linux_x86.go:72 +0x107 runtime.mpreinit(0xc000588380) /usr/local/go/src/runtime/os_linux.go:341 +0x78 runtime.mcommoninit(0xc000588380) /usr/local/go/src/runtime/proc.go:630 +0x108 runtime.allocm(0xc000072000, 0x1adcb70, 0x0) /usr/local/go/src/runtime/proc.go:1390 +0x14e runtime.newm(0x1adcb70, 0xc000072000) /usr/local/go/src/runtime/proc.go:1704 +0x39 runtime.startm(0x0, 0xc000267e01) /usr/local/go/src/runtime/proc.go:1869 +0x12a runtime.wakep(...) /usr/local/go/src/runtime/proc.go:1953 runtime.resetspinning() /usr/local/go/src/runtime/proc.go:2415 +0x93 runtime.schedule() /usr/local/go/src/runtime/proc.go:2527 +0x2de runtime.mstart1() /usr/local/go/src/runtime/proc.go:1104 +0x8e runtime.mstart() /usr/local/go/src/runtime/proc.go:1062 +0x6e goroutine 1 [runnable, locked to thread]: github.com/xdg/stringprep.init() /root/go/pkg/mod/github.com/xdg/stringprep@v1.0.3/tables.go:443 +0x19087 goroutine 43 [select]: go.opencensus.io/stats/view.(*worker).start(0xc00067e800) /root/go/pkg/mod/go.opencensus.io@v0.23.0/stats/view/worker.go:276 +0x100 created by go.opencensus.io/stats/view.init.0 /root/go/pkg/mod/go.opencensus.io@v0.23.0/stats/view/worker.go:34 +0x68 åŸå›  å®¿ä¸»æœºçš„ulimitå€¼æ¯”è¾ƒå°ï¼Œéœ€è¦å°†å†…å­˜çš„ulimitå€¼è°ƒå¤§ã€‚\n$ ulimit -l 64 è§£å†³æ–¹æ¡ˆ vi /lib/systemd/system/containerd.serviceã€‚åœ¨containerd.serviceæ–‡ä»¶ä¸­å¢åŠ LimitMEMLOCK=infinity å‚æ•°ã€‚\n[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitMEMLOCK=infinity LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target é‡å¯containerd\nsystemctl daemon-reload systemctl restart containerd systemctl status containerd ","categories":"","description":"","excerpt":"é—®é¢˜æè¿° å®¹å™¨å¯åŠ¨æŠ¥é”™ï¼šincrease the mlock limitï¼ŒåŸå› æ˜¯ulimit mlockå€¼æ¯”è¾ƒå°ï¼Œéœ€è¦å°†ulimitå€¼è°ƒå¤§ã€‚ â€¦","ref":"/kubernetes-notes/trouble-shooting/node/increase-the-mlock-limit/","tags":["é—®é¢˜æ’æŸ¥"],"title":"increase the mlock limit"},{"body":"1. k8sç‰ˆæœ¬å·è¯´æ˜ k8sç»´æŠ¤æœ€æ–°ä¸‰ä¸ªç‰ˆæœ¬çš„å‘å¸ƒåˆ†æ”¯ï¼ˆ[2022.7.2]å½“å‰æœ€æ–°ä¸‰ä¸ªç‰ˆæœ¬ä¸º1.24ã€1.23ã€1.22ï¼‰ï¼ŒKubernetes 1.19 å’Œæ›´æ–°çš„ç‰ˆæœ¬è·å¾—å¤§çº¦ 1 å¹´çš„è¡¥ä¸æ”¯æŒã€‚\nKubernetes ç‰ˆæœ¬è¡¨ç¤ºä¸ºÂ x.y.zï¼Œ å…¶ä¸­Â xÂ æ˜¯ä¸»è¦ç‰ˆæœ¬ï¼ŒyÂ æ˜¯æ¬¡è¦ç‰ˆæœ¬ï¼ŒzÂ æ˜¯è¡¥ä¸ç‰ˆæœ¬ã€‚éµå¾ªè¯­ä¹‰åŒ–ç‰ˆæœ¬è§„èŒƒã€‚\n2. ç‰ˆæœ¬åå·®ç­–ç•¥ 2.1. æ”¯æŒçš„ç‰ˆæœ¬åå·® æ€»ç»“ï¼š\nkubeletÂ ç‰ˆæœ¬ä¸èƒ½æ¯”Â kube-apiserverÂ ç‰ˆæœ¬æ–°ï¼Œæœ€å¤šåªå¯è½åä¸¤ä¸ªæ¬¡è¦ç‰ˆæœ¬ã€‚\nkube-controller-managerã€kube-schedulerÂ å’ŒÂ cloud-controller-managerÂ ä¸èƒ½æ¯”Â kube-apiserverÂ ç‰ˆæœ¬æ–°ã€‚æœ€å¤šè½åä¸€ä¸ªæ¬¡è¦ç‰ˆæœ¬ï¼ˆå…è®¸å®æ—¶å‡çº§ï¼‰ã€‚\nkubectlÂ åœ¨Â kube-apiserverÂ çš„ä¸€ä¸ªæ¬¡è¦ç‰ˆæœ¬ï¼ˆè¾ƒæ—§æˆ–è¾ƒæ–°ï¼‰ä¸­æ”¯æŒã€‚\nkube-proxyÂ å’ŒèŠ‚ç‚¹ä¸Šçš„Â kubeletÂ å¿…é¡»æ˜¯ç›¸åŒçš„æ¬¡è¦ç‰ˆæœ¬ã€‚\n1ï¼‰kube-apiserver åœ¨é«˜å¯ç”¨æ€§ï¼ˆHAï¼‰é›†ç¾¤ä¸­ï¼Œ æœ€æ–°ç‰ˆå’Œæœ€è€ç‰ˆçš„Â kube-apiserverÂ å®ä¾‹ç‰ˆæœ¬åå·®æœ€å¤šä¸ºä¸€ä¸ªæ¬¡è¦ç‰ˆæœ¬ã€‚\nä¾‹å¦‚ï¼š\næœ€æ–°çš„Â kube-apiserverÂ å®ä¾‹å¤„äºÂ 1.24Â ç‰ˆæœ¬ å…¶ä»–Â kube-apiserverÂ å®ä¾‹æ”¯æŒÂ 1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ 2ï¼‰kubelet kubeletÂ ç‰ˆæœ¬ä¸èƒ½æ¯”Â kube-apiserverÂ ç‰ˆæœ¬æ–°ï¼Œå¹¶ä¸”æœ€å¤šåªå¯è½åä¸¤ä¸ªæ¬¡è¦ç‰ˆæœ¬ã€‚\nä¾‹å¦‚ï¼š\nkube-apiserverÂ å¤„äºÂ 1.24Â ç‰ˆæœ¬ kubeletÂ æ”¯æŒÂ 1.24ã€1.23Â å’ŒÂ 1.22Â ç‰ˆæœ¬ è¯´æ˜ï¼š\nå¦‚æœ HA é›†ç¾¤ä¸­çš„Â kube-apiserverÂ å®ä¾‹ä¹‹é—´å­˜åœ¨ç‰ˆæœ¬åå·®ï¼Œè¿™ä¼šç¼©å°å…è®¸çš„Â kubeletÂ ç‰ˆæœ¬èŒƒå›´ã€‚\nä¾‹å¦‚ï¼š\nkube-apiserverÂ å®ä¾‹å¤„äºÂ 1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ kubeletÂ æ”¯æŒÂ 1.23Â å’ŒÂ 1.22Â ç‰ˆæœ¬ï¼Œ ï¼ˆä¸æ”¯æŒÂ 1.24Â ç‰ˆæœ¬ï¼Œå› ä¸ºè¿™å°†æ¯”Â kube-apiserverÂ 1.23Â ç‰ˆæœ¬çš„å®ä¾‹æ–°ï¼‰ 3ï¼‰kube-controller-managerã€kube-scheduler å’Œ cloud-controller-manager kube-controller-managerã€kube-schedulerÂ å’ŒÂ cloud-controller-managerÂ ä¸èƒ½æ¯”ä¸å®ƒä»¬é€šä¿¡çš„Â kube-apiserverÂ å®ä¾‹æ–°ã€‚ å®ƒä»¬åº”è¯¥ä¸Â kube-apiserverÂ æ¬¡è¦ç‰ˆæœ¬ç›¸åŒ¹é…ï¼Œä½†å¯èƒ½æœ€å¤šæ—§ä¸€ä¸ªæ¬¡è¦ç‰ˆæœ¬ï¼ˆå…è®¸å®æ—¶å‡çº§ï¼‰ã€‚\nä¾‹å¦‚ï¼š\nkube-apiserverÂ å¤„äºÂ 1.24Â ç‰ˆæœ¬ kube-controller-managerã€kube-schedulerÂ å’ŒÂ cloud-controller-managerÂ æ”¯æŒÂ 1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ è¯´æ˜ï¼š\nå¦‚æœ HA é›†ç¾¤ä¸­çš„Â kube-apiserverÂ å®ä¾‹ä¹‹é—´å­˜åœ¨ç‰ˆæœ¬åå·®ï¼Œ å¹¶ä¸”è¿™äº›ç»„ä»¶å¯ä»¥ä¸é›†ç¾¤ä¸­çš„ä»»ä½•Â kube-apiserverÂ å®ä¾‹é€šä¿¡ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡è´Ÿè½½å‡è¡¡å™¨ï¼‰ï¼Œè¿™ä¼šç¼©å°è¿™äº›ç»„ä»¶æ‰€å…è®¸çš„ç‰ˆæœ¬èŒƒå›´ã€‚\nä¾‹å¦‚ï¼š\nkube-apiserverÂ å®ä¾‹å¤„äºÂ 1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ kube-controller-managerã€kube-schedulerÂ å’ŒÂ cloud-controller-managerÂ ä¸å¯ä»¥è·¯ç”±åˆ°ä»»ä½•Â kube-apiserverÂ å®ä¾‹çš„è´Ÿè½½å‡è¡¡å™¨é€šä¿¡ kube-controller-managerã€kube-schedulerÂ å’ŒÂ cloud-controller-managerÂ æ”¯æŒÂ 1.23Â ç‰ˆæœ¬ï¼ˆä¸æ”¯æŒÂ 1.24Â ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒæ¯”Â 1.23Â ç‰ˆæœ¬çš„Â kube-apiserverÂ å®ä¾‹æ–°ï¼‰ 4ï¼‰kubectl kubectlÂ åœ¨Â kube-apiserverÂ çš„ä¸€ä¸ªæ¬¡è¦ç‰ˆæœ¬ï¼ˆè¾ƒæ—§æˆ–è¾ƒæ–°ï¼‰ä¸­æ”¯æŒã€‚\nä¾‹å¦‚ï¼š\nkube-apiserverÂ å¤„äºÂ 1.24Â ç‰ˆæœ¬ kubectlÂ æ”¯æŒÂ 1.25ã€1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ è¯´æ˜ï¼š\nå¦‚æœ HA é›†ç¾¤ä¸­çš„Â kube-apiserverÂ å®ä¾‹ä¹‹é—´å­˜åœ¨ç‰ˆæœ¬åå·®ï¼Œè¿™ä¼šç¼©å°æ”¯æŒçš„Â kubectlÂ ç‰ˆæœ¬èŒƒå›´ã€‚\nä¾‹å¦‚ï¼š\nkube-apiserverÂ å®ä¾‹å¤„äºÂ 1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ kubectlÂ æ”¯æŒÂ 1.24Â å’ŒÂ 1.23Â ç‰ˆæœ¬ï¼ˆå…¶ä»–ç‰ˆæœ¬å°†ä¸Â kube-apiserverÂ ç»„ä»¶ä¹‹ä¸€ç›¸å·®ä¸æ­¢ä¸€ä¸ªçš„æ¬¡è¦ç‰ˆæœ¬ï¼‰ 5ï¼‰kube-proxy kube-proxyÂ å’ŒèŠ‚ç‚¹ä¸Šçš„Â kubeletÂ å¿…é¡»æ˜¯ç›¸åŒçš„æ¬¡è¦ç‰ˆæœ¬ã€‚ kube-proxyÂ ç‰ˆæœ¬ä¸èƒ½æ¯”Â kube-apiserverÂ ç‰ˆæœ¬æ–°ã€‚ kube-proxyÂ æœ€å¤šåªèƒ½æ¯”Â kube-apiserverÂ è½åä¸¤ä¸ªæ¬¡è¦ç‰ˆæœ¬ã€‚ ä¾‹å¦‚ï¼š\nå¦‚æœÂ kube-proxyÂ ç‰ˆæœ¬å¤„äºÂ 1.22Â ç‰ˆæœ¬ï¼š\nkubeletÂ å¿…é¡»å¤„äºç›¸åŒçš„æ¬¡è¦ç‰ˆæœ¬Â 1.22ã€‚ kube-apiserverÂ ç‰ˆæœ¬å¿…é¡»ä»‹äºÂ 1.22Â å’ŒÂ 1.24Â ä¹‹é—´ï¼ŒåŒ…æ‹¬ä¸¤è€…ã€‚ 2.2. ç»„ä»¶å‡çº§é¡ºåº ä¼˜å…ˆå‡çº§kube-apiserverï¼Œå…¶ä»–çš„ç»„ä»¶æŒ‰ç…§ä¸Šè¿°çš„ç‰ˆæœ¬è¦æ±‚è¿›è¡Œå‡çº§ï¼Œæœ€å¥½ä¿æŒä¸€è‡´çš„ç‰ˆæœ¬ã€‚\n3. k8sç‰ˆæœ¬å‘å¸ƒå‘¨æœŸ k8sæ¯å¹´å¤§æ¦‚å‘å¸ƒä¸‰æ¬¡ï¼Œå³3-4ä¸ªæœˆå‘å¸ƒä¸€æ¬¡å¤§ç‰ˆæœ¬ï¼ˆå‘å¸ƒç‰ˆæœ¬ä¸ºÂ vX.YÂ é‡Œç¨‹ç¢‘åˆ›å»ºçš„ Git åˆ†æ”¯Â release-X.Yï¼‰ã€‚\nå‘å¸ƒè¿‡ç¨‹å¯è¢«è®¤ä¸ºå…·æœ‰ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š\nç‰¹æ€§å¢å¼ºå®šä¹‰ å®ç° ç¨³å®š 3.1. å‘å¸ƒå‘¨æœŸ 1ï¼‰æ­£å¸¸å¼€å‘ï¼ˆç¬¬ 1-11 å‘¨ï¼‰\n/sig {name}\n/sig {name}\n/kind {type}\n/lgtm\n/approved\n2ï¼‰ä»£ç å†»ç»“ï¼ˆç¬¬ 12-14 å‘¨ï¼‰\n/milestone {v1.y} /sig {name} /kind {bug, failing-test} /lgtm /approved 3ï¼‰å‘å¸ƒåï¼ˆç¬¬ 14 å‘¨ä»¥ä¸Šï¼‰\nå›åˆ°â€œæ­£å¸¸å¼€å‘â€é˜¶æ®µè¦æ±‚ï¼š\n/sig {name} /kind {type} /lgtm /approved å‚è€ƒï¼š å‘è¡Œç‰ˆæœ¬ | Kubernetes\nkubernetes/CHANGELOG at master Â· GitHub\nsig-release/releases at master Â· kubernetes/sig-release Â· GitHub\ndesign-proposals-archive/versioning.md at main Â· kubernetes/design-proposals-archive Â· GitHub\nKubernetes å‘å¸ƒå‘¨æœŸ | Kubernetes\n","categories":"","description":"","excerpt":"1. k8sç‰ˆæœ¬å·è¯´æ˜ k8sç»´æŠ¤æœ€æ–°ä¸‰ä¸ªç‰ˆæœ¬çš„å‘å¸ƒåˆ†æ”¯ï¼ˆ[2022.7.2]å½“å‰æœ€æ–°ä¸‰ä¸ªç‰ˆæœ¬ â€¦","ref":"/kubernetes-notes/setup/k8s-version-release/","tags":["Kubernetes"],"title":"k8sç‰ˆæœ¬è¯´æ˜"},{"body":"","categories":"","description":"","excerpt":"","ref":"/k8s-source-code-analysis/kubelet/","tags":"","title":"kubelet"},{"body":"åˆ›å»ºç”¨æˆ·åå’Œå¯†ç  # é€šè¿‡å†™æ–‡ä»¶åˆ›å»ºç”¨æˆ·ï¼Œ/home/mybotæ˜¯ç”¨æˆ·ç›®å½• echo \"mybot:x:1001:1001::/home/mybot:/bin/bash\" \u003e\u003e /etc/passwd # ç»™ç”¨æˆ·åˆ›å»ºå¯†ç  echo \"mybot:{password}\" | chpasswd # æŒ‡å®šæ–‡ä»¶ç³»ç»Ÿæ ¹ç›®å½•åˆ›å»ºå¯†ç ï¼Œä¾‹å¦‚ï¼š/mnt/dev/sdaï¼Œä¸€èˆ¬ä¸ºå¤–æŒ‚æ–‡ä»¶ç³»ç»Ÿï¼Œå¹¶æ²¡æœ‰chroot echo \"mybot:{password}\" | chpasswd -R /mnt/dev/sda # ç»™ç”¨æˆ·åˆ†é…sudoæƒé™ï¼ŒNOPASSWDè¡¨ç¤ºä¸éœ€è¦rootå¯†ç æ‰§è¡Œsudo echo \"mybot ALL=(root) NOPASSWD: ALL\" \u003e /etc/sudoers.d/mybot # è®¾ç½®å…è®¸sshç”¨æˆ·å¯†ç ç™»å½• sudo sed -i 's/^#\\?PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config # è®¾ç½®ä¸å…è®¸sshå¯†ç ç™»å½• sudo sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config # ä¸é‡å¯sshæœåŠ¡ä½¿å¾—é…ç½®ä¿®æ”¹ç”Ÿæ•ˆ sudo systemctl reload sshd ","categories":"","description":"","excerpt":"åˆ›å»ºç”¨æˆ·åå’Œå¯†ç  # é€šè¿‡å†™æ–‡ä»¶åˆ›å»ºç”¨æˆ·ï¼Œ/home/mybotæ˜¯ç”¨æˆ·ç›®å½• echo â€¦","ref":"/linux-notes/file/linux-command/","tags":["Linux"],"title":"Linuxå¸¸ç”¨å‘½ä»¤"},{"body":" æœ¬æ–‡æ¥è‡ªredis å®˜æ–¹é…ç½®æ–‡ä»¶\n# Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k =\u003e 1000 bytes # 1kb =\u003e 1024 bytes # 1m =\u003e 1000000 bytes # 1mb =\u003e 1024*1024 bytes # 1g =\u003e 1000000000 bytes # 1gb =\u003e 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. INCLUDES ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\" # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf MODULES ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so NETWORK ################################## NETWORK ##################################### # By default, if no \"bind\" configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \"bind\" configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 lookback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \"bind\" directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \"bind\" directive. protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 GENERAL ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize yes # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \"process is ready.\" # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \"/var/run/redis.pid\". # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \"\" # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT \u003cdbid\u003e where # dbid is a number between 0 and 'databases'-1 databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes SNAPSHOTTING ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save \u003cseconds\u003e \u003cchanges\u003e # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \"save\" lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \"\" save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that's set to 'yes' as it's almost always a win. # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./ REPLICATION ################################# REPLICATION ################################# # Master-Slave replication. Use slaveof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of slaves. # 2) Redis slaves are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition slaves automatically try to reconnect to masters # and resynchronize with them. # # slaveof \u003cmasterip\u003e \u003cmasterport\u003e # If the master is password protected (using the \"requirepass\" configuration # directive below) it is possible to tell the slave to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the slave request. # # masterauth \u003cmaster-password\u003e # When a slave loses its connection with the master, or when the replication # is still in progress, the slave can act in two different ways: # # 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if slave-serve-stale-data is set to 'no' the slave will reply with # an error \"SYNC with master in progress\" to all the kind of commands # but to INFO and SLAVEOF. # slave-serve-stale-data yes # You can configure a slave instance to accept writes or not. Writing against # a slave instance may be useful to store some ephemeral data (because data # written on a slave will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default slaves are read-only. # # Note: read only slaves are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only slave exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only slaves using 'rename-command' to shadow all the # administrative / dangerous commands. slave-read-only yes # Replication SYNC strategy: disk or socket. # # ------------------------------------------------------- # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY # ------------------------------------------------------- # # New slaves and reconnecting slaves that are not able to continue the replication # process just receiving differences, need to do what is called a \"full # synchronization\". An RDB file is transmitted from the master to the slaves. # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the slaves incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to slave sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more slaves # can be queued and served with the RDB file as soon as the current child producing # the RDB file finishes its work. With diskless replication instead once # the transfer starts, new slaves arriving will be queued and a new transfer # will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple slaves # will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the slaves. # # This is important since once the transfer starts, it is not possible to serve # new slaves arriving, that will be queued for the next RDB transfer, so the server # waits a delay in order to let more slaves arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # Slaves send PINGs to server in a predefined interval. It's possible to change # this interval with the repl_ping_slave_period option. The default value is 10 # seconds. # # repl-ping-slave-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of slave. # 2) Master timeout from the point of view of slaves (data, pings). # 3) Slave timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-slave-period otherwise a timeout will be detected # every time there is low traffic between the master and the slave. # # repl-timeout 60 # Disable TCP_NODELAY on the slave socket after SYNC? # # If you select \"yes\" Redis will use a smaller number of TCP packets and # less bandwidth to send data to slaves. But this can add a delay for # the data to appear on the slave side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \"no\" the delay for data to appear on the slave side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and slaves are many hops away, turning this to \"yes\" may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # slave data when slaves are disconnected for some time, so that when a slave # wants to reconnect again, often a full resync is not needed, but a partial # resync is enough, just passing the portion of data the slave missed while # disconnected. # # The bigger the replication backlog, the longer the time the slave can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a slave connected. # # repl-backlog-size 1mb # After a master has no longer connected slaves for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last slave disconnected, for # the backlog buffer to be freed. # # Note that slaves never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \"partially # resynchronize\" with the slaves: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The slave priority is an integer number published by Redis in the INFO output. # It is used by Redis Sentinel in order to select a slave to promote into a # master if the master is no longer working correctly. # # A slave with a low priority number is considered better for promotion, so # for instance if there are three slaves with priority 10, 100, 25 Sentinel will # pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the slave as not able to perform the # role of master, so a slave with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. slave-priority 100 # It is possible for a master to stop accepting writes if there are less than # N slaves connected, having a lag less or equal than M seconds. # # The N slaves need to be in \"online\" state. # # The lag in seconds, that must be \u003c= the specified value, is calculated from # the last ping received from the slave, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough slaves # are available, to the specified number of seconds. # # For example to require at least 3 slaves with a lag \u003c= 10 seconds use: # # min-slaves-to-write 3 # min-slaves-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-slaves-to-write is set to 0 (feature disabled) and # min-slaves-max-lag is set to 10. # A Redis master is able to list the address and port of the attached # slaves in different ways. For example the \"INFO replication\" section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover slave instances. # Another place where this info is available is in the output of the # \"ROLE\" command of a master. # # The listed IP and address normally reported by a slave is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the slave to connect with the master. # # Port: The port is communicated by the slave during the replication # handshake, and is normally the port that the slave is using to # list for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the slave may be actually reachable via different IP and port # pairs. The following two options can be used by a slave in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # slave-announce-ip 5.5.5.5 # slave-announce-port 1234 SECURITY ################################## SECURITY ################################### # Require clients to issue AUTH \u003cPASSWORD\u003e before processing any other # commands. This might be useful in environments in which you do not trust # others with access to the host running redis-server. # # This should stay commented out for backward compatibility and because most # people do not need auth (e.g. they run their own servers). # # Warning: since Redis is pretty fast an outside user can try up to # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. # # requirepass foobared # Command renaming. # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \"\" # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to slaves may cause problems. CLIENTS ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # # maxclients 10000 MEMORY MANAGEMENT ############################## MEMORY MANAGEMENT ################################ # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have slaves attached to an instance with maxmemory on, # the size of the output buffers needed to feed the slaves are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of slaves is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have slaves attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for slave # output buffers (but this is not needed if the policy is 'noeviction'). # # maxmemory \u003cbytes\u003e # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select among five behaviors: # # volatile-lru -\u003e Evict using approximated LRU among the keys with an expire set. # allkeys-lru -\u003e Evict any key using approximated LRU. # volatile-lfu -\u003e Evict using approximated LFU among the keys with an expire set. # allkeys-lfu -\u003e Evict any key using approximated LFU. # volatile-random -\u003e Remove a random key among the ones with an expire set. # allkeys-random -\u003e Remove a random key, any key. # volatile-ttl -\u003e Remove the key with the nearest expire time (minor TTL) # noeviction -\u003e Don't evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5 LAZY FREEING ############################# LAZY FREEING #################################### # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 4) During replication, when a slave performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transfered. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives: lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no slave-lazy-flush no APPEND ONLY MODE ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \"appendonly.aof\") appendfilename \"appendonly.aof\" # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don't fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \"everysec\", as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # \"no\" that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use \"always\" that's very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \"everysec\". # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \"appendfsync none\". In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \"yes\". Otherwise leave it as # \"no\" that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \"redis-check-aof\" utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \"REDIS\" # string and loads the prefixed RDB file, and continues loading the AOF # tail. # # This is currently turned off by default in order to avoid the surprise # of a format change, but will at some point be used as the default. aof-use-rdb-preamble no LUA SCRIPTING ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 REDIS CLUSTER ################################ REDIS CLUSTER ############################### # # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however # in order to mark it as \"mature\" we need to wait for a non trivial percentage # of users to deploy it in production. # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A slave of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a slave to actually have an exact measure of # its \"data age\", so the following two checks are performed: # # 1) If there are multiple slaves able to failover, they exchange messages # in order to try to give an advantage to the slave with the best # replication offset (more data from the master processed). # Slaves will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single slave computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \"connected\" state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the slave will not try to failover # at all. # # The point \"2\" can be tuned by user. Specifically a slave will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * slave-validity-factor) + repl-ping-slave-period # # So for example if node-timeout is 30 seconds, and the slave-validity-factor # is 10, and assuming a default repl-ping-slave-period of 10 seconds, the # slave will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large slave-validity-factor may allow slaves with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a slave at all. # # For maximum availability, it is possible to set the slave-validity-factor # to a value of 0, which means, that slaves will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-slave-validity-factor 10 # Cluster slaves are able to migrate to orphaned masters, that are masters # that are left without working slaves. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working slaves. # # Slaves migrate to orphaned masters only if there are still at least a # given number of other working slaves for their old master. This number # is the \"migration barrier\". A migration barrier of 1 means that a slave # will migrate only if there is at least 1 other working slave for its master # and so forth. It usually reflects the number of slaves you want for every # master in your cluster. # # Default is 1 (slaves migrate only if their masters remain with at least # one slave). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. CLUSTER DOCKER/NAT support ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 SLOW LOG ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 LATENCY MONITOR ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \"CONFIG SET latency-monitor-threshold \u003cmilliseconds\u003e\" if needed. latency-monitor-threshold 0 EVENT NOTIFICATION ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \"foo\" stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@\u003cdb\u003e__ prefix. # E Keyevent events, published with __keyevent@\u003cdb\u003e__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # A Alias for g$lshzxe, so that the \"AKE\" string means all the events. # # The \"notify-keyspace-events\" takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don't need # this feature and the feature has some overhead. Note that if you don't # specify at least one of K or E, no events will be delivered. notify-keyspace-events \"\" ADVANCED CONFIG ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb \u003c-- not recommended for normal workloads # -4: max size: 32 Kb \u003c-- not recommended # -3: max size: 16 Kb \u003c-- probably not recommended # -2: max size: 8 Kb \u003c-- good # -1: max size: 4 Kb \u003c-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \"don't start compressing until after 1 node into the list, # going from either the head or tail\" # So: [head]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]-\u003e[next]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[prev]-\u003e[tail] # 2 here means: don't compress head or head-\u003enext or tail-\u003eprev or tail, # but compress all nodes between them. # 3: [head]-\u003e[next]-\u003e[next]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[prev]-\u003e[prev]-\u003e[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \"steps\" are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \"activerehashing no\" if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \"activerehashing yes\" if you don't have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -\u003e normal clients including MONITOR clients # slave -\u003e slave clients # pubsub -\u003e clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit \u003cclass\u003e \u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and slave clients, since # subscribers and slaves receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here. # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \"hz\" value. # # By default \"hz\" is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R \u003c P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits | # +--------+------------+------------+------------+------------+------------+ # | 0 | 104 | 255 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 1 | 18 | 49 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 10 | 10 | 18 | 142 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 100 | 8 | 11 | 49 | 143 | 255 | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # # redis-benchmark -n 1000000 incr foo # redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less \u003c= 10). # # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # # lfu-log-factor 10 # lfu-decay-time 1 ACTIVE DEFRAGMENTATION ########################### ACTIVE DEFRAGMENTATION ####################### # # WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested # even in production and manually tested by multiple engineers for some # time. # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \"hot\" way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \"CONFIG SET activedefrag yes\". # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag yes # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage # active-defrag-cycle-min 25 # Maximal effort for defrag in CPU percentage # active-defrag-cycle-max 75 ","categories":"","description":"","excerpt":" æœ¬æ–‡æ¥è‡ªredis å®˜æ–¹é…ç½®æ–‡ä»¶\n# Redis configuration file example. # # Note that in â€¦","ref":"/linux-notes/redis/redis-conf-en/","tags":["Redis"],"title":"Redisé…ç½®è¯¦è§£ï¼ˆè‹±æ–‡ç‰ˆï¼‰"},{"body":"æœ¬æ–‡ä¸»è¦åˆ†æreplicaset-controllerçš„æºç é€»è¾‘ï¼Œreplicaså¯¹è±¡åˆ›å»ºä¸»è¦æ˜¯ç”±deployment-controllerä¸­å°è£…ã€‚è€Œreplicasæ˜¯podçš„ç»´æŠ¤æ§åˆ¶å™¨ã€‚å¯ä»¥æŠŠreplicasç†è§£ä¸ºdeploymentä¸­çš„ç‰ˆæœ¬æ§åˆ¶å™¨ï¼Œè¯¥æ§åˆ¶å™¨å°è£…æ¯æ¬¡ç‰ˆæœ¬çš„podå¯¹è±¡ã€‚\n1. startReplicaSetController startReplicaSetControllerå‡½æ•°æ˜¯ReplicaSetControllerçš„å…¥å£å‡½æ•°ã€‚åŸºæœ¬çš„æ“ä½œå³new controllerå¯¹è±¡ï¼Œç„¶åèµ·ä¸€ä¸ªgoroutineè¿è¡Œrunå‡½æ•°ã€‚\nfunc startReplicaSetController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) { go replicaset.NewReplicaSetController( klog.FromContext(ctx), controllerContext.InformerFactory.Apps().V1().ReplicaSets(), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.ClientBuilder.ClientOrDie(\"replicaset-controller\"), replicaset.BurstReplicas, ).Run(ctx, int(controllerContext.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs)) return nil, true, nil } 2. NewReplicaSetController NewReplicaSetControlleråˆå§‹åŒ–controllerå¯¹è±¡ï¼Œæœ€ç»ˆé€šè¿‡NewBaseControllerå®ç°å…·ä½“çš„åˆå§‹åŒ–æ“ä½œã€‚\n// NewReplicaSetController configures a replica set controller with the specified event recorder func NewReplicaSetController(logger klog.Logger, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int) *ReplicaSetController { eventBroadcaster := record.NewBroadcaster() if err := metrics.Register(legacyregistry.Register); err != nil { logger.Error(err, \"unable to register metrics\") } return NewBaseController(logger, rsInformer, podInformer, kubeClient, burstReplicas, apps.SchemeGroupVersion.WithKind(\"ReplicaSet\"), \"replicaset_controller\", \"replicaset\", controller.RealPodControl{ KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"replicaset-controller\"}), }, eventBroadcaster, ) } 2.1. NewBaseController NewBaseControlleræ˜¯ä¸€ä¸ªå¸¸è§çš„k8s controlleræ„å»ºå‡½æ•°ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\nåˆå§‹åŒ–å¸¸ç”¨clientï¼ŒåŒ…æ‹¬kube client æ·»åŠ event handlerå¯¹è±¡ã€‚ æ·»åŠ informerç´¢å¼•ã€‚ æ·»åŠ informer syncCacheçš„å‡½æ•°ï¼Œåœ¨å¤„ç†controlleré€»è¾‘å‰å…ˆåŒæ­¥ä¸€ä¸‹etcdçš„æ•°æ®åˆ°æœ¬åœ°cacheã€‚ èµ‹å€¼syncHandlerå‡½æ•°ï¼Œæ˜¯å…·ä½“å®ç°controlleré€»è¾‘çš„å‡½æ•°ã€‚ func NewBaseController(logger klog.Logger, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.PodControlInterface, eventBroadcaster record.EventBroadcaster) *ReplicaSetController { // åˆå§‹åŒ–å¸¸ç”¨é…ç½® rsc := \u0026ReplicaSetController{ GroupVersionKind: gvk, kubeClient: kubeClient, podControl: podControl, eventBroadcaster: eventBroadcaster, burstReplicas: burstReplicas, expectations: controller.NewUIDTrackingControllerExpectations(controller.NewControllerExpectations()), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), queueName), } // æ·»åŠ event handlerå¯¹è±¡ rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { rsc.addRS(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { rsc.updateRS(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { rsc.deleteRS(logger, obj) }, }) // æ·»åŠ informerç´¢å¼• rsInformer.Informer().AddIndexers(cache.Indexers{ controllerUIDIndex: func(obj interface{}) ([]string, error) { rs, ok := obj.(*apps.ReplicaSet) if !ok { return []string{}, nil } controllerRef := metav1.GetControllerOf(rs) if controllerRef == nil { return []string{}, nil } return []string{string(controllerRef.UID)}, nil }, }) rsc.rsIndexer = rsInformer.Informer().GetIndexer() rsc.rsLister = rsInformer.Lister() // åˆå§‹åŒ–informerçš„syncå‡½æ•° rsc.rsListerSynced = rsInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { rsc.addPod(logger, obj) }, // This invokes the ReplicaSet for every pod change, eg: host assignment. Though this might seem like // overkill the most frequent pod update is status, and the associated ReplicaSet will only list from // local storage, so it should be ok. UpdateFunc: func(oldObj, newObj interface{}) { rsc.updatePod(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { rsc.deletePod(logger, obj) }, }) rsc.podLister = podInformer.Lister() rsc.podListerSynced = podInformer.Informer().HasSynced // åˆå§‹åŒ–syncHandlerå‡½æ•°ï¼Œè¯¥å‡½æ•°ä¸ºå…·ä½“å®ç°controllerä¸šåŠ¡é€»è¾‘çš„å‡½æ•°ã€‚ rsc.syncHandler = rsc.syncReplicaSet return rsc } 3. Run Runå‡½æ•°ä»ç„¶æ˜¯k8s controllerçš„ä»£ç é£æ ¼ã€‚ä¸»è¦åŒ…å«äº†ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ã€‚\nåŒæ­¥æœ¬åœ°cacheå†…å®¹ è¿è¡Œå¤šä¸ªä¸é€€å‡ºçš„goroutineå¤„ç†æ§åˆ¶å™¨é€»è¾‘ã€‚ func (rsc *ReplicaSetController) Run(ctx context.Context, workers int) { // å·²åˆ é™¤éæ ¸å¿ƒé€»è¾‘ä»£ç ã€‚ defer rsc.queue.ShutDown() // å¤„ç†å‰åŒæ­¥ä¸‹cacheçš„å†…å®¹ã€‚ if !cache.WaitForNamedCacheSync(rsc.Kind, ctx.Done(), rsc.podListerSynced, rsc.rsListerSynced) { return } // è¿è¡ŒæŒ‡å®šä¸ªæ•°çš„goroutineæ¥å¤„ç†controlleré€»è¾‘ã€‚ for i := 0; i \u003c workers; i++ { go wait.UntilWithContext(ctx, rsc.worker, time.Second) } \u003c-ctx.Done() } 3.1. processNextWorkItem processNextWorkItemzä¸»è¦è¿è¡ŒsyncHandlerå‡½æ•°ï¼Œå’Œå¯¹è¿”å›çš„é”™è¯¯è¿›è¡Œå¤„ç†ã€‚\nå¦‚æœé”™è¯¯ä¸ºç©ºï¼Œåˆ™ä¸å†å…¥é˜Ÿã€‚ å¦‚æœé”™è¯¯ä¸ä¸ºç©ºï¼Œåˆ™å…¥é˜Ÿé‡æ–°å¤„ç†ã€‚ func (rsc *ReplicaSetController) worker(ctx context.Context) { for rsc.processNextWorkItem(ctx) { } } func (rsc *ReplicaSetController) processNextWorkItem(ctx context.Context) bool { key, quit := rsc.queue.Get() if quit { return false } defer rsc.queue.Done(key) // å…·ä½“çš„é€»è¾‘ä»£ç ç”±syncHandlerå®ç°ã€‚ err := rsc.syncHandler(ctx, key.(string)) if err == nil { // å¦‚æœé”™è¯¯ä¸ºç©ºï¼Œåˆ™ä¸å†å…¥é˜Ÿ rsc.queue.Forget(key) return true } utilruntime.HandleError(fmt.Errorf(\"sync %q failed with %v\", key, err)) // å¦‚æœé”™è¯¯ä¸ä¸ºç©ºï¼Œåˆ™é‡æ–°å…¥é˜Ÿ rsc.queue.AddRateLimited(key) return true } 4. syncReplicaSet syncReplicaSetæ˜¯syncHandlerçš„å…·ä½“å®ç°ï¼Œå¸¸è§çš„syncHandlerçš„å®ç°åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†\nè·å–é›†ç¾¤ä¸­çš„controllerå¯¹è±¡ï¼Œä¾‹å¦‚ rsã€‚ è·å–è¯¥controllerå¯¹è±¡åŠå…¶å­å¯¹è±¡çš„å½“å‰çŠ¶æ€ã€‚ å¯¹æ¯”å½“å‰çŠ¶æ€ä¸é¢„æœŸçŠ¶æ€æ˜¯å¦ä¸€è‡´ã€‚ æ›´æ–°å½“å‰çŠ¶æ€ï¼Œä»¥ä¸Šå¾ªç¯ç›´åˆ°å½“å‰çŠ¶æ€è¾¾åˆ°æœŸæœ›çŠ¶æ€ã€‚ func (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error { // å·²åˆ é™¤éæ ¸å¿ƒä»£ç  namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return err } // è·å–é›†ç¾¤ä¸­çš„rså¯¹è±¡ rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) if apierrors.IsNotFound(err) { logger.V(4).Info(\"deleted\", \"kind\", rsc.Kind, \"key\", key) rsc.expectations.DeleteExpectations(logger, key) return nil } rsNeedsSync := rsc.expectations.SatisfiedExpectations(logger, key) // è·å–æŒ‡å®šselectorä¸‹pod selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector) allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything()) filteredPods := controller.FilterActivePods(logger, allPods) filteredPods, err = rsc.claimPods(ctx, rs, selector, filteredPods) // å¤„ç†replicaé€»è¾‘ var manageReplicasErr error if rsNeedsSync \u0026\u0026 rs.DeletionTimestamp == nil { manageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs) } rs = rs.DeepCopy() newStatus := calculateStatus(rs, filteredPods, manageReplicasErr) // æ›´æ–°çŠ¶æ€ updatedRS, err := updateReplicaSetStatus(logger, rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus) // Resync the ReplicaSet after MinReadySeconds as a last line of defense to guard against clock-skew. if manageReplicasErr == nil \u0026\u0026 updatedRS.Spec.MinReadySeconds \u003e 0 \u0026\u0026 updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) \u0026\u0026 updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) { rsc.queue.AddAfter(key, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second) } return manageReplicasErr } 5. manageReplicas manageReplicasä¸»è¦å®ç°podçš„åˆ›å»ºå’Œåˆ é™¤ï¼Œä»è€Œä¿è¯å½“å‰rsä¸‹çš„podè·Ÿé¢„æœŸçš„ä¸€è‡´ã€‚\nfunc (rsc *ReplicaSetController) manageReplicas(ctx context.Context, filteredPods []*v1.Pod, rs *apps.ReplicaSet) error { // è®¡ç®—å½“å‰çš„podæ•°é‡å’Œé¢„æœŸçš„podæ˜¯å¦ä¸€è‡´ã€‚ diff := len(filteredPods) - int(*(rs.Spec.Replicas)) rsKey, err := controller.KeyFunc(rs) // å¦‚æœå°‘äºé¢„æœŸ if diff \u003c 0 { // åˆ™æ‰¹é‡åˆ›å»ºpod successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error { err := rsc.podControl.CreatePods(ctx, rs.Namespace, \u0026rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind)) }) // å¦‚æœpodæ•°é‡å¤šäºé¢„æœŸ } else if diff \u003e 0 { relatedPods, err := rsc.getIndirectlyRelatedPods(logger, rs) utilruntime.HandleError(err) // Choose which Pods to delete, preferring those in earlier phases of startup. podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff) errCh := make(chan error, diff) var wg sync.WaitGroup wg.Add(diff) for _, pod := range podsToDelete { go func(targetPod *v1.Pod) { defer wg.Done() // æ‰¹é‡åˆ é™¤pod if err := rsc.podControl.DeletePod(ctx, rs.Namespace, targetPod.Name, rs); err != nil { } }(pod) } wg.Wait() // å¤„ç†é”™è¯¯ select { case err := \u003c-errCh: // all errors have been reported before and they're likely to be the same, so we'll only return the first one we hit. if err != nil { return err } default: } } return nil } æ€»ç»“ replicaset-controllerçš„ä»£ç é€»è¾‘ç›¸å¯¹ç®€å•ï¼ŒåŸºæœ¬çš„ä»£ç é£æ ¼æ˜¯k8sæ§åˆ¶å™¨é€šç”¨çš„ä»£ç é€»è¾‘ï¼Œç”±äºk8sçš„ä»£ç é£æ ¼é«˜åº¦ä¸€è‡´ï¼Œå› æ­¤å¦‚æœè¯»æ¸…æ¥šä¸€ç±»controllerçš„æ§åˆ¶é€»è¾‘ã€‚å…¶ä»–çš„æ§åˆ¶å™¨çš„ä»£ç é€»è¾‘å¤§åŒå°å¼‚ã€‚\nå‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replicaset/replica_set.go ","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦åˆ†æreplicaset-controllerçš„æºç é€»è¾‘ï¼Œreplicaså¯¹è±¡åˆ›å»ºä¸»è¦æ˜¯ç”±deployment-controller â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/replicaset-controller/","tags":["æºç åˆ†æ"],"title":"kube-controller-manageræºç åˆ†æï¼ˆå››ï¼‰ä¹‹ ReplicaSetController"},{"body":"1. echo echoæ˜¯Shellçš„ä¸€ä¸ªå†…éƒ¨æŒ‡ä»¤ï¼Œç”¨äºåœ¨å±å¹•ä¸Šæ‰“å°å‡ºæŒ‡å®šçš„å­—ç¬¦ä¸²ã€‚å‘½ä»¤æ ¼å¼ï¼š\necho arg æ‚¨å¯ä»¥ä½¿ç”¨echoå®ç°æ›´å¤æ‚çš„è¾“å‡ºæ ¼å¼æ§åˆ¶ã€‚\n1.1. æ˜¾ç¤ºè½¬ä¹‰å­—ç¬¦ echo \"\\\"It is a test\\\"\" ç»“æœå°†æ˜¯ï¼š\n\"It is a test\" åŒå¼•å·ä¹Ÿå¯ä»¥çœç•¥ã€‚\n1.2. æ˜¾ç¤ºå˜é‡ name=\"OK\" echo \"$name It is a test\" ç»“æœå°†æ˜¯ï¼š\nOK It is a test åŒæ ·åŒå¼•å·ä¹Ÿå¯ä»¥çœç•¥ã€‚\nå¦‚æœå˜é‡ä¸å…¶å®ƒå­—ç¬¦ç›¸è¿çš„è¯ï¼Œéœ€è¦ä½¿ç”¨å¤§æ‹¬å·ï¼ˆ{ }ï¼‰ï¼š\nmouth=8 echo \"${mouth}-1-2009\" ç»“æœå°†æ˜¯ï¼š\n8-1-2009 1.3. æ˜¾ç¤ºæ¢è¡Œ echo \"OK!\\n\" echo \"It is a test\" è¾“å‡ºï¼š\nOK! It is a test 1.4. æ˜¾ç¤ºä¸æ¢è¡Œ echo \"OK!\\c\" echo \"It is a test\" è¾“å‡ºï¼š\nOK!It si a test 1.5. æ˜¾ç¤ºç»“æœé‡å®šå‘è‡³æ–‡ä»¶ echo \"It is a test\" \u003e myfile 1.6. åŸæ ·è¾“å‡ºå­—ç¬¦ä¸² è‹¥éœ€è¦åŸæ ·è¾“å‡ºå­—ç¬¦ä¸²ï¼ˆä¸è¿›è¡Œè½¬ä¹‰ï¼‰ï¼Œè¯·ä½¿ç”¨å•å¼•å·ã€‚ä¾‹å¦‚ï¼š\necho '$name\\\"' 1.7. æ˜¾ç¤ºå‘½ä»¤æ‰§è¡Œç»“æœ echo `date` ç»“æœå°†æ˜¾ç¤ºå½“å‰æ—¥æœŸ ä»ä¸Šé¢å¯çœ‹å‡ºï¼ŒåŒå¼•å·å¯æœ‰å¯æ— ï¼Œå•å¼•å·ä¸»è¦ç”¨åœ¨åŸæ ·è¾“å‡ºä¸­ã€‚\n2. printf printf å‘½ä»¤ç”¨äºæ ¼å¼åŒ–è¾“å‡ºï¼Œ æ˜¯echoå‘½ä»¤çš„å¢å¼ºç‰ˆã€‚å®ƒæ˜¯Cè¯­è¨€printf()åº“å‡½æ•°çš„ä¸€ä¸ªæœ‰é™çš„å˜å½¢ï¼Œå¹¶ä¸”åœ¨è¯­æ³•ä¸Šæœ‰äº›ä¸åŒã€‚ printf ä¸åƒ echo é‚£æ ·ä¼šè‡ªåŠ¨æ¢è¡Œï¼Œå¿…é¡»æ˜¾å¼æ·»åŠ æ¢è¡Œç¬¦(\\n)ã€‚ æ³¨æ„ï¼šprintf ç”± POSIX æ ‡å‡†æ‰€å®šä¹‰ï¼Œç§»æ¤æ€§è¦æ¯” echo å¥½ã€‚\nprintf å‘½ä»¤çš„è¯­æ³•ï¼š\nprintf format-string [arguments...] format-string ä¸ºæ ¼å¼æ§åˆ¶å­—ç¬¦ä¸²ï¼Œarguments ä¸ºå‚æ•°åˆ—è¡¨ã€‚\nprintf()åŠŸèƒ½å’Œç”¨æ³•ä¸ printf å‘½ä»¤ç±»ä¼¼\nè¿™é‡Œä»…è¯´æ˜ä¸Cè¯­è¨€printf()å‡½æ•°çš„ä¸åŒï¼š\nprintf å‘½ä»¤ä¸ç”¨åŠ æ‹¬å· format-string å¯ä»¥æ²¡æœ‰å¼•å·ï¼Œä½†æœ€å¥½åŠ ä¸Šï¼Œå•å¼•å·åŒå¼•å·å‡å¯ã€‚ å‚æ•°å¤šäºæ ¼å¼æ§åˆ¶ç¬¦(%)æ—¶ï¼Œformat-string å¯ä»¥é‡ç”¨ï¼Œå¯ä»¥å°†æ‰€æœ‰å‚æ•°éƒ½è½¬æ¢ã€‚ æ ¼å¼åªæŒ‡å®šäº†ä¸€ä¸ªå‚æ•°ï¼Œä½†å¤šå‡ºçš„å‚æ•°ä»ç„¶ä¼šæŒ‰ç…§è¯¥æ ¼å¼è¾“å‡ºï¼Œformat-string è¢«é‡ç”¨ arguments ä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä¸ç”¨é€—å·ã€‚ å¦‚æœæ²¡æœ‰ argumentsï¼Œé‚£ä¹ˆ %s ç”¨NULLä»£æ›¿ï¼Œ%d ç”¨ 0 ä»£æ›¿\nå¦‚æœä»¥ %d çš„æ ¼å¼æ¥æ˜¾ç¤ºå­—ç¬¦ä¸²ï¼Œé‚£ä¹ˆä¼šæœ‰è­¦å‘Šï¼Œæç¤ºæ— æ•ˆçš„æ•°å­—ï¼Œæ­¤æ—¶é»˜è®¤ç½®ä¸º 0\n# format-stringä¸ºåŒå¼•å·,å•å¼•å·ä¸åŒå¼•å·æ•ˆæœä¸€æ ·,æ²¡æœ‰å¼•å·ä¹Ÿå¯ä»¥è¾“å‡º $ printf \"%d %s\\n\" 1 \"abc\" 1 abc æ³¨æ„ï¼Œæ ¹æ®POSIXæ ‡å‡†ï¼Œæµ®ç‚¹æ ¼å¼%eã€%Eã€%fã€%gä¸%Gæ˜¯â€œä¸éœ€è¦è¢«æ”¯æŒâ€ã€‚è¿™æ˜¯å› ä¸ºawkæ”¯æŒæµ®ç‚¹é¢„ç®—ï¼Œä¸”æœ‰å®ƒè‡ªå·±çš„printfè¯­å¥ã€‚è¿™æ ·Shellç¨‹åºä¸­éœ€è¦å°†æµ®ç‚¹æ•°å€¼è¿›è¡Œæ ¼å¼åŒ–çš„æ‰“å°æ—¶ï¼Œå¯ä½¿ç”¨å°å‹çš„awkç¨‹åºå®ç°ã€‚ç„¶è€Œï¼Œå†…å»ºäºbashã€ksh93å’Œzshä¸­çš„printfå‘½ä»¤éƒ½æ”¯æŒæµ®ç‚¹æ ¼å¼ã€‚\nå‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. echo echoæ˜¯Shellçš„ä¸€ä¸ªå†…éƒ¨æŒ‡ä»¤ï¼Œç”¨äºåœ¨å±å¹•ä¸Šæ‰“å°å‡ºæŒ‡å®šçš„å­—ç¬¦ä¸²ã€‚å‘½ä»¤æ ¼å¼ï¼š\necho arg æ‚¨å¯ä»¥ä½¿ç”¨echoå®ç°æ›´å¤ â€¦","ref":"/linux-notes/shell/shell-echo/","tags":["Shell"],"title":"Shell echoå‘½ä»¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/shell/","tags":"","title":"Shellè„šæœ¬"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækubeletä¸­syncLoopIterationéƒ¨åˆ†ã€‚syncLoopIterationé€šè¿‡å‡ ç§channelæ¥å¯¹ä¸åŒç±»å‹çš„äº‹ä»¶è¿›è¡Œç›‘å¬å¹¶åšå¢åˆ æ”¹æŸ¥çš„å¤„ç†ã€‚\n1. syncLoop syncLoopæ˜¯å¤„ç†å˜æ›´çš„å¾ªç¯ã€‚ å®ƒç›‘å¬æ¥è‡ªä¸‰ç§channelï¼ˆfileï¼Œapiserverå’Œhttpï¼‰çš„æ›´æ”¹ã€‚ å¯¹äºçœ‹åˆ°çš„ä»»ä½•æ–°æ›´æ”¹ï¼Œå°†é’ˆå¯¹æ‰€éœ€çŠ¶æ€å’Œè¿è¡ŒçŠ¶æ€è¿è¡ŒåŒæ­¥ã€‚ å¦‚æœæ²¡æœ‰çœ‹åˆ°é…ç½®çš„å˜åŒ–ï¼Œå°†åœ¨æ¯ä¸ªåŒæ­¥é¢‘ç‡ç§’åŒæ­¥æœ€åå·²çŸ¥çš„æ‰€éœ€çŠ¶æ€ã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kubelet.go\n// syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates \u003c-chan kubetypes.PodUpdate, handler SyncHandler) { glog.Info(\"Starting kubelet main sync loop.\") // The resyncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync'd. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 { glog.Infof(\"skipping pod synchronization - %v\", rs) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } å…¶ä¸­è°ƒç”¨äº†syncLoopIterationçš„å‡½æ•°æ¥æ‰§è¡Œæ›´å…·ä½“çš„ç›‘æ§podå˜åŒ–çš„å¾ªç¯ã€‚\n2. syncLoopIteration syncLoopIterationä¸»è¦é€šè¿‡å‡ ç§channelæ¥å¯¹ä¸åŒç±»å‹çš„äº‹ä»¶è¿›è¡Œç›‘å¬å¹¶å¤„ç†ã€‚å…¶ä¸­åŒ…æ‹¬ï¼šconfigChã€plegChã€syncChã€houseKeepingChã€livenessManager.Updates()ã€‚\nsyncLoopIterationå®é™…æ‰§è¡Œäº†podçš„æ“ä½œï¼Œæ­¤éƒ¨åˆ†è®¾ç½®äº†å‡ ç§ä¸åŒçš„channel:\nconfigChï¼šå°†é…ç½®æ›´æ”¹çš„podåˆ†æ´¾ç»™äº‹ä»¶ç±»å‹çš„ç›¸åº”å¤„ç†ç¨‹åºå›è°ƒã€‚ plegChï¼šæ›´æ–°runtimeç¼“å­˜ï¼ŒåŒæ­¥podã€‚ syncChï¼šåŒæ­¥æ‰€æœ‰ç­‰å¾…åŒæ­¥çš„podã€‚ houseKeepingChï¼šè§¦å‘æ¸…ç†podã€‚ livenessManager.Updates()ï¼šå¯¹å¤±è´¥çš„podæˆ–è€…livenessæ£€æŸ¥å¤±è´¥çš„podè¿›è¡Œsyncæ“ä½œã€‚ syncLoopIterationéƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kubelet.go\n2.1. configCh configChå°†é…ç½®æ›´æ”¹çš„podåˆ†æ´¾ç»™äº‹ä»¶ç±»å‹çš„ç›¸åº”å¤„ç†ç¨‹åºå›è°ƒï¼Œè¯¥éƒ¨åˆ†ä¸»è¦é€šè¿‡SyncHandlerå¯¹podçš„ä¸åŒäº‹ä»¶è¿›è¡Œå¢åˆ æ”¹æŸ¥ç­‰æ“ä½œã€‚\nfunc (kl *Kubelet) syncLoopIteration(configCh \u003c-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u003c-chan time.Time, housekeepingCh \u003c-chan time.Time, plegCh \u003c-chan *pleg.PodLifecycleEvent) bool { select { case u, open := \u003c-configCh: // Update from a config source; dispatch it to the right handler // callback. if !open { glog.Errorf(\"Update channel is closed. Exiting the sync loop.\") return false } switch u.Op { case kubetypes.ADD: glog.V(2).Infof(\"SyncLoop (ADD, %q): %q\", u.Source, format.Pods(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: glog.V(2).Infof(\"SyncLoop (UPDATE, %q): %q\", u.Source, format.PodsWithDeletionTimestamps(u.Pods)) handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: glog.V(2).Infof(\"SyncLoop (REMOVE, %q): %q\", u.Source, format.Pods(u.Pods)) handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: glog.V(4).Infof(\"SyncLoop (RECONCILE, %q): %q\", u.Source, format.Pods(u.Pods)) handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: glog.V(2).Infof(\"SyncLoop (DELETE, %q): %q\", u.Source, format.Pods(u.Pods)) // DELETE is treated as a UPDATE because of graceful deletion. handler.HandlePodUpdates(u.Pods) case kubetypes.RESTORE: glog.V(2).Infof(\"SyncLoop (RESTORE, %q): %q\", u.Source, format.Pods(u.Pods)) // These are pods restored from the checkpoint. Treat them as new // pods. handler.HandlePodAdditions(u.Pods) case kubetypes.SET: // TODO: Do we want to support this? glog.Errorf(\"Kubelet does not support snapshot update\") } ... } å¯ä»¥çœ‹å‡ºsyncLoopIterationæ ¹æ®podUpdateçš„å€¼æ¥æ‰§è¡Œä¸åŒçš„podæ“ä½œï¼Œå…·ä½“å¦‚ä¸‹ï¼š\nADDï¼šHandlePodAdditions UPDATEï¼šHandlePodUpdates REMOVEï¼šHandlePodRemoves RECONCILEï¼šHandlePodReconcile DELETEï¼šHandlePodUpdates RESTOREï¼šHandlePodAdditions podsToSyncï¼šHandlePodSyncs å…¶ä¸­æ‰§è¡Œpodçš„handleræ“ä½œçš„æ˜¯SyncHandlerï¼Œè¯¥ç±»å‹æ˜¯ä¸€ä¸ªæ¥å£ï¼Œå®ç°ä½“ä¸ºkubeletæœ¬èº«ï¼Œå…·ä½“è§åç»­åˆ†æã€‚\n2.2. plegCh plegChï¼šæ›´æ–°runtimeç¼“å­˜ï¼ŒåŒæ­¥podã€‚æ­¤å¤„è°ƒç”¨äº†HandlePodSyncsçš„å‡½æ•°ã€‚\ncase e := \u003c-plegCh: if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { glog.V(2).Infof(\"SyncLoop (PLEG): %q, event: %#v\", format.Pod(pod), e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. glog.V(4).Infof(\"SyncLoop (PLEG): ignore irrelevant event: %#v\", e) } } if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } 2.3. syncCh syncChï¼šåŒæ­¥æ‰€æœ‰ç­‰å¾…åŒæ­¥çš„podã€‚æ­¤å¤„è°ƒç”¨äº†HandlePodSyncsçš„å‡½æ•°ã€‚\ncase \u003c-syncCh: // Sync pods waiting for sync podsToSync := kl.getPodsToSync() if len(podsToSync) == 0 { break } glog.V(4).Infof(\"SyncLoop (SYNC): %d pods; %s\", len(podsToSync), format.Pods(podsToSync)) handler.HandlePodSyncs(podsToSync) 2.4. livenessManager.Update livenessManager.Updates()ï¼šå¯¹å¤±è´¥çš„podæˆ–è€…livenessæ£€æŸ¥å¤±è´¥çš„podè¿›è¡Œsyncæ“ä½œã€‚æ­¤å¤„è°ƒç”¨äº†HandlePodSyncsçš„å‡½æ•°ã€‚\ncase update := \u003c-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { // The liveness manager detected a failure; sync the pod. // We should not use the pod from livenessManager, because it is never updated after // initialization. pod, ok := kl.podManager.GetPodByUID(update.PodUID) if !ok { // If the pod no longer exists, ignore the update. glog.V(4).Infof(\"SyncLoop (container unhealthy): ignore irrelevant update: %#v\", update) break } glog.V(1).Infof(\"SyncLoop (container unhealthy): %q\", format.Pod(pod)) handler.HandlePodSyncs([]*v1.Pod{pod}) } 2.5. housekeepingCh houseKeepingChï¼šè§¦å‘æ¸…ç†podã€‚æ­¤å¤„è°ƒç”¨äº†HandlePodCleanupsçš„å‡½æ•°ã€‚\ncase \u003c-housekeepingCh: if !kl.sourcesReady.AllReady() { // If the sources aren't ready or volume manager has not yet synced the states, // skip housekeeping, as we may accidentally delete pods from unready sources. glog.V(4).Infof(\"SyncLoop (housekeeping, skipped): sources aren't ready yet.\") } else { glog.V(4).Infof(\"SyncLoop (housekeeping)\") if err := handler.HandlePodCleanups(); err != nil { glog.Errorf(\"Failed cleaning pods: %v\", err) } } 3. SyncHandler SyncHandleræ˜¯ä¸€ä¸ªå®šä¹‰Podçš„ä¸åŒHandlerçš„æ¥å£ï¼Œå…·ä½“æ˜¯å®ç°è€…æ˜¯kubeletï¼Œè¯¥æ¥å£çš„æ–¹æ³•ä¸»è¦åœ¨syncLoopIterationä¸­è°ƒç”¨ï¼Œæ¥å£å®šä¹‰å¦‚ä¸‹ï¼š\n// SyncHandler is an interface implemented by Kubelet, for testability type SyncHandler interface { HandlePodAdditions(pods []*v1.Pod) HandlePodUpdates(pods []*v1.Pod) HandlePodRemoves(pods []*v1.Pod) HandlePodReconcile(pods []*v1.Pod) HandlePodSyncs(pods []*v1.Pod) HandlePodCleanups() error } SyncHandleréƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kubelet.go\n3.1. HandlePodAdditions HandlePodAdditionså…ˆæ ¹æ®podåˆ›å»ºæ—¶é—´å¯¹podè¿›è¡Œæ’åºï¼Œç„¶åéå†podåˆ—è¡¨ï¼Œæ¥æ‰§è¡Œpodçš„ç›¸å…³æ“ä½œã€‚\n// HandlePodAdditions is the callback in SyncHandler for pods being added from // a config source. func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods { ... } } å°†podæ·»åŠ åˆ°pod managerä¸­ã€‚\nfor _, pod := range pods { // Responsible for checking limits in resolv.conf if kl.dnsConfigurer != nil \u0026\u0026 kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } existingPods := kl.podManager.GetPods() // Always add the pod to the pod manager. Kubelet relies on the pod // manager as the source of truth for the desired state. If a pod does // not exist in the pod manager, it means that it has been deleted in // the apiserver and no action (other than cleanup) is required. kl.podManager.AddPod(pod) ... } å¦‚æœæ˜¯mirror podï¼Œåˆ™å¯¹mirror podè¿›è¡Œå¤„ç†ã€‚\nif kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } å¦‚æœå½“å‰podçš„çŠ¶æ€ä¸æ˜¯TerminatedçŠ¶æ€ï¼Œåˆ™åˆ¤æ–­æ˜¯å¦æ¥å—è¯¥podï¼Œå¦‚æœä¸æ¥å—åˆ™å°†podçŠ¶æ€æ”¹ä¸ºFailedã€‚\nif !kl.podIsTerminated(pod) { // Only go through the admission process if the pod is not // terminated. // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutTerminatedPods(existingPods) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue } } æ‰§è¡ŒdispatchWorkå‡½æ•°ï¼Œè¯¥å‡½æ•°æ˜¯syncHandlerä¸­è°ƒç”¨åˆ°çš„æ ¸å¿ƒå‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨pod workerä¸­å¯åŠ¨ä¸€ä¸ªå¼‚æ­¥å¾ªç¯ï¼Œæ¥åˆ†æ´¾podçš„ç›¸å…³æ“ä½œã€‚è¯¥å‡½æ•°çš„å…·ä½“æ“ä½œå¾…åç»­åˆ†æã€‚\nmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) æœ€ååŠ podæ·»åŠ åˆ°probe managerä¸­ã€‚\nkl.probeManager.AddPod(pod) 3.2. HandlePodUpdates HandlePodUpdatesåŒæ ·éå†podåˆ—è¡¨ï¼Œæ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚\n// HandlePodUpdates is the callback in the SyncHandler interface for pods // being updated from a config source. func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } å°†podæ›´æ–°åˆ°pod managerä¸­ã€‚\nfor _, pod := range pods { // Responsible for checking limits in resolv.conf if kl.dnsConfigurer != nil \u0026\u0026 kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } kl.podManager.UpdatePod(pod) ... } å¦‚æœæ˜¯mirror podï¼Œåˆ™å¯¹mirror podè¿›è¡Œå¤„ç†ã€‚\nif kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } æ‰§è¡ŒdispatchWorkå‡½æ•°ã€‚\n// TODO: Evaluate if we need to validate and reject updates. mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodUpdate, mirrorPod, start) 3.3. HandlePodRemoves HandlePodRemoveséå†podåˆ—è¡¨ã€‚\n// HandlePodRemoves is the callback in the SyncHandler interface for pods // being removed from a config source. func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } ä»pod managerä¸­åˆ é™¤podã€‚\nfor _, pod := range pods { kl.podManager.DeletePod(pod) ... } å¦‚æœæ˜¯mirror podï¼Œåˆ™å¯¹mirror podè¿›è¡Œå¤„ç†ã€‚\nif kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } è°ƒç”¨kubeletçš„deletePodå‡½æ•°æ¥åˆ é™¤podã€‚\n// Deletion is allowed to fail because the periodic cleanup routine // will trigger deletion again. if err := kl.deletePod(pod); err != nil { glog.V(2).Infof(\"Failed to delete pod %q, err: %v\", format.Pod(pod), err) } deletePod å‡½æ•°å°†éœ€è¦åˆ é™¤çš„podåŠ å…¥podKillingChçš„channelä¸­ï¼Œæœ‰podKillerç›‘å¬è¿™ä¸ªchannelå»æ‰§è¡Œåˆ é™¤ä»»åŠ¡ï¼Œå®ç°å¦‚ä¸‹ï¼š\n// deletePod deletes the pod from the internal state of the kubelet by: // 1. stopping the associated pod worker asynchronously // 2. signaling to kill the pod by sending on the podKillingCh channel // // deletePod returns an error if not all sources are ready or the pod is not // found in the runtime cache. func (kl *Kubelet) deletePod(pod *v1.Pod) error { if pod == nil { return fmt.Errorf(\"deletePod does not allow nil pod\") } if !kl.sourcesReady.AllReady() { // If the sources aren't ready, skip deletion, as we may accidentally delete pods // for sources that haven't reported yet. return fmt.Errorf(\"skipping delete because sources aren't ready yet\") } kl.podWorkers.ForgetWorker(pod.UID) // Runtime cache may not have been updated to with the pod, but it's okay // because the periodic cleanup routine will attempt to delete again later. runningPods, err := kl.runtimeCache.GetPods() if err != nil { return fmt.Errorf(\"error listing containers: %v\", err) } runningPod := kubecontainer.Pods(runningPods).FindPod(\"\", pod.UID) if runningPod.IsEmpty() { return fmt.Errorf(\"pod not found\") } podPair := kubecontainer.PodPair{APIPod: pod, RunningPod: \u0026runningPod} kl.podKillingCh \u003c- \u0026podPair // TODO: delete the mirror pod here? // We leave the volume/directory cleanup to the periodic cleanup routine. return nil } ä»probe managerä¸­ç§»é™¤podã€‚\nkl.probeManager.RemovePod(pod) 3.4. HandlePodReconcile éå†podåˆ—è¡¨ã€‚\n// HandlePodReconcile is the callback in the SyncHandler interface for pods // that should be reconciled. func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } å°†podæ›´æ–°åˆ°pod managerä¸­ã€‚\nfor _, pod := range pods { // Update the pod in pod manager, status manager will do periodically reconcile according // to the pod manager. kl.podManager.UpdatePod(pod) ... } å¿…è¦æ—¶è°ƒæ•´podçš„ReadyçŠ¶æ€ï¼Œæ‰§è¡ŒdispatchWorkå‡½æ•°ã€‚\n// Reconcile Pod \"Ready\" condition if necessary. Trigger sync pod for reconciliation. if status.NeedToReconcilePodReadiness(pod) { mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start) } å¦‚æœpodè¢«è®¾å®šä¸ºéœ€è¦è¢«é©±é€çš„ï¼Œåˆ™åˆ é™¤podä¸­çš„å®¹å™¨ã€‚\n// After an evicted pod is synced, all dead containers in the pod can be removed. if eviction.PodIsEvicted(pod.Status) { if podStatus, err := kl.podCache.Get(pod.UID); err == nil { kl.containerDeletor.deleteContainersInPod(\"\", podStatus, true) } } 3.5. HandlePodSyncs HandlePodSyncsæ˜¯syncHandleræ¥å£å›è°ƒå‡½æ•°ï¼Œè°ƒç”¨dispatchWorkï¼Œé€šè¿‡pod workeræ¥æ‰§è¡Œä»»åŠ¡ã€‚\n// HandlePodSyncs is the callback in the syncHandler interface for pods // that should be dispatched to pod workers for sync. func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start) } } 3.6. HandlePodCleanups HandlePodCleanupsä¸»è¦ç”¨æ¥æ‰§è¡Œpodçš„æ¸…ç†ä»»åŠ¡ï¼Œå…¶ä¸­åŒ…æ‹¬terminatingçš„podï¼Œorphanedçš„podç­‰ã€‚\né¦–å…ˆæŸ¥çœ‹podä½¿ç”¨åˆ°çš„cgroupã€‚\n// HandlePodCleanups performs a series of cleanup work, including terminating // pod workers, killing unwanted pods, and removing orphaned volumes/pod // directories. // NOTE: This function is executed by the main sync loop, so it // should not contain any blocking calls. func (kl *Kubelet) HandlePodCleanups() error { // The kubelet lacks checkpointing, so we need to introspect the set of pods // in the cgroup tree prior to inspecting the set of pods in our pod manager. // this ensures our view of the cgroup tree does not mistakenly observe pods // that are added after the fact... var ( cgroupPods map[types.UID]cm.CgroupName err error ) if kl.cgroupsPerQOS { pcm := kl.containerManager.NewPodContainerManager() cgroupPods, err = pcm.GetAllPodsFromCgroups() if err != nil { return fmt.Errorf(\"failed to get list of pods that still exist on cgroup mounts: %v\", err) } } ... } åˆ—å‡ºæ‰€æœ‰podåŒ…æ‹¬mirror podã€‚\nallPods, mirrorPods := kl.podManager.GetPodsAndMirrorPods() // Pod phase progresses monotonically. Once a pod has reached a final state, // it should never leave regardless of the restart policy. The statuses // of such pods should not be changed, and there is no need to sync them. // TODO: the logic here does not handle two cases: // 1. If the containers were removed immediately after they died, kubelet // may fail to generate correct statuses, let alone filtering correctly. // 2. If kubelet restarted before writing the terminated status for a pod // to the apiserver, it could still restart the terminated pod (even // though the pod was not considered terminated by the apiserver). // These two conditions could be alleviated by checkpointing kubelet. activePods := kl.filterOutTerminatedPods(allPods) desiredPods := make(map[types.UID]empty) for _, pod := range activePods { desiredPods[pod.UID] = empty{} } pod workeråœæ­¢ä¸å†å­˜åœ¨çš„podçš„ä»»åŠ¡ï¼Œå¹¶ä»probe managerä¸­æ¸…é™¤podã€‚\n// Stop the workers for no-longer existing pods. // TODO: is here the best place to forget pod workers? kl.podWorkers.ForgetNonExistingPodWorkers(desiredPods) kl.probeManager.CleanupPods(activePods) å°†éœ€è¦æ€æ­»çš„podåŠ å…¥åˆ°podKillingChçš„channelä¸­ï¼ŒpodKillerçš„ä»»åŠ¡ä¼šç›‘å¬è¯¥channelå¹¶è·å–éœ€è¦æ€æ­»çš„podåˆ—è¡¨æ¥æ‰§è¡Œæ€æ­»podçš„æ“ä½œã€‚\nrunningPods, err := kl.runtimeCache.GetPods() if err != nil { glog.Errorf(\"Error listing containers: %#v\", err) return err } for _, pod := range runningPods { if _, found := desiredPods[pod.ID]; !found { kl.podKillingCh \u003c- \u0026kubecontainer.PodPair{APIPod: nil, RunningPod: pod} } } å½“podä¸å†è¢«ç»‘å®šåˆ°è¯¥èŠ‚ç‚¹ï¼Œç§»é™¤podStatusï¼Œå…¶ä¸­removeOrphanedPodStatusesæœ€åè°ƒç”¨çš„å‡½æ•°æ˜¯statusManagerçš„RemoveOrphanedStatusesæ–¹æ³•ã€‚\nkl.removeOrphanedPodStatuses(allPods, mirrorPods) ç§»é™¤æ‰€æœ‰çš„orphaned volumeã€‚\n// Remove any orphaned volumes. // Note that we pass all pods (including terminated pods) to the function, // so that we don't remove volumes associated with terminated but not yet // deleted pods. err = kl.cleanupOrphanedPodDirs(allPods, runningPods) if err != nil { // We want all cleanup tasks to be run even if one of them failed. So // we just log an error here and continue other cleanup tasks. // This also applies to the other clean up tasks. glog.Errorf(\"Failed cleaning up orphaned pod directories: %v\", err) } ç§»é™¤mirror podã€‚\n// Remove any orphaned mirror pods. kl.podManager.DeleteOrphanedMirrorPods() åˆ é™¤ä¸å†è¿è¡Œçš„podçš„cgroupã€‚\n// Remove any cgroups in the hierarchy for pods that are no longer running. if kl.cgroupsPerQOS { kl.cleanupOrphanedPodCgroups(cgroupPods, activePods) } æ‰§è¡Œåƒåœ¾å›æ”¶ï¼ˆGCï¼‰æ“ä½œã€‚\nkl.backOff.GC() 4. dispatchWork dispatchWorké€šè¿‡pod workerå¯åŠ¨ä¸€ä¸ªå¼‚æ­¥çš„å¾ªç¯ã€‚\nå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// dispatchWork starts the asynchronous sync of the pod in a pod worker. // If the pod is terminated, dispatchWork func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { // If the pod is in a terminated state, there is no pod worker to // handle the work item. Check if the DeletionTimestamp has been // set, and force a status update to trigger a pod deletion request // to the apiserver. kl.statusManager.TerminatePod(pod) } return } // Run the sync in an async worker. kl.podWorkers.UpdatePod(\u0026UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) // Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } } ä»¥ä¸‹åˆ†æ®µè¿›è¡Œåˆ†æï¼š\nå¦‚æœpodçš„çŠ¶æ€æ˜¯å¤„äºTerminatedçŠ¶æ€ï¼Œåˆ™æ‰§è¡ŒstatusManagerçš„TerminatePodæ“ä½œã€‚\n// dispatchWork starts the asynchronous sync of the pod in a pod worker. // If the pod is terminated, dispatchWork func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { // If the pod is in a terminated state, there is no pod worker to // handle the work item. Check if the DeletionTimestamp has been // set, and force a status update to trigger a pod deletion request // to the apiserver. kl.statusManager.TerminatePod(pod) } return } ... } æ‰§è¡Œpod workerçš„UpdatePodå‡½æ•°ï¼Œè¯¥å‡½æ•°æ˜¯pod workerçš„æ ¸å¿ƒå‡½æ•°ï¼Œæ¥æ‰§è¡Œpodç›¸å…³æ“ä½œã€‚å…·ä½“é€»è¾‘å¾…ä¸‹æ–‡åˆ†æã€‚\n// Run the sync in an async worker. kl.podWorkers.UpdatePod(\u0026UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) å½“åˆ›å»ºç±»å‹æ˜¯SyncPodCreateï¼ˆå³åˆ›å»ºpodçš„æ—¶å€™ï¼‰ï¼Œç»Ÿè®¡æ–°podä¸­å®¹å™¨çš„æ•°ç›®ã€‚\n// Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } 5. PodWorkers.UpdatePod PodWorkersæ˜¯ä¸€ä¸ªæ¥å£ç±»å‹ï¼š\n// PodWorkers is an abstract interface for testability. type PodWorkers interface { UpdatePod(options *UpdatePodOptions) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) ForgetWorker(uid types.UID) } å…¶ä¸­UpdatePodæ˜¯ä¸€ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼Œé€šè¿‡podUpdatesçš„channelæ¥ä¼ é€’éœ€è¦å¤„ç†çš„podä¿¡æ¯ï¼Œå¯¹äºæ–°åˆ›å»ºçš„podæ¯ä¸ªpodéƒ½ä¼šç”±ä¸€ä¸ªgoroutineæ¥æ‰§è¡ŒmanagePodLoopã€‚\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/pod_workers.go\n// Apply the new setting to the specified pod. // If the options provide an OnCompleteFunc, the function is invoked if the update is accepted. // Update requests are ignored if a kill pod request is pending. func (p *podWorkers) UpdatePod(options *UpdatePodOptions) { pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() if podUpdates, exists = p.podUpdates[uid]; !exists { // We need to have a buffer here, because checkForUpdates() method that // puts an update into channel is called from the same goroutine where // the channel is consumed. However, it is guaranteed that in such case // the channel is empty, so buffer of size 1 is enough. podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // Creating a new pod worker either means this is a new pod, or that the // kubelet just restarted. In either case the kubelet is willing to believe // the status of the pod for the first pod worker sync. See corresponding // comment in syncPod. go func() { defer runtime.HandleCrash() p.managePodLoop(podUpdates) }() } if !p.isWorking[pod.UID] { p.isWorking[pod.UID] = true podUpdates \u003c- *options } else { // if a request to kill a pod is pending, we do not let anything overwrite that request. update, found := p.lastUndeliveredWorkUpdate[pod.UID] if !found || update.UpdateType != kubetypes.SyncPodKill { p.lastUndeliveredWorkUpdate[pod.UID] = *options } } } 6. managePodLoop managePodLoopé€šè¿‡è¯»å–podUpdateschannelçš„ä¿¡æ¯ï¼Œæ‰§è¡ŒsyncPodFnå‡½æ•°ï¼Œè€ŒsyncPodFnå‡½æ•°åœ¨newPodWorkersçš„æ—¶å€™èµ‹å€¼äº†ï¼Œå³kubelet.syncPodã€‚kubelet.syncPodå…·ä½“ä»£ç é€»è¾‘å¾…åç»­æ–‡ç« å•ç‹¬åˆ†æã€‚\n// newPodWorkersä¼ å…¥syncPodå‡½æ•° klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) newPodWorkerså‡½æ•°å‚è€ƒï¼š\nfunc newPodWorkers(syncPodFn syncPodFnType, recorder record.EventRecorder, workQueue queue.WorkQueue, resyncInterval, backOffPeriod time.Duration, podCache kubecontainer.Cache) *podWorkers { return \u0026podWorkers{ podUpdates: map[types.UID]chan UpdatePodOptions{}, isWorking: map[types.UID]bool{}, lastUndeliveredWorkUpdate: map[types.UID]UpdatePodOptions{}, syncPodFn: syncPodFn, // æ„é€ ä¼ å…¥klet.syncPodå‡½æ•° recorder: recorder, workQueue: workQueue, resyncInterval: resyncInterval, backOffPeriod: backOffPeriod, podCache: podCache, } } managePodLoopå‡½æ•°å‚è€ƒï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/pod_workers.go\nfunc (p *podWorkers) managePodLoop(podUpdates \u003c-chan UpdatePodOptions) { var lastSyncTime time.Time for update := range podUpdates { err := func() error { podUID := update.Pod.UID // This is a blocking call that would return only if the cache // has an entry for the pod that is newer than minRuntimeCache // Time. This ensures the worker doesn't start syncing until // after the cache is at least newer than the finished time of // the previous sync. status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil { // This is the legacy event thrown by manage pod loop // all other events are now dispatched from syncPodFn p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, \"error determining status: %v\", err) return err } err = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, }) lastSyncTime = time.Now() return err }() // notify the call-back function if the operation succeeded or not if update.OnCompleteFunc != nil { update.OnCompleteFunc(err) } if err != nil { // IMPORTANT: we do not log errors here, the syncPodFn is responsible for logging errors glog.Errorf(\"Error syncing pod %s (%q), skipping: %v\", update.Pod.UID, format.Pod(update.Pod), err) } p.wrapUp(update.Pod.UID, err) } } 7. æ€»ç»“ syncLoopIterationåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡å‡ ç§channelæ¥å¯¹ä¸åŒç±»å‹çš„äº‹ä»¶è¿›è¡Œç›‘å¬å¹¶å¤„ç†ã€‚å…¶ä¸­channelåŒ…æ‹¬ï¼šconfigChã€plegChã€syncChã€houseKeepingChã€livenessManager.Updates()ã€‚ ä¸åŒçš„SyncHandleræ‰§è¡Œä¸åŒçš„å¢åˆ æ”¹æŸ¥æ“ä½œã€‚ å…¶ä¸­HandlePodAdditionsã€HandlePodUpdatesã€HandlePodReconcileã€HandlePodSyncséƒ½è°ƒç”¨åˆ°äº†dispatchWorkæ¥æ‰§è¡Œpodçš„ç›¸å…³æ“ä½œã€‚HandlePodCleanupsçš„podæ¸…ç†ä»»åŠ¡ï¼Œé€šè¿‡channelçš„æ–¹å¼åŠ éœ€è¦æ¸…ç†çš„podç»™podKilleræ¥æ¸…ç†ã€‚ dispatchWorkè°ƒç”¨podWorkers.UpdatePodæ‰§è¡Œå¼‚æ­¥æ“ä½œã€‚ podWorkers.UpdatePodä¸­è°ƒç”¨managePodLoopæ¥æ‰§è¡Œpodç›¸å…³æ“ä½œå¾ªç¯ã€‚ channelç±»å‹åŠä½œç”¨ï¼š\nconfigChï¼šå°†é…ç½®æ›´æ”¹çš„podåˆ†æ´¾ç»™äº‹ä»¶ç±»å‹çš„ç›¸åº”å¤„ç†ç¨‹åºå›è°ƒã€‚ plegChï¼šæ›´æ–°runtimeç¼“å­˜ï¼ŒåŒæ­¥podã€‚ syncChï¼šåŒæ­¥æ‰€æœ‰ç­‰å¾…åŒæ­¥çš„podã€‚ houseKeepingChï¼šè§¦å‘æ¸…ç†podã€‚ livenessManager.Updates()ï¼šå¯¹å¤±è´¥çš„podæˆ–è€…livenessæ£€æŸ¥å¤±è´¥çš„podè¿›è¡Œsyncæ“ä½œã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/pod_workers.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækubeletä¸­syncLoopIterationéƒ¨ â€¦","ref":"/k8s-source-code-analysis/kubelet/syncloopiteration/","tags":["æºç åˆ†æ"],"title":"kubeletæºç åˆ†æï¼ˆå››ï¼‰ä¹‹ syncLoopIteration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/deployment/","tags":"","title":"ç‰ˆæœ¬å‘å¸ƒ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/concurrency/","tags":"","title":"å¹¶å‘ç¼–ç¨‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/","tags":"","title":"å®¹å™¨ç½‘ç»œ"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æè°ƒåº¦é€»è¾‘ä¸­çš„é¢„é€‰ç­–ç•¥ï¼Œå³ç¬¬ä¸€æ­¥ç­›é€‰å‡ºç¬¦åˆpodè°ƒåº¦æ¡ä»¶çš„èŠ‚ç‚¹ã€‚\n1. è°ƒç”¨å…¥å£ é¢„é€‰ï¼Œé€šè¿‡é¢„é€‰å‡½æ•°æ¥åˆ¤æ–­æ¯ä¸ªèŠ‚ç‚¹æ˜¯å¦é€‚åˆè¢«è¯¥Podè°ƒåº¦ã€‚\ngenericScheduler.Scheduleä¸­å¯¹findNodesThatFitçš„è°ƒç”¨è¿‡ç¨‹å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\nfunc (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ... // åˆ—å‡ºæ‰€æœ‰çš„èŠ‚ç‚¹ nodes, err := nodeLister.List() if err != nil { return \"\", err } if len(nodes) == 0 { return \"\", ErrNoNodesAvailable } // Used for all fit and priority funcs. err = g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return \"\", err } trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() // è°ƒç”¨findNodesThatFitè¿‡æ»¤å‡ºé¢„é€‰èŠ‚ç‚¹ filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", \u0026FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } // metrics metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) ... } æ ¸å¿ƒä»£ç ï¼š\n// è°ƒç”¨findNodesThatFitè¿‡æ»¤å‡ºé¢„é€‰èŠ‚ç‚¹ filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 2. findNodesThatFit findNodesThatFitåŸºäºç»™å®šçš„é¢„é€‰å‡½æ•°è¿‡æ»¤nodeï¼Œæ¯ä¸ªnodeä¼ å…¥åˆ°é¢„é€‰å‡½æ•°ä¸­æ¥ç¡®å®è¯¥èŠ‚ç‚¹æ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚\nfindNodesThatFitçš„å…¥å‚æ˜¯è¢«è°ƒåº¦çš„podå’Œå½“å‰çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œè¿”å›é¢„é€‰èŠ‚ç‚¹åˆ—è¡¨å’Œé”™è¯¯ã€‚\nfindNodesThatFitåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nè®¾ç½®å¯è¡ŒèŠ‚ç‚¹çš„æ€»æ•°ï¼Œä½œä¸ºé¢„é€‰èŠ‚ç‚¹æ•°ç»„çš„å®¹é‡ï¼Œé¿å…æ€»èŠ‚ç‚¹è¿‡å¤šéœ€è¦ç­›é€‰çš„èŠ‚ç‚¹è¿‡å¤šã€‚ é€šè¿‡NodeTreeä¸æ–­è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ¥åˆ¤æ–­è¯¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³podçš„è°ƒåº¦æ¡ä»¶ã€‚ é€šè¿‡ä¹‹å‰æ³¨å†Œçš„å„ç§é¢„é€‰å‡½æ•°æ¥åˆ¤æ–­å½“å‰èŠ‚ç‚¹æ˜¯å¦ç¬¦åˆpodçš„è°ƒåº¦æ¡ä»¶ã€‚ æœ€åè¿”å›æ»¡è¶³è°ƒåº¦æ¡ä»¶çš„nodeåˆ—è¡¨ï¼Œä¾›ä¸‹ä¸€æ­¥çš„ä¼˜é€‰æ“ä½œã€‚ findNodesThatFitå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\n// Filters the nodes to find the ones that fit based on the given predicate functions // Each node is passed through the predicate functions to determine if it is a fit func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) { var filtered []*v1.Node failedPredicateMap := FailedPredicateMap{} if len(g.predicates) == 0 { filtered = nodes } else { allNodes := int32(g.cache.NodeTree().NumNodes) numNodesToFind := g.numFeasibleNodesToFind(allNodes) // Create filtered list with enough space to avoid growing it // and allow assigning. filtered = make([]*v1.Node, numNodesToFind) errs := errors.MessageCountMap{} var ( predicateResultLock sync.Mutex filteredLen int32 equivClass *equivalence.Class ) ctx, cancel := context.WithCancel(context.Background()) // We can use the same metadata producer for all nodes. meta := g.predicateMetaProducer(pod, g.cachedNodeInfoMap) if g.equivalenceCache != nil { // getEquivalenceClassInfo will return immediately if no equivalence pod found equivClass = equivalence.NewClass(pod) } checkNode := func(i int) { var nodeCache *equivalence.NodeCache nodeName := g.cache.NodeTree().Next() if g.equivalenceCache != nil { nodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName) } fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.cache, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) if err != nil { predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return } if fits { length := atomic.AddInt32(\u0026filteredLen, 1) if length \u003e numNodesToFind { cancel() atomic.AddInt32(\u0026filteredLen, -1) } else { filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } else { predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() } } // Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) filtered = filtered[:filteredLen] if len(errs) \u003e 0 { return []*v1.Node{}, FailedPredicateMap{}, errors.CreateAggregateFromMessageCountMap(errs) } } if len(filtered) \u003e 0 \u0026\u0026 len(g.extenders) != 0 { for _, extender := range g.extenders { if !extender.IsInterested(pod) { continue } filteredList, failedMap, err := extender.Filter(pod, filtered, g.cachedNodeInfoMap) if err != nil { if extender.IsIgnorable() { glog.Warningf(\"Skipping extender %v as it returned error %v and has ignorable flag set\", extender, err) continue } else { return []*v1.Node{}, FailedPredicateMap{}, err } } for failedNodeName, failedMsg := range failedMap { if _, found := failedPredicateMap[failedNodeName]; !found { failedPredicateMap[failedNodeName] = []algorithm.PredicateFailureReason{} } failedPredicateMap[failedNodeName] = append(failedPredicateMap[failedNodeName], predicates.NewFailureReason(failedMsg)) } filtered = filteredList if len(filtered) == 0 { break } } } return filtered, failedPredicateMap, nil } ä»¥ä¸‹å¯¹findNodesThatFitåˆ†æ®µåˆ†æã€‚\n3. numFeasibleNodesToFind findNodesThatFitå…ˆåŸºäºæ‰€æœ‰çš„èŠ‚ç‚¹æ‰¾å‡ºå¯è¡Œçš„èŠ‚ç‚¹æ˜¯æ€»æ•°ã€‚numFeasibleNodesToFindçš„ä½œç”¨ä¸»è¦æ˜¯é¿å…å½“èŠ‚ç‚¹è¿‡å¤šï¼ˆè¶…è¿‡100ï¼‰å½±å“è°ƒåº¦çš„æ•ˆç‡ã€‚\nallNodes := int32(g.cache.NodeTree().NumNodes) numNodesToFind := g.numFeasibleNodesToFind(allNodes) // Create filtered list with enough space to avoid growing it // and allow assigning. filtered = make([]*v1.Node, numNodesToFind) numFeasibleNodesToFindåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nå¦‚æœæ‰€æœ‰çš„nodeèŠ‚ç‚¹å°äºminFeasibleNodesToFind(å½“å‰é»˜è®¤ä¸º100)åˆ™è¿”å›èŠ‚ç‚¹æ•°ã€‚ å¦‚æœèŠ‚ç‚¹æ•°è¶…100ï¼Œåˆ™å–æŒ‡å®šè®¡åˆ†çš„ç™¾åˆ†æ¯”çš„èŠ‚ç‚¹æ•°ï¼Œå½“è¯¥ç™¾åˆ†æ¯”åçš„æ•°ç›®ä»å°äºminFeasibleNodesToFindï¼Œåˆ™è¿”å›minFeasibleNodesToFindã€‚ å¦‚æœç™¾åˆ†æ¯”åçš„æ•°ç›®å¤§äºminFeasibleNodesToFindï¼Œåˆ™è¿”å›è¯¥ç™¾åˆ†æ¯”ã€‚ // numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops // its search for more feasible nodes. func (g *genericScheduler) numFeasibleNodesToFind(numAllNodes int32) int32 { if numAllNodes \u003c minFeasibleNodesToFind || g.percentageOfNodesToScore \u003c= 0 || g.percentageOfNodesToScore \u003e= 100 { return numAllNodes } numNodes := numAllNodes * g.percentageOfNodesToScore / 100 if numNodes \u003c minFeasibleNodesToFind { return minFeasibleNodesToFind } return numNodes } 4. checkNode checkNodeæ˜¯ä¸€ä¸ªæ ¡éªŒnodeæ˜¯å¦ç¬¦åˆè¦æ±‚çš„å‡½æ•°ï¼Œå…¶ä¸­å®é™…è°ƒç”¨åˆ°çš„æ ¸å¿ƒå‡½æ•°æ˜¯podFitsOnNodeã€‚å†é€šè¿‡workqueueå¹¶å‘æ‰§è¡ŒcheckNodeæ“ä½œã€‚\ncheckNodeä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡cacheä¸­çš„nodeTreeä¸æ–­è·å–ä¸‹ä¸€ä¸ªnodeã€‚ å°†å½“å‰nodeå’Œpodä¼ å…¥podFitsOnNodeåˆ¤æ–­å½“å‰nodeæ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚ å¦‚æœå½“å‰nodeç¬¦åˆè¦æ±‚å°±å°†å½“å‰nodeåŠ å…¥é¢„é€‰èŠ‚ç‚¹çš„æ•°ç»„ä¸­filteredã€‚ å¦‚æœå½“å‰nodeä¸æ»¡è¶³è¦æ±‚ï¼Œåˆ™åŠ å…¥åˆ°å¤±è´¥çš„æ•°ç»„ä¸­ï¼Œå¹¶è®°å½•åŸå› ã€‚ é€šè¿‡workqueue.ParallelizeUntilå¹¶å‘æ‰§è¡ŒcheckNodeå‡½æ•°ï¼Œä¸€æ—¦æ‰¾åˆ°é…ç½®çš„å¯è¡ŒèŠ‚ç‚¹æ•°ï¼Œå°±åœæ­¢æœç´¢æ›´å¤šèŠ‚ç‚¹ã€‚ checkNode := func(i int) { var nodeCache *equivalence.NodeCache nodeName := g.cache.NodeTree().Next() if g.equivalenceCache != nil { nodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName) } fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.cache, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) if err != nil { predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return } if fits { length := atomic.AddInt32(\u0026filteredLen, 1) if length \u003e numNodesToFind { cancel() atomic.AddInt32(\u0026filteredLen, -1) } else { filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } else { predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() } } workqueueçš„å¹¶å‘æ“ä½œï¼š\n// Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) ParallelizeUntilå…·ä½“ä»£ç å¦‚ä¸‹ï¼š\n// ParallelizeUntil is a framework that allows for parallelizing N // independent pieces of work until done or the context is canceled. func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) { var stop \u003c-chan struct{} if ctx != nil { stop = ctx.Done() } toProcess := make(chan int, pieces) for i := 0; i \u003c pieces; i++ { toProcess \u003c- i } close(toProcess) if pieces \u003c workers { workers = pieces } wg := sync.WaitGroup{} wg.Add(workers) for i := 0; i \u003c workers; i++ { go func() { defer utilruntime.HandleCrash() defer wg.Done() for piece := range toProcess { select { case \u003c-stop: return default: doWorkPiece(piece) } } }() } wg.Wait() } 5. podFitsOnNode podFitsOnNodeä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\npodFitsOnNodeä¼šæ£€æŸ¥ç»™å®šçš„æŸä¸ªNodeæ˜¯å¦æ»¡è¶³é¢„é€‰çš„å‡½æ•°ã€‚\nå¯¹äºç»™å®šçš„podï¼ŒpodFitsOnNodeä¼šæ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„podå­˜åœ¨ï¼Œå°½é‡å¤ç”¨ç¼“å­˜è¿‡çš„é¢„é€‰ç»“æœã€‚\npodFitsOnNodeä¸»è¦åœ¨Scheduleï¼ˆè°ƒåº¦ï¼‰å’ŒPreemptï¼ˆæŠ¢å ï¼‰çš„æ—¶å€™è¢«è°ƒç”¨ã€‚\nå½“åœ¨Scheduleä¸­è¢«è°ƒç”¨çš„æ—¶å€™ï¼Œä¸»è¦åˆ¤æ–­æ˜¯å¦å¯ä»¥è¢«è°ƒåº¦åˆ°å½“å‰èŠ‚ç‚¹ï¼Œä¾æ®ä¸ºå½“å‰èŠ‚ç‚¹ä¸Šæ‰€æœ‰å·²å­˜åœ¨çš„podåŠè¢«æåè¦è¿è¡Œåˆ°è¯¥èŠ‚ç‚¹çš„å…·æœ‰ç›¸ç­‰æˆ–æ›´é«˜ä¼˜å…ˆçº§çš„podã€‚\nå½“åœ¨Preemptä¸­è¢«è°ƒç”¨çš„æ—¶å€™ï¼Œå³å‘ç”ŸæŠ¢å çš„æ—¶å€™ï¼Œé€šè¿‡SelectVictimsOnNodeå‡½æ•°é€‰å‡ºéœ€è¦è¢«ç§»é™¤çš„podï¼Œç§»é™¤åç„¶åå°†é¢„è°ƒåº¦çš„podè°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚\npodFitsOnNodeåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\néå†ä¹‹å‰æ³¨å†Œå¥½çš„é¢„é€‰ç­–ç•¥predicates.Orderingï¼Œå¹¶è·å–é¢„é€‰ç­–ç•¥çš„æ‰§è¡Œå‡½æ•°ã€‚ éå†æ‰§è¡Œæ¯ä¸ªé¢„é€‰å‡½æ•°ï¼Œå¹¶è¿”å›æ˜¯å¦åˆé€‚ï¼Œé¢„é€‰å¤±è´¥çš„åŸå› å’Œé”™è¯¯ã€‚ å¦‚æœé¢„é€‰å‡½æ•°æ‰§è¡Œçš„ç»“æœä¸åˆé€‚ï¼Œåˆ™åŠ å…¥é¢„é€‰å¤±è´¥çš„æ•°ç»„ä¸­ã€‚ æœ€åè¿”å›é¢„é€‰å¤±è´¥çš„ä¸ªæ•°æ˜¯å¦ä¸º0ï¼Œå’Œé¢„é€‰å¤±è´¥çš„åŸå› ã€‚ å…¥å‚ï¼š\npod PredicateMetadata NodeInfo predicateFuncs schedulercache.Cache nodeCache SchedulingQueue alwaysCheckAllPredicates equivClass å‡ºå‚ï¼š\nfit PredicateFailureReason å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\n// podFitsOnNode checks whether a node given by NodeInfo satisfies the given predicate functions. // For given pod, podFitsOnNode will check if any equivalent pod exists and try to reuse its cached // predicate results as possible. // This function is called from two different places: Schedule and Preempt. // When it is called from Schedule, we want to test whether the pod is schedulable // on the node with all the existing pods on the node plus higher and equal priority // pods nominated to run on the node. // When it is called from Preempt, we should remove the victims of preemption and // add the nominated pods. Removal of the victims is done by SelectVictimsOnNode(). // It removes victims from meta and NodeInfo before calling this function. func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, cache schedulercache.Cache, nodeCache *equivalence.NodeCache, queue SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) { var ( eCacheAvailable bool failedPredicates []algorithm.PredicateFailureReason ) podsAdded := false // We run predicates twice in some cases. If the node has greater or equal priority // nominated pods, we run them when those pods are added to meta and nodeInfo. // If all predicates succeed in this pass, we run them again when these // nominated pods are not added. This second pass is necessary because some // predicates such as inter-pod affinity may not pass without the nominated pods. // If there are no nominated pods for the node or if the first run of the // predicates fail, we don't run the second pass. // We consider only equal or higher priority pods in the first pass, because // those are the current \"pod\" must yield to them and not take a space opened // for running them. It is ok if the current \"pod\" take resources freed for // lower priority pods. // Requiring that the new pod is schedulable in both circumstances ensures that // we are making a conservative decision: predicates like resources and inter-pod // anti-affinity are more likely to fail when the nominated pods are treated // as running, while predicates like pod affinity are more likely to fail when // the nominated pods are treated as not running. We can't just assume the // nominated pods are running because they are not running right now and in fact, // they may end up getting scheduled to a different node. for i := 0; i \u003c 2; i++ { metaToUse := meta nodeInfoToUse := info if i == 0 { podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(util.GetPodPriority(pod), meta, info, queue) } else if !podsAdded || len(failedPredicates) != 0 { break } // Bypass eCache if node has any nominated pods. // TODO(bsalamat): consider using eCache and adding proper eCache invalidations // when pods are nominated or their nominations change. eCacheAvailable = equivClass != nil \u0026\u0026 nodeCache != nil \u0026\u0026 !podsAdded for _, predicateKey := range predicates.Ordering() { var ( fit bool reasons []algorithm.PredicateFailureReason err error ) //TODO (yastij) : compute average predicate restrictiveness to export it as Prometheus metric if predicate, exist := predicateFuncs[predicateKey]; exist { if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, pod, metaToUse, nodeInfoToUse, equivClass, cache) } else { fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } if err != nil { return false, []algorithm.PredicateFailureReason{}, err } if !fit { // eCache is available and valid, and predicates result is unfit, record the fail reasons failedPredicates = append(failedPredicates, reasons...) // if alwaysCheckAllPredicates is false, short circuit all predicates when one predicate fails. if !alwaysCheckAllPredicates { glog.V(5).Infoln(\"since alwaysCheckAllPredicates has not been set, the predicate \" + \"evaluation is short circuited and there are chances \" + \"of other predicates failing as well.\") break } } } } } return len(failedPredicates) == 0, failedPredicates, nil } 5.1. predicateFuncs æ ¹æ®ä¹‹å‰åˆæ³¨å†Œå¥½çš„é¢„é€‰ç­–ç•¥å‡½æ•°æ¥æ‰§è¡Œé¢„é€‰ï¼Œåˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦ç¬¦åˆè°ƒåº¦ã€‚\nfor _, predicateKey := range predicates.Ordering() { if predicate, exist := predicateFuncs[predicateKey]; exist { if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, pod, metaToUse, nodeInfoToUse, equivClass, cache) } else { fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } é¢„é€‰ç­–ç•¥å¦‚ä¸‹ï¼š\nvar ( predicatesOrdering = []string{CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred} ) 6. PodFitsResources ä»¥ä¸‹ä»¥PodFitsResourcesè¿™ä¸ªé¢„é€‰å‡½æ•°ä¸ºä¾‹åšåˆ†æï¼Œå…¶ä»–é‡è¦çš„é¢„é€‰å‡½æ•°å¾…åç»­å•ç‹¬åˆ†æã€‚\nPodFitsResourcesç”¨æ¥æ£€æŸ¥ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºæ¥è¿è¡Œå½“å‰çš„podï¼ŒåŒ…æ‹¬CPUã€å†…å­˜ã€GPUç­‰ã€‚\nPodFitsResourcesåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nåˆ¤æ–­å½“å‰èŠ‚ç‚¹ä¸Špodæ€»æ•°åŠ ä¸Šé¢„è°ƒåº¦podä¸ªæ•°æ˜¯å¦å¤§äºnodeçš„å¯åˆ†é…podæ€»æ•°ï¼Œè‹¥æ˜¯åˆ™ä¸å…è®¸è°ƒåº¦ã€‚ åˆ¤æ–­podçš„requestå€¼æ˜¯å¦éƒ½ä¸º0ï¼Œè‹¥æ˜¯åˆ™å…è®¸è°ƒåº¦ã€‚ åˆ¤æ–­podçš„requestå€¼åŠ ä¸Šå½“å‰nodeä¸Šæ‰€æœ‰podçš„requestå€¼æ€»å’Œæ˜¯å¦å¤§äºnodeçš„å¯åˆ†é…èµ„æºï¼Œè‹¥æ˜¯åˆ™ä¸å…è®¸è°ƒåº¦ã€‚ åˆ¤æ–­podçš„æ‹“å±•èµ„æºrequestå€¼åŠ ä¸Šå½“å‰nodeä¸Šæ‰€æœ‰podå¯¹åº”çš„requestå€¼æ€»å’Œæ˜¯å¦å¤§äºnodeå¯¹åº”çš„å¯åˆ†é…èµ„æºï¼Œè‹¥æ˜¯åˆ™ä¸å…è®¸è°ƒåº¦ã€‚ PodFitsResourcesçš„æ³¨å†Œä»£ç å¦‚ä¸‹ï¼š\nfactory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) PodFitsResourceså…¥å‚ï¼š\npod\nnodeInfo\nPredicateMetadata\nPodFitsResourceså‡ºå‚ï¼š\nfit PredicateFailureReason PodFitsResourceså®Œæ•´ä»£ç ï¼š\næ­¤éƒ¨åˆ†çš„ä»£ç ä½äºpkg/scheduler/algorithm/predicates/predicates.go\n// PodFitsResources checks if a node has sufficient resources, such as cpu, memory, gpu, opaque int resources etc to run a pod. // First return value indicates whether a node has sufficient resources to run a pod while the second return value indicates the // predicate failure reasons if the node has insufficient resources to run the pod. func PodFitsResources(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } var predicateFails []algorithm.PredicateFailureReason allowedPodNumber := nodeInfo.AllowedPodNumber() if len(nodeInfo.Pods())+1 \u003e allowedPodNumber { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber))) } // No extended resources should be ignored by default. ignoredExtendedResources := sets.NewString() var podRequest *schedulercache.Resource if predicateMeta, ok := meta.(*predicateMetadata); ok { podRequest = predicateMeta.podRequest if predicateMeta.ignoredExtendedResources != nil { ignoredExtendedResources = predicateMeta.ignoredExtendedResources } } else { // We couldn't parse metadata - fallback to computing it. podRequest = GetResourceRequest(pod) } if podRequest.MilliCPU == 0 \u0026\u0026 podRequest.Memory == 0 \u0026\u0026 podRequest.EphemeralStorage == 0 \u0026\u0026 len(podRequest.ScalarResources) == 0 { return len(predicateFails) == 0, predicateFails, nil } allocatable := nodeInfo.AllocatableResource() if allocatable.MilliCPU \u003c podRequest.MilliCPU+nodeInfo.RequestedResource().MilliCPU { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceCPU, podRequest.MilliCPU, nodeInfo.RequestedResource().MilliCPU, allocatable.MilliCPU)) } if allocatable.Memory \u003c podRequest.Memory+nodeInfo.RequestedResource().Memory { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceMemory, podRequest.Memory, nodeInfo.RequestedResource().Memory, allocatable.Memory)) } if allocatable.EphemeralStorage \u003c podRequest.EphemeralStorage+nodeInfo.RequestedResource().EphemeralStorage { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceEphemeralStorage, podRequest.EphemeralStorage, nodeInfo.RequestedResource().EphemeralStorage, allocatable.EphemeralStorage)) } for rName, rQuant := range podRequest.ScalarResources { if v1helper.IsExtendedResourceName(rName) { // If this resource is one of the extended resources that should be // ignored, we will skip checking it. if ignoredExtendedResources.Has(string(rName)) { continue } } if allocatable.ScalarResources[rName] \u003c rQuant+nodeInfo.RequestedResource().ScalarResources[rName] { predicateFails = append(predicateFails, NewInsufficientResourceError(rName, podRequest.ScalarResources[rName], nodeInfo.RequestedResource().ScalarResources[rName], allocatable.ScalarResources[rName])) } } if glog.V(10) { if len(predicateFails) == 0 { // We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is // not logged. There is visible performance gain from it. glog.Infof(\"Schedule Pod %+v on Node %+v is allowed, Node is running only %v out of %v Pods.\", podName(pod), node.Name, len(nodeInfo.Pods()), allowedPodNumber) } } return len(predicateFails) == 0, predicateFails, nil } 6.1. NodeInfo NodeInfoæ˜¯nodeçš„èšåˆä¿¡æ¯ï¼Œä¸»è¦åŒ…æ‹¬ï¼š\nnodeï¼šk8s nodeçš„ç»“æ„ä½“ podsï¼šå½“å‰nodeä¸Špodçš„æ•°é‡ requestedResourceï¼šå½“å‰nodeä¸Šæ‰€æœ‰podçš„requestæ€»å’Œ allocatableResourceï¼šnodeçš„å®é™…æ‰€æœ‰çš„å¯åˆ†é…èµ„æº(å¯¹åº”äºNode.Status.Allocatable.*)ï¼Œå¯ç†è§£ä¸ºnodeçš„èµ„æºæ€»é‡ã€‚ æ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/cache/node_info.go\n// NodeInfo is node level aggregated information. type NodeInfo struct { // Overall node information. node *v1.Node pods []*v1.Pod podsWithAffinity []*v1.Pod usedPorts util.HostPortInfo // Total requested resource of all pods on this node. // It includes assumed pods which scheduler sends binding to apiserver but // didn't get it as scheduled yet. requestedResource *Resource nonzeroRequest *Resource // We store allocatedResources (which is Node.Status.Allocatable.*) explicitly // as int64, to avoid conversions and accessing map. allocatableResource *Resource // Cached taints of the node for faster lookup. taints []v1.Taint taintsErr error // imageStates holds the entry of an image if and only if this image is on the node. The entry can be used for // checking an image's existence and advanced usage (e.g., image locality scheduling policy) based on the image // state information. imageStates map[string]*ImageStateSummary // TransientInfo holds the information pertaining to a scheduling cycle. This will be destructed at the end of // scheduling cycle. // TODO: @ravig. Remove this once we have a clear approach for message passing across predicates and priorities. TransientInfo *transientSchedulerInfo // Cached conditions of node for faster lookup. memoryPressureCondition v1.ConditionStatus diskPressureCondition v1.ConditionStatus pidPressureCondition v1.ConditionStatus // Whenever NodeInfo changes, generation is bumped. // This is used to avoid cloning it if the object didn't change. generation int64 } 6.2. Resource Resourceæ˜¯å¯è®¡ç®—èµ„æºçš„é›†åˆä½“ã€‚ä¸»è¦åŒ…æ‹¬ï¼š\nMilliCPU Memory EphemeralStorage AllowedPodNumberï¼šå…è®¸çš„podæ€»æ•°(å¯¹åº”äºNode.Status.Allocatable.Pods().Value())ï¼Œä¸€èˆ¬ä¸º110ã€‚ ScalarResources // Resource is a collection of compute resource. type Resource struct { MilliCPU int64 Memory int64 EphemeralStorage int64 // We store allowedPodNumber (which is Node.Status.Allocatable.Pods().Value()) // explicitly as int, to avoid conversions and improve performance. AllowedPodNumber int // ScalarResources ScalarResources map[v1.ResourceName]int64 } ä»¥ä¸‹åˆ†æpodFitsOnNodeçš„å…·ä½“æµç¨‹ã€‚\n6.3. allowedPodNumber é¦–å…ˆè·å–èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œå…ˆåˆ¤æ–­å¦‚æœè¯¥èŠ‚ç‚¹å½“å‰æ‰€æœ‰çš„podçš„ä¸ªæ•°åŠ ä¸Šå½“å‰é¢„è°ƒåº¦çš„podæ˜¯å¦ä¼šå¤§äºè¯¥èŠ‚ç‚¹å…è®¸çš„podçš„æ€»æ•°ï¼Œä¸€èˆ¬ä¸º110ä¸ªã€‚å¦‚æœè¶…è¿‡ï¼Œåˆ™predicateFailsæ•°ç»„å¢åŠ 1ï¼Œå³å½“å‰èŠ‚ç‚¹ä¸é€‚åˆè¯¥podã€‚\nnode := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } var predicateFails []algorithm.PredicateFailureReason allowedPodNumber := nodeInfo.AllowedPodNumber() if len(nodeInfo.Pods())+1 \u003e allowedPodNumber { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber))) } 6.4. podRequest å¦‚æœpodRequestéƒ½ä¸º0ï¼Œåˆ™å…è®¸è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›ç»“æœã€‚\nif podRequest.MilliCPU == 0 \u0026\u0026 podRequest.Memory == 0 \u0026\u0026 podRequest.EphemeralStorage == 0 \u0026\u0026 len(podRequest.ScalarResources) == 0 { return len(predicateFails) == 0, predicateFails, nil } 6.5. AllocatableResource å¦‚æœå½“å‰é¢„è°ƒåº¦çš„podçš„requestèµ„æºåŠ ä¸Šå½“å‰nodeä¸Šæ‰€æœ‰podçš„requestæ€»å’Œå¤§äºè¯¥nodeçš„å¯åˆ†é…èµ„æºæ€»é‡ï¼Œåˆ™ä¸å…è®¸è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›ç»“æœã€‚å…¶ä¸­requestèµ„æºåŒ…æ‹¬CPUã€å†…å­˜ã€storageã€‚\nallocatable := nodeInfo.AllocatableResource() if allocatable.MilliCPU \u003c podRequest.MilliCPU+nodeInfo.RequestedResource().MilliCPU { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceCPU, podRequest.MilliCPU, nodeInfo.RequestedResource().MilliCPU, allocatable.MilliCPU)) } if allocatable.Memory \u003c podRequest.Memory+nodeInfo.RequestedResource().Memory { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceMemory, podRequest.Memory, nodeInfo.RequestedResource().Memory, allocatable.Memory)) } if allocatable.EphemeralStorage \u003c podRequest.EphemeralStorage+nodeInfo.RequestedResource().EphemeralStorage { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceEphemeralStorage, podRequest.EphemeralStorage, nodeInfo.RequestedResource().EphemeralStorage, allocatable.EphemeralStorage)) } 6.6. ScalarResources åˆ¤æ–­å…¶ä»–æ‹“å±•çš„æ ‡é‡èµ„æºï¼Œæ˜¯å¦è¯¥podçš„requestå€¼åŠ ä¸Šå½“å‰nodeä¸Šæ‰€æœ‰podçš„å¯¹åº”èµ„æºçš„requestæ€»å’Œå¤§äºè¯¥nodeä¸Šå¯¹åº”èµ„æºçš„å¯åˆ†é…æ€»é‡ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¸å…è®¸è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ã€‚\nfor rName, rQuant := range podRequest.ScalarResources { if v1helper.IsExtendedResourceName(rName) { // If this resource is one of the extended resources that should be // ignored, we will skip checking it. if ignoredExtendedResources.Has(string(rName)) { continue } } if allocatable.ScalarResources[rName] \u003c rQuant+nodeInfo.RequestedResource().ScalarResources[rName] { predicateFails = append(predicateFails, NewInsufficientResourceError(rName, podRequest.ScalarResources[rName], nodeInfo.RequestedResource().ScalarResources[rName], allocatable.ScalarResources[rName])) } } 7. æ€»ç»“ findNodesThatFitåŸºäºç»™å®šçš„é¢„é€‰å‡½æ•°è¿‡æ»¤nodeï¼Œæ¯ä¸ªnodeä¼ å…¥åˆ°é¢„é€‰å‡½æ•°ä¸­æ¥ç¡®å®è¯¥èŠ‚ç‚¹æ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚\nfindNodesThatFitçš„å…¥å‚æ˜¯è¢«è°ƒåº¦çš„podå’Œå½“å‰çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œè¿”å›é¢„é€‰èŠ‚ç‚¹åˆ—è¡¨å’Œé”™è¯¯ã€‚\nfindNodesThatFitåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nè®¾ç½®å¯è¡ŒèŠ‚ç‚¹çš„æ€»æ•°ï¼Œä½œä¸ºé¢„é€‰èŠ‚ç‚¹æ•°ç»„çš„å®¹é‡ï¼Œé¿å…æ€»èŠ‚ç‚¹è¿‡å¤šå¯¼è‡´éœ€è¦ç­›é€‰çš„èŠ‚ç‚¹è¿‡å¤šï¼Œæ•ˆç‡ä½ã€‚ é€šè¿‡NodeTreeä¸æ–­è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ¥åˆ¤æ–­è¯¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³podçš„è°ƒåº¦æ¡ä»¶ã€‚ é€šè¿‡ä¹‹å‰æ³¨å†Œçš„å„ç§é¢„é€‰å‡½æ•°æ¥åˆ¤æ–­å½“å‰èŠ‚ç‚¹æ˜¯å¦ç¬¦åˆpodçš„è°ƒåº¦æ¡ä»¶ã€‚ æœ€åè¿”å›æ»¡è¶³è°ƒåº¦æ¡ä»¶çš„nodeåˆ—è¡¨ï¼Œä¾›ä¸‹ä¸€æ­¥çš„ä¼˜é€‰æ“ä½œã€‚ 7.1. checkNode checkNodeæ˜¯ä¸€ä¸ªæ ¡éªŒnodeæ˜¯å¦ç¬¦åˆè¦æ±‚çš„å‡½æ•°ï¼Œå…¶ä¸­å®é™…è°ƒç”¨åˆ°çš„æ ¸å¿ƒå‡½æ•°æ˜¯podFitsOnNodeã€‚å†é€šè¿‡workqueueå¹¶å‘æ‰§è¡ŒcheckNodeæ“ä½œã€‚\ncheckNodeä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\né€šè¿‡cacheä¸­çš„nodeTreeä¸æ–­è·å–ä¸‹ä¸€ä¸ªnodeã€‚ å°†å½“å‰nodeå’Œpodä¼ å…¥podFitsOnNodeåˆ¤æ–­å½“å‰nodeæ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚ å¦‚æœå½“å‰nodeç¬¦åˆè¦æ±‚å°±å°†å½“å‰nodeåŠ å…¥é¢„é€‰èŠ‚ç‚¹çš„æ•°ç»„ä¸­filteredã€‚ å¦‚æœå½“å‰nodeä¸æ»¡è¶³è¦æ±‚ï¼Œåˆ™åŠ å…¥åˆ°å¤±è´¥çš„æ•°ç»„ä¸­ï¼Œå¹¶è®°å½•åŸå› ã€‚ é€šè¿‡workqueue.ParallelizeUntilå¹¶å‘æ‰§è¡ŒcheckNodeå‡½æ•°ï¼Œä¸€æ—¦æ‰¾åˆ°é…ç½®çš„å¯è¡ŒèŠ‚ç‚¹æ•°ï¼Œå°±åœæ­¢æœç´¢æ›´å¤šèŠ‚ç‚¹ã€‚ 7.2. podFitsOnNode å…¶ä¸­ä¼šè°ƒç”¨åˆ°æ ¸å¿ƒå‡½æ•°podFitsOnNodeã€‚\npodFitsOnNodeä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š\npodFitsOnNodeä¼šæ£€æŸ¥ç»™å®šçš„æŸä¸ªNodeæ˜¯å¦æ»¡è¶³é¢„é€‰çš„å‡½æ•°ã€‚\nå¯¹äºç»™å®šçš„podï¼ŒpodFitsOnNodeä¼šæ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„podå­˜åœ¨ï¼Œå°½é‡å¤ç”¨ç¼“å­˜è¿‡çš„é¢„é€‰ç»“æœã€‚\npodFitsOnNodeä¸»è¦åœ¨Scheduleï¼ˆè°ƒåº¦ï¼‰å’ŒPreemptï¼ˆæŠ¢å ï¼‰çš„æ—¶å€™è¢«è°ƒç”¨ã€‚\nå½“åœ¨Scheduleä¸­è¢«è°ƒç”¨çš„æ—¶å€™ï¼Œä¸»è¦åˆ¤æ–­æ˜¯å¦å¯ä»¥è¢«è°ƒåº¦åˆ°å½“å‰èŠ‚ç‚¹ï¼Œä¾æ®ä¸ºå½“å‰èŠ‚ç‚¹ä¸Šæ‰€æœ‰å·²å­˜åœ¨çš„podåŠè¢«æåè¦è¿è¡Œåˆ°è¯¥èŠ‚ç‚¹çš„å…·æœ‰ç›¸ç­‰æˆ–æ›´é«˜ä¼˜å…ˆçº§çš„podã€‚\nå½“åœ¨Preemptä¸­è¢«è°ƒç”¨çš„æ—¶å€™ï¼Œå³å‘ç”ŸæŠ¢å çš„æ—¶å€™ï¼Œé€šè¿‡SelectVictimsOnNodeå‡½æ•°é€‰å‡ºéœ€è¦è¢«ç§»é™¤çš„podï¼Œç§»é™¤åç„¶åå°†é¢„è°ƒåº¦çš„podè°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚\npodFitsOnNodeåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\néå†ä¹‹å‰æ³¨å†Œå¥½çš„é¢„é€‰ç­–ç•¥predicates.Orderingï¼Œå¹¶è·å–é¢„é€‰ç­–ç•¥çš„æ‰§è¡Œå‡½æ•°ã€‚ éå†æ‰§è¡Œæ¯ä¸ªé¢„é€‰å‡½æ•°ï¼Œå¹¶è¿”å›æ˜¯å¦åˆé€‚ï¼Œé¢„é€‰å¤±è´¥çš„åŸå› å’Œé”™è¯¯ã€‚ å¦‚æœé¢„é€‰å‡½æ•°æ‰§è¡Œçš„ç»“æœä¸åˆé€‚ï¼Œåˆ™åŠ å…¥é¢„é€‰å¤±è´¥çš„æ•°ç»„ä¸­ã€‚ æœ€åè¿”å›é¢„é€‰å¤±è´¥çš„ä¸ªæ•°æ˜¯å¦ä¸º0ï¼Œå’Œé¢„é€‰å¤±è´¥çš„åŸå› ã€‚ 7.3. PodFitsResources æœ¬æ–‡åªç¤ºä¾‹åˆ†æäº†å…¶ä¸­ä¸€ä¸ªé‡è¦çš„é¢„é€‰å‡½æ•°ï¼šPodFitsResources\nPodFitsResourcesç”¨æ¥æ£€æŸ¥ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºæ¥è¿è¡Œå½“å‰çš„podï¼ŒåŒ…æ‹¬CPUã€å†…å­˜ã€GPUç­‰ã€‚\nPodFitsResourcesåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nåˆ¤æ–­å½“å‰èŠ‚ç‚¹ä¸Špodæ€»æ•°åŠ ä¸Šé¢„è°ƒåº¦podä¸ªæ•°æ˜¯å¦å¤§äºnodeçš„å¯åˆ†é…podæ€»æ•°ï¼Œè‹¥æ˜¯åˆ™ä¸å…è®¸è°ƒåº¦ã€‚ åˆ¤æ–­podçš„requestå€¼æ˜¯å¦éƒ½ä¸º0ï¼Œè‹¥æ˜¯åˆ™å…è®¸è°ƒåº¦ã€‚ åˆ¤æ–­podçš„requestå€¼åŠ ä¸Šå½“å‰nodeä¸Šæ‰€æœ‰podçš„requestå€¼æ€»å’Œæ˜¯å¦å¤§äºnodeçš„å¯åˆ†é…èµ„æºï¼Œè‹¥æ˜¯åˆ™ä¸å…è®¸è°ƒåº¦ã€‚ åˆ¤æ–­podçš„æ‹“å±•èµ„æºrequestå€¼åŠ ä¸Šå½“å‰nodeä¸Šæ‰€æœ‰podå¯¹åº”çš„requestå€¼æ€»å’Œæ˜¯å¦å¤§äºnodeå¯¹åº”çš„å¯åˆ†é…èµ„æºï¼Œè‹¥æ˜¯åˆ™ä¸å…è®¸è°ƒåº¦ã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithm/predicates/predicates.go\n","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æè°ƒåº¦é€»è¾‘ä¸­çš„é¢„é€‰ç­–ç•¥ï¼Œå³ç¬¬ä¸€æ­¥ç­›é€‰å‡ºç¬¦åˆpodè°ƒåº¦æ¡ä»¶çš„èŠ‚ç‚¹ã€‚ â€¦","ref":"/k8s-source-code-analysis/kube-scheduler/findnodesthatfit/","tags":["æºç åˆ†æ"],"title":"kube-scheduleræºç åˆ†æï¼ˆå››ï¼‰ä¹‹ é¢„é€‰ç­–ç•¥"},{"body":"1. netplanç®€ä»‹ netplanæ˜¯ä¸€ä¸ªlinuxç½‘ç»œé…ç½®çš„æ¸²æŸ“å™¨ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªç½‘ç»œé…ç½®çš„yamlæ–‡ä»¶ï¼Œnetplanå°†è¯¥æ–‡ä»¶æ¸²æŸ“æˆlinux networkæ‰€éœ€çš„é…ç½®ã€‚\n2. netplanåŸç† netplanè¯»å–/etc/netplan/*.yamlçš„é…ç½®æ–‡ä»¶ï¼ŒNetplanåœ¨/runï¼ˆä¾‹å¦‚ï¼š/run/systemd/network/ï¼‰ä¸­ç”Ÿæˆç‰¹å®šäºåç«¯çš„é…ç½®æ–‡ä»¶ï¼Œå°†è®¾å¤‡çš„æ§åˆ¶äº¤ç»™ç‰¹å®šçš„ç½‘ç»œå®ˆæŠ¤è¿›ç¨‹ã€‚\n3. netplané…ç½® å…·ä½“çš„netplané…ç½®å¯ä»¥å‚è€ƒï¼š\nhttps://netplan.readthedocs.io/en/stable/howto/ https://netplan.readthedocs.io/en/stable/netplan-yaml/ é€šç”¨é…ç½®ï¼š\nnetwork: version: NUMBER # å¿…å¡« renderer: STRING # å¿…å¡« networkdï¼ˆé»˜è®¤ï¼‰æˆ–è€…NetworkManager ethernets: MAPPING # ç½‘å¡ç›¸å…³é…ç½® bonds: MAPPING # bondç›¸å…³é…ç½® vlans: MAPPING # VLANç›¸å…³é…ç½® bridges: MAPPING dummy-devices: MAPPING modems: MAPPING tunnels: MAPPING virtual-ethernets: MAPPING vrfs: MAPPING wifis: MAPPING nm-devices: MAPPING 3.1. ä½¿ç”¨ç›´è¿ç½‘å…³é…ç½® å…·ä½“å¯å‚è€ƒï¼šhttps://netplan.readthedocs.io/en/stable/netplan-yaml/#routing\nå¦‚æœæ˜¯æ²¡æœ‰bondè®¾ç½®åŠVLANè®¾ç½®ï¼Œåˆ™å¯ç”¨ä»¥ä¸‹ç½‘å…³é…ç½®ã€‚\nå‚æ•°è¯´æ˜ï¼š\naddressesï¼šç½‘å¡çš„IPåœ°å€ routes è·¯ç”±åœ°å€ï¼Œä¸€èˆ¬toåé¢å¯¹åº”0.0.0.0/0ï¼Œ viaå¯¹åº”ç½‘å…³åœ°å€ dhcp4ï¼šå¸ƒå°”ç±»å‹ï¼Œæ˜¯å¦ç»™IPv4å¼€å¯DHCPï¼Œé»˜è®¤æ˜¯falseã€‚ gateway4: IPv4çš„ç½‘å…³åœ°å€ï¼Œå·²ç»åºŸå¼ƒï¼Œè¢«routeså‚æ•°å–ä»£ã€‚gateway4å’Œrouteä¸¤è€…é…ç½®ä¸€ä¸ªå³å¯ã€‚ network: version: 2 renderer: networkd ethernets: ens3: addresses: [ \"192.168.10.1/24\" ] routes: - to: default # or 0.0.0.0/0 via: 9.9.9.9 on-link: true ä½¿ç”¨â€œon-linkâ€å…³é”®å­—ï¼Œå…¶ä¸­ç½‘å…³æ˜¯ç›´æ¥è¿æ¥åˆ°ç½‘ç»œçš„IPåœ°å€ï¼Œå³ä½¿è¯¥åœ°å€ä¸æ¥å£ä¸Šé…ç½®çš„å­ç½‘ä¸åŒ¹é…ã€‚\n3.2. é…ç½®bondç½‘å¡ å…·ä½“å¯å‚è€ƒï¼šhttps://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-bonds\nå¦‚æœæœ‰bondç½‘å¡åˆ™æŒ‰ä»¥ä¸‹çš„é…ç½®ã€‚\nbondå‚æ•° parameters è¯´æ˜ï¼š\nmode:ç½‘å¡çš„bondæ¨¡å¼ï¼ŒåŒ…æ‹¬balance-rr(é»˜è®¤ï¼Œå³è½®è¯¢), active-backupï¼ˆä¸»å¤‡æ¨¡å¼ï¼‰, balance-xor, broadcast, 802.3ad, balance-tlb , balance-alb mii-monitor-intervalï¼šæŒ‡å®šMIIç›‘æ§çš„é—´éš”æ—¶é—´(æ£€æŸ¥ç»‘å®šçš„æ¥å£æ˜¯å¦æœ‰carrier)ã€‚é»˜è®¤å€¼ä¸º0;è¿™å°†ç¦ç”¨MIIç›‘æ§ã€‚è¿™ç›¸å½“äºç½‘ç»œåç«¯çš„MIIMonitorSec=å­—æ®µã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´åç¼€ï¼Œè¯¥å€¼å°†è¢«è§£é‡Šä¸ºæ¯«ç§’ã€‚ lacp-rateï¼šé…ç½®lacpduçš„ä¼ è¾“é€Ÿç‡ã€‚è¿™åªåœ¨802.3adæ¨¡å¼ä¸‹æœ‰ç”¨ã€‚å¯èƒ½çš„å€¼æ˜¯slow(é»˜è®¤ä¸º30ç§’)å’Œfast(æ¯ç§’)ã€‚ transmit-hash-policyï¼šæŒ‡å®šç«¯å£é€‰æ‹©çš„ä¼ è¾“å“ˆå¸Œç­–ç•¥ã€‚è¿™åªåœ¨balance-xorã€802.3adå’Œbalance-tlbæ¨¡å¼ä¸‹æœ‰ç”¨ã€‚å–å€¼ä¸ºlayer2ã€layer3+4ã€layer2+3ã€encap2+3ã€encap3+4ã€‚ ä»¥ä¸‹ç¤ºä¾‹é…ç½®ä¸€ä¸ªæˆ–å¤šä¸ªbondç½‘å¡ï¼š\nnetwork: version: 2 renderer: networkd ethernets: ens1: {} ens2: {} ens3: {} ens4: {} bonds: bond0: # bond0 å³ç½‘å¡åç§° interfaces: # ç”±å“ªå‡ å—ç½‘å¡åšçš„bond0 - ens1 - ens2 addresses: [ \"192.168.10.1/24\" ] # é…ç½®bond0ç½‘å¡åœ°å€åŠè·¯ç”± routes: - to: default # or 0.0.0.0/0 via: 9.9.9.9 parameters: # bondç›¸å…³å‚æ•° mode: \"802.3ad\" mii-monitor-interval: \"100\" # è®¾ç½® MII é“¾è·¯ç›‘æ§é—´éš”ä¸º 100 æ¯«ç§’ã€‚ lacp-rate: \"fast\" # è®¾ç½® LACP é€Ÿç‡ä¸ºå¿«é€Ÿæ¨¡å¼ï¼ˆæ¯ç§’å‘é€ LACPDU æ•°æ®åŒ…ï¼‰ã€‚ transmit-hash-policy: \"layer3+4\" # è®¾ç½®ä¼ è¾“æ•£åˆ—ç­–ç•¥ä¸ºåŸºäºç¬¬3å±‚å’Œç¬¬4å±‚ï¼ˆIPåœ°å€å’Œç«¯å£ï¼‰çš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚ bond1: # bond1 å³ç½‘å¡åç§° interfaces: # ç”±å“ªå‡ å—ç½‘å¡åšçš„bond1 - ens3 - ens4 addresses: [ \"192.168.10.2/24\" ] # é…ç½®bond1ç½‘å¡åœ°å€åŠè·¯ç”± routes: - to: default # or 0.0.0.0/0 via: 9.9.9.9 parameters: # bondç›¸å…³å‚æ•° mode: \"802.3ad\" mii-monitor-interval: \"100\" lacp-rate: \"fast\" transmit-hash-policy: \"layer3+4\" 3.2.1. bondæ¨¡å¼802.3adè¯´æ˜ åœ¨ç½‘ç»œé…ç½®ä¸­ï¼Œbondï¼ˆåˆç§°ä¸º link aggregation æˆ– NIC teamingï¼‰æ˜¯å°†å¤šä¸ªç½‘ç»œæ¥å£èšåˆæˆä¸€ä¸ªå•ä¸€çš„é€»è¾‘æ¥å£ï¼Œä»¥æé«˜å¸¦å®½å’Œæä¾›å†—ä½™æ€§ã€‚bondæ¨¡å¼å†³å®šäº†è¿™äº›æ¥å£å¦‚ä½•ååŒå·¥ä½œã€‚\nmode: \"802.3ad\" æ˜¯ä¸€ç§ bondæ¨¡å¼ï¼Œå®ƒéµå¾ª IEEE 802.3ad æ ‡å‡†ï¼Œä¹Ÿç§°ä¸º LACPï¼ˆLink Aggregation Control Protocolï¼‰ã€‚è¿™ä¸ªæ¨¡å¼æä¾›äº†ä¸€ç§åŠ¨æ€åå•†æœºåˆ¶ï¼Œå¯ä»¥åœ¨å¤šä¸ªç½‘ç»œæ¥å£ä¹‹é—´èšåˆé“¾è·¯ã€‚\n802.3ad æ¨¡å¼çš„ç‰¹ç‚¹\nåŠ¨æ€åå•†ï¼šä½¿ç”¨ LACP åè®®åŠ¨æ€åå•†é“¾è·¯èšåˆï¼Œä½¿å¤šä¸ªç½‘ç»œæ¥å£ååŒå·¥ä½œã€‚ è´Ÿè½½å‡è¡¡ï¼šèƒ½å¤Ÿåœ¨å¤šä¸ªé“¾è·¯ä¸Šå‡è¡¡è´Ÿè½½ï¼Œæé«˜æ•´ä½“å¸¦å®½ã€‚ å†—ä½™æ€§ï¼šåœ¨ä»»ä½•ä¸€ä¸ªæ¥å£æ•…éšœæ—¶ï¼Œå…¶ä»–æ¥å£ä»èƒ½ç»§ç»­å·¥ä½œï¼Œæä¾›é“¾è·¯å†—ä½™ã€‚ å…¼å®¹æ€§ï¼šéœ€è¦äº¤æ¢æœºç«¯ä¹Ÿæ”¯æŒå¹¶é…ç½® LACP åè®®ã€‚ å…¸å‹åº”ç”¨åœºæ™¯\né«˜å¸¦å®½éœ€æ±‚ï¼šéœ€è¦èšåˆå¤šä¸ªç½‘ç»œæ¥å£æ¥æé«˜å¸¦å®½ï¼Œä¾‹å¦‚æœåŠ¡å™¨å¯¹ç½‘ç»œå¸¦å®½æœ‰è¾ƒé«˜è¦æ±‚çš„æƒ…å†µã€‚ é«˜å¯ç”¨æ€§éœ€æ±‚ï¼šéœ€è¦æä¾›ç½‘ç»œæ¥å£çš„å†—ä½™ï¼Œç¡®ä¿ç½‘ç»œè¿æ¥çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚ å¸¸ç”¨çš„æ¨¡å¼802.3adé…ç½®\nparameters: # bondç›¸å…³å‚æ•° mode: \"802.3ad\" mii-monitor-interval: \"100\" # è®¾ç½® MII é“¾è·¯ç›‘æ§é—´éš”ä¸º 100 æ¯«ç§’ã€‚ lacp-rate: \"fast\" # è®¾ç½® LACP é€Ÿç‡ä¸ºå¿«é€Ÿæ¨¡å¼ï¼ˆæ¯ç§’å‘é€ LACPDU æ•°æ®åŒ…ï¼‰ã€‚ transmit-hash-policy: \"layer3+4\" # è®¾ç½®ä¼ è¾“æ•£åˆ—ç­–ç•¥ä¸ºåŸºäºç¬¬3å±‚å’Œç¬¬4å±‚ï¼ˆIPåœ°å€å’Œç«¯å£ï¼‰çš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚ äº¤æ¢æœºé…ç½®è¦æ±‚\nä¸ºäº†ä½¿ 802.3ad æ¨¡å¼æ­£å¸¸å·¥ä½œï¼Œéœ€è¦ç¡®ä¿è¿æ¥çš„äº¤æ¢æœºæ”¯æŒå¹¶é…ç½®äº† LACPã€‚é€šè¿‡è¿™ç§é…ç½®ï¼Œå¯ä»¥å®ç°æœåŠ¡å™¨ç«¯å’Œäº¤æ¢æœºç«¯çš„é“¾è·¯èšåˆï¼Œæä¾›æ›´é«˜çš„å¸¦å®½å’Œå†—ä½™æ€§ã€‚\n3.3. é…ç½®VLAN å…·ä½“å¯å‚è€ƒï¼šhttps://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-vlans\nå¦‚æœæœ‰é…ç½®VLAN IDåˆ™æŒ‰ä»¥ä¸‹é…ç½®ï¼Œ\nVLANå‚æ•°è¯´æ˜ï¼š\nbond0.1000ï¼šVLANçš„ç½‘å¡åç§° idï¼šVLAN id linkï¼šé“¾æ¥çš„ç½‘å¡åç§°ï¼Œå¯ä»¥æ˜¯å®ä½“ç½‘å¡ä¹Ÿå¯ä»¥æ˜¯bondçš„ç½‘å¡ã€‚ ä»¥ä¸‹ç¤ºä¾‹é…ç½®ä¸€ä¸ªæˆ–å¤šä¸ªVLANç½‘å¡ã€‚\nnetwork: version: 2 renderer: networkd ethernets: ens1: {} ens2: {} ens3: {} bonds: bond0: interfaces: - ens1 - ens2 parameters: mode: \"802.3ad\" mii-monitor-interval: \"100\" lacp-rate: \"fast\" transmit-hash-policy: \"layer3+4\" vlans: bond0.1000: addresses: - \"192.168.10.1/24\" #é…ç½® bond0.1000 çš„åœ°å€å’Œè·¯ç”± routes: - to: \"0.0.0.0/0\" via: \"9.9.9.9\" id: 1000 # VLAN ID link: \"bond0\" # é…ç½®VLANå¯¹åº”çš„ç½‘å¡ ens3.2000: #å¤šä¸ªVLANçš„é…ç½® addresses: - \"192.168.10.2/24\" #é…ç½® bond0.1000 çš„åœ°å€å’Œè·¯ç”± routes: - to: \"0.0.0.0/0\" via: \"9.9.9.9\" id: 2000 # VLAN ID link: \"ens3\" # é…ç½®VLANå¯¹åº”çš„ç½‘å¡ æ€»ç»“ä¸Šè¿°ä¸‰ç§æƒ…å†µä¸‹addresseså’Œrouteså‚æ•°é…ç½®çš„åœ°æ–¹ï¼š\næ²¡æœ‰é…ç½®bondå’ŒVLANï¼Œåˆ™addresseså’Œrouteså‚æ•°é…ç½®åœ¨ethernets å¦‚æœæ²¡æœ‰é…ç½®VLANï¼Œä½†æ˜¯é…ç½®äº†bondï¼Œåˆ™é…ç½®åœ¨bondä¸‹ å¦‚æœé…ç½®äº†VLANï¼Œä¸è®ºæ˜¯å¦é…ç½®bondï¼Œéƒ½é…ç½®åœ¨VLANä¸‹ 4. netplanå‘½ä»¤ netplan -h usage: /usr/sbin/netplan [-h] [--debug] ... Network configuration in YAML options: -h, --help show this help message and exit --debug Enable debug messages Available commands: help Show this help message apply Apply current netplan config to running system generate Generate backend specific configuration files from /etc/netplan/*.yaml get Get a setting by specifying a nested key like \"ethernets.eth0.addresses\", or \"all\" info Show available features ip Retrieve IP information from the system set Add new setting by specifying a dotted key=value pair like ethernets.eth0.dhcp4=true rebind Rebind SR-IOV virtual functions of given physical functions to their driver status Query networking state of the running system try Try to apply a new netplan config to running system, with automatic rollback 4.1. netplan get æŸ¥è¯¢å½“å‰çš„é…ç½®å†…å®¹ï¼š\n$ netplan get network: version: 2 renderer: networkd ethernets: enp24s0f0: {} enp24s0f1: {} bonds: bond0: interfaces: - enp24s0f1 - enp24s0f0 parameters: mode: \"802.3ad\" mii-monitor-interval: \"100\" lacp-rate: \"fast\" transmit-hash-policy: \"layer3+4\" vlans: bond0.1000: addresses: - \"a.x.x.x/26\" dhcp4: false routes: - to: \"0.0.0.0/0\" via: \"b.x.x.x\" id: 1000 link: \"bond0\" 4.2. netplan apply é€šè¿‡ç¼–è¾‘/etc/netplan/*.yamlçš„æ–‡ä»¶ï¼Œå†æ‰§è¡Œnetplan applyçš„å‘½ä»¤å¯ä»¥ä½¿å¾—ç½‘ç»œé…ç½®ç”Ÿæ•ˆã€‚\n5. Systemd-networkd å¯ä»¥é€šè¿‡man systemd-networkdæŸ¥çœ‹è¯´æ˜\nsystemd-networkdæ˜¯ä¸€ç§ç®¡ç†ç½‘ç»œçš„ç³»ç»ŸæœåŠ¡ã€‚å®ƒæ£€æµ‹å’Œé…ç½®ç½‘ç»œè®¾å¤‡ï¼Œä»¥åŠåˆ›å»ºè™šæ‹Ÿç½‘ç»œè®¾å¤‡ã€‚\nsystemd-networkdå°†æ ¹æ®systemd.netdevæ–‡ä»¶ä¸­çš„é…ç½®åˆ›å»ºç½‘ç»œè®¾å¤‡ï¼Œå¹¶éµå®ˆè¿™äº›æ–‡ä»¶ä¸­çš„[Match]éƒ¨åˆ†ã€‚\nå½“systemd-networkdé€€å‡ºæ—¶ï¼Œå®ƒé€šå¸¸ä¼šä¿ç•™ç°æœ‰çš„ç½‘ç»œè®¾å¤‡å’Œé…ç½®ä¸å˜ã€‚å½“é…ç½®æ›´æ–°å¹¶é‡æ–°å¯åŠ¨systemd-networkdæ—¶ï¼Œnetdevå·²åˆ é™¤é…ç½®çš„æ¥å£ä¸ä¼šè¢«åˆ é™¤ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ¸…ç†ã€‚\nsystemd-networkdå¯ä»¥åœ¨è¿è¡Œæ—¶ä½¿ç”¨networkctlè¿›è¡Œæ§åˆ¶ã€‚\n5.1. é…ç½®æ–‡ä»¶ systemd-networkdçš„é…ç½®æ–‡ä»¶ä½äº/run/systemd/networkï¼Œéƒ¨åˆ†é…ç½®åœ¨/etc/systemd/networkä¸‹ã€‚\nä¸»è¦é…ç½®æ–‡ä»¶è·¯å¾„å’Œç”¨é€”\n.network æ–‡ä»¶ï¼šå®šä¹‰ç½‘ç»œæ¥å£çš„é…ç½®ã€‚ è·¯å¾„ï¼š/run/systemd/network/*.network ç”¨é€”ï¼šé…ç½®ç½‘ç»œæ¥å£çš„ IP åœ°å€ã€ç½‘å…³ã€DNS ç­‰ã€‚ .link æ–‡ä»¶ï¼šå®šä¹‰ç½‘ç»œæ¥å£çš„å±æ€§ï¼Œå¦‚åç§°ã€MAC åœ°å€ç­‰ã€‚ è·¯å¾„ï¼š/etc/systemd/network/*.link ç”¨é€”ï¼šé…ç½®ç½‘ç»œæ¥å£çš„å±æ€§ï¼Œæ¯”å¦‚åç§°ã€MAC åœ°å€ã€MTU ç­‰ã€‚ .netdev æ–‡ä»¶ï¼šå®šä¹‰è™šæ‹Ÿç½‘ç»œè®¾å¤‡ï¼ˆä¾‹å¦‚ bridgeã€bondã€vlan ç­‰ï¼‰çš„é…ç½®ã€‚ è·¯å¾„ï¼š/run/systemd/network/*.netdev ç”¨é€”ï¼šé…ç½®è™šæ‹Ÿç½‘ç»œè®¾å¤‡ã€‚ ä¾‹å¦‚ï¼š\n# cat 10-netplan-bond0.netdev [NetDev] Name=bond0 Kind=bond [Bond] Mode=802.3ad LACPTransmitRate=fast MIIMonitorSec=100ms TransmitHashPolicy=layer3+4 # cat 10-netplan-bond0.network [Match] Name=bond0 [Network] LinkLocalAddressing=ipv6 ConfigureWithoutCarrier=yes VLAN=bond0.1000 #cat 10-netplan-bond0.1000.network [Match] Name=bond0.1000 [Network] LinkLocalAddressing=ipv6 Address=192.168.0.1/26 ConfigureWithoutCarrier=yes [Route] Destination=0.0.0.0/0 Gateway=9.9.9.9 # cat 10-netplan-bond0.1000.netdev [NetDev] Name=bond0.1000 Kind=vlan [VLAN] Id=1000 5.2. netdevå‚æ•°è¯´æ˜ netdevçš„å‚æ•°å¯ä»¥å‚è€ƒï¼šhttps://manpages.ubuntu.com/manpages/bionic/man5/systemd.netdev.5.html\nbondéƒ¨åˆ†çš„å‚æ•°è¯´æ˜ï¼š\nThe \"[Bond]\" section accepts the following key: Mode= Specifies one of the bonding policies. The default is \"balance-rr\" (round robin). Possible values are \"balance-rr\", \"active-backup\", \"balance-xor\", \"broadcast\", \"802.3ad\", \"balance-tlb\", and \"balance-alb\". TransmitHashPolicy= Selects the transmit hash policy to use for slave selection in balance-xor, 802.3ad, and tlb modes. Possible values are \"layer2\", \"layer3+4\", \"layer2+3\", \"encap2+3\", and \"encap3+4\". LACPTransmitRate= Specifies the rate with which link partner transmits Link Aggregation Control Protocol Data Unit packets in 802.3ad mode. Possible values are \"slow\", which requests partner to transmit LACPDUs every 30 seconds, and \"fast\", which requests partner to transmit LACPDUs every second. The default value is \"slow\". vlanéƒ¨åˆ†è¯´æ˜ï¼š\nThe \"[VLAN]\" section only applies for netdevs of kind \"vlan\", and accepts the following key: Id= The VLAN ID to use. An integer in the range 0â€“4094. This option is compulsory. 6. networkctlå‘½ä»¤ # networkctl -h networkctl [OPTIONS...] COMMAND Query and control the networking subsystem. Commands: list [PATTERN...] List links status [PATTERN...] Show link status lldp [PATTERN...] Show LLDP neighbors label Show current address label entries in the kernel delete DEVICES... Delete virtual netdevs up DEVICES... Bring devices up down DEVICES... Bring devices down renew DEVICES... Renew dynamic configurations forcerenew DEVICES... Trigger DHCP reconfiguration of all connected clients reconfigure DEVICES... Reconfigure interfaces reload Reload .network and .netdev files edit FILES|DEVICES... Edit network configuration files cat FILES|DEVICES... Show network configuration files 6.1. networkctl status # networkctl status â— Interfaces: 8, 9, 7, 6, 5, 4, 3, 2, 1 State: routable Online state: online Address: 192.168.0.1 on bond0.1000 xxxx::4c0e:43ff:feba:xxxx on bond0 xxxx::4c0e:43ff:feba:xxxx on bond0.1000 Gateway: 9.9.9.9 on bond0.1000 Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: netdev ready Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: eno5: Gained carrier Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: Configuring with /run/systemd/network/10-netplan-bond0.1000.network. Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: Link UP Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0: Gained carrier Jul 18 02:21:16 192.168.0.1 systemd-networkd[6393]: bond0.1000: Gained carrier Jul 18 02:21:17 192.168.0.1 systemd-networkd[6393]: eno6: Gained carrier Jul 18 02:21:17 192.168.0.1 systemd-networkd[6393]: bond0.1000: Gained IPv6LL Jul 18 02:21:18 192.168.0.1 systemd-networkd[6393]: bond0: Gained IPv6LL Jul 18 02:21:18 192.168.0.1 systemd[1]: Finished systemd-networkd-wait-online.service - Wait for Network to be Configured. 6.2. networkctl list # networkctl list IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eno1 ether off unmanaged 3 eno2 ether off unmanaged 4 eno3 ether off unmanaged 5 eno4 ether off unmanaged 6 eno5 ether enslaved configured 7 eno6 ether enslaved configured 8 bond0 bond degraded configured 9 bond0.1000 vlan routable configured 9 links listed. 7. æ€»ç»“ æœ¬æ–‡å¤§æ¦‚ä»‹ç»äº†netplanå’Œsystemd-networkdä¸¤ä¸ªæ¨¡å—ï¼Œå…¶ä¸­systemd-networkdæ˜¯ä¸»è¦è´Ÿè´£linuxæœºå™¨ä¸Šçš„ç½‘ç»œç®¡ç†çš„åå°è¿›ç¨‹ã€‚å¯ä»¥é€šè¿‡networkctlçš„å‘½ä»¤è¡Œè¿›è¡Œé…ç½®å’ŒæŸ¥çœ‹ã€‚è€Œnetplanåˆ™æ˜¯ä¸€ä¸ªç½‘ç»œé…ç½®æ¨¡æ¿æ¸²æŸ“å·¥å…·ï¼Œé€šè¿‡yamlæ–‡ä»¶å¯ä»¥ç”Ÿæˆsystemd-networkdæ‰€ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ã€‚å…¶ä¸­netplançš„é…ç½®æ–‡ä»¶ä½äº/etc/netplan/ç›®å½•ï¼Œè€Œsystemd-networkdçš„é…ç½®æ–‡ä»¶ä¸º/run/systemd/networkç›®å½•ã€‚ä¸¤è€…çš„é…ç½®å‚æ•°å‡ ä¹æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œé€šè¿‡é…ç½®netplançš„å‚æ•°åˆ™å¯ä»¥é…ç½®systemd-networkdçš„å‚æ•°ã€‚\nå‚è€ƒï¼š\nhttps://netplan.io/ https://netplan.readthedocs.io/en/stable/netplan-yaml/ https://netplan.readthedocs.io/en/stable/netplan-yaml/#routing https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-bonds https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-vlans Systemd-networkd https://manpages.ubuntu.com/manpages/bionic/man5/systemd.netdev.5.html ","categories":"","description":"","excerpt":"1. netplanç®€ä»‹ netplanæ˜¯ä¸€ä¸ªlinuxç½‘ç»œé…ç½®çš„æ¸²æŸ“å™¨ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªç½‘ç»œé…ç½®çš„yamlæ–‡ä»¶ï¼Œnetplanå°†è¯¥æ–‡ä»¶æ¸²æŸ“ â€¦","ref":"/linux-notes/network/netplan/","tags":["network"],"title":"netplanä»‹ç»"},{"body":"1. æ¦‚è¿° ç½‘ç»œæ¥å£ç»‘å®šï¼ˆNetwork Interface Bondingï¼‰ï¼Œä¹Ÿç§°ä¸ºé“¾è·¯èšåˆï¼ˆLink Aggregationï¼‰æˆ–NIC Teamingï¼Œæ˜¯å°†å¤šä¸ªç‰©ç†ç½‘ç»œæ¥å£èšåˆæˆä¸€ä¸ªé€»è¾‘æ¥å£ï¼Œä»¥æé«˜å¸¦å®½å’Œæä¾›å†—ä½™æ€§çš„æŠ€æœ¯ã€‚è¿™ç§æŠ€æœ¯å¹¿æ³›åº”ç”¨äºæœåŠ¡å™¨å’Œé«˜æ€§èƒ½è®¡ç®—ç¯å¢ƒä¸­ï¼Œä»¥ç¡®ä¿ç½‘ç»œçš„é«˜å¯ç”¨æ€§å’Œé«˜æ€§èƒ½ã€‚\n2. ä¼˜åŠ¿ å¢åŠ å¸¦å®½ï¼šé€šè¿‡èšåˆå¤šä¸ªç½‘ç»œæ¥å£ï¼Œæ•´ä½“å¸¦å®½å¢åŠ ï¼Œä»è€Œæå‡ç½‘ç»œååé‡ã€‚ é«˜å¯ç”¨æ€§ï¼šåœ¨ä¸€ä¸ªæ¥å£å‘ç”Ÿæ•…éšœæ—¶ï¼Œå…¶ä»–æ¥å£å¯ä»¥ç»§ç»­å·¥ä½œï¼Œç¡®ä¿ç½‘ç»œè¿æ¥çš„è¿ç»­æ€§ã€‚ è´Ÿè½½å‡è¡¡ï¼šæ•°æ®æµé‡å¯ä»¥åœ¨å¤šä¸ªæ¥å£ä¹‹é—´å‡è¡¡åˆ†é…ï¼Œé¿å…å•ä¸€æ¥å£æˆä¸ºç“¶é¢ˆã€‚ ç®€åŒ–ç®¡ç†ï¼šå°†å¤šä¸ªæ¥å£ç®¡ç†ä¸ºä¸€ä¸ªé€»è¾‘æ¥å£ï¼Œç®€åŒ–äº†ç½‘ç»œé…ç½®å’Œç®¡ç†ã€‚ 3. Bonding æ¨¡å¼ Linux æ”¯æŒå¤šç§ Bonding æ¨¡å¼ï¼Œæ¯ç§æ¨¡å¼éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ï¼š\nmode=0 (balance-rr)ï¼šå¾ªç¯æ–¹å¼ï¼ˆRound-robinï¼‰ï¼Œæ¯ä¸ªæ•°æ®åŒ…ä¾æ¬¡ä»æ¯ä¸ªæ¥å£å‘é€ã€‚æä¾›è´Ÿè½½å‡è¡¡å’Œå®¹é”™åŠŸèƒ½ã€‚ mode=1 (active-backup)ï¼šä¸»å¤‡æ¨¡å¼ï¼ˆActive-backupï¼‰ï¼Œä¸€ä¸ªæ¥å£ä¸ºä¸»æ¥å£ï¼Œå…¶ä»–æ¥å£ä¸ºå¤‡ä»½æ¥å£ã€‚å½“ä¸»æ¥å£å¤±è´¥æ—¶ï¼Œå¤‡ä»½æ¥å£æ¥ç®¡ã€‚æä¾›é«˜å¯ç”¨æ€§ã€‚ mode=2 (balance-xor)ï¼šæ ¹æ®ä¼ è¾“æ•£åˆ—ç®—æ³•é€‰æ‹©æ¥å£ã€‚æä¾›è´Ÿè½½å‡è¡¡å’Œå®¹é”™åŠŸèƒ½ã€‚ mode=3 (broadcast)ï¼šå¹¿æ’­æ¨¡å¼ï¼Œæ‰€æœ‰æ•°æ®åŒ…é€šè¿‡æ‰€æœ‰æ¥å£å‘é€ã€‚æä¾›å®¹é”™åŠŸèƒ½ã€‚ mode=4 (802.3ad)ï¼šåŠ¨æ€é“¾è·¯èšåˆï¼ˆLACPï¼‰ï¼Œéœ€è¦äº¤æ¢æœºæ”¯æŒ IEEE 802.3adã€‚æä¾›è´Ÿè½½å‡è¡¡å’Œé«˜å¯ç”¨æ€§ã€‚ mode=5 (balance-tlb)ï¼šåŸºäºå‘é€è´Ÿè½½çš„è‡ªé€‚åº”ä¼ è¾“è´Ÿè½½å‡è¡¡ï¼ˆAdaptive Transmit Load Balancingï¼‰ã€‚æ— éœ€ç‰¹æ®Šäº¤æ¢æœºæ”¯æŒã€‚ mode=6 (balance-alb)ï¼šåŸºäºæ¥æ”¶è´Ÿè½½çš„è‡ªé€‚åº”è´Ÿè½½å‡è¡¡ï¼ˆAdaptive Load Balancingï¼‰ã€‚æ— éœ€ç‰¹æ®Šäº¤æ¢æœºæ”¯æŒã€‚ 4. é…ç½®ç¤ºä¾‹ ä»¥ä¸‹æ˜¯ä½¿ç”¨ systemd-networkd é…ç½® Bonding çš„ç¤ºä¾‹ã€‚\n4.1. é…ç½®ç‰©ç†æ¥å£ é¦–å…ˆï¼Œé…ç½®è¦ç»‘å®šçš„ç‰©ç†æ¥å£ã€‚ä¾‹å¦‚ï¼Œenp26s0f0 å’Œ enp26s0f1ï¼š\nåˆ›å»ºæ–‡ä»¶ /etc/systemd/network/10-enp26s0f0.networkï¼š\n[Match] Name=enp26s0f0 [Network] Bond=bond0 åˆ›å»ºæ–‡ä»¶ /etc/systemd/network/10-enp26s0f1.networkï¼š\n[Match] Name=enp26s0f1 [Network] Bond=bond0 4.2. é…ç½® Bonding æ¥å£ åˆ›å»ºæ–‡ä»¶ /etc/systemd/network/bond0.netdev æ¥å®šä¹‰ Bonding æ¥å£ï¼š\n[NetDev] Name=bond0 Kind=bond [Bond] Mode=802.3ad MIIMonitorSec=1s LACPTransmitRate=fast 4.3. é…ç½® Bonding æ¥å£çš„ç½‘ç»œè®¾ç½® åˆ›å»ºæ–‡ä»¶ /etc/systemd/network/10-bond0.network æ¥é…ç½® Bonding æ¥å£çš„ç½‘ç»œè®¾ç½®ï¼š\n[Match] Name=bond0 [Network] Address=192.168.1.10/24 Gateway=192.168.1.1 DNS=8.8.8.8 DNS=8.8.4.4 4.4. åº”ç”¨é…ç½® ä¿å­˜é…ç½®æ–‡ä»¶åï¼Œé‡æ–°å¯åŠ¨ systemd-networkd æœåŠ¡ä»¥åº”ç”¨æ–°çš„ç½‘ç»œé…ç½®ï¼š\nsudo systemctl restart systemd-networkd 4.5. æ£€æŸ¥é…ç½® æˆ–è€…æŸ¥çœ‹å…·ä½“æ¥å£çš„è¯¦ç»†ä¿¡æ¯ï¼š\n# networkctl status bond0 â— 8: bond0 Link File: /usr/lib/systemd/network/99-default.link Network File: /run/systemd/network/10-netplan-bond0.network State: degraded (configured) Online state: online Type: bond Kind: bond Driver: bonding Hardware Address: 4e:0e:43:ba:f7:82 MTU: 1500 (min: 68, max: 65535) QDisc: noqueue IPv6 Address Generation Mode: eui64 Mode: 802.3ad Miimon: 100ms Updelay: 0 Downdelay: 0 Number of Queues (Tx/Rx): 16/16 Auto negotiation: no Speed: 20Gbps Duplex: full Address: xxx::4c0e:43ff:feba:xxx Activation Policy: up Required For Online: yes DHCP6 Client DUID: DUID-EN/Vendor:0000ab111fbd6366525ac0ea 5. é€šè¿‡å‘½ä»¤é…ç½®bond 5.1. é€šè¿‡IPå‘½ä»¤åšbond #!/bin/bash # å®‰è£…å¿…è¦çš„è½¯ä»¶åŒ… sudo apt-get update sudo apt-get install -y ifenslave # åˆ›å»º Bond æ¥å£ sudo ip link add bond0 type bond # è®¾ç½® Bond æ¨¡å¼ sudo ip link set bond0 type bond mode 802.3ad æˆ–è€… modprobe bonding mode=4 miimon=100 lacp_rate=1 xmit_hash_policy=1 # æ·»åŠ ä»æ¥å£åˆ° Bond æ¥å£ sudo ip link set enp26s0f0 down sudo ip link set enp26s0f0 master bond0 sudo ip link set enp26s0f1 down sudo ip link set enp26s0f1 master bond0 # é…ç½® Bond æ¥å£çš„ IP åœ°å€ sudo ip addr add 192.168.1.10/24 dev bond0 # å¯ç”¨ Bond æ¥å£ sudo ip link set bond0 up # å¯ç”¨ä»æ¥å£ sudo ip link set enp26s0f0 up sudo ip link set enp26s0f1 up echo \"Bond æ¥å£é…ç½®å®Œæˆ\" æŸ¥çœ‹bondçŠ¶æ€\ncat /proc/net/bonding/bond0 ä½¿ç”¨ modprobe å·¥å…·é…ç½®ç½‘ç»œæ¥å£çš„ Bondï¼ˆç»‘å®šï¼‰æ“ä½œæ˜¯å¦ä¸€ç§åœ¨ Linux ä¸Šè®¾ç½®é“¾è·¯èšåˆçš„æ–¹æ³•ã€‚modprobe ç”¨äºåŠ è½½å’Œå¸è½½å†…æ ¸æ¨¡å—ï¼Œè€Œ bonding æ¨¡å—æ˜¯ç”¨äºå®ç°ç½‘ç»œæ¥å£ç»‘å®šçš„å†…æ ¸æ¨¡å—ã€‚\n","categories":"","description":"","excerpt":"1. æ¦‚è¿° ç½‘ç»œæ¥å£ç»‘å®šï¼ˆNetwork Interface Bondingï¼‰ï¼Œä¹Ÿç§°ä¸ºé“¾è·¯èšåˆï¼ˆLink Aggregationï¼‰æˆ–NIC â€¦","ref":"/linux-notes/network/bond/","tags":["network"],"title":"ç½‘å¡Bondingä»‹ç»"},{"body":"Dockeréƒ¨ç½² docker run -d -p 3000:3000 grafana/grafana:latest K8Séƒ¨ç½² helméƒ¨ç½²\nhelm repo add grafana https://grafana.github.io/helm-charts helm search repo grafana å‚è€ƒï¼š\nInstall Grafana | Grafana documentation\nDeploy Grafana on Kubernetes | Grafana documentation\nGitHub - grafana/helm-charts\n","categories":"","description":"","excerpt":"Dockeréƒ¨ç½² docker run -d -p 3000:3000 grafana/grafana:latest K8Séƒ¨ç½² helméƒ¨ â€¦","ref":"/kubernetes-notes/monitor/grafana-usage/","tags":["Monitor"],"title":"Grafanaéƒ¨ç½²"},{"body":"1. Wasmï¼ˆWebAssemblyï¼‰æ˜¯ä»€ä¹ˆ Wasmï¼Œå…¨ç§°ä¸ºWebAssemblyï¼Œæ˜¯åŸºäºå †æ ˆçš„è™šæ‹Ÿæœºçš„äºŒè¿›åˆ¶æŒ‡ä»¤æ ¼å¼ã€‚Wasmè¢«è®¾è®¡ä¸ºç¼–ç¨‹è¯­è¨€çš„å¯ç§»æ¤ç¼–è¯‘ç›®æ ‡ï¼Œæ”¯æŒåœ¨Webä¸Šéƒ¨ç½²å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨åº”ç”¨ç¨‹åºã€‚\nWebAssemblyçš„ä¸»è¦ç›®æ ‡æ˜¯æä¾›ä¸€ç§å¯ç§»æ¤ã€é«˜æ•ˆã€å®‰å…¨çš„æ‰§è¡Œç¯å¢ƒï¼Œä»¥åœ¨Webæµè§ˆå™¨ä¸­è¿è¡Œå„ç§ç¼–ç¨‹è¯­è¨€çš„ä»£ç ã€‚å®ƒä¸ä¾èµ–äºç‰¹å®šçš„ç¡¬ä»¶æˆ–æ“ä½œç³»ç»Ÿï¼ŒWebAssemblyå…è®¸å¼€å‘äººå‘˜ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œä¾‹å¦‚Cã€C++ã€Rustç­‰ï¼Œé€šè¿‡ç¼–è¯‘æˆWasmå­—èŠ‚ç æ¥åœ¨Webä¸Šè¿è¡Œã€‚\nWasmçš„ç‰¹ç‚¹ï¼š\né«˜æ•ˆæ€§èƒ½ï¼šWasmè¢«è®¾è®¡ä¸ºé«˜æ•ˆæ‰§è¡Œï¼Œå¹¶ä¸”ä¸åº•å±‚ç³»ç»Ÿç¡¬ä»¶ç´§å¯†å…³è”ï¼Œä½¿å…¶åœ¨Webæµè§ˆå™¨ä¸­å¯ä»¥è·å¾—æ¥è¿‘æœ¬æœºä»£ç çš„æ€§èƒ½ã€‚\nå®‰å…¨æ€§ï¼šWasmæ˜¯ä¸€ç§éš”ç¦»çš„æ‰§è¡Œç¯å¢ƒï¼Œå®ƒè¿è¡Œåœ¨æµè§ˆå™¨çš„æ²™ç®±ä¸­ï¼Œå…·æœ‰ä¸¥æ ¼çš„å®‰å…¨æ€§æªæ–½ï¼Œç¡®ä¿Wasmä»£ç ä¸èƒ½ç›´æ¥è®¿é—®Webæµè§ˆå™¨çš„æ•æ„Ÿèµ„æºå’ŒåŠŸèƒ½ã€‚\nå¯ç§»æ¤æ€§ï¼šç”±äºWasmæ˜¯ä¸€ç§ç‹¬ç«‹äºå¹³å°çš„ä¸­é—´è¡¨ç¤ºï¼Œå› æ­¤å¯ä»¥åœ¨å„ç§è®¾å¤‡å’Œæ“ä½œç³»ç»Ÿä¸Šè¿è¡Œï¼Œä»æ¡Œé¢è®¡ç®—æœºåˆ°ç§»åŠ¨è®¾å¤‡ã€‚\nè¯­è¨€æ— å…³æ€§ï¼šWasmå…è®¸ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ç¼–å†™ä»£ç ï¼Œè€Œä¸ä»…é™äºJavaScriptã€‚è¿™ä¸ºå¼€å‘äººå‘˜æä¾›äº†æ›´å¤šçš„çµæ´»æ€§ï¼Œä½¿å¾—åœ¨Webä¸Šè¿è¡Œé«˜æ€§èƒ½åº”ç”¨ç¨‹åºå˜å¾—æ›´åŠ å®¹æ˜“ã€‚\nä¸€å¥è¯æ¥æ¦‚æ‹¬ï¼š\nWasmæ˜¯ä¸€ç§å¯ç§»æ¤ã€é«˜æ•ˆã€å®‰å…¨ã€è·¨è¯­è¨€çš„äºŒè¿›åˆ¶ç¼–ç æ ¼å¼ã€‚å®ƒæ”¯æŒåœ¨å®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰å’ŒæœåŠ¡ç«¯è¿è¡Œåº”ç”¨ç¨‹åºã€‚\n2. WasmEdgeæ˜¯ä»€ä¹ˆ WasmEdge æ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ€§èƒ½å’Œå¯æ‰©å±•çš„ WebAssembly è¿è¡Œæ—¶ã€‚å®ƒæ˜¯å½“ä»Šæœ€å¿«çš„Wasm VMã€‚é€‚ç”¨äºäº‘åŸç”Ÿã€è¾¹ç¼˜å’Œå»ä¸­å¿ƒåŒ–åº”ç”¨ç¨‹åºã€‚å®ƒä¸ºserverlessåº”ç”¨ç¨‹åºã€åµŒå…¥å¼åŠŸèƒ½ã€å¾®æœåŠ¡ã€æ™ºèƒ½åˆçº¦å’Œ IoT è®¾å¤‡æä¾›æ”¯æŒã€‚\n3. å¦‚ä½•å°†golangç¼–è¯‘æˆwasmå¹¶è¿è¡Œ ä»£ç å¦‚ä¸‹ï¼š\npackage main func main() { println(\"Hello TinyGo from WasmEdge!\") } 3.1. ç¼–è¯‘wasmäºŒè¿›åˆ¶ ä½¿ç”¨tinygoç¼–è¯‘\nå®‰è£…tinygo\nubuntuç³»ç»Ÿ\nwget https://github.com/tinygo-org/tinygo/releases/download/v0.28.1/tinygo_0.28.1_amd64.deb sudo dpkg -i tinygo_0.28.1_amd64.deb tinygoç¼–è¯‘\ntinygo build -o hello.wasm -target wasm main.go 3.2. è¿è¡ŒwasmäºŒè¿›åˆ¶ å®‰è£…wasmedgeï¼Œå‚è€ƒï¼šhttps://wasmedge.org/docs/start/install\nwget -qO- https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- -p /usr/local è¿è¡ŒwasmäºŒè¿›åˆ¶\nå‚è€ƒï¼š\n[Go - WasmEdge Runtime](Go - WasmEdge Runtime)\n# wasmedge hello.wasm Hello TinyGo from WasmEdge! 3.3. æ€§èƒ½æå‡ è¦ä¸ºè¿™äº›åº”ç”¨ç¨‹åºè¾¾åˆ°åŸç”Ÿ Go æ€§èƒ½ï¼Œä½ å¯ä»¥ä½¿ç”¨Â wasmedgecÂ å‘½ä»¤æ¥ AOT ç¼–è¯‘Â wasmÂ ç¨‹åºï¼Œç„¶åä½¿ç”¨Â wasmedgeÂ å‘½ä»¤è¿è¡Œå®ƒã€‚\n$ wasmedgec hello.wasm hello.wasm $ wasmedge hello.wasm Hello TinyGo from WasmEdge! 4. å¦‚ä½•æ„å»ºwasmçš„å®¹å™¨é•œåƒ å®‰è£…buildahï¼Œå‚è€ƒï¼šhttps://github.com/containers/buildah/blob/main/install.md\nsudo apt-get -y update sudo apt-get -y install buildah æ­¥éª¤å¦‚ä¸‹ï¼š\nç¼–è¯‘wasmäºŒè¿›åˆ¶\nç¼–å†™dockerfileï¼Œä¾‹å¦‚ï¼š\nFROM scratch COPY hello.wasm / CMD [\"/hello.wasm\"] ä½¿ç”¨buildahæ„å»ºå’Œå‘å¸ƒé•œåƒã€‚\nbuildah build --annotation \"module.wasm.image/variant=compat-smart\" -t wasm-hello . å‚è€ƒï¼š\nhttps://webassembly.org/\nhttps://github.com/WasmEdge/WasmEdge\nhttps://github.com/second-state/wasmedge-containers-examples\nhttps://github.com/second-state/wasmedge-containers-examples/blob/main/simple_wasi_app-zh.md\nhttps://wasmedge.org/docs/develop/deploy/cri-runtime/containerd-crun\nhttps://wasmedge.org/docs/develop/go/hello_world\nManage WebAssembly Apps Using Container and Kubernetes Tools\n","categories":"","description":"","excerpt":"1. Wasmï¼ˆWebAssemblyï¼‰æ˜¯ä»€ä¹ˆ Wasmï¼Œå…¨ç§°ä¸ºWebAssemblyï¼Œæ˜¯åŸºäºå †æ ˆçš„è™šæ‹Ÿæœºçš„äºŒè¿›åˆ¶æŒ‡ä»¤æ ¼å¼ã€‚Wasmè¢«è®¾è®¡ä¸º â€¦","ref":"/kubernetes-notes/runtime/wasmedge/","tags":["Kubernetes","Runtime"],"title":"WasmEdgeä»‹ç»"},{"body":"1. è¯»å–æ•°æ®key ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ—å‡ºæ‰€æœ‰çš„keyã€‚\nETCDCTL_API=3 etcdctl --endpoints=\u003cetcd-ip-1\u003e:2379,\u003cetcd-ip-2\u003e:2379,\u003cetcd-ip-3\u003e:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt get / --prefix --keys-only å‚æ•°è¯´æ˜ï¼š\n--cacert=\"\"\tverify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\"\tidentify secure client using this TLS certificate file --key=\"\"\tidentify secure client using this TLS key file --endpoints=[127.0.0.1:2379]\tgRPC endpoints å¯ä»¥ä½¿ç”¨aliasæ¥é‡å‘½åetcdctlä¸€ä¸²çš„å‘½ä»¤\nalias ectl='ETCDCTL_API=3 etcdctl --endpoints=\u003cetcd-ip-1\u003e:2379,\u003cetcd-ip-2\u003e:2379,\u003cetcd-ip-3\u003e:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt' 2. é›†ç¾¤æ•°æ® 2.1. node /registry/minions/\u003cnode-ip-1\u003e /registry/minions/\u003cnode-ip-2\u003e /registry/minions/\u003cnode-ip-3\u003e å…¶ä»–ä¿¡æ¯ï¼š\n/registry/leases/kube-node-lease/\u003cnode-ip-1\u003e /registry/leases/kube-node-lease/\u003cnode-ip-2\u003e /registry/leases/kube-node-lease/\u003cnode-ip-3\u003e /registry/masterleases/\u003cnode-ip-2\u003e /registry/masterleases/\u003cnode-ip-3\u003e 3. k8så¯¹è±¡æ•°æ® k8så¯¹è±¡æ•°æ®çš„æ ¼å¼\n3.1. namespace /registry/namespaces/default /registry/namespaces/game /registry/namespaces/kube-node-lease /registry/namespaces/kube-public /registry/namespaces/kube-system 3.2. namespaceçº§åˆ«å¯¹è±¡ /registry/{resource}/{namespace}/{resource_name} ä»¥ä¸‹ä»¥å¸¸è§k8så¯¹è±¡ä¸ºä¾‹ï¼š\n# deployment /registry/deployments/default/game-2048 /registry/deployments/kube-system/prometheus-operator # replicasets /registry/replicasets/default/game-2048-c7d589ccf # pod /registry/pods/default/game-2048-c7d589ccf-8lsbw # statefulsets /registry/statefulsets/kube-system/prometheus-k8s # daemonsets /registry/daemonsets/kube-system/kube-proxy # secrets /registry/secrets/default/default-token-tbfmb # serviceaccounts /registry/serviceaccounts/default/default service\n# service /registry/services/specs/default/game-2048 # endpoints /registry/services/endpoints/default/game-2048 4. è¯»å–æ•°æ®value ç”±äºk8sé»˜è®¤etcdä¸­çš„æ•°æ®æ˜¯é€šè¿‡protobufæ ¼å¼å­˜å‚¨ï¼Œå› æ­¤çœ‹åˆ°çš„keyå’Œvalueçš„å€¼æ˜¯ä¸€ä¸²å­—ç¬¦ä¸²ã€‚\nalias ectl='ETCDCTL_API=3 etcdctl --endpoints=:2379,:2379,:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt'\n# ectl get /registry/namespaces/test -w json |jq { \"header\": { \"cluster_id\": 12113422651334595000, \"member_id\": 8381627376898157000, \"revision\": 12321629, \"raft_term\": 20 }, \"kvs\": [ { \"key\": \"L3JlZ2lzdHJ5L25hbWVzcGFjZXMvdGVzdA==\", \"create_revision\": 11670741, \"mod_revision\": 11670741, \"version\": 1, \"value\": \"azhzAAoPCgJ2MRIJTmFtZXNwYWNlElwKQgoEdGVzdBIAGgAiACokYWM1YmJjOTQtNTkxZi0xMWVhLWJiOTQtNmM5MmJmM2I3NmI1MgA4AEIICJuf3fIFEAB6ABIMCgprdWJlcm5ldGVzGggKBkFjdGl2ZRoAIgA=\" } ], \"count\": 1 } å…¶ä¸­keyå¯ä»¥é€šè¿‡base64è§£ç å‡ºæ¥\necho \"L3JlZ2lzdHJ5L25hbWVzcGFjZXMvdGVzdA==\" | base64 --decode # output /registry/namespaces/test valueæ˜¯å€¼å¯ä»¥é€šè¿‡å®‰è£…etcdhelperå·¥å…·è§£æå‡ºæ¥ã€‚\nalias ehelper='etcdhelper -key /etc/kubernetes/pki/apiserver-etcd-client.key -cert /etc/kubernetes/pki/apiserver-etcd-client.crt -cacert /etc/kubernetes/pki/etcd/ca.crt'\n# ehelper get /registry/namespaces/test /v1, Kind=Namespace { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"test\", \"uid\": \"ac5bbc94-591f-11ea-bb94-6c92bf3b76b5\", \"creationTimestamp\": \"2020-02-27T05:11:55Z\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Active\" } } 5. æ³¨æ„äº‹é¡¹ ç”±äºk8sçš„etcdæ•°æ®ä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼Œé»˜è®¤é€šè¿‡protobufæ ¼å¼å­˜å‚¨ï¼Œä¸è¦é€šè¿‡æ‰‹åŠ¨çš„æ–¹å¼å»ä¿®æ”¹æˆ–æ·»åŠ k8sæ•°æ®ã€‚ ä¸æ¨èä½¿ç”¨jsonæ ¼å¼å­˜å‚¨etcdæ•°æ®ï¼Œå¦‚æœéœ€è¦jsonæ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨--storage-media-type=application/jsonå‚æ•°å­˜å‚¨ï¼Œå‚è€ƒï¼šhttps://github.com/kubernetes/kubernetes/issues/44670 6. å¿«æ·å‘½ä»¤ ç”±äºetcdctlçš„å‘½ä»¤éœ€è¦æ·»åŠ å¾ˆå¤šè®¤è¯å‚æ•°å’Œendpointsçš„å‚æ•°ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨åˆ«åçš„æ–¹å¼æ¥ç®€åŒ–å‘½ä»¤ã€‚\n# etcdctl alias ectl='ETCDCTL_API=3 etcdctl --endpoints=\u003cetcd-ip-1\u003e:2379,\u003cetcd-ip-2\u003e:2379,\u003cetcd-ip-3\u003e:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt' # etcdhelper alias ehelper='etcdhelper -key /etc/kubernetes/pki/apiserver-etcd-client.key -cert /etc/kubernetes/pki/apiserver-etcd-client.crt -cacert /etc/kubernetes/pki/etcd/ca.crt' 6.1. etcdhelperçš„ä½¿ç”¨ etcdhelperæ–‡æ¡£å‚è€ƒï¼šhttps://github.com/openshift/origin/tree/master/tools/etcdhelper\n# å¿…è¦çš„è®¤è¯å‚æ•° -key - points to master.etcd-client.key -cert - points to master.etcd-client.crt -cacert - points to ca.crt # å‘½ä»¤æ“ä½œå‚æ•° ls - list all keys starting with prefix get - get the specific value of a key dump - dump the entire contents of the etcd ç¤ºä¾‹\n$ ehelper ls /registry/leases/ /registry/leases/kube-node-lease/\u003cip-1\u003e /registry/leases/kube-node-lease/\u003cip-2\u003e /registry/leases/kube-node-lease/\u003cip-3\u003e $ ehelper get \u003ckey\u003e 7. RBAC é™„RBACç›¸å…³çš„keyã€‚\nclusterrolebindings\n/registry/clusterrolebindings/cluster-admin /registry/clusterrolebindings/flannel /registry/clusterrolebindings/galaxy /registry/clusterrolebindings/helm /registry/clusterrolebindings/kube-state-metrics /registry/clusterrolebindings/kubeadm:kubelet-bootstrap /registry/clusterrolebindings/kubeadm:node-autoapprove-bootstrap /registry/clusterrolebindings/kubeadm:node-autoapprove-certificate-rotation /registry/clusterrolebindings/kubeadm:node-proxier /registry/clusterrolebindings/lbcf-controller /registry/clusterrolebindings/prometheus-k8s /registry/clusterrolebindings/prometheus-operator /registry/clusterrolebindings/system:aws-cloud-provider /registry/clusterrolebindings/system:basic-user /registry/clusterrolebindings/system:controller:attachdetach-controller /registry/clusterrolebindings/system:controller:certificate-controller /registry/clusterrolebindings/system:controller:clusterrole-aggregation-controller /registry/clusterrolebindings/system:controller:cronjob-controller /registry/clusterrolebindings/system:controller:daemon-set-controller /registry/clusterrolebindings/system:controller:deployment-controller /registry/clusterrolebindings/system:controller:disruption-controller /registry/clusterrolebindings/system:controller:endpoint-controller /registry/clusterrolebindings/system:controller:expand-controller /registry/clusterrolebindings/system:controller:generic-garbage-collector /registry/clusterrolebindings/system:controller:horizontal-pod-autoscaler /registry/clusterrolebindings/system:controller:job-controller /registry/clusterrolebindings/system:controller:namespace-controller /registry/clusterrolebindings/system:controller:node-controller /registry/clusterrolebindings/system:controller:persistent-volume-binder /registry/clusterrolebindings/system:controller:pod-garbage-collector /registry/clusterrolebindings/system:controller:pv-protection-controller /registry/clusterrolebindings/system:controller:pvc-protection-controller /registry/clusterrolebindings/system:controller:replicaset-controller /registry/clusterrolebindings/system:controller:replication-controller /registry/clusterrolebindings/system:controller:resourcequota-controller /registry/clusterrolebindings/system:controller:route-controller /registry/clusterrolebindings/system:controller:service-account-controller /registry/clusterrolebindings/system:controller:service-controller /registry/clusterrolebindings/system:controller:statefulset-controller /registry/clusterrolebindings/system:controller:ttl-controller /registry/clusterrolebindings/system:coredns /registry/clusterrolebindings/system:discovery /registry/clusterrolebindings/system:kube-controller-manager /registry/clusterrolebindings/system:kube-dns /registry/clusterrolebindings/system:kube-scheduler /registry/clusterrolebindings/system:node /registry/clusterrolebindings/system:node-proxier /registry/clusterrolebindings/system:public-info-viewer /registry/clusterrolebindings/system:volume-scheduler clusterroles\n/registry/clusterroles/admin /registry/clusterroles/cluster-admin /registry/clusterroles/edit /registry/clusterroles/flannel /registry/clusterroles/kube-state-metrics /registry/clusterroles/lbcf-controller /registry/clusterroles/prometheus-k8s /registry/clusterroles/prometheus-operator /registry/clusterroles/system:aggregate-to-admin /registry/clusterroles/system:aggregate-to-edit /registry/clusterroles/system:aggregate-to-view /registry/clusterroles/system:auth-delegator /registry/clusterroles/system:aws-cloud-provider /registry/clusterroles/system:basic-user /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:nodeclient /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient /registry/clusterroles/system:controller:attachdetach-controller /registry/clusterroles/system:controller:certificate-controller /registry/clusterroles/system:controller:clusterrole-aggregation-controller /registry/clusterroles/system:controller:cronjob-controller /registry/clusterroles/system:controller:daemon-set-controller /registry/clusterroles/system:controller:deployment-controller /registry/clusterroles/system:controller:disruption-controller /registry/clusterroles/system:controller:endpoint-controller /registry/clusterroles/system:controller:expand-controller /registry/clusterroles/system:controller:generic-garbage-collector /registry/clusterroles/system:controller:horizontal-pod-autoscaler /registry/clusterroles/system:controller:job-controller /registry/clusterroles/system:controller:namespace-controller /registry/clusterroles/system:controller:node-controller /registry/clusterroles/system:controller:persistent-volume-binder /registry/clusterroles/system:controller:pod-garbage-collector /registry/clusterroles/system:controller:pv-protection-controller /registry/clusterroles/system:controller:pvc-protection-controller /registry/clusterroles/system:controller:replicaset-controller /registry/clusterroles/system:controller:replication-controller /registry/clusterroles/system:controller:resourcequota-controller /registry/clusterroles/system:controller:route-controller /registry/clusterroles/system:controller:service-account-controller /registry/clusterroles/system:controller:service-controller /registry/clusterroles/system:controller:statefulset-controller /registry/clusterroles/system:controller:ttl-controller /registry/clusterroles/system:coredns /registry/clusterroles/system:csi-external-attacher /registry/clusterroles/system:csi-external-provisioner /registry/clusterroles/system:discovery /registry/clusterroles/system:heapster /registry/clusterroles/system:kube-aggregator /registry/clusterroles/system:kube-controller-manager /registry/clusterroles/system:kube-dns /registry/clusterroles/system:kube-scheduler /registry/clusterroles/system:kubelet-api-admin /registry/clusterroles/system:node /registry/clusterroles/system:node-bootstrapper /registry/clusterroles/system:node-problem-detector /registry/clusterroles/system:node-proxier /registry/clusterroles/system:persistent-volume-provisioner /registry/clusterroles/system:public-info-viewer /registry/clusterroles/system:volume-scheduler /registry/clusterroles/view rolebindings\n/registry/rolebindings/kube-public/kubeadm:bootstrap-signer-clusterinfo /registry/rolebindings/kube-public/system:controller:bootstrap-signer /registry/rolebindings/kube-system/kube-proxy /registry/rolebindings/kube-system/kube-state-metrics /registry/rolebindings/kube-system/kubeadm:kubeadm-certs /registry/rolebindings/kube-system/kubeadm:kubelet-config-1.14 /registry/rolebindings/kube-system/kubeadm:nodes-kubeadm-config /registry/rolebindings/kube-system/system::extension-apiserver-authentication-reader /registry/rolebindings/kube-system/system::leader-locking-kube-controller-manager /registry/rolebindings/kube-system/system::leader-locking-kube-scheduler /registry/rolebindings/kube-system/system:controller:bootstrap-signer /registry/rolebindings/kube-system/system:controller:cloud-provider /registry/rolebindings/kube-system/system:controller:token-cleaner roles\n/registry/roles/kube-public/kubeadm:bootstrap-signer-clusterinfo /registry/roles/kube-public/system:controller:bootstrap-signer /registry/roles/kube-system/extension-apiserver-authentication-reader /registry/roles/kube-system/kube-proxy /registry/roles/kube-system/kube-state-metrics-resizer /registry/roles/kube-system/kubeadm:kubeadm-certs /registry/roles/kube-system/kubeadm:kubelet-config-1.14 /registry/roles/kube-system/kubeadm:nodes-kubeadm-config /registry/roles/kube-system/system::leader-locking-kube-controller-manager /registry/roles/kube-system/system::leader-locking-kube-scheduler /registry/roles/kube-system/system:controller:bootstrap-signer /registry/roles/kube-system/system:controller:cloud-provider /registry/roles/kube-system/system:controller:token-cleaner å‚è€ƒï¼š\nhttps://github.com/etcd-io/etcd/tree/master/etcdctl https://github.com/openshift/origin/tree/master/tools/etcdhelper using-etcdctl-to-access-kubernetes-data å¦‚ä½•è¯»å–Kuberneteså­˜å‚¨åœ¨etcdä¸Šçš„æ•°æ® https://github.com/kubernetes/kubernetes/issues/44670 ","categories":"","description":"","excerpt":"1. è¯»å–æ•°æ®key ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ—å‡ºæ‰€æœ‰çš„keyã€‚\nETCDCTL_API=3 etcdctl â€¦","ref":"/kubernetes-notes/etcd/k8s-etcd-data/","tags":["Etcd"],"title":"Etcdä¸­çš„k8sæ•°æ®"},{"body":"Podè°ƒåº¦ åœ¨kubernetesé›†ç¾¤ä¸­ï¼ŒPodï¼ˆcontainerï¼‰æ˜¯åº”ç”¨çš„è½½ä½“ï¼Œä¸€èˆ¬é€šè¿‡RCã€Deploymentã€DaemonSetã€Jobç­‰å¯¹è±¡æ¥å®ŒæˆPodçš„è°ƒåº¦ä¸è‡ªæ„ˆåŠŸèƒ½ã€‚\n1. RCã€Deployment:å…¨è‡ªåŠ¨è°ƒåº¦ RCçš„åŠŸèƒ½å³ä¿æŒé›†ç¾¤ä¸­å§‹ç»ˆè¿è¡Œç€æŒ‡å®šä¸ªæ•°çš„Podã€‚\nåœ¨è°ƒåº¦ç­–ç•¥ä¸Šä¸»è¦æœ‰ï¼š\nç³»ç»Ÿå†…ç½®è°ƒåº¦ç®—æ³•[æœ€ä¼˜Node] NodeSelector[å®šå‘è°ƒåº¦] NodeAffinity[äº²å’Œæ€§è°ƒåº¦] 2. NodeSelector[å®šå‘è°ƒåº¦] k8sä¸­kube-schedulerè´Ÿè´£å®ç°Podçš„è°ƒåº¦ï¼Œå†…éƒ¨ç³»ç»Ÿé€šè¿‡ä¸€ç³»åˆ—ç®—æ³•æœ€ç»ˆè®¡ç®—å‡ºæœ€ä½³çš„ç›®æ ‡èŠ‚ç‚¹ã€‚å¦‚æœéœ€è¦å°†Podè°ƒåº¦åˆ°æŒ‡å®šNodeä¸Šï¼Œåˆ™å¯ä»¥é€šè¿‡Nodeçš„æ ‡ç­¾ï¼ˆLabelï¼‰å’ŒPodçš„nodeSelectorå±æ€§ç›¸åŒ¹é…æ¥è¾¾åˆ°ç›®çš„ã€‚\n1ã€kubectl label nodes {node-name} {label-key}={label-value}\n2ã€nodeSelector: {label-key}:{label-value}\nå¦‚æœç»™å¤šä¸ªNodeæ‰“äº†ç›¸åŒçš„æ ‡ç­¾ï¼Œåˆ™schedulerä¼šæ ¹æ®è°ƒåº¦ç®—æ³•ä»è¿™ç»„Nodeä¸­é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„Nodeæ¥è°ƒåº¦ã€‚\nå¦‚æœPodçš„nodeSelectorçš„æ ‡ç­¾åœ¨Nodeä¸­æ²¡æœ‰å¯¹åº”çš„æ ‡ç­¾ï¼Œåˆ™è¯¥Podæ— æ³•è¢«è°ƒåº¦æˆåŠŸã€‚\nNodeæ ‡ç­¾çš„ä½¿ç”¨åœºæ™¯ï¼š\nå¯¹é›†ç¾¤ä¸­ä¸åŒç±»å‹çš„Nodeæ‰“ä¸Šä¸åŒçš„æ ‡ç­¾ï¼Œå¯æ§åˆ¶åº”ç”¨è¿è¡ŒNodeçš„èŒƒå›´ã€‚ä¾‹å¦‚role=frontend;role=backend;role=databaseã€‚\n3. NodeAffinity[äº²å’Œæ€§è°ƒåº¦] NodeAffinityæ„ä¸ºNodeäº²å’Œæ€§è°ƒåº¦ç­–ç•¥ï¼ŒNodeSelectorä¸ºç²¾ç¡®åŒ¹é…ï¼ŒNodeAffinityä¸ºæ¡ä»¶èŒƒå›´åŒ¹é…ï¼Œé€šè¿‡Inï¼ˆå±äºï¼‰ã€NotInï¼ˆä¸å±äºï¼‰ã€Existsï¼ˆå­˜åœ¨ä¸€ä¸ªæ¡ä»¶ï¼‰ã€DoesNotExistï¼ˆä¸å­˜åœ¨ï¼‰ã€Gtï¼ˆå¤§äºï¼‰ã€Ltï¼ˆå°äºï¼‰ç­‰æ“ä½œç¬¦æ¥é€‰æ‹©Nodeï¼Œä½¿è°ƒåº¦æ›´åŠ çµæ´»ã€‚\nRequiredDuringSchedulingRequiredDuringExecutionï¼šç±»ä¼¼äºNodeSelectorï¼Œä½†åœ¨Nodeä¸æ»¡è¶³æ¡ä»¶æ—¶ï¼Œç³»ç»Ÿå°†ä»è¯¥Nodeä¸Šç§»é™¤ä¹‹å‰è°ƒåº¦ä¸Šçš„Podã€‚ RequiredDuringSchedulingIgnoredDuringExecutionï¼šä¸ä¸Šä¸€ä¸ªç±»ä¼¼ï¼ŒåŒºåˆ«æ˜¯åœ¨Nodeä¸æ»¡è¶³æ¡ä»¶æ—¶ï¼Œç³»ç»Ÿä¸ä¸€å®šä»è¯¥Nodeä¸Šç§»é™¤ä¹‹å‰è°ƒåº¦ä¸Šçš„Podã€‚ PreferredDuringSchedulingIgnoredDuringExecutionï¼šæŒ‡å®šåœ¨æ»¡è¶³è°ƒåº¦æ¡ä»¶çš„Nodeä¸­ï¼Œå“ªäº›Nodeåº”æ›´ä¼˜å…ˆåœ°è¿›è¡Œè°ƒåº¦ã€‚åŒæ—¶åœ¨Nodeä¸æ»¡è¶³æ¡ä»¶æ—¶ï¼Œç³»ç»Ÿä¸ä¸€å®šä»è¯¥Nodeä¸Šç§»é™¤ä¹‹å‰è°ƒåº¦ä¸Šçš„Podã€‚ å¦‚æœåŒæ—¶è®¾ç½®äº†NodeSelectorå’ŒNodeAffinityï¼Œåˆ™ç³»ç»Ÿå°†éœ€è¦åŒæ—¶æ»¡è¶³ä¸¤è€…çš„è®¾ç½®æ‰èƒ½è¿›è¡Œè°ƒåº¦ã€‚\n4. DaemonSetï¼šç‰¹å®šåœºæ™¯è°ƒåº¦ DaemonSetæ˜¯kubernetes1.2ç‰ˆæœ¬æ–°å¢çš„ä¸€ç§èµ„æºå¯¹è±¡ï¼Œç”¨äºç®¡ç†åœ¨é›†ç¾¤ä¸­æ¯ä¸ªNodeä¸Šä»…è¿è¡Œä¸€ä»½Podçš„å‰¯æœ¬å®ä¾‹ã€‚\nè¯¥ç”¨æ³•é€‚ç”¨çš„åº”ç”¨åœºæ™¯ï¼š\nåœ¨æ¯ä¸ªNodeä¸Šè¿è¡Œä¸€ä¸ªGlusterFSå­˜å‚¨æˆ–è€…Cephå­˜å‚¨çš„daemonè¿›ç¨‹ã€‚ åœ¨æ¯ä¸ªNodeä¸Šè¿è¡Œä¸€ä¸ªæ—¥å¿—é‡‡é›†ç¨‹åºï¼šfluentdæˆ–logstachã€‚ åœ¨æ¯ä¸ªNodeä¸Šè¿è¡Œä¸€ä¸ªå¥åº·ç¨‹åºï¼Œé‡‡é›†è¯¥Nodeçš„è¿è¡Œæ€§èƒ½æ•°æ®ï¼Œä¾‹å¦‚ï¼šPrometheus Node Exportorã€collectdã€New Relic agentæˆ–Ganglia gmondç­‰ã€‚ DaemonSetçš„Podè°ƒåº¦ç­–ç•¥ä¸RCç±»ä¼¼ï¼Œé™¤äº†ä½¿ç”¨ç³»ç»Ÿå†…ç½®ç®—æ³•åœ¨æ¯å°Nodeä¸Šè¿›è¡Œè°ƒåº¦ï¼Œä¹Ÿå¯ä»¥é€šè¿‡NodeSelectoræˆ–NodeAffinityæ¥æŒ‡å®šæ»¡è¶³æ¡ä»¶çš„NodeèŒƒå›´è¿›è¡Œè°ƒåº¦ã€‚\n5. Jobï¼šæ‰¹å¤„ç†è°ƒåº¦ kubernetesä»1.2ç‰ˆæœ¬å¼€å§‹æ”¯æŒæ‰¹å¤„ç†ç±»å‹çš„åº”ç”¨ï¼Œå¯ä»¥é€šè¿‡kubernetes Jobèµ„æºå¯¹è±¡æ¥å®šä¹‰å¹¶å¯åŠ¨ä¸€ä¸ªæ‰¹å¤„ç†ä»»åŠ¡ã€‚æ‰¹å¤„ç†ä»»åŠ¡é€šå¸¸å¹¶è¡Œï¼ˆæˆ–ä¸²è¡Œï¼‰å¯åŠ¨å¤šä¸ªè®¡ç®—è¿›ç¨‹å»å¤„ç†ä¸€æ‰¹å·¥ä½œé¡¹ï¼ˆwork itemï¼‰ï¼Œå¤„ç†å®Œåï¼Œæ•´ä¸ªæ‰¹å¤„ç†ä»»åŠ¡ç»“æŸã€‚\n5.1. æ‰¹å¤„ç†çš„ä¸‰ç§æ¨¡å¼ æ‰¹å¤„ç†æŒ‰ä»»åŠ¡å®ç°æ–¹å¼ä¸åŒåˆ†ä¸ºä»¥ä¸‹å‡ ç§æ¨¡å¼ï¼š\nJob Template Expansionæ¨¡å¼ ä¸€ä¸ªJobå¯¹è±¡å¯¹åº”ä¸€ä¸ªå¾…å¤„ç†çš„Work itemï¼Œæœ‰å‡ ä¸ªWork itemå°±äº§ç”Ÿå‡ ä¸ªç‹¬ç«‹çš„Jobï¼Œé€šè¿‡é€‚ç”¨äºWork itemæ•°é‡å°‘ï¼Œæ¯ä¸ªWork itemè¦å¤„ç†çš„æ•°æ®é‡æ¯”è¾ƒå¤§çš„åœºæ™¯ã€‚ä¾‹å¦‚æœ‰10ä¸ªæ–‡ä»¶ï¼ˆWork itemï¼‰,æ¯ä¸ªæ–‡ä»¶ï¼ˆWork itemï¼‰ä¸º100Gã€‚\nQueue with Pod Per Work Item é‡‡ç”¨ä¸€ä¸ªä»»åŠ¡é˜Ÿåˆ—å­˜æ”¾Work itemï¼Œä¸€ä¸ªJobå¯¹è±¡ä½œä¸ºæ¶ˆè´¹è€…å»å®Œæˆè¿™äº›Work itemï¼Œå…¶ä¸­Jobä¼šå¯åŠ¨Nä¸ªPodï¼Œæ¯ä¸ªPodå¯¹åº”ä¸€ä¸ªWork itemã€‚\nQueue with Variable Pod Count é‡‡ç”¨ä¸€ä¸ªä»»åŠ¡é˜Ÿåˆ—å­˜æ”¾Work itemï¼Œä¸€ä¸ªJobå¯¹è±¡ä½œä¸ºæ¶ˆè´¹è€…å»å®Œæˆè¿™äº›Work itemï¼Œå…¶ä¸­Jobä¼šå¯åŠ¨Nä¸ªPodï¼Œæ¯ä¸ªPodå¯¹åº”ä¸€ä¸ªWork itemã€‚ä½†Podçš„æ•°é‡æ˜¯å¯å˜çš„ã€‚\n5.2. Jobçš„ä¸‰ç§ç±»å‹ 1ï¼‰Non-parallel Jobs\né€šå¸¸ä¸€ä¸ªJobåªå¯åŠ¨ä¸€ä¸ªPod,é™¤éPodå¼‚å¸¸æ‰ä¼šé‡å¯è¯¥Pod,ä¸€æ—¦æ­¤Podæ­£å¸¸ç»“æŸï¼ŒJobå°†ç»“æŸã€‚\n2ï¼‰Parallel Jobs with a fixed completion count\nå¹¶è¡ŒJobä¼šå¯åŠ¨å¤šä¸ªPodï¼Œæ­¤æ—¶éœ€è¦è®¾å®šJobçš„.spec.completionså‚æ•°ä¸ºä¸€ä¸ªæ­£æ•°ï¼Œå½“æ­£å¸¸ç»“æŸçš„Podæ•°é‡è¾¾åˆ°è¯¥å€¼åˆ™Jobç»“æŸã€‚\n3ï¼‰Parallel Jobs with a work queue\nä»»åŠ¡é˜Ÿåˆ—æ–¹å¼çš„å¹¶è¡ŒJobéœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„Queueï¼ŒWork iteméƒ½åœ¨ä¸€ä¸ªQueueä¸­å­˜æ”¾ï¼Œä¸èƒ½è®¾ç½®Jobçš„.spec.completionså‚æ•°ã€‚\næ­¤æ—¶Jobçš„ç‰¹æ€§ï¼š\næ¯ä¸ªPodèƒ½ç‹¬ç«‹åˆ¤æ–­å’Œå†³å®šæ˜¯å¦è¿˜æœ‰ä»»åŠ¡é¡¹éœ€è¦å¤„ç† å¦‚æœæŸä¸ªPodæ­£å¸¸ç»“æŸï¼Œåˆ™Jobä¸ä¼šå†å¯åŠ¨æ–°çš„Pod å¦‚æœä¸€ä¸ªPodæˆåŠŸç»“æŸï¼Œåˆ™æ­¤æ—¶åº”è¯¥ä¸å­˜åœ¨å…¶ä»–Podè¿˜åœ¨å¹²æ´»çš„æƒ…å†µï¼Œå®ƒä»¬åº”è¯¥éƒ½å¤„äºå³å°†ç»“æŸã€é€€å‡ºçš„çŠ¶æ€ å¦‚æœæ‰€æœ‰çš„Podéƒ½ç»“æŸäº†ï¼Œä¸”è‡³å°‘ä¸€ä¸ªPodæˆåŠŸç»“æŸï¼Œåˆ™æ•´ä¸ªJobç®—æ˜¯æˆåŠŸç»“æŸ å‚è€ƒæ–‡ç« \nã€ŠKubernetesæƒå¨æŒ‡å—ã€‹ ","categories":"","description":"","excerpt":"Podè°ƒåº¦ åœ¨kubernetesé›†ç¾¤ä¸­ï¼ŒPodï¼ˆcontainerï¼‰æ˜¯åº”ç”¨çš„è½½ä½“ï¼Œä¸€èˆ¬é€š â€¦","ref":"/kubernetes-notes/concepts/pod/pod-scheduler/","tags":["Kubernetes"],"title":"Podè°ƒåº¦"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/cilium/","tags":"","title":"Cilium"},{"body":"æœ¬æ–‡ä¸»è¦åˆ†æDaemonSetControllerçš„æºç é€»è¾‘ï¼Œdaemonsetæ˜¯è¿è¡Œåœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœåŠ¡ï¼Œå¸¸ç”¨æ¥ä½œä¸ºagentç±»çš„æœåŠ¡æ¥é…ç½®ï¼Œä¹Ÿæ˜¯k8sæœ€å¸¸ç”¨çš„æ§åˆ¶å™¨ä¹‹ä¸€ã€‚\n1. startDaemonSetController startDaemonSetControlleræ˜¯å…¥å£å‡½æ•°ï¼Œå…ˆNewåRunã€‚\nfunc startDaemonSetController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) { dsc, err := daemon.NewDaemonSetsController( ctx, controllerContext.InformerFactory.Apps().V1().DaemonSets(), controllerContext.InformerFactory.Apps().V1().ControllerRevisions(), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.InformerFactory.Core().V1().Nodes(), controllerContext.ClientBuilder.ClientOrDie(\"daemon-set-controller\"), flowcontrol.NewBackOff(1*time.Second, 15*time.Minute), ) if err != nil { return nil, true, fmt.Errorf(\"error creating DaemonSets controller: %v\", err) } go dsc.Run(ctx, int(controllerContext.ComponentConfig.DaemonSetController.ConcurrentDaemonSetSyncs)) return nil, true, nil } 2. NewDaemonSetsController NewDaemonSetsControllerä»ç„¶æ˜¯å¸¸è§çš„k8s controlleråˆå§‹åŒ–é€»è¾‘:\nå¸¸ç”¨é…ç½®åˆå§‹åŒ– æ ¹æ®æ‰€éœ€è¦çš„å¯¹è±¡ï¼Œæ·»åŠ event handlerï¼Œä¾¿äºç›‘å¬æ‰€éœ€è¦å¯¹è±¡çš„äº‹ä»¶å˜åŒ–ï¼Œæ­¤å¤„åŒ…æ‹¬ds, podï¼Œnodeä¸‰ä¸ªå¯¹è±¡ã€‚ èµ‹å€¼syncHandlerï¼Œå³å…·ä½“å®ç°æ˜¯syncDaemonSetå‡½æ•°ã€‚ func NewDaemonSetsController( ctx context.Context, daemonSetInformer appsinformers.DaemonSetInformer, historyInformer appsinformers.ControllerRevisionInformer, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, kubeClient clientset.Interface, failedPodsBackoff *flowcontrol.Backoff, ) (*DaemonSetsController, error) { // å¸¸ç”¨é…ç½®åˆå§‹åŒ– dsc := \u0026DaemonSetsController{ kubeClient: kubeClient, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"daemonset-controller\"}), podControl: controller.RealPodControl{ KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"daemonset-controller\"}), }, crControl: controller.RealControllerRevisionControl{ KubeClient: kubeClient, }, burstReplicas: BurstReplicas, expectations: controller.NewControllerExpectations(), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"daemonset\"), } // æ·»åŠ event handler daemonSetInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addDaemonset(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updateDaemonset(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dsc.deleteDaemonset(logger, obj) }, }) dsc.dsLister = daemonSetInformer.Lister() dsc.dsStoreSynced = daemonSetInformer.Informer().HasSynced historyInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addHistory(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updateHistory(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dsc.deleteHistory(logger, obj) }, }) dsc.historyLister = historyInformer.Lister() dsc.historyStoreSynced = historyInformer.Informer().HasSynced // æ·»åŠ podçš„event handler podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addPod(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updatePod(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dsc.deletePod(logger, obj) }, }) dsc.podLister = podInformer.Lister() dsc.podStoreSynced = podInformer.Informer().HasSynced // æ·»åŠ nodeçš„event handler nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dsc.addNode(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dsc.updateNode(logger, oldObj, newObj) }, }, ) dsc.nodeStoreSynced = nodeInformer.Informer().HasSynced dsc.nodeLister = nodeInformer.Lister() // èµ‹å€¼syncHandlerï¼Œå…·ä½“çš„controllerå¤„ç†ä»£ç  dsc.syncHandler = dsc.syncDaemonSet dsc.enqueueDaemonSet = dsc.enqueue dsc.failedPodsBackoff = failedPodsBackoff return dsc, nil } 3. Run Runå‡½æ•°ä¸å…¶ä»–controllerçš„é€»è¾‘ä¸€è‡´ä¸å†åˆ†æï¼Œå…·ä½“å¯ä»¥é˜…è¯»æœ¬æºç åˆ†æç³»åˆ—çš„replicaset-controlleråˆ†æã€‚\nfunc (dsc *DaemonSetsController) Run(ctx context.Context, workers int) { defer utilruntime.HandleCrash() dsc.eventBroadcaster.StartStructuredLogging(0) dsc.eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{Interface: dsc.kubeClient.CoreV1().Events(\"\")}) defer dsc.eventBroadcaster.Shutdown() defer dsc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info(\"Starting daemon sets controller\") defer logger.Info(\"Shutting down daemon sets controller\") if !cache.WaitForNamedCacheSync(\"daemon sets\", ctx.Done(), dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) { return } for i := 0; i \u003c workers; i++ { go wait.UntilWithContext(ctx, dsc.runWorker, time.Second) } go wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, ctx.Done()) \u003c-ctx.Done() } 3.1. processNextWorkItem processNextWorkItemå¯å‚è€ƒreplicaset-controllerå¯¹è¯¥éƒ¨åˆ†çš„åˆ†æã€‚\nfunc (dsc *DaemonSetsController) runWorker(ctx context.Context) { for dsc.processNextWorkItem(ctx) { } } // processNextWorkItem deals with one key off the queue. It returns false when it's time to quit. func (dsc *DaemonSetsController) processNextWorkItem(ctx context.Context) bool { dsKey, quit := dsc.queue.Get() if quit { return false } defer dsc.queue.Done(dsKey) err := dsc.syncHandler(ctx, dsKey.(string)) if err == nil { dsc.queue.Forget(dsKey) return true } utilruntime.HandleError(fmt.Errorf(\"%v failed with : %v\", dsKey, err)) dsc.queue.AddRateLimited(dsKey) return true } 4. syncDaemonSet syncDaemonSetæ˜¯æ§åˆ¶å™¨å…·ä½“çš„å®ç°é€»è¾‘ï¼Œå³æ¯ä¸ªæ§åˆ¶å™¨çš„æ ¸å¿ƒå®ç°å¤§éƒ¨åˆ†åœ¨syncHandlerè¿™ä¸ªå‡½æ•°ä¸­ã€‚\nfunc (dsc *DaemonSetsController) syncDaemonSet(ctx context.Context, key string) error { // ä¸ºäº†çªå‡ºä»£ç é‡ç‚¹ï¼Œå·²åˆ é™¤éå¿…è¦éƒ¨åˆ†ã€‚ // è·å–é›†ç¾¤ä¸­çš„dså¯¹è±¡ namespace, name, err := cache.SplitMetaNamespaceKey(key) ds, err := dsc.dsLister.DaemonSets(namespace).Get(name) // è·å–æœºå™¨åˆ—è¡¨ nodeList, err := dsc.nodeLister.List(labels.Everything()) everything := metav1.LabelSelector{} if reflect.DeepEqual(ds.Spec.Selector, \u0026everything) { dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, SelectingAllReason, \"This daemon set is selecting all pods. A non-empty selector is required.\") return nil } // Construct histories of the DaemonSet, and get the hash of current history cur, old, err := dsc.constructHistory(ctx, ds) hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey] if !dsc.expectations.SatisfiedExpectations(logger, dsKey) { // Only update status. Don't raise observedGeneration since controller didn't process object of that generation. return dsc.updateDaemonSetStatus(ctx, ds, nodeList, hash, false) } // å¤„ç†daemonsetä¸­çš„podåˆ›å»ºåŠåˆ é™¤ err = dsc.updateDaemonSet(ctx, ds, nodeList, hash, dsKey, old) // æ›´æ–°çŠ¶æ€ statusErr := dsc.updateDaemonSetStatus(ctx, ds, nodeList, hash, true) switch { case err != nil \u0026\u0026 statusErr != nil: logger.Error(statusErr, \"Failed to update status\", \"daemonSet\", klog.KObj(ds)) return err case err != nil: return err case statusErr != nil: return statusErr } return nil } 5. manage manageæ˜¯å…·ä½“çš„daemonsetçš„podåˆ›å»ºåŠåˆ é™¤çš„å…·ä½“ä»£ç ã€‚manageå…¥å£åœ¨updateDaemonSetçš„å‡½æ•°ä¸­ã€‚\nfunc (dsc *DaemonSetsController) updateDaemonSet(ctx context.Context, ds *apps.DaemonSet, nodeList []*v1.Node, hash, key string, old []*apps.ControllerRevision) error { // manageå¤„ç†podçš„åˆ›å»ºåŠåˆ é™¤ err := dsc.manage(ctx, ds, nodeList, hash) err = dsc.cleanupHistory(ctx, ds, old) if err != nil { return fmt.Errorf(\"failed to clean up revisions of DaemonSet: %w\", err) } return nil } ä»¥ä¸‹æ˜¯manageçš„å…·ä½“ä»£ç \nfunc (dsc *DaemonSetsController) manage(ctx context.Context, ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error { // æŸ¥æ‰¾daemonsetä¸­podå’Œnodeçš„æ˜ å°„ nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ctx, ds, false) // è®¡ç®—éœ€è¦åˆ›å»ºå’Œåˆ é™¤çš„pod var nodesNeedingDaemonPods, podsToDelete []string for _, node := range nodeList { nodesNeedingDaemonPodsOnNode, podsToDeleteOnNode := dsc.podsShouldBeOnNode( logger, node, nodeToDaemonPods, ds, hash) nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, nodesNeedingDaemonPodsOnNode...) podsToDelete = append(podsToDelete, podsToDeleteOnNode...) } // Remove unscheduled pods assigned to not existing nodes when daemonset pods are scheduled by scheduler. // If node doesn't exist then pods are never scheduled and can't be deleted by PodGCController. podsToDelete = append(podsToDelete, getUnscheduledPodsWithoutNode(nodeList, nodeToDaemonPods)...) // æ ¹æ®ä¸Šè¿°çš„è®¡ç®—ç»“æœï¼Œå®ç°å…·ä½“åˆ›å»ºå’Œåˆ é™¤podçš„æ“ä½œ if err = dsc.syncNodes(ctx, ds, podsToDelete, nodesNeedingDaemonPods, hash); err != nil { return err } return nil } 6. syncNodes syncNodesæ ¹æ®ä¼ å…¥çš„éœ€è¦åˆ›å»ºå’Œåˆ é™¤çš„podï¼Œå®ç°å…·ä½“çš„åˆ›å»ºå’Œåˆ é™¤podçš„æ“ä½œã€‚\nfunc (dsc *DaemonSetsController) syncNodes(ctx context.Context, ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error { // å·²åˆ é™¤æ¬¡è¦ä»£ç  dsKey, err := controller.KeyFunc(ds) batchSize := integer.IntMin(createDiff, controller.SlowStartInitialBatchSize) for pos := 0; createDiff \u003e pos; batchSize, pos = integer.IntMin(2*batchSize, createDiff-(pos+batchSize)), pos+batchSize { errorCount := len(errCh) createWait.Add(batchSize) for i := pos; i \u003c pos+batchSize; i++ { go func(ix int) { defer createWait.Done() // æ‰¹é‡åˆ›å»ºpod err := dsc.podControl.CreatePods(ctx, ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) }(i) } createWait.Wait() } deleteWait := sync.WaitGroup{} deleteWait.Add(deleteDiff) for i := 0; i \u003c deleteDiff; i++ { go func(ix int) { defer deleteWait.Done() // æ‰¹é‡åˆ é™¤pod if err := dsc.podControl.DeletePod(ctx, ds.Namespace, podsToDelete[ix], ds); err != nil { dsc.expectations.DeletionObserved(logger, dsKey) } }(i) } deleteWait.Wait() // å¤„ç†é”™è¯¯ errors := []error{} close(errCh) for err := range errCh { errors = append(errors, err) } return utilerrors.NewAggregate(errors) } å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/controller/daemon/daemon_controller.go ","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦åˆ†æDaemonSetControllerçš„æºç é€»è¾‘ï¼Œdaemonsetæ˜¯è¿è¡Œåœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœåŠ¡ï¼Œå¸¸ç”¨æ¥ä½œä¸ºagentç±»çš„æœåŠ¡æ¥é… â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/daemonset-controller/","tags":["æºç åˆ†æ"],"title":"kube-controller-manageræºç åˆ†æï¼ˆäº”ï¼‰ä¹‹ DaemonSetController"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/helm/","tags":"","title":"helmå·¥å…·"},{"body":"1.27 å‚è€ƒï¼š\nKubernetes åœ¨ v1.27 ä¸­ç§»é™¤çš„ç‰¹æ€§å’Œä¸»è¦å˜æ›´\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md#changelog-since-v1260\n202304 | K8s 1.27 æ­£å¼å‘å¸ƒ - DaoCloud Enterprise\nï¼ˆä¸€ï¼‰é‡è¦æ›´æ–° k8s.gcr.io é‡å®šå‘åˆ° registry.k8s.io ç›¸å…³è¯´æ˜ Kubernetes é¡¹ç›®ä¸ºäº†æ‰˜ç®¡å…¶å®¹å™¨é•œåƒï¼Œä½¿ç”¨ç¤¾åŒºæ‹¥æœ‰çš„ä¸€ä¸ªåä¸º registry.k8s.io. çš„é•œåƒä»“åº“ã€‚ä» 3 æœˆ 20 æ—¥èµ·ï¼Œæ‰€æœ‰æ¥è‡ªè¿‡æœŸÂ k8s.gcr.ioÂ ä»“åº“çš„æµé‡å°†è¢«é‡å®šå‘åˆ°Â registry.k8s.ioã€‚ å·²å¼ƒç”¨çš„ k8s.gcr.io ä»“åº“æœ€ç»ˆå°†è¢«æ·˜æ±°ã€‚Kubernetes v1.27 ç‰ˆæœ¬ä¸ä¼šå‘å¸ƒåˆ°æ—§çš„ä»“åº“ã€‚\nåŸåœ°è°ƒæ•´ Pod èµ„æº (alpha) å‚è€ƒï¼šKubernetes 1.27: åŸåœ°è°ƒæ•´ Pod èµ„æº (alpha) | Kubernetes\nåœ¨ Kubernetes v1.27 ä¸­ï¼Œæ·»åŠ äº†ä¸€ä¸ªæ–°çš„ alpha ç‰¹æ€§ï¼Œå…è®¸ç”¨æˆ·è°ƒæ•´åˆ†é…ç»™ Pod çš„ CPU å’Œå†…å­˜èµ„æºå¤§å°ï¼Œè€Œæ— éœ€é‡æ–°å¯åŠ¨å®¹å™¨ã€‚ é¦–å…ˆï¼ŒAPI å±‚é¢ç°åœ¨å…è®¸ä¿®æ”¹ Pod å®¹å™¨ä¸­çš„Â resourcesÂ å­—æ®µä¸‹çš„Â cpuÂ å’ŒÂ memoryÂ èµ„æºã€‚èµ„æºä¿®æ”¹åªéœ€ patch æ­£åœ¨è¿è¡Œçš„ pod è§„çº¦å³å¯ã€‚\nStatefulSet PVC è‡ªåŠ¨åˆ é™¤åŠŸèƒ½ç‰¹æ€§ Beta åœ¨ v1.23 ä¸­å¼•å…¥çš„Â StatefulSetAutoDeletePVC åŠŸèƒ½å°†åœ¨ 1.27 ç‰ˆæœ¬ä¸­å‡çº§ä¸º Betaï¼Œå¹¶é»˜è®¤å¼€å¯ã€‚Â ç„¶è€Œï¼Œé»˜è®¤å¼€å¯å¹¶ä¸æ„å‘³ç€æ‰€æœ‰ StatefulSet çš„ PVC éƒ½å°†è‡ªåŠ¨åˆ é™¤ã€‚\nä¼˜åŒ–å¤§å‹é›†ç¾¤ä¸­ kube-proxy çš„ iptables æ¨¡å¼æ€§èƒ½ åŠŸèƒ½ MinimizeIPTablesRestoreÂ åœ¨ 1.26 ç‰ˆæœ¬ä¸­å¼•å…¥ï¼Œå¹¶åœ¨ 1.27 ç‰ˆæœ¬ä¸­å‡çº§ä¸º Beta å¹¶é»˜è®¤å¯ç”¨ã€‚ è¯¥åŠŸèƒ½æ—¨åœ¨æ”¹å–„å¤§å‹é›†ç¾¤ä¸­ kube-proxy çš„ iptables æ¨¡å¼æ€§èƒ½ã€‚\nå¦‚æœæ‚¨é‡åˆ° Service ä¿¡æ¯æœªæ­£ç¡®åŒæ­¥åˆ° iptables çš„é—®é¢˜ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°† kube-proxy å¯åŠ¨å‚æ•°è®¾ç½®ä¸ºÂ --feature-gates=MinimizeIPTablesRestore=falseÂ æ¥ç¦ç”¨è¯¥åŠŸèƒ½ï¼ˆå¹¶å‘ç¤¾åŒºæäº¤é—®é¢˜ï¼‰ã€‚ æ‚¨è¿˜å¯ä»¥æŸ¥çœ‹ kube-proxy çš„ metrics ä¿¡æ¯ä¸­çš„ sync_proxy_rules_iptables_partial_restore_failures_total æŒ‡æ ‡æ¥ç›‘æ§è§„åˆ™åŒæ­¥å¤±è´¥çš„æ¬¡æ•°ã€‚\nKubelet äº‹ä»¶é©±åŠ¨ PLEG å‡çº§ä¸º Beta åœ¨èŠ‚ç‚¹ Pod è¾ƒå¤šçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å®¹å™¨è¿è¡Œæ—¶çš„ Event é©±åŠ¨ Pod çŠ¶æ€æ›´æ–°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ•ˆç‡ã€‚Â åœ¨ 1.27 ä¸­ï¼Œè¯¥åŠŸèƒ½å·²ç»è¾¾åˆ°äº† Beta æ¡ä»¶ï¼ŒåŸºç¡€çš„ E2E æµ‹è¯•ä»»åŠ¡å·²ç»æ·»åŠ ã€‚ ä¹‹æ‰€ä»¥é»˜è®¤å…³é—­è¯¥åŠŸèƒ½ï¼Œæ˜¯å› ä¸ºç¤¾åŒºè®¤ä¸ºè¯¥åŠŸèƒ½è¿˜éœ€è¦è¡¥å……ä»¥ä¸‹éªŒè¯ï¼šå‹åŠ›æµ‹è¯•ã€æ¢å¤æµ‹è¯•å’Œå¸¦é€€é¿é€»è¾‘çš„é‡è¯•ã€‚\nPod è°ƒåº¦å°±ç»ªæ€åŠŸèƒ½å¢å¼º è°ƒåº¦å°±ç»ªæ€åŠŸèƒ½ PodSchedulingReadinessï¼Œåœ¨ v1.26 ä½œä¸º Alpha åŠŸèƒ½å¼•å…¥ï¼Œä» v1.27 å¼€å§‹è¯¥åŠŸèƒ½å‡çº§ä¸º Betaï¼Œé»˜è®¤å¼€å¯ã€‚\nDeployment æ»šåŠ¨æ›´æ–°è¿‡ç¨‹ä¸­çš„è°ƒåº¦ä¼˜åŒ– åœ¨ v1.27 ä¸­ï¼ŒPodTopologySpread è°ƒåº¦ç­–ç•¥å¯ä»¥åŒºåˆ†è°ƒåº¦ Pod æ ‡ç­¾çš„å€¼Â ï¼ˆè¿™é‡Œé€šå¸¸æŒ‡ Pod çš„ pod-template-hash æ ‡ç­¾ï¼Œä¸åŒ replica set å¯¹åº”çš„ Pod è¯¥æ ‡ç­¾çš„å€¼ä¸åŒï¼‰ï¼Œ è¿™æ ·æ»šåŠ¨æ›´æ–°åï¼Œæ–°çš„ Pod å®ä¾‹ä¼šè¢«è°ƒåº¦å¾—æ›´åŠ å‡åŒ€Â ã€‚\nå…³äºåŠ å¿« Pod å¯åŠ¨çš„è¿›å±• è¦å¯ç”¨å¹¶è¡Œé•œåƒæ‹‰å–ï¼Œè¯·åœ¨ kubelet é…ç½®ä¸­å°†Â serializeImagePullsÂ å­—æ®µè®¾ç½®ä¸º falseã€‚ å½“Â serializeImagePullsÂ è¢«ç¦ç”¨æ—¶ï¼Œå°†ç«‹å³å‘é•œåƒæœåŠ¡å‘é€é•œåƒæ‹‰å–è¯·æ±‚ï¼Œå¹¶å¯ä»¥å¹¶è¡Œæ‹‰å–å¤šä¸ªé•œåƒã€‚\nä¸ºäº†åœ¨èŠ‚ç‚¹ä¸Šå…·æœ‰å¤šä¸ª Pod çš„åœºæ™¯ä¸­åŠ å¿« Pod å¯åŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨çªç„¶æ‰©ç¼©çš„æƒ…å†µä¸‹ï¼Œ kubelet éœ€è¦åŒæ­¥ Pod çŠ¶æ€å¹¶å‡†å¤‡ ConfigMapã€Secret æˆ–å·ã€‚è¿™å°±éœ€è¦å¤§å¸¦å®½è®¿é—® kube-apiserverã€‚åœ¨ v1.27 ä¸­ï¼Œkubelet ä¸ºäº†æé«˜ Pod å¯åŠ¨æ€§èƒ½ï¼Œå°†è¿™äº›é»˜è®¤å€¼åˆ†åˆ«æé«˜åˆ°äº† 50 å’Œ 100ã€‚\nï¼ˆäºŒï¼‰å¼ƒç”¨å˜æ›´ kubelet ç§»é™¤äº†å‘½ä»¤è¡Œå‚æ•° --container-runtimeã€‚\nå¼ƒç”¨çš„å‘½ä»¤è¡Œå‚æ•°Â --pod-eviction-timeoutÂ å°†è¢«ä» kube-controller-manager ä¸­ç§»é™¤ã€‚\n1.26 å‚è€ƒï¼š\nKubernetes 1.26 ä¸­çš„ç§»é™¤ã€å¼ƒç”¨å’Œä¸»è¦å˜æ›´ | Kubernetes\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#changelog-since-v1250\nhttps://docs.daocloud.io/blogs/221209-k8s-1.26/\nhttps://www.alibabacloud.com/help/zh/ack/product-overview/kubernetes-1-26-release-notes?spm=a2c63.p38356.0.0.64624df9xXfEkZ\nï¼ˆä¸€ï¼‰é‡è¦æ›´æ–° Kubelet Evented PLEG for Better Performance è¯¥åŠŸèƒ½è®© kubelet åœ¨è·Ÿè¸ªèŠ‚ç‚¹ä¸­ Pod çŠ¶æ€æ—¶ï¼Œé€šè¿‡å°½å¯èƒ½ä¾èµ–å®¹å™¨è¿è¡Œæ—¶æ¥å£(CRI) çš„é€šçŸ¥æ¥å‡å°‘å®šæœŸè½®è®­ï¼Œè¿™ä¼šå‡å°‘ kubelet å¯¹ CPU çš„ä½¿ç”¨\næ–°å¢ Alpha Feature GateÂ â€”â€” EventedPLEG æ¥æ§åˆ¶æ˜¯å¦å¼€å¯è¯¥åŠŸèƒ½ã€‚\nä¼˜åŒ– kube-proxy æ€§èƒ½ï¼Œå®ƒåªå‘é€åœ¨è°ƒç”¨ iptables-restore ä¸­æ›´æ”¹çš„è§„åˆ™ï¼Œè€Œä¸æ˜¯æ•´ä¸ªè§„åˆ™é›† PR#112200Â client-go çš„ SharedInformerFactory å¢åŠ  Shutdown æ–¹æ³•ï¼Œæ¥ç­‰å¾… Factory å†…æ‰€æœ‰è¿è¡Œçš„ informer éƒ½ç»“æŸã€‚ ï¼ˆäºŒï¼‰å¼ƒç”¨å˜æ›´ Kubelet ä¸å†æ”¯æŒ v1alpha2 ç‰ˆæœ¬çš„ CRIï¼Œæ¥å…¥çš„å®¹å™¨è¿è¡Œæ—¶å¿…é¡»å®ç° v1 ç‰ˆæœ¬çš„å®¹å™¨è¿è¡Œæ—¶æ¥å£ã€‚ Kubernetes v1.26 å°†ä¸æ”¯æŒ containerd 1.5.x åŠæ›´æ—©çš„ç‰ˆæœ¬ï¼›éœ€è¦å‡çº§åˆ° containerd 1.6.x æˆ–æ›´é«˜ç‰ˆæœ¬åï¼Œæ‰èƒ½å°†è¯¥èŠ‚ç‚¹çš„ kubelet å‡çº§åˆ° 1.26ã€‚\n1.25 å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#whats-new-major-themes\nKubernetes 1.25 çš„ç§»é™¤è¯´æ˜å’Œä¸»è¦å˜æ›´ | Kubernetes\nï¼ˆä¸€ï¼‰é‡è¦æ›´æ–° cgroup v2 å‡çº§åˆ° GA Kubernetes 1.25 å°† cgroup v2 æ­£å¼å‘å¸ƒï¼ˆGAï¼‰ï¼Œ è®©kubeletä½¿ç”¨æœ€æ–°çš„å®¹å™¨èµ„æºç®¡ç†èƒ½åŠ›ã€‚ä¸€äº› Kubernetes ç‰¹æ€§ä¸“é—¨ä½¿ç”¨ cgroup v2 æ¥å¢å¼ºèµ„æºç®¡ç†å’Œéš”ç¦»ã€‚ ä¾‹å¦‚ï¼ŒMemoryQoS ç‰¹æ€§æé«˜äº†å†…å­˜åˆ©ç”¨ç‡å¹¶ä¾èµ– cgroup v2 åŠŸèƒ½æ¥å¯ç”¨å®ƒã€‚kubelet ä¸­çš„æ–°èµ„æºç®¡ç†ç‰¹æ€§ä¹Ÿå°†åˆ©ç”¨æ–°çš„ cgroup v2 ç‰¹æ€§å‘å‰å‘å±•ã€‚\nCSI å†…è”å­˜å‚¨å·æ­£å¼å‘å¸ƒGA CSI å†…è”å­˜å‚¨å·ä¸å…¶ä»–ç±»å‹çš„ä¸´æ—¶å·ç›¸ä¼¼ï¼Œå¦‚Â configMapã€downwardAPIÂ å’ŒÂ secretã€‚ é‡è¦çš„åŒºåˆ«æ˜¯ï¼Œå­˜å‚¨æ˜¯ç”± CSI é©±åŠ¨æä¾›çš„ï¼Œå®ƒå…è®¸ä½¿ç”¨ç¬¬ä¸‰æ–¹ä¾›åº”å•†æä¾›çš„ä¸´æ—¶å­˜å‚¨ã€‚ å·è¢«å®šä¹‰ä¸º Pod è§„çº¦çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶éµå¾ª Pod çš„ç”Ÿå‘½å‘¨æœŸï¼Œè¿™æ„å‘³ç€å·éšç€ Pod çš„è°ƒåº¦è€Œåˆ›å»ºï¼Œå¹¶éšç€ Pod çš„é”€æ¯è€Œé”€æ¯ã€‚\nEphemeral Containersè¿›å…¥ç¨³å®šç‰ˆæœ¬ å½“pod crashçš„æ—¶å€™ï¼Œæ— æ³•é€šè¿‡kubectl exec è¿›å…¥å®¹å™¨ï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥é€šè¿‡ä¸´æ—¶å®¹å™¨[Ephemeral Containers](ä¸´æ—¶å®¹å™¨ | Kubernetes)\nï¼ˆäºŒï¼‰å¼ƒç”¨å˜æ›´ Kubernetes v1.25 å°†ç§»é™¤ PodSecurityPolicyï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ Pod Security Admissionï¼ˆå³ PodSecurity å®‰å…¨å‡†å…¥æ§åˆ¶å™¨ï¼‰ã€‚\næ¸…ç† IPTables é“¾çš„æ‰€æœ‰æƒÂ ä» v1.25 å¼€å§‹ï¼ŒKubelet å°†é€æ¸è¿ç§»ä¸ºä¸åœ¨Â natÂ è¡¨ä¸­åˆ›å»ºä»¥ä¸‹ iptables é“¾ï¼š\nKUBE-MARK-DROP KUBE-MARK-MASQ KUBE-POSTROUTING 1.24 æœ€æ–°å‘è¡Œç‰ˆæœ¬ï¼š1.24.2 (å‘å¸ƒæ—¥æœŸ:Â 2022-06-15ï¼‰\nä¸å†æ”¯æŒï¼š2023-09-29\nè¡¥ä¸ç‰ˆæœ¬ï¼šÂ 1.24.1ã€Â 1.24.2\nComplete 1.24Â ScheduleÂ andÂ Changelog\nKubernetes 1.24 ä½¿ç”¨ go1.18æ„å»ºï¼Œé»˜è®¤æƒ…å†µä¸‹å°†ä¸å†éªŒè¯ä½¿ç”¨ SHA-1 å“ˆå¸Œç®—æ³•ç­¾åçš„è¯ä¹¦ã€‚\nï¼ˆä¸€ï¼‰é‡è¦æ›´æ–° 1.24.0ä¸»è¦å‚è€ƒkubernetes/CHANGELOG-1.24.major-themes\n1ï¼‰kubeletå®Œå…¨ç§»é™¤Dockershimã€æœ€é‡å¤§æ›´æ–°ã€‘ åœ¨ v1.20 ä¸­å¼ƒç”¨åï¼Œdockershim ç»„ä»¶å·²ä» kubelet ä¸­åˆ é™¤ã€‚ä» v1.24 å¼€å§‹ï¼Œæ‚¨å°†éœ€è¦ä½¿ç”¨å…¶ä»–å—æ”¯æŒçš„è¿è¡Œæ—¶ä¹‹ä¸€ï¼ˆä¾‹å¦‚ containerd æˆ– CRI-Oï¼‰ï¼Œæˆ–è€…å¦‚æœæ‚¨ä¾èµ– Docker å¼•æ“ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ï¼Œåˆ™ä½¿ç”¨ cri-dockerdã€‚æœ‰å…³ç¡®ä¿æ‚¨çš„é›†ç¾¤å·²å‡†å¤‡å¥½è¿›è¡Œæ­¤ç§»é™¤çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æœ¬[æŒ‡å—](Is Your Cluster Ready for v1.24? | Kubernetes)ã€‚\n2ï¼‰Beta API é»˜è®¤å…³é—­ é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ä¼šåœ¨é›†ç¾¤ä¸­å¯ç”¨æ–°çš„ beta APIã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç°æœ‰çš„ beta API å’Œç°æœ‰ beta API çš„æ–°ç‰ˆæœ¬å°†ç»§ç»­å¯ç”¨ã€‚\n3ï¼‰å­˜å‚¨å®¹é‡å’Œå·æ‰©å±•åˆ°GA å­˜å‚¨å®¹é‡è·Ÿè¸ªæ”¯æŒé€šè¿‡ CSIStorageCapacity å¯¹è±¡å…¬å¼€å½“å‰å¯ç”¨çš„å­˜å‚¨å®¹é‡ï¼Œå¹¶å¢å¼ºä½¿ç”¨å…·æœ‰åæœŸç»‘å®šçš„ CSI å·çš„ pod çš„è°ƒåº¦ã€‚\nå·æ‰©å±•å¢åŠ äº†å¯¹è°ƒæ•´ç°æœ‰æŒä¹…å·å¤§å°çš„æ”¯æŒã€‚\n4ï¼‰é¿å… IP åˆ†é…ç»™serviceçš„å†²çª Kubernetes 1.24 å¼•å…¥äº†ä¸€é¡¹æ–°çš„é€‰æ‹©åŠ å…¥åŠŸèƒ½ï¼Œå…è®¸æ‚¨ä¸ºæœåŠ¡çš„é™æ€ IP åœ°å€åˆ†é…è½¯é¢„ç•™èŒƒå›´ã€‚é€šè¿‡æ‰‹åŠ¨å¯ç”¨æ­¤åŠŸèƒ½ï¼Œé›†ç¾¤å°†æ›´å–œæ¬¢ä»æœåŠ¡ IP åœ°å€æ± ä¸­è‡ªåŠ¨åˆ†é…ï¼Œä»è€Œé™ä½å†²çªé£é™©ã€‚\nå¯ä»¥åˆ†é… Service ClusterIPï¼š\nåŠ¨æ€ï¼Œè¿™æ„å‘³ç€é›†ç¾¤å°†è‡ªåŠ¨åœ¨é…ç½®çš„æœåŠ¡ IP èŒƒå›´å†…é€‰æ‹©ä¸€ä¸ªç©ºé—² IPã€‚\né™æ€ï¼Œè¿™æ„å‘³ç€ç”¨æˆ·å°†åœ¨é…ç½®çš„æœåŠ¡ IP èŒƒå›´å†…è®¾ç½®ä¸€ä¸ª IPã€‚\nService ClusterIP æ˜¯å”¯ä¸€çš„ï¼Œå› æ­¤ï¼Œå°è¯•ä½¿ç”¨å·²åˆ†é…çš„ ClusterIP åˆ›å»º Service å°†è¿”å›é”™è¯¯ã€‚\nï¼ˆäºŒï¼‰å¼ƒç”¨å˜æ›´ 1ï¼‰kubeadm kubeadm.k8s.io/v1beta2 å·²è¢«å¼ƒç”¨ï¼Œå¹¶å°†åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­åˆ é™¤ï¼Œå¯èƒ½åœ¨ 3 ä¸ªç‰ˆæœ¬ï¼ˆä¸€å¹´ï¼‰ä¸­ã€‚æ‚¨åº”è¯¥å¼€å§‹å°† kubeadm.k8s.io/v1beta3 ç”¨äºæ–°é›†ç¾¤ã€‚è¦è¿ç§»ç£ç›˜ä¸Šçš„æ—§é…ç½®æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ kubeadm config migrate å‘½ä»¤ã€‚\né»˜è®¤ kâ€‹â€‹ubeadm é…ç½®ä¸º containerd å¥—æ¥å­—ï¼ˆUnixï¼šunix:///var/run/containerd/containerd.sockï¼ŒWindowsï¼šnpipe:////./pipe/containerd-containerdï¼‰è€Œä¸æ˜¯ Docker çš„é…ç½®.å¦‚æœåœ¨é›†ç¾¤åˆ›å»ºæœŸé—´ Init|JoinConfiguration.nodeRegistration.criSocket å­—æ®µä¸ºç©ºï¼Œå¹¶ä¸”åœ¨ä¸»æœºä¸Šå‘ç°å¤šä¸ªå¥—æ¥å­—ï¼Œåˆ™æ€»æ˜¯ä¼šæŠ›å‡ºé”™è¯¯å¹¶è¦æ±‚ç”¨æˆ·é€šè¿‡è®¾ç½®å­—æ®µä¸­çš„å€¼æ¥æŒ‡å®šè¦ä½¿ç”¨çš„å¥—æ¥å­—ã€‚ä½¿ç”¨ crictl ä¸ CRI å¥—æ¥å­—è¿›è¡Œæ‰€æœ‰é€šä¿¡ï¼Œä»¥æ‰§è¡Œè¯¸å¦‚æ‹‰å–å›¾åƒå’Œè·å–æ­£åœ¨è¿è¡Œçš„å®¹å™¨åˆ—è¡¨ç­‰æ“ä½œï¼Œè€Œä¸æ˜¯åœ¨ Docker çš„æƒ…å†µä¸‹ä½¿ç”¨ docker CLIã€‚\nkubeadm è¿ç§»åˆ°æ ‡ç­¾å’Œæ±¡ç‚¹ä¸­ä¸å†ä½¿ç”¨ master ä¸€è¯ã€‚å¯¹äºæ–°çš„é›†ç¾¤ï¼Œæ ‡ç­¾ node-role.kubernetes.io/master å°†ä¸å†æ·»åŠ åˆ°æ§åˆ¶å¹³é¢èŠ‚ç‚¹ï¼Œåªä¼šæ·»åŠ æ ‡ç­¾ node-role.kubernetes.io/control-planeã€‚\n2ï¼‰kube-apiserver ä¸å®‰å…¨çš„åœ°å€æ ‡å¿— --addressã€--insecure-bind-addressã€--port å’Œ --insecure-portï¼ˆè‡ª 1.20 èµ·æƒ°æ€§ï¼‰è¢«åˆ é™¤\nå¼ƒç”¨äº†--master-countflag å’Œ--endpoint-reconciler-type=master-countreconcilerï¼Œè½¬è€Œä½¿ç”¨ lease reconcilerã€‚\nå·²å¼ƒç”¨Service.Spec.LoadBalancerIPã€‚\n3ï¼‰kube-controller-manager kube-controller-manager ä¸­çš„ä¸å®‰å…¨åœ°å€æ ‡å¿— --address å’Œ --port è‡ª v1.20 èµ·æ— æ•ˆï¼Œå¹¶åœ¨ v1.24 ä¸­è¢«åˆ é™¤ã€‚ 4ï¼‰kubelet --pod-infra-container-image kubelet æ ‡å¿—å·²å¼ƒç”¨ï¼Œå°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚\nä»¥ä¸‹ä¸ dockershim ç›¸å…³çš„æ ‡å¿—ä¹Ÿä¸ dockershim ä¸€èµ·è¢«åˆ é™¤ --experimental-dockershim-root-directoryã€--docker-endpointã€--image-pull-progress-deadlineã€--network-pluginã€--cni-conf -dirï¼Œ--cni-bin-dirï¼Œ--cni-cache-dirï¼Œ--network-plugin-mtuã€‚(#106907,Â @cyclinder)\n1.23 æœ€æ–°å‘è¡Œç‰ˆæœ¬ï¼š1.23.8 (å‘å¸ƒæ—¥æœŸ:Â 2022-06-15ï¼‰\nä¸å†æ”¯æŒï¼š2023-02-28\nè¡¥ä¸ç‰ˆæœ¬ï¼šÂ 1.23.1ã€Â 1.23.2ã€Â 1.23.3ã€Â 1.23.4ã€Â 1.23.5ã€Â 1.23.6ã€Â 1.23.7ã€Â 1.23.8\nComplete 1.23Â ScheduleÂ andÂ Changelog\nKubernetes æ˜¯ä½¿ç”¨ golang 1.17 æ„å»ºçš„ã€‚æ­¤ç‰ˆæœ¬çš„ go åˆ é™¤äº†ä½¿ç”¨ GODEBUG=x509ignoreCN=0 ç¯å¢ƒè®¾ç½®æ¥é‡æ–°å¯ç”¨å°† X.509 æœåŠ¡è¯ä¹¦çš„ CommonName è§†ä¸ºä¸»æœºåçš„å·²å¼ƒç”¨æ—§è¡Œä¸ºçš„èƒ½åŠ›ã€‚\nï¼ˆä¸€ï¼‰ é‡è¦æ›´æ–° 1ï¼‰FlexVolume å·²å¼ƒç”¨ FlexVolume å·²å¼ƒç”¨ã€‚ Out-of-tree CSI é©±åŠ¨ç¨‹åºæ˜¯åœ¨ Kubernetes ä¸­ç¼–å†™å·é©±åŠ¨ç¨‹åºçš„æ¨èæ–¹å¼ã€‚FlexVolume é©±åŠ¨ç¨‹åºçš„ç»´æŠ¤è€…åº”å®æ–½ CSI é©±åŠ¨ç¨‹åºå¹¶å°† FlexVolume çš„ç”¨æˆ·è½¬ç§»åˆ° CSIã€‚ FlexVolume çš„ç”¨æˆ·åº”å°†å…¶å·¥ä½œè´Ÿè½½è½¬ç§»åˆ° CSI é©±åŠ¨ç¨‹åºã€‚\n2ï¼‰IPv4/IPv6 åŒæ ˆç½‘ç»œåˆ° GA IPv4/IPv6 åŒæ ˆç½‘ç»œä» GA æ¯•ä¸šã€‚ä» 1.21 å¼€å§‹ï¼ŒKubernetes é›†ç¾¤é»˜è®¤å¯ç”¨æ”¯æŒåŒæ ˆç½‘ç»œã€‚åœ¨ 1.23 ä¸­ï¼Œç§»é™¤äº† IPv6DualStack åŠŸèƒ½é—¨ã€‚åŒæ ˆç½‘ç»œçš„ä½¿ç”¨ä¸æ˜¯å¼ºåˆ¶æ€§çš„ã€‚å°½ç®¡å¯ç”¨äº†é›†ç¾¤ä»¥æ”¯æŒåŒæ ˆç½‘ç»œï¼Œä½† Pod å’ŒæœåŠ¡ç»§ç»­é»˜è®¤ä¸ºå•æ ˆã€‚è¦ä½¿ç”¨åŒæ ˆç½‘ç»œï¼šKubernetes èŠ‚ç‚¹å…·æœ‰å¯è·¯ç”±çš„ IPv4/IPv6 ç½‘ç»œæ¥å£ï¼Œä½¿ç”¨æ”¯æŒåŒæ ˆçš„ CNI ç½‘ç»œæ’ä»¶ï¼ŒPod é…ç½®ä¸ºåŒæ ˆï¼ŒæœåŠ¡çš„ .spec.ipFamilyPolicy å­—æ®µè®¾ç½®ä¸º PreferDualStack æˆ–éœ€è¦åŒæ ˆã€‚\n3ï¼‰Horizoâ€‹â€‹ntalPodAutoscaler v2 åˆ° GA Horizoâ€‹â€‹ntalPodAutoscaler API çš„ç¬¬ 2 ç‰ˆåœ¨ 1.23 ç‰ˆæœ¬ä¸­é€æ¸ç¨³å®šã€‚ Horizoâ€‹â€‹ntalPodAutoscaler autoscaling/v2beta2 API å·²å¼ƒç”¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯æ–°çš„ autoscaling/v2 APIï¼ŒKubernetes é¡¹ç›®å»ºè®®å°†å…¶ç”¨äºæ‰€æœ‰ç”¨ä¾‹ã€‚\n4ï¼‰Schedulerç®€åŒ–å¤šç‚¹æ’ä»¶é…ç½® kube-scheduler æ­£åœ¨ä¸ºæ’ä»¶æ·»åŠ ä¸€ä¸ªæ–°çš„ã€ç®€åŒ–çš„é…ç½®å­—æ®µï¼Œä»¥å…è®¸åœ¨ä¸€ä¸ªä½ç½®å¯ç”¨å¤šä¸ªæ‰©å±•ç‚¹ã€‚æ–°çš„ multiPoint æ’ä»¶å­—æ®µæ—¨åœ¨ä¸ºç®¡ç†å‘˜ç®€åŒ–å¤§å¤šæ•°è°ƒåº¦ç¨‹åºè®¾ç½®ã€‚é€šè¿‡ multiPoint å¯ç”¨çš„æ’ä»¶å°†è‡ªåŠ¨ä¸ºå®ƒä»¬å®ç°çš„æ¯ä¸ªå•ç‹¬çš„æ‰©å±•ç‚¹æ³¨å†Œã€‚ä¾‹å¦‚ï¼Œå®ç° Score å’Œ Filter æ‰©å±•çš„æ’ä»¶å¯ä»¥åŒæ—¶ä¸ºä¸¤è€…å¯ç”¨ã€‚è¿™æ„å‘³ç€å¯ä»¥å¯ç”¨å’Œç¦ç”¨æ•´ä¸ªæ’ä»¶ï¼Œè€Œæ— éœ€æ‰‹åŠ¨ç¼–è¾‘å•ä¸ªæ‰©å±•ç‚¹è®¾ç½®ã€‚è¿™äº›æ‰©å±•ç‚¹ç°åœ¨å¯ä»¥è¢«æŠ½è±¡å‡ºæ¥ï¼Œå› ä¸ºå®ƒä»¬ä¸å¤§å¤šæ•°ç”¨æˆ·æ— å…³ã€‚\nï¼ˆäºŒï¼‰å·²çŸ¥é—®é¢˜ åœ¨ 1.22 Kubernetes ç‰ˆæœ¬é™„å¸¦çš„ etcd v3.5.0 ç‰ˆæœ¬ä¸­å‘ç°äº†æ•°æ®æŸåé—®é¢˜ã€‚è¯·é˜…è¯» etcd çš„æœ€æ–°[ç”Ÿäº§å»ºè®®](etcd/CHANGELOG at main Â· etcd-io/etcd Â· GitHub)ã€‚\nè¿è¡Œetcd v3.5.2 v3.5.1å’Œv3.5.0é«˜è´Ÿè·ä¼šå¯¼è‡´æ•°æ®æŸåé—®é¢˜ã€‚å¦‚æœetcdè¿›ç¨‹è¢«æ€,å¶å°”æœ‰äº›å·²æäº¤çš„äº‹åŠ¡å¹¶ä¸åæ˜ åœ¨æ‰€æœ‰çš„æˆå‘˜ã€‚å»ºè®®å‡çº§åˆ°v3.5.3ã€‚\næœ€ä½æ¨èetcdç‰ˆæœ¬è¿è¡Œåœ¨ç”Ÿäº§3.3.18 + 3.4.2 + v3.5.3 +ã€‚\n1.22 æœ€æ–°å‘è¡Œç‰ˆæœ¬ï¼š1.22.11 (å‘å¸ƒæ—¥æœŸ:Â 2022-06-15ï¼‰\nä¸å†æ”¯æŒï¼š2022-10-28\nè¡¥ä¸ç‰ˆæœ¬ï¼šÂ 1.22.1ã€Â 1.22.2ã€Â 1.22.3ã€Â 1.22.4ã€Â 1.22.5ã€Â 1.22.6ã€Â 1.22.7ã€Â 1.22.8ã€Â 1.22.9ã€Â 1.22.10ã€Â 1.22.11\nComplete 1.22Â ScheduleÂ andÂ Changelog\nï¼ˆâ€”ï¼‰é‡è¦æ›´æ–° 1ï¼‰kubeadm å…è®¸érootç”¨æˆ·å…è®¸kubeadmã€‚\nç°åœ¨V1beta3é¦–é€‰APIç‰ˆæœ¬;v1beta2 APIä¹Ÿä»ç„¶æ˜¯å¯ç”¨çš„,å¹¶æ²¡æœ‰å¼ƒç”¨ã€‚\nç§»é™¤å¯¹docker cgroup driverçš„æ£€æŸ¥ï¼Œkubeadmé»˜è®¤ä½¿ç”¨systemd cgroup driverï¼Œéœ€è¦æ‰‹åŠ¨å°†runtimeé…ç½®ä¸ºsystemdã€‚\nv1beta3ä¸­åˆ é™¤ClusterConfiguration.DNSå­—æ®µï¼Œå› ä¸ºCoreDNSæ˜¯å”¯ä¸€æ”¯æŒDNSç±»å‹ã€‚\n2ï¼‰etcd etcdä½¿ç”¨v3.5.0ç‰ˆæœ¬ã€‚ï¼ˆä½†æ˜¯åœ¨1.23ç‰ˆæœ¬ä¸­å‘ç°v3.5.0æœ‰æ•°æ®æŸåçš„é—®é¢˜ï¼‰ 3ï¼‰kubelet èŠ‚ç‚¹æ”¯æŒswapå†…å­˜ã€‚\nä½œä¸ºÎ±ç‰¹æ€§,Kubernetes v1.22å¹¶ä¸”å¯ä»¥ä½¿ç”¨cgroup v2 APIæ¥æ§åˆ¶å†…å­˜åˆ†é…å’Œéš”ç¦»ã€‚è¿™ä¸ªåŠŸèƒ½çš„ç›®çš„æ˜¯æ”¹å–„å·¥ä½œè´Ÿè½½å’ŒèŠ‚ç‚¹å¯ç”¨æ€§æ—¶å¯¹å†…å­˜èµ„æºçš„äº‰ç”¨ã€‚\nå‚è€ƒï¼š\nå‘è¡Œç‰ˆæœ¬ | Kubernetes\nkubernetes/CHANGELOG at master Â· GitHub\nsig-release/releases at master Â· kubernetes/sig-release Â· GitHub\nKubernetes 1.24 æ­£å¼å‘å¸ƒï¼Œè¿™é‡Œæ˜¯æ›´æ–°åŠŸèƒ½æ€»è§ˆ\n","categories":"","description":"","excerpt":"1.27 å‚è€ƒï¼š\nKubernetes åœ¨ v1.27 â€¦","ref":"/kubernetes-notes/setup/k8s-changelog/","tags":["Kubernetes"],"title":"k8sç‰ˆæœ¬è®°å½•"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/network/gateway/","tags":"","title":"k8sç½‘å…³"},{"body":"1. kubeconfigè¯´æ˜ é»˜è®¤æƒ…å†µä¸‹ï¼ŒkubectlÂ åœ¨Â $HOME/.kubeÂ ç›®å½•ä¸‹æŸ¥æ‰¾åä¸ºÂ configÂ çš„æ–‡ä»¶ã€‚ ä½ å¯ä»¥é€šè¿‡è®¾ç½®Â KUBECONFIGÂ ç¯å¢ƒå˜é‡æˆ–è€…è®¾ç½®Â --kubeconfigå‚æ•°æ¥æŒ‡å®šå…¶ä»– kubeconfig æ–‡ä»¶ã€‚\nkubeconfigå†…å®¹ç¤ºä¾‹ï¼š\nä»¥ä¸‹è¯ä¹¦ä»¥æ–‡ä»¶çš„å½¢å¼è¯»å–ã€‚\napiVersion: v1 kind: Config clusters: - cluster: certificate-authority: */ca.crt server: https://****** name: demo contexts: - context: cluster: demo user: demo name: demo current-context: demo preferences: {} users: - name: demo user: client-certificate: */client.crt client-key: */client.key 2. å°† Kubeconfig ä¸­çš„è¯ä¹¦æ–‡ä»¶è½¬ä¸ºè¯ä¹¦æ•°æ® 2.1. è¯ä¹¦æ–‡ä»¶è½¬ä¸ºè¯ä¹¦æ•°æ® å¦‚æœéœ€è¦å°†è¯ä¹¦æ–‡ä»¶æ›¿æ¢ä¸ºè¯ä¹¦æ•°æ®æ ¼å¼ï¼Œå¯ä»¥é€šè¿‡base64çš„å‘½ä»¤è§£ç æˆdataæ•°æ®ï¼ŒåŒæ—¶ä¿®æ”¹ç›¸åº”çš„å‚æ•°ã€‚\nè·å–è¯ä¹¦æ–‡ä»¶çš„ base64 ç¼–ç ï¼š\ncat \"è¯ä¹¦æ–‡ä»¶\" | base64 å°†è¾“å‡ºç»“æœçš„æ¢è¡Œç¬¦å»æ‰ï¼Œå¡«åˆ°å¯¹åº”è¯ä¹¦çš„dataä¸­ã€‚\nå°†Â certificate-authorityÂ æ”¹ä¸ºÂ certificate-authority-dataï¼Œå¹¶ä¸”å°†Â */ca.crtÂ è¯ä¹¦æ–‡ä»¶ç» base64 ç¼–ç åçš„å­—ç¬¦ä¸²å¡«å…¥è¯¥ä½ç½®ã€‚\nå°†Â client-certificateÂ æ”¹ä¸ºÂ client-certificate-dataï¼Œå¹¶ä¸”å°†Â */client.crtÂ è¯ä¹¦æ–‡ä»¶ç» base64 ç¼–ç åçš„å­—ç¬¦ä¸²å¡«å…¥è¯¥ä½ç½®ã€‚\nå°†Â client-keyÂ æ”¹ä¸ºÂ client-key-dataï¼Œå¹¶ä¸”å°†Â */client.keyÂ è¯ä¹¦æ–‡ä»¶ base64 ç¼–ç åçš„å­—ç¬¦ä¸²å¡«å…¥è¯¥ä½ç½®ã€‚\nä¾‹å¦‚ï¼š\napiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZADQFURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRJd01ESXhNREEwTURJMU1Wb1hEVE13TURJd09EQTBNREkxTVZvd0ZURVRNQkVHQTFVRQpBeE1LYldsdWFXdDFZbVZEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTUc1CjYwU0txcHVXeE1mWlVabmVFakM5bjFseHFQSzdUTlVQbzROejFWcWxaQkt6NzJDVVErZjBtVGNQLy9oS3BQUVAKaG9pNndyaXJRUmVERTErRFIrOTZHVDIrSGZ3L2VHQTI5ZmErNS80UG5PWlpTUEVpS3MxVVdhc0VqSDJVZG4xTwpEejVRZk1ESkFjZlBoTzV0eUZFaGZNa2hid0Y2QkJONnh5RmJJdXl4OThmZGx5SWJJUnpLSml6VWZQcUx2WUZoCmFQbjF4WFZyT2QyMnFtblgzL2VxZXM4aG51SmpJdlVPbWRDRlhjQVRYdE00Wmw2bERvWUs2VS9vaEFzM0x4VzAKWUV4ZkcxMzFXdjIrR0t4WWV2Q0FuMitSQ3NBdFpTZk9zcVljMmorYS9FODVqdzcySlFkNGd6eGlHMCszaU14WApWaGhpcWFrY1owZlRCc0FtZHY4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUIwR0ExVWRKUVFXCk1CUUdDQ3NHQVFVRkJ3TUNCZ2dyQmdFRkJRY0RBVEFQQmdOVkhSTUJBZjhFQlRBREFRSC9NQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDKzFuU2w0dnJDTEV6eWg0VWdXR3ZWSldtV2ltM2dBWFFJU1R2WG56NXZqOXE3Z0JYSwpCRVUyakVHTFF2UEJQWUZwUjhmZllCZCtqT2xtYS9IdU9ISmw0RUxhaHJKbnIwaU9YcytoeVlpV0ZUKzZ2R05RCmY4QnAvNTlkYzY1ejVVMnlUQjd4VkhMcGYzRTRZdUN2NmZhdy9PZTNUUzZUbThZdFBXREgxNDBOR2ZKMHlWRlYKSzZsQnl5THMwMzZzT1V5ZUJpcEduOUxyKytvb09mTVZIU2dpaEJlcEl3ZVVvYk05YU1ram1Hb2VjNk5HTUN3NwpkaFNWTmdMNGxMSnRvRktoVDdTZHFjMmk2SWlwbkJrdUlHUWRJUFliQnF6MkN5eVMyRkZmeEJsV2lmNmcxMTFTClphSUlpQ0lLbXNqeDJvTFBhOUdNSjR6bERNR1hLY1ZyNnhhVQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== server: https://****** name: demo contexts: - context: cluster: demo user: demo name: demo current-context: demo preferences: {} users: - name: demo user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURBRENDQWVpZ0F3SUJBZ0lCQWpBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRJd01ESXhNekV5TXpreU5sb1hEVEl4TURJeE16RXlNemt5Tmxvd01URVhNQlVHQTFVRQpDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGakFVQmdOVkJBTVREVzFwYm1scmRXSmxMWFZ6WlhJd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDdEp5MThYOGVBaVlJc1g3Z0xQazBOZEFuRlNJU0kKeXNGMGIzS21CTk9VclRIcGtnaUFwRldmZDJaVXNZR3Iwd0VBL0FIWm9PMUxPUHdqYzEyb2o1bGIwWlNNWTlMaQpVeW9UQ3huREZIVDJIQnlPNCtGRk5pVCtXTU5FTURURWxERXhlano1WVIwb1pZVjN2V2Z5T3l2SlBna1dFU29IClVVcnVvQmRRbU5LUzhCeXhIUmFvcnFJUFRVRGkzUFJIbTlGZERKV1lNTWpnbDZmbmdHWkRQMnpjTlNFZjRGNzYKc2FUK0VhMU1kbjV5akRCNXczaHJvZXBBclc0QVUyR3NTRFZyTHY2UDFiSWV0RDdONjZrT1BBNklIUlRKbUNLLwp2aEJrbGVPMGFwcERzTERDT3hpMkc2Q1BRNDZVYVZUOEhZMk9sQU5nSmtRRDNwYjFXTnlhQlVpRkFnTUJBQUdqClB6QTlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBTHVrZ1dxWnRtbkZITlcxYgpqL012ekcxTmJyUEtWYXVobml5RzRWWnRZYzR1Uk5iSGhicEhEdThyamc2dVB1Y0xMdHAzblU2UGw4S2J2WFpiCmplNmJQR2xvV3VBcFIrVW9KRFQ2VEpDK2o2Qm5CSXpWQkNOL21lSWVPQ0hEK1k5L2dtbzRnd2Q4c2F3U0Z1bjMKZTFVekF2cHBwdTVZY05wcU92aUkxT2NjNGdxNTd2V1h1MFRIdUJkM0VtQ2JZRXUzYXhOL25ldnhOYnYxbDFRSQovSzRaOWw3MXFqaEp3SVlBaHUzek5pTWpCU1VTRjJkZnd2NmFnclhSUnN6b1Z4ejE5Mm9qM2pWU215cXZxeVFrCmZXckpsc3VhY1NDdTlKUE44OUQrVXkwVnZXZmhPdmp4cXVRSktwUW9hMzlQci81Q3YweXFKUkFIMkk5Wk1IZEYKNkJQRVBRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: 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 2.2. è¯ä¹¦æ•°æ®è½¬æˆè¯ä¹¦æ–‡ä»¶ grep 'certificate-authority-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d \u003e ca.crt grep 'client-key-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d \u003e client.key grep 'client-certificate-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d \u003e client.crt 3. ç”Ÿæˆé›†ç¾¤çº§åˆ«çš„kubeconfig 3.1. åˆ›å»ºServiceAccount # 1. åˆ›å»ºç”¨æˆ· cat\u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: namespace: ${ServiceAccountNS} name: ${ServiceAccountName} EOF # 2. åˆ›å»ºSecretç»‘å®šServiceAccount # k8s 1.24åçš„ç‰ˆæœ¬ä¸å†è‡ªåŠ¨ç”Ÿæˆsecretï¼Œç»‘å®šåå½“åˆ é™¤ServiceAccountæ—¶ä¼šè‡ªåŠ¨åˆ é™¤secret cat\u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: ${SecretName} namespace: ${ServiceAccountNS} annotations: kubernetes.io/service-account.name: \"${ServiceAccountName}\" type: kubernetes.io/service-account-token EOF 3.2. åˆ›å»ºå’Œç»‘å®šæƒé™ å¦‚æœä¸åˆ›å»ºè‡ªå®šä¹‰æƒé™ï¼Œå¯ä»¥ä½¿ç”¨è‡ªå¸¦çš„ClusterRole: cluster-admin/admin/edit/viewã€‚å¦‚æœéœ€è¦ç”Ÿæˆé›†ç¾¤adminçš„æƒé™ï¼Œå¯ä»¥ç»‘å®š cluster-adminçš„æƒé™ã€‚å»ºè®®æƒé™æˆæƒæ—¶éµå¾ªæœ€å°æƒé™åŸåˆ™ã€‚å³éœ€è¦ä½¿ç”¨çš„æƒé™æˆæƒï¼Œä¸éœ€è¦ä½¿ç”¨çš„æƒé™ä¸æˆæƒï¼Œå¯¹äºåˆ é™¤ç­‰æ•æ„Ÿæƒé™æ…é‡æˆæƒã€‚\nå¦‚æœè¦åˆ›å»ºè‡ªå®šä¹‰æƒé™ï¼Œä½¿ç”¨ClusterRoleå’ŒClusterRoleBindingçš„å¯¹è±¡ã€‚\nåªè¯»æƒé™ï¼šverbs: [\"get\",\"list\",\"watch\"]\nå¯å†™æƒé™ï¼šverbs: [\"create\",\"update\",\"patch\",\"delete\",\"deletecollection\"]\nå…¨éƒ¨æƒé™ï¼šverbs: [\"*\"]\n# 3. åˆ›å»ºæƒé™: å¯ä»¥è‡ªå®šä¹‰æƒé™ï¼Œæˆ–è€…ä½¿ç”¨clusterrole: cluster-admin/admin/edit/viewæƒé™ # åªè¯»æƒé™ï¼šverbs: [\"get\",\"list\",\"watch\"] # å¯å†™æƒé™ï¼šverbs: [\"create\",\"update\",\"patch\",\"delete\",\"deletecollection\"] # å…¨éƒ¨æƒé™ï¼šverbs: [\"*\"] cat\u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ${ClusterRoleName} rules: - apiGroups: [\"*\"] resources: [\"pods\",\"deployments\"] verbs: [\"*\"] EOF # 4. ç»‘å®šæƒé™ cat\u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ${ClusterRoleBindingName} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ${ClusterRoleName} EOF 3.3. åˆ›å»ºkubeconfig # 5. åŸºäºsecretè·å–token TOKEN=$(kubectl get secret ${SecretName} -n ${ServiceAccountNS} -o jsonpath={\".data.token\"} | base64 -d) # 6. åˆ›å»ºkubeconfigæ–‡ä»¶ kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-cluster ${KubeConfigCluster} \\ --server=https://${APISERVER} \\ --certificate-authority=${CaFilePath} \\ --embed-certs=true kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-credentials ${USER} --token=${TOKEN} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-context ${USER}@${KubeConfigCluster} --cluster=${KubeConfigCluster} --user=${USER} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config use-context ${USER}@${KubeConfigCluster} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config view 4. ç”Ÿæˆnamespaceçš„kubeconfig 4.1. åˆ›å»ºServiceAccount åˆ›å»ºServiceAccoutåŒä¸Šï¼Œæ²¡æœ‰åŒºåˆ«ã€‚\n4.2. åˆ›å»ºå’Œç»‘å®šæƒé™ åˆ›å»ºnamespaceçº§åˆ«çš„æƒé™ç”¨Roleå’ŒRoleBindingçš„å¯¹è±¡ã€‚ä¸€ä¸ªServiceAccountå¯ä»¥ç»‘å®šå¤šä¸ªæƒé™ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºClusterRoleBindingæˆ–RoleBindingæ¥å®ç°å¯¹å¤šä¸ªnamespaceçš„æƒé™æ§åˆ¶ã€‚\n# 3. åˆ›å»ºæƒé™: å¯ä»¥è‡ªå®šä¹‰æƒé™ï¼Œæˆ–è€…ä½¿ç”¨clusterrole: admin/edit/viewæƒé™ cat\u003c\u003cEOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: ${USER_NAMESPACE} name: ${RoleName} rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"*\"] EOF # 4. ç»‘å®šæƒé™ cat\u003c\u003cEOF | kubectl apply -f - kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ${RoleBindingName} namespace: ${USER_NAMESPACE} subjects: - kind: ServiceAccount name: ${ServiceAccountName} namespace: ${ServiceAccountNS} roleRef: kind: Role name: ${RoleName} apiGroup: rbac.authorization.k8s.io EOF 4.3. åˆ›å»ºkubeconfig # 5. åŸºäºsecretè·å–token TOKEN=$(kubectl get secret ${SecretName} -n ${ServiceAccountNS} -o jsonpath={\".data.token\"} | base64 -d) # 6. åˆ›å»ºkubeconfigæ–‡ä»¶ kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-cluster ${KubeConfigCluster} \\ --server=https://${APISERVER} \\ --certificate-authority=${CaFilePath} \\ --embed-certs=true kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-credentials ${USER} --token=${TOKEN} # ä¸clusterçº§åˆ«kubeconfigä¸åŒçš„å¢åŠ äº†--namespace kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-context ${USER}@${KubeConfigCluster} \\ --cluster=${KubeConfigCluster} --user=${USER} --namespace=${USER_NAMESPACE} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config use-context ${USER}@${KubeConfigCluster} kubectl --kubeconfig=${KubeDir}/${USER}.yaml config view 5. åˆ›å»ºtokenæ¥è®¿é—®apiserver ä»¥ä¸Šæ–¹å¼æ˜¯é€šè¿‡åˆ›å»ºServiceAccountçš„Secretçš„æ–¹å¼æ¥è·å–tokenï¼Œè¯¥tokenæ˜¯æ°¸ä¹…ç”Ÿæ•ˆçš„ï¼Œå¦‚æœè¦ä½¿tokenå¤±æ•ˆå¯ä»¥åˆ é™¤å¯¹åº”çš„Secretå¯¹è±¡ï¼Œ\nå¦‚æœè¦ä½¿ç”¨æŒ‡å®šè¿‡æœŸæœŸé™çš„tokenæƒé™ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆï¼š\n# ç”Ÿæˆè¿‡æœŸæ—¶é—´365å¤©çš„token TOKEN=$(kubectl -n ${NAMESPACE} create token ${USER} --duration 8760h) kubectl --kubeconfig=${KubeDir}/${USER}.yaml config set-credentials ${USER} --token=${TOKEN} å‚è€ƒï¼š\nå¦‚ä½•å°† Kubeconfig ä¸­çš„è¯ä¹¦æ–‡ä»¶è½¬ä¸ºè¯ä¹¦æ•°æ® - CODING å¸®åŠ©ä¸­å¿ƒ ","categories":"","description":"","excerpt":"1. kubeconfigè¯´æ˜ é»˜è®¤æƒ…å†µä¸‹ï¼ŒkubectlÂ åœ¨Â $HOME/.kubeÂ ç›®å½•ä¸‹æŸ¥æ‰¾åä¸ºÂ configÂ çš„æ–‡ä»¶ã€‚ ä½ å¯ä»¥é€šè¿‡è®¾ â€¦","ref":"/kubernetes-notes/operation/kubectl/kubeconfig/","tags":["Kubernetes"],"title":"kubeconfigçš„ä½¿ç”¨"},{"body":"1. ifè¯­å¥ if è¯­å¥é€šè¿‡å…³ç³»è¿ç®—ç¬¦åˆ¤æ–­è¡¨è¾¾å¼çš„çœŸå‡æ¥å†³å®šæ‰§è¡Œå“ªä¸ªåˆ†æ”¯ã€‚Shell æœ‰ä¸‰ç§ if ... else è¯­å¥ï¼š\nif ... fi è¯­å¥ï¼› if ... else ... fi è¯­å¥ï¼› if ... elif ... else ... fi è¯­å¥ã€‚ 1.1. if ... else if ... else è¯­å¥çš„è¯­æ³•ï¼š\nif [ expression ] then Statement(s) to be executed if expression is true fi å¦‚æœ expression è¿”å› trueï¼Œthen åè¾¹çš„è¯­å¥å°†ä¼šè¢«æ‰§è¡Œï¼›å¦‚æœè¿”å› falseï¼Œä¸ä¼šæ‰§è¡Œä»»ä½•è¯­å¥ã€‚ æœ€åå¿…é¡»ä»¥ fi æ¥ç»“å°¾é—­åˆ ifï¼Œfi å°±æ˜¯ if å€’è¿‡æ¥æ‹¼å†™ã€‚ æ³¨æ„ï¼šexpression å’Œæ–¹æ‹¬å·([ ])ä¹‹é—´å¿…é¡»æœ‰ç©ºæ ¼ï¼Œå¦åˆ™ä¼šæœ‰è¯­æ³•é”™è¯¯ã€‚\n1.2. if ... else ... fi if ... else ... fi è¯­å¥çš„è¯­æ³•ï¼š\nif [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi å¦‚æœ expression è¿”å› trueï¼Œé‚£ä¹ˆ then åè¾¹çš„è¯­å¥å°†ä¼šè¢«æ‰§è¡Œï¼›å¦åˆ™ï¼Œæ‰§è¡Œ else åè¾¹çš„è¯­å¥ã€‚\n1.3. if ... elif ... fi å¤šåˆ†æé€‰æ‹© if ... elif ... fi è¯­å¥å¯ä»¥å¯¹å¤šä¸ªæ¡ä»¶è¿›è¡Œåˆ¤æ–­ï¼Œè¯­æ³•ä¸ºï¼š\nif [ expression 1 ] then Statement(s) to be executed if expression 1 is true elif [ expression 2 ] then Statement(s) to be executed if expression 2 is true elif [ expression 3 ] then Statement(s) to be executed if expression 3 is true else Statement(s) to be executed if no expression is true fi å“ªä¸€ä¸ª expression çš„å€¼ä¸º trueï¼Œå°±æ‰§è¡Œå“ªä¸ª expression åé¢çš„è¯­å¥ï¼›å¦‚æœéƒ½ä¸º falseï¼Œé‚£ä¹ˆä¸æ‰§è¡Œä»»ä½•è¯­å¥ã€‚\nif ... else è¯­å¥ä¹Ÿå¯ä»¥å†™æˆä¸€è¡Œï¼Œç”¨åˆ†å·éš”å¼€ï¼Œä»¥å‘½ä»¤çš„æ–¹å¼æ¥è¿è¡Œ\nif ... else è¯­å¥ä¹Ÿç»å¸¸ä¸ test å‘½ä»¤ç»“åˆä½¿ç”¨ï¼Œtest å‘½ä»¤ç”¨äºæ£€æŸ¥æŸä¸ªæ¡ä»¶æ˜¯å¦æˆç«‹ï¼Œä¸æ–¹æ‹¬å·([ ])ç±»ä¼¼ã€‚\nif test $[num1] -eq $[num2] 2. caseè¯­å¥ case ... esac ä¸å…¶ä»–è¯­è¨€ä¸­çš„ switch ... case è¯­å¥ç±»ä¼¼ï¼Œæ˜¯ä¸€ç§å¤šåˆ†æé€‰æ‹©ç»“æ„ã€‚\ncase å€¼ inæ¨¡å¼1) #-------\u003eåŒ¹é…å€¼ command1 command2 command3 ;; #------\u003ebreak æ¨¡å¼2ï¼‰ command1 command2 command3 ;; *) # -------\u003eç›¸å½“äºdefault command1 command2 command3 ;; esac #-----\u003eç»“æŸæ ‡å¿— caseå·¥ä½œæ–¹å¼å¦‚ä¸Šæ‰€ç¤ºã€‚\nå–å€¼åé¢å¿…é¡»ä¸ºå…³é”®å­— inï¼Œ æ¯ä¸€æ¨¡å¼å¿…é¡»ä»¥å³æ‹¬å·ç»“æŸ å–å€¼å¯ä»¥ä¸ºå˜é‡æˆ–å¸¸æ•°ã€‚ åŒ¹é…å‘ç°å–å€¼ç¬¦åˆæŸä¸€æ¨¡å¼åï¼Œå…¶é—´æ‰€æœ‰å‘½ä»¤å¼€å§‹æ‰§è¡Œç›´è‡³ ;;ã€‚ ;; ä¸å…¶ä»–è¯­è¨€ä¸­çš„ break ç±»ä¼¼ï¼Œæ„æ€æ˜¯è·³åˆ°æ•´ä¸ª case è¯­å¥çš„æœ€åã€‚ å–å€¼å°†æ£€æµ‹åŒ¹é…çš„æ¯ä¸€ä¸ªæ¨¡å¼ã€‚ä¸€æ—¦æ¨¡å¼åŒ¹é…ï¼Œåˆ™æ‰§è¡Œå®ŒåŒ¹é…æ¨¡å¼ç›¸åº”å‘½ä»¤åä¸å†ç»§ç»­å…¶ä»–æ¨¡å¼ã€‚ å¦‚æœæ— ä¸€åŒ¹é…æ¨¡å¼ï¼Œä½¿ç”¨æ˜Ÿå· * æ•è·è¯¥å€¼ï¼Œå†æ‰§è¡Œåé¢çš„å‘½ä»¤ã€‚ å‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. ifè¯­å¥ if è¯­å¥é€šè¿‡å…³ç³»è¿ç®—ç¬¦åˆ¤æ–­è¡¨è¾¾å¼çš„çœŸå‡æ¥å†³å®šæ‰§è¡Œå“ªä¸ªåˆ†æ”¯ã€‚Shell æœ‰ä¸‰ç§ if ... else è¯­å¥ï¼š\nif ... â€¦","ref":"/linux-notes/shell/shell-if/","tags":["Shell"],"title":"Shell åˆ¤æ–­è¯­å¥"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækubeletä¸­syncPodçš„éƒ¨åˆ†ã€‚\n1. managePodLoop managePodLoopé€šè¿‡è¯»å–podUpdateschannelçš„ä¿¡æ¯ï¼Œæ‰§è¡ŒsyncPodFnå‡½æ•°ï¼Œè€ŒsyncPodFnå‡½æ•°åœ¨newPodWorkersçš„æ—¶å€™èµ‹å€¼äº†ï¼Œå³kubelet.syncPodã€‚\nmanagePodLoopå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/pod_workers.go\nfunc (p *podWorkers) managePodLoop(podUpdates \u003c-chan UpdatePodOptions) { var lastSyncTime time.Time for update := range podUpdates { err := func() error { podUID := update.Pod.UID // This is a blocking call that would return only if the cache // has an entry for the pod that is newer than minRuntimeCache // Time. This ensures the worker doesn't start syncing until // after the cache is at least newer than the finished time of // the previous sync. status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil { // This is the legacy event thrown by manage pod loop // all other events are now dispatched from syncPodFn p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, \"error determining status: %v\", err) return err } // è¯¥éƒ¨åˆ†çš„syncPodFnå®é™…ä¸Šçš„å®ç°å‡½æ•°æ˜¯kubelet.syncPod err = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, }) lastSyncTime = time.Now() return err }() // notify the call-back function if the operation succeeded or not if update.OnCompleteFunc != nil { update.OnCompleteFunc(err) } if err != nil { // IMPORTANT: we do not log errors here, the syncPodFn is responsible for logging errors glog.Errorf(\"Error syncing pod %s (%q), skipping: %v\", update.Pod.UID, format.Pod(update.Pod), err) } p.wrapUp(update.Pod.UID, err) } } ä»¥ä¸‹åˆ†æsyncPodç›¸å…³é€»è¾‘ã€‚\n2. syncPod syncPodå¯ä»¥ç†è§£ä¸ºæ˜¯ä¸€ä¸ªå•ä¸ªpodè¿›è¡ŒåŒæ­¥ä»»åŠ¡çš„äº‹åŠ¡è„šæœ¬ã€‚å…¶ä¸­å…¥å‚æ˜¯syncPodOptionsï¼ŒsyncPodOptionsè®°å½•äº†éœ€è¦åŒæ­¥çš„podçš„ç›¸å…³ä¿¡æ¯ã€‚å…·ä½“å®šä¹‰å¦‚ä¸‹ï¼š\n// syncPodOptions provides the arguments to a SyncPod operation. type syncPodOptions struct { // the mirror pod for the pod to sync, if it is a static pod mirrorPod *v1.Pod // pod to sync pod *v1.Pod // the type of update (create, update, sync) updateType kubetypes.SyncPodType // the current status podStatus *kubecontainer.PodStatus // if update type is kill, use the specified options to kill the pod. killPodOptions *KillPodOptions } syncPodä¸»è¦æ‰§è¡Œä»¥ä¸‹çš„å·¥ä½œæµï¼š\nå¦‚æœæ˜¯æ­£åœ¨åˆ›å»ºçš„podï¼Œåˆ™è®°å½•pod workerçš„å¯åŠ¨latencyã€‚ è°ƒç”¨generateAPIPodStatusä¸ºpodæä¾›v1.PodStatusä¿¡æ¯ã€‚ å¦‚æœpodæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œè®°å½•podçš„å¯åŠ¨latencyã€‚ æ›´æ–°status managerä¸­çš„podçŠ¶æ€ã€‚ å¦‚æœpodä¸åº”è¯¥è¢«è¿è¡Œåˆ™æ€æ­»podã€‚ å¦‚æœpodæ˜¯ä¸€ä¸ªstatic podï¼Œå¹¶ä¸”æ²¡æœ‰å¯¹åº”çš„mirror podï¼Œåˆ™åˆ›å»ºä¸€ä¸ªmirror podã€‚ å¦‚æœæ²¡æœ‰podçš„æ•°æ®ç›®å½•åˆ™ç»™podåˆ›å»ºå¯¹åº”çš„æ•°æ®ç›®å½•ã€‚ ç­‰å¾…volumeè¢«attachæˆ–mountã€‚ è·å–podçš„secretæ•°æ®ã€‚ è°ƒç”¨container runtimeçš„SyncPodå‡½æ•°ï¼Œæ‰§è¡Œç›¸å…³podæ“ä½œã€‚ æ›´æ–°podçš„ingresså’Œegressçš„traffic limitã€‚ å½“ä»¥ä¸Šä»»åŠ¡æµä¸­æœ‰ä»»ä½•çš„errorï¼Œåˆ™return errorã€‚åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡ŒsyncPodçš„ä»»åŠ¡æµä¼šè¢«å†æ¬¡æ‰§è¡Œã€‚å¯¹äºé”™è¯¯ä¿¡æ¯ä¼šè¢«è®°å½•åˆ°eventä¸­ï¼Œæ–¹ä¾¿debugã€‚\nä»¥ä¸‹å¯¹syncPodçš„æ‰§è¡Œè¿‡ç¨‹è¿›è¡Œåˆ†æã€‚\nsyncPodçš„ä»£ç ä½äºpkg/kubelet/kubelet.go\n2.1. SyncPodKill é¦–å…ˆï¼Œè·å–syncPodOptionsçš„podä¿¡æ¯ã€‚\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType ... } å¦‚æœpodæ˜¯éœ€è¦è¢«æ€æ­»çš„ï¼Œåˆ™æ‰§è¡ŒkillPodï¼Œä¼šåœ¨æŒ‡å®šçš„å®½é™æœŸå†…æ€æ­»podã€‚\n// if we want to kill a pod, do it now! if updateType == kubetypes.SyncPodKill { killPodOptions := o.killPodOptions if killPodOptions == nil || killPodOptions.PodStatusFunc == nil { return fmt.Errorf(\"kill pod options are required if update type is kill\") } apiPodStatus := killPodOptions.PodStatusFunc(pod, podStatus) kl.statusManager.SetPodStatus(pod, apiPodStatus) // we kill the pod with the specified grace period since this is a termination if err := kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \"error killing pod: %v\", err) // there was an error killing the pod, so we return that error directly utilruntime.HandleError(err) return err } return nil } 2.2. SyncPodCreate å¦‚æœpodæ˜¯éœ€è¦è¢«åˆ›å»ºçš„ï¼Œåˆ™è®°å½•podçš„å¯åŠ¨latencyï¼Œlatencyä¸podåœ¨apiserverä¸­ç¬¬ä¸€æ¬¡è¢«è®°å½•ç›¸å…³ã€‚\n// Latency measurements for the main workflow are relative to the // first time the pod was seen by the API server. var firstSeenTime time.Time if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok { firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get() } // Record pod worker start latency if being created // TODO: make pod workers record their own latencies if updateType == kubetypes.SyncPodCreate { if !firstSeenTime.IsZero() { // This is the first time we are syncing the pod. Record the latency // since kubelet first saw the pod if firstSeenTime is set. metrics.PodWorkerStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime)) } else { glog.V(3).Infof(\"First seen time not recorded for pod %q\", pod.UID) } } é€šè¿‡podå’Œpod statusç”Ÿæˆæœ€ç»ˆçš„api pod statuså¹¶è®¾ç½®podçš„IPã€‚\n// Generate final API pod status with pod and status manager status apiPodStatus := kl.generateAPIPodStatus(pod, podStatus) // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576) // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and // set pod IP to hostIP directly in runtime.GetPodStatus podStatus.IP = apiPodStatus.PodIP è®°å½•podåˆ°runningçŠ¶æ€çš„æ—¶é—´ã€‚\n// Record the time it takes for the pod to become running. existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok || existingStatus.Phase == v1.PodPending \u0026\u0026 apiPodStatus.Phase == v1.PodRunning \u0026\u0026 !firstSeenTime.IsZero() { metrics.PodStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime)) } å¦‚æœpodæ˜¯ä¸å¯è¿è¡Œçš„ï¼Œåˆ™æ›´æ–°podå’Œcontainerçš„çŠ¶æ€å’Œç›¸åº”çš„åŸå› ã€‚\nrunnable := kl.canRunPod(pod) if !runnable.Admit { // Pod is not runnable; update the Pod and Container statuses to why. apiPodStatus.Reason = runnable.Reason apiPodStatus.Message = runnable.Message // Waiting containers are not creating. const waitingReason = \"Blocked\" for _, cs := range apiPodStatus.InitContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } for _, cs := range apiPodStatus.ContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } } å¹¶æ›´æ–°status managerä¸­çš„çŠ¶æ€ä¿¡æ¯ï¼Œæ€æ­»ä¸å¯è¿è¡Œçš„podã€‚\n// Update status in the status manager kl.statusManager.SetPodStatus(pod, apiPodStatus) // Kill pod if it should not be running if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { var syncErr error if err := kl.killPod(pod, nil, podStatus, nil); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \"error killing pod: %v\", err) syncErr = fmt.Errorf(\"error killing pod: %v\", err) utilruntime.HandleError(syncErr) } else { if !runnable.Admit { // There was no error killing the pod, but the pod cannot be run. // Return an error to signal that the sync loop should back off. syncErr = fmt.Errorf(\"pod cannot be run: %s\", runnable.Message) } } return syncErr } å¦‚æœç½‘ç»œæ’ä»¶è¿˜æ²¡åˆ°ReadyçŠ¶æ€ï¼Œåˆ™åªæœ‰åœ¨ä½¿ç”¨hostç½‘ç»œæ¨¡å¼çš„æƒ…å†µä¸‹æ‰å¯åŠ¨podã€‚\n// If the network plugin is not ready, only start the pod if it uses the host network if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026\u0026 !kubecontainer.IsHostNetworkPod(pod) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, \"%s: %v\", NetworkNotReadyErrorMsg, rs) return fmt.Errorf(\"%s: %v\", NetworkNotReadyErrorMsg, rs) } 2.3. Cgroups ç»™podåˆ›å»ºCgroupsï¼Œå¦‚æœcgroups-per-qoså‚æ•°å¼€å¯ï¼Œåˆ™ç”³è¯·ç›¸åº”çš„èµ„æºã€‚å¯¹äºterminatedçš„podä¸éœ€è¦åˆ›å»ºæˆ–æ›´æ–°podçš„Cgroupsã€‚\nå½“é‡æ–°å¯åŠ¨kubeletå¹¶ä¸”å¯ç”¨cgroups-per-qosæ—¶ï¼Œåº”è¯¥é—´æ­‡æ€§åœ°ç»ˆæ­¢æ‰€æœ‰podçš„è¿è¡Œå®¹å™¨å¹¶åœ¨qos cgroup hierarchyä¸‹é‡æ–°å¯åŠ¨ã€‚\nå¦‚æœpodçš„cgroupå·²ç»å­˜åœ¨æˆ–è€…podç¬¬ä¸€æ¬¡è¿è¡Œï¼Œä¸æ€æ­»podä¸­å®¹å™¨ã€‚\n// Create Cgroups for the pod and apply resource parameters // to them if cgroups-per-qos flag is enabled. pcm := kl.containerManager.NewPodContainerManager() // If pod has already been terminated then we need not create // or update the pod's cgroup if !kl.podIsTerminated(pod) { // When the kubelet is restarted with the cgroups-per-qos // flag enabled, all the pod's running containers // should be killed intermittently and brought back up // under the qos cgroup hierarchy. // Check if this is the pod's first sync firstSync := true for _, containerStatus := range apiPodStatus.ContainerStatuses { if containerStatus.State.Running != nil { firstSync = false break } } // Don't kill containers in pod if pod's cgroups already // exists or the pod is running for the first time podKilled := false if !pcm.Exists(pod) \u0026\u0026 !firstSync { if err := kl.killPod(pod, nil, podStatus, nil); err == nil { podKilled = true } } ... å¦‚æœpodè¢«æ€æ­»å¹¶ä¸”é‡å¯ç­–ç•¥æ˜¯Neverï¼Œåˆ™ä¸åˆ›å»ºæˆ–æ›´æ–°å¯¹åº”çš„Cgroupsï¼Œå¦åˆ™åˆ›å»ºå’Œæ›´æ–°podçš„Cgroupsã€‚\n// Create and Update pod's Cgroups // Don't create cgroups for run once pod if it was killed above // The current policy is not to restart the run once pods when // the kubelet is restarted with the new flag as run once pods are // expected to run only once and if the kubelet is restarted then // they are not expected to run again. // We don't create and apply updates to cgroup if its a run once pod and was killed above if !(podKilled \u0026\u0026 pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { if err := kl.containerManager.UpdateQOSCgroups(); err != nil { glog.V(2).Infof(\"Failed to update QoS cgroups while syncing pod: %v\", err) } if err := pcm.EnsureExists(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, \"unable to ensure pod container exists: %v\", err) return fmt.Errorf(\"failed to ensure that the pod: %v cgroups exist and are correctly applied: %v\", pod.UID, err) } } } å…¶ä¸­åˆ›å»ºCgroupsæ˜¯é€šè¿‡containerManagerçš„UpdateQOSCgroupsæ¥æ‰§è¡Œã€‚\nif err := kl.containerManager.UpdateQOSCgroups(); err != nil { glog.V(2).Infof(\"Failed to update QoS cgroups while syncing pod: %v\", err) } 2.4. Mirror Pod å¦‚æœpodæ˜¯ä¸€ä¸ªstatic podï¼Œæ²¡æœ‰å¯¹åº”çš„mirror podï¼Œåˆ™åˆ›å»ºä¸€ä¸ªmirror podï¼›å¦‚æœå­˜åœ¨mirror podåˆ™åˆ é™¤å†é‡å»ºä¸€ä¸ªmirror podã€‚\n// Create Mirror Pod for Static Pod if it doesn't already exist if kubepod.IsStaticPod(pod) { podFullName := kubecontainer.GetPodFullName(pod) deleted := false if mirrorPod != nil { if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) { // The mirror pod is semantically different from the static pod. Remove // it. The mirror pod will get recreated later. glog.Warningf(\"Deleting mirror pod %q because it is outdated\", format.Pod(mirrorPod)) if err := kl.podManager.DeleteMirrorPod(podFullName); err != nil { glog.Errorf(\"Failed deleting mirror pod %q: %v\", format.Pod(mirrorPod), err) } else { deleted = true } } } if mirrorPod == nil || deleted { node, err := kl.GetNode() if err != nil || node.DeletionTimestamp != nil { glog.V(4).Infof(\"No need to create a mirror pod, since node %q has been removed from the cluster\", kl.nodeName) } else { glog.V(4).Infof(\"Creating a mirror pod for static pod %q\", format.Pod(pod)) if err := kl.podManager.CreateMirrorPod(pod); err != nil { glog.Errorf(\"Failed creating a mirror pod for %q: %v\", format.Pod(pod), err) } } } } 2.5. makePodDataDirs ç»™podåˆ›å»ºæ•°æ®ç›®å½•ã€‚\n// Make data directories for the pod if err := kl.makePodDataDirs(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, \"error making pod data directories: %v\", err) glog.Errorf(\"Unable to make pod data directories for pod %q: %v\", format.Pod(pod), err) return err } å…¶ä¸­æ•°æ®ç›®å½•åŒ…æ‹¬\nPodDirï¼š{kubelet.rootDirectory}/pods/podUID PodVolumesDirï¼š{PodDir}/volumes PodPluginsDirï¼š{PodDir}/plugins // makePodDataDirs creates the dirs for the pod datas. func (kl *Kubelet) makePodDataDirs(pod *v1.Pod) error { uid := pod.UID if err := os.MkdirAll(kl.getPodDir(uid), 0750); err != nil \u0026\u0026 !os.IsExist(err) { return err } if err := os.MkdirAll(kl.getPodVolumesDir(uid), 0750); err != nil \u0026\u0026 !os.IsExist(err) { return err } if err := os.MkdirAll(kl.getPodPluginsDir(uid), 0750); err != nil \u0026\u0026 !os.IsExist(err) { return err } return nil } 2.6. mount volumes å¯¹éterminatedçŠ¶æ€çš„podæŒ‚è½½volumeã€‚\n// Volume manager will not mount volumes for terminated pods if !kl.podIsTerminated(pod) { // Wait for volumes to attach/mount if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, \"Unable to mount volumes for pod %q: %v\", format.Pod(pod), err) glog.Errorf(\"Unable to mount volumes for pod %q: %v; skipping pod\", format.Pod(pod), err) return err } } 2.7. PullSecretsForPod è·å–podçš„secretæ•°æ®ã€‚\n// Fetch the pull secrets for the pod pullSecrets := kl.getPullSecretsForPod(pod) getPullSecretsForPodå…·ä½“å®ç°å‡½æ•°å¦‚ä¸‹ï¼š\n// getPullSecretsForPod inspects the Pod and retrieves the referenced pull // secrets. func (kl *Kubelet) getPullSecretsForPod(pod *v1.Pod) []v1.Secret { pullSecrets := []v1.Secret{} for _, secretRef := range pod.Spec.ImagePullSecrets { secret, err := kl.secretManager.GetSecret(pod.Namespace, secretRef.Name) if err != nil { glog.Warningf(\"Unable to retrieve pull secret %s/%s for %s/%s due to %v. The image pull may not succeed.\", pod.Namespace, secretRef.Name, pod.Namespace, pod.Name, err) continue } pullSecrets = append(pullSecrets, *secret) } return pullSecrets } 2.8. containerRuntime.SyncPod è°ƒç”¨container runtimeçš„SyncPodå‡½æ•°ï¼Œæ‰§è¡Œç›¸å…³podæ“ä½œï¼Œç”±æ­¤kubelet.syncPodçš„æ“ä½œé€»è¾‘è½¬å…¥containerRuntime.SyncPodå‡½æ•°ä¸­ã€‚\n// Call the container runtime's SyncPod callback result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { // Do not return error if the only failures were pods in backoff for _, r := range result.SyncResults { if r.Error != kubecontainer.ErrCrashLoopBackOff \u0026\u0026 r.Error != images.ErrImagePullBackOff { // Do not record an event here, as we keep all event logging for sync pod failures // local to container runtime so we get better errors return err } } return nil } 3. Runtime.SyncPod SyncPodä¸»è¦æ‰§è¡Œsyncæ“ä½œä½¿å¾—è¿è¡Œçš„podè¾¾åˆ°æœŸæœ›çŠ¶æ€çš„podã€‚ä¸»è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\nè®¡ç®—sandboxå’Œcontainerçš„å˜åŒ–ã€‚ å¿…è¦çš„æ—¶å€™æ€æ­»podã€‚ æ€æ­»æ‰€æœ‰ä¸éœ€è¦è¿è¡Œçš„containerã€‚ å¿…è¦æ—¶åˆ›å»ºsandboxã€‚ åˆ›å»ºinit containerã€‚ åˆ›å»ºæ­£å¸¸çš„containerã€‚ Runtime.SyncPodéƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kuberuntime/kuberuntime_manager.go\n3.1. computePodActions è®¡ç®—sandboxå’Œcontainerçš„å˜åŒ–ã€‚\n// Step 1: Compute sandbox and container changes. podContainerChanges := m.computePodActions(pod, podStatus) glog.V(3).Infof(\"computePodActions got %+v for pod %q\", podContainerChanges, format.Pod(pod)) if podContainerChanges.CreateSandbox { ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err) } if podContainerChanges.SandboxID != \"\" { m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, \"Pod sandbox changed, it will be killed and re-created.\") } else { glog.V(4).Infof(\"SyncPod received new pod %q, will create a sandbox for it\", format.Pod(pod)) } } 3.2. killPodWithSyncResult å¿…è¦çš„æ—¶å€™æ€æ­»podã€‚\n// Step 2: Kill the pod if the sandbox has changed. if podContainerChanges.KillPod { if !podContainerChanges.CreateSandbox { glog.V(4).Infof(\"Stopping PodSandbox for %q because all other containers are dead.\", format.Pod(pod)) } else { glog.V(4).Infof(\"Stopping PodSandbox for %q, will start new one\", format.Pod(pod)) } killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil { glog.Errorf(\"killPodWithSyncResult failed: %v\", killResult.Error()) return } if podContainerChanges.CreateSandbox { m.purgeInitContainers(pod, podStatus) } } 3.3. killContainer æ€æ­»æ‰€æœ‰ä¸éœ€è¦è¿è¡Œçš„containerã€‚\n// Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill { glog.V(3).Infof(\"Killing unwanted container %q(id=%q) for pod %q\", containerInfo.name, containerID, format.Pod(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) glog.Errorf(\"killContainer %q(id=%q) for pod %q failed: %v\", containerInfo.name, containerID, format.Pod(pod), err) return } } 3.4. createPodSandbox å¿…è¦æ—¶åˆ›å»ºsandboxã€‚\n// Step 4: Create a sandbox for the pod if necessary. ... glog.V(4).Infof(\"Creating sandbox for pod %q\", format.Pod(pod)) createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil { createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg) glog.Errorf(\"createPodSandbox for pod %q failed: %v\", format.Pod(pod), err) ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), referr) } m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, \"Failed create pod sandbox: %v\", err) return } glog.V(4).Infof(\"Created PodSandbox %q for pod %q\", podSandboxID, format.Pod(pod)) 3.5. start init container åˆ›å»ºinit containerã€‚\n// Step 5: start the init container. if container := podContainerChanges.NextInitContainerToStart; container != nil { // Start the next init container. startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) glog.V(4).Infof(\"Backing Off restarting init container %+v in pod %v\", container, format.Pod(pod)) return } glog.V(4).Infof(\"Creating init container %+v in pod %v\", container, format.Pod(pod)) if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil { startContainerResult.Fail(err, msg) utilruntime.HandleError(fmt.Errorf(\"init container start failed: %v: %s\", err, msg)) return } // Successfully started the container; clear the entry in the failure glog.V(4).Infof(\"Completed init container %q for pod %q\", container.Name, format.Pod(pod)) } 3.6. start containers åˆ›å»ºæ­£å¸¸çš„containerã€‚\n// Step 6: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart { container := \u0026pod.Spec.Containers[idx] startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) glog.V(4).Infof(\"Backing Off restarting container %+v in pod %v\", container, format.Pod(pod)) continue } glog.V(4).Infof(\"Creating container %+v in pod %v\", container, format.Pod(pod)) // é€šè¿‡startContaineræ¥è¿è¡Œå®¹å™¨ if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { startContainerResult.Fail(err, msg) // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch { case err == images.ErrImagePullBackOff: glog.V(3).Infof(\"container start failed: %v: %s\", err, msg) default: utilruntime.HandleError(fmt.Errorf(\"container start failed: %v: %s\", err, msg)) } continue } } 4. startContainer startContainerå¯åŠ¨ä¸€ä¸ªå®¹å™¨å¹¶è¿”å›æ˜¯å¦æˆåŠŸã€‚\nä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\næ‹‰å–é•œåƒ åˆ›å»ºå®¹å™¨ å¯åŠ¨å®¹å™¨ è¿è¡Œpost start lifecycle hooks(å¦‚æœæœ‰è®¾ç½®æ­¤é¡¹) startContainerå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\nstartContaineréƒ¨åˆ†ä»£ç ä½äºpkg/kubelet/kuberuntime/kuberuntime_container.go\n// startContainer starts a container and returns a message indicates why it is failed on error. // It starts the container through the following steps: // * pull the image // * create the container // * start the container // * run the post start lifecycle hooks (if applicable) func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) { // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return msg, err } // Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { glog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } glog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) // For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainerConfig } containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, \"Created container\") if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } // Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, \"Started container\") // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } // Step 4: execute the post start hook. if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { glog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } return \"\", nil } ä»¥ä¸‹å¯¹startContaineråˆ†æ®µåˆ†æï¼š\n4.1. pull image é€šè¿‡EnsureImageExistsæ–¹æ³•æ‹‰å–æ‹‰å–æŒ‡å®špodå®¹å™¨çš„é•œåƒï¼Œå¹¶è¿”å›é•œåƒä¿¡æ¯å’Œé”™è¯¯ã€‚\n// Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return msg, err } 4.2. CreateContainer é¦–å…ˆç”Ÿæˆcontainerçš„*v1.ObjectReferenceå¯¹è±¡ï¼Œè¯¥å¯¹è±¡åŒ…æ‹¬containerçš„ç›¸å…³ä¿¡æ¯ã€‚\n// Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { glog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } glog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) ç»Ÿè®¡containerçš„é‡å¯æ¬¡æ•°ï¼Œæ–°çš„å®¹å™¨é»˜è®¤é‡å¯æ¬¡æ•°ä¸º0ã€‚\n// For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } ç”Ÿæˆcontainerçš„é…ç½®ã€‚\ncontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainerConfig } è°ƒç”¨runtimeServiceï¼Œæ‰§è¡ŒCreateContainerçš„æ“ä½œã€‚\ncontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, \"Created container\") if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } 4.3. StartContainer æ‰§è¡ŒruntimeServiceçš„StartContaineræ–¹æ³•ï¼Œæ¥å¯åŠ¨å®¹å™¨ã€‚\n// Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, \"Started container\") // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } 4.4. execute post start hook å¦‚æœæœ‰æŒ‡å®šLifecycle.PostStartï¼Œåˆ™æ‰§è¡ŒPostStartæ“ä½œï¼ŒPostStartå¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œåˆ™å®¹å™¨ä¼šæ ¹æ®é‡å¯çš„è§„åˆ™è¿›è¡Œé‡å¯ã€‚\n// Step 4: execute the post start hook. if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { glog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } 5. æ€»ç»“ kubeletçš„å·¥ä½œæ˜¯ç®¡ç†podåœ¨Nodeä¸Šçš„ç”Ÿå‘½å‘¨æœŸï¼ˆåŒ…æ‹¬å¢åˆ æ”¹æŸ¥ï¼‰ï¼Œkubeleté€šè¿‡å„ç§ç±»å‹çš„managerå¼‚æ­¥å·¥ä½œå„è‡ªæ‰§è¡Œå„è‡ªçš„ä»»åŠ¡ï¼Œå…¶ä¸­ä½¿ç”¨åˆ°äº†å¤šç§çš„channelæ¥æ§åˆ¶çŠ¶æ€ä¿¡å·å˜åŒ–çš„ä¼ é€’ï¼Œä¾‹å¦‚æ¯”è¾ƒé‡è¦çš„channelæœ‰podUpdates \u003c-chan UpdatePodOptionsï¼Œæ¥ä¼ é€’podçš„å˜åŒ–æƒ…å†µã€‚\nåˆ›å»ºpodçš„è°ƒç”¨é€»è¾‘\nsyncLoopIteration--\u003ekubetypes.ADD--\u003eHandlePodAdditions(u.Pods)--\u003edispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)--\u003epodWorkers.UpdatePod--\u003emanagePodLoop(podUpdates)--\u003esyncPod(o syncPodOptions)--\u003econtainerRuntime.SyncPod--\u003estartContainer\nå‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/pod_workers.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kuberuntime/kuberuntime_container.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†ækubeletä¸­syncPodçš„éƒ¨åˆ†ã€‚\n1. â€¦","ref":"/k8s-source-code-analysis/kubelet/syncpod/","tags":["æºç åˆ†æ"],"title":"kubeletæºç åˆ†æï¼ˆäº”ï¼‰ä¹‹ syncPod"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/access/","tags":"","title":"è®¿é—®æ§åˆ¶"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/storage/","tags":"","title":"å®¹å™¨å­˜å‚¨"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/text/","tags":"","title":"æ–‡æœ¬å¤„ç†"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æä¼˜é€‰ç­–ç•¥é€»è¾‘ï¼Œå³ä»é¢„é€‰çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å‡ºæœ€ä¼˜çš„èŠ‚ç‚¹ã€‚ä¼˜é€‰ç­–ç•¥çš„å…·ä½“å®ç°å‡½æ•°ä¸ºPrioritizeNodesã€‚PrioritizeNodesæœ€ç»ˆè¿”å›æ˜¯ä¸€ä¸ªè®°å½•äº†å„ä¸ªèŠ‚ç‚¹åˆ†æ•°çš„åˆ—è¡¨ã€‚\n1. è°ƒç”¨å…¥å£ genericScheduler.Scheduleä¸­å¯¹PrioritizeNodesçš„è°ƒç”¨è¿‡ç¨‹å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\nfunc (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ... trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) // æ‰§è¡Œä¼˜é€‰é€»è¾‘çš„æ“ä½œï¼Œè¿”å›è®°å½•å„ä¸ªèŠ‚ç‚¹åˆ†æ•°çš„åˆ—è¡¨ priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) ... } æ ¸å¿ƒä»£ç ï¼š\n// åŸºäºé¢„é€‰èŠ‚ç‚¹filteredNodesè¿›ä¸€æ­¥ç­›é€‰ä¼˜é€‰çš„èŠ‚ç‚¹ï¼Œè¿”å›è®°å½•å„ä¸ªèŠ‚ç‚¹åˆ†æ•°çš„åˆ—è¡¨ priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 2. PrioritizeNodes ä¼˜é€‰ï¼Œä»æ»¡è¶³çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å‡ºæœ€ä¼˜çš„èŠ‚ç‚¹ã€‚PrioritizeNodesæœ€ç»ˆè¿”å›æ˜¯ä¸€ä¸ªè®°å½•äº†å„ä¸ªèŠ‚ç‚¹åˆ†æ•°çš„åˆ—è¡¨ã€‚\nå…·ä½“æ“ä½œå¦‚ä¸‹ï¼š\nPrioritizeNodesé€šè¿‡å¹¶è¡Œè¿è¡Œå„ä¸ªä¼˜å…ˆçº§å‡½æ•°æ¥å¯¹èŠ‚ç‚¹è¿›è¡Œä¼˜å…ˆçº§æ’åºã€‚ æ¯ä¸ªä¼˜å…ˆçº§å‡½æ•°ä¼šç»™èŠ‚ç‚¹æ‰“åˆ†ï¼Œæ‰“åˆ†èŒƒå›´ä¸º0-10åˆ†ã€‚ 0 è¡¨ç¤ºä¼˜å…ˆçº§æœ€ä½çš„èŠ‚ç‚¹ï¼Œ10è¡¨ç¤ºä¼˜å…ˆçº§æœ€é«˜çš„èŠ‚ç‚¹ã€‚ æ¯ä¸ªä¼˜å…ˆçº§å‡½æ•°ä¹Ÿæœ‰å„è‡ªçš„æƒé‡ã€‚ ä¼˜å…ˆçº§å‡½æ•°è¿”å›çš„èŠ‚ç‚¹åˆ†æ•°ä¹˜ä»¥æƒé‡ä»¥è·å¾—åŠ æƒåˆ†æ•°ã€‚ æœ€åç»„åˆï¼ˆæ·»åŠ ï¼‰æ‰€æœ‰åˆ†æ•°ä»¥è·å¾—æ‰€æœ‰èŠ‚ç‚¹çš„æ€»åŠ æƒåˆ†æ•°ã€‚ PrioritizeNodesä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\nå¦‚æœæ²¡æœ‰è®¾ç½®ä¼˜é€‰å‡½æ•°å’Œæ‹“å±•å‡½æ•°ï¼Œåˆ™å…¨éƒ¨èŠ‚ç‚¹è®¾ç½®ç›¸åŒçš„åˆ†æ•°ï¼Œç›´æ¥è¿”å›ã€‚ ä¾æ¬¡ç»™nodeæ‰§è¡Œmapå‡½æ•°è¿›è¡Œæ‰“åˆ†ã€‚ å†å¯¹ä¸Šè¿°mapå‡½æ•°çš„æ‰§è¡Œç»“æœæ‰§è¡Œreduceå‡½æ•°è®¡ç®—æœ€ç»ˆå¾—åˆ†ã€‚ æœ€åæ ¹æ®ä¸åŒä¼˜å…ˆçº§å‡½æ•°çš„æƒé‡å¯¹å¾—åˆ†å–åŠ æƒå¹³å‡æ•°ã€‚ å…¥å‚ï¼š\npod nodeNameToInfo meta interface{}, priorityConfigs nodes extenders å‡ºå‚ï¼š\nHostPriorityListï¼šè®°å½•èŠ‚ç‚¹åˆ†æ•°çš„åˆ—è¡¨ã€‚ HostPriorityå®šä¹‰å¦‚ä¸‹:\n// HostPriority represents the priority of scheduling to a particular host, higher priority is better. type HostPriority struct { // Name of the host Host string // Score associated with the host Score int } PrioritizeNodeså®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\n// PrioritizeNodes prioritizes the nodes by running the individual priority functions in parallel. // Each priority function is expected to set a score of 0-10 // 0 is the lowest priority score (least preferred node) and 10 is the highest // Each priority function can also have its own weight // The node scores returned by the priority function are multiplied by the weights to get weighted scores // All scores are finally combined (added) to get the total weighted scores of all nodes func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) { // If no priority configs are provided, then the EqualPriority function is applied // This is required to generate the priority list in the required format if len(priorityConfigs) == 0 \u0026\u0026 len(extenders) == 0 { result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } result = append(result, hostPriority) } return result, nil } var ( mu = sync.Mutex{} wg = sync.WaitGroup{} errs []error ) appendError := func(err error) { mu.Lock() defer mu.Unlock() errs = append(errs, err) } results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) for i, priorityConfig := range priorityConfigs { if priorityConfig.Function != nil { // DEPRECATED wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() var err error results[index], err = config.Function(pod, nodeNameToInfo, nodes) if err != nil { appendError(err) } }(i, priorityConfig) } else { results[i] = make(schedulerapi.HostPriorityList, len(nodes)) } } processNode := func(index int) { nodeInfo := nodeNameToInfo[nodes[index].Name] var err error for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) return } } } workqueue.Parallelize(16, len(nodes), processNode) for i, priorityConfig := range priorityConfigs { if priorityConfig.Reduce == nil { continue } wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() if err := config.Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if glog.V(10) { for _, hostPriority := range results[index] { glog.Infof(\"%v -\u003e %v: %v, Score: (%d)\", pod.Name, hostPriority.Host, config.Name, hostPriority.Score) } } }(i, priorityConfig) } // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } // Summarize all scores. result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) for j := range priorityConfigs { result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } if len(extenders) != 0 \u0026\u0026 nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } // wait for all go routines to finish wg.Wait() for i := range result { result[i].Score += combinedScores[result[i].Host] } } if glog.V(10) { for i := range result { glog.V(10).Infof(\"Host %s =\u003e Score %d\", result[i].Host, result[i].Score) } } return result, nil } ä»¥ä¸‹å¯¹PrioritizeNodesåˆ†æ®µè¿›è¡Œåˆ†æã€‚\n3. EqualPriorityMap å¦‚æœæ²¡æœ‰æä¾›ä¼˜é€‰å‡½æ•°å’Œæ‹“å±•å‡½æ•°ï¼Œåˆ™å°†æ‰€æœ‰çš„èŠ‚ç‚¹è®¾ç½®ä¸ºç›¸åŒçš„ä¼˜å…ˆçº§ï¼Œå³èŠ‚ç‚¹çš„scoreéƒ½ä¸º1ï¼Œç„¶åç›´æ¥è¿”å›ç»“æœã€‚(ä½†ä¸€èˆ¬æƒ…å†µä¸‹ä¼˜é€‰å‡½æ•°åˆ—è¡¨éƒ½ä¸ä¸ºç©º)\n// If no priority configs are provided, then the EqualPriority function is applied // This is required to generate the priority list in the required format if len(priorityConfigs) == 0 \u0026\u0026 len(extenders) == 0 { result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } result = append(result, hostPriority) } return result, nil } EqualPriorityMapå…·ä½“å®ç°å¦‚ä¸‹ï¼š\n// EqualPriorityMap is a prioritizer function that gives an equal weight of one to all nodes func EqualPriorityMap(_ *v1.Pod, _ interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } return schedulerapi.HostPriority{ Host: node.Name, Score: 1, }, nil } 4. processNode processNodeå°±æ˜¯åŸºäºindexæ‹¿å‡ºnodeçš„ä¿¡æ¯ï¼Œè°ƒç”¨ä¹‹å‰æ³¨å†Œçš„å„ç§ä¼˜é€‰å‡½æ•°ï¼ˆæ­¤å¤„æ˜¯mapFunctionï¼‰ï¼Œé€šè¿‡ä¼˜é€‰å‡½æ•°å¯¹nodeå’Œpodè¿›è¡Œå¤„ç†ï¼Œæœ€åè¿”å›ä¸€ä¸ªè®°å½•nodeåˆ†æ•°çš„åˆ—è¡¨resultã€‚processNodeåŒæ ·ä¹Ÿä½¿ç”¨workqueue.Parallelizeæ¥è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚(processNodeç±»ä¼¼äºé¢„é€‰é€»è¾‘findNodesThatFitä¸­ä½¿ç”¨åˆ°çš„checkNodeçš„ä½œç”¨)\nå…¶ä¸­ä¼˜é€‰å‡½æ•°æ˜¯é€šè¿‡priorityConfigsæ¥è®°å½•ï¼Œæ¯ç±»ä¼˜é€‰å‡½æ•°åŒ…æ‹¬PriorityMapFunctionå’ŒPriorityReduceFunctionä¸¤ç§å‡½æ•°ã€‚ä¼˜é€‰å‡½æ•°çš„æ³¨å†Œéƒ¨åˆ†å¯å‚è€ƒregisterAlgorithmProviderã€‚\nprocessNode := func(index int) { nodeInfo := nodeNameToInfo[nodes[index].Name] var err error for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) return } } } // å¹¶è¡Œæ‰§è¡ŒprocessNode workqueue.Parallelize(16, len(nodes), processNode) priorityConfigså®šä¹‰å¦‚ä¸‹ï¼š\næ ¸å¿ƒå±æ€§ï¼š\nMap ï¼šPriorityMapFunction Reduceï¼šPriorityReduceFunction // PriorityConfig is a config used for a priority function. type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int } å…·ä½“çš„ä¼˜é€‰å‡½æ•°å¤„ç†é€»è¾‘å¾…ä¸‹æ–‡åˆ†æï¼Œæœ¬æ–‡ä¼šä»¥NewSelectorSpreadPriorityå‡½æ•°ä¸ºä¾‹ã€‚\n5. PriorityMapFunction PriorityMapFunctionæ˜¯ä¸€ä¸ªè®¡ç®—ç»™å®šèŠ‚ç‚¹çš„æ¯ä¸ªèŠ‚ç‚¹ç»“æœçš„å‡½æ•°ã€‚\nPriorityMapFunctionå®šä¹‰å¦‚ä¸‹ï¼š\n// PriorityMapFunction is a function that computes per-node results for a given node. // TODO: Figure out the exact API of this method. // TODO: Change interface{} to a specific type. type PriorityMapFunction func(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) PriorityMapFunctionæ˜¯åœ¨processNodeä¸­è°ƒç”¨çš„ï¼Œä»£ç å¦‚ä¸‹ï¼š\nresults[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) ä¸‹æ–‡ä¼šåˆ†æNewSelectorSpreadPriorityåœ¨çš„mapå‡½æ•°CalculateSpreadPriorityMapã€‚\n6. PriorityReduceFunction PriorityReduceFunctionæ˜¯ä¸€ä¸ªèšåˆæ¯ä¸ªèŠ‚ç‚¹ç»“æœå¹¶è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„æœ€ç»ˆå¾—åˆ†çš„å‡½æ•°ã€‚\nPriorityReduceFunctionå®šä¹‰å¦‚ä¸‹ï¼š\n// PriorityReduceFunction is a function that aggregated per-node results and computes // final scores for all nodes. // TODO: Figure out the exact API of this method. // TODO: Change interface{} to a specific type. type PriorityReduceFunction func(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error PrioritizeNodesä¸­å¯¹reduceå‡½æ•°è°ƒç”¨éƒ¨åˆ†å¦‚ä¸‹ï¼š\nfor i, priorityConfig := range priorityConfigs { if priorityConfig.Reduce == nil { continue } wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() if err := config.Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if glog.V(10) { for _, hostPriority := range results[index] { glog.Infof(\"%v -\u003e %v: %v, Score: (%d)\", pod.Name, hostPriority.Host, config.Name, hostPriority.Score) } } }(i, priorityConfig) } ä¸‹æ–‡ä¼šåˆ†æNewSelectorSpreadPriorityåœ¨çš„reduceå‡½æ•°CalculateSpreadPriorityReduceã€‚\n7. Summarize all scores å…ˆç­‰å¾…è®¡ç®—å®Œæˆå†è®¡ç®—åŠ æƒå¹³å‡æ•°ã€‚\n// Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„åŠ æƒå¹³å‡æ•°ã€‚\n// Summarize all scores. result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) for j := range priorityConfigs { result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } å½“è®¾ç½®äº†æ‹“å±•çš„è®¡ç®—æ–¹å¼ï¼Œåˆ™å¢åŠ æ‹“å±•è®¡ç®—æ–¹å¼çš„åŠ æƒå¹³å‡æ•°ã€‚\nif len(extenders) != 0 \u0026\u0026 nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } // wait for all go routines to finish wg.Wait() for i := range result { result[i].Score += combinedScores[result[i].Host] } } 8. NewSelectorSpreadPriority ä»¥ä¸‹ä»¥NewSelectorSpreadPriorityè¿™ä¸ªä¼˜é€‰å‡½æ•°æ¥åšåˆ†æï¼Œå…¶ä»–é‡è¦çš„ä¼˜é€‰å‡½æ•°å¾…åç»­ä¸“é—¨åˆ†æã€‚\nNewSelectorSpreadPriorityä¸»è¦çš„åŠŸèƒ½æ˜¯å°†å±äºç›¸åŒserviceå’Œrsä¸‹çš„podå°½é‡åˆ†å¸ƒåœ¨ä¸åŒçš„nodeä¸Šã€‚\nè¯¥å‡½æ•°çš„æ³¨å†Œä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/algorithmprovider/defaults/defaults.go\n// ServiceSpreadingPriority is a priority config factory that spreads pods by minimizing // the number of pods (belonging to the same service) on the same node. // Register the factory so that it's available, but do not include it as part of the default priorities // Largely replaced by \"SelectorSpreadPriority\", but registered for backward compatibility with 1.0 factory.RegisterPriorityConfigFactory( \"ServiceSpreadingPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, algorithm.EmptyControllerLister{}, algorithm.EmptyReplicaSetLister{}, algorithm.EmptyStatefulSetLister{}) }, Weight: 1, }, ) NewSelectorSpreadPriorityçš„å…·ä½“å®ç°å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/algorithm/priorities/selector_spreading.go\n// NewSelectorSpreadPriority creates a SelectorSpread. func NewSelectorSpreadPriority( serviceLister algorithm.ServiceLister, controllerLister algorithm.ControllerLister, replicaSetLister algorithm.ReplicaSetLister, statefulSetLister algorithm.StatefulSetLister) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { selectorSpread := \u0026SelectorSpread{ serviceLister: serviceLister, controllerLister: controllerLister, replicaSetLister: replicaSetLister, statefulSetLister: statefulSetLister, } return selectorSpread.CalculateSpreadPriorityMap, selectorSpread.CalculateSpreadPriorityReduce } NewSelectorSpreadPriorityä¸»è¦åŒ…æ‹¬mapå’Œreduceä¸¤ç§å‡½æ•°ï¼Œåˆ†åˆ«å¯¹åº”CalculateSpreadPriorityMapï¼ŒCalculateSpreadPriorityReduceã€‚\n8.1. CalculateSpreadPriorityMap CalculateSpreadPriorityMapçš„ä¸»è¦ä½œç”¨æ˜¯å°†ç›¸åŒserviceã€RCã€RSæˆ–statefulsetçš„podåˆ†å¸ƒåœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚å½“è°ƒåº¦ä¸€ä¸ªpodçš„æ—¶å€™ï¼Œå…ˆå¯»æ‰¾ä¸è¯¥podåŒ¹é…çš„serviceã€RSã€RCæˆ–statefulsetï¼Œç„¶åå¯»æ‰¾ä¸å…¶selectoråŒ¹é…çš„å·²å­˜åœ¨çš„podï¼Œå¯»æ‰¾å­˜åœ¨è¿™ç±»podæœ€å°‘çš„èŠ‚ç‚¹ã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nå¯»æ‰¾ä¸è¯¥podå¯¹åº”çš„serviceã€RSã€RCã€statefulsetåŒ¹é…çš„selectorã€‚ éå†å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰podï¼Œå°†è¯¥èŠ‚ç‚¹ä¸Šå·²å­˜åœ¨çš„selectoråŒ¹é…åˆ°çš„podçš„ä¸ªæ•°ä½œä¸ºè¯¥èŠ‚ç‚¹çš„åˆ†æ•°ï¼ˆæ­¤æ—¶ï¼Œåˆ†æ•°å¤§çš„è¡¨ç¤ºåŒ¹é…åˆ°çš„podè¶Šå¤šï¼Œè¶Šä¸ç¬¦åˆè¢«è°ƒåº¦çš„æ¡ä»¶ï¼Œè¯¥åˆ†æ•°åœ¨reduceé˜¶æ®µä¼šè¢«æŒ‰10åˆ†åˆ¶å¤„ç†æˆåˆ†æ•°å¤§çš„è¶Šç¬¦åˆè¢«è°ƒåº¦çš„æ¡ä»¶ï¼‰ã€‚ æ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/algorithm/priorities/selector_spreading.go\n// CalculateSpreadPriorityMap spreads pods across hosts, considering pods // belonging to the same service,RC,RS or StatefulSet. // When a pod is scheduled, it looks for services, RCs,RSs and StatefulSets that match the pod, // then finds existing pods that match those selectors. // It favors nodes that have fewer existing matching pods. // i.e. it pushes the scheduler towards a node where there's the smallest number of // pods which match the same service, RC,RSs or StatefulSets selectors as the pod being scheduled. func (s *SelectorSpread) CalculateSpreadPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { var selectors []labels.Selector node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } priorityMeta, ok := meta.(*priorityMetadata) if ok { selectors = priorityMeta.podSelectors } else { selectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) } if len(selectors) == 0 { return schedulerapi.HostPriority{ Host: node.Name, Score: int(0), }, nil } count := int(0) for _, nodePod := range nodeInfo.Pods() { if pod.Namespace != nodePod.Namespace { continue } // When we are replacing a failed pod, we often see the previous // deleted version while scheduling the replacement. // Ignore the previous deleted version for spreading purposes // (it can still be considered for resource restrictions etc.) if nodePod.DeletionTimestamp != nil { glog.V(4).Infof(\"skipping pending-deleted pod: %s/%s\", nodePod.Namespace, nodePod.Name) continue } for _, selector := range selectors { if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) { count++ break } } } return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } ä»¥ä¸‹åˆ†æ®µåˆ†æï¼š\nå…ˆè·å¾—selectorã€‚\nselectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) è®¡ç®—èŠ‚ç‚¹ä¸ŠåŒ¹é…selectorçš„podçš„ä¸ªæ•°ï¼Œä½œä¸ºè¯¥èŠ‚ç‚¹åˆ†æ•°ï¼Œè¯¥åˆ†æ•°å¹¶ä¸æ˜¯æœ€ç»ˆèŠ‚ç‚¹çš„åˆ†æ•°ï¼Œåªæ˜¯ä¸­é—´è¿‡æ¸¡çš„è®°å½•çŠ¶æ€ã€‚\ncount := int(0) for _, nodePod := range nodeInfo.Pods() { ... for _, selector := range selectors { if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) { count++ break } } } 8.2. CalculateSpreadPriorityReduce CalculateSpreadPriorityReduceæ ¹æ®èŠ‚ç‚¹ä¸Šç°æœ‰åŒ¹é…podçš„æ•°é‡è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ååˆ†åˆ¶çš„åˆ†æ•°ï¼Œå…·æœ‰è¾ƒå°‘ç°æœ‰åŒ¹é…podçš„èŠ‚ç‚¹çš„åˆ†æ•°è¶Šé«˜ï¼Œè¡¨ç¤ºèŠ‚ç‚¹è¶Šå¯èƒ½è¢«è°ƒåº¦åˆ°ã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nè®°å½•æ‰€æœ‰èŠ‚ç‚¹ä¸­åŒ¹é…åˆ°podä¸ªæ•°æœ€å¤šçš„èŠ‚ç‚¹çš„åˆ†æ•°ï¼ˆå³åŒ¹é…åˆ°çš„podæœ€å¤šçš„ä¸ªæ•°ï¼‰ã€‚ éå†æ‰€æœ‰çš„èŠ‚ç‚¹ï¼ŒæŒ‰æ¯”ä¾‹å–ååˆ†åˆ¶çš„å¾—åˆ†ï¼Œè®¡ç®—æ–¹å¼ä¸ºï¼š(èŠ‚ç‚¹ä¸­æœ€å¤šåŒ¹é…podçš„ä¸ªæ•°-å½“å‰èŠ‚ç‚¹podçš„ä¸ªæ•°)/èŠ‚ç‚¹ä¸­æœ€å¤šåŒ¹é…podçš„ä¸ªæ•°ã€‚æ­¤æ—¶ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¯¥èŠ‚ç‚¹ä¸ŠåŒ¹é…åˆ°çš„podçš„ä¸ªæ•°è¶Šå°‘ï¼Œè¶Šå¯èƒ½è¢«è°ƒåº¦åˆ°ï¼Œå³æ»¡è¶³æŠŠç›¸åŒselectorçš„podåˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹çš„éœ€æ±‚ã€‚ æ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/algorithm/priorities/selector_spreading.go\n// CalculateSpreadPriorityReduce calculates the source of each node // based on the number of existing matching pods on the node // where zone information is included on the nodes, it favors nodes // in zones with fewer existing matching pods. func (s *SelectorSpread) CalculateSpreadPriorityReduce(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error { countsByZone := make(map[string]int, 10) maxCountByZone := int(0) maxCountByNodeName := int(0) for i := range result { if result[i].Score \u003e maxCountByNodeName { maxCountByNodeName = result[i].Score } zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == \"\" { continue } countsByZone[zoneID] += result[i].Score } for zoneID := range countsByZone { if countsByZone[zoneID] \u003e maxCountByZone { maxCountByZone = countsByZone[zoneID] } } haveZones := len(countsByZone) != 0 maxCountByNodeNameFloat64 := float64(maxCountByNodeName) maxCountByZoneFloat64 := float64(maxCountByZone) MaxPriorityFloat64 := float64(schedulerapi.MaxPriority) for i := range result { // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName \u003e 0 { fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) } // If there is zone information present, incorporate it if haveZones { zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID != \"\" { zoneScore := MaxPriorityFloat64 if maxCountByZone \u003e 0 { zoneScore = MaxPriorityFloat64 * (float64(maxCountByZone-countsByZone[zoneID]) / maxCountByZoneFloat64) } fScore = (fScore * (1.0 - zoneWeighting)) + (zoneWeighting * zoneScore) } } result[i].Score = int(fScore) if glog.V(10) { glog.Infof( \"%v -\u003e %v: SelectorSpreadPriority, Score: (%d)\", pod.Name, result[i].Host, int(fScore), ) } } return nil } ä»¥ä¸‹åˆ†æ®µåˆ†æï¼š\nå…ˆè·å–æ‰€æœ‰èŠ‚ç‚¹ä¸­åŒ¹é…åˆ°çš„podæœ€å¤šçš„ä¸ªæ•°ã€‚\nfor i := range result { if result[i].Score \u003e maxCountByNodeName { maxCountByNodeName = result[i].Score } zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == \"\" { continue } countsByZone[zoneID] += result[i].Score } éå†æ‰€æœ‰çš„èŠ‚ç‚¹ï¼ŒæŒ‰æ¯”ä¾‹å–ååˆ†åˆ¶çš„å¾—åˆ†ã€‚\nfor i := range result { // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName \u003e 0 { fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) } ... } 9. æ€»ç»“ ä¼˜é€‰ï¼Œä»æ»¡è¶³çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å‡ºæœ€ä¼˜çš„èŠ‚ç‚¹ã€‚PrioritizeNodesæœ€ç»ˆè¿”å›æ˜¯ä¸€ä¸ªè®°å½•äº†å„ä¸ªèŠ‚ç‚¹åˆ†æ•°çš„åˆ—è¡¨ã€‚\n9.1. PrioritizeNodes ä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š\nå¦‚æœæ²¡æœ‰è®¾ç½®ä¼˜é€‰å‡½æ•°å’Œæ‹“å±•å‡½æ•°ï¼Œåˆ™å…¨éƒ¨èŠ‚ç‚¹è®¾ç½®ç›¸åŒçš„åˆ†æ•°ï¼Œç›´æ¥è¿”å›ã€‚ ä¾æ¬¡ç»™nodeæ‰§è¡Œmapå‡½æ•°è¿›è¡Œæ‰“åˆ†ã€‚ å†å¯¹ä¸Šè¿°mapå‡½æ•°çš„æ‰§è¡Œç»“æœæ‰§è¡Œreduceå‡½æ•°è®¡ç®—æœ€ç»ˆå¾—åˆ†ã€‚ æœ€åæ ¹æ®ä¸åŒä¼˜å…ˆçº§å‡½æ•°çš„æƒé‡å¯¹å¾—åˆ†å–åŠ æƒå¹³å‡æ•°ã€‚ å…¶ä¸­æ¯ç±»ä¼˜é€‰å‡½æ•°ä¼šåŒ…å«mapå‡½æ•°å’Œreduceå‡½æ•°ä¸¤ç§ã€‚\n9.2. NewSelectorSpreadPriority å…¶ä¸­ä»¥NewSelectorSpreadPriorityè¿™ä¸ªä¼˜é€‰å‡½æ•°ä¸ºä¾‹ä½œåˆ†æï¼Œè¯¥å‡½æ•°çš„åŠŸèƒ½æ˜¯å°†ç›¸åŒserviceã€RSã€RCæˆ–statefulsetä¸‹podå°½é‡åˆ†æ•£åˆ°ä¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚åŒ…æ‹¬mapå‡½æ•°å’Œreduceå‡½æ•°ä¸¤éƒ¨åˆ†ï¼Œå…·ä½“å¦‚ä¸‹ã€‚\n9.2.1. CalculateSpreadPriorityMap åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nå¯»æ‰¾ä¸è¯¥podå¯¹åº”çš„serviceã€RSã€RCã€statefulsetåŒ¹é…çš„selectorã€‚ éå†å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰podï¼Œå°†è¯¥èŠ‚ç‚¹ä¸Šå·²å­˜åœ¨çš„selectoråŒ¹é…åˆ°çš„podçš„ä¸ªæ•°ä½œä¸ºè¯¥èŠ‚ç‚¹çš„åˆ†æ•°ï¼ˆæ­¤æ—¶ï¼Œåˆ†æ•°å¤§çš„è¡¨ç¤ºåŒ¹é…åˆ°çš„podè¶Šå¤šï¼Œè¶Šä¸ç¬¦åˆè¢«è°ƒåº¦çš„æ¡ä»¶ï¼Œè¯¥åˆ†æ•°åœ¨reduceé˜¶æ®µä¼šè¢«æŒ‰10åˆ†åˆ¶å¤„ç†æˆåˆ†æ•°å¤§çš„è¶Šç¬¦åˆè¢«è°ƒåº¦çš„æ¡ä»¶ï¼‰ã€‚ 9.2.2. CalculateSpreadPriorityReduce åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nè®°å½•æ‰€æœ‰èŠ‚ç‚¹ä¸­åŒ¹é…åˆ°podä¸ªæ•°æœ€å¤šçš„èŠ‚ç‚¹çš„åˆ†æ•°ï¼ˆå³åŒ¹é…åˆ°çš„podæœ€å¤šçš„ä¸ªæ•°ï¼‰ã€‚ éå†æ‰€æœ‰çš„èŠ‚ç‚¹ï¼ŒæŒ‰æ¯”ä¾‹å–ååˆ†åˆ¶çš„å¾—åˆ†ï¼Œè®¡ç®—æ–¹å¼ä¸ºï¼š(èŠ‚ç‚¹ä¸­æœ€å¤šåŒ¹é…podçš„ä¸ªæ•°-å½“å‰èŠ‚ç‚¹podçš„ä¸ªæ•°)/èŠ‚ç‚¹ä¸­æœ€å¤šåŒ¹é…podçš„ä¸ªæ•°ã€‚æ­¤æ—¶ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¯¥èŠ‚ç‚¹ä¸ŠåŒ¹é…åˆ°çš„podçš„ä¸ªæ•°è¶Šå°‘ï¼Œè¶Šå¯èƒ½è¢«è°ƒåº¦åˆ°ï¼Œå³æ»¡è¶³æŠŠç›¸åŒselectorçš„podåˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹çš„éœ€æ±‚ã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithm/priorities/selector_spreading.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æä¼˜é€‰ç­–ç•¥é€»è¾‘ï¼Œå³ä»é¢„é€‰çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å‡ºæœ€ä¼˜çš„èŠ‚ç‚¹ã€‚ä¼˜é€‰ç­–ç•¥çš„å…·ä½“ â€¦","ref":"/k8s-source-code-analysis/kube-scheduler/prioritizenodes/","tags":["æºç åˆ†æ"],"title":"kube-scheduleræºç åˆ†æï¼ˆäº”ï¼‰ä¹‹ ä¼˜é€‰ç­–ç•¥"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/tools/","tags":"","title":"è¿ç»´å·¥å…·"},{"body":" æœ¬æ–‡ä¸»è¦ä»‹ç»etcd-operatorçš„éƒ¨ç½²åŠä½¿ç”¨\n1. éƒ¨ç½²RBAC ä¸‹è½½create_role.shã€cluster-role-binding-template.yamlã€cluster-role-template.yaml\nä¾‹å¦‚ï¼š\n|-- cluster-role-binding-template.yaml |-- cluster-role-template.yaml |-- create_role.sh # éƒ¨ç½²rbac kubectl create ns operator bash create_role.sh --namespace=operator # namespaceä¸etcd-operatorçš„nsä¸€è‡´ ç¤ºä¾‹ï¼š\nbash create_role.sh --namespace=operator + ROLE_NAME=etcd-operator + ROLE_BINDING_NAME=etcd-operator + NAMESPACE=default + for i in '\"$@\"' + case $i in + NAMESPACE=operator + echo 'Creating role with ROLE_NAME=etcd-operator, NAMESPACE=operator' Creating role with ROLE_NAME=etcd-operator, NAMESPACE=operator + sed -e 's/\u003cROLE_NAME\u003e/etcd-operator/g' -e 's/\u003cNAMESPACE\u003e/operator/g' cluster-role-template.yaml + kubectl create -f - clusterrole.rbac.authorization.k8s.io/etcd-operator created + echo 'Creating role binding with ROLE_NAME=etcd-operator, ROLE_BINDING_NAME=etcd-operator, NAMESPACE=operator' Creating role binding with ROLE_NAME=etcd-operator, ROLE_BINDING_NAME=etcd-operator, NAMESPACE=operator + sed -e 's/\u003cROLE_NAME\u003e/etcd-operator/g' -e 's/\u003cROLE_BINDING_NAME\u003e/etcd-operator/g' -e 's/\u003cNAMESPACE\u003e/operator/g' cluster-role-binding-template.yaml + kubectl create -f - clusterrolebinding.rbac.authorization.k8s.io/etcd-operator created 1.1. create_role.sh è„šæœ¬ create_role.shæœ‰ä¸‰ä¸ªå…¥å‚ï¼Œå¯ä»¥æŒ‡å®š--namespaceå‚æ•°ï¼Œè¯¥å‚æ•°ä¸etcd-operatoréƒ¨ç½²çš„namespaceåº”ä¸€è‡´ã€‚é»˜è®¤ä¸ºdefaultã€‚\n#!/usr/bin/env bash set -o errexit set -o nounset set -o pipefail ETCD_OPERATOR_ROOT=$(dirname \"${BASH_SOURCE}\")/../.. print_usage() { echo \"$(basename \"$0\") - Create Kubernetes RBAC role and role bindings for etcd-operator Usage: $(basename \"$0\") [options...] Options: --role-name=STRING Name of ClusterRole to create (default=\\\"etcd-operator\\\", environment variable: ROLE_NAME) --role-binding-name=STRING Name of ClusterRoleBinding to create (default=\\\"etcd-operator\\\", environment variable: ROLE_BINDING_NAME) --namespace=STRING namespace to create role and role binding in. Must already exist. (default=\\\"default\\\", environment variable: NAMESPACE) \" \u003e\u00262 } ROLE_NAME=\"${ROLE_NAME:-etcd-operator}\" ROLE_BINDING_NAME=\"${ROLE_BINDING_NAME:-etcd-operator}\" NAMESPACE=\"${NAMESPACE:-default}\" for i in \"$@\" do case $i in --role-name=*) ROLE_NAME=\"${i#*=}\" ;; --role-binding-name=*) ROLE_BINDING_NAME=\"${i#*=}\" ;; --namespace=*) NAMESPACE=\"${i#*=}\" ;; -h|--help) print_usage exit 0 ;; *) print_usage exit 1 ;; esac done echo \"Creating role with ROLE_NAME=${ROLE_NAME}, NAMESPACE=${NAMESPACE}\" sed -e \"s/\u003cROLE_NAME\u003e/${ROLE_NAME}/g\" \\ -e \"s/\u003cNAMESPACE\u003e/${NAMESPACE}/g\" \\ \"cluster-role-template.yaml\" | \\ kubectl create -f - echo \"Creating role binding with ROLE_NAME=${ROLE_NAME}, ROLE_BINDING_NAME=${ROLE_BINDING_NAME}, NAMESPACE=${NAMESPACE}\" sed -e \"s/\u003cROLE_NAME\u003e/${ROLE_NAME}/g\" \\ -e \"s/\u003cROLE_BINDING_NAME\u003e/${ROLE_BINDING_NAME}/g\" \\ -e \"s/\u003cNAMESPACE\u003e/${NAMESPACE}/g\" \\ \"cluster-role-binding-template.yaml\" | \\ kubectl create -f - 1.2. cluster-role-binding-template.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: \u003cROLE_BINDING_NAME\u003e roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: \u003cROLE_NAME\u003e subjects: - kind: ServiceAccount name: default namespace: \u003cNAMESPACE\u003e 1.3. cluster-role-template.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: \u003cROLE_NAME\u003e rules: - apiGroups: - etcd.database.coreos.com resources: - etcdclusters - etcdbackups - etcdrestores verbs: - \"*\" - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - \"*\" - apiGroups: - \"\" resources: - pods - services - endpoints - persistentvolumeclaims - events verbs: - \"*\" - apiGroups: - apps resources: - deployments verbs: - \"*\" # The following permissions can be removed if not using S3 backup and TLS - apiGroups: - \"\" resources: - secrets verbs: - get 2. éƒ¨ç½²etcd-operator kubectl create -f etcd-operator.yaml etcd-operator.yamlå¦‚ä¸‹ï¼š\napiVersion: apps/v1 kind: Deployment metadata: name: etcd-operator namespace: operator # ä¸rbacæŒ‡å®šçš„nsä¸€è‡´ labels: app: etcd-operator spec: replicas: 1 selector: matchLabels: app: etcd-operator template: metadata: labels: app: etcd-operator spec: containers: - name: etcd-operator image: registry.cn-shenzhen.aliyuncs.com/huweihuang/etcd-operator:v0.9.4 command: - etcd-operator # Uncomment to act for resources in all namespaces. More information in doc/user/clusterwide.md - -cluster-wide env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name æŸ¥çœ‹CRD\n#kubectl get customresourcedefinitions NAME CREATED AT etcdclusters.etcd.database.coreos.com 2020-08-01T13:02:18Z æŸ¥çœ‹etcd-operatorçš„æ—¥å¿—æ˜¯å¦OKã€‚\nk logs -f etcd-operator-545df8d445-qpf6n -n operator time=\"2020-08-01T13:02:18Z\" level=info msg=\"etcd-operator Version: 0.9.4\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Git SHA: c8a1c64\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Go Version: go1.11.5\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Go OS/Arch: linux/amd64\" time=\"2020-08-01T13:02:18Z\" level=info msg=\"Event(v1.ObjectReference{Kind:\\\"Endpoints\\\", Namespace:\\\"operator\\\", Name:\\\"etcd-operator\\\", UID:\\\"7de38cff-1b7b-4bf2-9837-473fa66c9366\\\", APIVersion:\\\"v1\\\", ResourceVersion:\\\"41195930\\\", FieldPath:\\\"\\\"}): type: 'Normal' reason: 'LeaderElection' etcd-operator-545df8d445-qpf6n became leader\" ä»¥ä¸Šå†…å®¹è¡¨ç¤ºetcd-operatorè¿è¡Œæ­£å¸¸ã€‚\n3. éƒ¨ç½²etcdé›†ç¾¤ kubectl create -f etcd-cluster.yaml å½“å¼€å¯clusterwideåˆ™etcdé›†ç¾¤ä¸etcd-operatorçš„nså¯ä¸åŒã€‚\netcd-cluster.yaml\napiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"default-etcd-cluster\" ## Adding this annotation make this cluster managed by clusterwide operators ## namespaced operators ignore it annotations: etcd.database.coreos.com/scope: clusterwide namespace: etcd # æ­¤å¤„çš„nsè¡¨ç¤ºetcdé›†ç¾¤éƒ¨ç½²åœ¨å“ªä¸ªnsä¸‹ spec: size: 3 version: \"v3.3.18\" repository: registry.cn-shenzhen.aliyuncs.com/huweihuang/etcd pod: busyboxImage: registry.cn-shenzhen.aliyuncs.com/huweihuang/busybox:1.28.0-glibc æŸ¥çœ‹é›†ç¾¤éƒ¨ç½²ç»“æœ\n$ kgpo -n etcd NAME READY STATUS RESTARTS AGE default-etcd-cluster-b6phnpf8z8 1/1 Running 0 3m3s default-etcd-cluster-hhgq4sbtgr 1/1 Running 0 109s default-etcd-cluster-ttfh5fj92b 1/1 Running 0 2m29s 4. è®¿é—®etcdé›†ç¾¤ æŸ¥çœ‹service\n$ kgsvc -n etcd NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default-etcd-cluster ClusterIP None \u003cnone\u003e 2379/TCP,2380/TCP 5m37s default-etcd-cluster-client ClusterIP 192.168.255.244 \u003cnone\u003e 2379/TCP 5m37s ä½¿ç”¨serviceåœ°å€è®¿é—®\n# æŸ¥çœ‹é›†ç¾¤å¥åº·çŠ¶æ€ $ ETCDCTL_API=3 etcdctl --endpoints 192.168.255.244:2379 endpoint health 192.168.255.244:2379 is healthy: successfully committed proposal: took = 1.96126ms # å†™æ•°æ® $ ETCDCTL_API=3 etcdctl --endpoints 192.168.255.244:2379 put foo bar OK # è¯»æ•°æ® $ ETCDCTL_API=3 etcdctl --endpoints 192.168.255.244:2379 get foo foo bar 5. é”€æ¯etcd-operator kubectl delete -f example/deployment.yaml kubectl delete endpoints etcd-operator kubectl delete crd etcdclusters.etcd.database.coreos.com kubectl delete clusterrole etcd-operator kubectl delete clusterrolebinding etcd-operator å‚è€ƒï¼š\nhttps://github.com/coreos/etcd-operator https://github.com/coreos/etcd-operator/blob/master/doc/user/install_guide.md https://github.com/coreos/etcd-operator/blob/master/doc/user/client_service.md https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md https://github.com/coreos/etcd-operator/blob/master/doc/user/cluster_tls.md ","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦ä»‹ç»etcd-operatorçš„éƒ¨ç½²åŠä½¿ç”¨\n1. éƒ¨ç½²RBAC ä¸‹ â€¦","ref":"/kubernetes-notes/etcd/etcd-operator-usage/","tags":["Etcd"],"title":"etcd-operatorçš„ä½¿ç”¨"},{"body":"1. Podä¼¸ç¼© k8sä¸­RCçš„ç”¨æ¥ä¿æŒé›†ç¾¤ä¸­å§‹ç»ˆè¿è¡ŒæŒ‡å®šæ•°ç›®çš„å®ä¾‹ï¼Œé€šè¿‡RCçš„scaleæœºåˆ¶å¯ä»¥å®ŒæˆPodçš„æ‰©å®¹å’Œç¼©å®¹ï¼ˆä¼¸ç¼©ï¼‰ã€‚\n1.1. æ‰‹åŠ¨ä¼¸ç¼©ï¼ˆscaleï¼‰ kubectl scale rc redis-slave --replicas=3 1.2. è‡ªåŠ¨ä¼¸ç¼©ï¼ˆHPAï¼‰ Horizontal Pod Autoscalerï¼ˆHPAï¼‰æ§åˆ¶å™¨ç”¨äºå®ç°åŸºäºCPUä½¿ç”¨ç‡è¿›è¡Œè‡ªåŠ¨Podä¼¸ç¼©çš„åŠŸèƒ½ã€‚HPAæ§åˆ¶å™¨åŸºäºMasterçš„kube-controller-manageræœåŠ¡å¯åŠ¨å‚æ•°--horizontal-pod-autoscaler-sync-periodå®šä¹‰æ˜¯æ—¶é•¿ï¼ˆé»˜è®¤30ç§’ï¼‰ï¼Œå‘¨æœŸæ€§ç›‘æ§ç›®æ ‡Podçš„CPUä½¿ç”¨ç‡ï¼Œå¹¶åœ¨æ»¡è¶³æ¡ä»¶æ—¶å¯¹ReplicationControlleræˆ–Deploymentä¸­çš„Podå‰¯æœ¬æ•°è¿›è¡Œè°ƒæ•´ï¼Œä»¥ç¬¦åˆç”¨æˆ·å®šä¹‰çš„å¹³å‡Pod CPUä½¿ç”¨ç‡ã€‚Pod CPUä½¿ç”¨ç‡æ¥æºäºheapsterç»„ä»¶ï¼Œå› æ­¤éœ€å®‰è£…è¯¥ç»„ä»¶ã€‚\nå¯ä»¥é€šè¿‡kubectl autoscaleå‘½ä»¤è¿›è¡Œå¿«é€Ÿåˆ›å»ºæˆ–è€…ä½¿ç”¨yamlé…ç½®æ–‡ä»¶è¿›è¡Œåˆ›å»ºã€‚åˆ›å»ºä¹‹å‰éœ€å·²å­˜åœ¨ä¸€ä¸ªRCæˆ–Deploymentå¯¹è±¡ï¼Œå¹¶ä¸”è¯¥RCæˆ–Deploymentä¸­çš„Podå¿…é¡»å®šä¹‰resources.requests.cpuçš„èµ„æºè¯·æ±‚å€¼ï¼Œä»¥ä¾¿heapsteré‡‡é›†åˆ°è¯¥Podçš„CPUã€‚\n1.2.1. é€šè¿‡kubectl autoscaleåˆ›å»º ä¾‹å¦‚ï¼š\nphp-apache-rc.yaml\napiVersion: v1 kind: ReplicationController metadata: name: php-apache spec: replicas: 1 template: metadata: name: php-apache labels: app: php-apache spec: containers: - name: php-apache image: gcr.io/google_containers/hpa-example resources: requests: cpu: 200m ports: - containerPort: 80 åˆ›å»ºphp-apacheçš„RC\nkubectl create -f php-apache-rc.yaml php-apache-svc.yaml\napiVersion: v1 kind: Service metadata: name: php-apache spec: ports: - port: 80 selector: app: php-apache åˆ›å»ºphp-apacheçš„Service\nkubectl create -f php-apache-svc.yaml åˆ›å»ºHPAæ§åˆ¶å™¨\nkubectl autoscale rc php-apache --min=1 --max=10 --cpu-percent=50 1.2.2. é€šè¿‡yamlé…ç½®æ–‡ä»¶åˆ›å»º hpa-php-apache.yaml\napiVersion: v1 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: v1 kind: ReplicationController name: php-apache minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 åˆ›å»ºhpa\nkubectl create -f hpa-php-apache.yaml æŸ¥çœ‹hpa\nkubectl get hpa 2. Podæ»šåŠ¨å‡çº§ k8sä¸­çš„æ»šåŠ¨å‡çº§é€šè¿‡æ‰§è¡Œkubectl rolling-updateå‘½ä»¤å®Œæˆï¼Œè¯¥å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„RCï¼ˆä¸æ—§çš„RCåœ¨åŒä¸€ä¸ªå‘½åç©ºé—´ä¸­ï¼‰ï¼Œç„¶åè‡ªåŠ¨æ§åˆ¶æ—§çš„RCä¸­çš„Podå‰¯æœ¬æ•°é€æ¸å‡å°‘ä¸º0ï¼ŒåŒæ—¶æ–°çš„RCä¸­çš„Podå‰¯æœ¬æ•°ä»0é€æ¸å¢åŠ åˆ°é™„åŠ å€¼ï¼Œä½†æ»šåŠ¨å‡çº§ä¸­Podå‰¯æœ¬æ•°ï¼ˆåŒ…æ‹¬æ–°Podå’Œæ—§Podï¼‰ä¿æŒåŸé¢„æœŸå€¼ã€‚\n2.1. é€šè¿‡é…ç½®æ–‡ä»¶å®ç° redis-master-controller-v2.yaml\napiVersion: v1 kind: ReplicationController metadata: name: redis-master-v2 labels: name: redis-master version: v2 spec: replicas: 1 selector: name: redis-master version: v2 template: metadata: labels: name: redis-master version: v2 spec: containers: - name: master image: kubeguide/redis-master:2.0 ports: - containerPort: 6371 æ³¨æ„äº‹é¡¹ï¼š\nRCçš„åå­—ï¼ˆnameï¼‰ä¸èƒ½ä¸æ—§RCçš„åå­—ç›¸åŒ åœ¨selectorä¸­åº”è‡³å°‘æœ‰ä¸€ä¸ªLabelä¸æ—§çš„RCçš„Labelä¸åŒï¼Œä»¥æ ‡è¯†å…¶ä¸ºæ–°çš„RCã€‚ä¾‹å¦‚æœ¬ä¾‹ä¸­æ–°å¢äº†versionçš„Labelã€‚ è¿è¡Œkubectl rolling-update\nkubectl rolling-update redis-master -f redis-master-controller-v2.yaml 2.2. é€šè¿‡kubectl rolling-updateå‘½ä»¤å®ç° kubectl rolling-update redis-master --image=redis-master:2.0 ä¸ä½¿ç”¨é…ç½®æ–‡ä»¶å®ç°ä¸åŒåœ¨äºï¼Œè¯¥æ‰§è¡Œç»“æœæ—§çš„RCè¢«åˆ é™¤ï¼Œæ–°çš„RCä»ä½¿ç”¨æ—§çš„RCçš„åå­—ã€‚\n2.3. å‡çº§å›æ»š kubectl rolling-updateåŠ å‚æ•°--rollbackå®ç°å›æ»šæ“ä½œ\nkubectl rolling-update redis-master --image=kubeguide/redis-master:2.0 --rollback å‚è€ƒæ–‡ç« \nã€ŠKubernetesæƒå¨æŒ‡å—ã€‹ ","categories":"","description":"","excerpt":"1. Podä¼¸ç¼© k8sä¸­RCçš„ç”¨æ¥ä¿æŒé›†ç¾¤ä¸­å§‹ç»ˆè¿è¡ŒæŒ‡å®šæ•°ç›®çš„å®ä¾‹ï¼Œé€šè¿‡RCçš„scaleæœºåˆ¶å¯ä»¥å®ŒæˆPodçš„æ‰©å®¹å’Œç¼©å®¹ï¼ˆä¼¸ç¼©ï¼‰ã€‚\n1.1. â€¦","ref":"/kubernetes-notes/concepts/pod/pod-operation/","tags":["Kubernetes"],"title":"Podä¼¸ç¼©ä¸å‡çº§"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/git/","tags":"","title":"GITå‘½ä»¤"},{"body":"runcä»£ç ç›®å½•ç»“æ„ â”œâ”€â”€ create.go # createCommand â”œâ”€â”€ delete.go # deleteCommand â”œâ”€â”€ events.go # eventsCommand â”œâ”€â”€ exec.go # execCommand â”œâ”€â”€ features.go # featuresCommand â”œâ”€â”€ kill.go # killCommand â”œâ”€â”€ libcontainer # æ ¸å¿ƒå®ç°é€»è¾‘ â”œâ”€â”€ list.go # listCommand â”œâ”€â”€ main.go # mainå‡½æ•° â”œâ”€â”€ pause.go # pauseCommand â”œâ”€â”€ ps.go # psCommand â”œâ”€â”€ restore.go # restoreCommand â”œâ”€â”€ run.go # runCommand â”œâ”€â”€ spec.go # specCommand â”œâ”€â”€ start.go # startCommand â”œâ”€â”€ state.go # stateCommand â”œâ”€â”€ update.go # updateCommand â”œâ”€â”€ utils_linux.go # startContainer libcontainerç›®å½•ç»“æ„ libcontainer â”œâ”€â”€ apparmor â”œâ”€â”€ capabilities â”œâ”€â”€ cgroups â”œâ”€â”€ configs â”œâ”€â”€ console_linux.go â”œâ”€â”€ container.go â”œâ”€â”€ container_linux.go â”œâ”€â”€ container_linux_test.go â”œâ”€â”€ criu_opts_linux.go â”œâ”€â”€ devices â”œâ”€â”€ error.go â”œâ”€â”€ factory_linux.go â”œâ”€â”€ factory_linux_test.go â”œâ”€â”€ init_linux.go â”œâ”€â”€ integration â”œâ”€â”€ intelrdt â”œâ”€â”€ keys â”œâ”€â”€ logs â”œâ”€â”€ message_linux.go â”œâ”€â”€ mount_linux.go â”œâ”€â”€ network_linux.go â”œâ”€â”€ notify_linux.go â”œâ”€â”€ notify_linux_test.go â”œâ”€â”€ notify_v2_linux.go â”œâ”€â”€ nsenter â”œâ”€â”€ process.go â”œâ”€â”€ process_linux.go â”œâ”€â”€ restored_process.go â”œâ”€â”€ rootfs_linux.go â”œâ”€â”€ rootfs_linux_test.go â”œâ”€â”€ seccomp â”œâ”€â”€ setns_init_linux.go â”œâ”€â”€ specconv â”œâ”€â”€ standard_init_linux.go â”œâ”€â”€ state_linux.go â”œâ”€â”€ state_linux_test.go â”œâ”€â”€ stats_linux.go â”œâ”€â”€ sync.go â”œâ”€â”€ system â”œâ”€â”€ user â”œâ”€â”€ userns â””â”€â”€ utils Mainå‡½æ•° runcçš„ä»£ç ä»“åº“ä¸»è¦ä½¿ç”¨äº†github.com/urfave/cliçš„å‘½ä»¤æ¡†æ¶ï¼ˆè¯¥æ¡†æ¶ä¸cobraå‘½ä»¤æ¡†æ¶ç±»ä¼¼ï¼‰ã€‚æ·»åŠ äº†å¤šä¸ªé‡è¦çš„å­å‘½ä»¤ã€‚\nfunc main() { app := cli.NewApp() app.Name = \"runc\" app.Usage = usage app.Commands = []cli.Command{ checkpointCommand, createCommand, deleteCommand, eventsCommand, execCommand, killCommand, listCommand, pauseCommand, psCommand, restoreCommand, resumeCommand, runCommand, specCommand, startCommand, stateCommand, updateCommand, featuresCommand, } runCommand ä»¥runCommandä¸ºä¾‹åˆ†æå­å‘½ä»¤çš„è°ƒç”¨æµç¨‹ã€‚\ngithub.com/urfave/cliå‘½ä»¤æ¡†æ¶ä»£ç æ ¼å¼ï¼š\nåˆ›å»ºä¸€ä¸ªCommandç»“æ„ä½“ï¼ŒåŒ…å«ï¼š\nNameï¼šå‘½ååç§°\nUsageï¼šä½¿ç”¨è¯´æ˜\nDescriptionï¼šæè¿°å‘½ä»¤ä¿¡æ¯\nFlagsï¼šè§£æå‚æ•°\nAction: command runçš„æ ¸å¿ƒé€»è¾‘ã€‚\n// default action is to start a container var runCommand = cli.Command{ Name: \"run\", Usage: \"create and run a container\", // åˆ é™¤æè¿°ä¿¡æ¯ ArgsUsage: ``, Description: ``, Flags: []cli.Flag{ cli.StringFlag{ Name: \"bundle, b\", Value: \"\", Usage: `path to the root of the bundle directory, defaults to the current directory`, }, // åˆ é™¤å¤šä½™çš„FLAGä»£ç  }, Action: func(context *cli.Context) error { if err := checkArgs(context, 1, exactArgs); err != nil { return err } // æ ¸å¿ƒä»£ç ï¼Œå¯åŠ¨å®¹å™¨ status, err := startContainer(context, CT_ACT_RUN, nil) if err == nil { // exit with the container's exit status so any external supervisor is // notified of the exit with the correct exit status. os.Exit(status) } return fmt.Errorf(\"runc run failed: %w\", err) }, } startContainer å¯åŠ¨å®¹å™¨çš„æµç¨‹ï¼š\nsetup specä¿¡æ¯ã€‚\nåŸºäºspecä¿¡æ¯åˆ›å»ºcontainerã€‚\né€šè¿‡runnerå¯åŠ¨è¿›ç¨‹ã€‚\nåˆ é™¤errorå¤„ç†ä»£ç \nfunc startContainer(context *cli.Context, action CtAct, criuOpts *libcontainer.CriuOpts) (int, error) { if err := revisePidFile(context); err != nil { return -1, err } spec, err := setupSpec(context) id := context.Args().First() notifySocket := newNotifySocket(context, os.Getenv(\"NOTIFY_SOCKET\"), id) if notifySocket != nil { notifySocket.setupSpec(spec) } container, err := createContainer(context, id, spec) if notifySocket != nil { if err := notifySocket.setupSocketDirectory(); err != nil { return -1, err } if action == CT_ACT_RUN { if err := notifySocket.bindSocket(); err != nil { return -1, err } } } // Support on-demand socket activation by passing file descriptors into the container init process. listenFDs := []*os.File{} if os.Getenv(\"LISTEN_FDS\") != \"\" { listenFDs = activation.Files(false) } r := \u0026runner{ enableSubreaper: !context.Bool(\"no-subreaper\"), shouldDestroy: !context.Bool(\"keep\"), container: container, listenFDs: listenFDs, notifySocket: notifySocket, consoleSocket: context.String(\"console-socket\"), detach: context.Bool(\"detach\"), pidFile: context.String(\"pid-file\"), preserveFDs: context.Int(\"preserve-fds\"), action: action, criuOpts: criuOpts, init: true, } return r.run(spec.Process) } å¾…å®Œå–„\n","categories":"","description":"","excerpt":"runcä»£ç ç›®å½•ç»“æ„ â”œâ”€â”€ create.go # createCommand â”œâ”€â”€ delete.go # deleteCommand â€¦","ref":"/k8s-source-code-analysis/kubelet/runc-code-analysis/","tags":["æºç åˆ†æ"],"title":"runcæºç åˆ†æ"},{"body":"1. for forå¾ªç¯ä¸€èˆ¬æ ¼å¼ä¸ºï¼š\nfor å˜é‡ in åˆ—è¡¨ do command1 command2 ... commandN done åˆ—è¡¨æ˜¯ä¸€ç»„å€¼ï¼ˆæ•°å­—ã€å­—ç¬¦ä¸²ç­‰ï¼‰ç»„æˆçš„åºåˆ—ï¼Œæ¯ä¸ªå€¼é€šè¿‡ç©ºæ ¼åˆ†éš”ã€‚æ¯å¾ªç¯ä¸€æ¬¡ï¼Œå°±å°†åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ä¸ªå€¼èµ‹ç»™å˜é‡ã€‚ in åˆ—è¡¨æ˜¯å¯é€‰çš„ï¼Œå¦‚æœä¸ç”¨å®ƒï¼Œfor å¾ªç¯ä½¿ç”¨å‘½ä»¤è¡Œçš„ä½ç½®å‚æ•°ã€‚\nç¤ºä¾‹ï¼š\nfor loop in 1 2 3 4 5 do echo \"The value is: $loop\" done # è¿è¡Œç»“æœï¼š The value is: 1 The value is: 2 The value is: 3 The value is: 4 The value is: 5 2. while whileå¾ªç¯ç”¨äºä¸æ–­æ‰§è¡Œä¸€ç³»åˆ—å‘½ä»¤ï¼Œä¹Ÿç”¨äºä»è¾“å…¥æ–‡ä»¶ä¸­è¯»å–æ•°æ®\nwhile command do Statement(s) to be executed if command is true done å‘½ä»¤æ‰§è¡Œå®Œæ¯•ï¼Œæ§åˆ¶è¿”å›å¾ªç¯é¡¶éƒ¨ï¼Œä»å¤´å¼€å§‹ç›´è‡³æµ‹è¯•æ¡ä»¶ä¸ºå‡ã€‚\nç¤ºä¾‹ï¼š\nCOUNTER=0 while [ $COUNTER -lt 5 ] do COUNTER='expr $COUNTER+1' echo $COUNTER done whileå¾ªç¯å¯ç”¨äºè¯»å–é”®ç›˜ä¿¡æ¯ã€‚ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œè¾“å…¥ä¿¡æ¯è¢«è®¾ç½®ä¸ºå˜é‡FILMï¼ŒæŒ‰\u003cCtrl-D\u003eç»“æŸå¾ªç¯ã€‚\necho 'type \u003cCTRL-D\u003e to terminate' echo -n 'enter your most liked film: ' while read FILM do echo \"Yeah! great film the $FILM\" done 3. until until å¾ªç¯æ‰§è¡Œä¸€ç³»åˆ—å‘½ä»¤ç›´è‡³æ¡ä»¶ä¸º true æ—¶åœæ­¢ã€‚until å¾ªç¯ä¸ while å¾ªç¯åœ¨å¤„ç†æ–¹å¼ä¸Šåˆšå¥½ç›¸åã€‚ä¸€èˆ¬whileå¾ªç¯ä¼˜äºuntilå¾ªç¯ï¼Œä½†åœ¨æŸäº›æ—¶å€™ï¼Œä¹Ÿåªæ˜¯æå°‘æ•°æƒ…å†µä¸‹ï¼Œuntil å¾ªç¯æ›´åŠ æœ‰ç”¨ã€‚ until å¾ªç¯æ ¼å¼ä¸ºï¼š\nuntil command do Statement(s) to be executed until command is true done command ä¸€èˆ¬ä¸ºæ¡ä»¶è¡¨è¾¾å¼ï¼Œå¦‚æœè¿”å›å€¼ä¸º falseï¼Œåˆ™ç»§ç»­æ‰§è¡Œå¾ªç¯ä½“å†…çš„è¯­å¥ï¼Œå¦åˆ™è·³å‡ºå¾ªç¯ã€‚\nç¤ºä¾‹ï¼š\n#!/bin/bash a=0 until [ ! $a -lt 10 ] do echo $a a=`expr $a + 1` done 4. breakå‘½ä»¤ breakå‘½ä»¤å…è®¸è·³å‡ºæ‰€æœ‰ï¼ˆç»ˆæ­¢æ‰§è¡Œåé¢çš„æ‰€æœ‰å¾ªç¯ï¼‰ã€‚\nåœ¨åµŒå¥—å¾ªç¯ä¸­ï¼Œbreak å‘½ä»¤åé¢è¿˜å¯ä»¥è·Ÿä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºè·³å‡ºç¬¬å‡ å±‚å¾ªç¯\nbreak n è¡¨ç¤ºè·³å‡ºç¬¬ n å±‚å¾ªç¯ã€‚\nç¤ºä¾‹ï¼š\n#!/bin/bash while : do echo -n \"Input a number between 1 to 5: \" read aNum case $aNum in 1|2|3|4|5) echo \"Your number is $aNum!\" ;; *) echo \"You do not select a number between 1 to 5, game is over!\" break ;; esac done 5. continueå‘½ä»¤ continueå‘½ä»¤ä¸breakå‘½ä»¤ç±»ä¼¼ï¼Œåªæœ‰ä¸€ç‚¹å·®åˆ«ï¼Œå®ƒä¸ä¼šè·³å‡ºæ‰€æœ‰å¾ªç¯ï¼Œä»…ä»…è·³å‡ºå¾ªç¯\nåŒæ ·ï¼Œcontinue åé¢ä¹Ÿå¯ä»¥è·Ÿä¸€ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºè·³å‡ºç¬¬å‡ å±‚å¾ªç¯ã€‚\nç¤ºä¾‹ï¼š\n#!/bin/bash while : do echo -n \"Input a number between 1 to 5: \" read aNum case $aNum in 1|2|3|4|5) echo \"Your number is $aNum!\" ;; *) echo \"You do not select a number between 1 to 5!\" continue echo \"Game is over!\" ;; esac done å‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. for forå¾ªç¯ä¸€èˆ¬æ ¼å¼ä¸ºï¼š\nfor å˜é‡ in åˆ—è¡¨ do command1 command2 ... commandN done â€¦","ref":"/linux-notes/shell/shell-loop/","tags":["Shell"],"title":"Shell å¾ªç¯è¯­å¥"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/test/","tags":"","title":"å•å…ƒæµ‹è¯•"},{"body":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æè°ƒåº¦ä¸­çš„æŠ¢å é€»è¾‘ï¼Œå½“podä¸é€‚åˆä»»ä½•èŠ‚ç‚¹çš„æ—¶å€™ï¼Œå¯èƒ½podä¼šè°ƒåº¦å¤±è´¥ï¼Œè¿™æ—¶å€™å¯èƒ½ä¼šå‘ç”ŸæŠ¢å ã€‚æŠ¢å é€»è¾‘çš„å…·ä½“å®ç°å‡½æ•°ä¸ºScheduler.preemptã€‚\n1. è°ƒç”¨å…¥å£ å½“podä¸é€‚åˆä»»ä½•èŠ‚ç‚¹çš„æ—¶å€™ï¼Œå¯èƒ½podä¼šè°ƒåº¦å¤±è´¥ã€‚è¿™æ—¶å€™å¯èƒ½ä¼šå‘ç”ŸæŠ¢å ã€‚\nscheduleOneå‡½æ•°ä¸­å…³äºæŠ¢å è°ƒç”¨çš„é€»è¾‘å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†çš„ä»£ç ä½äº/pkg/scheduler/scheduler.go\n// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. func (sched *Scheduler) scheduleOne() { ... suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() // æ‰§è¡ŒæŠ¢å é€»è¾‘ sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } ... } å…¶ä¸­æ ¸å¿ƒä»£ç ä¸ºï¼š\n// åŸºäºsched.schedule(pod)è¿”å›çš„errå’Œå½“å‰å¾…è°ƒåº¦çš„podæ‰§è¡ŒæŠ¢å ç­–ç•¥ sched.preempt(pod, fitError) 2. Scheduler.preempt å½“podè°ƒåº¦å¤±è´¥çš„æ—¶å€™ï¼Œä¼šæŠ¢å ä½ä¼˜å…ˆçº§podçš„ç©ºé—´æ¥ç»™é«˜ä¼˜å…ˆçº§çš„podã€‚å…¶ä¸­å…¥å‚ä¸ºè°ƒåº¦å¤±è´¥çš„podå¯¹è±¡å’Œè°ƒåº¦å¤±è´¥çš„errã€‚\næŠ¢å çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nåˆ¤æ–­æ˜¯å¦æœ‰å…³é—­æŠ¢å æœºåˆ¶ï¼Œå¦‚æœå…³é—­æŠ¢å æœºåˆ¶åˆ™ç›´æ¥è¿”å›ã€‚ è·å–è°ƒåº¦å¤±è´¥podçš„æœ€æ–°å¯¹è±¡æ•°æ®ã€‚ æ‰§è¡ŒæŠ¢å ç®—æ³•Algorithm.Preemptï¼Œè¿”å›é¢„è°ƒåº¦èŠ‚ç‚¹å’Œéœ€è¦è¢«å‰”é™¤çš„podåˆ—è¡¨ã€‚ å°†æŠ¢å ç®—æ³•è¿”å›çš„nodeæ·»åŠ åˆ°podçš„Status.NominatedNodeNameä¸­ï¼Œå¹¶åˆ é™¤éœ€è¦è¢«å‰”é™¤çš„podã€‚ å½“æŠ¢å ç®—æ³•è¿”å›çš„nodeæ˜¯nilçš„æ—¶å€™ï¼Œæ¸…é™¤podçš„Status.NominatedNodeNameä¿¡æ¯ã€‚ æ•´ä¸ªæŠ¢å æµç¨‹çš„æœ€ç»ˆç»“æœå®é™…ä¸Šæ˜¯æ›´æ–°Pod.Status.NominatedNodeNameå±æ€§çš„ä¿¡æ¯ã€‚å¦‚æœæŠ¢å ç®—æ³•è¿”å›çš„èŠ‚ç‚¹ä¸ä¸ºç©ºï¼Œåˆ™å°†è¯¥nodeæ›´æ–°åˆ°Pod.Status.NominatedNodeNameä¸­ï¼Œå¦åˆ™å°±å°†Pod.Status.NominatedNodeNameè®¾ç½®ä¸ºç©ºã€‚\n2.1. preempt preemptçš„å…·ä½“å®ç°å‡½æ•°ï¼š\næ­¤éƒ¨åˆ†çš„ä»£ç ä½äº/pkg/scheduler/scheduler.go\n// preempt tries to create room for a pod that has failed to schedule, by preempting lower priority pods if possible. // If it succeeds, it adds the name of the node where preemption has happened to the pod annotations. // It returns the node name and an error if any. func (sched *Scheduler) preempt(preemptor *v1.Pod, scheduleErr error) (string, error) { if !util.PodPriorityEnabled() || sched.config.DisablePreemption { glog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" + \" No preemption is performed.\") return \"\", nil } preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil { glog.Errorf(\"Error getting the updated preemptor pod object: %v\", err) return \"\", err } node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) metrics.PreemptionVictims.Set(float64(len(victims))) if err != nil { glog.Errorf(\"Error preempting victims to make room for %v/%v.\", preemptor.Namespace, preemptor.Name) return \"\", err } var nodeName = \"\" if node != nil { nodeName = node.Name err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil { glog.Errorf(\"Error in preemption process. Cannot update pod %v/%v annotations: %v\", preemptor.Namespace, preemptor.Name, err) return \"\", err } for _, victim := range victims { if err := sched.config.PodPreemptor.DeletePod(victim); err != nil { glog.Errorf(\"Error preempting pod %v/%v: %v\", victim.Namespace, victim.Name, err) return \"\", err } sched.config.Recorder.Eventf(victim, v1.EventTypeNormal, \"Preempted\", \"by %v/%v on node %v\", preemptor.Namespace, preemptor.Name, nodeName) } } // Clearing nominated pods should happen outside of \"if node != nil\". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { glog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } return nodeName, err } ä»¥ä¸‹å¯¹preemptçš„å®ç°åˆ†æ®µåˆ†æã€‚\nå¦‚æœè®¾ç½®å…³é—­æŠ¢å æœºåˆ¶ï¼Œåˆ™ç›´æ¥è¿”å›ã€‚\nif !util.PodPriorityEnabled() || sched.config.DisablePreemption { glog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" + \" No preemption is performed.\") return \"\", nil } è·å–å½“å‰podçš„æœ€æ–°çŠ¶æ€ã€‚\npreemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil { glog.Errorf(\"Error getting the updated preemptor pod object: %v\", err) return \"\", err } GetUpdatedPodçš„å®ç°å°±æ˜¯å»æ‹¿podçš„å¯¹è±¡ã€‚\nfunc (p *podPreemptor) GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) { return p.Client.CoreV1().Pods(pod.Namespace).Get(pod.Name, metav1.GetOptions{}) } æ¥ç€æ‰§è¡ŒæŠ¢å çš„ç®—æ³•ã€‚æŠ¢å çš„ç®—æ³•è¿”å›é¢„è°ƒåº¦èŠ‚ç‚¹çš„ä¿¡æ¯å’Œå› æŠ¢å è¢«å‰”é™¤çš„podçš„ä¿¡æ¯ã€‚å…·ä½“çš„æŠ¢å ç®—æ³•é€»è¾‘ä¸‹æ–‡åˆ†æã€‚\nnode, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) å°†é¢„è°ƒåº¦èŠ‚ç‚¹çš„ä¿¡æ¯æ›´æ–°åˆ°podçš„Status.NominatedNodeNameå±æ€§ä¸­ã€‚\nerr = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) SetNominatedNodeNameçš„å…·ä½“å®ç°ä¸ºï¼š\nfunc (p *podPreemptor) SetNominatedNodeName(pod *v1.Pod, nominatedNodeName string) error { podCopy := pod.DeepCopy() podCopy.Status.NominatedNodeName = nominatedNodeName _, err := p.Client.CoreV1().Pods(pod.Namespace).UpdateStatus(podCopy) return err } æ¥ç€åˆ é™¤å› æŠ¢å è€Œéœ€è¦è¢«å‰”é™¤çš„podã€‚\nerr := sched.config.PodPreemptor.DeletePod(victim) PodPreemptor.DeletePodçš„å…·ä½“å®ç°å°±æ˜¯åˆ é™¤å…·ä½“çš„podã€‚\nfunc (p *podPreemptor) DeletePod(pod *v1.Pod) error { return p.Client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, \u0026metav1.DeleteOptions{}) } å¦‚æœæŠ¢å ç®—æ³•å¾—å‡ºçš„nodeå¯¹è±¡ä¸ºnilï¼Œåˆ™å°†podçš„Status.NominatedNodeNameå±æ€§è®¾ç½®ä¸ºç©ºã€‚\n// Clearing nominated pods should happen outside of \"if node != nil\". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { glog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } RemoveNominatedNodeNameçš„å…·ä½“å®ç°å¦‚ä¸‹ï¼š\nfunc (p *podPreemptor) RemoveNominatedNodeName(pod *v1.Pod) error { if len(pod.Status.NominatedNodeName) == 0 { return nil } return p.SetNominatedNodeName(pod, \"\") } 2.2. NominatedNodeName Pod.Status.NominatedNodeNameçš„è¯´æ˜ï¼š\nnominatedNodeNameæ˜¯è°ƒåº¦å¤±è´¥çš„podæŠ¢å åˆ«çš„podçš„æ—¶å€™ï¼Œè¢«æŠ¢å podçš„è¿è¡ŒèŠ‚ç‚¹ã€‚ä½†åœ¨å‰”é™¤è¢«æŠ¢å podä¹‹å‰è¯¥è°ƒåº¦å¤±è´¥çš„podä¸ä¼šè¢«è°ƒåº¦ã€‚åŒæ—¶ä¹Ÿä¸ä¿è¯æœ€ç»ˆè¯¥podä¸€å®šä¼šè°ƒåº¦åˆ°nominatedNodeNameçš„æœºå™¨ä¸Šï¼Œä¹Ÿå¯èƒ½å› ä¸ºä¹‹åèµ„æºå……è¶³ç­‰åŸå› è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ä¸Šã€‚æœ€ç»ˆè¯¥podä¼šè¢«åŠ åˆ°è°ƒåº¦çš„é˜Ÿåˆ—ä¸­ã€‚\nå…¶ä¸­åŠ å…¥åˆ°è°ƒåº¦é˜Ÿåˆ—çš„å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š\nfunc NewConfigFactory(args *ConfigFactoryArgs) scheduler.Configurator { ... // unscheduled pod queue args.PodInformer.Informer().AddEventHandler( ... Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToSchedulingQueue, UpdateFunc: c.updatePodInSchedulingQueue, DeleteFunc: c.deletePodFromSchedulingQueue, }, }, ) ... } addPodToSchedulingQueue:\nfunc (c *configFactory) addPodToSchedulingQueue(obj interface{}) { if err := c.podQueue.Add(obj.(*v1.Pod)); err != nil { runtime.HandleError(fmt.Errorf(\"unable to queue %T: %v\", obj, err)) } } PriorityQueue.Add:\n// Add adds a pod to the active queue. It should be called only when a new pod // is added so there is no chance the pod is already in either queue. func (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() err := p.activeQ.Add(pod) if err != nil { glog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { if p.unschedulableQ.get(pod) != nil { glog.Errorf(\"Error: pod %v/%v is already in the unschedulable queue.\", pod.Namespace, pod.Name) p.deleteNominatedPodIfExists(pod) p.unschedulableQ.delete(pod) } p.addNominatedPodIfNeeded(pod) p.cond.Broadcast() } return err } addNominatedPodIfNeeded:\n// addNominatedPodIfNeeded adds a pod to nominatedPods if it has a NominatedNodeName and it does not // already exist in the map. Adding an existing pod is not going to update the pod. func (p *PriorityQueue) addNominatedPodIfNeeded(pod *v1.Pod) { nnn := NominatedNodeName(pod) if len(nnn) \u003e 0 { for _, np := range p.nominatedPods[nnn] { if np.UID == pod.UID { glog.Errorf(\"Pod %v/%v already exists in the nominated map!\", pod.Namespace, pod.Name) return } } p.nominatedPods[nnn] = append(p.nominatedPods[nnn], pod) } } NominatedNodeName:\n// NominatedNodeName returns nominated node name of a Pod. func NominatedNodeName(pod *v1.Pod) string { return pod.Status.NominatedNodeName } 3. genericScheduler.Preempt æŠ¢å ç®—æ³•ä¾ç„¶æ˜¯åœ¨ScheduleAlgorithmæ¥å£ä¸­å®šä¹‰ã€‚\n// ScheduleAlgorithm is an interface implemented by things that know how to schedule pods // onto machines. type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt receives scheduling errors for a pod and tries to create room for // the pod by preempting lower priority pods if possible. // It returns the node where preemption happened, a list of preempted pods, a // list of pods whose nominated node name should be removed, and error if any. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) // Predicates() returns a pointer to a map of predicate functions. This is // exposed for testing. Predicates() map[string]FitPredicate // Prioritizers returns a slice of priority config. This is exposed for // testing. Prioritizers() []PriorityConfig } Preemptçš„å…·ä½“å®ç°ä¸ºgenericSchedulerç»“æ„ä½“ã€‚\nPreemptçš„ä¸»è¦å®ç°æ˜¯æ‰¾åˆ°å¯ä»¥è°ƒåº¦çš„èŠ‚ç‚¹å’Œä¸Šé¢å› æŠ¢å è€Œéœ€è¦è¢«å‰”é™¤çš„podã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\næ ¹æ®è°ƒåº¦å¤±è´¥çš„åŸå› å¯¹æ‰€æœ‰èŠ‚ç‚¹å…ˆè¿›è¡Œä¸€æ‰¹ç­›é€‰ï¼Œç­›é€‰å‡ºæ½œåœ¨çš„è¢«è°ƒåº¦èŠ‚ç‚¹åˆ—è¡¨ã€‚ é€šè¿‡selectNodesForPreemptionç­›é€‰å‡ºéœ€è¦ç‰ºç‰²çš„podå’Œå…¶èŠ‚ç‚¹ã€‚ åŸºäºæ‹“å±•æŠ¢å é€»è¾‘å†æ¬¡å¯¹ä¸Šè¿°ç­›é€‰å‡ºæ¥çš„ç‰ºç‰²è€…åšè¿‡æ»¤ã€‚ åŸºäºä¸Šè¿°çš„è¿‡æ»¤ç»“æœï¼Œé€‰æ‹©ä¸€ä¸ªæœ€ç»ˆå¯èƒ½å› æŠ¢å è¢«è°ƒåº¦çš„èŠ‚ç‚¹ã€‚ åŸºäºä¸Šè¿°çš„å€™é€‰èŠ‚ç‚¹ï¼Œæ‰¾å‡ºè¯¥èŠ‚ç‚¹ä¸Šä¼˜å…ˆçº§ä½äºå½“å‰è¢«è°ƒåº¦podçš„ç‰ºç‰²è€…podåˆ—è¡¨ã€‚ å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\n// preempt finds nodes with pods that can be preempted to make room for \"pod\" to // schedule. It chooses one of the nodes and preempts the pods on the node and // returns 1) the node, 2) the list of preempted pods if such a node is found, // 3) A list of pods whose nominated node name should be cleared, and 4) any // possible error. func (g *genericScheduler) Preempt(pod *v1.Pod, nodeLister algorithm.NodeLister, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) { // Scheduler may return various types of errors. Consider preemption only if // the error is of type FitError. fitError, ok := scheduleErr.(*FitError) if !ok || fitError == nil { return nil, nil, nil, nil } err := g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return nil, nil, nil, err } if !podEligibleToPreemptOthers(pod, g.cachedNodeInfoMap) { glog.V(5).Infof(\"Pod %v/%v is not eligible for more preemption.\", pod.Namespace, pod.Name) return nil, nil, nil, nil } allNodes, err := nodeLister.List() if err != nil { return nil, nil, nil, err } if len(allNodes) == 0 { return nil, nil, nil, ErrNoNodesAvailable } potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError.FailedPredicates) if len(potentialNodes) == 0 { glog.V(3).Infof(\"Preemption will not help schedule pod %v/%v on any node.\", pod.Namespace, pod.Name) // In this case, we should clean-up any existing nominated node name of the pod. return nil, nil, []*v1.Pod{pod}, nil } pdbs, err := g.cache.ListPDBs(labels.Everything()) if err != nil { return nil, nil, nil, err } // æ‰¾å‡ºå¯èƒ½è¢«æŠ¢å çš„èŠ‚ç‚¹ nodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil { return nil, nil, nil, err } // We will only check nodeToVictims with extenders that support preemption. // Extenders which do not support preemption may later prevent preemptor from being scheduled on the nominated // node. In that case, scheduler will find a different host for the preemptor in subsequent scheduling cycles. nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } // é€‰å‡ºæœ€ç»ˆè¢«æŠ¢å çš„èŠ‚ç‚¹ candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } // Lower priority pods nominated to run on this node, may no longer fit on // this node. So, we should remove their nomination. Removing their // nomination updates these pods and moves them to the active queue. It // lets scheduler find another place for them. // æ‰¾å‡ºè¢«å¼ºå èŠ‚ç‚¹ä¸Šç‰ºç‰²è€…podåˆ—è¡¨ nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } return nil, nil, nil, fmt.Errorf( \"preemption failed: the target node %s has been deleted from scheduler cache\", candidateNode.Name) } ä»¥ä¸‹å¯¹genericScheduler.Preemptåˆ†æ®µè¿›è¡Œåˆ†æã€‚\n3.1. selectNodesForPreemption selectNodesForPreemptionå¹¶è¡Œåœ°æ‰€æœ‰èŠ‚ç‚¹ä¸­æ‰¾å¯èƒ½è¢«æŠ¢å çš„èŠ‚ç‚¹ã€‚\nnodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates,g.predicateMetaProducer, g.schedulingQueue, pdbs) selectNodesForPreemptionä¸»è¦åŸºäºselectVictimsOnNodeæ„é€ ä¸€ä¸ªcheckNodeçš„å‡½æ•°ï¼Œç„¶åå¹¶å‘æ‰§è¡Œè¯¥å‡½æ•°ã€‚\nselectNodesForPreemptionå…·ä½“å®ç°å¦‚ä¸‹ï¼š\n// selectNodesForPreemption finds all the nodes with possible victims for // preemption in parallel. func selectNodesForPreemption(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, potentialNodes []*v1.Node, predicates map[string]algorithm.FitPredicate, metadataProducer algorithm.PredicateMetadataProducer, queue SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) (map[*v1.Node]*schedulerapi.Victims, error) { nodeToVictims := map[*v1.Node]*schedulerapi.Victims{} var resultLock sync.Mutex // We can use the same metadata producer for all nodes. meta := metadataProducer(pod, nodeNameToInfo) checkNode := func(i int) { nodeName := potentialNodes[i].Name var metaCopy algorithm.PredicateMetadata if meta != nil { metaCopy = meta.ShallowCopy() } pods, numPDBViolations, fits := selectVictimsOnNode(pod, metaCopy, nodeNameToInfo[nodeName], predicates, queue, pdbs) if fits { resultLock.Lock() victims := schedulerapi.Victims{ Pods: pods, NumPDBViolations: numPDBViolations, } nodeToVictims[potentialNodes[i]] = \u0026victims resultLock.Unlock() } } workqueue.Parallelize(16, len(potentialNodes), checkNode) return nodeToVictims, nil } 3.1.1. selectVictimsOnNode selectVictimsOnNodeæ‰¾åˆ°åº”è¯¥è¢«æŠ¢å çš„ç»™å®šèŠ‚ç‚¹ä¸Šçš„æœ€å°podé›†åˆï¼Œä»¥ä¾¿ç»™è°ƒåº¦å¤±è´¥çš„podå®‰æ’è¶³å¤Ÿçš„ç©ºé—´ã€‚è¯¥å‡½æ•°æœ€ç»ˆè¿”å›çš„æ˜¯ä¸€ä¸ªpodçš„æ•°ç»„ã€‚å½“æœ‰æ›´ä½ä¼˜å…ˆçº§çš„podå¯èƒ½è¢«é€‰æ‹©çš„æ—¶å€™ï¼Œè¾ƒé«˜ä¼˜å…ˆçº§çš„podä¸ä¼šè¢«é€‰å…¥è¯¥å¾…å‰”é™¤çš„podé›†åˆã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nå…ˆæ£€æŸ¥å½“è¯¥èŠ‚ç‚¹ä¸Šæ‰€æœ‰ä½äºé¢„è¢«è°ƒåº¦podä¼˜å…ˆçº§çš„podç§»é™¤åï¼Œè¯¥podèƒ½å¦è¢«è°ƒåº¦åˆ°å½“å‰èŠ‚ç‚¹ä¸Šã€‚ å¦‚æœä¸Šè¿°æ£€æŸ¥å¯ä»¥ï¼Œåˆ™å°†è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰ä½ä¼˜å…ˆçº§podæŒ‰ç…§ä¼˜å…ˆçº§æ¥æ’åºã€‚ // selectVictimsOnNode finds minimum set of pods on the given node that should // be preempted in order to make enough room for \"pod\" to be scheduled. The // minimum set selected is subject to the constraint that a higher-priority pod // is never preempted when a lower-priority pod could be (higher/lower relative // to one another, not relative to the preemptor \"pod\"). // The algorithm first checks if the pod can be scheduled on the node when all the // lower priority pods are gone. If so, it sorts all the lower priority pods by // their priority and then puts them into two groups of those whose PodDisruptionBudget // will be violated if preempted and other non-violating pods. Both groups are // sorted by priority. It first tries to reprieve as many PDB violating pods as // possible and then does them same for non-PDB-violating pods while checking // that the \"pod\" can still fit on the node. // NOTE: This function assumes that it is never called if \"pod\" cannot be scheduled // due to pod affinity, node affinity, or node anti-affinity reasons. None of // these predicates can be satisfied by removing more pods from the node. func selectVictimsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, fitPredicates map[string]algorithm.FitPredicate, queue SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) ([]*v1.Pod, int, bool) { potentialVictims := util.SortableList{CompFunc: util.HigherPriorityPod} nodeInfoCopy := nodeInfo.Clone() removePod := func(rp *v1.Pod) { nodeInfoCopy.RemovePod(rp) if meta != nil { meta.RemovePod(rp) } } addPod := func(ap *v1.Pod) { nodeInfoCopy.AddPod(ap) if meta != nil { meta.AddPod(ap, nodeInfoCopy) } } // As the first step, remove all the lower priority pods from the node and // check if the given pod can be scheduled. podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() { if util.GetPodPriority(p) \u003c podPriority { potentialVictims.Items = append(potentialVictims.Items, p) removePod(p) } } potentialVictims.Sort() // If the new pod does not fit after removing all the lower priority pods, // we are almost done and this node is not suitable for preemption. The only condition // that we should check is if the \"pod\" is failing to schedule due to pod affinity // failure. // TODO(bsalamat): Consider checking affinity to lower priority pods if feasible with reasonable performance. if fits, _, err := podFitsOnNode(pod, meta, nodeInfoCopy, fitPredicates, nil, nil, queue, false, nil); !fits { if err != nil { glog.Warningf(\"Encountered error while selecting victims on node %v: %v\", nodeInfo.Node().Name, err) } return nil, 0, false } var victims []*v1.Pod numViolatingVictim := 0 // Try to reprieve as many pods as possible. We first try to reprieve the PDB // violating victims and then other non-violating ones. In both cases, we start // from the highest priority victims. violatingVictims, nonViolatingVictims := filterPodsWithPDBViolation(potentialVictims.Items, pdbs) reprievePod := func(p *v1.Pod) bool { addPod(p) fits, _, _ := podFitsOnNode(pod, meta, nodeInfoCopy, fitPredicates, nil, nil, queue, false, nil) if !fits { removePod(p) victims = append(victims, p) glog.V(5).Infof(\"Pod %v is a potential preemption victim on node %v.\", p.Name, nodeInfo.Node().Name) } return fits } for _, p := range violatingVictims { if !reprievePod(p) { numViolatingVictim++ } } // Now we try to reprieve non-violating victims. for _, p := range nonViolatingVictims { reprievePod(p) } return victims, numViolatingVictim, true } 3.2. processPreemptionWithExtenders processPreemptionWithExtendersåŸºäºselectNodesForPreemptioné€‰å‡ºçš„ç‰ºç‰²è€…è¿›è¡Œæ‰©å±•çš„æŠ¢å é€»è¾‘ç»§ç»­ç­›é€‰ç‰ºç‰²è€…ã€‚\n// We will only check nodeToVictims with extenders that support preemption. // Extenders which do not support preemption may later prevent preemptor from being scheduled on the nominated // node. In that case, scheduler will find a different host for the preemptor in subsequent scheduling cycles. nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } processPreemptionWithExtenderså®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// processPreemptionWithExtenders processes preemption with extenders func (g *genericScheduler) processPreemptionWithExtenders( pod *v1.Pod, nodeToVictims map[*v1.Node]*schedulerapi.Victims, ) (map[*v1.Node]*schedulerapi.Victims, error) { if len(nodeToVictims) \u003e 0 { for _, extender := range g.extenders { if extender.SupportsPreemption() \u0026\u0026 extender.IsInterested(pod) { newNodeToVictims, err := extender.ProcessPreemption( pod, nodeToVictims, g.cachedNodeInfoMap, ) if err != nil { if extender.IsIgnorable() { glog.Warningf(\"Skipping extender %v as it returned error %v and has ignorable flag set\", extender, err) continue } return nil, err } // Replace nodeToVictims with new result after preemption. So the // rest of extenders can continue use it as parameter. nodeToVictims = newNodeToVictims // If node list becomes empty, no preemption can happen regardless of other extenders. if len(nodeToVictims) == 0 { break } } } } return nodeToVictims, nil } 3.3. pickOneNodeForPreemption pickOneNodeForPreemptionä»ç­›é€‰å‡ºçš„nodeä¸­å†æŒ‘é€‰ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºæœ€ç»ˆè°ƒåº¦èŠ‚ç‚¹ã€‚\ncandidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } pickOneNodeForPreemptionå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š\n// pickOneNodeForPreemption chooses one node among the given nodes. It assumes // pods in each map entry are ordered by decreasing priority. // It picks a node based on the following criteria: // 1. A node with minimum number of PDB violations. // 2. A node with minimum highest priority victim is picked. // 3. Ties are broken by sum of priorities of all victims. // 4. If there are still ties, node with the minimum number of victims is picked. // 5. If there are still ties, the first such node is picked (sort of randomly). // The 'minNodes1' and 'minNodes2' are being reused here to save the memory // allocation and garbage collection time. func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node { if len(nodesToVictims) == 0 { return nil } minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 for node, victims := range nodesToVictims { if len(victims.Pods) == 0 { // We found a node that doesn't need any preemption. Return it! // This should happen rarely when one or more pods are terminated between // the time that scheduler tries to schedule the pod and the time that // preemption logic tries to find nodes for preemption. return node } numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods \u003c minNumPDBViolatingPods { minNumPDBViolatingPods = numPDBViolatingPods minNodes1 = nil lenNodes1 = 0 } if numPDBViolatingPods == minNumPDBViolatingPods { minNodes1 = append(minNodes1, node) lenNodes1++ } } if lenNodes1 == 1 { return minNodes1[0] } // There are more than one node with minimum number PDB violating pods. Find // the one with minimum highest priority victim. minHighestPriority := int32(math.MaxInt32) var minNodes2 = make([]*v1.Node, lenNodes1) lenNodes2 := 0 for i := 0; i \u003c lenNodes1; i++ { node := minNodes1[i] victims := nodesToVictims[node] // highestPodPriority is the highest priority among the victims on this node. highestPodPriority := util.GetPodPriority(victims.Pods[0]) if highestPodPriority \u003c minHighestPriority { minHighestPriority = highestPodPriority lenNodes2 = 0 } if highestPodPriority == minHighestPriority { minNodes2[lenNodes2] = node lenNodes2++ } } if lenNodes2 == 1 { return minNodes2[0] } // There are a few nodes with minimum highest priority victim. Find the // smallest sum of priorities. minSumPriorities := int64(math.MaxInt64) lenNodes1 = 0 for i := 0; i \u003c lenNodes2; i++ { var sumPriorities int64 node := minNodes2[i] for _, pod := range nodesToVictims[node].Pods { // We add MaxInt32+1 to all priorities to make all of them \u003e= 0. This is // needed so that a node with a few pods with negative priority is not // picked over a node with a smaller number of pods with the same negative // priority (and similar scenarios). sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) } if sumPriorities \u003c minSumPriorities { minSumPriorities = sumPriorities lenNodes1 = 0 } if sumPriorities == minSumPriorities { minNodes1[lenNodes1] = node lenNodes1++ } } if lenNodes1 == 1 { return minNodes1[0] } // There are a few nodes with minimum highest priority victim and sum of priorities. // Find one with the minimum number of pods. minNumPods := math.MaxInt32 lenNodes2 = 0 for i := 0; i \u003c lenNodes1; i++ { node := minNodes1[i] numPods := len(nodesToVictims[node].Pods) if numPods \u003c minNumPods { minNumPods = numPods lenNodes2 = 0 } if numPods == minNumPods { minNodes2[lenNodes2] = node lenNodes2++ } } // At this point, even if there are more than one node with the same score, // return the first one. if lenNodes2 \u003e 0 { return minNodes2[0] } glog.Errorf(\"Error in logic of node scoring for preemption. We should never reach here!\") return nil } 3.4. getLowerPriorityNominatedPods getLowerPriorityNominatedPodsçš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nè·å–å€™é€‰èŠ‚ç‚¹ä¸Šçš„podåˆ—è¡¨ã€‚ è·å–å¾…è°ƒåº¦podçš„ä¼˜å…ˆçº§å€¼ã€‚ éå†è¯¥èŠ‚ç‚¹çš„podåˆ—è¡¨ï¼Œå¦‚æœä½äºå¾…è°ƒåº¦podçš„ä¼˜å…ˆçº§åˆ™æ”¾å…¥ä½ä¼˜å…ˆçº§podåˆ—è¡¨ä¸­ã€‚ genericScheduler.Preemptä¸­ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š\n// Lower priority pods nominated to run on this node, may no longer fit on // this node. So, we should remove their nomination. Removing their // nomination updates these pods and moves them to the active queue. It // lets scheduler find another place for them. nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } getLowerPriorityNominatedPodsä»£ç å¦‚ä¸‹ï¼š\næ­¤éƒ¨åˆ†ä»£ç ä½äºpkg/scheduler/core/generic_scheduler.go\n// getLowerPriorityNominatedPods returns pods whose priority is smaller than the // priority of the given \"pod\" and are nominated to run on the given node. // Note: We could possibly check if the nominated lower priority pods still fit // and return those that no longer fit, but that would require lots of // manipulation of NodeInfo and PredicateMeta per nominated pod. It may not be // worth the complexity, especially because we generally expect to have a very // small number of nominated pods per node. func (g *genericScheduler) getLowerPriorityNominatedPods(pod *v1.Pod, nodeName string) []*v1.Pod { pods := g.schedulingQueue.WaitingPodsForNode(nodeName) if len(pods) == 0 { return nil } var lowerPriorityPods []*v1.Pod podPriority := util.GetPodPriority(pod) for _, p := range pods { if util.GetPodPriority(p) \u003c podPriority { lowerPriorityPods = append(lowerPriorityPods, p) } } return lowerPriorityPods } 4. æ€»ç»“ 4.1. Scheduler.preempt å½“podè°ƒåº¦å¤±è´¥çš„æ—¶å€™ï¼Œä¼šæŠ¢å ä½ä¼˜å…ˆçº§podçš„ç©ºé—´æ¥ç»™é«˜ä¼˜å…ˆçº§çš„podã€‚å…¶ä¸­å…¥å‚ä¸ºè°ƒåº¦å¤±è´¥çš„podå¯¹è±¡å’Œè°ƒåº¦å¤±è´¥çš„errã€‚\næŠ¢å çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\nåˆ¤æ–­æ˜¯å¦æœ‰å…³é—­æŠ¢å æœºåˆ¶ï¼Œå¦‚æœå…³é—­æŠ¢å æœºåˆ¶åˆ™ç›´æ¥è¿”å›ã€‚ è·å–è°ƒåº¦å¤±è´¥podçš„æœ€æ–°å¯¹è±¡æ•°æ®ã€‚ æ‰§è¡ŒæŠ¢å ç®—æ³•Algorithm.Preemptï¼Œè¿”å›é¢„è°ƒåº¦èŠ‚ç‚¹å’Œéœ€è¦è¢«å‰”é™¤çš„podåˆ—è¡¨ã€‚ å°†æŠ¢å ç®—æ³•è¿”å›çš„nodeæ·»åŠ åˆ°podçš„Status.NominatedNodeNameä¸­ï¼Œå¹¶åˆ é™¤éœ€è¦è¢«å‰”é™¤çš„podã€‚ å½“æŠ¢å ç®—æ³•è¿”å›çš„nodeæ˜¯nilçš„æ—¶å€™ï¼Œæ¸…é™¤podçš„Status.NominatedNodeNameä¿¡æ¯ã€‚ æ•´ä¸ªæŠ¢å æµç¨‹çš„æœ€ç»ˆç»“æœå®é™…ä¸Šæ˜¯æ›´æ–°Pod.Status.NominatedNodeNameå±æ€§çš„ä¿¡æ¯ã€‚å¦‚æœæŠ¢å ç®—æ³•è¿”å›çš„èŠ‚ç‚¹ä¸ä¸ºç©ºï¼Œåˆ™å°†è¯¥nodeæ›´æ–°åˆ°Pod.Status.NominatedNodeNameä¸­ï¼Œå¦åˆ™å°±å°†Pod.Status.NominatedNodeNameè®¾ç½®ä¸ºç©ºã€‚\n4.2. genericScheduler.Preempt Preemptçš„ä¸»è¦å®ç°æ˜¯æ‰¾åˆ°å¯ä»¥è°ƒåº¦çš„èŠ‚ç‚¹å’Œä¸Šé¢å› æŠ¢å è€Œéœ€è¦è¢«å‰”é™¤çš„podã€‚\nåŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š\næ ¹æ®è°ƒåº¦å¤±è´¥çš„åŸå› å¯¹æ‰€æœ‰èŠ‚ç‚¹å…ˆè¿›è¡Œä¸€æ‰¹ç­›é€‰ï¼Œç­›é€‰å‡ºæ½œåœ¨çš„è¢«è°ƒåº¦èŠ‚ç‚¹åˆ—è¡¨ã€‚ é€šè¿‡selectNodesForPreemptionç­›é€‰å‡ºéœ€è¦ç‰ºç‰²çš„podå’Œå…¶èŠ‚ç‚¹ã€‚ åŸºäºæ‹“å±•æŠ¢å é€»è¾‘å†æ¬¡å¯¹ä¸Šè¿°ç­›é€‰å‡ºæ¥çš„ç‰ºç‰²è€…åšè¿‡æ»¤ã€‚ åŸºäºä¸Šè¿°çš„è¿‡æ»¤ç»“æœï¼Œé€‰æ‹©ä¸€ä¸ªæœ€ç»ˆå¯èƒ½å› æŠ¢å è¢«è°ƒåº¦çš„èŠ‚ç‚¹ã€‚ åŸºäºä¸Šè¿°çš„å€™é€‰èŠ‚ç‚¹ï¼Œæ‰¾å‡ºè¯¥èŠ‚ç‚¹ä¸Šä¼˜å…ˆçº§ä½äºå½“å‰è¢«è°ƒåº¦podçš„ç‰ºç‰²è€…podåˆ—è¡¨ã€‚ å‚è€ƒï¼š\nhttps://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go ","categories":"","description":"","excerpt":" ä»¥ä¸‹ä»£ç åˆ†æåŸºäº kubernetes v1.12.0 ç‰ˆæœ¬ã€‚\næœ¬æ–‡ä¸»è¦åˆ†æè°ƒåº¦ä¸­çš„æŠ¢å é€»è¾‘ï¼Œå½“podä¸é€‚åˆä»»ä½•èŠ‚ç‚¹çš„æ—¶å€™ï¼Œå¯èƒ½podä¼šè°ƒ â€¦","ref":"/k8s-source-code-analysis/kube-scheduler/preempt/","tags":["æºç åˆ†æ"],"title":"kube-scheduleræºç åˆ†æï¼ˆå…­ï¼‰ä¹‹ æŠ¢å é€»è¾‘"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/principle/","tags":"","title":"åŸç†ç¯‡"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/resource/","tags":"","title":"èµ„æºéš”ç¦»"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/mysql/","tags":"","title":"Mysql"},{"body":"1. å‡½æ•°å®šä¹‰ å‡½æ•°å¯ä»¥è®©æˆ‘ä»¬å°†ä¸€ä¸ªå¤æ‚åŠŸèƒ½åˆ’åˆ†æˆè‹¥å¹²æ¨¡å—ï¼Œè®©ç¨‹åºç»“æ„æ›´åŠ æ¸…æ™°ï¼Œä»£ç é‡å¤åˆ©ç”¨ç‡æ›´é«˜ã€‚Shell ä¹Ÿæ”¯æŒå‡½æ•°ã€‚Shell å‡½æ•°å¿…é¡»å…ˆå®šä¹‰åä½¿ç”¨ã€‚\nShell å‡½æ•°çš„å®šä¹‰æ ¼å¼å¦‚ä¸‹ï¼š\nfunction_name () { list of commands [ return value ] } ä¹Ÿå¯ä»¥åœ¨å‡½æ•°åå‰åŠ ä¸Šå…³é”®å­— functionï¼š\nfunction function_name () { list of commands [ return value ] } 2. å‡½æ•°è¿”å›å€¼ å‡½æ•°è¿”å›å€¼ï¼Œå¯ä»¥æ˜¾å¼å¢åŠ returnè¯­å¥ï¼›å¦‚æœä¸åŠ ï¼Œä¼šå°†æœ€åä¸€æ¡å‘½ä»¤è¿è¡Œç»“æœä½œä¸ºè¿”å›å€¼ã€‚\nShell å‡½æ•°è¿”å›å€¼åªèƒ½æ˜¯æ•´æ•°ï¼Œä¸€èˆ¬ç”¨æ¥è¡¨ç¤ºå‡½æ•°æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œ0è¡¨ç¤ºæˆåŠŸï¼Œå…¶ä»–å€¼è¡¨ç¤ºå¤±è´¥ã€‚\nå¦‚æœ return å…¶ä»–æ•°æ®ï¼Œæ¯”å¦‚ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¾€å¾€ä¼šå¾—åˆ°é”™è¯¯æç¤ºï¼šâ€œnumeric argument requiredâ€ã€‚\nå¦‚æœä¸€å®šè¦è®©å‡½æ•°è¿”å›å­—ç¬¦ä¸²ï¼Œé‚£ä¹ˆå¯ä»¥å…ˆå®šä¹‰ä¸€ä¸ªå˜é‡ï¼Œç”¨æ¥æ¥æ”¶å‡½æ•°çš„è®¡ç®—ç»“æœï¼Œè„šæœ¬åœ¨éœ€è¦çš„æ—¶å€™è®¿é—®è¿™ä¸ªå˜é‡æ¥è·å¾—å‡½æ•°è¿”å›å€¼ã€‚å‡½æ•°è¿”å›å€¼åœ¨è°ƒç”¨è¯¥å‡½æ•°åé€šè¿‡ $?ã€$?è¡¨ç¤ºä¸Šä¸ªå‘½ä»¤çš„é€€å‡ºçŠ¶æ€ï¼Œæˆ–å‡½æ•°çš„è¿”å›å€¼ã€‚ã€‘ æ¥è·å¾—ã€‚\n3. å‡½æ•°è°ƒç”¨ è°ƒç”¨å‡½æ•°åªéœ€è¦ç»™å‡ºå‡½æ•°åï¼Œä¸éœ€è¦åŠ æ‹¬å·ã€‚\nåƒåˆ é™¤å˜é‡ä¸€æ ·ï¼Œåˆ é™¤å‡½æ•°ä¹Ÿå¯ä»¥ä½¿ç”¨ unset ä¸è¿‡è¦åŠ ä¸Š .f é€‰é¡¹\n$unset .f function_name å¦‚æœä½ å¸Œæœ›ç›´æ¥ä»ç»ˆç«¯è°ƒç”¨å‡½æ•°ï¼Œå¯ä»¥å°†å‡½æ•°å®šä¹‰åœ¨ä¸»ç›®å½•ä¸‹çš„ .profile æ–‡ä»¶ï¼Œè¿™æ ·æ¯æ¬¡ç™»å½•åï¼Œåœ¨å‘½ä»¤æç¤ºç¬¦åé¢è¾“å…¥å‡½æ•°åå­—å°±å¯ä»¥ç«‹å³è°ƒç”¨ã€‚\nç¤ºä¾‹ï¼š\n#!/bin/bash # Define your function here Hello () { echo \"Url is http://see.xidian.edu.cn/cpp/shell/\" } # Invoke your function Hello è¿è¡Œç»“æœï¼š\n$./test.sh Hello World $ 4. å‡½æ•°å‚æ•° åœ¨Shellä¸­ï¼Œè°ƒç”¨å‡½æ•°æ—¶å¯ä»¥å‘å…¶ä¼ é€’å‚æ•°ã€‚åœ¨å‡½æ•°ä½“å†…éƒ¨ï¼Œé€šè¿‡ $n çš„å½¢å¼æ¥è·å–å‚æ•°çš„å€¼ï¼Œä¾‹å¦‚ï¼Œ$1è¡¨ç¤ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œ$2è¡¨ç¤ºç¬¬äºŒä¸ªå‚æ•°...\næ³¨æ„ï¼Œ$10 ä¸èƒ½è·å–ç¬¬åä¸ªå‚æ•°ï¼Œè·å–ç¬¬åä¸ªå‚æ•°éœ€è¦${10}ã€‚å½“n\u003e=10æ—¶ï¼Œéœ€è¦ä½¿ç”¨${n}æ¥è·å–å‚æ•°ã€‚\nç‰¹æ®Šå˜é‡ç”¨æ¥å¤„ç†å‚æ•°:\nç‰¹æ®Šå˜é‡ è¯´æ˜ $# ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°ä¸ªæ•°ã€‚ $* æ˜¾ç¤ºæ‰€æœ‰ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°ã€‚ $@ ä¸$*ç›¸åŒï¼Œä½†æ˜¯ç•¥æœ‰åŒºåˆ«ï¼Œè¯·æŸ¥çœ‹Shellç‰¹æ®Šå˜é‡ã€‚ $? å‡½æ•°çš„è¿”å›å€¼ã€‚ å¸¦å‚æ•°çš„å‡½æ•°ç¤ºä¾‹ï¼š\n#!/bin/bash funWithParam(){ echo \"The value of the first parameter is $1 !\" echo \"The value of the second parameter is $2 !\" echo \"The value of the tenth parameter is $10 !\" echo \"The value of the tenth parameter is ${10} !\" echo \"The value of the eleventh parameter is ${11} !\" echo \"The amount of the parameters is $# !\" # å‚æ•°ä¸ªæ•° echo \"The string of the parameters is $* !\" # ä¼ é€’ç»™å‡½æ•°çš„æ‰€æœ‰å‚æ•° } funWithParam 1 2 3 4 5 6 7 8 9 34 73 è¿è¡Œè„šæœ¬ï¼š\nThe value of the first parameter is 1 ! The value of the second parameter is 2 ! The value of the tenth parameter is 10 ! The value of the tenth parameter is 34 ! The value of the eleventh parameter is 73 ! The amount of the parameters is 12 ! The string of the parameters is 1 2 3 4 5 6 7 8 9 34 73 !\" å‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"1. å‡½æ•°å®šä¹‰ å‡½æ•°å¯ä»¥è®©æˆ‘ä»¬å°†ä¸€ä¸ªå¤æ‚åŠŸèƒ½åˆ’åˆ†æˆè‹¥å¹²æ¨¡å—ï¼Œè®©ç¨‹åºç»“æ„æ›´åŠ æ¸…æ™°ï¼Œä»£ç é‡å¤åˆ©ç”¨ç‡æ›´é«˜ã€‚Shell ä¹Ÿæ”¯æŒå‡½æ•°ã€‚Shell å‡½æ•° â€¦","ref":"/linux-notes/shell/shell-function/","tags":["Shell"],"title":"Shell å‡½æ•°"},{"body":"Unix å‘½ä»¤é»˜è®¤ä»æ ‡å‡†è¾“å…¥è®¾å¤‡(stdin)è·å–è¾“å…¥ï¼Œå°†ç»“æœè¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºè®¾å¤‡(stdout)æ˜¾ç¤ºã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ ‡å‡†è¾“å…¥è®¾å¤‡å°±æ˜¯é”®ç›˜ï¼Œæ ‡å‡†è¾“å‡ºè®¾å¤‡å°±æ˜¯ç»ˆç«¯ï¼Œå³æ˜¾ç¤ºå™¨ã€‚\n1. è¾“å‡ºé‡å®šå‘ å‘½ä»¤çš„è¾“å‡ºä¸ä»…å¯ä»¥æ˜¯æ˜¾ç¤ºå™¨ï¼Œè¿˜å¯ä»¥å¾ˆå®¹æ˜“çš„è½¬ç§»å‘åˆ°æ–‡ä»¶ï¼Œè¿™è¢«ç§°ä¸ºè¾“å‡ºé‡å®šå‘ã€‚\nå‘½ä»¤è¾“å‡ºé‡å®šå‘çš„è¯­æ³•ä¸ºï¼š\n$ command \u003e file è¿™æ ·ï¼Œè¾“å‡ºåˆ°æ˜¾ç¤ºå™¨çš„å†…å®¹å°±å¯ä»¥è¢«é‡å®šå‘åˆ°æ–‡ä»¶ã€‚\nè¾“å‡ºé‡å®šå‘ä¼šè¦†ç›–æ–‡ä»¶å†…å®¹ï¼›å¦‚æœä¸å¸Œæœ›æ–‡ä»¶å†…å®¹è¢«è¦†ç›–ï¼Œå¯ä»¥ä½¿ç”¨ \u003e\u003e è¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾\n2. è¾“å…¥é‡å®šå‘ å’Œè¾“å‡ºé‡å®šå‘ä¸€æ ·ï¼ŒUnix å‘½ä»¤ä¹Ÿå¯ä»¥ä»æ–‡ä»¶è·å–è¾“å…¥ï¼Œè¯­æ³•ä¸ºï¼š\n$ command \u003c file è¿™æ ·ï¼Œæœ¬æ¥éœ€è¦ä»é”®ç›˜è·å–è¾“å…¥çš„å‘½ä»¤ä¼šè½¬ç§»åˆ°æ–‡ä»¶è¯»å–å†…å®¹ã€‚\næ³¨æ„ï¼šè¾“å‡ºé‡å®šå‘æ˜¯å¤§äºå·(\u003e)ï¼Œè¾“å…¥é‡å®šå‘æ˜¯å°äºå·(\u003c)ã€‚\n3. é‡å®šå‘æ·±å…¥è®²è§£ ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ¯ä¸ª Unix/Linux å‘½ä»¤è¿è¡Œæ—¶éƒ½ä¼šæ‰“å¼€ä¸‰ä¸ªæ–‡ä»¶ï¼š\næ ‡å‡†è¾“å…¥æ–‡ä»¶(stdin)ï¼šstdinçš„æ–‡ä»¶æè¿°ç¬¦ä¸º0ï¼ŒUnixç¨‹åºé»˜è®¤ä»stdinè¯»å–æ•°æ®ã€‚ æ ‡å‡†è¾“å‡ºæ–‡ä»¶(stdout)ï¼šstdout çš„æ–‡ä»¶æè¿°ç¬¦ä¸º1ï¼ŒUnixç¨‹åºé»˜è®¤å‘stdoutè¾“å‡ºæ•°æ®ã€‚ æ ‡å‡†é”™è¯¯æ–‡ä»¶(stderr)ï¼šstderrçš„æ–‡ä»¶æè¿°ç¬¦ä¸º2ï¼ŒUnixç¨‹åºä¼šå‘stderræµä¸­å†™å…¥é”™è¯¯ä¿¡æ¯ã€‚ é»˜è®¤æƒ…å†µä¸‹ï¼Œcommand \u003e file å°† stdout é‡å®šå‘åˆ° fileï¼Œcommand \u003c file å°†stdin é‡å®šå‘åˆ° fileã€‚\nå¦‚æœå¸Œæœ› stderr é‡å®šå‘åˆ° fileï¼Œå¯ä»¥è¿™æ ·å†™ï¼š\n$command 2 \u003e file å¦‚æœå¸Œæœ› stderr è¿½åŠ åˆ° file æ–‡ä»¶æœ«å°¾ï¼Œå¯ä»¥è¿™æ ·å†™ï¼š\n$command 2 \u003e\u003e file 2 è¡¨ç¤ºæ ‡å‡†é”™è¯¯æ–‡ä»¶(stderr)ã€‚\nå¦‚æœå¸Œæœ›å°† stdout å’Œ stderr åˆå¹¶åé‡å®šå‘åˆ° fileï¼Œå¯ä»¥è¿™æ ·å†™ï¼š\n$command \u003e file 2\u003e\u00261 æˆ–\n$command \u003e\u003e file 2\u003e\u00261 å¦‚æœå¸Œæœ›å¯¹ stdin å’Œ stdout éƒ½é‡å®šå‘ï¼Œå¯ä»¥è¿™æ ·å†™ï¼š\n$command \u003c file1 \u003efile2 command å‘½ä»¤å°† stdin é‡å®šå‘åˆ° file1ï¼Œå°† stdout é‡å®šå‘åˆ° file2ã€‚\nå…¨éƒ¨å¯ç”¨çš„é‡å®šå‘å‘½ä»¤åˆ—è¡¨\nå‘½ä»¤ è¯´æ˜ command \u003e file å°†è¾“å‡ºé‡å®šå‘åˆ° fileã€‚ command \u003c file å°†è¾“å…¥é‡å®šå‘åˆ° fileã€‚ command \u003e\u003e file å°†è¾“å‡ºä»¥è¿½åŠ çš„æ–¹å¼é‡å®šå‘åˆ° fileã€‚ n \u003e file å°†æ–‡ä»¶æè¿°ç¬¦ä¸º n çš„æ–‡ä»¶é‡å®šå‘åˆ° fileã€‚ n \u003e\u003e file å°†æ–‡ä»¶æè¿°ç¬¦ä¸º n çš„æ–‡ä»¶ä»¥è¿½åŠ çš„æ–¹å¼é‡å®šå‘åˆ° fileã€‚ n \u003e\u0026 m å°†è¾“å‡ºæ–‡ä»¶ m å’Œ n åˆå¹¶ã€‚ n \u003c\u0026 m å°†è¾“å…¥æ–‡ä»¶ m å’Œ n åˆå¹¶ã€‚ \u003c\u003c tag å°†å¼€å§‹æ ‡è®° tag å’Œç»“æŸæ ‡è®° tag ä¹‹é—´çš„å†…å®¹ä½œä¸ºè¾“å…¥ã€‚ 4. Here Document Here Document ç›®å‰æ²¡æœ‰ç»Ÿä¸€çš„ç¿»è¯‘ï¼Œè¿™é‡Œæš‚è¯‘ä¸ºâ€åµŒå…¥æ–‡æ¡£â€œã€‚Here Document æ˜¯ Shell ä¸­çš„ä¸€ç§ç‰¹æ®Šçš„é‡å®šå‘æ–¹å¼ï¼Œå®ƒçš„åŸºæœ¬çš„å½¢å¼å¦‚ä¸‹ï¼š\ncommand \u003c\u003c delimiter document delimiter å®ƒçš„ä½œç”¨æ˜¯å°†ä¸¤ä¸ª delimiter ä¹‹é—´çš„å†…å®¹(document) ä½œä¸ºè¾“å…¥ä¼ é€’ç»™ commandã€‚ æ³¨æ„ï¼š\nç»“å°¾çš„delimiter ä¸€å®šè¦é¡¶æ ¼å†™ï¼Œå‰é¢ä¸èƒ½æœ‰ä»»ä½•å­—ç¬¦ï¼Œåé¢ä¹Ÿä¸èƒ½æœ‰ä»»ä½•å­—ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼å’Œ tab ç¼©è¿›ã€‚ å¼€å§‹çš„delimiterå‰åçš„ç©ºæ ¼ä¼šè¢«å¿½ç•¥æ‰ã€‚ 5. /dev/null æ–‡ä»¶ å¦‚æœå¸Œæœ›æ‰§è¡ŒæŸä¸ªå‘½ä»¤ï¼Œä½†åˆä¸å¸Œæœ›åœ¨å±å¹•ä¸Šæ˜¾ç¤ºè¾“å‡ºç»“æœï¼Œé‚£ä¹ˆå¯ä»¥å°†è¾“å‡ºé‡å®šå‘åˆ° /dev/nullï¼š\n$ command \u003e /dev/null /dev/null æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ–‡ä»¶ï¼Œå†™å…¥åˆ°å®ƒçš„å†…å®¹éƒ½ä¼šè¢«ä¸¢å¼ƒï¼›å¦‚æœå°è¯•ä»è¯¥æ–‡ä»¶è¯»å–å†…å®¹ï¼Œé‚£ä¹ˆä»€ä¹ˆä¹Ÿè¯»ä¸åˆ°ã€‚ä½†æ˜¯ /dev/null æ–‡ä»¶éå¸¸æœ‰ç”¨ï¼Œå°†å‘½ä»¤çš„è¾“å‡ºé‡å®šå‘åˆ°å®ƒï¼Œä¼šèµ·åˆ°â€ç¦æ­¢è¾“å‡ºâ€œçš„æ•ˆæœã€‚\nå¦‚æœå¸Œæœ›å±è”½ stdout å’Œ stderrï¼Œå¯ä»¥è¿™æ ·å†™ï¼š\n$ command \u003e /dev/null 2\u003e\u00261 6. æ–‡ä»¶åŒ…å« åƒå…¶ä»–è¯­è¨€ä¸€æ ·ï¼ŒShell ä¹Ÿå¯ä»¥åŒ…å«å¤–éƒ¨è„šæœ¬ï¼Œå°†å¤–éƒ¨è„šæœ¬çš„å†…å®¹åˆå¹¶åˆ°å½“å‰è„šæœ¬ã€‚\nShell ä¸­åŒ…å«è„šæœ¬å¯ä»¥ä½¿ç”¨ï¼š\n. filename æˆ–\nsource filename ä¸¤ç§æ–¹å¼çš„æ•ˆæœç›¸åŒï¼Œç®€å•èµ·è§ï¼Œä¸€èˆ¬ä½¿ç”¨ç‚¹å·(.)ï¼Œä½†æ˜¯æ³¨æ„ç‚¹å·(.)å’Œæ–‡ä»¶åä¸­é—´æœ‰ä¸€ç©ºæ ¼ã€‚\næ³¨æ„ï¼šè¢«åŒ…å«è„šæœ¬ä¸éœ€è¦æœ‰æ‰§è¡Œæƒé™ã€‚\nå‚è€ƒï¼š\nhttp://c.biancheng.net/cpp/shell/ ","categories":"","description":"","excerpt":"Unix å‘½ä»¤é»˜è®¤ä»æ ‡å‡†è¾“å…¥è®¾å¤‡(stdin)è·å–è¾“å…¥ï¼Œå°†ç»“æœè¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºè®¾å¤‡(stdout)æ˜¾ç¤ºã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ ‡å‡†è¾“å…¥è®¾å¤‡å°±æ˜¯é”®ç›˜ï¼Œæ ‡å‡† â€¦","ref":"/linux-notes/shell/shell-stdout/","tags":["Shell"],"title":"Shell é‡å®šå‘"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/framework/","tags":"","title":"æ¡†æ¶ä¸å·¥å…·"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/operation/","tags":"","title":"è¿ç»´æŒ‡å—"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/standard/","tags":"","title":"ä»£ç è§„èŒƒ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/develop/","tags":"","title":"å¼€å‘æŒ‡å—"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/trouble-shooting/","tags":"","title":"é—®é¢˜æ’æŸ¥"},{"body":" æœ¬æ–‡ä¸»è¦åˆ†æcontroller-runtimeçš„æºç ï¼Œæºç ç‰ˆæœ¬ä¸ºv0.16.3\n1. æ¦‚è¿° controller-runtimeæºç åœ°å€ï¼šhttps://github.com/kubernetes-sigs/controller-runtimeã€‚\ncontroller-runtimeé¡¹ç›®æ˜¯ä¸€ä¸ªç”¨äºå¿«é€Ÿæ„å»ºk8s operatorçš„å·¥å…·åŒ…ã€‚å…¶ä¸­kubebuilderå’Œoperator-sdké¡¹ç›®éƒ½æ˜¯é€šè¿‡controller-runtimeé¡¹ç›®æ¥å¿«é€Ÿç¼–å†™k8s operatorçš„å·¥å…·ã€‚\næœ¬æ–‡ä»¥kubebuilderçš„ä»£ç ç”Ÿæˆæ¶æ„ä¸ºä¾‹ï¼Œåˆ†æcontroller-runtimeçš„é€»è¾‘ã€‚kubebuilderæ¡†æ¶ç”Ÿæˆçš„ä»£ç å‚è€ƒï¼šhttps://github.com/huweihuang/venus\n2. controller-runtimeæ¶æ„å›¾ ä»£ç ç›®å½•ï¼š\npkg â”œâ”€â”€ builder â”œâ”€â”€ cache â”œâ”€â”€ client # clientç”¨äºæ“ä½œk8sçš„å¯¹è±¡ â”œâ”€â”€ cluster â”œâ”€â”€ config â”œâ”€â”€ controller # controlleré€»è¾‘ â”œâ”€â”€ envtest â”œâ”€â”€ event â”œâ”€â”€ finalizer â”œâ”€â”€ handler â”œâ”€â”€ internal # æ ¸å¿ƒä»£ç  controllerçš„å…·ä½“å®ç° â”œâ”€â”€ leaderelection â”œâ”€â”€ manager # æ ¸å¿ƒä»£ç  â”œâ”€â”€ predicate â”œâ”€â”€ ratelimiter â”œâ”€â”€ reconcile â”œâ”€â”€ recorder â”œâ”€â”€ scheme â”œâ”€â”€ source â””â”€â”€ webhook 3. Operatoræ¡†æ¶é€»è¾‘ ä»£ç å‚è€ƒï¼šhttps://github.com/huweihuang/venus/blob/main/cmd/app/operator.go#L71\noperatorä»£ç æ¡†æ¶çš„ä¸»ä½“é€»è¾‘åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ã€‚\nmanagerï¼šä¸»è¦ç”¨æ¥ç®¡ç†å¤šä¸ªçš„controllerï¼Œæ„å»ºï¼Œæ³¨å†Œï¼Œè¿è¡Œcontrollerã€‚\ncontrollerï¼šä¸»è¦ç”¨æ¥å°è£…reconcilerçš„æ§åˆ¶å™¨ã€‚\nreconcilerï¼šå…·ä½“æ‰§è¡Œä¸šåŠ¡é€»è¾‘çš„å‡½æ•°ã€‚\nmangerçš„æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ã€‚\nmgr:=ctrl.NewManagerï¼šæ„å»ºä¸€ä¸ªmanagerå¯¹è±¡ã€‚\nReconciler.SetupWithManager(mgr)ï¼šæ³¨å†Œcontrolleråˆ°managerå¯¹è±¡ã€‚\nmgr.Start(ctrl.SetupSignalHandler())ï¼šè¿è¡Œmanagerä»è€Œè¿è¡Œcontrollerçš„é€»è¾‘ã€‚\nä»£ç å¦‚ä¸‹ï¼š\n// æ„å»ºmanagerå¯¹è±¡ï¼Œä¸»è¦çš„åˆå§‹åŒ–å‚æ•°åŒ…æ‹¬ // - kubeconfig // - controllerçš„optionå‚æ•° mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, Metrics: metricsserver.Options{BindAddress: opt.MetricsAddr}, HealthProbeBindAddress: opt.ProbeAddr, LeaderElection: opt.EnableLeaderElection, LeaderElectionID: \"52609143.huweihuang.com\", Controller: config.Controller{ MaxConcurrentReconciles: opt.MaxConcurrentReconciles, }, }) // å°†controlleræ³¨å†Œåˆ°managerä¸­ï¼Œå¹¶åˆå§‹åŒ–controllerå¯¹è±¡ã€‚ if err = (\u0026venuscontroller.RedisReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \"unable to create controller\", \"controller\", \"Redis\") return err } // è¿è¡Œmanagerå¯¹è±¡ï¼Œä»è€Œè¿è¡Œcontrollerä¸­çš„reconcileé€»è¾‘ã€‚ if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \"problem running manager\") return err } 4. NewManager NewManageråˆå§‹åŒ–ä¸€ä¸ªmanager ç”¨æ¥ç®¡ç†å’Œåˆ›å»ºcontrollerå¯¹è±¡ã€‚ä¸€ä¸ªmanagerå¯ä»¥å…³è”å¤šä¸ªcontrollerå¯¹è±¡ã€‚manageræ˜¯ä¸€ä¸ªæ¥å£ï¼Œè€Œæœ€ç»ˆæ˜¯å®ç°ç»“æ„ä½“æ˜¯controllerManagerçš„å¯¹è±¡ã€‚\n4.1. Manageræ¥å£ type Manager interface { // Cluster holds a variety of methods to interact with a cluster. cluster.Cluster // Add will set requested dependencies on the component, and cause the component to be // started when Start is called. // Depending on if a Runnable implements LeaderElectionRunnable interface, a Runnable can be run in either // non-leaderelection mode (always running) or leader election mode (managed by leader election if enabled). // é€šè¿‡Runnableæ¥å£å°†å…·ä½“çš„controlleræ³¨å†Œåˆ°managerä¸­ã€‚ Add(Runnable) error // Start starts all registered Controllers and blocks until the context is cancelled. // Returns an error if there is an error starting any controller. // // If LeaderElection is used, the binary must be exited immediately after this returns, // otherwise components that need leader election might continue to run after the leader // lock was lost. // è¿è¡Œå…·ä½“çš„é€»è¾‘ Start(ctx context.Context) error } 4.2. NewControllerManager Newæ„å»ºä¸€ä¸ªå…·ä½“çš„controllerManagerçš„å¯¹è±¡ã€‚\nfunc New(config *rest.Config, options Options) (Manager, error) { // Set default values for options fields options = setOptionsDefaults(options) ... errChan := make(chan error) runnables := newRunnables(options.BaseContext, errChan) return \u0026controllerManager{ stopProcedureEngaged: pointer.Int64(0), cluster: cluster, runnables: runnables, errChan: errChan, recorderProvider: recorderProvider, resourceLock: resourceLock, metricsServer: metricsServer, controllerConfig: options.Controller, logger: options.Logger, elected: make(chan struct{}), webhookServer: options.WebhookServer, leaderElectionID: options.LeaderElectionID, leaseDuration: *options.LeaseDuration, renewDeadline: *options.RenewDeadline, retryPeriod: *options.RetryPeriod, healthProbeListener: healthProbeListener, readinessEndpointName: options.ReadinessEndpointName, livenessEndpointName: options.LivenessEndpointName, pprofListener: pprofListener, gracefulShutdownTimeout: *options.GracefulShutdownTimeout, internalProceduresStop: make(chan struct{}), leaderElectionStopped: make(chan struct{}), leaderElectionReleaseOnCancel: options.LeaderElectionReleaseOnCancel, }, nil } 5. SetupWithManager SetupWithManagerå°†å…·ä½“çš„controlleræ³¨å†Œåˆ°managerä¸­ã€‚å…¶ä¸­é€šè¿‡Completeå®Œæˆcontrollerçš„åˆå§‹åŒ–ã€‚\n// SetupWithManager sets up the controller with the Manager. func (r *RedisReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026venusv1.Redis{}). Complete(r) } SetupWithManageré€šè¿‡NewControllerManagedByæ–¹æ³•æ„å»ºäº†ä¸€ä¸ªBuilderçš„å¯¹è±¡ã€‚\n// Builder builds a Controller. type Builder struct { forInput ForInput ownsInput []OwnsInput watchesInput []WatchesInput mgr manager.Manager globalPredicates []predicate.Predicate ctrl controller.Controller ctrlOptions controller.Options name string } // ControllerManagedBy returns a new controller builder that will be started by the provided Manager. func ControllerManagedBy(m manager.Manager) *Builder { return \u0026Builder{mgr: m} } é€šè¿‡builderå¯¹è±¡å®Œæˆcontrollerçš„åˆå§‹åŒ–ã€‚\n5.1. controlleråˆå§‹åŒ– // Build builds the Application Controller and returns the Controller it created. func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) { // Set the ControllerManagedBy if err := blder.doController(r); err != nil { // åˆå§‹åŒ–controller return nil, err } // Set the Watch if err := blder.doWatch(); err != nil { // æ·»åŠ event handler return nil, err } return blder.ctrl, nil } doControlleræœ€ç»ˆé€šè¿‡è°ƒç”¨NewUnmanagedæ„å»ºä¸€ä¸ªcontrollerå¯¹è±¡ã€‚å¹¶ä¼ å…¥è‡ªå®šä¹‰çš„reconcilerå¯¹è±¡ã€‚\nfunc New(name string, mgr manager.Manager, options Options) (Controller, error) { c, err := NewUnmanaged(name, mgr, options) ... // Add the controller as a Manager components return c, mgr.Add(c) } // NewUnmanaged returns a new controller without adding it to the manager. The // caller is responsible for starting the returned controller. func NewUnmanaged(name string, mgr manager.Manager, options Options) (Controller, error) { if options.Reconciler == nil { return nil, fmt.Errorf(\"must specify Reconciler\") } ... // Create controller with dependencies set return \u0026controller.Controller{ Do: options.Reconciler, // å°†å…·ä½“çš„reconcilerå‡½æ•°ä¼ é€’åˆ°controllerçš„reconcilerã€‚ MakeQueue: func() workqueue.RateLimitingInterface { return workqueue.NewRateLimitingQueueWithConfig(options.RateLimiter, workqueue.RateLimitingQueueConfig{ Name: name, }) }, // åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ— MaxConcurrentReconciles: options.MaxConcurrentReconciles, // è®¾ç½®controllerçš„å¹¶å‘æ•° CacheSyncTimeout: options.CacheSyncTimeout, Name: name, LogConstructor: options.LogConstructor, RecoverPanic: options.RecoverPanic, LeaderElected: options.NeedLeaderElection, }, nil 5.2. æ·»åŠ event handler doWatchæœ€ç»ˆä¼šè¿è¡Œinformer.startæ·»åŠ event handlerã€‚\n// Watch implements controller.Controller. func (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error { c.mu.Lock() defer c.mu.Unlock() // Controller hasn't started yet, store the watches locally and return. // // These watches are going to be held on the controller struct until the manager or user calls Start(...). if !c.Started { c.startWatches = append(c.startWatches, watchDescription{src: src, handler: evthdler, predicates: prct}) return nil } c.LogConstructor(nil).Info(\"Starting EventSource\", \"source\", src) return src.Start(c.ctx, evthdler, c.Queue, prct...) } add event handler\nfunc (is *Informer) Start(ctx context.Context, handler handler.EventHandler, queue workqueue.RateLimitingInterface, prct ...predicate.Predicate) error { // Informer should have been specified by the user. if is.Informer == nil { return fmt.Errorf(\"must specify Informer.Informer\") } _, err := is.Informer.AddEventHandler(internal.NewEventHandler(ctx, queue, handler, prct).HandlerFuncs()) if err != nil { return err } return nil } 6. mgr.Start controllerManagerè¿è¡Œä¹‹å‰æ³¨å†Œçš„runnablesçš„å‡½æ•°ï¼Œå…¶ä¸­åŒ…æ‹¬controllerçš„å‡½æ•°ã€‚\nfunc (cm *controllerManager) Start(ctx context.Context) (err error) { // Start and wait for caches. if err := cm.runnables.Caches.Start(cm.internalCtx); err != nil { } // Start the non-leaderelection Runnables after the cache has synced. if err := cm.runnables.Others.Start(cm.internalCtx); err != nil { } // Start the leader election and all required runnables. { ctx, cancel := context.WithCancel(context.Background()) cm.leaderElectionCancel = cancel go func() { if cm.resourceLock != nil { if err := cm.startLeaderElection(ctx); err != nil { cm.errChan \u003c- err } } else { // Treat not having leader election enabled the same as being elected. if err := cm.startLeaderElectionRunnables(); err != nil { cm.errChan \u003c- err } close(cm.elected) } }() } ... } 6.1. controller.start startä¸»è¦åŒ…å«2ä¸ªéƒ¨åˆ†\nåŒæ­¥cacheï¼šWaitForSync å¯åŠ¨æŒ‡å®šå¹¶å‘æ•°çš„workerï¼šprocessNextWorkItem è¯¥éƒ¨åˆ†çš„ä»£ç é€»è¾‘è·Ÿk8s controller-managerä¸­çš„å…·ä½“çš„controllerçš„é€»è¾‘ç±»ä¼¼ã€‚\nfunc (c *Controller) Start(ctx context.Context) error { ... err := func() error { ... for _, watch := range c.startWatches { syncingSource, ok := watch.src.(source.SyncingSource) // åŒæ­¥list-watchä¸­çš„cacheæ•°æ®ã€‚ if err := func() error { if err := syncingSource.WaitForSync(sourceStartCtx); err != nil { ... } } } // è¿è¡ŒæŒ‡å®šå¹¶å‘æ•°çš„processNextWorkItemä»»åŠ¡ã€‚ // Launch workers to process resources c.LogConstructor(nil).Info(\"Starting workers\", \"worker count\", c.MaxConcurrentReconciles) wg.Add(c.MaxConcurrentReconciles) for i := 0; i \u003c c.MaxConcurrentReconciles; i++ { go func() { defer wg.Done() // Run a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the reconcileHandler is never invoked concurrently with the same object. for c.processNextWorkItem(ctx) { } }() } ... } 6.2. processNextWorkItem ç»å…¸çš„processNextWorkItemå‡½æ•°ï¼Œæœ€ç»ˆè°ƒç”¨reconcileHandleræ¥å¤„ç†å…·ä½“çš„é€»è¾‘ã€‚\nfunc (c *Controller) processNextWorkItem(ctx context.Context) bool { obj, shutdown := c.Queue.Get() if shutdown { // Stop working return false } defer c.Queue.Done(obj) ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(1) defer ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(-1) c.reconcileHandler(ctx, obj) return true } 7. reconcileHandler reconcileHandleréƒ¨åˆ†çš„ä»£ç æ˜¯æ•´ä¸ªreconcileré€»è¾‘ä¸­çš„æ ¸å¿ƒï¼Œè‡ªå®šä¹‰çš„reconcilerå‡½æ•°æœ€ç»ˆæ˜¯è°ƒç”¨äº†reconcileHandleræ¥å®ç°ï¼Œå¹¶ä¸”è¯¥å‡½æ•°æè¿°äº†å…·ä½“çš„ä»»åŠ¡é˜Ÿåˆ—å¤„ç†çš„å‡ ç§ç±»å‹ã€‚\nerr != nilï¼šå¦‚æœé”™è¯¯ä¸ä¸ºç©ºï¼Œåˆ™é‡æ–°å…¥é˜Ÿï¼Œç­‰å¾…å¤„ç†ã€‚ result.RequeueAfter \u003e 0ï¼šå¦‚æœæŒ‡å®šRequeueAfter \u003e 0ï¼Œåˆ™åšå»¶è¿Ÿå…¥é˜Ÿå¤„ç†ã€‚ result.Requeueï¼šå¦‚ä½•æŒ‡å®šäº†requeueåˆ™è¡¨ç¤ºé©¬ä¸Šé‡æ–°å…¥é˜Ÿå¤„ç†ã€‚ err == nil : å¦‚æœé”™è¯¯ä¸ºç©ºï¼Œè¡¨ç¤ºreconcileæˆåŠŸï¼Œåˆ™ç§»é™¤é˜Ÿåˆ—çš„ä»»åŠ¡ã€‚ func (c *Controller) reconcileHandler(ctx context.Context, obj interface{}) { // è·å–k8s crdçš„å…·ä½“å¯¹è±¡ req, ok := obj.(reconcile.Request) if !ok { // As the item in the workqueue is actually invalid, we call // Forget here else we'd go into a loop of attempting to // process a work item that is invalid. c.Queue.Forget(obj) c.LogConstructor(nil).Error(nil, \"Queue item was not a Request\", \"type\", fmt.Sprintf(\"%T\", obj), \"value\", obj) // Return true, don't take a break return } // è°ƒç”¨ç”¨æˆ·å®šä¹‰çš„Reconcileå‡½æ•°ï¼Œå¹¶å¯¹è¿”å›ç»“æœè¿›è¡Œå¤„ç†ã€‚ result, err := c.Reconcile(ctx, req) switch { case err != nil: // å¦‚æœé”™è¯¯ä¸ä¸ºç©ºï¼Œåˆ™é‡æ–°å…¥é˜Ÿï¼Œç­‰å¾…å¤„ç† if errors.Is(err, reconcile.TerminalError(nil)) { ctrlmetrics.TerminalReconcileErrors.WithLabelValues(c.Name).Inc() } else { c.Queue.AddRateLimited(req) } case result.RequeueAfter \u003e 0: // å¦‚æœæŒ‡å®šäº†å»¶è¿Ÿå…¥é˜Ÿï¼Œåˆ™åšå»¶è¿Ÿå…¥é˜Ÿå¤„ç† c.Queue.Forget(obj) c.Queue.AddAfter(req, result.RequeueAfter) case result.Requeue: // å¦‚æœæŒ‡å®šäº†é©¬ä¸Šå…¥é˜Ÿï¼Œåˆ™åšç›¸åº”å¤„ç† c.Queue.AddRateLimited(req) default: // å¦‚æœé”™è¯¯ä¸ºç©ºï¼Œè¡¨ç¤ºreconcileæˆåŠŸï¼Œåˆ™ç§»é™¤é˜Ÿåˆ—çš„ä»»åŠ¡ log.V(5).Info(\"Reconcile successful\") // Finally, if no error occurs we Forget this item so it does not // get queued again until another change happens. c.Queue.Forget(obj) } } 8. æ€»ç»“ controller-runtimeå°è£…äº†k8s-controller-manageræ§åˆ¶å™¨çš„ä¸»è¦é€»è¾‘ï¼Œå…¶ä¸­å°±åŒ…æ‹¬åˆ›å»ºlist-watchå¯¹è±¡ï¼ŒwaitForSyncç­‰ï¼Œåˆ›å»ºä»»åŠ¡é˜Ÿåˆ—ï¼Œå°†ä»»åŠ¡å¤„ç†çš„goroutineæŠ½è±¡æˆä¸€ä¸ªreconcileå‡½æ•°ï¼Œä½¿ç”¨æˆ·æ›´æ–¹ä¾¿çš„ç¼–å†™operatorå·¥å…·ã€‚ kubebuilderæ˜¯ä¸€ä¸ªåŸºäºcontroller-runtimeæ¡†æ¶çš„å‘½ä»¤ç”Ÿæˆå·¥å…·ã€‚å¯ä»¥ç”¨äºå¿«é€Ÿç”Ÿæˆå’Œéƒ¨ç½²crdå¯¹è±¡ï¼Œå¿«é€Ÿç”Ÿæˆcontroller-runtimeæ¡†æ¶çš„åŸºæœ¬ä»£ç ã€‚ controller-runtimeæ¡†æ¶çš„æœ€æ ¸å¿ƒæ˜¯reconcileHandlerå‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†reconcileçš„4ç§é”™è¯¯å¤„ç†åŠå…¥é˜Ÿé‡è¯•çš„ç±»å‹ã€‚å¯ä»¥æ ¹æ®å…·ä½“çš„ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ–¹æ³•æ¥å¤„ç†ã€‚ å‚è€ƒï¼š\ncontroller-runtimeæºç åˆ†æ\ncontroller-runtimeç»†èŠ‚åˆ†æ\n","categories":"","description":"","excerpt":" æœ¬æ–‡ä¸»è¦åˆ†æcontroller-runtimeçš„æºç ï¼Œæºç ç‰ˆæœ¬ä¸ºv0.16.3\n1. æ¦‚è¿° controller-runtimeæºç åœ° â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/controller-runtime/","tags":["æºç åˆ†æ"],"title":"controller-runtimeæºç åˆ†æ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/memcached/","tags":"","title":"Memcached"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/web/","tags":"","title":"Webç¼–ç¨‹"},{"body":"æœ¬æ–‡ä¸»è¦åˆ†æclient-goä¸­ä½¿ç”¨çš„workqueueï¼Œä»è€Œæ¥åˆ†æk8sæ˜¯å¦‚ä½•åŸºäºä»»åŠ¡é˜Ÿåˆ—åšå¹¶å‘æ§åˆ¶çš„ã€‚å…¶ä¸­ä»£ç å‚è€ƒï¼š\nhttps://github.com/kubernetes/client-go/tree/release-1.30 1. æ¦‚è¿° k8sçš„æ§åˆ¶å™¨å¤§å¤šæ˜¯åŸºäºä»»åŠ¡é˜Ÿåˆ—çš„æ–¹å¼è¿›è¡Œå¹¶å‘æ§åˆ¶ï¼Œç”šè‡³åŒ…æ‹¬åŸºäºcontroller-managerå¼€å‘çš„è‡ªå®šä¹‰operatoræ§åˆ¶å™¨ã€‚ä»¥ä¸‹æˆ‘ä»¬ä»¥deployment controllerä¸ºä¾‹å±•ç¤ºworkqueueåœ¨k8sä¸­çš„ä½¿ç”¨ã€‚\n2. Deploymentä¸­çš„workqueue 2.1. åˆå§‹åŒ–workqueue Deployment çš„æ„å»ºå‡½æ•°NewDeploymentControllerä¸­åˆå§‹åŒ–äº†ä»»åŠ¡é˜Ÿåˆ—å’Œå¯¹äºeventäº‹ä»¶å¤„ç†ç›¸å¯¹åº”çš„workqueueçš„æ“ä½œã€‚\nåˆå§‹åŒ–workqueueçš„é™é€Ÿä»»åŠ¡é˜Ÿåˆ—ã€‚ æ·»åŠ event handlerä½¿å¾—deploymentå¯¹è±¡å…¥é˜Ÿï¼Œç­‰å¾…å¤„ç†ã€‚ func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { dc := \u0026DeploymentController{ // åˆå§‹åŒ–ä¸€ä¸ªé™é€Ÿçš„ä»»åŠ¡é˜Ÿåˆ— queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } // æ·»åŠ  add, update, deleteçš„event handler, å…¶ä¸­handlerçš„æ“ä½œéƒ½æ·»åŠ å¯¹è±¡åˆ°ä»»åŠ¡é˜Ÿåˆ—ä¸­ã€‚ dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addDeployment(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateDeployment(logger, oldObj, newObj) }, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: func(obj interface{}) { dc.deleteDeployment(logger, obj) }, }) // å®šä¹‰é˜Ÿåˆ—å¤„ç†å‡½æ•° dc.enqueueDeployment = dc.enqueue ä»¥ä¸‹æ˜¾ç¤ºevent handlerå…·ä½“é€»è¾‘ï¼Œå¯è§ï¼Œæ— è®ºæ˜¯addï¼Œupdateï¼Œdelete event handleréƒ½æ˜¯è°ƒç”¨enqueueDeploymentçš„å‡½æ•°ï¼Œè€ŒenqueueDeploymentå®é™…ä¸Šå°±æ˜¯åˆå§‹åŒ–å‡½æ•°ä¸­çš„dc.enqueueã€‚\nfunc (dc *DeploymentController) addDeployment(logger klog.Logger, obj interface{}) { d := obj.(*apps.Deployment) logger.V(4).Info(\"Adding deployment\", \"deployment\", klog.KObj(d)) dc.enqueueDeployment(d) } func (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) { oldD := old.(*apps.Deployment) curD := cur.(*apps.Deployment) logger.V(4).Info(\"Updating deployment\", \"deployment\", klog.KObj(oldD)) dc.enqueueDeployment(curD) } func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interface{}) { d, ok := obj.(*apps.Deployment) logger.V(4).Info(\"Deleting deployment\", \"deployment\", klog.KObj(d)) dc.enqueueDeployment(d) } 2.2. æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ— ä»¥ä¸‹æ˜¯dc.enqueueçš„å…·ä½“å®ç°ï¼Œå¯è§æœ€ç»ˆçš„è°ƒç”¨workqueueçš„Addæ–¹æ³•ï¼Œæ·»åŠ å¯¹è±¡åˆ°ä»»åŠ¡é˜Ÿåˆ—ä¸­ã€‚\nfunc (dc *DeploymentController) enqueue(deployment *apps.Deployment) { key, err := controller.KeyFunc(deployment) // æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—ä¸­ã€‚ dc.queue.Add(key) } å…¶ä¸­keyæ˜¯namespace/nameçš„æ‹¼æ¥å­—ç¬¦ä¸²ï¼Œé€šè¿‡MetaNamespaceKeyFuncå‡½æ•°è·å–å¯¹è±¡çš„keyã€‚\n// é€šè¿‡è¯¥å‡½æ•°è·å–k8så¯¹è±¡ä¸­å¤„ç†çš„keyã€‚ func MetaNamespaceKeyFunc(obj interface{}) (string, error) { if key, ok := obj.(ExplicitKey); ok { return string(key), nil } objName, err := ObjectToName(obj) if err != nil { return \"\", err } return objName.String(), nil } // æ‹¼æ¥namespaceå’Œname func (objName ObjectName) String() string { if len(objName.Namespace) \u003e 0 { return objName.Namespace + \"/\" + objName.Name } return objName.Name } 2.3. è¯»å–ä»»åŠ¡é˜Ÿåˆ— deployment controllerä¼šè°ƒç”¨dc.queue.Get()æ¥è¯»å–ä»»åŠ¡é˜Ÿåˆ—ä¸­çš„å¯¹è±¡ã€‚è¯¥goroutineæ˜¯ä¸€ä¸ªå¸¸é©»çš„é€»è¾‘å®æ—¶è·å–ã€‚\nfunc (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool { // è¯»å–ä»»åŠ¡é˜Ÿåˆ— key, quit := dc.queue.Get() if quit { return false } // è¿”å›å°†keyè®¾ç½®ä¸ºdoneã€‚ defer dc.queue.Done(key) // å¤„ç†ä»»åŠ¡ err := dc.syncHandler(ctx, key.(string)) dc.handleErr(ctx, err, key) return true } ä»»åŠ¡çš„é”™è¯¯å¤„ç†ï¼š\nå¦‚æœé”™è¯¯ä¸ºç©ºï¼Œåˆ™è°ƒç”¨Forgetç§»å‡ºé˜Ÿåˆ— å¦‚æœå°äºæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ™åŠ å…¥å»¶è¿Ÿé˜Ÿåˆ— å¦‚æœå¤§äºæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ™ç§»å‡ºé˜Ÿåˆ—ã€‚ func (dc *DeploymentController) handleErr(ctx context.Context, err error, key interface{}) { // å¦‚æœé”™è¯¯ä¸ºç©ºï¼Œåˆ™è°ƒç”¨Forgetç§»å‡ºé˜Ÿåˆ— if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) { dc.queue.Forget(key) return } ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string)) // å¦‚æœå°äºæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ™åŠ å…¥å»¶è¿Ÿé˜Ÿåˆ— if dc.queue.NumRequeues(key) \u003c maxRetries { dc.queue.AddRateLimited(key) return } // å¦‚æœå¤§äºæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ™ç§»å‡ºé˜Ÿåˆ—ã€‚ utilruntime.HandleError(err) logger.V(2).Info(\"Dropping deployment out of the queue\", \"deployment\", klog.KRef(ns, name), \"err\", err) dc.queue.Forget(key) } syncDeploymentè·å–deploymentå¯¹è±¡ï¼ŒåŸºäºnså’Œnameé‡æ–°è·å–deploymentã€‚\nfunc (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error { // æ‹†åˆ†keyè·å–nså’Œname namespace, name, err := cache.SplitMetaNamespaceKey(key) // åŸºäºnså’Œnameè·å–deploymentå¯¹è±¡ deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { logger.V(2).Info(\"Deployment has been deleted\", \"deployment\", klog.KRef(namespace, name)) return nil } // deepcopy deploymentå¯¹è±¡ d := deployment.DeepCopy() å¾…å®Œå–„\n","categories":"","description":"","excerpt":"æœ¬æ–‡ä¸»è¦åˆ†æclient-goä¸­ä½¿ç”¨çš„workqueueï¼Œä»è€Œæ¥åˆ†æk8sæ˜¯å¦‚ä½•åŸºäºä»»åŠ¡é˜Ÿåˆ—åšå¹¶å‘æ§åˆ¶çš„ã€‚å…¶ä¸­ä»£ç å‚è€ƒï¼š â€¦","ref":"/k8s-source-code-analysis/kube-controller-manager/workqueue/","tags":["æºç åˆ†æ"],"title":"workqueueæºç åˆ†æ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/nginx/","tags":"","title":"Nginx"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/runtime/","tags":"","title":"Runtime"},{"body":"","categories":"","description":"","excerpt":"","ref":"/golang-notes/code/","tags":"","title":"æºç åˆ†æ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/etcd/","tags":"","title":"Etcdé›†ç¾¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/keepalived/","tags":"","title":"Keepalived"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/ide/","tags":"","title":"IDEé…ç½®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/multi-cluster/","tags":"","title":"å¤šé›†ç¾¤ç®¡ç†"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/keymap/","tags":"","title":"å¿«æ·é”®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/edge/","tags":"","title":"è¾¹ç¼˜å®¹å™¨"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/baremetal/","tags":"","title":"è£¸é‡‘å±"},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux-notes/llm/","tags":"","title":"å¤§æ¨¡å‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/kvm/","tags":"","title":"è™šæ‹ŸåŒ–"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/monitor/","tags":"","title":"ç›‘æ§ä½“ç³»"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/serverless/","tags":"","title":"Serverless"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-notes/cluster-optimization/","tags":"","title":"é›†ç¾¤ä¼˜åŒ–"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/golang/","tags":"","title":"Golang"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cni/","tags":"","title":"CNI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/disk/","tags":"","title":"disk"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/serverless/","tags":"","title":"Serverless"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/network/","tags":"","title":"Network"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubevirt/","tags":"","title":"KubeVirt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/nginx/","tags":"","title":"Nginx"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%A3%B8%E9%87%91%E5%B1%9E/","tags":"","title":"è£¸é‡‘å±"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/operator/","tags":"","title":"Operator"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/openyurt/","tags":"","title":"OpenYurt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/monitor/","tags":"","title":"Monitor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/apisix/","tags":"","title":"ApiSix"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/karmada/","tags":"","title":"Karmada"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/runtime/","tags":"","title":"Runtime"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubeedge/","tags":"","title":"Kubeedge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%A4%9A%E9%9B%86%E7%BE%A4/","tags":"","title":"å¤šé›†ç¾¤"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/linux/","tags":"","title":"Linux"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/iptables/","tags":"","title":"iptables"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/etcd/","tags":"","title":"Etcd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/virtualkubelet/","tags":"","title":"VirtualKubelet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git/","tags":"","title":"Git"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/keepalived/","tags":"","title":"Keepalived"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/","tags":"","title":"å¿«æ·é”®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/memcached/","tags":"","title":"Memcached"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tcpip/","tags":"","title":"TCPIP"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","tags":"","title":"æºç åˆ†æ"},{"body":"1. kubeletç®€ä»‹ åœ¨kubernetesé›†ç¾¤ä¸­ï¼Œæ¯ä¸ªNodeèŠ‚ç‚¹éƒ½ä¼šå¯åŠ¨kubeletè¿›ç¨‹ï¼Œç”¨æ¥å¤„ç†MasterèŠ‚ç‚¹ä¸‹å‘åˆ°æœ¬èŠ‚ç‚¹çš„ä»»åŠ¡ï¼Œç®¡ç†Podå’Œå…¶ä¸­çš„å®¹å™¨ã€‚kubeletä¼šåœ¨API Serverä¸Šæ³¨å†ŒèŠ‚ç‚¹ä¿¡æ¯ï¼Œå®šæœŸå‘Masteræ±‡æŠ¥èŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µï¼Œå¹¶é€šè¿‡cAdvisorç›‘æ§å®¹å™¨å’ŒèŠ‚ç‚¹èµ„æºã€‚å¯ä»¥æŠŠkubeletç†è§£æˆã€Server-Agentã€‘æ¶æ„ä¸­çš„agentï¼Œæ˜¯Nodeä¸Šçš„podç®¡å®¶ã€‚\næ›´å¤škubeleté…ç½®å‚æ•°ä¿¡æ¯å¯å‚è€ƒkubelet --help\n2. èŠ‚ç‚¹ç®¡ç† èŠ‚ç‚¹é€šè¿‡è®¾ç½®kubeletçš„å¯åŠ¨å‚æ•°â€œ--register-nodeâ€ï¼Œæ¥å†³å®šæ˜¯å¦å‘API Serveræ³¨å†Œè‡ªå·±ï¼Œé»˜è®¤ä¸ºtrueã€‚å¯ä»¥é€šè¿‡kubelet --helpæˆ–è€…æŸ¥çœ‹kubernetesæºç ã€cmd/kubelet/app/server.goä¸­ã€‘æ¥æŸ¥çœ‹è¯¥å‚æ•°ã€‚\nkubeletçš„é…ç½®æ–‡ä»¶\né»˜è®¤é…ç½®æ–‡ä»¶åœ¨/etc/kubernetes/kubeletä¸­ï¼Œå…¶ä¸­\n--api-serversï¼šç”¨æ¥é…ç½®MasterèŠ‚ç‚¹çš„IPå’Œç«¯å£ã€‚ --kubeconfigï¼šç”¨æ¥é…ç½®kubeconfigçš„è·¯å¾„ï¼Œkubeconfigæ–‡ä»¶å¸¸ç”¨æ¥æŒ‡å®šè¯ä¹¦ã€‚ --hostname-overrideï¼šç”¨æ¥é…ç½®è¯¥èŠ‚ç‚¹åœ¨é›†ç¾¤ä¸­æ˜¾ç¤ºçš„ä¸»æœºåã€‚ --node-status-update-frequencyï¼šé…ç½®kubeletå‘Masterå¿ƒè·³ä¸ŠæŠ¥çš„é¢‘ç‡ï¼Œé»˜è®¤ä¸º10sã€‚ 3. Podç®¡ç† kubeletæœ‰å‡ ç§æ–¹å¼è·å–è‡ªèº«Nodeä¸Šæ‰€éœ€è¦è¿è¡Œçš„Podæ¸…å•ã€‚ä½†æœ¬æ–‡åªè®¨è®ºé€šè¿‡API Serverç›‘å¬etcdç›®å½•ï¼ŒåŒæ­¥Podåˆ—è¡¨çš„æ–¹å¼ã€‚\nkubeleté€šè¿‡API Server Clientä½¿ç”¨WatchAndListçš„æ–¹å¼ç›‘å¬etcdä¸­/registry/nodes/${å½“å‰èŠ‚ç‚¹åç§°}å’Œ/registry/podsçš„ç›®å½•ï¼Œå°†è·å–çš„ä¿¡æ¯åŒæ­¥åˆ°æœ¬åœ°ç¼“å­˜ä¸­ã€‚\nkubeletç›‘å¬etcdï¼Œæ‰§è¡Œå¯¹Podçš„æ“ä½œï¼Œå¯¹å®¹å™¨çš„æ“ä½œåˆ™æ˜¯é€šè¿‡Docker Clientæ‰§è¡Œï¼Œä¾‹å¦‚å¯åŠ¨åˆ é™¤å®¹å™¨ç­‰ã€‚\nkubeletåˆ›å»ºå’Œä¿®æ”¹Podæµç¨‹ï¼š\nä¸ºè¯¥Podåˆ›å»ºä¸€ä¸ªæ•°æ®ç›®å½•ã€‚ ä»API Serverè¯»å–è¯¥Podæ¸…å•ã€‚ ä¸ºè¯¥PodæŒ‚è½½å¤–éƒ¨å·ï¼ˆExternal Volumeï¼‰ ä¸‹è½½Podç”¨åˆ°çš„Secretã€‚ æ£€æŸ¥è¿è¡Œçš„Podï¼Œæ‰§è¡ŒPodä¸­æœªå®Œæˆçš„ä»»åŠ¡ã€‚ å…ˆåˆ›å»ºä¸€ä¸ªPauseå®¹å™¨ï¼Œè¯¥å®¹å™¨æ¥ç®¡Podçš„ç½‘ç»œï¼Œå†åˆ›å»ºå…¶ä»–å®¹å™¨ã€‚ Podä¸­å®¹å™¨çš„å¤„ç†æµç¨‹ï¼š 1ï¼‰æ¯”è¾ƒå®¹å™¨hashå€¼å¹¶åšç›¸åº”å¤„ç†ã€‚ 2ï¼‰å¦‚æœå®¹å™¨è¢«ç»ˆæ­¢äº†ä¸”æ²¡æœ‰æŒ‡å®šé‡å¯ç­–ç•¥ï¼Œåˆ™ä¸åšä»»ä½•å¤„ç†ã€‚ 3ï¼‰è°ƒç”¨Docker Clientä¸‹è½½å®¹å™¨é•œåƒï¼Œè°ƒç”¨Docker Clientè¿è¡Œå®¹å™¨ã€‚ 4. å®¹å™¨å¥åº·æ£€æŸ¥ Podé€šè¿‡æ¢é’ˆçš„æ–¹å¼æ¥æ£€æŸ¥å®¹å™¨çš„å¥åº·çŠ¶æ€ï¼Œå…·ä½“å¯å‚è€ƒPodè¯¦è§£#Podå¥åº·æ£€æŸ¥ã€‚\n5. cAdvisorèµ„æºç›‘æ§ kubeleté€šè¿‡cAdvisorè·å–æœ¬èŠ‚ç‚¹ä¿¡æ¯åŠå®¹å™¨çš„æ•°æ®ã€‚cAdvisorä¸ºè°·æ­Œå¼€æºçš„å®¹å™¨èµ„æºåˆ†æå·¥å…·ï¼Œé»˜è®¤é›†æˆåˆ°kubernetesä¸­ã€‚\ncAdvisorè‡ªåŠ¨é‡‡é›†CPU,å†…å­˜ï¼Œæ–‡ä»¶ç³»ç»Ÿï¼Œç½‘ç»œä½¿ç”¨æƒ…å†µï¼Œå®¹å™¨ä¸­è¿è¡Œçš„è¿›ç¨‹ï¼Œé»˜è®¤ç«¯å£ä¸º4194ã€‚å¯ä»¥é€šè¿‡Node IP+Portè®¿é—®ã€‚\næ›´å¤šå‚è€ƒï¼šhttp://github.com/google/cadvisor\nå‚è€ƒã€ŠKubernetesæƒå¨æŒ‡å—ã€‹\n","categories":"","description":"","excerpt":"1. kubeletç®€ä»‹ åœ¨kubernetesé›†ç¾¤ä¸­ï¼Œæ¯ä¸ªNodeèŠ‚ç‚¹éƒ½ä¼šå¯åŠ¨kubeletè¿›ç¨‹ï¼Œç”¨æ¥å¤„ç†MasterèŠ‚ç‚¹ä¸‹å‘åˆ°æœ¬èŠ‚ç‚¹çš„ä»» â€¦","ref":"/kubernetes-notes/principle/component/kubernetes-core-principle-kubelet/","tags":["Kubernetes"],"title":"Kubernetesæ ¸å¿ƒåŸç†ï¼ˆå››ï¼‰ä¹‹kubelet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysql/","tags":"","title":"Mysql"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pod/","tags":"","title":"Pod"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docker/","tags":"","title":"Docker"},{"body":"Summary ç›®å½• åºè¨€ äº‘åŸç”Ÿä½“ç³» 12-Factor K8SçŸ¥è¯†ä½“ç³» å®‰è£…ä¸é…ç½® éƒ¨ç½²k8sé›†ç¾¤ ä½¿ç”¨kubeadmå®‰è£…ç”Ÿäº§ç¯å¢ƒkubernetes ä½¿ç”¨kubesprayå®‰è£…kubernetes ä½¿ç”¨minikubeå®‰è£…kubernetes ä½¿ç”¨kindå®‰è£…kubernetes å®‰è£…k8s dashboard kubeadmå‡çº§k8sé›†ç¾¤ kubeadmç®¡ç†è¯ä¹¦ k8sè¯ä¹¦åŠç§˜é’¥ k8sç‰ˆæœ¬è¯´æ˜ k8sç‰ˆæœ¬è®°å½• åŸºæœ¬æ¦‚å¿µ kubernetesæ¶æ„ Kubernetesæ€»æ¶æ„å›¾ åŸºäºDockeråŠKubernetesæŠ€æœ¯æ„å»ºå®¹å™¨äº‘ï¼ˆPaaSï¼‰å¹³å°æ¦‚è¿° kuberneteså¯¹è±¡ ç†è§£kuberneteså¯¹è±¡ kuberneteså¸¸ç”¨å¯¹è±¡è¯´æ˜ Pod Podä»‹ç» Podå®šä¹‰æ–‡ä»¶ Podç”Ÿå‘½å‘¨æœŸ Podå¥åº·æ£€æŸ¥ Podå­˜å‚¨å· Podæ§åˆ¶å™¨ Podä¼¸ç¼©ä¸å‡çº§ é…ç½® ConfigMap Workload æ ¸å¿ƒåŸç† æ ¸å¿ƒç»„ä»¶ Api Server Controller Manager Scheduler Kubelet æµç¨‹å›¾ Podåˆ›å»ºæµç¨‹ PVCåˆ›å»ºæµç¨‹ å®¹å™¨ç½‘ç»œ Dockerç½‘ç»œ K8Sç½‘ç»œ Podçš„DNSç­–ç•¥ ç½‘ç»œæ’ä»¶ Flannelä»‹ç» CNI CNIæ¥å£ä»‹ç» Macvlanä»‹ç» å®¹å™¨å­˜å‚¨ å­˜å‚¨å·æ¦‚å¿µ Volume Persistent Volume Persistent Volume Claim Storage Class Dynamic Volume Provisioning CSI csi-cephfs-plugin éƒ¨ç½²csi-cephfs éƒ¨ç½²cephfs-provisioner FlexVolumeä»‹ç» èµ„æºéš”ç¦» èµ„æºé…é¢ Podé™é¢ èµ„æºæœåŠ¡è´¨é‡ Lxcfsèµ„æºè§†å›¾éš”ç¦» è¿ç»´æŒ‡å— kubernetesé›†ç¾¤é—®é¢˜æ’æŸ¥ kubectlå·¥å…· kubectlå®‰è£…ä¸é…ç½® kubectlå‘½ä»¤è¯´æ˜ kubectlå‘½ä»¤åˆ«å kubectlè¿›å…¥node shell helmå·¥å…· helmçš„ä½¿ç”¨ èŠ‚ç‚¹è¿ç§» å®‰å…¨è¿ç§»èŠ‚ç‚¹ æŒ‡å®šNodeè°ƒåº¦ä¸éš”ç¦» é•œåƒä»“åº“ é…ç½®ç§æœ‰çš„é•œåƒä»“åº“ æ‹‰å–ç§æœ‰é•œåƒ è®¿é—®æ§åˆ¶ ä½¿ç”¨RBACé‰´æƒ ç‰ˆæœ¬å‘å¸ƒ é‡‘ä¸é›€å‘å¸ƒ å¼€å‘æŒ‡å— client-goçš„ä½¿ç”¨åŠæºç åˆ†æ CSIæ’ä»¶å¼€å‘ nfs-client-provisioneræºç åˆ†æ csi-provisioneræºç åˆ†æ operatorå¼€å‘ kubebuilderçš„ä½¿ç”¨ å¦‚ä½•å¼€å‘ä¸€ä¸ªOperator k8sç¤¾åŒºå¼€å‘æŒ‡å— é—®é¢˜æ’æŸ¥ èŠ‚ç‚¹ç›¸å…³é—®é¢˜ keycreate permission denied Cgroupä¸æ”¯æŒpidèµ„æº Cgroupå­ç³»ç»Ÿæ— æ³•æŒ‚è½½ Podé©±é€ é•œåƒæ‹‰å–å¤±è´¥é—®é¢˜ PVC Terminating æºç åˆ†æ Kubernetesæºç åˆ†æç¬”è®° kubelet NewKubeletCommand NewMainKubelet startKubelet syncLoopIteration syncPod kube-controller-manager NewControllerManagerCommand DeploymentController Informeræœºåˆ¶ kube-scheduler NewSchedulerCommand registerAlgorithmProvider scheduleOne findNodesThatFit PrioritizeNodes preempt kube-apiserver NewAPIServerCommand Runtime Runtime Runcå’ŒContainerdæ¦‚è¿° Containerd å®‰è£…Containerd Docker Dockerå­¦ä¹ ç¬”è®° Kata Container kataå®¹å™¨ç®€ä»‹ kataé…ç½® GPU nvidia-device-pluginä»‹ç» Etcd Etcdä»‹ç» Raftç®—æ³• Etcdå¯åŠ¨é…ç½®å‚æ•° Etcdè®¿é—®æ§åˆ¶ etcdctlå‘½ä»¤å·¥å…· etcdctlå‘½ä»¤å·¥å…·-V3 etcdctlå‘½ä»¤å·¥å…·-V2 Etcdä¸­çš„k8sæ•°æ® Etcd-Operatorçš„ä½¿ç”¨ å¤šé›†ç¾¤ç®¡ç† k8så¤šé›†ç¾¤ç®¡ç†çš„æ€è€ƒ Virtual Kubelet Virtual Kubeletä»‹ç» Virtual Kubelet å‘½ä»¤ Karmada Karmadaä»‹ç» è¾¹ç¼˜å®¹å™¨ KubeEdgeä»‹ç» KubeEdgeæºç åˆ†æ cloudcore edgecore OpenYurt OpenYurtéƒ¨ç½² OpenYurtéƒ¨ç½²ä¹‹è°ƒæ•´k8sé…ç½® OpenYurtæºç åˆ†æ YurtHubæºç åˆ†æï¼ˆ1) TunnelServeræºç åˆ†æï¼ˆ1ï¼‰ Tunnel-Agentæºç åˆ†æ è™šæ‹ŸåŒ– è™šæ‹ŸåŒ–ç›¸å…³æ¦‚å¿µ KubeVirt KubeVirtçš„ä»‹ç» KubeVirtçš„ä½¿ç”¨ ç›‘æ§ä½“ç³» ç›‘æ§ä½“ç³»ä»‹ç» kube-prometheus-stackçš„ä½¿ç”¨ cAdvisorä»‹ç» Heapsterä»‹ç» Influxdbä»‹ç» ","categories":"","description":"","excerpt":"Summary ç›®å½• åºè¨€ äº‘åŸç”Ÿä½“ç³» 12-Factor K8SçŸ¥è¯†ä½“ç³» å®‰è£…ä¸é…ç½® éƒ¨ç½²k8sé›†ç¾¤ ä½¿ç”¨kubeadmå®‰è£…ç”Ÿäº§ç¯ â€¦","ref":"/kubernetes-notes/summary/","tags":"","title":""},{"body":"Summary åºè¨€ åºè¨€ è®¡ç®— CPU\nå†…å­˜\nå­˜å‚¨ Linux æ–‡ä»¶ç³»ç»Ÿ Linux ä»‹ç» æ–‡ä»¶ç³»ç»Ÿ æ–‡ä»¶å­˜å‚¨ç»“æ„ æ–‡ä»¶æƒé™ ç£ç›˜ LVMçš„ä½¿ç”¨ ç£ç›˜å‘½ä»¤ Raidä»‹ç» ç½‘ç»œ TCP/IP TCP/IPåŸºç¡€ IPåè®® TCPä¸UDPåè®® Http HttpåŸºç¡€ HttpæŠ¥æ–‡ HttpçŠ¶æ€ç  ç½‘ç»œå‘½ä»¤ iptablesä»‹ç» iptableså‘½ä»¤ tcpdumpå‘½ä»¤ wrkå‹æµ‹å‘½ä»¤ VLANä»‹ç» ç¨‹åº è¿›ç¨‹\nShell è„šæœ¬\nShellç®€ä»‹ Shellå˜é‡ Shellè¿ç®—ç¬¦ Shellæ•°ç»„ Shell echoå‘½ä»¤ Shellåˆ¤æ–­è¯­å¥ Shellå¾ªç¯è¯­å¥ Shellå‡½æ•° Shellé‡å®šå‘ è¿ç»´å·¥å…· Ansibleçš„ä½¿ç”¨ Supervisorçš„ä½¿ç”¨ Confdçš„ä½¿ç”¨ NFSçš„ä½¿ç”¨ ceph-fuseçš„ä½¿ç”¨ ssh tips Gitç®¡ç† Git ä»‹ç» Git å¸¸ç”¨å‘½ä»¤ Git å‘½ä»¤åˆ†ç±» Git commitè§„èŒƒ Git å‘½ä»¤åˆ«å æ•°æ®åº“ Mysql ç³»ç»Ÿç®¡ç† æ•°æ®è¡¨æ“ä½œ è¡¨å†…å®¹æ“ä½œ Redis Redisä»‹ç» Redisé›†ç¾¤æ¨¡å¼éƒ¨ç½² Redisä¸»ä»åŠå“¨å…µæ¨¡å¼éƒ¨ç½² Redisé…ç½®è¯¦è§£(ä¸­æ–‡ç‰ˆ) Redisé…ç½®è¯¦è§£(è‹±æ–‡ç‰ˆ) Memcached Memcachedçš„ä½¿ç”¨ Memcachedå‘½ä»¤ ç½‘ç»œå·¥å…· Nginx Nginxå®‰è£…ä¸é…ç½® Nginxä½œä¸ºåå‘ä»£ç† Nginx httpæœåŠ¡å™¨ é…ç½®Nginxå…è´¹è¯ä¹¦ Keepalived Keepalivedç®€ä»‹ Keepalivedçš„å®‰è£…ä¸é…ç½® Keepalivedçš„ç›¸å…³æ“ä½œ Keepalivedçš„é…ç½®è¯¦è§£ Baremetal bmcæ¦‚å¿µ å·¥å…·æŠ€å·§ å¿«æ·é”® vscodeå¿«æ·é”® eclipseå¿«æ·é”® chromeå¿«æ·é”® tmuxå¿«æ·é”® iterm2 rzszçš„ä½¿ç”¨ IDEå·¥å…· vscodeé…ç½® Golandé…ç½® vim vimå‘½ä»¤ vimrcé…ç½® basic vimrc ","categories":"","description":"","excerpt":"Summary åºè¨€ åºè¨€ è®¡ç®— CPU\nå†…å­˜\nå­˜å‚¨ Linux æ–‡ä»¶ç³»ç»Ÿ Linux ä»‹ç» æ–‡ä»¶ç³»ç»Ÿ æ–‡ä»¶å­˜å‚¨ç»“æ„ æ–‡ä»¶æƒé™ ç£ç›˜ LVM â€¦","ref":"/linux-notes/summary/","tags":"","title":""},{"body":" About åšå®¢è®°å½• 2016å¹´ï¼š ä½¿ç”¨CSDNåšå®¢ï¼Œç½‘å€ï¼šhttps://blog.csdn.net/huwh_ 2017å¹´ï¼š å¼€å§‹ç ”ç©¶é™æ€ç½‘ç«™æŠ€æœ¯ï¼Œé‡‡ç”¨hexoæ¥æ­å»ºä¸ªäººåšå®¢ç½‘ç«™ã€‚ å¹¶ä¸”å¼€å§‹åŸºäºYuHsuançš„åšå®¢ä¸»é¢˜ä¼˜åŒ–äº†è‡ªå·±ä¸ªäººçš„åšå®¢ä¸»é¢˜hexo-theme-huweihuangï¼Œå¹¶å°†è¯¥ä¸»é¢˜åº”ç”¨åˆ°ä¸ªäººåšå®¢www.huweihuang.comç½‘ç«™ä¸Šã€‚ åŒæ—¶è¯¥åšå®¢ä¸»é¢˜ä¹Ÿè¢«å¤šäººç”¨äºè‡ªå·±çš„åšå®¢ä¸Šã€‚ 2018å¹´ï¼š ä¸ºäº†ä¾¿äºå°†ç¬”è®°æ•´ç†æˆçŸ¥è¯†ä½“ç³»ï¼Œå¼€å§‹ä½¿ç”¨Gitbookæ¥ç®¡ç†ç¬”è®°ï¼Œå…ˆåå‘å¸ƒäº†kubernetes-notesï¼Œgolang-notesï¼Œlinux-notesç­‰ã€‚ 2022å¹´ï¼š ç”±äºGitbookå®˜æ–¹ç½‘ç«™ä¸»é¢˜æ²¡æœ‰å¼€æºï¼Œä½¿ç”¨Gitbookå®˜æ–¹ç½‘ç«™æ¥å¢åŠ æ–‡æ¡£ç¾è§‚æ€§ï¼šk8s.huweihuang.comï¼Œä½†æ˜¯å®šåˆ¶æ€§å—é™ã€‚ å¼€å§‹ç ”ç©¶hugoé™æ€ç½‘ç«™æŠ€æœ¯ï¼Œä½¿ç”¨googleå¼€æºçš„æ–‡æ¡£ä¸»é¢˜docsyï¼Œå¹¶å‘å¸ƒæ–°çš„åšå®¢åœ°å€ï¼šblog.huweihuang.comã€‚ å¾®ä¿¡å…¬ä¼—å· æœç´¢ï¼šå®¹å™¨äº‘æ¶æ„\n","categories":"","description":"","excerpt":" About åšå®¢è®°å½• 2016å¹´ï¼š ä½¿ç”¨CSDNåšå®¢ï¼Œç½‘å€ï¼šhttps://blog.csdn.net/huwh_ 2017å¹´ï¼š å¼€å§‹ç ”ç©¶ â€¦","ref":"/about/","tags":"","title":"About"},{"body":"1. å®‰è£… ä»¥centosä¸ºä¾‹ã€‚\nyum install -y ansible 2. é…ç½® é»˜è®¤é…ç½®ç›®å½•åœ¨/etc/ansible/ï¼Œä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ä¸ªé…ç½®ï¼š\nansible.cfgï¼šansibleçš„é…ç½®æ–‡ä»¶ hostsï¼šé…ç½®ansibleæ‰€è¿æ¥çš„æœºå™¨IPä¿¡æ¯ 2.1. ansible.cfg 2.2. hosts # This is the default ansible 'hosts' file. # # It should live in /etc/ansible/hosts # # - Comments begin with the '#' character # - Blank lines are ignored # - Groups of hosts are delimited by [header] elements # - You can enter hostnames or ip addresses # - A hostname/ip can be a member of multiple groups # Ex 1: Ungrouped hosts, specify before any group headers. # green.example.com # blue.example.com # 192.168.100.1 # 192.168.100.10 # Ex 2: A collection of hosts belonging to the 'webservers' group # [webservers] # alpha.example.org # beta.example.org # 192.168.1.100 # 192.168.1.110 # If you have multiple hosts following a pattern you can specify # them like this: # www[001:006].example.com # Ex 3: A collection of database servers in the 'dbservers' group # [dbservers] # # db01.intranet.mydomain.net # db02.intranet.mydomain.net # 10.25.1.56 # 10.25.1.57 # Here's another example of host ranges, this time there are no # leading 0s: # db-[99:101]-node.example.com [k8s] 192.168.201.52 192.168.201.53 192.168.201.54 192.168.201.55 192.168.201.56 192.168.201.57 # password setting [all:vars] ansible_connection=ssh ansible_ssh_user=root ansible_ssh_pass=xxx 3. ansibleçš„å‘½ä»¤ å‘½ä»¤æ ¼å¼ä¸ºï¼šansible [options]\nhost-patternï¼šå³hostsæ–‡ä»¶ä¸­é…ç½®çš„é›†ç¾¤åç§° optionsï¼šå‘½ä»¤æ“ä½œç¬¦ ä¾‹å¦‚ï¼šansible k8s -a 'uname -r'\n[root@k8s-master ansible]# ansible k8s -a 'uname -r' 172.16.201.56 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.55 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.54 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.53 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.52 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 172.16.201.57 | SUCCESS | rc=0 \u003e\u003e 4.16.11-1.el7.elrepo.x86_64 å…·ä½“çš„å‘½ä»¤ä¿¡æ¯ï¼š\nUsage: ansible \u003chost-pattern\u003e [options] Define and run a single task 'playbook' against a set of hosts Options: -a MODULE_ARGS, --args=MODULE_ARGS module arguments --ask-vault-pass ask for vault password -B SECONDS, --background=SECONDS run asynchronously, failing after X seconds (default=N/A) -C, --check don't make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON, if filename prepend with @ -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY specify inventory host path or comma separated host list. --inventory-file is deprecated -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else -m MODULE_NAME, --module-name=MODULE_NAME module name to execute (default=command) -M MODULE_PATH, --module-path=MODULE_PATH prepend colon-separated path(s) to module library (default=[u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']) -o, --one-line condense output --playbook-dir=BASEDIR Since this tool does not use playbooks, use this as a subsitute playbook directory.This sets the relative path for many features including roles/ group_vars/ etc. -P POLL_INTERVAL, --poll=POLL_INTERVAL set the poll interval if using -B (default=15) --syntax-check perform a syntax check on the playbook, but do not execute it -t TREE, --tree=TREE log output to this directory --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=None) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password Some modules do not make sense in Ad-Hoc (include, meta, etc) 4. ansible-playbook Usage: ansible-playbook [options] playbook.yml [playbook2 ...] Runs Ansible playbooks, executing the defined tasks on the targeted hosts. Options: --ask-vault-pass ask for vault password -C, --check don't make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON, if filename prepend with @ --flush-cache clear the fact cache for every host in inventory --force-handlers run handlers even if a task fails -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY specify inventory host path or comma separated host list. --inventory-file is deprecated -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else --list-tags list all available tags --list-tasks list all tasks that would be executed -M MODULE_PATH, --module-path=MODULE_PATH prepend colon-separated path(s) to module library (default=[u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']) --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values --start-at-task=START_AT_TASK start the playbook at the task matching this name --step one-step-at-a-time: confirm each task before running --syntax-check perform a syntax check on the playbook, but do not execute it -t TAGS, --tags=TAGS only run plays and tasks tagged with these values --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=None) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password ","categories":"","description":"","excerpt":"1. å®‰è£… ä»¥centosä¸ºä¾‹ã€‚\nyum install -y ansible 2. é…ç½® é»˜è®¤é…ç½®ç›®å½•åœ¨/etc/ansible/ï¼Œä¸»è¦æœ‰ â€¦","ref":"/linux-notes/tools/ansible-usage/","tags":["Linux"],"title":"ansibleçš„ä½¿ç”¨"},{"body":" ä»¥ä¸‹è½¬è‡ªhttps://github.com/amix/vimrc/blob/master/vimrcs/basic.vim\nbasic vimrc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Maintainer: \" Amir Salihefendic â€” @amix3k \" \" Awesome_version: \" Get this config, nice color schemes and lots of plugins! \" \" Install the awesome version from: \" \" https://github.com/amix/vimrc \" \" Sections: \" -\u003e General \" -\u003e VIM user interface \" -\u003e Colors and Fonts \" -\u003e Files and backups \" -\u003e Text, tab and indent related \" -\u003e Visual mode related \" -\u003e Moving around, tabs and buffers \" -\u003e Status line \" -\u003e Editing mappings \" -\u003e vimgrep searching and cope displaying \" -\u003e Spell checking \" -\u003e Misc \" -\u003e Helper functions \" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e General \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Sets how many lines of history VIM has to remember set history=500 \" Enable filetype plugins filetype plugin on filetype indent on \" Set to auto read when a file is changed from the outside set autoread \" With a map leader it's possible to do extra key combinations \" like \u003cleader\u003ew saves the current file let mapleader = \",\" \" Fast saving nmap \u003cleader\u003ew :w!\u003ccr\u003e \" :W sudo saves the file \" (useful for handling the permission-denied error) command W w !sudo tee % \u003e /dev/null \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e VIM user interface \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Set 7 lines to the cursor - when moving vertically using j/k set so=7 \" Avoid garbled characters in Chinese language windows OS let $LANG='en' set langmenu=en source $VIMRUNTIME/delmenu.vim source $VIMRUNTIME/menu.vim \" Turn on the Wild menu set wildmenu \" Ignore compiled files set wildignore=*.o,*~,*.pyc if has(\"win16\") || has(\"win32\") set wildignore+=.git\\*,.hg\\*,.svn\\* else set wildignore+=*/.git/*,*/.hg/*,*/.svn/*,*/.DS_Store endif \"Always show current position set ruler \" Height of the command bar set cmdheight=2 \" A buffer becomes hidden when it is abandoned set hid \" Configure backspace so it acts as it should act set backspace=eol,start,indent set whichwrap+=\u003c,\u003e,h,l \" Ignore case when searching set ignorecase \" When searching try to be smart about cases set smartcase \" Highlight search results set hlsearch \" Makes search act like search in modern browsers set incsearch \" Don't redraw while executing macros (good performance config) set lazyredraw \" For regular expressions turn magic on set magic \" Show matching brackets when text indicator is over them set showmatch \" How many tenths of a second to blink when matching brackets set mat=2 \" No annoying sound on errors set noerrorbells set novisualbell set t_vb= set tm=500 \" Properly disable sound on errors on MacVim if has(\"gui_macvim\") autocmd GUIEnter * set vb t_vb= endif \" Add a bit extra margin to the left set foldcolumn=1 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Colors and Fonts \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Enable syntax highlighting syntax enable \" Enable 256 colors palette in Gnome Terminal if $COLORTERM == 'gnome-terminal' set t_Co=256 endif try colorscheme desert catch endtry set background=dark \" Set extra options when running in GUI mode if has(\"gui_running\") set guioptions-=T set guioptions-=e set t_Co=256 set guitablabel=%M\\ %t endif \" Set utf8 as standard encoding and en_US as the standard language set encoding=utf8 \" Use Unix as the standard file type set ffs=unix,dos,mac \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Files, backups and undo \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Turn backup off, since most stuff is in SVN, git et.c anyway... set nobackup set nowb set noswapfile \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Text, tab and indent related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Use spaces instead of tabs set expandtab \" Be smart when using tabs ;) set smarttab \" 1 tab == 4 spaces set shiftwidth=4 set tabstop=4 \" Linebreak on 500 characters set lbr set tw=500 set ai \"Auto indent set si \"Smart indent set wrap \"Wrap lines \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Visual mode related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Visual mode pressing * or # searches for the current selection \" Super useful! From an idea by Michael Naumann vnoremap \u003csilent\u003e * :\u003cC-u\u003ecall VisualSelection('', '')\u003cCR\u003e/\u003cC-R\u003e=@/\u003cCR\u003e\u003cCR\u003e vnoremap \u003csilent\u003e # :\u003cC-u\u003ecall VisualSelection('', '')\u003cCR\u003e?\u003cC-R\u003e=@/\u003cCR\u003e\u003cCR\u003e \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Moving around, tabs, windows and buffers \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Map \u003cSpace\u003e to / (search) and Ctrl-\u003cSpace\u003e to ? (backwards search) map \u003cspace\u003e / map \u003cc-space\u003e ? \" Disable highlight when \u003cleader\u003e\u003ccr\u003e is pressed map \u003csilent\u003e \u003cleader\u003e\u003ccr\u003e :noh\u003ccr\u003e \" Smart way to move between windows map \u003cC-j\u003e \u003cC-W\u003ej map \u003cC-k\u003e \u003cC-W\u003ek map \u003cC-h\u003e \u003cC-W\u003eh map \u003cC-l\u003e \u003cC-W\u003el \" Close the current buffer map \u003cleader\u003ebd :Bclose\u003ccr\u003e:tabclose\u003ccr\u003egT \" Close all the buffers map \u003cleader\u003eba :bufdo bd\u003ccr\u003e map \u003cleader\u003el :bnext\u003ccr\u003e map \u003cleader\u003eh :bprevious\u003ccr\u003e \" Useful mappings for managing tabs map \u003cleader\u003etn :tabnew\u003ccr\u003e map \u003cleader\u003eto :tabonly\u003ccr\u003e map \u003cleader\u003etc :tabclose\u003ccr\u003e map \u003cleader\u003etm :tabmove map \u003cleader\u003et\u003cleader\u003e :tabnext \" Let 'tl' toggle between this and the last accessed tab let g:lasttab = 1 nmap \u003cLeader\u003etl :exe \"tabn \".g:lasttab\u003cCR\u003e au TabLeave * let g:lasttab = tabpagenr() \" Opens a new tab with the current buffer's path \" Super useful when editing files in the same directory map \u003cleader\u003ete :tabedit \u003cc-r\u003e=expand(\"%:p:h\")\u003ccr\u003e/ \" Switch CWD to the directory of the open buffer map \u003cleader\u003ecd :cd %:p:h\u003ccr\u003e:pwd\u003ccr\u003e \" Specify the behavior when switching between buffers try set switchbuf=useopen,usetab,newtab set stal=2 catch endtry \" Return to last edit position when opening files (You want this!) au BufReadPost * if line(\"'\\\"\") \u003e 1 \u0026\u0026 line(\"'\\\"\") \u003c= line(\"$\") | exe \"normal! g'\\\"\" | endif \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Status line \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Always show the status line set laststatus=2 \" Format the status line set statusline=\\ %{HasPaste()}%F%m%r%h\\ %w\\ \\ CWD:\\ %r%{getcwd()}%h\\ \\ \\ Line:\\ %l\\ \\ Column:\\ %c \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Editing mappings \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Remap VIM 0 to first non-blank character map 0 ^ \" Move a line of text using ALT+[jk] or Command+[jk] on mac nmap \u003cM-j\u003e mz:m+\u003ccr\u003e`z nmap \u003cM-k\u003e mz:m-2\u003ccr\u003e`z vmap \u003cM-j\u003e :m'\u003e+\u003ccr\u003e`\u003cmy`\u003emzgv`yo`z vmap \u003cM-k\u003e :m'\u003c-2\u003ccr\u003e`\u003emy`\u003cmzgv`yo`z if has(\"mac\") || has(\"macunix\") nmap \u003cD-j\u003e \u003cM-j\u003e nmap \u003cD-k\u003e \u003cM-k\u003e vmap \u003cD-j\u003e \u003cM-j\u003e vmap \u003cD-k\u003e \u003cM-k\u003e endif \" Delete trailing white space on save, useful for some filetypes ;) fun! CleanExtraSpaces() let save_cursor = getpos(\".\") let old_query = getreg('/') silent! %s/\\s\\+$//e call setpos('.', save_cursor) call setreg('/', old_query) endfun if has(\"autocmd\") autocmd BufWritePre *.txt,*.js,*.py,*.wiki,*.sh,*.coffee :call CleanExtraSpaces() endif \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Spell checking \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Pressing ,ss will toggle and untoggle spell checking map \u003cleader\u003ess :setlocal spell!\u003ccr\u003e \" Shortcuts using \u003cleader\u003e map \u003cleader\u003esn ]s map \u003cleader\u003esp [s map \u003cleader\u003esa zg map \u003cleader\u003es? z= \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Misc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Remove the Windows ^M - when the encodings gets messed up noremap \u003cLeader\u003em mmHmt:%s/\u003cC-V\u003e\u003ccr\u003e//ge\u003ccr\u003e'tzt'm \" Quickly open a buffer for scribble map \u003cleader\u003eq :e ~/buffer\u003ccr\u003e \" Quickly open a markdown buffer for scribble map \u003cleader\u003ex :e ~/buffer.md\u003ccr\u003e \" Toggle paste mode on and off map \u003cleader\u003epp :setlocal paste!\u003ccr\u003e \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" =\u003e Helper functions \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Returns true if paste mode is enabled function! HasPaste() if \u0026paste return 'PASTE MODE ' endif return '' endfunction \" Don't close window, when deleting a buffer command! Bclose call \u003cSID\u003eBufcloseCloseIt() function! \u003cSID\u003eBufcloseCloseIt() let l:currentBufNum = bufnr(\"%\") let l:alternateBufNum = bufnr(\"#\") if buflisted(l:alternateBufNum) buffer # else bnext endif if bufnr(\"%\") == l:currentBufNum new endif if buflisted(l:currentBufNum) execute(\"bdelete! \".l:currentBufNum) endif endfunction function! CmdLine(str) call feedkeys(\":\" . a:str) endfunction function! VisualSelection(direction, extra_filter) range let l:saved_reg = @\" execute \"normal! vgvy\" let l:pattern = escape(@\", \"\\\\/.*'$^~[]\") let l:pattern = substitute(l:pattern, \"\\n$\", \"\", \"\") if a:direction == 'gv' call CmdLine(\"Ack '\" . l:pattern . \"' \" ) elseif a:direction == 'replace' call CmdLine(\"%s\" . '/'. l:pattern . '/') endif let @/ = l:pattern let @\" = l:saved_reg endfunction å‚è€ƒ\nhttps://github.com/amix/vimrc ","categories":"","description":"","excerpt":" ä»¥ä¸‹è½¬è‡ªhttps://github.com/amix/vimrc/blob/master/vimrcs/basic.vim\nbasic â€¦","ref":"/linux-notes/ide/vim/basic-vimrc/","tags":["VIM"],"title":"basic vimrc"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"1. å®‰è£…ceph-fuse yum install -y ceph-fuse å¦‚æœå®‰è£…å¤±è´¥ï¼Œå…ˆæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå†æ‰§è¡Œä¸Šè¿°å®‰è£…å‘½ä»¤\nyum -y install epel-release rpm -Uhv http://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm 2. é…ç½®å®¢æˆ·ç«¯è®¿é—®çš„key mkdir /etc/ceph/ vi /etc/ceph/ceph.client.admin.keyring\n[client.admin] key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx== 3. ceph-fuse æŒ‚è½½ ceph-fuse -m \u003cmons_IP1\u003e:6789,\u003cmons_IP2\u003e:6789,\u003cmons_IP3\u003e:6789 -r \u003ccephé›†ç¾¤ä¸­çš„ç›®å½•\u003e \u003cå®¿ä¸»æœºç›®å½•\u003e -o nonempty ä¾‹å¦‚ï¼š\n# ceph-fuse -m 192.168.18.3:6789,192.168.18.4:6789,192.168.18.5:6789 -r /pvc-volumes /root/cephfsdir -o nonempty 2019-03-27 17:58:04.435985 7fc61b67cec0 -1 did not load config file, using default settings. ceph-fuse[18051]: starting ceph client 2019-03-27 17:58:04.469144 7fc61b67cec0 -1 init, newargv = 0x55cecaba81c0 newargc=13 ceph-fuse[18051]: starting fuse 4. æŸ¥çœ‹æ˜¯å¦æŒ‚è½½æˆåŠŸ # df -h Filesystem Size Used Avail Use% Mounted on ... ceph-fuse 1.6T 8.8G 1.6T 1% /root/cephfsdir 5. ceph-fuseå‘½ä»¤è¯´æ˜ # ceph-fuse --help 2019-03-27 18:01:16.421376 7fae11998ec0 -1 did not load config file, using default settings. usage: ceph-fuse [-m mon-ip-addr:mon-port] \u003cmount point\u003e [OPTIONS] --client_mountpoint/-r \u003croot_directory\u003e use root_directory as the mounted root, rather than the full Ceph tree. usage: ceph-fuse mountpoint [options] general options: -o opt,[opt...] mount options -h --help print help -V --version print version FUSE options: -d -o debug enable debug output (implies -f) -f foreground operation -s disable multi-threaded operation --conf/-c FILE read configuration from the given configuration file --id/-i ID set ID portion of my name --name/-n TYPE.ID set name --cluster NAME set cluster name (default: ceph) --setuser USER set uid to user or uid (and gid to user's gid) --setgroup GROUP set gid to group or gid --version show version and quit ","categories":"","description":"","excerpt":"1. å®‰è£…ceph-fuse yum install -y ceph-fuse å¦‚æœå®‰è£…å¤±è´¥ï¼Œå…ˆæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå†æ‰§è¡Œä¸Šè¿°å®‰è£…å‘½ä»¤\nyum â€¦","ref":"/linux-notes/tools/ceph-fuse/","tags":["Linux"],"title":"ceph-fuseçš„ä½¿ç”¨"},{"body":" confdçš„æºç å‚è€ƒï¼šhttps://github.com/kelseyhightower/confd\n1. confdçš„éƒ¨ç½² ä»¥ä¸‹Linuxç³»ç»Ÿä¸ºä¾‹ã€‚\nä¸‹è½½confdçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¸‹è½½åœ°å€ä¸ºï¼šhttps://github.com/kelseyhightower/confd/releasesã€‚ä¾‹å¦‚ï¼š\n# Download the binary wget https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64 # é‡å‘½åäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå¹¶ç§»åŠ¨åˆ°PATHçš„ç›®å½•ä¸‹ mv confd-0.16.0-linux-amd64 /usr/local/bin/confd chmod +x /usr/local/bin/confd # éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸ confd --help 2. confdçš„é…ç½® Confdé€šè¿‡è¯»å–åç«¯å­˜å‚¨çš„é…ç½®ä¿¡æ¯æ¥åŠ¨æ€æ›´æ–°å¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼Œå¯¹åº”çš„åç«¯å­˜å‚¨å¯ä»¥æ˜¯etcdï¼Œredisç­‰ï¼Œå…¶ä¸­etcdçš„v3ç‰ˆæœ¬å¯¹åº”çš„å­˜å‚¨åç«¯ä¸ºetcdv3ã€‚\n2.1. confd.toml confd.tomlä¸ºconfdæœåŠ¡æœ¬èº«çš„é…ç½®æ–‡ä»¶ï¼Œä¸»è¦è®°å½•äº†ä½¿ç”¨çš„å­˜å‚¨åç«¯ã€åè®®ã€confdirç­‰å‚æ•°ã€‚\nç¤ºä¾‹ï¼š\nå­˜å‚¨åç«¯etcdv3ï¼š backend = \"etcdv3\" confdir = \"/etc/confd\" log-level = \"debug\" interval = 5 nodes = [ \"http://192.168.10.4:12379\", ] scheme = \"http\" watch = true å…¶ä¸­watchå‚æ•°è¡¨ç¤ºå®æ—¶ç›‘å¬åç«¯å­˜å‚¨çš„å˜åŒ–ï¼Œå¦‚æœ‰å˜åŒ–åˆ™æ›´æ–°confdç®¡ç†çš„é…ç½®ã€‚\nå­˜å‚¨åç«¯ä¸ºredis backend = \"redis\" confdir = \"/etc/confd\" log-level = \"debug\" interval = 1 # é—´éš” 1 ç§’åŒæ­¥ä¸€æ¬¡é…ç½®æ–‡ä»¶ nodes = [ \"127.0.0.1:6379\", ] scheme = \"http\" client_key = \"123456\" # redisçš„å¯†ç ï¼Œä¸æ˜¯ password å‚æ•° #watch = true å¦‚æœæ²¡æœ‰å¯åŠ¨watchå‚æ•°ï¼Œåˆ™ä¼šä¾æ®intervalå‚æ•°å®šæœŸå»rediså­˜å‚¨åç«¯æ‹¿å–æ•°æ®ï¼Œå¹¶æ¯”è¾ƒä¸å½“å‰é…ç½®æ•°æ®æ˜¯å¦æœ‰å˜åŒ–ï¼ˆä¸»è¦æ¯”è¾ƒmd5å€¼ï¼‰ï¼Œå¦‚æœæœ‰å˜åŒ–åˆ™æ›´æ–°é…ç½®ï¼Œæ²¡æœ‰å˜åŒ–åˆ™å®šæœŸå†å»æ‹¿å–æ•°æ®ï¼Œä»¥æ­¤å¾ªç¯ã€‚\nå¦‚æœå¯åŠ¨äº†watchå‚æ•°ï¼Œåˆ™ä¿®æ”¹rediså­˜å‚¨æ•°æ®çš„åŒæ—¶ï¼Œè¿˜è¦æ‰§è¡Œpublishçš„æ“ä½œï¼Œä¿ƒä½¿confdå»è§¦å‘æ¯”è¾ƒé…ç½®å¹¶æ›´æ–°é…ç½®çš„æ“ä½œã€‚\npublishçš„å‘½ä»¤æ ¼å¼å¦‚ä¸‹:\npublish __keyspace@0__:{prefix}/{key} set(or del) 2.2. åˆ›å»ºconfdir confdiråº•ä¸‹åŒ…å«ä¸¤ä¸ªç›®å½•:\nconf.d:confdçš„é…ç½®æ–‡ä»¶ï¼Œä¸»è¦åŒ…å«é…ç½®çš„ç”Ÿæˆé€»è¾‘ï¼Œä¾‹å¦‚æ¨¡æ¿æºï¼Œåç«¯å­˜å‚¨å¯¹åº”çš„keysï¼Œå‘½ä»¤æ‰§è¡Œç­‰ã€‚ templates:é…ç½®æ¨¡æ¿Templateï¼Œå³åŸºäºä¸åŒç»„ä»¶çš„é…ç½®ï¼Œä¿®æ”¹ä¸ºç¬¦åˆÂ Golang text templatesçš„æ¨¡æ¿æ–‡ä»¶ã€‚ sudo mkdir -p /etc/confd/{conf.d,templates} 2.2.1. Template Resources æ¨¡æ¿æºé…ç½®æ–‡ä»¶æ˜¯TOMLæ ¼å¼çš„æ–‡ä»¶ï¼Œä¸»è¦åŒ…å«é…ç½®çš„ç”Ÿæˆé€»è¾‘ï¼Œä¾‹å¦‚æ¨¡æ¿æºï¼Œåç«¯å­˜å‚¨å¯¹åº”çš„keysï¼Œå‘½ä»¤æ‰§è¡Œç­‰ã€‚é»˜è®¤ç›®å½•åœ¨/etc/confd/conf.dã€‚\nå‚æ•°è¯´æ˜ï¼š\nå¿…è¦å‚æ•°\ndest (string) - The target file. keys (array of strings) - An array of keys. src (string) - The relative path of a configuration template. å¯é€‰å‚æ•°\ngid (int) - The gid that should own the file. Defaults to the effective gid. mode (string) - The permission mode of the file. uid (int) - The uid that should own the file. Defaults to the effective uid. reload_cmd (string) - The command to reload config. check_cmd (string) - The command to check config. Use {{src}} to reference the rendered source template. prefix (string) - The string to prefix to keys. ä¾‹å­\nä¾‹å¦‚ï¼š/etc/confd/conf.d/myapp-nginx.toml\n[template] prefix = \"/myapp\" src = \"nginx.tmpl\" dest = \"/tmp/myapp.conf\" owner = \"nginx\" mode = \"0644\" keys = [ \"/services/web\" ] check_cmd = \"/usr/sbin/nginx -t -c {{.src}}\" reload_cmd = \"/usr/sbin/service nginx reload\" 2.2.2. Template Templateå®šä¹‰äº†å•ä¸€åº”ç”¨é…ç½®çš„æ¨¡æ¿ï¼Œé»˜è®¤å­˜å‚¨åœ¨/etc/confd/templatesç›®å½•ä¸‹ï¼Œæ¨¡æ¿æ–‡ä»¶ç¬¦åˆGoçš„text/templateæ ¼å¼ã€‚\næ¨¡æ¿æ–‡ä»¶å¸¸ç”¨å‡½æ•°æœ‰baseï¼Œgetï¼Œgetsï¼Œlsdirï¼Œjsonç­‰ã€‚å…·ä½“å¯å‚è€ƒhttps://github.com/kelseyhightower/confd/blob/master/docs/templates.mdã€‚\nä¾‹å­ï¼š\n/etc/confd/templates/nginx.tmpl\n{{range $dir := lsdir \"/services/web\"}} upstream {{base $dir}} { {{$custdir := printf \"/services/web/%s/*\" $dir}}{{range gets $custdir}} server {{$data := json .Value}}{{$data.IP}}:80; {{end}} } server { server_name {{base $dir}}.example.com; location / { proxy_pass {{base $dir}}; } } {{end}} 3. åˆ›å»ºåç«¯å­˜å‚¨çš„é…ç½®æ•°æ® ä»¥etcdv3å­˜å‚¨ä¸ºä¾‹ï¼Œåœ¨etcdä¸­åˆ›å»ºä»¥ä¸‹æ•°æ®ã€‚\netcdctl --endpoints=$endpoints put /services/web/cust1/2 '{\"IP\": \"10.0.0.2\"}' etcdctl --endpoints=$endpoints put /services/web/cust2/2 '{\"IP\": \"10.0.0.4\"}' etcdctl --endpoints=$endpoints put /services/web/cust2/1 '{\"IP\": \"10.0.0.3\"}' etcdctl --endpoints=$endpoints put /services/web/cust1/1 '{\"IP\": \"10.0.0.1\"}' 4. å¯åŠ¨confdçš„æœåŠ¡ confdæ”¯æŒä»¥daemonæˆ–è€…onetimeä¸¤ç§æ¨¡å¼è¿è¡Œï¼Œå½“ä»¥daemonæ¨¡å¼è¿è¡Œæ—¶ï¼Œconfdä¼šç›‘å¬åç«¯å­˜å‚¨çš„é…ç½®å˜åŒ–ï¼Œå¹¶æ ¹æ®é…ç½®æ¨¡æ¿åŠ¨æ€ç”Ÿæˆç›®æ ‡é…ç½®æ–‡ä»¶ã€‚\nconfdå¯ä»¥ä½¿ç”¨-config-fileå‚æ•°æ¥æŒ‡å®šconfdçš„é…ç½®æ–‡ä»¶ï¼Œè€Œå°†å…¶ä»–å‚æ•°å†™åœ¨é…ç½®æ–‡ä»¶ä¸­ã€‚\n/usr/local/bin/confd -config-file /etc/confd/conf/confd.toml å¦‚æœä»¥daemonæ¨¡å¼è¿è¡Œï¼Œåœ¨å‘½ä»¤åé¢æ·»åŠ \u0026ç¬¦å·ï¼Œä¾‹å¦‚ï¼š\nconfd -watch -backend etcdv3 -node http://172.16.5.4:12379 \u0026 ä»¥ä¸‹ä»¥onetimeæ¨¡å¼è¿è¡Œä¸ºä¾‹ã€‚å…¶ä¸­å¯¹åº”çš„åç«¯å­˜å‚¨ç±»å‹æ˜¯etcdv3ã€‚\n# æ‰§è¡Œå‘½ä»¤ confd -onetime -backend etcdv3 -node http://172.16.5.4:12379 # output 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Backend set to etcdv3 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Starting confd 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Backend source(s) set to http://172.16.5.4:12379 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO /root/myapp/twemproxy/conf/twemproxy.conf has md5sum 6f0f43abede612c75cb840a4840fbea3 should be 32f48664266e3fd6b56ee73a314ee272 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Target config /root/myapp/twemproxy/conf/twemproxy.conf out of sync 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Target config /root/myapp/twemproxy/conf/twemproxy.conf has been updated 5. æŸ¥çœ‹ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ åœ¨/etc/confd/conf.d/myapp-nginx.tomlä¸­å®šä¹‰çš„é…ç½®æ–‡ä»¶çš„ç”Ÿæˆè·¯å¾„ä¸º/tmp/myapp.confã€‚\n[root@k8s-dbg-master-1 dest]# cat myapp.conf upstream cust1 { server 10.0.0.1:80; server 10.0.0.2:80; } server { server_name cust1.example.com; location / { proxy_pass cust1; } } upstream cust2 { server 10.0.0.3:80; server 10.0.0.4:80; } server { server_name cust2.example.com; location / { proxy_pass cust2; } } 6. confdåŠ¨æ€æ›´æ–°twemproxy 6.1. twemproxy.toml confdçš„æ¨¡æ¿æºæ–‡ä»¶é…ç½®ï¼š/etc/confd/conf.d/twemproxy.toml\n[template] src = \"twemproxy.tmpl\" dest = \"/root/myapp/twemproxy/conf/twemproxy.conf\" keys = [ \"/twemproxy/pool\" ] check_cmd = \"/usr/local/bin/nutcracker -t -c /root/myapp/twemproxy/conf/twemproxy.conf\" reload_cmd = \"bash /root/myapp/twemproxy/reload.sh\" 6.2. twemproxy.tmpl æ¨¡æ¿æ–‡ä»¶ï¼š/etc/confd/templates/twemproxy.tmpl\nglobal: worker_processes: 4 # å¹¶å‘è¿›ç¨‹æ•°, å¦‚æœä¸º0, è¿™ fallback å›åŸæ¥çš„å•è¿›ç¨‹æ¨¡å‹(ä¸æ”¯æŒ config reload!) user: nobody # worker è¿›ç¨‹çš„ç”¨æˆ·, é»˜è®¤ nobody. åªè¦ä¸»è¿›ç¨‹æ˜¯ root ç”¨æˆ·å¯åŠ¨æ‰ç”Ÿæ•ˆ. group: nobody # worker è¿›ç¨‹çš„ç”¨æˆ·ç»„ worker_shutdown_timeout: 30 # å•ä½ä¸ºç§’. ç”¨äº reload è¿‡ç¨‹ä¸­åœ¨æ”¹æ—¶é—´æ®µä¹‹åå¼ºåˆ¶é€€å‡ºæ—§çš„ worker è¿›ç¨‹. pools: {{range gets \"/twemproxy/pool/*\"}} {{base .Key}}: {{$pool := json .Value}} listen: {{$pool.ListenAddr.IP}}:{{$pool.ListenAddr.Port}} hash: fnv1a_64 # é€‰æ‹©å®ä¾‹çš„ hash è§„åˆ™ distribution: ketama auto_eject_hosts: true # server æœ‰é—®é¢˜æ˜¯å¦å‰”é™¤ redis: true # æ˜¯å¦ä¸º Redis åè®® {{if $pool.Password}}redis_auth: {{$pool.Password}}{{end}} server_retry_timeout: 5000 # è¢«å‰”é™¤å¤šé•¿æ—¶é—´åä¼šé‡è¯• server_connections: 25 # NOTE: server è¿æ¥æ± çš„å¤§å°, é»˜è®¤ä¸º 1, å»ºè®®è°ƒæ•´ server_failure_limit: 5 # å¤±è´¥å¤šå°‘æ¬¡åæš‚æ—¶å‰”é™¤ timeout: 1000 # Server è¶…æ—¶æ—¶é—´, 1 sec backlog: 1024 # è¿æ¥é˜Ÿåˆ—å¤§å° preconnect: true # é¢„è¿æ¥å¤§å° servers:{{range $server := $pool.Servers}} - {{$server.IP}}:{{$server.Port}}:1 {{if $server.Master}}master{{end}} {{end}} {{end}} 6.3. etcdä¸­çš„é…ç½®æ ¼å¼ etcdä¸­çš„é…ç½®é€šè¿‡ä¸€ä¸ªmapæ¥å®šä¹‰ä¸ºå®Œæ•´çš„é…ç½®å†…å®¹ã€‚å…¶ä¸­keyæ˜¯twemproxyä¸­poolçš„åç§°ï¼Œvalueæ˜¯poolçš„æ‰€æœ‰å†…å®¹ã€‚\né…ç½®å¯¹åº”goç»“æ„ä½“å¦‚ä¸‹ï¼š\ntype Pool struct{ ListenAddr ListenAddr `json:\"ListenAddr,omitempty\"` Servers []Server `json:\"Servers,omitempty\"` Password string `json:\"Password,omitempty\"` } type ListenAddr struct { IP string `json:\"IP,omitempty\"` Port string `json:\"Port,omitempty\"` } type Server struct { IP string `json:\"IP,omitempty\"` Port string `json:\"Port,omitempty\"` Master bool `json:\"Master,omitempty\"` } é…ç½®å¯¹åº”JSONæ ¼å¼å¦‚ä¸‹ï¼š\n{ \"ListenAddr\": { \"IP\": \"192.168.5.7\", \"Port\": \"22225\" }, \"Servers\": [ { \"IP\": \"10.233.116.168\", \"Port\": \"6379\", \"Master\": true }, { \"IP\": \"10.233.110.207\", \"Port\": \"6379\", \"Master\": false } ], \"Password\": \"987654\" } 6.4. ç”Ÿæˆtwemproxyé…ç½®æ–‡ä»¶ global: worker_processes: 4 # å¹¶å‘è¿›ç¨‹æ•°, å¦‚æœä¸º0, è¿™ fallback å›åŸæ¥çš„å•è¿›ç¨‹æ¨¡å‹(ä¸æ”¯æŒ config reload!) user: nobody # worker è¿›ç¨‹çš„ç”¨æˆ·, é»˜è®¤ nobody. åªè¦ä¸»è¿›ç¨‹æ˜¯ root ç”¨æˆ·å¯åŠ¨æ‰ç”Ÿæ•ˆ. group: nobody # worker è¿›ç¨‹çš„ç”¨æˆ·ç»„ worker_shutdown_timeout: 30 # å•ä½ä¸ºç§’. ç”¨äº reload è¿‡ç¨‹ä¸­åœ¨æ”¹æ—¶é—´æ®µä¹‹åå¼ºåˆ¶é€€å‡ºæ—§çš„ worker è¿›ç¨‹. pools: redis1: listen: 192.168.5.7:22223 hash: fnv1a_64 # é€‰æ‹©å®ä¾‹çš„ hash è§„åˆ™ distribution: ketama auto_eject_hosts: true # server æœ‰é—®é¢˜æ˜¯å¦å‰”é™¤ redis: true # æ˜¯å¦ä¸º Redis åè®® redis_auth: 987654 server_retry_timeout: 5000 # è¢«å‰”é™¤å¤šé•¿æ—¶é—´åä¼šé‡è¯• server_connections: 25 # NOTE: server è¿æ¥æ± çš„å¤§å°, é»˜è®¤ä¸º 1, å»ºè®®è°ƒæ•´ server_failure_limit: 5 # å¤±è´¥å¤šå°‘æ¬¡åæš‚æ—¶å‰”é™¤ timeout: 1000 # Server è¶…æ—¶æ—¶é—´, 1 sec backlog: 1024 # è¿æ¥é˜Ÿåˆ—å¤§å° preconnect: true # é¢„è¿æ¥å¤§å° servers: - 10.233.116.169:6379:1 redis2: listen: 192.168.5.7:22224 hash: fnv1a_64 # é€‰æ‹©å®ä¾‹çš„ hash è§„åˆ™ distribution: ketama auto_eject_hosts: true # server æœ‰é—®é¢˜æ˜¯å¦å‰”é™¤ redis: true # æ˜¯å¦ä¸º Redis åè®® redis_auth: 987654 server_retry_timeout: 5000 # è¢«å‰”é™¤å¤šé•¿æ—¶é—´åä¼šé‡è¯• server_connections: 25 # NOTE: server è¿æ¥æ± çš„å¤§å°, é»˜è®¤ä¸º 1, å»ºè®®è°ƒæ•´ server_failure_limit: 5 # å¤±è´¥å¤šå°‘æ¬¡åæš‚æ—¶å‰”é™¤ timeout: 1000 # Server è¶…æ—¶æ—¶é—´, 1 sec backlog: 1024 # è¿æ¥é˜Ÿåˆ—å¤§å° preconnect: true # é¢„è¿æ¥å¤§å° servers: - 10.233.110.223:6379:1 master - 10.233.111.21:6379:1 7. confdçš„å‘½ä»¤ $ confd --help Usage of confd: -app-id string Vault app-id to use with the app-id backend (only used with -backend=vault and auth-type=app-id) -auth-token string Auth bearer token to use -auth-type string Vault auth backend type to use (only used with -backend=vault) -backend string backend to use (default \"etcd\") -basic-auth Use Basic Auth to authenticate (only used with -backend=consul and -backend=etcd) -client-ca-keys string client ca keys -client-cert string the client cert -client-key string the client key -confdir string confd conf directory (default \"/etc/confd\") -config-file string the confd config file (default \"/etc/confd/confd.toml\") -file value the YAML file to watch for changes (only used with -backend=file) -filter string files filter (only used with -backend=file) (default \"*\") -interval int backend polling interval (default 600) -keep-stage-file keep staged files -log-level string level which confd should log messages -node value list of backend nodes -noop only show pending changes -onetime run once and exit -password string the password to authenticate with (only used with vault and etcd backends) -path string Vault mount path of the auth method (only used with -backend=vault) -prefix string key path prefix -role-id string Vault role-id to use with the AppRole, Kubernetes backends (only used with -backend=vault and either auth-type=app-role or auth-type=kubernetes) -scheme string the backend URI scheme for nodes retrieved from DNS SRV records (http or https) (default \"http\") -secret-id string Vault secret-id to use with the AppRole backend (only used with -backend=vault and auth-type=app-role) -secret-keyring string path to armored PGP secret keyring (for use with crypt functions) -separator string the separator to replace '/' with when looking up keys in the backend, prefixed '/' will also be removed (only used with -backend=redis) -srv-domain string the name of the resource record -srv-record string the SRV record to search for backends nodes. Example: _etcd-client._tcp.example.com -sync-only sync without check_cmd and reload_cmd -table string the name of the DynamoDB table (only used with -backend=dynamodb) -user-id string Vault user-id to use with the app-id backend (only used with -backend=value and auth-type=app-id) -username string the username to authenticate as (only used with vault and etcd backends) -version print version and exit -watch enable watch support å‚è€ƒæ–‡ç« ï¼š\nhttps://github.com/kelseyhightower/confd/blob/master/docs/installation.md\nhttps://github.com/kelseyhightower/confd/blob/master/docs/quick-start-guide.md\nhttps://github.com/kelseyhightower/confd/blob/master/docs/template-resources.md\nhttps://github.com/kelseyhightower/confd/blob/master/docs/templates.md\n","categories":"","description":"","excerpt":" confdçš„æºç å‚è€ƒï¼šhttps://github.com/kelseyhightower/confd\n1. confdçš„éƒ¨ç½² ä»¥ â€¦","ref":"/linux-notes/tools/confd-usage/","tags":["Linux"],"title":"confdçš„ä½¿ç”¨"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/containerd/","tags":"","title":"Containerd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/csi/","tags":"","title":"CSI"},{"body":" èƒ¡ä¼Ÿç…Œä¸ªäººåšå®¢ Learn More Download Welcome to huweihuang's personal blog !\n","categories":"","description":"","excerpt":" èƒ¡ä¼Ÿç…Œä¸ªäººåšå®¢ Learn More Download Welcome to huweihuang's personal blog !\n","ref":"/","tags":"","title":"Goldydocs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ide/","tags":"","title":"IDE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubeadm/","tags":"","title":"kubeadm"},{"body":"1. NFSç®€ä»‹ NFSï¼Œæ˜¯Network File Systemçš„ç®€å†™ï¼Œå³ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿã€‚ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿæ˜¯FreeBSDæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿä¸­çš„ä¸€ç§ï¼Œä¹Ÿè¢«ç§°ä¸ºNFS. NFSå…è®¸ä¸€ä¸ªç³»ç»Ÿåœ¨ç½‘ç»œä¸Šä¸ä»–äººå…±äº«ç›®å½•å’Œæ–‡ä»¶ã€‚ é€šè¿‡ä½¿ç”¨NFSï¼Œç”¨æˆ·å’Œç¨‹åºå¯ä»¥åƒè®¿é—®æœ¬åœ°æ–‡ä»¶ä¸€æ ·è®¿é—®è¿œç«¯ç³»ç»Ÿä¸Šçš„æ–‡ä»¶ã€‚\n2. NFSçš„å®‰è£…ä¸é…ç½® 2.1 æœåŠ¡ç«¯ NFSéœ€è¦å®‰è£…nfs-utilsã€rpcbindä¸¤ä¸ªåŒ…ã€‚\n#å¯ä»¥å…ˆæ£€æŸ¥ä¸‹æœ¬åœ°æ˜¯å¦å·²ç»å®‰è£…ï¼Œå¦‚æœå®‰è£…åˆ™æ— éœ€é‡å¤å®‰è£…åŒ… [root@k8s-dbg-master-1 build]# rpm -qa|grep rpcbind rpcbind-0.2.0-42.el7.x86_64 [root@k8s-dbg-master-1 build]# rpm -qa|grep nfs libnfsidmap-0.25-17.el7.x86_64 nfs-utils-1.3.0-0.48.el7_4.x86_64 2.1.1. å®‰è£…nfs-utilsã€rpcbindä¸¤ä¸ªåŒ… #centosç³»ç»Ÿ yum -y install nfs-utils rpcbind #Ubuntuç³»ç»Ÿ #æœåŠ¡ç«¯ apt-get install nfs-kernel-server #å®¢æˆ·ç«¯ apt-get install nfs-common 2.1.2. åˆ›å»ºå…±äº«ç›®å½• æœåŠ¡ç«¯å…±äº«ç›®å½•ï¼š/data/nfs-storage/\nmkdir /data/nfs-storage/ 2.1.3. NFSå…±äº«ç›®å½•æ–‡ä»¶é…ç½® vi /etc/exports #æ·»åŠ ä»¥ä¸‹ä¿¡æ¯ /data/nfs-storage *(rw,insecure,sync,no_subtree_check,no_root_squash) ä»¥ä¸Šé…ç½®åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š\nç¬¬ä¸€éƒ¨åˆ†å°±æ˜¯æœ¬åœ°è¦å…±äº«å‡ºå»çš„ç›®å½•ã€‚ ç¬¬äºŒéƒ¨åˆ†ä¸ºå…è®¸è®¿é—®çš„ä¸»æœºï¼ˆå¯ä»¥æ˜¯ä¸€ä¸ªIPä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªIPæ®µï¼‰ï¼Œ*ä»£è¡¨å…è®¸æ‰€æœ‰çš„ç½‘æ®µè®¿é—®ã€‚ ç¬¬ä¸‰éƒ¨åˆ†å°æ‹¬å·é‡Œé¢çš„ï¼Œä¸ºä¸€äº›æƒé™é€‰é¡¹ã€‚ æƒé™è¯´æ˜\nrw ï¼šè¯»å†™ï¼› ro ï¼šåªè¯»ï¼› sync ï¼šåŒæ­¥æ¨¡å¼ï¼Œå†…å­˜ä¸­æ•°æ®æ—¶æ—¶å†™å…¥ç£ç›˜ï¼› async ï¼šä¸åŒæ­¥ï¼ŒæŠŠå†…å­˜ä¸­æ•°æ®å®šæœŸå†™å…¥ç£ç›˜ä¸­ï¼› secure ï¼šnfsé€šè¿‡1024ä»¥ä¸‹çš„å®‰å…¨TCP/IPç«¯å£å‘é€ insecure ï¼šnfsé€šè¿‡1024ä»¥ä¸Šçš„ç«¯å£å‘é€ no_root_squash ï¼šåŠ ä¸Šè¿™ä¸ªé€‰é¡¹åï¼Œrootç”¨æˆ·å°±ä¼šå¯¹å…±äº«çš„ç›®å½•æ‹¥æœ‰è‡³é«˜çš„æƒé™æ§åˆ¶ï¼Œå°±åƒæ˜¯å¯¹æœ¬æœºçš„ç›®å½•æ“ä½œä¸€æ ·ã€‚ä¸å®‰å…¨ï¼Œä¸å»ºè®®ä½¿ç”¨ï¼› root_squash ï¼šå’Œä¸Šé¢çš„é€‰é¡¹å¯¹åº”ï¼Œrootç”¨æˆ·å¯¹å…±äº«ç›®å½•çš„æƒé™ä¸é«˜ï¼Œåªæœ‰æ™®é€šç”¨æˆ·çš„æƒé™ï¼Œå³é™åˆ¶äº†rootï¼› subtree_check ï¼šå¦‚æœå…±äº«/usr/binä¹‹ç±»çš„å­ç›®å½•æ—¶ï¼Œå¼ºåˆ¶nfsæ£€æŸ¥çˆ¶ç›®å½•çš„æƒé™ï¼ˆé»˜è®¤ï¼‰ no_subtree_check ï¼šå’Œä¸Šé¢ç›¸å¯¹ï¼Œä¸æ£€æŸ¥çˆ¶ç›®å½•æƒé™ all_squash ï¼šä¸ç®¡ä½¿ç”¨NFSçš„ç”¨æˆ·æ˜¯è°ï¼Œä»–çš„èº«ä»½éƒ½ä¼šè¢«é™å®šæˆä¸ºä¸€ä¸ªæŒ‡å®šçš„æ™®é€šç”¨æˆ·èº«ä»½ï¼› anonuid/anongid ï¼šè¦å’Œroot_squash ä»¥åŠ all_squashä¸€åŒä½¿ç”¨ï¼Œç”¨äºæŒ‡å®šä½¿ç”¨NFSçš„ç”¨æˆ·é™å®šåçš„uidå’Œgidï¼Œå‰ææ˜¯æœ¬æœºçš„/etc/passwdä¸­å­˜åœ¨è¿™ä¸ªuidå’Œgidã€‚ 2.1.4. å¯åŠ¨NFSæœåŠ¡ #å…ˆå¯åŠ¨rpcbind service rpcbind start #åå¯åŠ¨nfs service nfs start #å¯ä»¥è®¾ç½®å¼€æœºå¯åŠ¨ chkconfig rpcbind on chkconfig nfs on 2.1.5. æœåŠ¡ç«¯éªŒè¯ é€šè¿‡showmount -eå‘½ä»¤å¦‚æœæ­£å¸¸æ˜¾ç¤ºå…±äº«ç›®å½•ï¼Œè¡¨ç¤ºå®‰è£…æ­£å¸¸ã€‚\n[root@k8s-dbg-master-1 build]# showmount -e Export list for k8s-dbg-master-1: /data/nfs-storage * 2.2 å®¢æˆ·ç«¯ 2.2.1. å®‰è£…nfs-utilsçš„åŒ… yum install nfs-utils.x86_64 -y 2.2.2. åˆ›å»ºæŒ‚è½½ç‚¹ å®¢æˆ·ç«¯æŒ‚è½½ç›®å½•ï¼š/mnt/store\nmkdir /mnt/store 2.2.3. æŸ¥çœ‹NFSæœåŠ¡å™¨çš„å…±äº« root@k8s-dbg-node-5:~# showmount -e 172.16.5.4 Export list for 172.16.5.4: /data/nfs-storage * 2.2.4. æŒ‚è½½ mount -t nfs \u003cNFS_SERVER_IP\u003e:\u003cNFS_SERVER_SHARED_DIR\u003e \u003cNFS_CLIENT_MOUNT_DIR\u003e #ä¾‹å¦‚ï¼š mount -t nfs 172.16.5.4:/data/nfs-storage /mnt/store 2.2.5. éªŒè¯æŒ‚è½½ä¿¡æ¯ ä½¿ç”¨mountå‘½ä»¤\nroot@k8s-dbg-node-5:~# mount |grep /mnt/store 172.16.5.4:/data/nfs-storage/k8s-storage/ssd on /mnt/store type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.16.200.24,local_lock=none,addr=172.16.5.4) ä½¿ç”¨df -hå‘½ä»¤\nroot@k8s-dbg-node-5:~# df -h|grep nfs 172.16.5.4:/data/nfs-storage 40G 25G 13G 67% /mnt/store åˆ›å»ºæ–‡ä»¶æµ‹è¯•\n#è¿›å…¥å®¢æˆ·ç«¯çš„æŒ‚è½½ç›®å½•ï¼Œåˆ›å»ºæ–‡ä»¶ cd /mnt/store touch test.txt #è¿›å…¥æœåŠ¡ç«¯çš„å…±äº«ç›®å½•ï¼ŒæŸ¥çœ‹å®¢æˆ·ç«¯åˆ›å»ºçš„æ–‡ä»¶æ˜¯å¦åŒæ­¥ cd /data/nfs-storage ls ","categories":"","description":"","excerpt":"1. NFSç®€ä»‹ NFSï¼Œæ˜¯Network File Systemçš„ç®€å†™ï¼Œå³ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿã€‚ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿæ˜¯FreeBSDæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿä¸­çš„ä¸€ â€¦","ref":"/linux-notes/tools/nfs-usage/","tags":["Linux"],"title":"NFSçš„ä½¿ç”¨"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/shell/","tags":"","title":"Shell"},{"body":"1. ssh/scpå…å¯†ç  AæœåŠ¡å™¨åœ°å€ï¼š10.8.216.25ï¼Œä¸‹é¢ç®€ç§°A\nBæœåŠ¡å™¨åœ°å€ï¼š10.8.216.26ï¼Œä¸‹é¢ç®€ç§°B\nå®ç°Aç™»å½•Bå…å¯†ç ã€‚\n1.1. åœ¨Aç”Ÿæˆå¯†é’¥å¯¹ æ— å¯†ç æ–¹å¼ï¼š\nssh-keygen -t rsa -P è‡ªå®šä¹‰å¯†ç å‚æ•°ï¼š\nssh-keygen -C \u003ccomment\u003e -f \u003ckeyfile\u003e -t rsa -P \"\u003cpassphrase\u003e\" æ‰§è¡Œä¸Šè¿°å‘½ä»¤ï¼Œä¸€è·¯å›è½¦ï¼Œä¼šåœ¨å½“å‰ç™»å½•ç”¨æˆ·çš„homeç›®å½•ä¸‹çš„.sshç›®å½•ä¸‹ç”Ÿæˆid_rsaå’Œid_rsa.pubä¸¤ä¸ªæ–‡ä»¶ï¼Œåˆ†åˆ«ä»£è¡¨å¯†é’¥å¯¹çš„ç§é’¥å’Œå…¬é’¥ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n1.2. æ‹·è´Açš„å…¬é’¥ï¼ˆid_rsa.pubï¼‰åˆ°B è¿™é‡Œæ‹·è´åˆ°Bçš„rootç”¨æˆ·homeç›®å½•ä¸‹ä¸ºä¾‹ï¼š\nscp /root/.ssh/id_rsa.pub root@10.8.216.26:/root 1.3. ç™»å½•B æ‹·è´Açš„id_rsa.pubå†…å®¹åˆ°.sshç›®å½•ä¸‹çš„authorized_keysæ–‡ä»¶ä¸­\ncd /root cat id_rsa.pub \u003e\u003e .ssh/authorized_keys å¦‚å›¾ï¼š\n1.4. ç™»å½•æˆ–æ‹·è´ æ­¤æ—¶åœ¨Aä¸­ç”¨SSHç™»å½•Bæˆ–å‘Bæ‹·è´æ–‡ä»¶ï¼Œå°†ä¸éœ€è¦å¯†ç \nssh root@10.8.216.26 scp abc.txt root@10.8.216.26:/root 2. é…ç½®è·³æ¿æœºå¿«é€Ÿç™»å½• 2.1. é…ç½®ssh configæ–‡ä»¶ ssh config è·¯å¾„ï¼š~/.ssh/config\nAddKeysToAgent yes ServerAliveInterval 3 Host jump HostName {jump_ip} Port {port} User {username} forwardagent yes identityfile ~/.ssh/id_rsa Host *.gw user {username} port {port} proxycommand ssh -W $(echo %h | sed -e \"s/.gw$//\"):%p jump Host bj* User {username} Port {port} proxycommand ssh -W 192.168.123.$(echo %h | awk -F 'bj' '{print $2}'):%p jump å¤šå±‚è·³æ¿æœº\nHost jump1 Hostname {jump1_ip} Port {port} User {username} forwardagent yes identityfile ~/.ssh/id_rsa Host jump2 Hostname {jump2_ip} Port {port} User {username} ProxyCommand ssh -q -x -W %h:%p jump1 Host * Hostname %h Port {port} User {username} ProxyCommand ssh -q -x -W %h:%p jump2 2.2. è®°å½•æœºå™¨æ–‡ä»¶ å°†å…³é”®å­—å’ŒIPå†™å…¥æ–‡ä»¶è®°å½•ï¼Œä¾‹å¦‚ ~/.my_hostsã€‚\nç¤ºä¾‹ï¼šå¯ä»¥æ˜¯IP + ç¯å¢ƒç­‰å…³é”®å­—ï¼Œä¸­é—´ç”¨ç©ºæ ¼éš”å¼€ã€‚\n# release 192.168.123.11 rel-node-11 192.168.123.12 rel-node-12 # pre 192.168.321.13 pre-node-13 192.168.321.14 pre-node-14 192.168.321.15 pre-node-15 # dev 192.168.111.16 dev-node-16 192.168.111.17 dev-node-17 2.3. å®‰è£…fzf # for mac brew install fzf 2.4. è®¾ç½®å‘½ä»¤åˆ«å è®¾ç½® alias åˆ°shell rc æ–‡ä»¶(.bashrc / .zshrc)\nalias goto=\"ssh \\$(cat ~/.my_hosts | fzf | awk '{ printf(\\\"%s.gw\\\", \\$1)}')\" 2.5. ä½¿ç”¨ ä½¿ç”¨åˆ«åå‘½ä»¤ï¼Œè¾“å…¥å…³é”®å­—æœç´¢ï¼Œç‚¹å‡»å›è½¦è¿›å…¥æŒ‡å®šæœºå™¨ã€‚\nä¹Ÿå¯ä»¥ä½¿ç”¨sshå‘½ä»¤ç™»å½•æœºå™¨åˆ«åã€‚\nssh bj11 3. sshé…ç½®é¡¹è¯´æ˜ å¯ä»¥é€šè¿‡manæŸ¥çœ‹sshé…ç½®è¯´æ˜\nman ssh_config é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š\nHost jump port 22 Host * !jump StrictHostKeyChecking no HostName %h UserKnownHostsFile /dev/null LogLevel ERROR IdentityFile ~/.ssh/id_rsa ProxyCommand ssh -p 22 -F /dev/null jump -W %h:%p SendEnv LANG LC_* é…ç½®é¡¹è¯´æ˜ï¼š\nHost: æ ‡è¯†è®¾å¤‡ï¼Œ*è¡¨ç¤ºé€šé…æ‰€æœ‰å­—ç¬¦ï¼Œ!è¡¨ç¤ºä¾‹å¤–é€šé…ã€‚\nStrictHostKeyChecking noï¼šè¿æ¥æ—¶ä¸è¿›è¡Œå…¬é’¥äº¤äº’ç¡®è®¤æ“ä½œã€‚\nUserKnownHostsFile /dev/nullï¼šä¸æç¤ºç¡®è®¤known_hostsæ–‡ä»¶ã€‚\nProxyCommandï¼šä»£ç†å‘½ä»¤\nå¦‚æœä½¿ç”¨å‘½ä»¤åŠ å‚æ•°çš„æ–¹å¼ï¼š\nssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"ssh -p 22 jump -W %h:%p\" ","categories":"","description":"","excerpt":"1. ssh/scpå…å¯†ç  AæœåŠ¡å™¨åœ°å€ï¼š10.8.216.25ï¼Œä¸‹é¢ç®€ç§°A\nBæœåŠ¡å™¨åœ°å€ï¼š10.8.216.26ï¼Œä¸‹é¢ç®€ç§°B\nå®ç°Aç™»å½•B â€¦","ref":"/linux-notes/tools/ssh-tips/","tags":["Linux"],"title":"ssh tips"},{"body":"1. Supervisorç®€ä»‹ Supervisord æ˜¯ç”¨ Python å®ç°çš„ä¸€æ¬¾çš„è¿›ç¨‹ç®¡ç†å·¥å…·ï¼Œsupervisord è¦æ±‚ç®¡ç†çš„ç¨‹åºæ˜¯é daemon ç¨‹åºï¼Œsupervisord ä¼šå¸®ä½ æŠŠå®ƒè½¬æˆ daemon ç¨‹åºï¼Œå› æ­¤å¦‚æœç”¨ supervisord æ¥ç®¡ç†è¿›ç¨‹ï¼Œè¿›ç¨‹éœ€è¦ä»¥édaemonçš„æ–¹å¼å¯åŠ¨ã€‚\nä¾‹å¦‚ï¼šç®¡ç†nginx çš„è¯ï¼Œå¿…é¡»åœ¨ nginx çš„é…ç½®æ–‡ä»¶é‡Œæ·»åŠ ä¸€è¡Œè®¾ç½® daemon off è®© nginx ä»¥é daemon æ–¹å¼å¯åŠ¨ã€‚\n2. Supervisorå®‰è£… ä»¥centosç³»ç»Ÿä¸ºä¾‹ï¼Œä»¥ä¸‹ä¸¤ç§æ–¹å¼é€‰æ‹©å…¶ä¸€ã€‚\n# yum install çš„æ–¹å¼ yum install -y supervisor # easy_installçš„æ–¹å¼ yum install -y python-setuptools easy_install supervisor echo_supervisord_conf \u003e/etc/supervisord.conf 3. Supervisorçš„é…ç½® 3.1. supervisord.confçš„é…ç½® å¦‚æœä½¿ç”¨yum install -y supervisorçš„å‘½ä»¤å®‰è£…ï¼Œä¼šç”Ÿæˆé»˜è®¤é…ç½®/etc/supervisord.confå’Œç›®å½•/etc/supervisord.dï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªè¡Œåˆ›å»ºã€‚\nåœ¨/etc/supervisord.dçš„ç›®å½•ä¸‹åˆ›å»ºconfå’Œlogä¸¤ä¸ªç›®å½•ï¼Œconfç”¨äºå­˜æ”¾ç®¡ç†è¿›ç¨‹çš„é…ç½®ï¼Œlogç”¨äºå­˜æ”¾ç®¡ç†è¿›ç¨‹çš„æ—¥å¿—ã€‚\ncd /etc/supervisord.d mkdir conf log ä¿®æ”¹/etc/supervisord.confçš„[include]éƒ¨åˆ†ï¼Œå³è½½å…¥/etc/supervisord.d/confç›®å½•ä¸‹çš„æ‰€æœ‰é…ç½®ã€‚\nvi /etc/supervisord.conf ... [include] files = supervisord.d/conf/*.conf ... ä¹Ÿå¯ä»¥ä¿®æ”¹supervisoråº”ç”¨æ—¥å¿—çš„ç›®å½•ï¼Œé»˜è®¤æ—¥å¿—è·¯å¾„ä¸º/var/log/supervisor/supervisord.logã€‚\nvi /etc/supervisord.conf ... [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ... 3.2. ç®¡ç†åº”ç”¨çš„é…ç½® è¿›å…¥åˆ°/etc/supervisord.d/confç›®å½•ï¼Œåˆ›å»ºç®¡ç†åº”ç”¨çš„é…ç½®ï¼Œå¯ä»¥åˆ›å»ºå¤šä¸ªåº”ç”¨é…ç½®ã€‚\nä¾‹å¦‚ï¼Œåˆ›å»ºconfd.confé…ç½®ã€‚\n[program:confd] directory = /usr/local/bin ; ç¨‹åºçš„å¯åŠ¨ç›®å½• command = /usr/local/bin/confd -config-file /etc/confd/confd.toml ; å¯åŠ¨å‘½ä»¤ï¼Œä¸å‘½ä»¤è¡Œå¯åŠ¨çš„å‘½ä»¤æ˜¯ä¸€æ ·çš„ autostart = true ; åœ¨ supervisord å¯åŠ¨çš„æ—¶å€™ä¹Ÿè‡ªåŠ¨å¯åŠ¨ startsecs = 5 ; å¯åŠ¨ 5 ç§’åæ²¡æœ‰å¼‚å¸¸é€€å‡ºï¼Œå°±å½“ä½œå·²ç»æ­£å¸¸å¯åŠ¨äº† autorestart = true ; ç¨‹åºå¼‚å¸¸é€€å‡ºåè‡ªåŠ¨é‡å¯ startretries = 3 ; å¯åŠ¨å¤±è´¥è‡ªåŠ¨é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤æ˜¯ 3 user = root ; ç”¨å“ªä¸ªç”¨æˆ·å¯åŠ¨ redirect_stderr = true ; æŠŠ stderr é‡å®šå‘åˆ° stdoutï¼Œé»˜è®¤ false stdout_logfile_maxbytes = 20MB ; stdout æ—¥å¿—æ–‡ä»¶å¤§å°ï¼Œé»˜è®¤ 50MB stdout_logfile_backups = 20 ; stdout æ—¥å¿—æ–‡ä»¶å¤‡ä»½æ•° ; stdout æ—¥å¿—æ–‡ä»¶ï¼Œéœ€è¦æ³¨æ„å½“æŒ‡å®šç›®å½•ä¸å­˜åœ¨æ—¶æ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨åˆ›å»ºç›®å½•ï¼ˆsupervisord ä¼šè‡ªåŠ¨åˆ›å»ºæ—¥å¿—æ–‡ä»¶ï¼‰ stdout_logfile = /etc/supervisord.d/log/confd.log ;æ—¥å¿—ç»Ÿä¸€æ”¾åœ¨logç›®å½•ä¸‹ ; å¯ä»¥é€šè¿‡ environment æ¥æ·»åŠ éœ€è¦çš„ç¯å¢ƒå˜é‡ï¼Œä¸€ç§å¸¸è§çš„ç”¨æ³•æ˜¯ä¿®æ”¹ PYTHONPATH ; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere 4. Surpervisorçš„å¯åŠ¨ # supervisordäºŒè¿›åˆ¶å¯åŠ¨ supervisord -c /etc/supervisord.conf # æ£€æŸ¥è¿›ç¨‹ ps aux | grep supervisord æˆ–è€…ä»¥systemdçš„æ–¹å¼ç®¡ç†\nvi /etc/rc.d/init.d/supervisord\n#!/bin/sh # # /etc/rc.d/init.d/supervisord # # Supervisor is a client/server system that # allows its users to monitor and control a # number of processes on UNIX-like operating # systems. # # chkconfig: - 64 36 # description: Supervisor Server # processname: supervisord # Source init functions . /etc/rc.d/init.d/functions prog=\"supervisord\" prefix=\"/usr\" exec_prefix=\"${prefix}\" prog_bin=\"${exec_prefix}/bin/supervisord\" PIDFILE=\"/var/run/$prog.pid\" start() { echo -n $\"Starting $prog: \" daemon $prog_bin --pidfile $PIDFILE -c /etc/supervisord.conf [ -f $PIDFILE ] \u0026\u0026 success $\"$prog startup\" || failure $\"$prog startup\" echo } stop() { echo -n $\"Shutting down $prog: \" [ -f $PIDFILE ] \u0026\u0026 killproc $prog || success $\"$prog shutdown\" echo } case \"$1\" in start) start ;; stop) stop ;; status) status $prog ;; restart) stop start ;; *) echo \"Usage: $0 {start|stop|restart|status}\" ;; esac è®¾ç½®å¼€æœºå¯åŠ¨åŠsystemdæ–¹å¼å¯åŠ¨ã€‚\nsudo chmod +x /etc/rc.d/init.d/supervisord sudo chkconfig --add supervisord sudo chkconfig supervisord on sudo service supervisord start 5. supervisorctl\u0026supervisord Supervisord å®‰è£…å®Œæˆåæœ‰ä¸¤ä¸ªå¯ç”¨çš„å‘½ä»¤è¡Œ supervisord å’Œ supervisorctlï¼Œå‘½ä»¤ä½¿ç”¨è§£é‡Šå¦‚ä¸‹ï¼š\n5.1. supervisorctl supervisorctl stop programxxxï¼Œåœæ­¢æŸä¸€ä¸ªè¿›ç¨‹(programxxx)ï¼Œprogramxxx ä¸º [program:beepkg] é‡Œé…ç½®çš„å€¼ï¼Œè¿™ä¸ªç¤ºä¾‹å°±æ˜¯ beepkgã€‚ supervisorctl start programxxxï¼Œå¯åŠ¨æŸä¸ªè¿›ç¨‹ã€‚ supervisorctl restart programxxxï¼Œé‡å¯æŸä¸ªè¿›ç¨‹ã€‚ supervisorctl statusï¼ŒæŸ¥çœ‹è¿›ç¨‹çŠ¶æ€ã€‚ supervisorctl stop groupworker ï¼Œé‡å¯æ‰€æœ‰å±äºåä¸º groupworker è¿™ä¸ªåˆ†ç»„çš„è¿›ç¨‹(start,restart åŒç†)ã€‚ supervisorctl stop allï¼Œåœæ­¢å…¨éƒ¨è¿›ç¨‹ï¼Œæ³¨ï¼šstartã€restartã€stop éƒ½ä¸ä¼šè½½å…¥æœ€æ–°çš„é…ç½®æ–‡ä»¶ã€‚ supervisorctl reloadï¼Œè½½å…¥æœ€æ–°çš„é…ç½®æ–‡ä»¶ï¼Œåœæ­¢åŸæœ‰è¿›ç¨‹å¹¶æŒ‰æ–°çš„é…ç½®å¯åŠ¨ã€ç®¡ç†æ‰€æœ‰è¿›ç¨‹ã€‚ supervisorctl updateï¼Œæ ¹æ®æœ€æ–°çš„é…ç½®æ–‡ä»¶ï¼Œå¯åŠ¨æ–°é…ç½®æˆ–æœ‰æ”¹åŠ¨çš„è¿›ç¨‹ï¼Œé…ç½®æ²¡æœ‰æ”¹åŠ¨çš„è¿›ç¨‹ä¸ä¼šå—å½±å“è€Œé‡å¯ã€‚ æ›´å¤šå‚è€ƒï¼š\n$ supervisorctl --help supervisorctl -- control applications run by supervisord from the cmd line. Usage: /usr/bin/supervisorctl [options] [action [arguments]] Options: -c/--configuration -- configuration file path (default /etc/supervisord.conf) -h/--help -- print usage message and exit -i/--interactive -- start an interactive shell after executing commands -s/--serverurl URL -- URL on which supervisord server is listening (default \"http://localhost:9001\"). -u/--username -- username to use for authentication with server -p/--password -- password to use for authentication with server -r/--history-file -- keep a readline history (if readline is available) action [arguments] -- see below Actions are commands like \"tail\" or \"stop\". If -i is specified or no action is specified on the command line, a \"shell\" interpreting actions typed interactively is started. Use the action \"help\" to find out about available actions. ä¾‹å¦‚ï¼š\n# supervisorctl status confd RUNNING pid 31256, uptime 0:11:24 twemproxy RUNNING pid 31255, uptime 0:11:24 5.2. supervisord supervisordï¼Œåˆå§‹å¯åŠ¨ Supervisordï¼Œå¯åŠ¨ã€ç®¡ç†é…ç½®ä¸­è®¾ç½®çš„è¿›ç¨‹ã€‚ $ supervisord --help supervisord -- run a set of applications as daemons. Usage: /usr/bin/supervisord [options] Options: -c/--configuration FILENAME -- configuration file -n/--nodaemon -- run in the foreground (same as 'nodaemon true' in config file) -h/--help -- print this usage message and exit -v/--version -- print supervisord version number and exit -u/--user USER -- run supervisord as this user (or numeric uid) -m/--umask UMASK -- use this umask for daemon subprocess (default is 022) -d/--directory DIRECTORY -- directory to chdir to when daemonized -l/--logfile FILENAME -- use FILENAME as logfile path -y/--logfile_maxbytes BYTES -- use BYTES to limit the max size of logfile -z/--logfile_backups NUM -- number of backups to keep when max bytes reached -e/--loglevel LEVEL -- use LEVEL as log level (debug,info,warn,error,critical) -j/--pidfile FILENAME -- write a pid file for the daemon process to FILENAME -i/--identifier STR -- identifier used for this instance of supervisord -q/--childlogdir DIRECTORY -- the log directory for child process logs -k/--nocleanup -- prevent the process from performing cleanup (removal of old automatic child log files) at startup. -a/--minfds NUM -- the minimum number of file descriptors for start success -t/--strip_ansi -- strip ansi escape codes from process output --minprocs NUM -- the minimum number of processes available for start success --profile_options OPTIONS -- run supervisord under profiler and output results based on OPTIONS, which is a comma-sep'd list of 'cumulative', 'calls', and/or 'callers', e.g. 'cumulative,callers') 6. Supervisoræ§åˆ¶å° åœ¨/etc/supervisord.confä¸­ä¿®æ”¹[inet_http_server] çš„å‚æ•°ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n[inet_http_server] ; inet (TCP) server disabled by default port=*:9001 ; ip_address:port specifier, *:port for all iface username=root ; default is no username (open server) password=xxxx ; default is no password (open server) ä¿®æ”¹åé‡å¯supervisorè¿›ç¨‹ï¼Œåœ¨æµè§ˆå™¨è®¿é—® http://\u003chost-ip\u003e:9001ã€‚\nå…·ä½“å¦‚ä¸‹ï¼š\n7. supervisor.confè¯¦ç»†é…ç½® cat /etc/supervisord.conf\n; Sample supervisor config file. [unix_http_server] file=/var/run/supervisor/supervisor.sock ; (the path to the socket file) ;chmod=0700 ; sockef file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) nodaemon=false ; (start in foreground if true;default false) minfds=1024 ; (min. avail startup file descriptors;default 1024) minprocs=200 ; (min. avail process descriptors;default 200) ;umask=022 ; (process file creation umask;default 022) ;user=chrism ; (default is current user, required if root) ;identifier=supervisor ; (supervisord identifier, default is 'supervisor') ;directory=/tmp ; (default is not to cd during start) ;nocleanup=true ; (don't clean up tempfiles at start;default false) ;childlogdir=/tmp ; ('AUTO' child log dir, default $TEMP) ;environment=KEY=value ; (key value pairs to add to environment) ;strip_ansi=false ; (strip ansi escape codes in logs; def. false) ; the below section must remain in the config file for RPC ; (supervisorctl/web interface) to work, additional interfaces may be ; added by defining them in separate rpcinterface: sections [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///var/run/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ; The below sample program section shows all possible program subsection values, ; create one or more 'real' program: sections to be able to control them under ; supervisor. ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;autorestart=true ; retstart at unexpected quit (default: true) ;startsecs=10 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; 'expected' exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=1,B=2 ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample eventlistener section shows all possible ; eventlistener subsection values, create one or more 'real' ; eventlistener: sections to be able to handle event notifications ; sent by supervisor. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;autorestart=unexpected ; restart at unexpected quit (default: unexpected) ;startsecs=10 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; 'expected' exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups ; # of stderr logfile backups (default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=1,B=2 ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample group section shows all possible group values, ; create one or more 'real' group: sections to create \"heterogeneous\" ; process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/conf/*.conf å‚è€ƒï¼š\nhttp://supervisord.org/\n","categories":"","description":"","excerpt":"1. Supervisorç®€ä»‹ Supervisord æ˜¯ç”¨ Python å®ç°çš„ä¸€æ¬¾çš„è¿›ç¨‹ç®¡ç†å·¥å…·ï¼Œsupervisord è¦æ±‚ç®¡ç†çš„ç¨‹åº â€¦","ref":"/linux-notes/tools/supervisor-usage/","tags":["Linux"],"title":"Supervisorçš„ä½¿ç”¨"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vim/","tags":"","title":"VIM"},{"body":"1. viçš„æ¨¡å¼ 1.1. æ™®é€šæ¨¡å¼ ç”±Shellè¿›å…¥viç¼–è¾‘å™¨æ—¶ï¼Œé¦–å…ˆè¿›å…¥æ™®é€šæ¨¡å¼ã€‚åœ¨æ™®é€šæ¨¡å¼ä¸‹ï¼Œä»é”®ç›˜è¾“å…¥ä»»ä½•å­—ç¬¦éƒ½è¢«å½“ä½œå‘½ä»¤æ¥è§£é‡Šã€‚æ™®é€šæ¨¡å¼ä¸‹æ²¡æœ‰ä»»ä½•æç¤ºç¬¦ï¼Œè¾“å…¥å‘½ä»¤åç«‹å³æ‰§è¡Œï¼Œä¸éœ€è¦å›è½¦ï¼Œè€Œä¸”è¾“å…¥çš„å­—ç¬¦ä¸ä¼šåœ¨å±å¹•ä¸Šæ˜¾ç¤ºå‡ºæ¥ã€‚\n1.2. ç¼–è¾‘æ¨¡å¼ ç¼–è¾‘æ¨¡å¼ä¸»è¦ç”¨äºæ–‡æœ¬çš„ç¼–è¾‘ã€‚è¯¥æ¨¡å¼ä¸‹ç”¨æˆ·è¾“å…¥çš„ä»»ä½•å­—ç¬¦éƒ½è¢«ä½œä¸ºæ–‡ä»¶çš„å†…å®¹ä¿å­˜èµ·æ¥ï¼Œå¹¶åœ¨å±å¹•ä¸Šæ˜¾ç¤ºå‡ºæ¥ã€‚\n1.3. å‘½ä»¤æ¨¡å¼ å‘½ä»¤æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·å¯ä»¥å¯¹æ–‡ä»¶è¿›è¡Œä¸€äº›é«˜çº§å¤„ç†ã€‚å°½ç®¡æ™®é€šæ¨¡å¼ä¸‹çš„å‘½ä»¤å¯ä»¥å®Œæˆå¾ˆå¤šåŠŸèƒ½ï¼Œä½†è¦æ‰§è¡Œä¸€äº›å¦‚å­—ç¬¦ä¸²æŸ¥æ‰¾ã€æ›¿æ¢ã€æ˜¾ç¤ºè¡Œå·ç­‰æ“ä½œè¿˜æ˜¯å¿…é¡»è¦è¿›å…¥å‘½ä»¤æ¨¡å¼ã€‚\nä¹Ÿæœ‰æ–‡ç« ç§°ä¸ºä¸¤ç§å·¥ä½œæ¨¡å¼ï¼Œå³æŠŠå‘½ä»¤æ¨¡å¼åˆå¹¶åˆ°æ™®é€šæ¨¡å¼ã€‚\nå¦‚æœä¸ç¡®å®šå½“å‰å¤„äºå“ªç§æ¨¡å¼ï¼ŒæŒ‰ä¸¤æ¬¡ Esc é”®å°†å›åˆ°æ™®é€šæ¨¡å¼ã€‚\n2. vimå‘½ä»¤æ±‡æ€» é«˜çº§æ±‡æ€»\n3. vimå‘½ä»¤åˆ†ç±» 3.1. åŸºç¡€ç¼–è¾‘ã€ç§»åŠ¨å…‰æ ‡ æŒ‡ä»¤ è§£é‡Š $ è¡Œå°¾ ^ è¡Œé¦– w ä¸‹ä¸€ä¸ªå•è¯ (è¯é¦–ï¼‰ e ä¸‹ä¸€ä¸ªå•è¯ï¼ˆè¯å°¾ï¼‰ b å‰ä¸€ä¸ªå•è¯ x del åˆ é™¤åä¸€ä¸ªå­—ç¬¦ X backspace åˆ é™¤å‰ä¸€ä¸ªå­—ç¬¦ u æ’¤é”€ ctrl + r é‡åš k ä¸Š h ä¸‹ g å·¦ l å³ i æ’å…¥ï¼Œå¼€å§‹å†™ä¸œè¥¿ s è¦†ç›– esc é€€å‡ºè¾“å…¥æ¨¡å¼ï¼Œè¿›å…¥æ™®é€šæ¨¡å¼ï¼Œå¯æ‰§è¡Œå„ç§å‘½ä»¤ 3.2. æ“ä½œå’Œé‡å¤æ“ä½œ æŒ‡ä»¤ è§£é‡Š f æŸ¥æ‰¾å­—ç¬¦ï¼ŒæŒ‰fåå†æŒ‰éœ€è¦ç§»åŠ¨åˆ°çš„å­—ç¬¦ï¼Œå…‰æ ‡å°±ä¼šç§»åŠ¨åˆ°é‚£ f; å°±ä¼šç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª ;çš„ä½ç½® F åå‘æŸ¥æ‰¾å­—ç¬¦ . é‡å¤ä¸Šä¸€ä¸ªæ“ä½œ v é€‰æ‹©æ¨¡å¼ï¼Œç”¨ä¸Šä¸‹å·¦å³é€‰æ‹©æ–‡æœ¬ï¼ŒæŒ‰ç›¸åº”çš„æŒ‡ä»¤ç›´æ¥æ‰§è¡Œï¼Œå¦‚ï¼šé€‰ä¸­åæ‰§è¡Œ d å°±ç›´æ¥åˆ é™¤é€‰ä¸­çš„æ–‡æœ¬ ctrl + v å—çŠ¶é€‰æ‹©æ¨¡å¼ï¼Œå¯ä»¥çºµå‘é€‰æ‹©æ–‡æœ¬å—ï¼Œè€Œéä»¥è¡Œçš„å½¢å¼ d é«˜çº§åˆ é™¤æŒ‡ä»¤ï¼š dw åˆ é™¤ä¸€ä¸ªå•è¯ df( é…åˆ f ï¼Œåˆ é™¤ä»å…‰æ ‡å¤„åˆ° ( çš„å­—ç¬¦ï¼Œå•è¡Œæ“ä½œ dd åˆ é™¤å½“å‰è¡Œ d2w åˆ é™¤ä¸¤ä¸ªå•è¯ d2t, åˆ é™¤å½“å‰ä½ç½®åˆ°åé¢ç¬¬äºŒä¸ª , ä¹‹é—´çš„å†…å®¹ï¼Œä¸åŒ…å« , ï¼ˆt = toï¼‰ 3.3. å¤åˆ¶ å’Œ ç²˜è´´ æŒ‡ä»¤ è§£é‡Š y å¤åˆ¶ yy å¤åˆ¶å½“å‰è¡Œ p ç²˜è´´åˆ°åé¢ P ç²˜è´´åˆ°å‰é¢ o åœ¨å½“å‰è¡Œçš„ä¸‹ä¸€è¡Œæ·»åŠ ç©ºè¡Œå¹¶å¼€å§‹è¾“å…¥ O åœ¨å½“å‰è¡Œçš„ä¸Šä¸€è¡Œæ·»åŠ ç©ºè¡Œå¹¶å¼€å§‹è¾“å…¥ æ‰€æœ‰ç»è¿‡ d x e å¤„ç†çš„å­—ç¬¦ä¸²éƒ½å·²ç»å¤åˆ¶åˆ°äº†ç²˜è´´æ¿ä¸Šã€‚\n3.4. æœç´¢ æŒ‡ä»¤ è§£é‡Š / ä»å½“å‰ä½ç½®å‘åæœç´¢ ï¼Ÿ ä»å½“å‰ä½ç½®åå‰æœç´¢ n æœç´¢å®Œä¹‹åï¼Œå¦‚æœæœ‰å¤šä¸ªç»“æœï¼Œè·³åˆ° ä¸‹ä¸€ä¸ªåŒ¹ é…é¡¹ N è·³åˆ° ä¸Šä¸€ä¸ª åŒ¹é…é¡¹ * ç›´æ¥åŒ¹é…å½“å‰å…‰æ ‡ä¸‹é¢çš„å­—ç¬¦ä¸²ï¼Œç§»åˆ°ä¸‹ä¸€ä¸ªåŒ¹é…é¡¹ï¼Œè·Ÿ/ ? æ²¡æœ‰å…³ç³» # ä¸Šä¸€ä¸ªåŒ¹é…é¡¹ 3.5. æ ‡è®° å’Œ å® æ ‡è®°\nm åè·Ÿ a - z ä»»æ„å­—ç¬¦æ¥è®¾ç½®ä¸€ä¸ªæ ‡è®°\n`` `åè·Ÿ å­—ç¬¦æ¥è·³åˆ°è¿™ä¸ªæ ‡è®°ç‚¹\nå¤§å†™ A - Z æ˜¯å…¨å±€çš„ï¼Œå°å†™ a - z\n'. ä»£è¡¨æœ€åç¼–è¾‘ä½ç½®\nå®\nq åæ¥ a - z å¼€å§‹å½•åˆ¶å®\nq ç»“æŸå®çš„å½•åˆ¶\n@ åæ¥ a - z è¯»å–å®\n@@ ä»£è¡¨æœ€åä¸€ä¸ªå®\n3.6. é«˜çº§ç§»åŠ¨ % åœ¨é…å¯¹çš„ () [] ä¹‹é—´ç§»åŠ¨\nH M L ç§»åŠ¨åˆ°ç¼–è¾‘å™¨å¯è§†èŒƒå›´çš„å¤´éƒ¨ï¼Œä¸­é—´ï¼Œå°¾éƒ¨\nG åˆ°æ–‡ä»¶çš„å°¾éƒ¨ï¼Œå‰é¢æ·»åŠ æ•°å­—å†æŒ‰ G è·³åˆ°è¾“å…¥çš„è¡Œï¼Œå†™è¡Œå·çš„æ—¶å€™æ˜¯çœ‹ä¸è§çš„\n- + è·³åˆ°ä¸Šä¸€è¡Œï¼Œä¸‹ä¸€è¡Œ\n( ) è·³åˆ°å½“å‰å¥å­çš„ é¦– / å°¾\n{ } è·³åˆ° å‰ä¸€ä¸ª / åä¸€ä¸ª ç©ºè¡Œ\n[[ jumps to the previous { in column 0\n]] jumps to the next } column 0\n3.7. é«˜çº§æŒ‡ä»¤ J åˆå¹¶å½“å‰è¡Œä¸ä¸‹ä¸€è¡Œã€‚åˆå¹¶å·²é€‰ä¸­çš„æ‰€æœ‰è¡Œã€‚\nr æ›¿æ¢å½“å‰å­—ç¬¦åˆ°ä¸‹ä¸€ä¸ªè¾“å…¥çš„å­—ç¬¦ã€‚å¦‚ï¼š r åæ¥ 4 ä¼šæŠŠå½“å‰å­—ç¬¦æ›¿æ¢æˆ 4\nC æ˜¯ c$ çš„ç¼©å†™ï¼šä¿®æ”¹ä»å…‰æ ‡åˆ°ç»“å°¾\nD æ˜¯ d$ çš„ç¼©å†™ï¼šåˆ é™¤ä»å…‰æ ‡åˆ°ç»“å°¾\nY æ˜¯ yy çš„ç¼©å†™ï¼šå¤åˆ¶å½“å‰è¡Œ\ns åˆ é™¤å…‰æ ‡ä¸‹å­—ç¬¦ï¼Œå¹¶å¼€å§‹ç¼–è¾‘\nS åˆ é™¤å½“å‰è¡Œï¼Œå¹¶å¼€å§‹ç¼–è¾‘\n\u003c å‘å‰ç¼©è¿›ï¼Œä¸€è¡Œï¼Œæˆ–å¤šè¡Œï¼ŒèŒƒå›´è®¾ç½®åœ¨å‰é¢æåˆ°äº†ï¼Œtç­‰ç­‰\n\u003e å‘åç¼©è¿›ï¼Œä¸€è¡Œï¼Œæˆ–å¤šè¡Œ\n= æ ¼å¼åŒ–ï¼Œä¸€è¡Œï¼Œæˆ–å¤šè¡Œ\n~ åˆ‡æ¢å…‰æ ‡ä¸‹çš„å­—ç¬¦å¤§å°å†™\nå‚è€ƒï¼š\næœ¬æ–‡ç”±ä»¥ä¸‹æ–‡ç« æ•´ç†å¾—\nhttp://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.html https://segmentfault.com/a/1190000016056004#articleHeader15 ","categories":"","description":"","excerpt":"1. viçš„æ¨¡å¼ 1.1. æ™®é€šæ¨¡å¼ ç”±Shellè¿›å…¥viç¼–è¾‘å™¨æ—¶ï¼Œé¦–å…ˆè¿›å…¥æ™®é€šæ¨¡å¼ã€‚åœ¨æ™®é€šæ¨¡å¼ä¸‹ï¼Œä»é”®ç›˜è¾“å…¥ä»»ä½•å­—ç¬¦éƒ½è¢«å½“ä½œå‘½ä»¤æ¥è§£é‡Šã€‚æ™® â€¦","ref":"/linux-notes/ide/vim/vim-keymap/","tags":["VIM"],"title":"vim å‘½ä»¤"},{"body":"vimrc ä¸­æ–‡ç‰ˆ ç”± https://blog.51cto.com/zpf666/2335640 è½¬è½½\n\"~/.vimrc \"vim config file \"date 2018-12-26 \"Created by bert \"blog:https://blog.51cto.com/zpf666 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eå…¨å±€é…ç½®\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"å…³é—­viå…¼å®¹æ¨¡å¼\" set nocompatible \"è®¾ç½®å†å²è®°å½•æ­¥æ•°\" set history=1000 \"å¼€å¯ç›¸å…³æ’ä»¶\" \"ä¾¦æµ‹æ–‡ä»¶ç±»å‹\" filetype on \"è½½å…¥æ–‡ä»¶ç±»å‹æ’ä»¶\" filetype plugin on \"ä¸ºç‰¹å®šæ–‡ä»¶ç±»å‹è½½å…¥ç›¸å…³ç¼©è¿›æ–‡ä»¶\" filetype indent on \"å½“æ–‡ä»¶åœ¨å¤–éƒ¨è¢«ä¿®æ”¹æ—¶ï¼Œè‡ªåŠ¨æ›´æ–°è¯¥æ–‡ä»¶\" set autoread \"æ¿€æ´»é¼ æ ‡çš„ä½¿ç”¨\" set mouse=a set selection=exclusive set selectmode=mouse,key \"ä¿å­˜å…¨å±€å˜é‡\" set viminfo+=! \"å¸¦æœ‰å¦‚ä¸‹ç¬¦å·çš„å•è¯ä¸è¦è¢«æ¢è¡Œåˆ†å‰²\" set iskeyword+=_,$,@,%,#,- \"é€šè¿‡ä½¿ç”¨: commandså‘½ä»¤ï¼Œå‘Šè¯‰æˆ‘ä»¬æ–‡ä»¶çš„å“ªä¸€è¡Œè¢«æ”¹å˜è¿‡\" set report=0 \"è¢«åˆ†å‰²çš„çª—å£é—´æ˜¾ç¤ºç©ºç™½ï¼Œä¾¿äºé˜…è¯»\" set fillchars=vert:\\ ,stl:\\ ,stlnc:\\ \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eå­—ä½“å’Œé¢œè‰²\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"è‡ªåŠ¨å¼€å¯è¯­æ³•é«˜äº®\" syntax enable \"è®¾ç½®å­—ä½“\" \"set guifont=dejaVu\\ Sans\\ MONO\\ 10 set guifont=Courier_New:h10:cANSI \"è®¾ç½®é¢œè‰²\" \"colorscheme desert \"é«˜äº®æ˜¾ç¤ºå½“å‰è¡Œ\" set cursorline hi cursorline guibg=#00ff00 hi CursorColumn guibg=#00ff00 \"é«˜äº®æ˜¾ç¤ºæ™®é€štxtæ–‡ä»¶ï¼ˆéœ€è¦txt.vimè„šæœ¬ï¼‰\" au BufRead,BufNewFile * setfiletype txt \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eä»£ç æŠ˜å åŠŸèƒ½\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"æ¿€æ´»æŠ˜å åŠŸèƒ½\" set foldenable \"set nofenï¼ˆè¿™ä¸ªæ˜¯å…³é—­æŠ˜å åŠŸèƒ½ï¼‰\" \"è®¾ç½®æŒ‰ç…§è¯­æ³•æ–¹å¼æŠ˜å ï¼ˆå¯ç®€å†™set fdm=XXï¼‰\" \"æœ‰6ç§æŠ˜å æ–¹æ³•ï¼š \"manual æ‰‹å·¥å®šä¹‰æŠ˜å \" \"indent æ›´å¤šçš„ç¼©è¿›è¡¨ç¤ºæ›´é«˜çº§åˆ«çš„æŠ˜å \" \"expr ç”¨è¡¨è¾¾å¼æ¥å®šä¹‰æŠ˜å \" \"syntax ç”¨è¯­æ³•é«˜äº®æ¥å®šä¹‰æŠ˜å \" \"diff å¯¹æ²¡æœ‰æ›´æ”¹çš„æ–‡æœ¬è¿›è¡ŒæŠ˜å \" \"marker å¯¹æ–‡ä¸­çš„æ ‡å¿—è¿›è¡ŒæŠ˜å \" set foldmethod=manual \"set fdl=0ï¼ˆè¿™ä¸ªæ˜¯ä¸é€‰ç”¨ä»»ä½•æŠ˜å æ–¹æ³•ï¼‰\" \"è®¾ç½®æŠ˜å åŒºåŸŸçš„å®½åº¦\" \"å¦‚æœä¸ä¸º0ï¼Œåˆ™åœ¨å±å¹•å·¦ä¾§æ˜¾ç¤ºä¸€ä¸ªæŠ˜å æ ‡è¯†åˆ— \"åˆ†åˆ«ç”¨â€œ-â€å’Œâ€œ+â€æ¥è¡¨ç¤ºæ‰“å¼€å’Œå…³é—­çš„æŠ˜å  set foldcolumn=0 \"è®¾ç½®æŠ˜å å±‚æ•°ä¸º3\" setlocal foldlevel=3 \"è®¾ç½®ä¸ºè‡ªåŠ¨å…³é—­æŠ˜å \" set foldclose=all \"ç”¨ç©ºæ ¼é”®æ¥ä»£æ›¿zoå’Œzcå¿«æ·é”®å®ç°å¼€å…³æŠ˜å \" \"zo O-pen a fold (æ‰“å¼€æŠ˜å ) \"zc C-lose a fold (å…³é—­æŠ˜å ) \"zf F-old creation (åˆ›å»ºæŠ˜å ) \"nnoremap \u003cspace\u003e @=((foldclosed(line('.')) \u003c 0) ? 'zc' : 'zo')\u003cCR\u003e \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eæ–‡å­—å¤„ç†\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"ä½¿ç”¨ç©ºæ ¼æ¥æ›¿æ¢Tab\" set expandtab \"è®¾ç½®æ‰€æœ‰çš„Tabå’Œç¼©è¿›ä¸º4ä¸ªç©ºæ ¼\" set tabstop=4 \"è®¾å®š\u003c\u003cå’Œ\u003e\u003eå‘½ä»¤ç§»åŠ¨æ—¶çš„å®½åº¦ä¸º4\" set shiftwidth=4 \"ä½¿å¾—æŒ‰é€€æ ¼é”®æ—¶å¯ä»¥ä¸€æ¬¡åˆ é™¤4ä¸ªç©ºæ ¼\" set softtabstop=4 set smarttab \"ç¼©è¿›ï¼Œè‡ªåŠ¨ç¼©è¿›ï¼ˆç»§æ‰¿å‰ä¸€è¡Œçš„ç¼©è¿›ï¼‰\" \"set autoindent å‘½ä»¤æ‰“å¼€è‡ªåŠ¨ç¼©è¿›ï¼Œæ˜¯ä¸‹é¢é…ç½®çš„ç¼©å†™ \"å¯ä½¿ç”¨autoindentå‘½ä»¤çš„ç®€å†™ï¼Œå³â€œ:set aiâ€å’Œâ€œ:set noaiâ€ \"è¿˜å¯ä»¥ä½¿ç”¨â€œ:set ai sw=4â€åœ¨ä¸€ä¸ªå‘½ä»¤ä¸­æ‰“å¼€ç¼©è¿›å¹¶è®¾ç½®ç¼©è¿›çº§åˆ« set ai set cindent \"æ™ºèƒ½ç¼©è¿›\" set si \"è‡ªåŠ¨æ¢è¡Œâ€ set wrap \"è®¾ç½®è½¯å®½åº¦\" set sw=4 \"è¡Œå†…æ›¿æ¢\" set gdefault \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eVim ç•Œé¢\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"å¢å¼ºæ¨¡å¼ä¸­çš„å‘½ä»¤è¡Œè‡ªåŠ¨å®Œæˆæ“ä½œ\" set wildmenu \"æ˜¾ç¤ºæ ‡å°º\" set ruler \"è®¾ç½®å‘½ä»¤è¡Œçš„é«˜åº¦\" set cmdheight=1 \"æ˜¾ç¤ºè¡Œæ•°\" set nu \"ä¸è¦å›¾å½¢æŒ‰é’®\" set go= \"åœ¨æ‰§è¡Œå®å‘½ä»¤æ—¶ï¼Œä¸è¿›è¡Œæ˜¾ç¤ºé‡ç»˜ï¼›åœ¨å®å‘½ä»¤æ‰§è¡Œå®Œæˆåï¼Œä¸€æ¬¡æ€§é‡ç»˜ï¼Œä»¥ä¾¿æé«˜æ€§èƒ½\" set lz \"ä½¿å›æ ¼é”®ï¼ˆbackspaceï¼‰æ­£å¸¸å¤„ç†indent, eol, startç­‰\" set backspace=eol,start,indent \"å…è®¸ç©ºæ ¼é”®å’Œå…‰æ ‡é”®è·¨è¶Šè¡Œè¾¹ç•Œ\" set whichwrap+=\u003c,\u003e,h,l \"è®¾ç½®é­”æœ¯\" set magic \"å…³é—­é‡åˆ°é”™è¯¯æ—¶çš„å£°éŸ³æç¤º\" \"å…³é—­é”™è¯¯ä¿¡æ¯å“é“ƒ\" set noerrorbells \"å…³é—­ä½¿ç”¨å¯è§†å“é“ƒä»£æ›¿å‘¼å«\" set novisualbell \"é«˜äº®æ˜¾ç¤ºåŒ¹é…çš„æ‹¬å·([{å’Œ}])\" set showmatch \"åŒ¹é…æ‹¬å·é«˜äº®çš„æ—¶é—´ï¼ˆå•ä½æ˜¯ååˆ†ä¹‹ä¸€ç§’ï¼‰\" set mat=2 \"å…‰æ ‡ç§»åŠ¨åˆ°bufferçš„é¡¶éƒ¨å’Œåº•éƒ¨æ—¶ä¿æŒ3è¡Œè·ç¦»\" set scrolloff=3 \"æœç´¢é€å­—ç¬¦é«˜äº®\" set hlsearch set incsearch \"æœç´¢æ—¶ä¸åŒºåˆ†å¤§å°å†™\" \"è¿˜å¯ä»¥ä½¿ç”¨ç®€å†™ï¼ˆâ€œ:set icâ€å’Œâ€œ:set noicâ€ï¼‰\" set ignorecase \"ç”¨æµ…è‰²é«˜äº®æ˜¾ç¤ºå½“å‰è¡Œ\" autocmd InsertLeave * se nocul autocmd InsertEnter * se cul \"è¾“å…¥çš„å‘½ä»¤æ˜¾ç¤ºå‡ºæ¥ï¼Œçœ‹çš„æ¸…æ¥š\" set showcmd \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eç¼–ç è®¾ç½®\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"è®¾ç½®ç¼–ç \" set encoding=utf-8 set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936 \"è®¾ç½®æ–‡ä»¶ç¼–ç \" set fileencodings=utf-8 \"è®¾ç½®ç»ˆç«¯ç¼–ç \" set termencoding=utf-8 \"è®¾ç½®è¯­è¨€ç¼–ç \" set langmenu=zh_CN.UTF-8 set helplang=cn \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eå…¶ä»–è®¾ç½®\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"å¼€å¯æ–°è¡Œæ—¶ä½¿ç”¨æ™ºèƒ½è‡ªåŠ¨ç¼©è¿›\" set smartindent set cin set showmatch \"åœ¨å¤„ç†æœªä¿å­˜æˆ–åªè¯»æ–‡ä»¶çš„æ—¶å€™ï¼Œå¼¹å‡ºç¡®è®¤\" set confirm \"éšè—å·¥å…·æ \" set guioptions-=T \"éšè—èœå•æ \" set guioptions-=m \"ç½®ç©ºé”™è¯¯é“ƒå£°çš„ç»ˆç«¯ä»£ç \" set vb t_vb= \"æ˜¾ç¤ºçŠ¶æ€æ ï¼ˆé»˜è®¤å€¼ä¸º1ï¼Œè¡¨ç¤ºæ— æ³•æ˜¾ç¤ºçŠ¶æ€æ ï¼‰\" set laststatus=2 \"çŠ¶æ€è¡Œæ˜¾ç¤ºçš„å†…å®¹\" set statusline=%F%m%r%h%w\\ [FORMAT=%{\u0026ff}]\\ [TYPE=%Y]\\ [POS=%l,%v][%p%%]\\ %{strftime(\\\"%d/%m/%y\\ -\\ %H:%M\\\")} \"ç²˜è´´ä¸æ¢è¡Œé—®é¢˜çš„è§£å†³æ–¹æ³•\" set pastetoggle=\u003cF9\u003e \"è®¾ç½®èƒŒæ™¯é¢œè‰²\" set background=dark \"æ–‡ä»¶ç±»å‹è‡ªåŠ¨æ£€æµ‹ï¼Œä»£ç æ™ºèƒ½è¡¥å…¨\" set completeopt=longest,preview,menu \"å…±äº«å‰ªåˆ‡æ¿\" set clipboard+=unnamed \"ä»ä¸å¤‡ä»½\" set nobackup set noswapfile \"è‡ªåŠ¨ä¿å­˜\" set autowrite \"æ˜¾ç¤ºä¸­æ–‡å¸®åŠ©\" if version \u003e= 603 set helplang=cn set encoding=utf-8 endif \"è®¾ç½®é«˜äº®ç›¸å…³é¡¹\" highlight Search ctermbg=black ctermfg=white guifg=white guibg=black \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=\u003eåœ¨shellè„šæœ¬å¼€å¤´è‡ªåŠ¨å¢åŠ è§£é‡Šå™¨ä»¥åŠä½œè€…ç­‰ç‰ˆæƒä¿¡æ¯\u003c=\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"æ–°å»º.py,.cc,.sh,.javaæ–‡ä»¶ï¼Œè‡ªåŠ¨æ’å…¥æ–‡ä»¶å¤´\" autocmd BufNewFile *.py,*.cc,*.sh,*.java exec \":call SetTitle()\" \"å®šä¹‰å‡½æ•°SetTitleï¼Œè‡ªåŠ¨æ’å…¥æ–‡ä»¶å¤´\" func SetTitle() if expand (\"%:e\") == 'sh' call setline(1, \"#!/bin/bash\") call setline(2, \"#Author:bert\") call setline(3, \"#Blog:https://blog.51cto.com/zpf666\") call setline(4, \"#Time:\".strftime(\"%F %T\")) call setline(5, \"#Name:\".expand(\"%\")) call setline(6, \"#Version:V1.0\") call setline(7, \"#Description:This is a production script.\") endif endfunc ","categories":"","description":"","excerpt":"vimrc ä¸­æ–‡ç‰ˆ ç”± https://blog.51cto.com/zpf666/2335640 è½¬è½½\n\"~/.vimrc \"vim â€¦","ref":"/linux-notes/ide/vim/vimrc-cn/","tags":["VIM"],"title":"vim é…ç½®"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","tags":"","title":"å¤§æ¨¡å‹"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":"","title":"é—®é¢˜æ’æŸ¥"}]